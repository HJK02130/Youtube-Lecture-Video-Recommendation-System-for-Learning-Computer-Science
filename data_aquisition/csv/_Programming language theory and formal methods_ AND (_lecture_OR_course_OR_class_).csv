id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
ECNBf3KfOYM,27,"Marco Gaboardi (SUNY, Buffalo)
https://simons.berkeley.edu/talks/formal-methods-and-proofs-privacy-properties-part-iii
Data Privacy: Foundations and Applications Boot Camp",2019-01-31T23:54:11Z,"Formal Methods and Proofs of Privacy Properties, Part III",https://i.ytimg.com/vi/ECNBf3KfOYM/hqdefault.jpg,Simons Institute,PT1H1M56S,false,148,6,0,0,1,so with the second part I don't think there will be a third part but let's focus on the second one so I already show you that the goal of formal method is a building application that are correct and now in this part I don't know the number but I will focus on a formal verification which is a we can think about it as a kind of sub branch of formal methods that aim at approving application correct so before it was about a building application that are correct with respect to differential privacy now we take something that people can design in their own way and we try to prove them different differentially private yes so formal verification is quite old it started more or less when it started and I like usually presented through the world of terrain who already thought a bit about this problem and it was asking himself how can one check a routine in the sense of making sure that it is right and the ants were was already there in order that the man who checks ok his mistake was this one not only a man but a man in the machine who checks may not have too difficult that as the programmer should make a number of definite assertion which can be checking the Mid Valley and from which the correctness of the old program and directly follow and this is what I will do in this hour I will take some example and do exactly this I go step by step through assertions and show you how we can prove a program differentially private if I get there other pictures okay I could have taken a real picture right so mainly they the overall picture is disease we provide the program with some annotation and the verification tool try to check if this is a valid program so that satisfy the property we want in particular for us it will be differential privacy an important aspect of modern verification is that we use a lot of tools which help us doing this in a is automated as possible so let's look back at our example that we started with this one so we already saw that these fail to meet the specification it was the example about addition now let's try to fix the program first I just replace 0 with Tech's so now I'm pretty confident that this program computer Edition and so it meets the specification now how can we prove that it meet the specification if you have already seen these and maybe is not anything really new but a standard way to do this is to try to push through assertion the this precondition forward and try to derive at the post condition from it going through the problem another approach would be to pull the post condition backward so trying to see if this is indeed the follow from that ok now this is a kind of game I will choose the direction of going forward because I think is a little more easy to present on paper although automated technique try to go more backward but I will try to use the forward direction and I call it a game because I will introduce some role of the game which tell us how to step from a platform that were posed for mana by looking at each of these so every time I later I will do several step at the same time but every time I pass through one of this line I will apply a rule of the game now I will use only rule that are correct that they are sound and I will avoid doing this year but they are proved correct again with respect to some semantic model as I was saying before I will not do it here but there is a the rule that I when I introduce a new rule I need to do this proof I did it before I will not do it here but this is important is human proof sir is there like another turtle under this like by you or by like another machine you can do either you can usually we do it on paper we do a combination so you can prove it with another machine so yeah they are still now mainly they if we think about trust the trust has been moved to this the soundness of desire Allah so these are the trusted components so let's try to prove corrected in this program now I reformulate a bit what I call the precondition I just introduced to and the postcondition introduced to variable v NW which make it a little easier but they if you do a little of rewriting the precondition is the same and I will start by pushing this condition so it's exactly the same condition I just remove the precondition name through the function and we are just putting it in the function so I assume that this is a simpler rule I can apply I just push it now the question is how shall I move next so what is the first rule that I introduce here we have an assignment that say R is equal to X so what shall I do now now what we can do is using this rule let's say that if we have an assignment like this where to X we assign e then we can derive a starting from a formula where arbre occurrence of X's are replaced by E the formula itself let me show you some example that maybe it clarifies what I mean so for instance I can I can prove from a is equal to Y I can push it forward and say that X is equal to Y because I exaggerate the property of a another example I can from X plus 1 greater than 0 I can say if I do X equal to X plus 1 that X is greater than 0 because again I inherit the property of X plus 1 another kind of example is from 3 equal to 3 applying the fact that X is equal to 3 I can derive notice how I formulated so the formula lets this is the only place where I go a little backwards so the formula is the one on the bottom and they replace in the top so I replace all the occurrences for instance here yeah I replace X with free so should the precondition I can also that is good but this is good as well yeah exactly that's where a bit of creativity is needed so that's where you can choose so you see here I replace again these X with X plus 1 replace this X with ay so going down I have some choice okay so if I apply this ruler here mainly what I get is that doing a semi transformational I have R equal to B I can apply this rule again and I also get that n is equal to W and here is where I used this additional variable now let me change another rule that I can do that I didn't write up explicitly is that I have a bit of game in now I can change this formula if I preserve the validity of the formula so in particular ER I rewrite R equal to V plus W minus n so why I'm allowed to do that because N and W are equal so I'm adding and removing the same element okay all right so why do I need this at an 11 is because now I'm going into a while where I need some form of invariant to be preserved and this is the reason why I do this and we will see in a moment when I when I give the rule for a while actually let me remove I change it a little more because I now say it something like this R plus 1 is equal to V plus W and minus 1 again I'm adding plus 1 on both sides so their qualities preserve and I forget a bit the rest that who make a bit simpler now I try to propagate it through the while and I keep applying the rule way so before and that this get transform in particular notice that now I replace this R plus 1 with R and then I replace the N minus 1 with end now if you notice here I have the same rule that I the same constraint I had before entering the wine I don't know if I emphasize this enough so let's go back here so I add the dis constraint R equal to B plus W minus and so what I what I show with this step is that this can be recreated after going through the wine now these bring us to the wire order which is this one if we can find the formula that and if we can find the formula for which we can do this so we take can take the formula and the condition of the wine and prove the same formula after the body then that I can prove that this formula is also true for the wine it absolves for the while and in particular if we load with the negation of the condition after the wine yes does that show that the condition is ever violated like there's a termination issue and well yeah there is a some termination issue that I'm a completely living under the carpet here Y was negative the problem of Y being negative was not true exactly correctness of formulas yeah yeah yeah exactly so usually there is we distinguish between total correctness and partial correctness whether you also prove termination or you assume termination there are technique to deal with all of these I just try to leave them under the carpet to avoid the discussion but I'm happy to discuss more interesting yeah but there is that issue so in general we call this kind of formula invariant because is an invariant through the loop and it allow us to prove that the formula still owns after the loop with the negation of the condition so in particular if I do this I use this rule and the same formula will apply here sorry but also with the negation of the condition so the formula is this and I applied the negation of the condition which say that mainly n is equal to zero and so this will become just the apply equal V plus W and this is actually what we need for matching the post condition okay so this is a an eye-level introduction to how verification work and now I did it manually but let's try to do it with the tool I just want to give you a feeling of how far we can go that wise all right nice night this design very bad alright so then I want to say also in the meantime that is connecting I wanted also to correct something I said before pink actually already in the first version that contain at the exponential mechanism I say the something wrong before so is available this year and yeah it was there all right so these I don't know if you can see it that is something let me correct this is exactly how we did it before so this is exactly the example we were looking at where requires is the precondition and ensure is the post condition and we also provide an invariant to the program now if I try this is a tool that people at Microsoft Research design for this kind of verification now if I try to execute the original problem the original program we have some error we cannot prove it correct and we know that is not correct because it doesn't compute the addition because of this zero so if now I fix it to X the verification goes through and assay is verified okay so this is to give you a bit of a flavor of how much of this is automated in particular you see what we need to provide is this part and this part now there is a lot of research in to try to remove this part at all so inferring invariant is a very active research topic and I think also this tool does some inference of invariant so ideally we would like really to provide just preconditions and postconditions and all the rest done be done automatically okay all right but finding the right is probably the hardest part yeah finding the invariant is they are this part and people are considering many different technique to do that from constraint solving quite advanced constraint solving to using machine learning technique but I think especially in the constraint solving they have very sophisticated technique to find the invariant even assuming that the precondition and post condition are correct they they find the invariant yes in if you're verifying more complicated programs like how easy is it to specify things like the post condition like if I wanted to like your example the post condition is something that's basically like a first order operation exactly so let's look at one more complex post condition like sorriness like I mean is this the Trivium no I'm saying like this sounds very long time saying okay you know where between like i6y and like this program is differentially private does it start to become kind of challenging yeah so I think let's take assorted Anessa which is something that people have looked a lot into so specifying the post condition is quite easy because you need to specify a price of course there is a a madam a some kind of mathematical way of specifying actually here in like in this tool you can use the same language to specify both the program and post condition and you can specify a predicate that say this list is sorted which doesn't have to do at all with the algorithm and then usually is really just about finding the right invariant let's see so in writing such a predicate you're allowed to use coil you are allowed to use quantifier yeah I will come back to this a bit later when I will talk about solving so let's talk about differential privacy and let me take a simple example this is lightly different from the one I took before this is just compute I try to take the simplest one just computing CDF and then add in Laplace to him this CDF of one value so just how many element of the data set are less than C so now the question is water precondition impose condition here so ideally by the definition we know that we would like something like this for every two databases differing in one individual the precondition and then the result are epsilon this Delta indistinguishable okay so yeah this is what ideally we would like now there are two problem here so the first problem has to do with the precondition with actually both of them is that this quantification of two databases I already mentioned this before so who are these two databases if you look at the verification we did before we were working with the input but here we are talking about two input okay the second one is this one how we formulate the result our epsilon Delta indistinguishable so to solve the first approach there is a classic technique in verification which is consider relational reasoning where we consider instead of one program we consider two programs in particular but now we can talk about the input of one and the input of the other here I took the same program because we are interested in verifying differential privacy which usually talk about one program but in general and for verification is convenient where the two programs although I will use only one program in my presentation now the problem is the post condition so now we can we can express the precondition the problem is the post condition because this is what epsilon-delta in this thing we shall be little min now why this is a problem the problem is as to do with the old verification infrastructure so I said that we want to rely as much as possible on automated and semi-automated tool as you saw in the simple demo I gave that we have behind the tool that salt constraint now do we have some good decision or semi decision procedure for epsilon indistinguishability and is it the way to go so just if you are not too familiar with solver or maybe you are more familiar than me with some kind of solver let me try to give you an idea of how we think about solver in programming languages so I'm sure you all know how to solve a linear set of constraints I think is something that most of you have seen before and usually we can consider them as kind of easy because they are a simple constrain this doesn't mean that is always good having many of them we know that there are there is complexity issue but here I'm not considering complexity issues but I'm trying to motivate the kind of salt that we use so these are in my view quite simple then we have nonlinear set of constraint which start to be more complex and then we have first order formulas with twenty fires and these are all also quite complex depending if you are over the really if you are over the integrator if you are over mix in the Galleria and then finally there is what we use which are a satisfy satisfy ability model OTS SMT solvers which put together different theories using first order logic okay so in this kind of setting we can specify predicates we can have numeric for instance a nonlinear arithmetic and we can abandon arithmetic we can have a bunch of different areas theories put together ok this is a the kind of usual target of our verification why this is the usual kind of target because we can do what John was asking here we can write a predicate that say this list is sorted by using logic formulas and then we can try to verify them so we base our verification a lot of their deeds now why this is good for us so I think the one that is the main point is this one which often is not what people that come from the theory background think about so usually you are for the theory background people are more used to try to reduce a problem to another in a specific way that you can solve and maybe the constraint using a linear arithmetic and you are done for us instead we are trying to have a kind of a very general way to encode the constraint because we want to consider all the possible programs and we want to consider very complex pre and post condition so we don't want to reduce to linear constraint we want to add as much as possible I relied to the solver the effort of trying to verify them so the other reason is a sanity solver are good at handling combination of inductive theories and numeric theories for instance if you want to work with arrays you have a theory of array that you can use directly in the solder and also in theory they could support counting theories like the one that we need the to reason about probabilities the problem actually they can support theory like counting theories but the problem is is unclear how to make them scale in practice all right any question on this then I go back to our goal so because we use this has 70 solver and as I said they could reason about probabilities Baron is not clear how to make them efficient and skill to big imagine probability distribution with a very big support then we want to avoid as much as possible reasoning about probabilities for this reason we my collaborator and I but also people before I look at how to verify this relation and properties and probabilistic relational property and a technique that emergen is of is the following we call it reasoning with approximate probabilistic coupling and I will explain later why is this but is the idea of just use some some logical constraint as a post condition and require also some additional property in the case of differential privacy we want to ask that we pay them most Epsilon so this seems maybe like natural because it's what we did also for pink but in some sense is a non-trivial change because we need to guarantee that this is a good sound way of characterizing differential privacy and I will talk a bit more about this later okay since I to simplify I will just consider one program but I will use a design this is one in two to talk about the two possible run the database one in database two and so on okay so how do we verify our example so our precondition now is that b1 and b2 differing in one individual data just as a shortened I will write it in this way so I will write b1 squiggle 1 b2 okay now I add slides for doing this example completely button then I realize I'm running out of time so let's say that by some standard reasoning like the one I showed you before in the example of odd we can do the non that non probabilistic part okay in particular in this example what will happen is that the adjacency relation we have on the data set will be propagated on the counting we are counting with CDF and so it will be just that propagated this adjacency relation to the county so now the interesting part is what we do with Laplace because now we have a probabilities so what can we say about Laplace and here is where we use this first rule let's say that if I have two inputs which are adjacent and I add Laplace with Absalom parameter epsilon then I can say that they output are equal by paying epsilon you really mean like whatever epsilon like this is not to be taken as the like variance while sits just yet are you agree but usually we use Laplace as a Laplace of one over okay yes that's yes it depends on how you implement the Laplace presentation okay but I agree all right so now remember I said before when we introduce Rolo we need to prove them sound in the semantics now is in the semantics that we use this approximate coupling I mentioned before and I will come back to that but for there will mainly is this rule that we get okay you just say mainly by paying epsilon we cannot distinguish anymore the output now by applying this rule we get that the output cannot be distinguished and so we are done we prove this differentially private because that was our goal so our goal just to make sure going back here it was exactly to prove this equivalence and paying at most Epsilon and we did this okay now I will skip a bit on how this account for multiple queries and I can do composition and I can sum the epsilon and so on because it's a in spirit a similar to what we saw so far with being a rabbit and puds and I will try to focus on something a bit different also because last time I showed that part a Thomas told me oh but this is on tribulus oh I want to show something that is non-trivial it was a some years ago and actually this comment motivated what I will present the next alright so as I say that we can extend with composition and so on but let's look at something that doesn't use composition so let's look at report noisy max for the one that are not familiar so here we want we let's say we have some item and we have some scoring function for each item and we want to report the one that maximizes the disk or okay one way to do this is for instance by computing in Instagram add in Laplace and then take the element that is the maximal score now I learn not to click at least one for each presentation I'm allowed to do that so now if we just do a naive analysis of these algorithms we do any analysis with composition we can show that these k-epsilon differentially private but this is not the interest the interest of this algorithm what we want to do is instead probably prove this algorithm salon differentially private and how can why this is Epsilon differentially private the idea is that mainly we look at two databases we look at a bunch of query we add noise to each one of them and then I assume that they are one sensitive and then we are looking at something like this where the green part I come from database D Prime and the blue part from database D and it is true that if we look at realization maybe we can have that one the max that is Maxum one will not happen in the other but if we look over all and we try to coordinate the noise we can show that mainly we are allowed to pay just for the max result now this is a very high level intuition maybe other people at different intuition but let's try to prove it formally okay now I will not prove exactly report a noisy max but I will prove what is called the report one-sided noisy max so this is lightly difference the main difference is that instead of using Laplace here we use the positive a Laplace okay so one sided Laplace wide yeah or an exponential distribution now why do we do this first because composition doesn't apply at all because one Laplace is even not differentially private so we cannot apply composition second is that this gives an implementation of the exponential mechanism so at the end of the day what we will have proved is that in an implementation of the exponential mechanism then through this report Nassim axis difference now let me try to go quickly through the corner so this function takes K queries takes the daytime than Epsilon I return one index in particular what it does it goes through the goes through the queries it had the two each query these exponential noise or this one-sided Laplace and then try to just find a maximal one and keep that a maximal one and then return the maximum okay we use a bit of variables in particular we use the current result and we check the current result against the best result we saw so far and if this is the case we say that the index all of the query we are looking at is the max and we update the best to the current now there is only one thing that is a bit stranger this equal to one that we use for the first step I was I guess I was wondering if it changes anything or makes it easier or harder if you just sort of consider it two separate things one where you you know add noise to all the scores and then then we go from you know you're sort of doing to two things in one while live yeah yeah you know if undergraduate students or anything to go by makes the program harder to reason about that's true so I think that concretely nothing change you need to use the same kind of reasoning but you is the modularization of how you structured approach the change a bit there is a lot of work that we can do in that direction now what strategy shall we use so here I want we we use something that people use a lot on pen and paper but for us we need to make it explicit and is this definition of point-wise differential privacy or this a reformulation of point-wise differential privacy if you so I assume that the that the output domain is discrete then what we can do is to look at the indistinguishability on one particular element of the output domain and then check that the deltas a map in our case we don't use that in this setting I don't use Delta we just use epsilon but we can deal also with Delta but the key point is that we can focus on one s at the time now why this is interesting because if we turn this into a rule and we remember that what the strategy we use to prove is symbolic so we don't go brute force what we can do is that we can generalize over s and so s becomes just one possible output is that what you do on paper you don't really take as one is two free but you take an arbitrary yes is the way natural proof on pen and paper go but now we can internalize these also in our proof so we can say that if for every s we are able to show that we pay something and we have this post condition that say if the first output is equal to s then also the second output is equal to s then we can prove the overall program differentially private now there is a this post condition is a detail I'm happy to explain why it has exactly the shape if you want later but the key point here also bond focus on the Delta the key point here for us is a really the generalization so that we can say we look at 1s in general which in yes which in our setting allow us really to just focus on a particular s so we have this variable s and now that we can use and that this will be were specific [Music] okay I'm not sure where I got what can you explain the first post condition so it says if under both inputs you would get output s and you paid epsilon and Delta s which is specific to s and if I this exposes like something I was confused about what it means the statement paid epsilon Delta like what that actually means in particular what does that mean in conjunction with like a get I will come back later that's exactly the coupling near there is a why we call this coupling so we pay to get different couplings that's the idea but I will come back if you let me go through the verification I will come back to this ok I see it so let me go back yeah speaking of semantics so here we are we are saying sorry this should have been sent yeah so if I'm a adjacent and I Ribeira this means that I cannot distinguish anymore at the two output because they are equal but two arrived there I yet to pay epsilon and Delta that's why I try to put the path sent it means that so in your privacy budget in the usual way you have to consume epsilon and Delta do we're asking like what are the semantics of the statement hey that's one dollar so is it a statement of the form you know I've got these two I'm tracking two distributions and it's a it's like a coupling statement that says there exists a coupling of these two probability distributions under which they are equal with probability one and let me go to the definition so that we have the exact semantics so for them and then I will go back if needed so what we call approximate coupling is still so coupling usually is used to put two distribution on the same space and they are we do to the modification one is we have an explicit relation between that make the coupling explicit and the other is we do this approximation but let me go today which is a usually divergence but let me go [Music] this is the definition and unfortunately is a bit longer but let me try to read you so now we have two distribution mu1 mu2 and we say that we have an approximate coupling where the approximate is for epsilon delta and for the formula as the is a relation so mu1 mu2 are two distribution and we have a coupling in our sense if we can see two probability distribution on the joint space and d are we are a bit different from the usual coupling because usually you want one probability distribution on the joint space here we take two of them and the reason is because we also end on some distribution and so this to did these two are sometimes needed so and we want that the left margin of mu L is a mu one at the right margin of mu R is mu 2 this is similar to standard coupling and now one of the key point is that the support of this coupling is in this relation s okay and then we require mainly that they that these two are epsilon indistinguishable this to distribution epsilon Delta indistinguishable this is the precise semantics now the way I was using pain is to say that to get desired I see this in this way to get this relation s you are paying in the sense that you are making them indistinguishable with respect to this our nation s epsilon and Delta let me give you an example that is different so we already saw an example let me go back so this is one I was saying that if we start with two input that are adjacent and we are the Laplace of 1 over epsilon then we can show the equivalence relation by paying epsilon so it means you look at the same value and they are epsilon indistinguishable so that's saying that you L and you are like output 1 equals of the two is the description of s yes exactly so mu1 mu2 mainly are in the diagonal so mu and mu are are in the diagonal the diagonal is where they are equal but this is not the only way let me show you another example which is a similar one second let me just mention this so now you are not looking at the diagonal anymore but you look at output 1 plus K and that equal to output 2 and what you pay and is the same program here I was doing this for the exponential but we have a similar hole for Laplace now you pay K times epsilon this is what you call a sliding property for instance in your a smooth sensitivity paper I'm still parsing sorry so input 1 minus input 2 somewhere between minus K and K prime minus K and then yeah this is because we are looking at the sari and the one side that the so is a bit more Isaac I think ok but if you think it this is still like it I mean this is almost like that statement about the diagonal right this is like you know you are using a different relation but it's still one where like the two random variables are just like a one on one correspondence this is exactly the sliding property of laplace generalize to only the points more interesting one is that one you showed us that you were gonna use for report noisy max that I think one for this before stop no earlier you had the one without put 1 equals s enough but you so for instance this is another so if we take we use it a lot in the proof so if you take again you are the Laplace one side that Laplace now what you can do is that to just use a coupling which is a zero indistinguishable by requiring that they output preserve the distance over the input I mean same distribution same yeah which is relaxing and adding the noise doesn't make things worse yeah exactly these rules do you know so so I will probably not show completely the example because I'm running out of time but I think is better to just discuss so these are all can be proved inside the system but not inside the logic and let me be more specific usually this kind of logic are built in some system that allow us to reason about their soundness as well and then you can use these rules as macro to do the proof like I was doing so they are proof correct but inside the bigger system so just that I will not go through this but just the key point of report on Isaac Mac so let me go here is that is that we need to proceed by cases so what we do is and mainly to think about in this way now we are reasoning about s so we can think about the loops before s we can think about the cases and then the loop after s since the output must be as we know that we don't want to pay for the loop before and for the loop after so for the we use this coupling that we just saw the one that pays zero in all the iteration before as and all the iteration after s and then we use another coupling for the case where we are in s which is this one and these make sure that so we'll use these with k equal that one but mainly to make sure that we can prove this post condition result so that if we take the branch if we go through the leaf for the first run we can force also the second run to go into Dave and then we keep a pain zero until the end this is a at eye level the proof and the detail in our in my slides let me conclude by saying we have other coupling dis approximate coupling for other divergences and remy divergence and that CDP and so on is not a totally trivial to do this for from the formal perspective and there are also 4f divergence more in general this method can be automated and just in to work on these with house and house will be around so during the first workshops I will give more detail about their way of automating this approach and they I think is very interesting another work I wanted to mention is that a very similar kind of verification can also be done for reasoning about probability of failure and this allow us to reason about accuracy for instance for report noisy max we can with a similar proof we can prove its accuracy as you would do pen what this logic internalizes the Union bound now I take a few more minutes with the Sun pointer that I think I would like I think they would be interesting for some of the possible interaction in this semester so I think they first Adam already mention it and I think is an interesting question try to find a common way to try to build some software infrastructure that support a different effort for differential privacy but I will go first on this because Adam I already mentioned this several time the other one is I think it would be really great to add basic mechanism like let's say Laplace or exponential mechanisms that are fully verified and and in the sense of having some basic component may be written in a low-level language like C but which is something we are sure that is correct maybe correct also against the some of the low-level attacker and I personally plan to work on this but I would like to hear more from other people about this and whether this is something interesting another topic that I think we should be discussed at this semester is a finding hunter example to differential privacy implementation so since differential privacy has been deployed by many different people that sometime they don't have the expertise that there is in this room and many of them are worried that their implementation are not private so I think having some way of thinking about finding violation is a good way to help and the deployment of differential privacy sorry I did the ballot list and another thing that I already discuss with several of you is we use a lot disk coupling base argument and we also use a lot of divergences in our verification and the recently several work have shown lower bounds based on a some kind of argument based on coupling and the idea is usually in the proof that I saw taking some divergence maybe depending on the problem that you formulate the problem in terms of some divergence and try to show that by some transformation you have a bound on the divergence you need for actual privacy last one I think formal methods doesn't need to be fought only about something for privacy in this semester but there are also other direction that it can be useful for for instance John and I are working on some program analysis technique for more adaptive data analysis and houses work a lot on fairness and formal methods so I think it would be very interesting for me and therefore other formal methods people learn more about other possible oh right this is my question was about how and end can we like just just what are the ends of verifying a basic mechanism at hand do you mean like on particular hardware so in some model of what C is yeah so that's a good question so different so I think what I was referring to with some of the world that are going on in more the cryptographic world where they are trying to verify some crypto primitive end to end and usually the way at least some of the work I have in mind the way they do this is they prove some property of the 11 language which is then translated to C and then we use some infrastructure that is already built to reason about C programs and these can go to assembly so we can go to some architecture or to some intermediate architecture like an al BM or things of this kind so I think the lowest we push the argument and the better it is now the the effort I think which is me that is really reason a bit of two things one is differential privacy on finite did something but is not the final word people at Harbor the victor vulture was working on this here's some result I think there is the need of thinking more about differential privacy in the finite space finite memory and and then like maybe there is a more formal method effort in trying to understand how to push down there is a lot of software a lot of verification effort that has already been done like C to give you an example C has already been formalized different memory model different architecture so there is a lot of working you think Marco for all three yeah [Music] [Applause] 
tPqnK2iNGZo,28,"Support the show, consider donating: 
BTC: 1CD83r9EzFinDNWwmRW4ssgCbhsM5bxXwg (https://epicenter.tv/tipbtc)
BCC: 1M4dvWxjL5N9WniNtatKtxW7RcGV73TQTd (http://epicenter.tv/tipbch)
ETH: 0x8cdb49ca5103Ce06717C4daBBFD4857183f50935 (https://epicenter.tv/tipeth)

With the rise of smart contract technology, we've become acutely aware of the need for smart contract code to accurately reflect the intentions of its author; and for the code to have certain (safe) behaviors in all circumstances. Creating the languages and software tools to enable ordinary developers to write safe contracts has become an intense research endeavor in the cryptocurrency space.

Scilla is a Turing incomplete intermediate level language; inspired from the paradigms of functional programming and formal verification; that makes it easy for smart contract developers to automatically prove statements about smart contract behavior. For example, Scilla could allow a future multi-signature smart contract author to mathematically prove that funds in that contract would always be retrievable by certain addresses (and never get stuck like the Parity incident). The ability to mathematically prove such safety properties of the smart contract has the potential to be an enabling invention prior to widescale use of this technology.

In this episode, we are joined by Dr. Amrit Kumar and Dr. Ilya Sergey to discuss Scilla, the smart contract language of the upcoming Zilliqa blockchain. In a previous episode, we've already covered the vision and technical approach of Zilliqa to solve the transaction scalability problem of permissionless blockchains. This episode focuses specifically on their smart contract language development efforts.

Topics discussed in this episode:
- Updated on Zilliqa's progress since our last episode
- The technology of mechanised proofs
- Dr. Ilya Serger's effort to mechanically prove safety properties of a blockchain consensus network
- Aims of the Scilla language
- Future capabilities enabled by the Scilla language
- Developer experience and perspective using formal verification tools
- How Scilla compares to Michelson, Tezos' approach to smart contract languages with a similar end goal
- Current state of development of Scilla, and next milestones

Links mentioned in this episode: 
- Our previous episode on the Zilliqa platform: https://www.youtube.com/watch?v=Dppk6n9LHBk
- Ilya Sergey's paper on mechanising blockchain consensus: http://ilyasergey.net/papers/toychain-accepted.pdf
- Scilla whitepaper: https://arxiv.org/abs/1801.00687
- Michelson, Tezos platform's smart contract language: https://www.michelson-lang.com/
- Zilliqa blog for updates on platform development: https://blog.zilliqa.com/
- Coq, a formal proof management system: https://coq.inria.fr/

Sponsors: 
- Gnosis X: Build a revolutionary prediction market dApp on Gnosis & win up to $100,000 in GNO tokens - http://epicenter.tv/gnosisx

This episode is also available on :
- Epicenter.tv: https://epicenter.tv/238
- YouTube: http://youtu.be/tPqnK2iNGZo
- Souncloud: http://soundcloud.com/epicenterbitcoin/eb-238

Watch or listen, Epicenter is available wherever you get your podcasts.

Epicenter is hosted by Brian Fabian Crain, Sébastien Couture & Meher Roy.",2018-06-05T20:34:11Z,#238 Dr. Amrit Kumar & Dr. Ilya Sergey: Scilla – A Formal Verification Oriented Contract Language,https://i.ytimg.com/vi/tPqnK2iNGZo/hqdefault.jpg,Epicenter Podcast,PT1H11M30S,false,1438,25,0,0,2,"[Music] this episode of epicenter is brought to by kenosis an open platform for businesses to create their own prediction market applications on top of a theorem network they recently launched venosus X a challenge inviting developers to build apps on top of the kenosis platform to learn more go to epicenter of TV slash enosis x hi welcome to epicenter he showed which talks about the technologies projects and people driving the decentralized technology and cryptocurrency revolution my name is maja Roy and my name is sonia agarwal and today we have with us two guests from the silica project with us they are Amit Kumar who is the co-founder and head of research at silica and Illyria Sergey a assistant professor at the University College London so let's go ahead and start off with some background I'm right if you tell us a little bit about how you got involved with the blocks in space before everything you know hello to everyone and no it's great to be back here so yeah I'm um working with silica right now and before that I was a I did a PhD in temple security & privacy at in Rio and then I came up with to Singapore at National University of Singapore where I did a postdoc for around a year and when I started to explore blockchains especially from the privacy perspective so that was interested in Manero which probably friendly and privacy enhancing cryptocurrencies and we try to analyze how how much privacy it gives in practice so this is how I got introduced into blockchain and then one day uh my advisor at anywhere so asked me if I was interested in building a scalable collection platform called silica and this is this is how I jumped into silica great and Ilya how about you how did you get involved with the space all right first of all great pleasure to be here it's my first time and I'm about to enjoy it here alright so my background is in formal methods and programming language series so I did my PhD in Cattolica University of Louisville that's in Belgium and then I was a postdoc in in their software Institute and then I took this position as an assistant professor or lecturer is the coeds at University College London so since I doing most of my research career I was looking at the properties of programs that as we all know can be buggy as hell and can go wrong so I was looking for ways to prove programs behaving correctly being secured and obeying certain properties applications of blocking technology specifically around 2015 when etherium started to emerge seems it seems like a super exciting area were lots of research and DL theory and formal methods can be applied so since then I start to look at this thinking about how can we possibly write safe and secure applications that run on top of this distributed architecture what are the properties that we are interested in and find out what other right programming language mechanism to express those sorts of computations and those sorts of properties and this is how we ended up collaborating with silica that I'm currently advising and we jointly are designing and building Scylla as a programming language on top of the silicon platform awesome cool so I'm ready so you know you guys were on epicenter in the past about I think November of last year so could you tell us a little bit and on that last type of time you told us a lot about the basics of the protocol and I think this is like when you guys are like first getting started could you guys or when you first went public maybe could you guys tell us a little bit more about what are some of the progress and update that has come to silica since the last time you guys are on yeah sure so at that point of time we didn't have Scylla of course you were still working on designing and having a having this idea of what our language to be so we have progressed in two different directions one is of course on the Scylla site where we have you know we we released our white paper that explains the design rationale and know for Silla in January this year and in parallel we were working on implementing the silica core protocol so we are at a stage where I mean our first estimate has been released that desmos allows you that kind of implements a Bitcoin like infrastructure where you can do payments we have a block Explorer and a web wallet that allows people to interact with the backend blockchain infrastructure you cannot join the network for now but we are going to open it up very soon so you can become a miner they still have a lot of things to do for instance we have to put gas mechanics and incentive layer that's not there yet but we are actively working on it but in parallel we are working on on Silla together with where we are working on designing and implementing the interpreter and the language spec you know you'll see what kind of primitives the language will provide and support and recently we did a demo a couple of days ago where we demonstrated the first contract that that that that can run on silica that was a crowdfunding contract so yeah things are things are working pretty pretty well and fast outside but there are lots lots lots of things to do very cool yeah when I actually our first read you guys's paper areas like super excited because I first like you know a lot of people use the work like proof of work and Nakamoto consensus interchangeably and I always used to bring up to people like technically you could do proof of work PFT I've just never seen anyone do it and it was really cool to see that you guys were actually trying to do proof of work PFT so in your current test net do you guys have implemented the sharding mechanism and stuff is that like live yes so we have sharding Rich's life so in the backend when you send transactions those transactions get shortened the network is shortened so it's not like a big network as in as in Bitcoin or a theme it's it started so we have shorts built up and those shorts process are only a subset of transactions so yes charting is is life that's super cool I mean I personally look forward to joining the test that when you when you go live with it so so this episode is not focused on the sharding mechanism of of silica but rather the smart contract language which is Scylla and like Ilya you've you've worked for a long time on programming language theory and software verification and like scrolling through your past papers I see lots of work with this idea of mechanized proofs and mechanized proofs will of course play a role later on in the conversation when we discuss Scylla but before we start on Scylla could you give us an idea of what is a mechanic's mechanized proof all right thanks for the question here so before I actually start talking about mechanized force let me erase the question what is a proof and this is rather philosophical so turns out that when we talk about proving something we usually have a certain statement in mind like a sum of two even numbers is even for example that already implies that we have a theory of natural numbers in our head and we know what an even number means and we know how the sum is defined so all these together are the fact are the other components about which we can make statements and then there is a notion of logic in which we make the statements well most of the humans they have certain sense of logic they know what is true and what is not true but when we actually try to establish that certain statement is true ie it is correct we start invoking the rules of the logic for example we know if a and B are correct then either like both a and B are correct separately so this is like a simple rule that we all have in our heads or we know that if we if a implies B or and we know a then B holds true so all this rules is something that mathematicians started to formalize and spell out in a mathematical system around the end of the 19th century and this is how the basics of logics have been laid out with many prominent scientists ciphers guru such as George and and many others and then there came the notion of a constructive proof so what are the statements that we can possibly prove well and this largely depends on what the rules we what the what rules we use and what are the what are the axioms so most of us are familiar with axioms of planar geometry such that the two parallel lines never intersect so this is something we take from granted but there are other things that we don't take from granted we derive from this initial assumptions and we apply those rules ok so now what is the true statement well the true statement actually largely depends on your initial assumptions and on the rules that you use to prove this statement in different logics they come with different set of rules and most of us they have certain version of constructive logic in our head when we know how to combine facts together so we can derive more composite statements and mathematicians make it precise so if we can prove certain statement in a logic of choice then this statement is considered to be a true and if we can approve this data it's considered to be a false so the notion of falsehood is actually the the whole platter of statements that cannot be proved so now approaching the the idea of mechanized proofs obviously most of us heard of proofs that are wrong a famous example is form as grade theorem that has existed for 300 years and only has been in it's only believed to be proven finally by Andrew Wiles in 1993 so why all the previous attempts were were unsuccessful well because this sequence of rules that mathematicians used to apply or proven the ultimate statement about about the numbers infamous theorem there were some rules which were applied wrongly so some rules which were applied out of place and for example I'll give you steam so let me tell you that the sum okay so the example I gave before the sum of two even numbers is even this is something that we sort of believe because we know how numbers function but if I tell you that the sum of any two numbers is even well you know that suppose and you know it from experience but if I ask to prove you you will be probably invoking the definition of natural numbers the definition of addition the notion of evenness the notion of oddity and at some point when you try to combine the facts you'll see that you have no rule to deliver this final statement so this means that the proof is inconsistent the proof is wrong and obviously when we write such proof of proofs on papers and the proofs might be about any arbitrary mathematical object so I use natural numbers as examples but those might be programs or those might be distributed protocols so it is often the case that the proofs contained are wrongly applied rules and this is where the idea of mechanized proof comes to their help so if we can write a proof as a sequence or rather a tree of applications of rules building our rules are the notion of outer notion of to ground up from the axioms by applying more and more and more proofs and maybe reusing the libraries of existing proofs in the way that programmers use the software libraries then we can delegate the task of checking the validity of such proof to a computer and this is what the software known as proof assistance or mechanized theorem provers are doing they usually do not prove statements automatically that can be done but usually for very simple statements or statements within certain theories that are known to have decidable facts but I'd like the world the boldest comp is very complex and most of the theorems they actually require a human in the loop to construct this proofs and this is why we still have mathematicians and there are not a lot of job because humans are required to construct those proofs but quite frankly humans are not required to check the proofs this is something that the computer can do and if we give the computer the formal system such as the system of natural numbers which is where our objects leave we give the statement about certain objects from the system and we give the proof of this statement then the software can put all these three things together and check that every single who in this derivation of the proof has been applied correctly and then if it says yes every single rules applied correctly and your axioms are good and the old consistent then you have the notion of mechanically checking proof so that is the proof of the statement that you had here the question is what if your statement is not good enough but that's a separate a separate question but the mechanized proofs is this idea that the computer can actually check certain derivation and that that can be done automatically so uh that's that's really fascinating right so essentially the way I see it is like any programmer at some level they have data and they are functions and then they use functions to build other functions and then there is like a hierarchy of small functionality that is used to compose something big and mathematics is similar because like some complex statement like the foremast theorem could depend on simpler proofs of other other simpler things which depend on proofs of other simpler things which depends then on axioms that we take for the logic that logical system that we are considering and so whenever in a sense the way I understand a mechanic's mechanized proof is I could express how I am proving Fermat's Last Theorem as as this cascading set of smaller proof statements give it to a computer and then the computer will verify the whole derivation of the proof and will give high assurance that that proof is correct this is very much correct vision thank you in fact this analogy that you have just distilled with the libraries of functions that the program has used and the textbooks of theorems that the mathematician refer to when they actually prove new theorems this actually has a name and this is called Kari Javert correspondence or carry forward is Omar's business so Hospital Kari and Howard are the two very famous mathematicians who lived in the 20th century and they noticed this very elegant correspondence between the models of a certain classes of programming languages such as functional programming language so these are models that are now in power languages such as oak mo haskell and some others so and the idea of mathematical proof in the constructive logic so turns out that the statements the mathematicians write they are very much similar to the types that people give to the programs written and functional language so for example if you have a function that takes an integer and returns a string you can think of it as of an application that takes something that is an integer and they gives you something that is a string so even the symbol is similar so that's an error and that's why this analogy is so powerful and this is why it actually has been very much used in implementation of the PUFA systems that now sort of combine proving and programming so writing a theorem is the very same as writing a type a notation to your function and proving the theorem is very much similar to implement in this function it's like your programming but your types come before your implementations so in the programming that's usually the other way around so you first write the implementation and you think of the types and in fact it shouldn't come as a big surprise that programmers especially those who are programming languages such as Haskell and oh camel which have very very powerful type system they rigid do some sort of mechanized theorem proving even though they might not realize it so when they make sure that the type program type checks against the types they give so the compiler tries to construct the proof from the program and the types because like the theories there is very simple so you don't need to have a human in the loop to make this additional third object which is the proof so compiler does that and then the compiler itself checks it so the process of type checking in languages like Haskell or camel is it incorporates to some extent a very simple version of theorem proven and the statement of theorem means that the program satisfies this type and the fact that program satisfies this type means that the program has certain properties and it doesn't go wrong so that's the famous statement so the well typed programs don't go wrong if you cannot send the type to the program it doesn't work so here we already have industrial skills here improvement well type systems they only give you very simple properties such as basic shape of your computations but in order to prove some more complex properties such as safety and temporal something that we'll probably discuss later for that you need to have a human but yes so to elaborate on what we've said theorems and programs they are actually the two sides of the same coin and sorry the theorem sometimes there are two sides of the same coin and fools two theorems is the same as programs two types so that's carry forwards of course I'd like to actually take an example in order to sort of drive home this point and I mean I like to take an example that sort of derives maybe not derive but is related to one of the papers you published which is mechanizing blockchain consensus essentially in this paper as I understand it you've you've implemented a distributed consensus algorithm and the blockchain data structure and you you prove that certain properties of this blockchain network will always be met so I'll try to take a simpler example right so when for example I am a blockchain node what I'm essentially doing is I'm hearing of transactions coming in to the blockchain node and these transactions are collected in a memo and then periodically then the software that is running on my node will compile some of these transactions into a block right and it will try to add that block to the blockchain so there is some some some piece of software that is taking raw transactions and then compiling them into a block and then this block has to be valid right so we might imagine that we want a simple rule the simple rule is when we are when we are doing this process of taking transactions and compiling them into a block we don't want an older transaction which has already been confirmed on the network to be included in this in this block the the transactions ought to be unique that is a property we desire out of a node software and so our node has a previous list of transactions and it saw new transactions it was now compiled but we as these users of the software want to prove this node running software will never create a block which repeats an older transaction and the way I understand it is like you can prove that a particular implementation that you are using will never make such a mistake using mechanized proves is that right yes this is this is very correct so let me just because this development in the light of the previous of the previous analogy so you have so what we did we actually define a new set of objects objects that represent the implementation of a very very simplistic block chain consensus protocol and the objects are essentially the execution instructions that each node can invoke locally so this is this is what we can write statements about so the statement that you present we actually didn't prove that because that was sort of embedded by design it as I said the model was simplified in multiple possible ways and because we were focusing on slightly more global properties but the statement would be here are the instructions that the node can execute assuming that it has freedom to execute them in any order and so the whole thing just doesn't crash would it be the invariant that at any point of time the node doesn't propose a stale transaction where the stable transaction is such that has been already proposed so this is what's called a safety property or an invariant and this is something that we can proof and the proof will will involve case analysis and introspection on the way the semantics for this instructions is defined so your again your objects are these instructions and semantics your statement is this invariant and the proof is something that you can write out of out of the semantics but you see here's here's an interesting point so what we had to do is to build the whole new small theory of blocking protocol implementations that's not granted that's not something that we can just take from an existent implementation so what we need to do is to embed it into the general mathematical framework that allows to build this proofs and check them mechanically but yes this is possible and in fact we focused on somewhat more global properties so what we proved is an eventual consistency in the form of consensus meaning that if you are a local node and you interact with other nodes then if at some point there is no communication between you and other nodes then you know that all of you share the same knowledge of the ledger in other words you have reached the consensus how did we prove it well we took the rules of the system and we formulated the property that implies implies this agreement notion so that's that's where so are you guys planning on like formally verifying than your actual blockchain implementation so like I know that the smart contracting system that you guys are creating is designed to be formally verified but so similar to how kaizo says writing their entire code base in okay mo so that they can formally verify the code base itself is this something you guys are looking into doing in the future or even right now okay so let me ask some part of this question to verify the entire infrastructure is a very ambitious project so they're just like to give you an estimate projects of this scale actually a smaller scale we're fine something like mechanically defining and verifying and a small OS kernel or small compiler for a subset of C this is something that the humanity has conducted and that took literally tens of many years because you really I mean nothing is taken from granted you need to build the whole system ground up in this framework define the semantics of every single primitive every single bit vector operation and whatnot and then prove properties of that and then compositionally build do the proofs of the entire system so we are we are not sure if we are planning to undertake such an effort right now and furthermore so the fact that something has written a camel doesn't make it immediately verifiable it makes it easier to translate into the language of theorem provers so that's a big help but it's not like a camel lends you some foundational verification results so what we had what we will probably try to start from is extending this work that mahir mentioned that we that we have done on mechanized blocking consensus and formulate the abstract model of Zuleika protocol in these terms so we at least could be sure that foundationally the protocol has no flaws in it that doesn't mean the implementation has no bugs this the whole different level of complexity and typically it's done in several steps so at the top level you have your abstract specification abstract model when you prove some general properties thereof and then you show that your implementation or at least some parts of it very fine parts of the abstract specification and by means of proving this refinement you show that actually the same properties they are preserved but this this step this is the complex part so proving the refinement is complex and we might be able to do it in the long run for some parts of the protocol but not for not for of it and that's a big thing so as I said nothing is taken for granted all the crypto primitives all the system OS system interaction primitives this is like if you want to have ultimate guarantees this is also something that needs to be embedded into the proof assistant and established correct and approving cryptographic implementations correct that's a huge endeavor by itself and several labs at in there MIT and India are currently taking on just refine these libraries that implement sha-256 and and similar ones so it would be good they can the ideal world if we could compose all these verifications results together but as always there is there is something there is some friction on the boundary we don't all speak the same interfaces and it's a it's a big it's a big goal so to answer the question I think what is feasible in the medium term around is to purify the properties of model of silica and this is what we will be probably aiming for but I'm Reed might be able to elaborate on a future agenda I just add one point to it which answer I mean last time I talked about silica so you you you you have a few things in an abstract manner that you're using right when it's proof-of-work so using pbft and then you're using something called collective signing which is essentially if you have n people signing a message you could reduce that to just one signature so the signature size could be reduced from you know linear to constant size so you could even pick those pieces together I mean could you could either proof properties about those pieces individually and then you could try to combine at a abstract level at a very abstract level but you know as as you just said you know proving properties about the implementation is another is another thing so yeah we will rather go with you know with baby steps and and see how far you can go [Music] this episode of epicenter is brought to you by kenosis enosis is an open platform for businesses to create their own prediction markets on the theory network prediction markets are powerful tools for aggregating information about the expected outcome of future events so this can be used for things like information gathering incentivizing behaviors making governance decisions or even creating insurance products so in order to turn kenosis into the most powerful forecasting tool in the world they recently launched gnosis X it's a challenge that invites developers to build applications on top of the platform and the best applications per category will be rewarded up to $100,000 in GNO tokens so throughout the year kenosis will announce different categories for the challenge and the current challenge has categories for science in R&D token diligence and blockchain of project integration gnosis also provides the SDK which allows you to easily get started with everything you need to get coding and they also provide dedicated support channels throughout the challenge for teams and solar builders are you up for the challenge guess serve now to learn more and to sign up go to epicenter TV slash canosa's x we like to thank gnosis for their support of epicenter the connection to the smart contracts here is is different like so when you have a an implementation of a blockchain node that is extremely complex and may be like proving properties of the implementation is is like tens of many years of work but on the other side for smart contracts the program's might be simple enough to be Ameen amenable so that amenable to proofs and the proofs might be useless so for example the instance I was thinking of was there are many people that have these days crypto wealth right lots of crypto wealth and presumably they might want to hand this crypto voice down to the next generation in the form of a will right and the will can take the form of a smart contract so there's a smart contract I put my silica tokens or ether tokens in it and then when I die somebody puts in a message and then that smart contract so when I'm alive smart contract allows my address to withdraw the tokens from that from that smart contract but when somebody puts a message that I'm dead I'm no more then my then a different address which is controlled by let's say let's say my my child is then able to extract the ether or silica out of the smart contract so as a small program like that a smart contract is can be really useful and the the place where mechanized proofs perhaps enters this space is if I'm going to commit lots of tokens to this smart contract well I want to know that all of this money will never get stuck in the smart contract that either me or my children will always be able to extract it right and correct that you're focusing on mechanized proofs in the smart contract language due to like these sorts of applications in mind yes so let me answer yes this is a fantastic summary of what we are trying to do so the properties of smart contracts which we are mostly interested in are this large-scale safety and aliveness property so what you've given as an example is a perfect case of Linus property that informally means that something good eventually happens so something good in these contexts means that a person successors children's or grandchildren you'll be able to cash the check but that might not happen right now that might not take place now and certain conditions need to happen for this for this event to occur and you name this condition like somebody needs to send a send message that the person has passed away and then the successor should contact with another scientist action what not so that is a great example of specifying this temporal property so what you've just said in plain English can be written in terms of let's say a lifetime execution property of the smart contract so if we have smart contract and the way to make statements about about the way make the statements about how it behaves over its lifetime so what changes does it make over certain conditions where does it transfer money under which condition that happens then we should be able to write this post and this is how this is what we try to build into the design of Silla so that's the language whose semantics give you this notion of lifetime executions of the contract so you make it can make statements about them and you can phrase the statements in the context of of a proof assistant that already comes with all these foundations of Silla contract execution built in so actually knows the language that you're speaking and then you can use the machinery to write the proofs and the software will check that this process agree so in fact the property like you've mentioned a very similar one we have proven or a model of smart contract and coke which is which is the software or for chicken poops I'll tell more about it later that's the purpose of a kickstarter contract which says that under certain conditions if you have bagged the campaign if the campaign fails and the goal has not reached and the deadline has passed and you happen to be a backer in the past then in the future you'll be able to withdraw your money and do it exactly one so this is the kind of glamorous profits that you want to establish and we and our language design helps you to do so cool so could you tell us a little bit then about more about what are the broader design goals of Scylla and like so you know one was easy transpose ability to caulk another one I've noticed was a very big focus on preventing reentrant see bugs what are some of the other major design considerations that went into this okay so let me make a first step and then Amrita can can add more okay so from a general perspective I think it's very important to have a language that allows for formal certificates to be published and made available to people so it's often the case in systems like a theorem that the contract is compiled to bytecode and it's advertised as something that delivers certain surface but essentially the clients interact with it on their own risks so we want to reduce this risk by given a possibility to prove to put formal statements and prove specifications for both contracts can and cannot do along with their proofs or some digest of the proofs to do the blockchain so this is the concept that has been studied but not in the context of blockchain it was very popular in late 90s was called proof carrying code when a certain code comes with a certain specification and the proof of the specification so now we want to try to do it in practice and the big challenge here is actually to come up with useful specifications so this is something that cannot be done automatically so the contract might be float but so might be the specification so it doesn't expose the flow so the big challenge who actually provide the right illogic abstractions to make statements about the contracts but so far we are building the language even makes possible to specify properties like never suggested so that is my part of the vision and number it might be able to complement it lists with additional details yeah so when we started out you know we felt that we are building up a new block chain platform and he wanted to have a platform with spark or not features into it but then we feel that the way smart contracts exist today we have certain issues and you need a way to kind of improve upon them and one way was to you know structure the language in a way that it you know you can eliminate certain certain issues that appear at the language level and also to meet the language you know kind of proof friendly to some accept so it becomes easier for you to to write let's say Co proof so it becomes easier for you to reason about you know the correctness properties or safety or lightness properties about the contract so and that's why you know people you kind of filled out you know these days I need to develop a new language and this is how we started started Silla I see and so you mentioned it's like one of the goals is to provide a platform for user for developers to submit formal proofs along with their code so a couple questions here right so one so does that mean that the language the code that goes on chain is a seller code rather than so like you know you you got in the paper it's it presents Scylla as this intermediate language so is the idea that it's this intermediate language that will be pushed to the blockchain or is it the lower level language okay so let me take that we were discussing this particular design choice quite a lot and right now the conclusion is that we want to put intermediate level language on the blockchain because we value attract ability or the ability and possibility for formal verification more than the benefits whatever benefits the low level representation brings so I think there is there is a certain bias in the community seen since the theorem was very very extremely successful as an implementation of a smart contract platform that's a very low level representation should go there because that's probably efficient to store that's efficient to execute but well EDM is not what actually executed per se in most of the theory on clients so it's Judith down to some low level architecture so it could run faster so if we do that we suppose the low-level UVM so why can't we just take a slightly higher level language and compile it as we process the blog that comes down so we are not particularly constrained with efficiency here because there's going to be compiled down to something low level and practablity is the main concern that's why we are going to put the intermediate level language is Silla if I may just add two to this you know if you look at certain auditing you know tools or services that exists right now if you look at the smart contract that's going on the blocks it's in byte code and then if you want to audit it becomes very difficult you either need to be compile it or DSM bullet or you know figure out a way to to manage it in human terms but if you have if you have an intermediate representation that actually speaks you know the logic that you the contract actually implements it becomes much easier to understand what the contract is actually doing rather than you know putting a byte code on though on the blockchain and then separately publishing the source code somewhere else so I think it's better I mean of course there are certain trade-offs that you will have to clear all with but to to because we are targeting security I we feel that it's a better idea to actually go with the Silla kind of representation directly on the blockchain yeah I mean this is always one of like the very like weird things I found about our theory I'm where you know on the blockchain all there is is that EVM bytecode and then whenever people want to look at the actual solidity code basically the default is just to go to ether scan and look at the solidity byte code there and you know that's a very like centralized thing that maybe we don't want to like we want something that's more accessible in a decentralized way yeah so actually like this this sort of brings an imagination in into my head and I'm gonna lay out that imagination to sort of sketch out the vision you what I think the vision you're building towards is so today if I am a smart contract user right and let's say I'm going to use a multi signature contract right and I'm going to put money into this multi signature contract well before putting money into the multi signature contract a power user like me wants to know if that contract is security audited right and I then I'll go to this probably this audit from least Authority or trail of bits or something like that and I'll read the audit report and then if the auditors say good things have heard the contract then I'll end up committing money to it but somehow somehow in this process I am trusting the judgment of the auditor right Leslie program and the programmer wrote the contract and then the auditor audited it and I am trusting the combination of the judgments of these two people in deciding that this contract is is safe so what you seem to be building towards is like 10 years later it's when I when I see a Multi signature contract I will go to the contract and then link to the contract will be a set of mathematical proofs that my browser can verify mechanically and these mathematical proofs will ensure that large chunks of the functionality of that multi signature contract are correct from a mathematical proof perspective I do not need to rely on the judgment of a human security auditor in order to be sure that this multi-sig contract is is okay and sort of Scylla is sort of your way of building a language that allows this future to come about that is correct so instead of trusting the human auditor you will be trusting the cold-hearted soulless machine that takes the specification of the contract and the proof and checks them and I hope it didn't come out bad it's not like we are trying to take out people's job rather build a new generation of Oh new opportunities for job because someone will actually have to write those probes as I said like ultimate like full automation is hardly possible and that requires trained qualified mathematicians and formal method experts to conduct some of those proofs but yeah that's that's essentially is the vision so the Devils in the details when we say correct we usually don't specify correct on which meaning so the most critical part of this thing is actually to come up with right specification and this is something that we don't have the ultimate ask for for because it's very much domain-specific you came up with the specification for this wheel transfer in contract which was quite good was it the and the complete specification for this contract well I'm not sure I think you'll still need human auditors who will come up with the right specifications so the provers could prove them and then all will be safely put onto the blockchain and the machines could check that the proofs are good but I don't think we are at the stage when the specifications can be derived automatically and this is what we need to do much if I just may add one point to this which is you know if if we open this up at the community level you know you could say that look here is a I mean this how our security standards came out right you know people came up with an idea as a draft and then you know people started voting for it and then we realize that okay these are the functionalities or specification they desire from from a new your security kind of contract and then you know what you had this all these functions and interfaces that came up so you could you could have something along those lines here as well so you could say that look now that ERC 20 kind of contract has been established the standard has been established now what are the properties that you are actually seeking so you know you could say that I want my money to be you know when I when I make a transfer I want to ensure that the right amount gets transfer or if the mapping is if the ERC 20 contract mean here's a mapping does not mean doesn't get manipulated or you know changed gets changed over time so you know you could have another you know another line of this vetting process where a committee can jump in and say here is the PR the properties that we expect this contract to have and then you could have one other human people who would come in and say look here is a proof that that I have come up with can you bet that this poof so you know it goes to another level where you are not just building standard for for contracts but you're building standards for proofs for instance and properties so like you said the at the moment the process of generating these proofs is a very like human intensive process and it's like you said you almost need like trained mathematicians to do this so do you think this would cause like a lot of people to not even like go through this process so for example look you know in aetherium there are some very you know some decent tools like for very very preliminary checking such as like oh yeah hey and stuff but by your colleague at Loyola actually right and but you know there are so many contract on aetherium that are like vulnerable to things that Oriente would have easily picked up and so if it's like smart contract developers aren't even taking the time to like Oriente is literally like a one-click thing but still so many people aren't even taking the time to do that how do you how do we encourage developers and to like go through this like rigorous formal verification process so uh this is actually well known trade off Ian's not just in the contract engineering it's a well known trade off in software engineering as well and where the community has spent the last five decades building tools for testing analysis and verification so by no means verification is their ultimate question that solves like all possible issues and one issue is the issue of how difficult is it to adopt the approach so even though we designed this whole language so it would be verification friendly we will sure provide the suit for basic analysis and basic testing of the contract so this is sort of the right way to validate your code base the same way that the developers validate the code they write and other in other domains like okay so you can verify the compiler but then we will spend 10 years doing that or you can just implement the compiler and test it and have a 99% assurance that it's mostly bug free and if these are the chances you were willing to take well we will give you the tools for that so like one easy really really easy to implement analysis that is in our roadmap used to make sure that the contract does it doesn't leak money so with colleagues from in u.s. we have recently submitted ok so we made it public it's not published at the conference yes the paper that has an analysis for EVM that the text with a high degree of assurance that the certain contract might actually block money the same way parity will it did that or it might give money to someone who is not supposed to receive that and we are planning to do the same analyzers for sale as well and as the second part we will probably make this another an analyzer certifying so for a property as generic and as simple as the contract doesn't leak money we actually don't have to we will not have we will not need to have trained proof engineers to prove that so there will be two that will be checking that and providing this certificate but that's again that's only again because the property is very basic and the property is very simple like the property of this will transparent contract is something very domain-specific and that probably will require a certain amount of human effort will deliver it but that's with high restaurants comes higher cost of verification so dumb properties you can prove almost for free complicated ones you will need to invest the other point that I would like to add to this is if you look at the applications or the contracts that actually exists right now on popular blogging platforms like a cerium they can be categorized into very popular ones and not SuperFlow ones and those world which are very popular ones which are essentially let's see are c20 kind of contract or exchanges or you know some berries or auctions and for that you could have templates the way you have templates for your c20 so you could say that here is how an e or C 20 contract should look like and here are the possible properties that you may want to prove on this contract and then you could have associated proof directly either coming from let's see open real community that we have or from from anyone who has was experience in developing core proofs the hard way which no one wants is is to impose developer to submit a contract in fact that you cannot submit a contract without actually having a proof that's that would be really cruel now say for anyone because maybe you are very familiar with JavaScript or solidity or sila but you may not be very very familiar with compost so even if you want to do this you know you you you are still you know reluctant to do you know in doing this because then you really push away those developers will actually have very cool ideas but just because they don't have they are not familiar with coke pools you know they they can't build on you so yeah we will help have helper libraries for standard contracts and we'll have some you know basic properties that you can always prove or any contract for instance and then if you are if you know if your specialist you know feel free to write your own contract and you own properties okay let's first talk about what is because we have mentioned this to multiple times in the podcast and Iliya give us an idea what it can do okay so if the question is about a very short overview what the tool can do or what it was designed for so the tool is actually a very bad ok it first of all it's very old and it's very established so it's started in late 80s and it has been mostly a tremendous success in the past that say twelve thirteen years so with initially it was almost a verbatim implementation of one of those very powerful logics to conduct mathematical proofs the logic is called calculus of inductive construction then it was extended to a number of times but because of this correspondence between proofs and the program's it turned out that also comes with a very decent programming language it's not two incomplete you cannot write non terminating loops in it but you can create surprisingly many interesting and useful programs so pretty much anything you can come up with well anything that probably terminates and then it turned out to be the case that are in the very same environment you can write programs and you can write mathematical statements about them and you can write proof so all these three components that I mentioned before they can be implemented in the same framework and that was a very powerful insight and since then people wrote compilers in Coke which we are also verified and coke that's a famous concert project by nvm and savvy lor who was spearheading it and they wrote OS kernels and to the extent that people women wrote distributed systems and coke and it's interesting because obviously coke doesn't come and keep with all the distribute its socket machinery and all the backend so when there is a certain level of trust involved so what you write and coke is the logic of nodes and then you trust that the network implementation is correct and it corresponds to what is your mathematical model of this network competition but that is just all to witness that coke is very very powerful and from if you are a programmer and you look at it the code looks very much similar to code in languages such as Kokomo so it's a functional programming language we can recognize similar library similar primitives so if you actually want to write something in code that compiles and runs you don't even need to know about it's proof component it's fairly it's straight forward and that's it okay I have quite a number of years of experience with coke I used it for proven distributed systems correct and concurrent systems correct and that seems like a natural choice to adopt so essentially we designed Scylla to incorporate a significant subset of coke and add slightly more on top of it to account for the blockchain specifics and communication between the contracts but if you look at the most of the silicon it will look very similar to a subset of coke code and that's why it's so easy to translate Stiletto coke and proofs and who facts so we've been mentioning a lot about like you know Oh camel and Haskell and these like more established functional languages that are also design are not designed perhaps that are easier to translate to caulk right so what made you guys decide to create your own language rather than try to adapt the existing languages so for example you know one of the things that was happening you know aetherium when could the EBM I decided to create its own some more contract languages like solidity and stuff and now you know I it seems to me that there's a general sentiment that like you know why did we go out like we shouldn't have gone and like created our own languages now we have to create all the toolings around it and this sort of what some of the hype around webassembly is becoming where it's like oh this is awesome we can use the more established tooling around rust and go and C++ so did you got what were some of the trade-offs you guys had to make on like between using more established systems languages versus building your own alright so the process of designing a programming language is a series of very very painful trade offs and the trade those that we had to consider is expressivity first respectability so haskell and O'Connell okay let me focus on functional languages because those are closer to mathematics and that's the reason why there is a lot of hype in the smart contract community about how contract should be written and functional paradigm so they're fantastic languages they're very expressive and you can express a lot of computation in a very few lines of code but that comes with the cost that the semantics of Haskell and semantics of a camel that is what the programmers need to know in order to write programs in them it's very big and it makes it very costly to embed this language in a formal reasoning framework Cypress code so in this way there wasn't it so essentially the the rationale was to scale down to the bare set of features that I required to write reasonable smart contracts keep it readable but do not add stuff on top something like syntactic sugar something like type class operators what Haskell has type kind polymorphism and all these crazy goodies that dedicated Haskell hackers use so that's that's right so we wanted to have a minimalistic caboose and this is essentially how the basic research in programming languages go so if you want to have a formal system about which you can prove things you try to remove all the goodies that actually make programmers life easier and this is why we position this as an intermediate level language so we don't exclude the fact that something like a very large subset of a comma and Haskell can be translated down to Scylla when we come to this point but we wouldn't be proven stuff about this top-level program it would be prune stuff about the programs that are that are represented instead so again the trade-off expressivity versus tractability or for the sake of formal reason so in a sense the the more expressive this a man takes the more the range of things programmers range of basic constructs the programmers can use in writing a program the more expressive a language it it is the harder it would be to prove properties about the programs that that result out of it and in silly ends Scylla is meant to be meant to have very little expressivity but amenable to proofs and then in the future presumably somebody could write in Oh camel have it compiled down to Scylla and then prove properties about the code in in in this insula that is correct okay so uh we have a we have around like fifteen minutes left and one of these things we'd like to focus on and understand is how does Scylla compared to other efforts that are going on you know in the formal verification and smart contract space so of course like this idea that smart contracts ought to have proofs and they need to design languages so that tear improving is easy has been taken up with by other projects and one of the most popular at least in terms of funding levels is is the pesos project which has developed this language called Mickelson or Michelson it seems to have very similar aims to Silla could you could you give us an idea of how the tezo's approach and this Silla approach might be similar and how they might be different all right so the question is about Mickelson language in tezo's and our Silla yes so the design goes are very similar in a sense that both tezo's community and us are very much interested and for more verification or rigor of the stated properties of the contracts furthermore we share a lot of background because tezo's comes from Oh camel laps and most of the camel laps employs well specifically I Benjamin canoe and actually brakeman who were one of the founders they all come with so little camel background and formal methods background so it is quite unsurprising that they are after the same thing I think them and they also are functional programmers so Mickelson is very much a functional programming like a functional programming language so the devil is in the details so for some reason Mickelson was designed as a stack-based the snake based language something that mimics EVM or similar formalism closely but it is not yet clear what are the trade-offs with regard to the form of verification so one of the claim that the interpreter for this tech based formalism is actually quite simple and it can be fully embedded and poke well I wouldn't say that the interpreter for Silla is much more difficult but a reason about the stack is not something very nature so that's that we haven't seen that many proofs about tezo's contracts so it's it's hard to have an apples to apples comparison and see what the poke embedding or let's say crowdfunding extractive contract enters in the concern would be like and how their proofs compare in terms of length so this is ultimately goes to benchmark in the sizes and see and chicken whose proofs are a larger or not since they don't have them yet they might come soon that will be so uh in fact the okay so the idea of having this low-level step based based language in Mickelson leads to the second development and now they have liquidity which is a more high-level language which is actually quite similar to solar in the sense that it also looks like a subset of a camel and it has very similar functional programming mattias the thing which is missing from there or maybe it was a design choice is the explicit communication between the contracts in a sense that each contract is kind of this sealed autonomous actor that only receives the messages from other contracts and send messages to other contexts and this is only the way for the contracts to interact on the blockchain so this is something that Silla as a language enforces syntactically so you like if you write the contract is going to be shaped as this actor that receives messages and sends messages so it's not enforced in liquidity and either it is in Mickelson and that's why I can only speculate the station properties like what we discussed before about this passenger in the future is going to be a bit more complicated should they have a formal and a formal model of liquidity somewhere income so yeah so basically the the main difference is that liquidity / Mickelson don't have them this after message passing model enforced at the language level and the second is Mickelson a stack-based which makes it slightly more low-level then and still if I just may add to one point this is if you look at time since you're talking about other languages we should we should talk about this language from atheneum which is bamboo and again the idea was with bamboo is is the straightest trade-off between expressivity and the properties or the correct the safety in property that you can prove about about the contract so I would say that it's it's bamboo is another language which is lexis expressive to some extent but it's more structured in the sense that what they call you know polymorphic contracts and the idea is that you know the contracting gives you a rigid structure and it tells you how the contract will change itself depending on how users are interacting with it so instead of having five let's say five functions or five transitions in your contract you would have one contract and then another contract and then another contract and this contract will eventually change depending on how users interact with this contract so it has a very pretty different kind of behavior to some extent how this contract will behave so I I feel that you know it's the idea that we are doing in Seoul is pretty close to me every Lea will be able to elaborate more on this but I feel that that my bamboo is has some elements that see La Silla has as well to some extent this skirt another kind of important difference I saw I'd notice as well was that in Mickelson there seems to be a focus on making it purely functional while in Silla there is this ability to have like some sort of state that you their functions are allowed to modify and what were some of the trade-offs you did on making that decision okay so the question is why still is not purely functional while Mickelson and liquidity seem to be a purely functional so uh this is actually an interesting question because it very much comes down to the terminology of what you count as a functional and what you count as an imperative language so both Silla and Mickelson have stayed but in Mickelson this state or liquidity which makes it even more explicit this state is made explicit so basically you pass this state around like accumulating certain components to it and you can even ship it to another contract and you can get it back from another contract and then you can finally give it back to the blocking back and it will probably store it so this is probably the level of verbosity that we wanted to exclude I mean I said that we offer taken minimalistic choices but having a state that the contract modified seems so paramount and so important that we said okay so we are going to have state we are going to have very small set of operators that interact with this state and wells and stated something like this this was the sort of mutable I stated something the formal methods community knows how to deal with so there are many verifiers there are many program logics that can proof automatically the effects of program interaction with the state so we didn't really see the benefit of making it absolutely pure functional and to this extent Silla also has a bunch of other effects that are expressed on the semantics so in addition to change in state we also have send messages and in the future we will have emitting events so making these things part of the execution was important and obviously you can model all that through some state pass and mechanisms but we thought that by the way for all right in certain specifications it will be better to implement them the way with it so bottom line most of the state manipulation you can do in Silla you can encode them Tasos in Intel's of Nicholson as well it's going to be less straightforward and it's at the moment it's not clear whether it's going to be exploitable in a certain way or not as I said since Mickelson implements interaction between the contracts using this state pass and mechanism rather than by sending messages the implications are not clear so another question then maybe somewhat related is you know we discussed earlier one of the other main features of the silica platform is the whole full sharding mechanism so what were some of the design choices that went into the language as well as the overall vm design that were you know one of the big things that people often won't need with charting is you need the ability to do like a synchronous contract calls and stuff and so what were some of the thought process of a charting that went into the design of this system I mean for instance to start with though we don't have an EVM the way VM to be EVM it has a VM we are currently have we currently have an interpreter that takes silicon track interpreters now the problem is you know you you have a short in architecture and you need a way to cut off as you said you know to ask messages you know in an asynchronous manner so it's a it's it's a work in progress where there are three different angles that we're exploring right now one is can you ensure that all from all transactions that somehow you know may be in conflict because they are touching the same seat or reading from the same stage or manipulating the same state gets shorted to the same shock so this is this is one idea if you can do that then you know you can even meet you know certain issues that near the idea is that can you actually have some kind of a language support which easier would be able to I don't can you have some it some some level at the language which allows you to kind of merge certain transactions going in different shots and and there is could be other ideas for instance can you have some kind of a conflict detect detector or resolve was somewhere in the architecture that helps you helps you to you know solve those problems so we have we are currently actively exploring all these themes three directions and we have plans to write a paper around this which will be happy to make a publication so what we like to get a sense of is what is the current state of development of Silla and what can the community expect to see in in the form of releases over the next six months so or what what we have right now is is an interpreter that it's not complete of course it's still working promise but it is it's it is in a stage where we can write we can still write some interesting contracts so we had an example in the sale of white paper initially or the kickstarter contract so that contract can be doubled by by our interpreter right now so we have that support ready there are some other examples that we are currently working on we will we are working on examples like ER c 2007 21 so uh the the deputies in the stage berry skin can handle certain contracts it cannot handle all contracts that that you would imagine right now it doesn't have certain supports for certain data structures for instance but we do have we do have a working prototype where you know your user can deploy a contract to the silicon network and the silicon network and no Dena silicon network can actually run that contract and commit the state to the blockchain and the entire chain is ready our plan is to have the complete implement you know implementation of the interpreter ready by end of June when we are planning to have our second version of the test net where we have plans to you know to show to people you know here's how Silla looks like and you can play around with contracts let me add to that so on the verification side we also have the prototype implementation of Silla in Coke which is done by means of what's called shallow embedding so essentially instead of implementing the whole language interpreter in Coke we only implement those primitives that Coke doesn't have and since there is a lot of shared parts between the syntax the rest is taken from Kok verbatim and in this prototype we can prove the properties like what we discussed and the very same Kickstarter contract is implemented in this in this formalism and the properties are established what is missing though is the connection link between what runs on the blockchain and what is formalized and coke so that can be done by syntactic translation this is something that we will provide provide very very soon so you could write a contract and solar then yes expected to call their you prove the properties you know that is right and then you can go back to your implementation and rabbits on the silica blockchain cool we look forward to the the releases especially in the in the next version of the test net if people are able to deploy basic silicon tracks that would be pretty pretty cool we look forward to that so Amrit and Ilya it was a pleasure to have you on the show and we hope we'll catch up again on Silla and silica six months down the line when there's there's more to report especially maybe what might be interesting is to discuss the interaction of programming languages and shouting and your in your paper there silica always remains one of the projects I a follow so I'm really looking forward to what comes out of this project when it goes into the main net so for our listeners thank you so much for joining us as you know we release new episodes of epicenter Bitcoin every Tuesday you can subscribe to iTunes SoundCloud or your favorite podcast app for iOS and Android to keep up to date with the epicenter you can watch the video version of this show on our youtube channel at youtube.com slash epicenter Bitcoin also we've recently created a gaiter community where you can chat with us and other epicentre listeners at epicenter dot TV slash gator I find that the conversations here are are very intellectual because of the nature of the community we have we have built so check our channel out and of course your reviews and your feedback on Twitter and iTunes is very valuable to us and it keeps us going so support the show with leaving us a review on iTunes and we look forward to being back next week thank you for listening [Music] [Music] "
89fKiaMxHrA,27,"Lectures by Professor Eric Hehner

http://www.cs.utoronto.ca/~hehner/FMSD/",2016-04-01T16:21:19Z,Formal Methods of Software Design - Introduction [0/33],https://i.ytimg.com/vi/89fKiaMxHrA/hqdefault.jpg,Preserve Knowledge,PT11M57S,false,21102,255,3,0,15,hi I'm Eric hey nur I'm from Toronto Canada and I have the pleasure to give this course on formal methods of software engineering I hope you enjoy it and I hope it helps you become a better software engineer formal methods are on the theoretical side of software engineering every branch of engineering has its theory Electrical Engineering has Maxwell's equations and kerkoff's laws civil engineering has geometry and the theory of material stress to be an engineer you have to know the relevant theories and be able to apply them to practical problems there was civil engineering long before any Theory people built buildings but not skyscrapers sometimes the buildings collapsed that was considered normal then now there's an advanced theory and we don't let people build buildings unless they know the theory and their buildings rarely collapse software engineering is newer than the other branches of engineering and its theory is even newer right now there are a lot of people building software who don't know any theory and the software often has bugs that's considered normal but it's changing it changed very quickly for the VLSI industry when Intel discovered a bug in their floating-point hardware that costs the many millions of dollars to fix now formal methods are used to verify algorithms that will be cast in silicon before they are sold they are used by software companies that produce safety critical software where people's lives are at stake all of these applications have made use of formal methods the banking industry and creators of text editors and web browsers and mail programs don't yet use formal methods when that software fails no one dies but Microsoft is now starting to introduce formal methods to some of its programming the applications where you really have to use formal methods are those where you really must get the software in a few years I think the use of formal methods will be normal practice there are two ways to look at programs one way is that programs are commands to a computer and the other way is that programs are mathematical expressions we need to think of them as commands to a computer when we want to get the program executed we need to think of them as mathematical expressions when we're programming so we can use the theory of programming to help us get the program's right why theory people sometimes use the word theory to mean a guess they might say here's my theory of what happened or maybe they mean an explanation of something what I mean is a means of proving theorems it's sometimes called formal theory because it consists of a formalism which means the rules for writing formulas and rules for manipulating the formulas rules for calculating with them rules for proving theorems whenever I say theory I mean formal theory so I won't bother saying formal we need the theory to prove our programs are correct we need to program by calculation just as a theory of motion lets us calculate the trajectory of a planet we want the same kind of precision most of all what we can get from the theory is understanding just as a theory of motion gives us an understanding of the motion of planets and stars a theory of programming gives us a better understanding of programs when I said formal theory I did not mean a careful and detailed explanation likewise by informal I don't mean a sloppy or sketchy explanation by formal I mean using mathematical formulas and by informal I mean using natural language explanations it's quite possible to be informal and careful and detailed and just as possible to be formal and sloppy and sketchy there isn't really any choice about whether to be formal or informal when you start a programming project you have to start informal discussing it with other people finding out what's wanted maybe drawing some pictures and you have to end formal because you end with a program and that's completely formal the only question is how to go from an informal start to a formal finish when you are finished you have to test your program to see if it's working but how do you know if it's working well if it crashes obviously it isn't working but if it doesn't crash how do you know if the answers it gives are correct and you can test only some cases not all of them because there are too many to test maybe even infinitely many there may be errors that would show up in the cases you didn't test the only way is mathematical proof proof tells whether the program is correct for all inputs even if there are infinitely many inputs some programs tackle a verification problem given a specification and a finished program prove whether the program is consistent with the specification that's a really hard problem it's much easier to do the proof as your program proving each programming step as you make it that's because the information needed for the proof is exactly the same information needed to make the programming step what makes verification after development so hard is that it has to reconstruct that information this course concentrates on program development with proof at each step and on program modification again with proof that the modification is correct the first usable theory of programming in 1969 was triples or sometimes it's called logic it's still the best-known theory and if you've seen any formal methods before it was probably that one since then lots of other better methods have been developed these have all been used by industry but only to a very limited extent the method that has been used most is model checking model checking is really the exhaustive automated testing people like it because it's a lot easier than proving it checks all possible states of all executions against the list of properties that executions should have so model checking works only on programs that have a finite number of states but a good model checker can handle an incredible number of states they say 10 to the 60th power of them that's something like the number of atoms in our entire galaxy amazing now 10 to the 60 is approximately equal to 2 to the 200 so that's the space of 200 bits and that's about the same as six ordinary 32-bit variables so we're talking about programs with six or fewer ordinary variables that's not very impressive at all sometimes there's a way around this limitation if the program has a state space that's too large for model checking you might be able to find an abstraction of the program a simpler program with a small state space such that model checking the simpler program still answers your questions about the larger program finding a good abstract is hard and you have to prove that it doesn't lose the properties of the big program that you want to check so we're back to proving there's really no way around it you're welcome to look up any of these formal methods that interest you but we won't be using any of them in this course the theory we will be using is simpler than any of those theories in those theories a specification is a pair of predicate or a function from predicates to predicates or a temporal logic expression in our theory a specification is just a binary expression that's the kind of expression you right after the word if in a program our theory is also more general than any of those theories some of them are only for terminating computation and some are only for non terminating computation ours is for both some of those other theories are only for sequential computation and others only for parallel computation ours is for both some of those other theories are only for standalone computation with no interaction and others are only for interactive computation ours is for both and our theory also gives time and space bounds and can be used for real-time programming also it also works for probabilistic computation that includes random numbers and produces its output with some probability distribution the prerequisite for this course is that you have some programming experience I don't care what language you used any programming language will do as long as it has an assignment statement and an if statement we'll be using a language that's so small it will take you 30 seconds to learn it here's the textbook for the course I wrote it so it fits the course exactly and the best news of all you can download it free from my website one of the nice things about having the lectures online is that you can watch them on a computer or any portable device anytime and anyplace you want to you can stop anytime and resume later you can replay anything you want to see again you don't have to play it straight through as though it were a classroom lecture you can and should pause at any point where you need to think about what was just said before you go on I hope you enjoy the course and benefit from it 
Z--EIHWOeoE,27,"Path to formal methods

Helpful?  Please support me on Patreon: https://www.patreon.com/roelvandepaar

With thanks & praise to God, and with thanks to the many people who have made this project possible! | Content (except music & images) licensed under CC BY-SA https://meta.stackexchange.com/help/licensing | Music: https://www.bensound.com/licensing | Images: https://stocksnap.io/license & others | With thanks to user uli (cs.stackexchange.com/users/169), user Uday Reddy (cs.stackexchange.com/users/207), user Dave Clarke (cs.stackexchange.com/users/31), and the Stack Exchange Network (cs.stackexchange.com/questions/386). Trademarks are property of their respective owners. Disclaimer: All information is provided ""AS IS"" without warranty of any kind. You are responsible for your own actions. Please contact me if anything is amiss at Roel D.OT VandePaar A.T gmail.com",2021-02-27T08:05:25Z,Path to formal methods (3 Solutions!!),https://i.ytimg.com/vi/Z--EIHWOeoE/hqdefault.jpg,Roel Van de Paar,PT3M9S,false,2,0,0,0,0,[Music] what's up ah everybody starts a video with what's up that's boring it's cliche it's okay beep now this video will quickly show you a technical question as well as possible answers hope you enjoyed please subscribe to my channel me and my family would really appreciate that and god bless [Music] oh [Music] um [Music] me [Music] [Music] [Music] [Music] [Music] oh [Music] me [Music] click like and subscribe thank you for watching god bless 
dS-glXTfOkk,27,"This video is about the use and importance of the course Automata Theory and Formal Languages to the program BS Computer Science. 
List of prerequisite knowledge to underdstand the course, general description,cognitive and procedural objectives.",2020-07-25T12:23:47Z,Automata Theory and Formal Languages (Course Overview),https://i.ytimg.com/vi/dS-glXTfOkk/hqdefault.jpg,EasyMath ni Maam Carol,PT20M41S,false,346,18,0,0,9,for this video we are going to do a course overview of automata theory and formal languages the subject automata theory in formal languages is offered to bachelor of science in computer science during the first semester of their third year the prerequisite knowledge needed in order to understand the subject automata theory and formal languages are the following first one is programming programming subjects are offered during first and second semesters of their first year the second is data structures data structures one and two are offered during second semester or fourth of first year and first semester of second year now algorithm and complexities which is the third prerequisite knowledge is offered during the second sem of second year so um important thing important non-concepts associated general um ideas programming it still um gives you an idea on grammar not associated than some other mathematical concepts yeah take note now done some mathematics in the modern world kung meron language and expressions that we use to communicate with other people so it's either tagalog or english some but the thing did some uh computers and machines melanin corresponding mathematical expressions sentences algorithm and complexities uh algorithm and complexity so if that will be the case now prerequisite knowledge on programming data structures algorithm and complexities then you will know na and you will notice automata theory in formal languages level of difficulty as compared to uh first three nato so the general description of the course automata theory and formal languages can be found learning outcomes one and two na is specifying chad duns the chat memorandum order 25 series of 2015. so paradoxical uh automata theory and formal languages these are the things that we are going to do design finite state machines regular expressions push down automata and turing machine for modeling a given language so define the classes of p and np and explain their significance to computing applications and apply the concept of state machines in the design and implementation of software so what's up in general description it also gives us an idea from anima gigging content or scope non course nato s what are the things that you need to understand the theories to describe formal languages the concept of formal grammar and their types as well as the type of language the concept of finite automaton as a regular language recognizer the concept of regular expression as a description of a regular language the concept of a push down automata to recognize any context-free language the relation between grammars languages and recognizers the principles and operation of a turing machine and its different types the concept of computational complexity the methods to calculate the computational complexity the concept of p and np complexity classes and the limits of computation languages object not in english these are mathematical grammars or computational grammars and languages recognizer so um i don't want you to worry at this point in time but it would be good if you would browse and look for the definition of this words nah binigay saying you but um we are going to uh discuss this one by one and in detail i um um yeah just um hold on and kayan adam there are several procedural objectives that are needed to be done in this course procedural main procedural objective if you remember doon's uh chad memorandum order 25 young cs01 hong kong zero five kailangan i executed [Music] so these are the procedural objectives associated to automated theory and formal languages assess how to address a problem of recognition of words for a certain grammar elaborate correctly the faces for the construction of a recognizer from the description of the grammar to the design of the automaton combine and extrapolate the acquired knowledge about developing a lexical or synthetic recognizer for a grammar acquire skills to assess the efficiency of a given automaton in the task of recognizing a specific also to discern whether the automaton is minimal or not learn the practical application of theoretical models of the exposed computation or calculation devices included are grammars finite automata push down automata and turing machines for solving problems or arithmetic computation know the methods to calculate the computational complexity of an algorithm an automaton or a turing machine acquire the ability to transform a formal statement to an informal statement students theory and formal languages now the automata theory is the study of abstract machines so take note the word abstract and uh the computational problems that can be solved using them so puru computation or mathematical computations it is a theory in theoretical computer science so my definition didn't say theoretical computer science and theoretical computer science focuses on mathematical concepts of computing in the intelligence we will associate a tangible one with our um step-by-step study abstract machines so the word automata which is uh the plural of the word automaton comes from the the greek word and automata young word automata greek letters which means self-making so ibiza being washed which is actually what machine does it makes or produces output on its own uh as long as structures one is part of the finite state machines the partner push down automaton apartment turing machines complex turing machine so they are automatic language uh theory that is uh undergraduate subject launch continuous data young hybrid so young finite state machine push down automaton turing machine involved a discrete data union so why do we study automata theory all computer science students must learn to integrate theory and practice so to recognize the importance of obstruction and to appreciate the value of good engineering design everything starts with a plan or with a concept [Music] existing programming language now automata theory is closely related to the definition of computer science and its objectives so subbing adito computer science is a science whose main objective is the resolution of problems by means of computers that is digital systems you are given a particular problem and in the long 4k main problem you have to analyze it and you have to resolve the problem using computer and digital systems so the main objective of computer science has to be understood as the systematic study of algorithmic processes and data structures that describe and transform information and not the development of programs itself computer science you're a scientist so um critical thinking and analysis and as long as your analysis and critical thinking is correct then it can easily be transferred now see i think this one this person is a a doctor richard m carp on computer science so the richard m karp is a cheering of award laureates uh 1985 for the development of efficient algorithms for network flow and introduce the anti-complete so uh look at this object according to him we must remember that computer science is a scientific discipline okay malala immune and scientific discipline and not only a branch of high technology so what about formal language the concept of language may refer either to the human capacity for acquiring and using complex systems of communication or a specific instance of such a system of complex communication so generally pakistan language it is a form of communicating for one from one individual to another but when it comes to automata theory and formal language it says here that a language is formal if it is provided with a mathematically rigorous representation of its alphabet and formation rules in computer science formal languages are used for the precise definition of programming languages [Music] is it is a programming language that allows us to use digital forms and it requires digital forms parashat function and that's how it is union uh ebik's formal language so it has something to do with the precise definition of programming languages so next video we are going to start with the first module young finite we will discuss it um in detail in topic per topic okay so that ends our video thank you very much and god bless subscribe and share please comment down below [Music] you 
UG70-z4l_wQ,27,"Lectures by Professor Eric Hehner

http://www.cs.utoronto.ca/~hehner/FMSD/",2016-04-01T16:21:19Z,Formal Methods of Software Design - Binary Theory [1/33],https://i.ytimg.com/vi/UG70-z4l_wQ/hqdefault.jpg,Preserve Knowledge,PT29M30S,false,6014,87,1,0,4,formal methods of software engineering means the use of mathematics is an aid to writing programs before we can start applying mathematics to programming we have to introduce the relevant mathematics now don't worry about this I'm just going to introduce the math that we really are going to use and nothing more and you may have already learned most of this math in other courses if you did this is mostly review but there will be a few things you didn't already know and I want to make sure we're all using the same notations if you look at the last couple of pages of the textbook you'll see all the notations used in the course and I bet you already know most of them to start with this lecture talks about binary theory which is also called boolean algebra or maybe basic logic or some people say propositional calculus the expressions of binary Theory are called binary expressions some binary expressions are called theorems and others are called anti theorems binary expressions are used to represent anything that comes in two kinds the theorems represent one kind and the anti theorems represent the other kind for example if you think that statements about the world coming to kinds namely the true statements and the false statements then you can use binary expressions to represent statements you can use the theorems to represent the true statements and the Antti theorems to represent the false statements I don't say that statements about the world really do come in just those two kind and it's no business of binary Theory to say which statements are true and which are false but if someone supplies us with statements that are labeled true and false we can use binary theory to represent them another application is digital circuits because the output of a digital circuit is either high voltage or low voltage we can use the theorems to represent circuits with high voltage output and the ante theorems to represent the circuits with low voltage output another application is human behavior as viewed by the legal system we can use the theorems to represent innocent behavior and the ante theorems to represent guilty behavior here are two the two simplest binary expressions if we're using them to represent statements we might call the first one true and the other one false if we're using them to represent circuits we might call the first one power and the other one ground and similarly we could choose words from other application areas or to be independent of all application areas we can call the first one top and the other one bottom I tend to call them true and false the next notation is the not operator or negation operator it has one operand actually there are four operators that have one operand but two of them are degenerate which means they don't make use of their operand and one is just the identity operator next are the operators that have two operands actually there are sixteen operators that have two operands but six of them are degenerate leaving only ten good operators and we're using only six of them the first one is pronounced x and y it's a conjunction and its operands are called conjuncts the next one is pronounced X or Y it's a disjunction and its operands are called disjunct the next one is pronounced X implies Y or we could say X is stronger than or equal to Y it's an implication and it's left operand is called the antecedent and it's right operand is called the consequent this one is pronounced X is implied by Y or X is weaker than or equal to Y it's a reverse implication it's left operand as the consequent and it's right operand is the antecedent this one is pronounced x equals y it's an equation it's not an assignment nothing is changing value here it's an expression whose result is true or false its operands don't have any special names we just say left side and right side the last one says X is unequal to Y there are a lot of operators that have three operands 256 of them but we just want one of them it's called conditional composition or if-then-else feet and the operands are called the if part the then part and the else part there's a precedence table to say what the order of evaluation is you can find it on the last page of the textbook and if you want some other order of evaluation that's what parentheses round brackets are for on that same table it says that some operators are associative for example conjunction is associative that means it doesn't matter which way we parenthesize the result is the same so we don't bother to write the parentheses and we can write a longer chain of conjunctions without parentheses because always of parentheses ngey the same result same for disjunction even though equals and unequals are also associative we don't use their associativity to save parentheses that's because they are continuing operators that means if I write x equals y equals Z I mean x equals y and y equals Z that's standard mathematical practice but it's not the practice in most programming language likewise the implications are continuing so if I write X implies Y implies Z I mean X implies Y and Y implies Z and we can make longer chains of equations longer chains of implications and even a mixture of the two and it means a conjunction of single equations and implications you notice I didn't need parentheses to write a conjunction of equations but I did need them to write a conjunction of implications that's because on the precedence table equals comes before and but implies comes after and the equation and implication operators come in two sizes the big operators mean exactly the same thing as the little operators all the same laws apply the only difference between a little operator and a big operator is the precedence so you can choose the size that saves you some parentheses too many parentheses cluttering up an expression can make it confusing on the last line is an example of a mixture of big operators being used in a continuing way binary expressions are evaluated according to these truth tables the operands are on the top line and the result is inside the table for negation it says that not applied to true gives false and not applied to false gives true actually it says more than that the symbol for true is representing any theorem and the symbol for false is representing any anti theorem so it says that if you apply not to a theorem you get an anti theorem and if you apply not to an anti theorem you get a theorem this row shows why conjunction is called and you see that the result is true just when both left and both the left conjunct and the right conjunct are true and on this roll you see why disjunction is called or the result is true when either the left disjunct or the right disjunct is true it's false only when they're both false if I could remember to call them top and bottom which would be better because it's independent of application then I should say that conjunction gives the minimum of its two operands and disjunction gives the maximum of its two operands do you see that implication is the ordering operator it means lower than or equal to the first entry says top is lower than or equal to top but because they're equal the next entry says that top isn't lower than or equal to bottom bottom is lower than or equal to top and last bottom is lower than or equal to itself if I'm saying true and false I could call this operator falser than or equal to on the next line we have reverse implication which means higher than or equal to or if you prefer truer than or equal to the equal operator is pretty obvious and so is the unequal operator its result is true just when its operands are unequal some people like to call this operator exclusive or because it's result is true when either the left operand or the right operand is true but not both of them the if-then-else fee is easy to remember because if the if part is true then the result is the same as the then part if the if part is false then the result is the same as the else part the if at the beginning and the fee at the end are like opening and closing brackets for this operator the purpose of variables in an expression is to substitute something else in their place substituting something for a variable is called instantiation when you substitute you have to be a little careful about the precedence of operators for example if we had the expression x + y we can replace x by anything so let's replace it by false and we can replace Y by anything so let's replace it by false or true so the result is supposed to be the conjunction of false on the left with false or true on the right but because of the precedence table we have to put parentheses around false or true we started with a conjunction and after we replace the operands we should still have a conjunction if we didn't add the parenthesis it would end up as a disjunction the other rule of substitution is that it has to be systematic if the same variable occurs more than once you have to replace it with the same thing at each occurrence different variables can be replaced by different things but they could also be replaced by the same thing if we want so from X and y we can get false and false or we can get true and false if we want well that's all there is to binary expressions so now we have to apply binary theory to some application area an application always supplies its own new binary expressions for example it might supply these new binary expressions the grass is green the sky is green there is life elsewhere in the universe and intelligent messages are coming from space I don't know what this application is about but anyway it gives us these new binary expressions when we get to number theory it will give us these new binary expressions and infinitely many others the application also has to tell us which of the new expressions are theorems and which are anti theorems when you're classifying binary expressions as theorems and Antti theorems it's really important to be consistent that means you don't want to classify some binary expression as both a theorem and an anti theorem you might like to be complete also which means you've classified every binary expression one way or the other but you don't have to be complete you do have to be consistent finding out if a binary expression is a theorem or an T theorem is called proving and there are five rules for how to do that the first rule is trivial it's the axiom rule long ago mathematicians thought that some things in mathematics mathematics were obviously true and those things were called axioms for example it was obvious to them that parallel lines never meet so that was an axiom later on mathematicians discovered that there are interesting non-euclidean geometries in which parallel lines do meet every single thing they thought was obviously true turned out to be just a choice if you say it's true you get one kind of mathematics if you say it's false that's another kind of mathematics in my opinion there is nothing true or false in mathematics an application area may have true and false statements and so you have to design your mathematics to apply to that application area for example we include this expression in our mathematics to represent the truth in some applications that when you put quantities together the total quantity does not depend on the order in which you put them together there are other applications where that's not true but we're designing our mathematics for those applications where it is true so we make it an axiom and according to the axiom rule it's a theorem and we'll see that that makes it equivalent to top however just like everyone else in the world I'll probably say that X plus y equals y plus X is true even though that's not quite right we just choose to use it to represent a truth in an application area but I'm getting way ahead of myself here so let's get back to binary theory the only axiom of binary theory is top and the only anti axiom is bottom for our silly example application area we have an axiom that says grass is green and an anti axiom that says the sky is green now here's another axiom I have no idea if there's life elsewhere in the universe so I won't make that an axiom or an anti axiom I'll just leave it unclassified until we find out if it's true or false and I also have no idea if intelligent messages are coming from space so I'll leave that unclassified - but if intelligent messages are coming from space then there must be life elsewhere so we might want this implication to be an axiom the second proof rule is the evaluation rule it says if all the sub expressions are classified then use the truth tables if you know what the operands are then the truth tables tell you what the whole expression is next comes the completion rule it says if an expression contains unclassified sub expressions and all ways of classifying them place it in the same class than it is in that class if you don't know what the operands are you might still be able to tell what the whole expression is for example there is life elsewhere in the universe or true now if there's life elsewhere then that's true or true which is true if there isn't life elsewhere then that's false or true which is still true so either way it's true so the completion rule says it's a theorem here's another example either there's life elsewhere or there isn't if we put true in both places we get true if we put false in both places we get true so it's true and one more example there's life elsewhere and there isn't if we put true in both places we get false if we put false in both places we get false so it's false next we have the consistency rule it says if a classified binary expression contains binary sub expressions and the one only one way of classifying them is consistent then they are classified that way this time we know what the whole expression is and we're wondering what the parts might be here's an example we're given that X is a theorem and also that X implies Y is a theorem and the question is what's y here's how we figure it suppose Y is an anti theorem now we have X as a theorem and we're supposing Y is an anti theorem so we can use a truth table to find out that X implies Y is an anti theorem but we already know it's a theorem so that's inconsistent so Y has to be a theorem here's another example we're given that not X is a theorem what's X if X is a theorem then the truth tables say that not X is an antiserum but we already know it's a theorem so X has to be an anti theorem this example is important because it means that we never need to talk about anti axioms or anti theorems if we want to say something as an anti theorem we just say it's negation is a theorem we also don't need to have both true and false for false we could say not true the logic tradition here seems a bit odd to me traditionally logicians use both the words true and false but they don't use both axiom and anti axiom and they don't use both theorem and and theorem oh well on to the last proof rule the instance rule if a binary expression is classified as a theorem or an anti theorem then all its instances have that same classification for example in the textbook somewhere there's the axiom x equals x so by the axiom rule it's a theorem and so if I substitute any expression for both X's it's still a theorem notice the two sizes of equal sign here just to keep the precedents right we could evaluate this expression to find out that it's a theorem but the instance rule says it's a theorem just because the two sides are the same in this example we cannot evaluate the two sides but still it is an instance of x equals x so it's a theorem that's all the rules in this course we can use all five rules and that's called classical logic in a logic course it's interesting to see what you can do if you don't allow the completion rule that's called constructive logic and if you don't allow the consistency rule and the completion rule it's called evaluation logic but we can use all five rules now I want to talk a little about format it helps a lot if you leave more space around operators with less precedence since conjunction comes ahead of disjunction on the precedence table this expression should be written with the spacing shown you may think that's a silly little point but look what happens if you write the same expression with a different spacing it's misleading and causes errors when an expression is too long to fit on one line it should be broken at a sensible place which usually means at the main connective programmers seem to appreciate the importance of good format better than mathematicians because programmers formulas mainly programs are much longer than mathematicians formulas now about the parentheses or brackets there's a popular convention in programming that puts the opening bracket of a loop over on the right side and the closing bracket over on the left do you know why I'm old enough to remember where that convention comes from it comes from the days when programs were punched onto cards one line per card now you might want to reorder the lines in the body of the loop and you do that by reordering the cards or you might want to delete the first line by throwing away one card or add a new first line or delete the last line or add one more line to the end so you don't want the opening bracket on the first line of the body of the loop because after editing it might be in the middle of a loop somewhere or get thrown away and the same for the closing bracket but we don't use punched cards anymore when we edit we can select just what we want to delete and it doesn't have to be a whole line and we can insert text where we want it and everything else moves over so there's no purpose to that convention anymore still all OSI and Java programmers are following it without even knowing why let's get rid of that and put our left brackets on the left and our right brackets on the right and if we don't have brackets format the same way but without the brackets here's a continuing equation nicely formatted it really means that expression 0 equals expression 1 and expression 1 equals expression 2 and expression 2 equals expression 3 a continuing equation is very useful as a proof that the first expression equals the last one to help the reader understand the proof we put hints over on the right side of the page hint 0 is supposed to say why expression 0 equals expression 1 usually it's just the name of some law and in 1 says why expression 1 equals expression 2 and so on the hints go between the two expressions that we're trying to bridge let's look at an example actually this example is a law that's in the back of the book it's called portation and it's a really useful law you should probably memorize this one but suppose we don't know it's a law if we want to prove this equation we can do so as follows I started with the left side but I could just as well start with the right side and work down to the left side I usually start with the more complicated side but here they're about equally complicated the first hint says material implication that's a law from the back of the book that equates an implication and a disjunction a law is just a theorem that's worth giving a name and worth remembering let me spread it out a little I know now it's badly spaced but I just want to show how we're instantiating that law we're replacing a with a and B both occurrences of a get replaced the second time had to add parentheses and we're replacing B with C so that's how the first line of the proof becomes the second line and the same for the other lines now here's another way to write the proof I put the whole equation that I'm trying to prove on the top line and show that it's equal to true on the bottom line one reason I like this way is that proof is just the same as simplifying proof means simplify to true and another reason is that equations aren't the only thing we prove we could be proving a conjunction or a disjunction or anything we can always prove by simplifying to true that's enough for this lecture I'll finish up chapter one next time 
Mkh9vyIGMEw,27,"Lectures by Professor Eric Hehner

http://www.cs.utoronto.ca/~hehner/FMSD/",2016-04-01T16:23:03Z,Formal Methods of Software Design - Data Theory Design [24/33],https://i.ytimg.com/vi/Mkh9vyIGMEw/hqdefault.jpg,Preserve Knowledge,PT29M30S,false,146,0,0,0,0,this lecture we study theory design and implementation programmers have two roles to play here in one role they are theory users they use numbers and lists and lots of things that have been invented by other people and formalized as theories so programmers can prove their refinements programmers also use whatever features their programming language includes and it's the responsibility of the language designers and implementers to define those features formally as theories the other role programmers play is designers and implementers of new theories every time you write a program you invent new abstractions and new structures in order to prove your refinements you have to formalize what you invent so you are designing and implementing new theories and that's what I want to talk about this lecture I'm going to present stacks queues and trees as case studies I suppose you already know what these data structures are and how they're used that's the material of a data structures course not this course in this course we need to be able to design the axioms that define the structures we invent so I'll use stacks queues and trees as examples of theory design and implementation the first work on this was done about 1970 some of it at the University of Toronto where I was but then programs were not considered to be mathematical expressions but functions work so push and pop and all the other operations on these structures were not formalized as programs that change the state of memory instead they were formalized as functions that have a data structure as input parameter and another one as a result these are the data theories that we look at in this lecture now that we do consider programs to be mathematical expressions to we can formalize the operations as programs and that's the topic of the next lecture let's start with the stack like any Theory stack theory introduces some new notations and some axioms about these new notations stack is the bunch of all stacks whose items are of some type and I'll use the big X for the type empty is a particular stack bush is a function that takes a stack and an item and gives back another stack pop takes a stack and produces a stack top takes a stack and produces an item and here are some axioms that say formally most of what I just said informally okay here's the empty stack if we push something onto it we get another stack let's call it s1 and if we push something onto that we get another stack let's say s to push again and get s3 and again and get s4 actually this picture should be a tree because from each stack you can get many different stacks by pushing different things on to it if we push again how do we know we get a different stack from the ones we already have push just says we get a stock it doesn't say a different stack so as far as these axioms are concerned it could be empty of course that's not what we want so we need another axiom to prevent that it says the result of a push is never empty okay but maybe it's still a stack we had to prevent a loop like that we need another axiom it says the only way to pushers can result in the same stack is if they both start with the same stack and push the same thing onto it so that eliminates all possibility of looping but maybe there's a whole other line of stacks with no beginning and no end maybe there are infinitely many other lines of stacks the axioms allow it empty is a stack pushing always gives another stack pushing never gives empty and two pushes don't give the same stack to eliminate other lines we need another axiom do you know which one its induction that says there are no other stacks except those you get from construction so let's rewrite the first two axioms as a single axiom and then we can see what induction is it says of all a bunch of satisfying construction stack is the smallest an alternative way of writing it is in predicate form it says if you prove P about empty and if assuming P of s allows you to prove P of the result of pushing something onto s then you have proven P of all stocks so far we don't have any axioms that say a stack is a last in first out structure we need to say if you push something onto a stack and then pop the stack you get back exactly the stack you had before the push and if you push something onto a stack and then ask what is the top item it's exactly the item you just pushed that's all the Atmos tack axioms actually it's more than all list axioms because some of them are restatements of others now I want to talk about implementation the theory introduced these new names all we have to do to implement stacks is to define each of them using constructs or theories that are already implemented let's say lists and functions are already implemented and let's say the type of items is the integers then we can define stack as all lists of integers we can define empty as the empty list push can catenate the new item to the end of the list pop says if the stack is empty then return empty else return the list with its last item cut off top says if the stack is empty return 0 else return the last item so that's an implementation of stack theory using list theory and function theory well I claim it is but we don't accept claims in this course without proof how would you prove that this implementation is correct or perhaps I should say prove that this is an implementation we need to prove that the axioms of the theory are satisfied by the definitions of the implementation that means the axioms of the theory are implied by the definitions of the implementation the axioms are the specification of the datatype so we are proving that a specification is implied by or refined by or implemented by the implementation same as usual so we assume the definitions of the implementation and prove the axioms of the theory that sounds wrong because axioms don't need to be proven what we're really proving is that the implementation satisfies the axioms here's a proof of one of the axioms I won't go through it in detail but I just want to point out that we have to use the definitions of the implementation of course and we also have to use list theory and function theory because those are the theories we used in the implementation and notice right here it says if s cottony x equals the empty list if you can't eat an item onto a list it can't be empty so that's false and the if reduces to its else part that means that the zero in the venn part is never used it could have been anything and the proof would still work the other axioms are proven similarly so I won't show them right now we've implemented stacks so now we can declare variables of type stack and we can assign to those variables a gets empty and B gets push a 2 and so on there's a very important question that comes up whenever you introduce some new axioms the question is are the axioms consistent are they consistent with each other and are they consistent with the axioms we already had if the axioms are inconsistent the way to prove it is to prove false if the axioms are consistent there's only one way to prove it and that is to implement them we implemented stack theory and that proves it's consistent actually implementation proves that if the theories used in the implementation are consistent then the theory we implemented is consistent also so it's just relative consistency ultimately consistency comes from the fact that the most basic theories are implemented in hardware and we can run them logicians have another word for implementation they call it building a model they use the word model the opposite way from everyone else in the world according to physicists and architects and computer scientists and all the dictionaries a model is more abstract less detailed than the thing it models and the theory is a kind of model a mathematical model stack theory tells us what properties we want of a stack but it doesn't say how we get those properties an implementation is more detailed and says exactly how we get those properties only logicians use the word model to mean the more concrete more detailed implementation their usual language of implementation is set theory they say they are building a set to prove consistency again that's just relative consistency but they are very sure that the set theory is consistent because they've been staring at it for a century now there were some inconsistencies at first but they fixed them and they haven't noticed any new inconsistencies for a long time I prefer the assurance of a working computer over the assurance that logicians don't see any inconsistency the other question about axioms is completeness this question isn't nearly so important as consistency stock theory is incomplete and here are a couple of binary expressions that are neither theorems nor anti theorems according to the theory stock theory doesn't tell us what Popov empty and top of empty are our particular implementation makes these two binary expressions theorems and that's a good example of an implementation being more detailed than the theory it implements to prove that a theory is incomplete you have to implement it twice so that in one implementation some expression is a theorem and in the other it's an anti theorem then you know that in the theory it was neither we could make top of empty B 1 or 2 or anything and the proof of implementation would still work here's a point that has been made before but it's worth making a game a theory or specification acts as a firewall between the people who use the theory and the people who implement the theory this becomes more important as the software becomes larger the users can use only the properties provided by the theory and not any extra properties that the implementation may and the implementer has to provide all the properties of the theory that way the user doesn't have to know about the implementation and the implementer doesn't have to know about the uses of the theory so they are each free to make changes on their own side of the firewall without affecting anything on the other side here again on the axioms of data stack theory this axiom says that popping the stock results in a stock a consequence is that popping empty results in a stack so the implementation was obliged to provide an answer for Popov empty even though it didn't matter which stack was the result we already have an axiom that tells the result of popping a non empty stack which is a stack that's the result of a push I think it might be better not to have an axiom that says Popov empty is a stock so let's get rid of this axiom now an implementer can provide an error message as the result of Popov empty and that's much more useful similarly this axiom has the consequence the top of empty is a value of type X we already have an axiom that says what the top of a non-empty stack is and that's all we need for top let's get rid of this axiom so an implementer isn't obliged to give a value of type X for top of empty and instead can make an error message as the result we don't need this one because it's just a repetition of these axioms using predicates so I'll cross it off but maybe we don't need this induction axiom either we need it if we want to prove something about all stacks but maybe we just want to prove their program using a stack is correct and we don't need to prove anything about all stacks so cross it off now let's see are there any more axioms I can get rid of you know we don't really need empty the way stacks are used is to work on top of the stack you're given and when you're done leave the stack the way you found it if you really need empty you could make the equivalent effect by first pushing some special value onto the stack and then test for the test for empty is just a test to see if that special value is the current top of stack so we don't need these except that we do need to know that there's at least one stack so we can declare variables of type stack we don't even need this one that says pushing always makes different stacks all we really need is just these four axioms we need to be able to declare variables of type stack we always need to be able to push and keep pushing and when we pop we need to get back a stack we had before and we want top to be the last item pushed if you're a mathematician you want the strongest theory possible you want as many axioms as possible so you can prove as much as possible about stocks if you're a software engineer you want the fewest axioms possible you want just the axioms you need to prove your programs correct you want the weakest theory possible because that makes it easiest to implement now aren't it data queue theory we need an empty queue which is a queue we can get away without an empty stack but we have to have an empty queue and we need a join operation that takes a queue and an item and gives back a queue or maybe we want this slightly weaker axiom it's almost the same but it doesn't say anything about domains maybe we want to say that join doesn't produce empty and to joins produce the same queue if and only if they start with the same queue and join the same thing on or maybe not I'm not sure we need a leave operation that takes a queue and gives back a shorter queue but this one is too strong we can weaken it to not talk about domains but we need to weaken it more so it doesn't say anything about leave of empty maybe this is what we want we also need front to tell us what the front item is but again weaker and even weaker if we want to prove something about all Q's we need induction if not we should leave that one out what's left to say is that Q's our first-in first-out that takes four more axioms this one says if something joins an empty queue and then something leaves the queue the result is the empty queue this one says if something joins a non-empty queue and then something leaves the queue the result is the same as if something leaves first and then something joins in other words if the queue is non-empty joining and leaving commute this one says if something joins an empty queue then it's the front and the last one says if something joins a non-empty queue then the front doesn't change and that's it for queue theory the last theory in this lecture is tree theory and I'll start with a strong version we might start with an empty tree empty stupid name never mind and then a way to build bigger trees out of littler ones graft takes a tree which will be used as the left subtree and a value of type x which will be the new root and another tree which will be the right subtree and gives back a tree empty and graft are the constructors so we're building binary trees with information at the internal nodes and the leaves are all empty trees and here's induction the next one says that grafting does not produce an empty tree and the next one says that two grafts result in the same tree if and only if they start with the same trees and items these axioms should be looking pretty familiar to you now because they're so similar to the axioms for other data structures and finally we have three axioms that undo what graph does left takes a tree and returns its left subtree root takes a tree and returns its root value and right takes a tree and returns its right subtree and that's all the axioms for binary trees and it's more axioms than a software engineer needs or wants all that's needed is that there is at least one tree so we can declare variables of type tree and graft produces new trees from old ones and left root and right do their job that's enough so that if you put data in a tree you will find it when you come back there later that's all the user wants and more would just make the implementers job harder I would like to say a few words about tree implementation we have to define each of the new names using theories that are already implemented and I will assume once again that lists and functions are implemented amp tree is an empty list and graft makes a list with three items item 0 is the left sub-tree item 1 is the root and item 2 is the right subtree in this implementation here is an example tree we're dealing with lists that could be any length so how do we reserve memory for a tree value the standard technique for storing values that could be small or large or any size is to reserve some particular size space and whenever the value fits put it there and whenever it doesn't fit put an address or pointer there and put the value somewhere else for our implementation leave space that can hold an empty list in case it's the empty tree or two addresses and an integer in case it's a non empty tree I'm assuming the items are integers so here's what our example tree looks like it looks like a tree the pointers are there not because our implementation of tree says anything about pointers and certainly not because the axioms say anything about pointers but because it's a standard way to store things whose size is unpredictable something very strange has happened in the design of programming languages and the programming taught in introductory courses pointers are to data what go twos are to program pointers are data addresses and go throughs our code addresses we don't need go twos if the programming language allows recursive program definitions and implements them well which means it implements most recursive calls as go twos we don't need pointers if the programming language allows recursive data definition and implements them well which means as pointers and some programming languages do allow recursive data definitions and do implement them well but most of the popular imperative languages allow recursive program definition with varying qualities of implementation and disallow recursive data definition the introductory programming courses say don't use go to z' and they have a whole chapter on how to use pointers to me that's a strange and consistency in language design and programming practice here's another way we might implement trees just to show that there's more than one way to do it the empty tree is the number 0 and graft makes a record or structure or object with three fields or attributes in this implementation our example tree looks like this when formatted nicely like this it looks like a table of contents which is also a great example of a tree 
