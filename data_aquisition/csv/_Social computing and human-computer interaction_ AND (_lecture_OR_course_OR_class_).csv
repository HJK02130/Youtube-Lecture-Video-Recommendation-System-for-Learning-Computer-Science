id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
76hyrwCNiHg,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-17T16:27:09Z,"L17: Navigating information spaces. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/76hyrwCNiHg/hqdefault.jpg,Josh Bongard,PT49M34S,false,73,0,0,0,0,"okay let's let's get started um lectures 9 10 and 11 we're a little bit behind where I'd like to be at this point in the semester so I'm going to go relatively quickly through lecture 10 and hopefully we can move on to lecture 11 today I think a lot of the material in lecture 10 is somewhat obvious we've spent quite a bit of time now talking about the underlying structure of information spaces and thinking as HCI designers about how to make that structure intuitive to your users to navigate through it so we've kind of already talked about a lot of a material in 10 so I apologize if it seems to be going by pretty quickly but I want to try and get on to lecture 11 today any questions about deliverable 7 yes pointer center like why do you have a stuff that happens under their hand because I why why do we have the user Center their hand I have rebuilt the functions of it for them excellent right so good question so we already have it built in that if their hand isn't centered it should still be able to recognize the hand asking them to Center their hand in addition is not so much for the KN ends benefit you need to think from the users point of view why am I why are we asking the users to try and keep their hand more or less centered I guess I'll have to go out of friends exactly right so if they're if they're concentrating on trying to learn digits in their hand is kind of drifting towards the edge you know what happens when your hand is near the edge of the motions field of view right so it's just to try and make sure that we're getting is clean frames as possible from the user so get them in the habit of keeping their hand in more or less one place right so it's easy it's mainly just a safety precaution but as you'll see especially when you start to work with naive users who haven't worked of leap motion device you want to try and put stuff in there that allows leap motion to capture as clean a frame as possible right you're all experts now would leap Motion's in ensuring that it captures a clean frame from you but not from your use right you don't want to have to get into the nitty-gritty details of leap motion with your naive users so we're just going to try and train them to keep their hand near the near the center any other questions about lecture 7 yes I was it I was just having this um but strange issue where the UM like the image wouldn't blow it correctly then select remaining the program in in matplotlib yeah yes okay so mad plod lib was obviously not designed for real-time graphics and for displaying images during real-time and so on so it can be a little finicky so for those are you there trying to read in images and video to matplotlib as I mentioned last time it may or may not work you may want to start up a separate pygame process which draws the video and then closes when it's when it's done so I mentioned last time the using processes or using pipes in in Python so it might you might want to go and look at a tutorial there so you might have your infinite loop and when it's in a certain state which is to draw an image or video you start up or you spawn a separate process running on your machine and that process or that Python program is only job is to draw something to the screen an image or a video and when it finishes that process dies you can do that sort of thing with with pipes in essence what your main loop is doing is calling or opening a new program and that and then just starting it up and your program will continue that process will run in parallel and it'll exit when it's when it's done I think that's the easiest way to try and deal with these issues when they come up rather than trying to shoehorn a whole bunch of images and videos into matplotlib the other option as I mentioned is to just decamp from matplotlib all together and do everything in in PI can it's up to you any other questions about deliverable 7 ok so back to information spaces we're still on lecture 9 I promise you we will finish that one today okay so we were again we were talking about ontologies and you've created this new space that has these new conceptual objects in it there are particular relationships between these objects you're trying to help the user navigate through this space what are the kinds of things that we can we can do we ended last time with this issue about what sensor Modell what sensor modalities which of the five senses can be exploited to help the user underlie they understand the underlying space right so in your case you're doing some 3d graphics because obviously the user's hand exists in three-dimensional space if you remember all the way back to the beginning of the course we had this little cartoon of a human and an interactive system the human pushes against the interactive system they do something and they see something happen in response right they see or they hear a response what sorts of what sorts of auditory and visual stimuli can you provide so that those expectations are met for the underlying structure so the example I gave way back at the beginning of the course was you're playing a nintendo wii game where you're shooting arrows through a bow and the user can hear that when they're further from the screen and they let the arrow fly they hear the arrow leave the remote control and then they hear the arrow funk the target from the speakers in the screen right so in that hypothetical game they're exploiting they're exploiting auditory stimuli from both pieces of hardware to help you understand the structure of that ontology in that game which is that the distance and orientation from the target matters ok going to show you a video now which is an interesting use of again actually it is the wiimote in this case exploiting hardware and software and paying very careful attention to what people expect when they push against the physical world and observe the physical world pushing back they help support that sensor motor loop that we all have to municate the underlying structure of the information space you're going to see in this video okay hi my name is Johnny Lee and in this video I'm going to show you how to perform head tracking and create desktop virtual reality displays using the Nintendo Wii Remote now first what do I mean by desktop VR display well you think about most computer screens they're typically used to display a flat image a little bit like this picture in this picture frame even if the picture is of something 3d like a video game the picture is so flat so it isn't change depending on what angle you view the screen in but that's not be are displayed however is a little bit like taking a picture out of the picture frame and then just having the frame now the scene actually changes depending on what angle I view the screen at so this essentially becomes a portal or little window into another room now to do this the computer needs to know the location of your head relative to the screen and this is called head tracking now to perform head tracking we're going to be using the Wii below and in the sense of our when we're actually going to be using them backwards we're going to put the wii remote and next to the TE and actually move as a sensor bar instead the Wii Remote actually contains an infrared camera and in the sense of our is simply to sources of infrared light when the camera sees the two dots of light it's going to give an approximate location of my head or horizontally or vertically and in a distance from the screen okay the tricky part is and now we're going to find some way to mount the sensor bar onto our head one common trick is to get a baseball cap and then mount the hardware to the cup and this is definitely going to work but it's a little bit goofy so instead some hardware stores sell these safety glasses with LEDs built in on the side meant to be used as headlamps now if you replace the LEDs with infrared ones you essentially get your head mounted sensor bar and a nice sporty safety goggle phone charger once we created our head mounted sensor bar and then I'm connected my wii remote to my pc we're ready to do some head tracking behind me is a demo program of a 3d room with some targets floating in it because the effect only works with a person wearing the sensor bar I'm going to have to show you the effect through a moving camera now to do this I'm literally just going to hold the sensor bar at the base of the camera and then move it around it's just a quick note to power the sensor are I simply turn on my wii after i've connected why we wrote to my pc first i'm going to show you what it looks like without head tracking which is what displays will normally look like you can see that although it's a picture of a 3d room the image looks very two-dimensional and bound to the surface of the TV and now with head tracking turn on the TV actually looks like the entrance to a real room just like in real life by moving our head around we can look behind objects if you look really closely some cart it's actually appear to be floating on in front of the screen reaching into the real world if we get closer to the screen we get closer to your objects and we can even get behind the ones while loading in front of the screen as I pull the camera back keep an eye on the frontmost target head tracking provides the illusion that the target is actually floating directly above the laptop screen car in front of the TV now using this picture of a football stadium if you move right you can see more of the field if you move left you can see more of the stand and if you get closer to the screen you see more of everything just like a real window if I use my I our glasses and keep the sensor bar on the TV I can use a second wii remote to point and shoot like any wii game while also doing a head tracking so now ducking and shifting your body is actually meaningful to a game you can also see now how the perspective is incorrect if you are not the one wearing glasses so head tracking for year displays only going to work for one person at a time but for that one person the 3d experience is going to be far more realistic and immersive than anything else we're seeing homes today so if you're watching this and your nintendo wii game developer wanna see some games anyway as usual you can visit my website to download this software and find out more information about my other wiimote projects thanks for watching ok so at the end there Johnny Lee was asking that we wiimote developers to make some games they were listening to him they bought his technology from him and Johnny Lee doesn't have to work anymore so there you go pay attention in this class there's a there's money to be made ok so again pretty simple information space it's a three-dimensional space there are these things in this space called targets and these targets have a location in three-dimensional space right even without the very fashionable glasses on you got the idea right this is a three-dimensional space but the moment he turned the technology on hopefully you could see the difference right your brain was telling you that you were looking at a three-dimensional space even though you know you're not you're looking at the surface of his TV screen why did your brain keep insisting that you are look at a three-dimensional space so this is the ultimate in an intuitive interface the way it's set up is to support all of the expectations your brain has of moving in a three-dimensional space what are those expectations we receive distance perceived distance right but again I don't perceive distance my my eyes perceive photons that fall on the surface of my retina my brain in furs distance from photons how what's the general interaction that's going on with a three-dimensional space that allows my brain to infer distance occlusion right so that's important as I move some things that were occluded become less occluded and others become more occluded but you could do occlusion without the head tracking device right so you could watch the same video games you can watch Johnny play it without wearing the things and you would see targets being occluded by others and you would know that that means the occluded targets are further back in the three-dimensional space but your brain would still be telling you that you're looking at a two-dimensional surface so occlusion is important but it's not the key here to make your brain to trick your brain into thinking that you're looking at a three-dimensional space the head tracking right so what it was tracking the position of the head but not just tracking the absolute position of the head but the position three-dimensional position of the head relative to what to the screen right why so that's an important part of creating this feedback loop yep depth perception but again how does your brain perceive depth what is going on when it's looking at the screen when your head is moving and you're looking at the screen that casts the illusion of depth where there is actually no depth when you're farther away it looks smaller than you see less ok so when objects are further away they tend to be smaller which is true here right but again you can do that without the glasses right so if you're not wearing the head mounted device you see that some objects become occluded some are further away it's not enough your brain is still telling you you're looking at a two-dimensional surface what's the missing piece here as you move your head around in three-dimensional space and your eyes are open so you're seeing the objects around you what happens to those objects yes some of them become occluded objects that are further away tend to be smaller than objects that are closer what else happens oh you're all those days in places if you're all on that screen they all stay in the same place yes it changes their location okay we're getting getting closer now yeah you're getting cats right so they move right the objects in the game move how do they move so now we're not talking about occlusion or size we're talking about the movement of the targets right we're getting closer now absolute absolutely the moment I move or the movement or the moment my head moves my brain expects to see all of your faces move my brain knows a fair bit about the physical world the brain assumes you're not all trying to trick me and you're not all moving at once it's probably me that's moving so when I move to the right and I can feel based on my sensory systems that I'm moving to the right all of you seem to flow to the left if I were to walk towards you your faces would become larger some of you are further from me than others so what happens when I move to the right what happens to all of your heads from my field of view they all move to the left can we be a little bit more specific than that there it is that's the key right so the wiimote the head tracking detects how my head is moving relative to the screen and it computes specific movement trajectories for all the objects objects that are supposed to be further away move to the left less than objects that are closer to me right so if you took all those objects and move them to the left at the same amount the illusion would collapse and you'd be looking at a flat screen again right the fact that all the objects were moving at different velocities when Johnny moved his head instantly convinced your brain that it was looking at a three-dimensional space right if you draw a bunch of heads on a piece of paper and move your head relative to the piece of paper it looks like a piece of paper because the heads that are drawn on it aren't moving right now when you're in software those kinds of opportunities are freed up and it you can cast the illusion that you're looking at a three-dimensional scene without wearing those 3d glasses right ok so johnny was taken he was taking account of this very specific expectation that our brain has when we move or push against the world in this case figuratively and we observe how the world pushes back figuratively I move to the right all the heads move to the left in the heads the back move to the left less than the ones at the front if you can support that illusion your brain cannot come to any other conclusion except that it's looking at a field of objects embedded in the three-dimensional space okay last question about this video hopefully it looked three-dimensional to you did it you weren't wearing the ir emitters he just finished telling you that this only works for one person who's wearing the glasses you weren't wearing the glasses but it looks 3d how did that happen exactly so your brain was actually being tricked by two illusions there's the one we just talked about and then when a camera moved so when you're watching a video in first person meaning the cameras sort of from his point of view your brain is putting itself into Johnny shoes right so you feel as if you are the one who's moving around so in that case when he held the camera on top of the we bar and moved it around it cast that illusion that you were inside the camera right looking out of the camera if you go back and watch the video you actually watch him at the end where he's wearing the thing and he's jumping around and he's playing the game and it doesn't look three-dimensional might look a little bit three-dimensional but not as much as when he he was moving the camera because your brain is now watching Johnny you're not in Johnny's frame of you're not watching the scene from Johnny's frame of reference make sense ok so again when this is done right if you exploit software and hardware in the right way to support what your brain expects your brain is in a head is moving about in a three-dimensional space you can create the most intuitive user interfaces which not only do you not need a tutorial any written text whatsoever it happens immediately because your brain can't tell the difference between movement and three-dimensional space and movement through a virtual three-dimensional space this is one of the best examples in this whole course of bringing together people in technology thinking now not just about people's anatomical differences but thinking about the actual cognitive mechanisms going on in the head the expectations that brains make as they move about through the physical world okay alright so that's exploiting media or hardware and software we're talking about ontology is here we've talked about design already so helping people think about the structure of the space we want to try and give it some sort of coherent design right maybe click places that are close to one another in information space look similar have the same sort of color we might use standard templates something to sort of make whatever your the structure you're trying to display consistent right so in Gapminder bigger circles meant larger populations it would be very confusing if you went to a different visualization inside Gapminder and smaller circles meant more of something right so the there's a consistent visual metaphor in Gapminder across all the visualizations that Gapminder provides okay in most complicated information spaces you the user may not be the only person moving through that space there may be other users moving through that space and there may be other agents or what are now called BOTS I guess that are also moving through this space and we're going to define an agent or a bot if you like as a little piece of code that also has perceptual and motor capabilities so the bot is also able to see the information space and manipulate it or move through it it might see the space differently from how you see it and it may move through the space differently from how the user moves through it but it is like you able to perceive and move so how could we create bots that do some work in this in this information space to help make the structure a little bit more obvious one common technique that's used in a lot of bots is a trick that's borrowed from Mother Nature so a lot of social insects especially ants as they move around they drop a chemical signal in their environment called pheromones and they use that to build up an understanding of their space and communicate the structure of that space to their fellow ants so pheromones are actually quite complicated but the basic idea is very intuitive you're an aunt that lives in the colony here you live in a vast featureless desert so there's not a lot of landmarks it's kind of hard to find your way around you get up in the morning you leave the nest and you wander around at random and if you're lucky you find a food source and as you are moving around randomly you're leaving pheromone as you move so once you collect some food you turn around continue dropping pheromone and follow the pheromone gradient back to the nest that's all you do follow Pharaoh if there's no pheromone walk randomly if there is pheromone follow the pheromone gradient if you have a whole bunch of ants that are doing this they're all coming out of this out of this nest and there's a bunch of food sources around here eventually but a bunch of paths will start to appear between the various food sources and the nest but most importantly the shortest paths to the closest food sources will become much stronger the smell of pheromone will be stronger along those paths than it is on the pads too distant food sources how is that possible I told you what the ants do how is it that has two shorter to closer food sources become more reinforced than ones that are further away what's going on here yes I guess absolutely right so the more ants that are traveling a path and they're laying your pheromone as they go that path is going to get more pheromone but why are there more ants on the paths connecting to the closer food sources the ants aren't try it there they don't do any navigation they have no GPS to say okay I'm at this food source this one is closer to the nest so as I walk back i'm going to emit more pheromone than from a distant food source they're not as far as we know most ant species aren't that smart well there once they have the food turning around and a trail they'll take really lot quicker for that trail back of the colony so other ants think instead of one that they travels really long absolutely so imagine we have two aunts and at a point in time one ant is that food source a and the other aunt is that food source B and a is closer to the nest than be both ants do the same thing they start walking back towards the nest and as they go they drop pheromone the one that's at a is going to get back to the nest before the one that gets back to be it's going to turn around or other ants are going to leave the nest what are they going to do they're going to follow a there isn't even a path be yet and even if there is it's going to take longer for it to appear right so just because of the fact that those paths happen to be shorter they get reinforced more by ants traveling along them and like a magic trick without ants being able to compute distances collectively they are able to construct that shortest paths between between points right once the snow starts to fall here as well we're going to see an example of 12,000 humans doing pretty much exactly the same thing right but instead of pheromones its footfalls enough in the snow okay so with that very simple trick if you have an information space that is a network or a graph made up of nodes and edges and you want to try and paint or construct shortest paths you could go in and write a very complex computer science algorithm to compute them all calculate them and put them in a visualization for your users or write six lines of Python code for a bond that does pretty much what I just described for the ants let a thousand of those bots loose in your information space and they will do this British Telecom in the UK actually uses this technique for their for their telecommunications so you have a network structure and you've got packets traveling along it so you have a fixed network structure but it's dynamic in the sense that certain paths get clogged by packets so you want to continuously compute what is the shortest path between computer a and computer be this is a pretty simple way to do it British telecoms been using this technique for for quite a while to do so okay okay just one last example here I showed you this visualization last time from the online course that we run Ludo BOTS we try to take the same thing into account in a much simpler way so instead of having instead of having virtual bots that are doing this whenever a user poses a submission to any of the programming projects we just draw a dot to the screen right so you can sort of see at a glance where most students have gone or at least where most successful students have gone maybe lots of students have traveled this path but they either didn't post their submissions or they couldn't finish the submissions or what have you right so trying to exploit in this case the the overall community user community to help advertise the underlying structure of this information space what is the most well-traveled path what are the easier assignments than than others okay oops ok so we've talked about information spaces let's move on to navigation of information spaces again when you're thinking about how to help your users navigate through this face your users have decades of experience with navigating physical spaces what can we say what what expectations do they have of navigating an unfamiliar physical space that you can drawn to help them navigate an unfamiliar virtual space well usually when you're starting to understand a new space you you enter into a number of specific navigation activities the first one of usually which is what's here right object identification what it would exist at the local spot that I'm at and then exploration right I'm here where can I go from here there's two doors that I can see out of here do they give onto the same hallway or different hallways once I start to move through this space then I start to do much more much less of object identification and exploration and much more of wayfinding I'm in this room I want to get back to my office within 10 minutes after class I can think in my head about what the shortest path is to get there because I have quite a bit of experience now with the physical topology between here and my office what other navigation activities do you see users conducting when they're moving through an unfamiliar face so you're playing a new video game you're on a new website you're trying to figure out the structure you're trying to figure out where you are where you need to get to what else what else do you do to try and learn about that space and how you can move through it just try going through random you know blocks around links ok and then going back pretty ritzy know where they go and how they relate absolutely so maybe you just explore a little bit you want to see whether you can reverse direction can I go back to the start or do I have to click reset to teleport back to wherever I was before can I repeat that path so is the topology of the space changing over time if I know how to get from A to B today and I come back tomorrow can I follow the same path from A to B or has the underlying path changed for the internet the answer is yes the pads do change over time right the old paths may no longer exist and be replaced with a new path what other kinds of navigation activities might you enter into when learning about a new space virtual or physical to be and I turn around whatever that means in the space does the space support turn around and can I see where I came from right does that is that supported in this in this space ok can list a whole bunch of other things what about the size of the space so if I try if I started a and I explore and instead of going on a random walk I try and go in the same direction do I come to an edge is there an edge to this space do I wrap around and end up where I where I started how big is this space so search has kind of been perfected now within most spaces there's a search bar you could teleport to any arbitrary place in the space so maybe you don't need to know the structure but assuming you do usually maybe unbeknownst to you you're conducting a lot of these navigation strategies how many ways are there to get from A to B is there only one path or are there multiple paths what's the most frequent neighborhood right so what's the social side of this space am I the only one moving through this space has there been someone at point a where I am right now is there someone else here now has there been someone else here in the past is this place more frequently visited than then that one does anyone played any Wikipedia games Wikipedia is pretty big right you know the underlying structure it's a network there's nodes which correspond to pages and links that connect nodes together is their structure to the Wikipedia space anyone can create pages and connect them together right there's no top-down control saying the space has to be hierarchical organized in this way does it have structure it'sit's relational right so on any page there are links that relate to other pages but imagine that we were to try and draw that space right I think on the previous lecture I had a picture picture of the internet circuit 2004 right it's this huge network of nodes and edges looks like a tangle of spaghetti what about Wikipedia is it a random tangle of spaghetti if you play some of these Wikipedia games and here's just one page that i found that has a list of them there is definitely structure to the Wikipedia universe no they've all been connected to broader topics they connect a broader topic eventually going out bra them and you can go back down specific if you want to ideas absolutely right and imagine naturally built zone yes you take a random page and it says here is this topic and this topic is an instance of this more abstract concept which takes you up to a more abstract level assuming their levels right or this concept is embodied in these more specific examples which take you down into more detail right so that suggests there's kind of a direct at least one dimension of direction that you can travel in Wikipedia which is more abstract or more calm Crete right again no top-down control of that there's a specific Wikipedia game where you start on a random page and you start reading the text and you click on the very first link that you see that's in the main text not any headers or anything like that takes you to a new page you click on the first link in the main text of that page and so on and so forth what do you think happens if you do that does it take you back around right what is it what is the structure here do you just keep moving around and around the answer is no you end up in a very specific place philosophy at least it used to you can play this game and see but typically you will end up with philosophy the Wikipedia founders did not say make sure that your first link is more abstract than the main body of the text itself it's just something that emerged from a whole bunch of people building wikipedia right so if you start to play these games like the philosophy game you start to realize that there is underlying structure in this space which you can directly see right and even if you do look at a visualization wikipedia it doesn't look like it's hard to see the structure in a direct visualization okay I've got a worked example here that we're going to skip over you can sort of do this as homework if you like this is a summary of the necklet Netflix prize data so this was a bunch of years ago now where you can submit your machine learning algorithm and if it did better at predicting people's ratings of movies if you did significantly better than netflix current state of the art algorithm you want a million dollars obviously it's worth a lot more than a million to to Netflix so you can take this data set that I described here and sort of do some of this navigation through the space so the team that ended up winning the million dollars they did a lot of this analytics up front right what is kind of the structure if any that's hidden inside this data set once you start to understand a little bit of the structure it suggests how to apply machine learning algorithms to it okay it's there for you to play around with if you if you like but it's just optional ok so again some other specific navigation behaviors that you see people doing when they're moving through a new virtual space landmark knowledge right so I'm moving around and I see something that's familiar ok I may not know where I am but I know I've been over over there right what counts as a landmark in a virtual space any examples there's a magnifying glass you can search there ok so the magnifying glass basically says from here you can get to anywhere else if you know what to query what else your move you're moving through a physical space you're a little bit lost you see something off in the distance that's kind of familiar you say ok I know where I'm going if I head over there I'll be in more familiar territory what's a virtual analog of that behavior oh my god sorry oh my god the home icon ok that's right so there I can teleport back to somewhere I know that's that's familiar absolutely I'm playing the Wikipedia game and I end up on a page I've never been on before but I see that there's a link that's purple instead of blue right or a link that I recognize how if I click there I may not even have been on that page that I'm going to end up on but it's closer to where I've been before right all those sorts of things which are again meeting our expectation about moving through physical spaces same sort of thing root knowledge right I see a landmark or I see a familiar road or a path I know if I get on that path it will eventually take me back to somewhere familiar survey knowledge once you have enough understanding of the structure of a space you can sort of close your eyes imagine yourself in an arbitrary point in space and imagine another arbitrary point in that space and mentally simulate what's the best way to get from point A to point B most of you are juniors or seniors you can probably play this game with the UVM campus right you close your eyes you think about two points on campus you could probably mentally simulate how to get from point A to point B in the most efficient manner right ok how do you do that well again you form a mental model of that space which is what we're going to start talking about in a moment in lecture 11 what is a mental model well as the name implies it's a model it's not the real thing it's some approximation of the actual physical campus that you can think about and manipulate mentally if you know what it looks like from different vantage points that means not only do you have a mental model of the UVM campus but you can manipulate it you can rotate it and think about looking at the voting building from the north from the east from the west from the south right you may not apply all of these ideas to a virtual space but often that is what you're you're doing even though you don't realize that's what you're you're doing assuming that you want you're creating a new information space for your users like your ASL educational game what sorts of visualizations are you going to create that allow your users to keep from getting lost in your system if they're at a point in your game and they're not sure what they're supposed to be doing can they turn around and go back the way they came is there something like the home icon that they can click on or gesture to take themselves back to a familiar point in your your game are they able to see at a glance how far they've come and how far left they have to go to complete all the lessons in your your system what kinds of things are you going to show to your users to help them understand the underlying structure of your system and help them navigate through it ok I think we're going to skip over this as well moving through an information space versus a physical space how is it easier or more difficult I think I'll one for now signage we've kind of already talked about this signage and labels again what are the sorts of pointers you might put in the space to help users move around what might be a sign you're going to add to your leap motion interface to help people navigate and again no text so it can't be a sign saying this way for lesson three what sorts of visualizations might stand in for a sign in your leap motion game a green checkmark would usually means in a moment you're going to be moving forward to something else that's it makes sense right a green checkmark you've done well and it's implied you're now going to go on to something else what other signs might you employ in your system okay so a progress bar or wheel or something that's showing your progress so you get a feel that you're moving through this space right you can gauge how far you've you've come right maybe at some point in the game I get to choose which digits I want to sign or which sequence I want to practice sequences I want to do 313131 862 8628 62 I get to choose I can either go this way to practice individual digits or I can go this way to practice sequencing right how might you show your users that they have a choice at this point they've reached some level of competency we're now these options open up how might you advertise that in design and keep a good venue to work you can have their going you can still felt like an arrow but you have one that's just good I just one digit and one where the digits going back and forth absolutely right so I could see a short video or short animation saying I get it that way is singles and that way is is doubles I could have arrows right that's a pretty common thing to do it's a pretty common sign but think about this specific application right so if there's a saw if I see two arrows I still need to figure out how do I go down either path absolutely so maybe there's a sign turning to there's an arrow pointed the left over here and a sign pointing to the right over here so with my secondary hand I float it to the right or float it to the left and I'm indicating which direction I want to go right that would be a pretty easy thing to to do or instead of arrows there's an icon saying this or this right so I know that I need to point in which direction I want to go next right what sorts of signage that doesn't require writing text to the screen helps your users understand how to navigate through the suspense okay we already mentioned social navigation so having your users leave markers in the environment so that the next user who comes along gets an idea for places to go popular places difficult places easy places and so on and back to the pheromones which we already talked about okay sorry I raced through that a little bit we got two minutes left so I will just introduce the next theme of the course we've spent a lot of time talking about interfaces and people now are going to spoke focus on a particular aspect of people which is the brain and cognition right in neuroscience all of the evidence that seems to be coming out of neural science suggests that the brain is a prediction machine for those of you that know about deep learning that's what deep learning is doing it's predicting what's in an image when it sees a whole image or parts of an image why is the brain a prediction machine because in order to survive and prosper in the world I need to think about performing a candidate action may be a dangerous action and be able to predict what is the sensory result of that action if I were to reach with my hand towards my bag my brain is making a prediction that it's going I'm going to see my hand approach the bag if I mentally simulate getting up on the desk here and balancing on my left foot my brain is making a prediction about what might happen which is possibly something dangerous all right so our brains evolved to be prediction machines and your brain does not turn off its prediction abilities when it's looking at a computer screen when you click on an icon or you see an ASL game that has these two icons on the screen and you do this your brain has already made a prediction about what it expects to see or hear in the next tenth of a second one second two seconds so I want you to keep that in mind as we work our way through these four lectures on cognitive psychology at root your brain is a prediction machine and we are writing hardware and software to try and support those expectations and those predictions as we go through lecture 11 through 14 we're going to start with the most objective aspect of our psychology which is creating these mental models the mental model is the thing that allows your brain to make the prediction that's that's pretty objective and pretty straightforward as we go we're going to move through some other aspects of cognition that are increasing increasing Lee subjective meaning there isn't hard neural science fact to back these things up but it seems to be what we do when we get to 14 we're going to talk about effects which is the psychology fancy word for emotion right so can technology detect your emotion k and your technology project the illusion that it has a motion and why the heck would you ever want to create a device that does shot okay I think we'll leave things there for today you have a quiz tonight we'll start on lecture 11 on Wednesday "
vctsp_ASs2Y,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-31T16:40:03Z,"L23: Tangible User Interfaces. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/vctsp_ASs2Y/hqdefault.jpg,Josh Bongard,PT49M26S,false,88,1,0,0,0,okay let's let's get started happy Monday on happy Halloween let's talk a little bit about deliverables hopefully you're working your way through deliverable 9 as you probably noticed by now it's quite different from 1 through 8 in that most of the details are up to you we're looking to see that you're able to apply these three different kinds of scaffolds to make things easier for your user to get started and we also want to see evidence in the three videos that you submit that you're gradually removing the scaffold so your system is detecting that the user is getting better at whatever you're asking them to do and based on the fact that they're getting better things are getting harder for them right you're gradually making the task more difficult or providing less hints not unlike what we're doing in assignment I where we're giving you less hints on what to do okay no questions okay oh yes yes absolutely you could do video and it's as long as again the evidence is in there we can see that your program has detected that it's gotten better that you've gotten better and it does something to do that too to fix that right if you're splicing it and suddenly the program shows something different and we don't see any evidence that your program has actually detected that the user has gotten better that's not what we're looking for doesn't need to be very long you can you can cut as you see fit yes if you need to that's fine so in this case yes you can use English text you can assume the instructor in the ta speak English that's that's fine yes absolutely any other questions ok so we will go over the tenth and final deliverable on Wednesday you're probably wondering what will happen after the following Wednesday you will be more or less on your own to fill out the rest of your educational system and we'll about that next week okay so other housekeeping notes some of you have probably heard about the computer science fair if you haven't I highly recommend that you submit your project from this class so the CES fair there's a first of all just by registering you get a free t-shirt so that sounds good first place for advanced projects which this class would count as is 300 bucks and I can tell you that in the previous two CS fairs the ASM n ASL educational system always wins right you've got an unfair advantage over your student colleagues in other classes right what's not to love about the leap motion device ASL education and so on so there is a pretty good chance that you can pick up the three hundred dollar grand prize I highly recommend that you submit even if you don't work even if you don't win the fair is usually sponsored by a large number of companies they come they check out the projects some students in the past have been offered jobs on the spot so this is also you can also think of the CS Fair as a reverse job fair so you're the one advertising yourself and companies come and have a look and see what our students are doing so the least you can get out of this is a free t-shirt maybe three hundred dollars maybe a good job so I highly recommend that you that you submit a project and the link for the CS fair I just dropped it into today's lecture okay so back to lecture we are now about to enter we're going to finish lecture 14 today on effective computing so what is the role of effect or emotion in it in HCI I'm gonna finish that today which will finish our discussion or crash course in cognitive psychology and we will move on today into lecture 15 which is the first lecture in this new theme on looking outward so interactive technologies that are out here in the world with us very differently from traditional laptops and desktops which are hidden behind a mouse and a keyboard and maybe a webcam which is the only way they can interact with the physical world we're starting to see interactive technologies that can more directly sense our world and more directly interact or push back against it lecture 15 and 16 are pretty short I've cut out some of the slides from those or a little bit behind where I want to be at this point in the semester so we may actually finish 15 today and move on to 16 ok so back to lecture 14 we were talking about sort of three different goals of this subfield of HCI called affective computing the first one is trying to get computers to recognize emotion right it would be nice that a system knows when we're frustrated and slows down the interaction or notices when we're bored and speeds up the interaction we ended last time by looking at technologies which give the impression of having emotion so hal gives the impression of having an emotion which makes how entertaining why would we ever want to do that outside of a movie I showed you kismet last time in kismet was the ancestor of G Bo and we watched the g bo video last time some of you maybe it was a good one for Halloween G Bo is in some way kind of frightening but assuming you would did want to buy G Bo why would you want to introduce that into your home and why would you want a machine that exhibits or advertises emotions even though you know G Bo doesn't have any emotions relatability how so what do you mean by relatability absolutely you can sort of infer G Bo's intent right without g bo telling you as the head rotates you know that G Bo tends to be interested in faces because G Bo tends to look at or attend two phases so like human attention our attention tends to be drawn to faces so we share something in common with Chiba we're interested in faces were interested in people right it exacerbates our natural tendency to anthropomorphize right or project human-like qualities onto inanimate in this case inanimate machines okay so again once we do this why might we want to try and create this tighter bond between human and system because again once once we can do that perhaps the human the human user starts to build up a bit of an emotional bond and is willing to continue interacting with the technology when things get a little bit rough right so this might be something that's useful in educational software so you're trying to learn something the system realizes that you're struggling and it says I feel bad that you're having a hard time with this particular lesson why don't we slow things down a bit right if the system verbally or visually advertises that it knows you're frustrated it signals that it has an emotional response to your frustration which is it feels bad it went too quickly let me slow things down I apologize perhaps you might be willing to continue on with the educational interaction with the system compared to 1 which is just blindly showing you the next lesson in the lesson plan I put up duolingo x' owl here has anyone used duolingo before yeah so there's this little owl the mascot that's kind of happy when you do a good job and you know is there to kind of help you help you along right there's this sort of face of duolingo learning a foreign language can often be a frustrating thing you've got to repeat things over and over again it sort of helps to have someone there or something there that recognizes your progress or lack thereof ok another place that affective computing is starting to be used is for therapeutic software this is an old article down here from NPR but kind of an interesting one about using virtual reality to come to provide therapy for combat stress or PTSD this is an approach known as exposure therapy so in this particular system they take someone who is suffering from PTSD they put them inside a mock-up of a vehicle and the veteran just sits in the vehicle until the veteran is able to calm themselves so possibly just sitting in a vehicle might remind them of a bad experience they had in a theater of war they're usually instrumented with physiological sensors that detect skin conductance and heart rate if their heart rate remains elevated then perhaps their therapy session that day is to just sit in the in the generic vehicle and then go home again the next day or the next therapy session when they come in if their heart rate is they able to slow then there is a projection onto the front winds the front screen of a windshield of the vehicle that shows maybe a featureless desert perhaps heart rate is elevated again in the veteran and that's all that happens that day right this is this idea of exposure therapy also used for phobias where you are very gradually exposed to the stressors that trigger in this case PTSD or your spider phobia or what what have you that idea has been around for a long time but once you have technology you can do this in a much more incremental fashion right you can ramp down the detail of the experience if the subject becomes stressed and then gradually ramp up the detail as they're able to remain calm it's an effective system because there's usually a visualization somewhere that's showing that the system knows what the current stress level is of the veteran right so there isn't an acute owl or an anthropomorphic machine in this case it's just the signal that says we know that this might be a stressful situation for here's the evidence that we know what your current stress level is and things are not going to get any worse until your stress level drops and then we will expose you to slightly more detail of this this situation okay so the third goal of affective computing is if you can recognize a motion in your users then you probably want to alter adapt the interaction to maximize positive emotion or pleasure that the user actually enjoyed interacting with the system like we've seen in HCI before we can sort of break down this generic term pleasure or the user reporting that they enjoyed the interaction with the system and try and isolate certain aspects of the system that they they enjoyed and we start with the more obvious ones which is physio pleasure right so this is the idea of ergonomics so if there's a hardware component of the system that the hardware is somehow elegant it fit it fits snugly in the hand that there's something about the physical interaction that is that is pleasurable social pleasure is that the device is kind of just there as a facilitator to allow you to interact with others a lot of these satisfaction we get out of our everyday lives is interacting with with others psycho pleasure is this idea that in the short term there might be something that is somewhat unpleasurable like learning some new vocabulary for a foreign language but at the end looking back on the lesson for example if it was challenging but you learn something it's much more pleasurable than something that was much too difficult or something that was too easy right so we've talked about this a few times already if your interactive system is trying to teach the user something or is trying to help them through a difficult experience that it's doing it in this sort of incremental fashion right that it's adapting and finding the sweet spot right at that point where the user is just challenged enough and that's going to be a big one of the biggest challenges for you in developing your educational system how can you detect when your users right at the edge of capability right there's nothing worse than a new game or a new application that forces you through a tutorial that's at a certain pace right a tutorial that recognizes you know what you're doing should immediately disappear and let you move on right so something that gets you to not only gets you to the point where you're right at the level of confidence but gets you there as quickly as possible usually leads to a pleasurable interaction finally and again we've mentioned this before is this ideological pleasure right whatever the system is that you're interacting with it sort of corresponds to your core values perhaps open software is very important to you so using open software as a pleasurable experience rather than being forced into using paper use software perhaps the technology is green somehow that it somehow meets with your higher level philosophical goals in life okay thus ends our discussion of effective computing and again the very subjective side of HCI okay so we're going to move on now into looking outward and start to look at technologies we're going to start with traditional sort of input/output devices and we're gonna walk in this series of lectures from 15 through twenty four through series of technologies which are increasingly embedded in the physical world in a similar manner to the way that we are we're going to end as you can see here with robots which have a body they see the world not unlike how we see it they push against the world and they see how the world pushes back okay we're going to start however with traditional input/output devices and tangible computing we've spent a lot of time talking about interactive devices that broadcast to our visual system they draw something on the screen that we see or they broadcast our auditory system they play something on the speakers that we hear but what about devices that can also broadcast to our other sensory modality like touch right there aren't many of them yet there is force-feedback joysticks we have computer input devices that they get information from us through tactile interaction right typing on keys moving the mouse but what about the other way technologies that push back against us this is a screenshot from the x-men movie does everybody remember this with the moving pins they would come up it's kind of hard to see in this picture they come up out of this surface and create a three-dimensional structure you could of course put your hands on such a structure and actually feel that three-dimensional structure so there are some prototypes of these kinds of moving pin physical displays what particular demographic might benefit from a moving pins display absolutely right blind users you can't broadcast to vision you can broadcast to auditory but the the of all of our five or six senses as you as you count them touch is one of the most under as the underdog of all of our senses our skin does an amazing amount of information extraction from the physical world what other output devices exist out there or that you could imagine that impinge upon our skin things that you can actually feel that are tangible we've seen a couple already in this class does anybody remember the react table so this music generation system where you put these blocks on the table it's kind of a trivial example where obviously you feel the block that you're placing there aren't many out there but there there are many in development and I want to focus on one today and to rate the reading for today is a research paper that was written and reporting on this system called ultra haptics I'll play the video for you in a moment I assign the reading for today as the actual research paper it gets pretty technical in dents in the middle the reading for today is just I think the first few sections the first few sections of that research paper okay I'm going to show you the video in a moment this particular subfield of HCI is known as tangible user interfaces tangible meaning there's usually an object there or something that you can touch one of the things that's interesting as you'll see in the ultra habit haptic system is that you can touch something that isn't actually there and they manage this using a pretty sophisticated system that interacts with the physical world so as I play the video pay attention to the way they create the illusion that the user is actually touching something when that thing is not actually there okay interactive surfaces are now common in everyday life they allow users to walk up and use them with no instructions however current methods for providing tactile feedback require the user to cover up the visual content by touching the display for attached devices to their hands we present ultra haptics a system that provides midair haptic feedback and requires no contact with either tools attachments or the surface itself ultra haptics uses a phased array of ultrasound transducers to create a tactile focal points in midair the array is driven by a set of five driver boards I'm sorry would you guys mind just closing the two doors in the back there thank you Cheers likes to receive emission the array is driven by a stack of five driver boards which receive emission happens from a PC the users hands are tracked by a leap motion controller and the haptic feedback is projected through an acoustically transparent display directly onto the user's bare hands there are four steps to our unique focusing method first we define a large volume around the transducer array within which we will model the ultrasound field then we position positive control points where we want to form focal points these tell the system to generate the highest intensity ultrasound possible at these locations they are then surrounded with non control points these have the opposite effect telling the system to generate the lowest intensity ultrasound at these locations finally the phase delay and amplitude are calculated for each transducer in the array to create an acoustic field that matches the control point this simulation illustrates the acoustic field as we move up from the transducer array color represents phase and brightness represents intensity at a height of 20 centimeters a focal point is formed above the ultrasounds D focuses once more similarly this simulation shows by the discrete focal points being formed at the same time by varying the tactile properties of focal points such as the frequency they can be made to feel different from each other in this scenario a tactile information layer is created above the display by moving their hand over the map they use it and feel the population density of a city the frequency of a purple bond represents the density in that area here focal points are created above elements of a music player interface this allows a user to locate themselves on the interface without looking tapping the focal point above the button starts and stops the music the focal point above the volume slider can be proud at this point it pulses to inform the user that the system has recognized that grass the focal point can then be slid up and down to change the volume these are just a few of the applications that become possible with the ultra haptic system so unfortunately I can't show you or have you feel what this feels like but I think you get the idea right there reaching into this system and they actually feel the volume knob between their fingers and when they grasp it the knob clicks or gives a little bit of a pulse to show that you have successfully grabbed the knob right and you can see that visually the green circles turned red how does this work let's start with the transducer array so transduction is turning one physical phenomenon into another one in this case it's taking electricity and turning it into ultrasound so ultra sound these are pressure waves that are coming up from that array so each one of those little speakers if you like you can think of them as speakers are amid are emitting a pressure wave that's what's coming up from the bottom of the device where I think in the last example here the device was pointed downward what happens so what happens then at the point between the users fingers that's right the focal points right of high-density ultrasounds how do they create that focal point absolutely so what's happening is actually vibration you are feeling the ultrasound pressure waves or the vibration in the air it's a vibration but it feels like you're touching something I haven't tried it myself I don't know what it actually feels it probably feels like something that is vibrating slightly it doesn't resist your grasp obviously because there's no physical object there but it sort of tricks your brain not unlike some of the optical illusions we saw into believing that there is an object there how does the transducer array create those focal points they could create them at arbitrary points in the three-dimensional space they showed you that they could create multiple points they can also create negative focal points or null places where there is sort of a vacuum you don't feel anything so by putting those lack of vibrations around that point where you feel a vibration it it exaggerated the vibration at exactly that point how do they do that how do you go from this two-dimensional array that's emitting pressure waves to a focused point of vibration so these are literally waves right pressure waves what happens when waves collide remember there are multiple waves that are being sent up by different elements in this array what happens when two waves of water collide absolutely right so if you're on the beach and you're watching two breaks that are coming in at an angle where they touch if you look at the height of that new wave it's usually about twice the height of the two that touch right that's positive interference what else is possible they can also cancel out you can get negative interference which are those null points so that array is sending out waves so imagine you had pumps out in the ocean and you could send electricity those pumps that would create waves of different amplitude and different frequency and you were setting those pumps to create a collision at a particular point in the ocean where the wave would be much higher that's exactly what they're doing in their phased array here it's a phased array meaning they're pressure waves coming out at different phases I think they I think they set a fixed amplitude you'd have to go back and listen to the I'm pretty sure you could create arbitrary points with just changing the phase I don't know if they change frequency and amplitude as well okay there was also a good friend of ours in that video the leap motion device what was leap doing in this case tracking the fingers right so perhaps you could create a new conceptual object called a sticky button or I could actually press on something and when if I double-click on it it will follow my finger right it's stuck to the tip of my finger they didn't actually do that in the demo but you could do that how would you have to combine the leap motion device with the phased-array to create that illusion that there's something stuck to my finger what's the feedback loop that you would need here right so we start with the XYZ coordinate of my fingertip we leap gives us that right we know that we want to produce a focal point at that position we go back in and do some machine learning or we do some transformation that will output different phases to the different arrays or to the different ultrasonic emitters that will produce a focal point at exactly that point the moment I move my finger the position changes the desired position of the focal point changes which means the phases are also going to to change okay again as in everything in HCI whether this will become a killer app who knows but it's an interesting completely new kind of interface right you can manipulate objects and feel those objects in three-dimensional space even though the objects are not actually there okay so why do this right why tangible user interfaces to ease rather than gooeys right one of the reasons why and this is why skin is one of the underdogs of our sensor modalities is that the moment you physically manipulate an object you bring your other senses to bear on it right so when you grasp an object you usually also visually attend to it and observe what you're doing to that object in the ultra haptic system this doesn't quite work because there is no object there to look at but if you're looking at your fingers as you're trying to grasp an object in the ultra haptic system your visual sense actually will help you hear how remember all the objects in ultra haptics are invisible I'm moving around and oh I find that there's something here and I'm trying to grasp it and I'm looking at my fingers as I'm trying to grasp it what do I get out of my visual sense in this case in addition to what I'm getting out of my tactile sense okay so I can look at the visual projection down on the two-dimensional surface right so imagine we turn that off there is none of that and we just tell someone to put their hand into this box and feel around and report back to us what objects are in that space and tell us about those objects how does looking at your fingers help here absolutely right it's again almost like a visual illusion right you don't see any object there but you're doing this and you're actually realizing that you're manipulating a sphere or maybe you're manipulating a cube maybe you have unintentionally grabbed the cube and rotated the cube you're not seeing a cube well you're not seeing a sphere or a cube but your brain is filling that in right we just finished this series of lectures about how the mind tends to fill in details that aren't there right remember Gestalt perception you've manipulated small objects that are spheres or rectangles or cubes for twenty or thirty years and you've seen the object but you've also seen how your fingers react as you move them around the surface of the object right so when we're physically manipulating an object we're bringing our other our other senses vision or maybe auditory as well to bear to learn about this object and how we can interact with it is it sticky does it stick to my finger can I rotate it is it movable maybe I grab it and try and move it and it doesn't move right my fingers slip off the edge of the object right all of those sorts of intuitions you can build up by physically manipulating an object what else happens when you're physically manipulating an object how else do you bring your visual and/or auditory senses to bear on the problem so imagine I put this ultra haptic system in front of you and we turned off the 2d display and we played this game tell me about the objects inside this space what would you want to see or what would you want to hear that would tell you something about the objects in this space all right let me give you a few more ideas here to get you get you going maybe in ultra haptics you can't sense temperature but you might be able to sense texture and softness we hear how it moves across another surface you could also bring in an auditory track to heighten the illusion perhaps there are objects that are sitting on a surface and you're trying to pull them across that surface and when you do a sound is emitting this tells you something again about that that object right how does it resist when you grasp it right is it soft is it rigid can you twist it can you flatten it what else what else is it connected to can you pull it is it glued to the ground or not there's a huge amount of information we build up about physical objects in our environment as young children by not just looking at them by physically manipulating them in developmental psychology again the study of children one universal with very young children is any object they can grab they put it in their mouths first of all because it might be something they can eat but you can also bite on something feeling with your tongue you can learn a lot about an object by putting it in your mouth and that's what children tend to do as they're bringing the object towards their mouths they're also seeing it and hearing something if it makes a sound and they're starting to associate tactile information with visual and auditory information about that object when you physically manipulate an object rather than trying to rotate it with a mouse on a screen you bring your physical skills to bear right if you've ever worked with CAD software and you're trying to rotate objects to particular orientation of three-dimensional space it's incredibly difficult imagine you were to connect ultra haptics with a CAD system where whatever object you're creating you can reach into the space and just turn it yourself right might make design a lot a lot easier much easier at least for me to sketch with a pen than it is with a with a mouse you're trying to sign your name with the mouse it's kind of kind of difficult okay some reasons why we might want to try and create tangible user interfaces here's another example from the subfield of tangible computing this is a fun one called illuminating clay there is a physical object in this case and it's malleable it's clay and we'll watch the video and then sort of think about how what the applications for that such a system might might be illuminating allows users a simple taneous Li interact both physical and computational representations of the landscape here we see two collaborators preparing a landscape model to be analyzed with the system if it bit 900 Minolta laser scanner allows the topography of the model to be captured at a rate of one Earth's the Mitsubishi LCD projector cast the results of landscape analysis back onto the surfaces of the model the work table comprises of a screwed white surface in a rotating platform onto which a landscape model is placed we experimented with different types of landscape modeling materials plasticine with the ductile fibers chloral allows the model to maintain the required topography the area around the platform is illuminated to the library of analysis functions that can be selected at will the remaining edges of the work surface are used to project cross-sections of the model terrain the scan cast mode projects analysis functions onto the model these include variables such as slope variation and curvature shadowing and solar radiation water flow and land erosion the cut cast mode allows users to add surfaces for projection without affecting the simulation results CAD cast allows the user to construct three-dimensional topographical models this system allows any object from the user's work environment to be used as an input to the system without the need for tacking and tethering or demarcation the interface provides a simple means for three-dimensional display where the tangible immediacy of physical models can be combined with the dynamic capabilities of confrontational simulation okay again here's a system of still waiting it's killer app one of the things one of the things I like about this is also is one of the first examples of augmented reality right we have the physical object and then that physical object is augmented with the real-time digital projection onto it you could imagine using this for civil engineering projects or other environmental engineering projects what else might be the killer app here so one of the things that illuminating clay is trying to do is help you visualize some aspect of the physical object which is change in slope better than you would if you just look directly at the object right you can kind of see the slope or the structure the topography of a lump of clay but if you project on top of it you get a much more detailed sense of the structure of that that system stress analysis right so maybe I'm using the clay or I'm using something else to build a model bridge and as I'm building the bridge as I'm building it in real time I can see where the stress points are right if I don't add some additional structural support here this is where the bridge is going to break you can do now with engineering software but usually it presupposes you make the structure and then you submit it to finite element analysis and it gives you back a report of where the stresses are imagined you could see that sort of information in real time as you're creating the mock-up of a single structure right that might be an interesting application of a future version of this kind of system okay last one we're going to look at today this is the interactive workbench this is a little bit like illuminating clay and a little bit like and a little bit like the ultra haptic system but unlike those two systems this system is really going to push back so the ultra haptic system kind of pushes back in a minor sense it's able to emit a vibration that pushes slightly against the skin can we do a little bit better than than that the actuated workbench consists of an 8 by 8 array of electromagnets it uses magnetic attraction and repulsion to move magnetic objects to dimensionally on a flat surface here we can see a magnet moving in stepwise Manhattan motion here we see an object moving in a smooth circular pattern although the array of electromagnets is fixed the system can create smooth motion by varying the strength of the electromagnetic fields in addition to magnets the electromagnetic array can move any small ferromagnetic object such as a paper clip here the user controls the pucks motion with a trackball smaller objects can be moved much faster though their motion is not always so smooth magnets of different sizes and shapes behave differently in the system's magnetic fields this stack of small magnets comes around whimsically we use computer vision as a preliminary object tracking technology here the user records a path by moving the puck on the surface and the system then replays that path through magnetic actuation the blue projection around the puck is a graphical visualization of the strengths of each adjacent magnetic field here is an example application intended to teach users about physics the red projected area on the surface represents the zone of attractive force while the blue area represents repulsive force these are kin field these forces by lightly holding the puck in different areas of the board when the user releases the puck it flies to the red zone of the board to which it is attracted magnetic drawing toys are effective for visualizing the actuated workbenches magnetic fields a Magna doodle allows us to see the fields used to trace the smooth circular path shown at the beginning of this video the dapper dan toy lets us see the magnetic activity in the movements of iron filings on the surface the actuated workbench can be used to control the planchette in a Ouija board game like other robust systems such as the diamond touch presented by Mitsubishi Electric research labs in Wis 2001 the actuated workbench works even when set on fire you can see what HDI grad students get up to and the research labs when the professors go home at night okay aside from the fact that it doesn't catch on fire or are only part of it catches on fire what might be the benefit of a system like this aside from the gee whiz factor yes that's right you can amaze your friends that's true other than that that's a good idea right so you can manipulate objects so you could make this a more tangible device and then tell it to move everything out of the way it could automatically clean up the workspace for you it's an interesting idea right so anything that requires fine manual dexterity and again that might be something you want to do at a distance bomb defusing is also something that takes years as you can imagine if careful training right and often that training is watching an expert do it and you're watching from a distance and then the instructor says now you try right how might this changed that kind of interaction aside from being able to manually do something at a distance you could watch the movement so an expert could record something on the table and then the novice could play it back and watch but even better than watching it you can feel it right it makes manual instruction so teaching how to people to do something with their hands it makes it tangible right so what you know learning about elect electromagnetic forces and all that it's kind of intangible this abstract thing with fields and forces and so on for some people it's too abstract but if you're holding this device and seeing magnetic fields and feeling the way in which the object is being pulled under your fingers makes it tangible right so imagine any task like that where you're learning how to draw or perform some fine motor task perhaps you're trying to learn how to draw Japanese kanji characters it's very difficult to do when you watch someone do it from a distance and then you reach in and try and reproduce what they do this can under their certain circumstances make it feel like your hand is on the hand of the instructor and instructors moving your hand through the process right it's a very new and exotic form of scaffolding right perhaps as the instructor or is the system feels that you're becoming more proficient at this manual task scaffold starts to be removed what would it mean to remove scaffolding in this system in this particular application of the novice holding the instructors and vice versa you have to reduce it yourself right so you turn off the instructors forces on the object and now you're just moving the objects but remember that with scaffolding we always try not to try and remove it as gradually as possible so maybe right so you stop moving and the forces that are coming from the instructor are increased right so the instructor the amount of control that the two people have over the direction of travel of the objects is a balance it's the sum of those forces the forces I'm exerting and the forces that the magnets which recorded the instructors movements are applying it's a sum of those two forces as I signal loss of competence or I don't know what to do next the instructors hand appears and helps and as I exhibit the fact that I'm getting better at the task the force is applied by the magnets gradually become less and less and if you do it gradually enough I start to do it and I don't even realize that the training wheels have been taken off okay we will stop there you have a quiz due tonight and we will move on to a Bic witnessed computing on Wednesday thanks very much 
QWhLUtwFbQw,28,"http://www.meclab.org

Playlist: https://www.youtube.com/watch?v=91BoRZllDb4&list=PLAuiGdPEdw0hhJ_XZUJrR9OeJoUgB1AiB",2019-10-24T15:08:47Z,"Human Computer Interaction. Lecture 15. Oct 24, 2019.",https://i.ytimg.com/vi/QWhLUtwFbQw/hqdefault.jpg,Josh Bongard,PT1H12M49S,false,48,0,0,0,0,okay good morning everybody let's get started how's everything going with to the verbal nine any questions no installation issues okay all right so let's carry on discussing looking outward talking about technology that is interacting with the physical world and in some cases it's human we're in real-time we talked about ubiquitous computing and thinking about how to create it in such a way that it disappears into the background if we continue on instrumenting our society and it becomes increasingly distracting that's probably not the way to go how do we create technology that disappears into the background but despite the fact that it's ubiquitous and invisible it is still supporting us in the things that we want to do and to sort of concretize that abstract idea in lecture 17 18 and 19 we're looking at ubiquitous technology which is scaffolding and supporting three different scientific projects and these scientific projects are attempting to ask a scientific question that would be difficult or impossible to answer without ubiquitous technology we'll finish our lecture today I'm social network inference of course there are a lot of social networks out on the internet and we can infer social dynamics among Twitter or Instagram by looking at messages going back and forth but of course there are tons of subtle social cues or subtle dynamics that are going on among a group that meets regularly face-to-face builds up friendships and competitions and teams and so on very difficult to study groups social groups especially groups of humans in their natural habitat and observe or infer from that social network what aspects of that social network influence how people behave in that social network well finish that lecture shortly we'll move on to activity tagging so most of us carry around ubiquitous technology with us for most of our waking lives these devices have sensors and they can infer something about the activities we're carrying out and given those inferences can they tell us something about our physical social or psychological well-being we may finish that today we may move on to the interesting human speech own project which is a yet a third project looking at how children acquire language okay so back to social network inference just as a reminder we were looking at this idea last time of instrumenting these twenty four graduate students that it seems take classes in common they tend to be in the same place at the same time on a regular basis throughout a six month period we're going to use this microphone that is sitting near to their mouth and near to their ear to infer something about what conversations they're involved in when are they speaking and when are they hearing speech from someone else who's a member of this group we ended last time by looking at this idea of mutual information so using mutual information applied to just the volume levels of the microphone to infer who's speaking to whom and when and how often use that to distill all that volume speaker volume information or microphone volume information down into a social network where the nodes are the graduate students and the weighted edges are the number of inferred conversations that those graduate students had and then given that social network can we then ask questions about where they sit in that social network who they're speaking to and whether and how that influences the way that they speak to other members of the group there are important privacy issues with this particular experiment so we're throwing away what the grad students said and we're just keeping for we're just inferring for features of prosody or way in which they speak how fast how high or low how long do they hold the floor before seating it to someone else and how many times do they alternate speakers in a given conversation okay we ended last time by looking at some correlations or more particularly anti correlations we started by computing D I J which is the difference if any in the way that I speaks to J compared to the way that I speaks to everyone except J if D is zero then the way that I speaks to J is exactly equal to the average way that I speaks to everyone else and increasingly positive values of D indicate that I is increasingly changing the rate or pitch or turn length that they engage in with J compared to the mean value of how they do so with everyone other than J right that's D then we can compare D against other things like how close are I and J and by close we mean how often did they engage in conversations and as we saw last time there's an anti correlation between D and C when D goes down and approaches zero they're speaking less then CI goes ci goes up which is the fraction of time that they speak with that person so people that they speak with often that's closer to the mean of how they speak in general to everyone else but when they're speaking to someone they speak to rarely so C is low then D is high they change their rate their pitch their turn length and our probability here says probably not turn frequency so far so good so using this technology the raw measurements from this ubiquitous technology and then doing a little bit of math here we have Marshall or the investigators have marshaled some evidence for a hypothesis that was difficult or impossible to measure are among human groups in the wild of course we can light them into a laboratory and ask them to do some collaborative tasks but that's somewhat artificial right how do humans how two grad students tend to behave when they're talking to their friends throughout a semester okay let's take the same data and we're gonna recompute it in a different way to tackle a different hypothesis if you just pay attention to the top of the slide for a moment do people change their way of speaking when they speak to quote-unquote well-connected people so now not their friends but people that have a prominent position in the social network so we're gonna be the same we're gonna ask does I change the way they speak when they speak to Jay compared to everyone else we're going to compare D against C sub J now which is the centrality of grad student J in this group of twenty four graduate students so what is centrality mean here but what is it representing going back to the actual behavior of the grad students we have twenty four of them and this person has a high C what does it mean he or she is connected to a lot of the other graduate students right according to the social network can we be more specific than that not just connected with lots of edges to others remember that we have weighted edges in this social network so what is centrality mean here lots of exchanges right so they engage this particularly central J engages in lots of conversations with lots of the other graduate students right okay that's what C represents in this case we find that there's a where the investigators found there's a positive correlation for some of these four features of prosody high C means high D low C means low D again if we pay attention to the p-values down here just as reminder the p-values are probabilities that that detected correlation is actually not there and at least in the case of rate of speech pitch and pitch P is exceedingly low which means this positive correlation is probably true we can probably trust the fact that when C goes up D goes up and vice versa for rate and pitch but not for turn length and turn frequency for most mathematicians the p-value is sufficiently high here that I wouldn't trust this positive correlation okay what's the interpretation of that what does it mean that C and D are positively correlated for rate and pitch what does that tell you about the behavior of this group of graduate students exactly central somehow according to our waited at social network here so the way that they speak the rate and pitch change more for grad student i when they are speaking to someone that has high centrality what would that mean are you surprised by that result I would assume that they would well the pitch being higher makes sense that that's okay but their journaling it seems like you would possibly so the turn length here I don't know if it explains it the paper I'm pretty sure it's turn length of about I and J so it could be that I is speaking for a short period of time and then J the central person is is hogging the floor or hard to say in this case the fact that pitch changes doesn't necessarily mean that pitch is higher or pitch is lower it just means that pitch of I when they're speaking to J is more different compared to their average pitch when they talk to the other 22 graduate students so then we can start to again try and explain this hypothesis and ask additional hypothesis questions like is it the fact that is somehow nervous or is aware that they're speaking to someone that's central they're using more formal language or they're deferring somehow to J this work doesn't have anything to say about that because again we're throwing away everything here except just features of speech but it is no longer an opinion or an idea we have concrete data supporting the fact that humans are changing their behavior it also tells us something else indirectly that I must somehow know that J is central otherwise I would not be changing their behavior when they speak to J compared to anyone else right also kind of kind of interesting okay so this isn't a class on social science we're not going to dig too much more into this experiment but again it's an interesting application of HCI we're thinking carefully about the technology and we're thinking about physical context if we want to infer a social network from conversations we want to have a speaker near the speaker the speaker's mouth and their ear and by you by doing it that in that way we can relatively eat it we can make it easier on a machine learning algorithm to infer that I is speaking or I is being spoken to by someone else in the group if we have I and J and they're both wearing the microphone and eyes volume is medium whenever J's volume is high you could put that two pieces of information together and the machine learning algorithm predicts J is speaking to I or vice versa or I is hearing speech but it's from someone outside the social group right so lots of interesting information by thinking carefully about the physical context of face-to-face verbal communication thinking carefully also about social context here who is speaking with whom are they in the same classroom are they speaking mostly in indoors and outdoors lots of interesting ideas here about how to apply ubiquitous computing okay so let's move on now and talk about let's talk about activity tagging like we've seen many times in HCI by now this is going to be an interesting mixture of the subjective and the objective activity tagging started about ten years ago and I think this experiment that were to look at today is from 2011 so eight years ago activity tagging and inference has come a long way since then I don't see any fitbit's in the room but I'm sure some of you were wearing some pretty ubiquitous technology now kind of kind of obvious at the time it was not so obvious how do we go about instrumenting human where's will they accept wearing something like a Fitbit assuming that they will what can we measure about their behavior what would they allow to be measured about their behavior and given those objective measurements like speaker volume or other things that are easy to measure how do we go about pushing those objective measurements through a machine learning algorithm to try and predict something increasingly subjective like well-being all right difficult to say what we're going to look at is this particular experiment today where they're going to take some of these correlates of well-being as far as we know it's very difficult to measure well-being directly we're gonna try and measure correlates of well-being and predict well-being as best we can before we do however let's think about physical and mental well-being you can unpack that into some obvious aspects of behavior that are probably predictive of well-being are you getting too little or too much sleep are you getting too little exercise about intellectual stimulation social interaction diet this one is still difficult to do you can try and convince someone to take a photo of every meal they take but it's not a very acceptable approach to measuring diets because again it's kind of onerous on the on the user and stress as well what are some other correlates of physical and mental well-being that we might try and create instrumentation to measure the correlates of it so physical activity is kind of done right as long as you're wearing a Fitbit that one's pretty straightforward what are there some what are some of these more difficult correlates of well-being what are some of these correlates of well-being that a little bit more difficult to measure directly right we could add screen time to this which is definitely a correlate of well-being for many of us and relatively easy to measure right it's already by definition on the phone that one's an easy one to capture it's a little bit more difficult intellectual stimulation right I'm hoping you're all intellectually stimulated this morning but hard to say and even if you were willing to be instrumented by myself I'm not sure what I would measure to know whether you're being intellectually stimulated or not tricky the quiz the quiz is my attempt to try and measure whether you were intellectually stimulate or not good point how might you go about measuring stress that's definitely a correlate of well-being yep absolutely right so skin conductance is a good Cora is a good proxy for anxiety and stress heart rate again assuming you're wearing a Fitbit or something similar you might be able to capture capture that there are some attempts to create small sensors that you place on the inside of your mouth that would actually record what you're eating so you don't need to take a picture of every meal as you can imagine the bar for acceptability of that kind of technology is pretty high but for certain demographics that might be accept acceptable if diet is the particular challenge they face for well-being okay so still an open problem lots of interesting potential technology we could design and deploy in this case so as I mentioned we're gonna look at what is now a bit of an older paper in this field but one of the first attempts to try and measure some of these correlates of physical and well-being and use it to actually predict well-being so they're gonna start we're gonna see in a moment they're gonna measure directly measure 7 over the correlates of well-being they're gonna try and measure them as continuously and automatically as possible right again we want this to try and disappear into the background we don't want to have the user's phone pinging them every five minutes and asking them what they're doing how they're feeling and so on we'd like this to be as unobtrusive as possible once these raw measurements are taken we have to try and create some sort of model that takes as input raw measurements and produces a number on the output which is a rough measure of how well you're doing so if for example we are measuring or measuring your amount of sleep over three different nights and you're not getting enough sleep this number should be low if you're spending all day in bed for three days in a row that number should also be low if you're getting between seven and eight hours of sleep this number should be high that's what we're trying to to aim for here what form should that model take and what machine learning algorithm might we use to try and train it finally assuming that we can distill down all these raw measurements into a rough estimate of physical and psychological well-being might be nice to show that to the user so they can try and modify their behavior accordingly so these are the three tasks we're gonna try and tackle in this case so I assigned the the actual research paper as today's reading they got a nice pretty picture here of this basic idea just to reinforce this and try and measure things directly model use those raw measurements to model well-being and then inform them and hope to promote well-being in this way ok so let's have a look at some of this raw data so again looks a little dated maybe at this time what are the sensors that are in play in this example here your the raw measurements what are collecting them GPS GPS so we've gotta exactly so we have a map down here so we're tracking their movement maybe their motion might tell us something about physical activity exactly a microphone so the authors of this paper the same authors of the paper we just looked at so again they're using a microphone and they're gonna try and use that microphone to infer something about social activity is the user hearing silence noise are they speaking are they being spoken to or is it something else they have an accelerometer so they're measuring something about the movement of the user itself but not just motion they're trying to predict from particular motion patterns whether someone is stationary walking and running so we're gonna come at this from lots of different directions using different sensors and different raw measurements okay let's tackle sleep first most of our phones today there's a sensor that knows whether the battery is being charged or not I've got a clock accelerometer digital compass GPS microphone camera if you're gonna write a sleep app to try and predict the number of hours of sleep per day that the user is getting how would you combine these sensors together to do so you would assume that the user uses their felling down stop throughout the day dog or take I understand by the super bear okay so whenever they're awake they're using it and if they're not using it they're asleep for some of us that may be true yes assuming some of us can exercise self-control throughout the day and I'm not using the phone like this seventy five minute period hopefully none of you are asleep so okay okay so if you have a desk in your bedroom especially if you're a student maybe you spend a lot of time there studying so again this might be helpful you can sort of see where this is going right there's gonna be no one combination that's perfect so we want to try and combine together as many of these as possible so acceleration moving around as the phone is moving I'm probably awake unless I'm asleep on a on a train while it's moving or a bus again maybe GPS is useful they're in their bedroom what other information we bring together to increase our prediction accuracy about sleep okay we could use the microphone if it's quiet that's probably helpful BPM right so heart rate again assuming someone is willing to wear something while they're sleeping so again we're getting into physical context some people would say yes some people would not find that acceptable we might have to find different subsets of sensors for different users what else actually on that means suggested me to put the phone beside me when I was asleep and you can measure my okay if your snore that makes it easier then maybe we'll use the microphone as well depends right so any one of these measurements is not going to be very accurate but combining them together might be might be the case I don't know about you but I usually plug my phone in at night when I go to sleep so if it's plugged in that's a good sign if it's quiet that's a good sign if I'm in my bedroom that's a good sign even for something like sleep it's a little bit tricky okay so you can read the paper they used a similar combination to what we just described and according to the authors the results show that a simple model and we'll look at the model in a moment that combines phone recharging lack of movement and lack of ambient noise is sufficient to predict the amount of outer slept plus or minus ninety minutes not great I know fit bits are much better at this now they detect sleep and also whether you're moving during sleep so you get a much more accurate reading but this was sort of state-of-the-art eight years ago okay what's the model so again the model sort of visualized on the right here not that surprisingly not that surprising so too much sleep or if you're spending too much time in bed that's not really a great thing not enough sleep obviously also not a good thing so they're defining here something called HR some idea which according to the researchers and the sleep immature at that time seven hours on average for most people was a good amount of sleep obviously there's a lot of standard deviation some people need less some people need more they also derive upper and lower limits again from the sleep literature if you're getting if you're getting eight or nine or ten hours that's not so great obviously there's a lower limit as well they're going to take each are some actual the actual amount of sleep over a 24-hour period here and we're going to push this through a simple mathematical model which has an exponent here and the exponent leads to exponential increase and we won't go through the math here but it basically produces this curve here so if we plug in H R sub actual and actually actual is probably not a good term because we don't know the actual amount of sleep the user is getting actual should actually say predicted so we're getting actual from the raw measurements on the phone it's predicting how many hours of sleep you got last night pushing that through this equation which gives you back the number sleep sub day which is how your sleep is positively or negatively contributing to your well-being so if you're getting very little sleep sleep sub day is zero if you're spending 9 10 12 14 hours in bed you're getting a value of sleep sub day also of zero so we're trying to maximize this value which is not the amount of sleep it's the amount that sleep is contributing to your positive physical well-being okay again you can see how we're moving from the objective to this subjective here okay let's sleep let's tackle physical activity now and again there's lots of technologies out here that are doing this very well assuming we have an accelerometer or some sort of motion data we can pretty accurately predict whether someone is in motion or they're in a vehicle which is in motion so again already tricky can we drill down even further and use motion data to predict which physical activity the user is engaged in the authors of this paper when they were writing this paper about nine years ago I visited them at Dartmouth and the investigator brought up an app on his phone and he could see exactly where his graduate students were on campus this one was biking towards the lab this man was walking across campus I'm assuming the graduate students knew that he had this app but who knows who knows okay so let's play the same game we want to infer not just that the user is in motion but can we infer which activity they're engaged in given phone data possible II before you most of us don't swim with our phones are they sleeping or swimming whatever they're doing they're not using the phone at the moment so obviously some of these activities are going to be more difficult than others it depends again on the physical context of the particular activity but as you mentioned we can use change in the acceleration data we can look for regular patterns of motion that will allow us to distinguish between walking and running moving oneself or being moved by another vehicle okay so let's focus in on walking and running for a moment this is the one we're gonna try and capture I'm gonna try and use accelerometer data here we're gonna start by measuring a subjects actual exertion so while a given subject is actually walking or running how much energy are they actually using we're gonna measure that first and then we're eventually gonna train a machine learning algorithm to predict that exertion we really don't want to know whether someone is walking or running assuming that this is about physical activity we're trying to predict how many calories they're burning we're gonna get actual exertion then we're gonna get raw acceleration data we're going to again create a model that's going to take as input this acceleration data and produce as output a prediction about calories burned and we're then going to try and use machine learning as usual to try and min nice prediction error of the model so the model outputs exertion predicted and we have exertion actual and we're gonna train the model to minimize that so we can go eventually assuming we've trained this model and assuming that the user is wearing their phone while they're walking or running the phone can make a relatively accurate prediction about how many calories are being burned at a given point in time okay how do we measure actual exertion luckily we don't all have to do this we're good in this case the author's relied on vo2 max or change in volume of oxygen while while performing a physical activity you can see this in action here so when when we're at rest or especially when we're exerting physical effort we're obviously taking in oxygen burning the oxygen and releasing carbon dioxide so assuming that you can measure that intake and outtake you can look at the difference of oxygen coming in and oxygen going out the more you're exercising the more you're exerting the more oxygen you're burning for a same time interval so that's what they actually measured and now we're going to try and predict this how might we go about predicting this first thing that might be useful for the model is resting metabolic rate if you're a highly trained athlete for the same activity you're burning less oxygen than someone who is not used to that particular activity we're then also going to use acceleration data we're going to try and break this down into the horizontal and vertical component of acceleration by doing so we might be able to infer the activity a little bit better let's start with horizontal motion okay so the phone is on the accelerometer is detecting and the accelerate accelerometer is detecting that there is a regular change in acceleration which I'm showing here we can think of this as leg movement leg movement if you want assuming somebody's wearing this in their pocket this might actually be on their thigh and it might actually get a good measurement of leg movement it's not measuring sleep it's gonna try and predict sleep from it so you could be more accurate if they were or like if they want it to at what point do you as maybe see a designer say okay look swimming for example again we could do it was gonna be so hard they can either accept this or they're just not going to get it is there a cut-off point that's pretty consistent that it's like well ex-work means too bad exactly so we could say to our users if you don't accept this technology then there's no way we can infer you're swimming for example and that's perfectly fine another company comes along and says we can do it we can infer from a distance we've created a phone app that uses microphone data does some very sophisticated machine learning and can detect the sound of somebody swimming as long as the phone is within 50 feet of the swimming pool right so of course you can decide not to try and predict it the goal and your BIC witness technology is it should disappear into the background I go for a swim and I leave my phone by the side of the pool is there a way if we think carefully about the physical and cultural and social context of going for a swim where you might be able to indirectly in for that activity might be difficult but if you can do it then you've got an advantage over someone who says you either accept a wearable or you don't once you start talking about sound or anything along those lines aren't you means that we have privacy absolutely so there's another acceptability issue which is you need to turn on your microphone and leave it on maybe that is not acceptable absolutely a lot of these practical considerations okay so back to our exercising user here they've got their accelerometer on and the accelerometer is detecting the fact that there is a regular motion possibly leg motion if the phone is actually near on their legs that's fine pretty straightforward you can then assuming that there is regular motion you can apply some additional mathematical methods from signal processing to tell you something about the frequency and other features of this repeating pattern not surprisingly the faster your legs are swinging back and forth the more you're exerting right so it's pretty straightforward to imagine a model that's able to find a relationship from frequency to exertion for the horizontal component moving back and forth what about the vertical component so imagine I'm walking at a certain frequency I'm walking on flat ground or I'm walking up a very steep hill or I'm walking up four flights of stairs in the innovation building same frequency but now the whole the vertical component is difficult is different I've got my phone I've got my accelerometer data I've got other sensors on the phone how do you infer the vertical component of motion it's going to be important if we want to try and predict exertion with calories burned more accelaration in either direction which two directions exactly so assuming we get acceleration data on three dimensions left and right forward and back up and down as long as I've got if I know I've got up and down then it's easy right if there's more acceleration up and there is down I'm going up think very carefully about this physical context if I get hot more positive why than negative why can the phone confidently predict that the user is walking uphill or going up stairs think about the accelerometer itself in the phone I don't know I think the etc robbery has a way to to normalize himself so positive Y is always the positive y is always up right so older phones this might be positive Y but this is now negative Y right it depends on how you put your phone in your pocket these days again maybe it doesn't matter so much right and this particular example doesn't matter too much but yet another example you need to think very carefully about this problem there was an interesting discussion I can't remember if it was in this paper or not about using a barometer which actually does exist on some phones there are very slight decreases in air pressure as obviously you move up if you're climbing climbing a mountain then absolutely the barometer is going to be useful here climbing a flight of stairs indoors air pressure is probably not going to be very useful so as suggested the usual way to do this is still just rely on accelerometer data but we look for a symmetries in the repeating pattern right if there is more time spent in positive Y than negative Y or vice-versa we can detect moving up and down which direction the user is moving in getting into some very detailed analysis of the acceleration data ok ok so we now assume that we've got our accelerometer data we've got our model that takes accelerometer data is input and produces a prediction given the horizontal and vertical components of the motion itself we assume that we have some vo2max information so we know the actual exertion of our user we're going to Train our model to minimize its prediction error against actual exertion okay again in this paper they used a very very simple model not unlike what we just saw for sleep they're going to they're going to take they're gonna take a metabolic actual here actual aerobic activity per day and again this is a little bit misleading this should not be actual this is the predicted activity from the accelerometer data we're gonna take high and low maximum and minimum recommended total aerobic activity per day in this case they weren't measuring the actual calories burned per day they were just asking was the number of calories burned per unit time above some threshold for how many minutes per week so you're exercising or you're not they kept it pretty low resolution and they pushed it through another another another model here which normalizes to give you physical sub day so again physical sub day is going to be near zero if you're not exercising enough or in this case if you spend all your time in the gym and it's gonna give you a high value if you're spending somewhere between 150 and 300 minutes per day per week exercising again there's a lot of hand waving here this is very subjective trying to go from raw accelerometer data to giving you giving the user a simple number you didn't spend enough time exercising this week or you spent perhaps too much time okay okay social interaction this one is not that different from what we saw in the previous experiment there again gonna go now from microphone volume to infer whether you're in conversation with someone or not and somewhat controversial they're going to use conversation as social interaction you can obviously engage in social engage in a meaningful social interaction with someone without actually speaking to them we're going to ignore that for now we're going to play the same trick here is we're gonna try and take where is it here adoration actual so actual sorry predicted duration of conversation relative to total time that the microphone is is active so how much time is is the microphone detecting that you are speaking or listening to someone else versus not and give you back social per day how many hours or how many minutes per day I'm sorry how how whether you're spending enough time in conversations or not I don't know what enough time is again lots of hand waving here if you're talking to your pets does that counts or not definitely meaningful it's definitely meaningful ok so here's some actual data from some of the alpha testers of this technology so we've got Monday to Sunday on the horizontal axis here and the vertical axis again is not the total amount of physical exercise social interaction or hours of sleep it's whether these three components of well-being are being maximized or minimized throughout the week assuming that we can trust these predicted values what does this tell us about this particular user if anything very busy weekend that makes sense okay okay and then they put this together in a very nice happened used some fancy graphics here to give sort of a dashboard view for the for the user I think the turtle is sleep and the fish school is exercise and the starfish oh no it's the school of fish which is social interaction makes sense and I don't know whether a starfish is active or not take care take your pic here but again the idea trying to take all this raw data and boil it down into something that's intuitive or engaging or entertaining so at least give a rough estimate to the users about how well they're doing from one week to the next how many of you would download this app onto your phone not today today maybe 2011 possibly okay obviously this is a has turned into a huge industry right there are a lot of people that are very curious about data analytics for predicting physical social and emotional well-being as you can see it's fraught with a lot of challenges can you infer can you infer various activities can you predict make good predictions about sleep activity diet stress and so on if you can does it require wearable technology how invasive does it need to be and is it going to be acceptable to the user are they going to accept wearing a device or accept privacy issues by leaving the phone and the accelerometer data on most of the time it depends okay okay so just as a summary here in this early example they were trying to infer sleep physical activity and social interaction phone plugged in is not moving it's quiets predict sleep infer the amount of energy spent from accelerometer data it infer something about social interaction from picking up voices you could imagine a lot of difficulties with these technologies you could imagine bringing in lots of other sensors to corroborate and improve these predictions if the user is willing to do so okay so let's move on we're a little bit ahead of schedule here I haven't put up the link for today's talk lecture I'll do that when I get back to my office we're going to look now at the human speech own project so we looked at inferring something about social behavior among a group of graduate students we just looked at inferring something about social and physical well-being for an individual where we're now going to look at a completely different domain which is how do children go about acquiring language what causes a child to emit a particular word for the first time in a particular setting this is a subject that's been studied in developmental psychology for decades and decades it is an extremely difficult question to answer language acquisition is still a very mysterious process could we deploy ubiquitous technology to try and measure and understand what's going on during the first few years of a child's development and use that data to predict when where why when where and why they will utter a certain word for the first time could imagine why I chose this particular experiment this is particularly difficult there's particular social context and physical context cultural context surrounding the early years of human children how do we go about doing this okay okay so we're going to look at this third experiment now we're gonna try and deploy some technology to be as unobtrusive as possible however we want to try and collect enough data to try and answer or explore this question of how children acquire language I'm going to look at again now somewhat dated experiment from ten years ago the questions tackled in this particular paper how did children learn language and now drilling down into that what aspects of their physical and social environment trigger language acquisition a young child may emit a particular word for the first time but they may have heard that word two days earlier how do we know that they heard that word and how do we know that hearing that word influence them emitting it on that particular day what events in the past influence the learning and utterance of new words okay so what's the typical approach to doing this the typical approach is to invite into the laboratory young children and their parents or caregivers and then ask give the caregiver the parents a very careful script to follow and then watch what happens so in this case as you can see in the little cartoon here little picture the parent is looking at a particular object and let's assume they're repeatedly pronouncing the name of that object blue block blue block blue block while they're looking at the object and the investigators observe that the child looks at the parents eyes and infers that the parent is looking at something else follows the gaze of the parent to the blue block and so now both parent and child are jointly attending to the same object this is a phenomenon known as joint attention we do this all the time most of you when I did this you looked over at the slides because although I'm not looking at the slide I'm giving a signal that I want you to attend to this object as the child follows the parents gaze to the blue block at the same time the child hears word blue block so perhaps we can derive the hypothesis that children acquire language assuming the child then said blue block we could infer or we could treat this observation as evidence supporting the hypothesis that children acquire language through joint attention if a parent is directing a child's attention to an object while saying the name of that object and then the child says that object as well that seems to suggest that hypothesis right seems to make sense but we gave the parent the script to do this what if we gave the parent the script to do something else and found that that also triggered the child to say the word blue block the investigator is wrapped up in the hypothesis and the experiment themselves can we instrument some technology to try and take the investigator out of the experiment this is an ongoing revolution in psychology most psychological experiments are what's known as being theory lated we start with the theory i theorize that children learn language through joint attention so I'm going to use the hypothesis to set up an experiment and test it it's okay but instead of being theory Laden it would be nice to be data-driven instead is to step out and just collect information about parents and children in their natural habitat and look at that raw data for patterns right so the investigator is no longer involved and is no longer influencing the interaction between parent and child all we could really say if in fact the child does say blue block more often with the parent in this example is children tend to say blue block when they're in a laboratory setting with white walls and scary people around and the parent who somehow acting a little bit strange because they're following a script that triggered the utterance of blue block in this case we can't with confidence generalize from this laboratory setting to say children in general acquire language because they jointly attend to what their parents are attending to okay so instead of the typical approach of doing this in a laboratory we're gonna look at an experiment from Professor Roy here where he and his wife observed their child continuously for the first three years of their child's life in its own environment their home it's a longitudinal study they're studying the child over a long period of time as you can imagine this is an extremely controversial experiment the scientist is studying his own child so this qualifies as human subjects research in the United States legally whenever you conduct scientific experiments that involve human subjects they have to give their written permission that they're involved in the experiment we're dealing with children who cannot legally agree to anything so who agrees on their behalf the parent who in this case is also the investigator right huge conflict of interest very controversial if you're not happy with that you're welcome to leave class early this morning no takers okay all right keep that in the back of your mind obviously this is somewhat controversial let's see what Professor Roy and his wife learned from this experiment I'm gonna play part of the TED talk about that Deb gave on the results of this experiment after the fact imagine if you could record your life everything you said everything you did available in a perfect memory store at your fingertips so you could go back and find memorable moments and relive them or sift through traces of time and discover patterns in your own life that previously had gone undiscovered well that's exactly the journey that my family began five and a half years ago this is my wife and collaborator rupal and on this day at this moment we walked into the house with their first child our beautiful baby boy and we walked into a house with a very special home video recording system this moment and thousands of other moments special for us were captured in our home because in every room in the house if you looked up you'd see a camera and I'm like can you guys hear in the back yeah okay the phone and if you look down you get this bird's-eye view of the room here's our living room a baby bedroom kitchen dining room and the rest of the house and all of these fed into a disk array that was designed for a continuous capture so here we are flying through a day in our home as we move from Sun late morning through incandescent evening and finally lights out for the day over the course of three years we recorded 8 to 10 hours a day amassing roughly a quarter million hours of multitrack audio and video so you're looking at a piece of what is by far the largest home video collection ever made and what this data represents for our family at a personal level the the impact has already been immense and we're still learning its value countless moments of unsolicited natural moments not posed moments are captured there and we're starting to learn how to discover them and find them but there's also a scientific reason that drove this project which was to use this kind of natural longitudinal data to understand the process of how a child learns language that child being my son and so with many privacy provisions put in place to protect everybody who's recorded in the data we made elements of the data available to my trusted research team at MIT so we could start teasing apart patterns in this massive data set trying to understand the influence of social environments on language acquisition so we're looking here at one of the first things we started to do this is my wife and I cooking breakfast in the kitchen and as we move through space and through time a very everyday pattern of life in the kitchen in order to convert this opaque 90 thousand hours of video into something we can start to see we use motion analysis to pull out as we move through space and through time what we call space-time worms and this has become a part of our toolkit for being able to look and see where the activities are in the data and with it trace the patterns of in particular where my son moved throughout the home so we could focus our transcription efforts all the speech environment around my son all the words that he heard from myself my wife our nanny and over time the words he began to produce so with that technology and that data and the ability to with machine assistance transcribed speech we've now transcribed well over seven million words of our home transcripts and with that let me take you now for a first tour into the data so you've all I'm sure I've seen time-lapse videos where a flower will blossom as you accelerate time I'd like you to now experience the blossoming of a speech form my son soon after his first birthday would say Gaga to me in water and over the course the next half year he slowly learned to approximate the proper adult form water so we're going to cruise through half a year in about 40 seconds no video here so you can focus on the sound think ooh sticks of a new kind of trajectory GOG on to the water [Music] [Applause] sure nailed it Jimmy so he didn't just learn water over the course of the 24 months the first two years that we really focused on this is a map of every word he learned in chronological order and because we have full transcripts we've identified each of the 503 words if you learn to produce by his second birthday he was an early talker and so we started to analyze why why were certain words born before others this is one of the first results that came out of our study a little over a year ago that really surprised us the way to interpret this apparently simple graph is on the vertical is an indication of how complex caregiver utterances are based on the length of utterances and the vertical axis is time and all of the data we aligned based on the following idea every time my son would learn a word we would trace back and look at all of the language he heard that contain that word and we would plot the relative length of the utterances and what we found was this curious phenomena that caregiver speech would systematically dip to a minimum making language as simple as possible and then slowly ascend back up in complexity and the amazing thing wasn't the fat bounced that dip lined up almost precisely with when each word was born word after word systematically sort of well I just paused there for a moment so this idea that the length of the utterance is before the child ever said the word was long so do you want a glass of water would you like some water and as the time approached at which the child uttered the word for the first time water the length of the utterances of the parents in the home was getting shorter and shorter would you like some water how about some water water water shorter than the child says it and then after the child utters the word water for the first time in the months that follow parents start to lengthen the length of the utterances that contain the word water yep exactly you stole my question this is a great example of scaffolding right this is exactly it it's a dip but the way to think about it is they're adding scaffolding and then when the child utters the word water they gradually remove it again I mentioned last time that scaffolding is an idea that came from developmental psychology it's known that parents do this it wasn't known until this experiment that they were doing it in this way with language so that's a perfectly valid hypothesis we can now ask questions for why this phenomenon exists it's difficult to imagine that parents are conscious of this process for all the words perhaps there's a simple heuristic at work here which is the parents are inferring that the child is getting close to emitting the adult form of the word and that triggers scaffolding in the parents that may or may not be true we could imagine if we had access to Professor Royce data we could go back into the data and look for additional information to prove or disprove that hypothesis if you remember back when we were having our discussion about visual design one of the great aspects of visual design is being able to actively interrogate a data set right we see a new phenomenon for the first time this verbal scaffolding forward birth which generates a hypothesis for why it's occurring how could we dive back into the data to find evidence for against the potus's another question is up because this is a only one child one child yeah we observe only one child there may be some other possibility so I learned that if we want to stabilize establish a cause of causation we need to do experiment so this is this is purely observational it cannot it doesn't tell us anything about causal mechanism right there was a proposal here for causal mechanism parents are aware of what's happening and that causes word birth causes them to reduce the length of their utterances which in turn causes children to utter the adult form for the first time this data can tell us nothing about causation but we can investigate go back into the data and find additional correlations and sort of marshal them around our hypothesized causal mechanism but ultimately if we want to try and prove causation or we need to go back to experiment yeah much more controversial okay let's carry on it appears that all three primary caregivers myself my wife and our nanny were systematically and I would think subconsciously restructuring our language to meet him at the moment of the birth of a word and bring him gently into more complex language and the implications of this there are many but one I just want to point out is that there must be amazing feedback loops it's not of course my son is learning from his linguistic environment but the environment is learning from him that environment people are in these type feedback loops and creating a kind of scaffolding that has not been noticed until now but that's looking at the speech context what about the visual context we're now looking at think of this as a dollhouse cutaway of the of our house we've taken those circular fisheye lens cameras and we've done some optical correction and then we can bring it into a three dimensional lights so welcome to my home this is a moment one moment captured across multiple cameras the reason we did this is to create the ultimate memory machines where you can go back and interactively fly around and then breathe video life into this system what I'm going to do is give you an accelerated view of 30 minutes again of just life in the living room that's me and my son on the floor and there's video analytics that are tracking our movements my son is leaving red ink I'm leaving green ink we're now on the couch looking out through the window at cars passing by and finally my son playing in a walking toy by himself now we freeze the action 30 minutes we turn time into the vertical axis and we open up for a view of these interaction traces we've just left behind and we see these amazing structures these little knots of two colors of thread we call social hotspots a spiral thread we call a solo hotspot and we think that these affect the way languages learn what we'd like to do is start understanding the interaction between these patterns and the language that my son is exposed to to see if we can predict how the structure of when words are heard affects when they're learned so in other words the relationship between words and what they're about in the world so here's how we're approaching this in this video again my son is being traced out he's leaving red ink behind and there's our nanny by the door she offers water and off go the two worms over to the kitchen to get water and what we've done is used the word water to tag that moment that bit of activity and now we take the power of data and take every time my son ever heard the word water and the context he saw it in and we use it to penetrate through the video and find every activity trace that Co occurred with the instance of water and what this data leaves in its wake is a landscape we call these word scapes this is the word scape for the word water and you can see most of the action is in the kitchen that's where those big Peaks are over to the left and just for contrast we can do this with any word we can take the word by as a goodbye and we're now sumed in over the entrance to the house and we look and we find as you'd expect a contrast in the landscape where the word by occurs much more in a structured way so we're using these structures to start predicting the order of language acquisition and that's so ongoing work now in my lab which we're peering into now at MIT this is at the Media Lab this has become my favorite way of video graphing just about any space three of the key people in this project Phillip the camp Ronny cubot and Brandon Roy are pictured here Phillip has been a close collaborator and all the visualizations you're seeing and Michael Fleischman was another PhD student in my lab who worked with me on his home video analysis and he made the following observation that just a way that we're analyzing how language connects to events which provide common ground for language that same idea we can take out of your home Deb and we can apply it to the world of public media and so our effort took an unexpected turn think of mass media as providing common ground and you have the recipe for taking this idea to a whole new place we've started analyzing television content using the same principles analyzing event structure of a TV signal episodes of shows commercials all of the components that make up the event structure we're now with satellite dishes pulling in and analyzing a good part of all the TV being watched in the United States and you don't have to now go an instrument living rooms with microphones to get people's conversations you just tuned in to publicly available social media feeds so we're pulling in about 3 billion comments a month and then the magic happens you have the advanced structure the common ground that the words are about coming out of the television feeds you've got the conversations that are about that those topics and through semantic analysis and this is actually real data you're looking at from our data are processing each yellow line is showing a link being made between a comment in the wild and a piece of event structure coming out of the television signal and the same idea now can be built up and we get this word scape except now words are not assembled in my living room instead the context the common ground activities are the content on television that's driving the conversations and so what we're seeing here these skyscrapers now are commentary that are linked to content on television same concept for looking at communication dynamics in a different very different sphere so fundamentally rather than for example measuring content based on how many people are watching this gives us the basic data for looking at engagement properties of content and just like we can look at feedback cycles and dynamics in you know in a family we can now open up the same concepts and look at much larger groups of people this is a subset of data from our database just 50 thousand out of several million and the social graph that connects them through publicly available sources if you put them on one plane a second plane is where the content lives so we have the programs and the the sporting events and the commercials and all of the link structures that tie them together make a content graph and then the important there are dimension each of the links that you're seeing rendered here is an actual connection made between something someone said and a piece of content and there are again now tens of millions of these links that give us a connective tissue of social graphs and how they relate to content and we can now start to probe the structure in interesting ways so if we for example trace the path of one piece of content that drives someone to comment on it and then we follower that comment goes and look at the entire social graph that becomes activated and then trace back to see the relationship between that social graph and content very interesting structure becomes visible we call this a taco viewing cleek a virtual living room if you will and there are fascinating dynamics at play it's not one-way a piece of content an event causes someone to talk they talk to other people that drives TuneIn behavior back into mass media and you have these cycles that driving overall behavior another example very different another actual person or database and we're finding at least hundreds if not thousands of these we've given this person a name this is a pro amateur or pro a media critic who has this - out brace a lot I think will pause the video there before you go just a quick note about this video obviously there's two phases here one of the students from last year summarized these two phases as phase one is Big Daddy and phase two is Big Brother so this harkens back to the good old days when clearly this was a good thing to measure everyone's behavior what they were watching and so on very different today we'll talk about the implications of this next time you have a quiz due tonight and you're working on deliverable 9 see you next Tuesday you 
VrLwFMgoxlw,27,"Guest Lecturer and Computer Science professor Björn Hartmann talks about Human-Computer Interaction. This video is for the CS10 Summer 2011 Professional Development for High School teachers. Notes are available. This video was taped during our Fall 2010 AP CS : Principles Pilot semester.
NOTE: The videos that he shows during the lecture will be edited into this video soon.
0",2011-07-17T23:49:15Z,"UC Berkeley CS10 FA10 Lecture 16, Human-Computer Interaction (HCI) with Björn Hartmann (1080p HD)",https://i.ytimg.com/vi/VrLwFMgoxlw/hqdefault.jpg,Dan Garcia,PT50M34S,false,3889,27,0,0,0,"I'm welcome everybody to I don't election of our lecture 13 there's something it's most importantly it's the day before it's the couple of hours before your midterm lecture which is the most important thing you all know that there's a midterm tonight at 6 to 8 p.m. it's my great pleasure to introduce for you if you were in Hartman professor in computer science he art is a recent faculty addition he came from Stanford I believe is that right yeah and Bjorn is the king of all things HCI there's a couple of h5 Oaks but he is the new hot fresh face and we're really excited to have him share the vision of what HCI is for you guys so without further ado thank you great then thanks for the introduction so I'm gonna give you a a broad overview of what I think is interesting in human-computer interaction today just a note on procedure there are a lot of slides don't bother about writing everything on them down the slides will be available at a later point online so but if there's anything that strikes your interest make a note of that now let me just give you a couple of words about my own background because as many of you I did not start in computer science so in the mid to late 90s I was at the University of Pennsylvania in Philadelphia and I was a communications major I was in the social sciences and I did experiments in psychology and then a new major came along called digital media design that was part social science part Fine Arts and part computer sciences hmm that sounds interesting so I signed up for that and progressively got sucked into engineering once I saw kind of the the power that computer science put at my fingertips so I stayed in Philadelphia through my bachelor's and then also did a master's in computer science because it's it seemed like the next logical step however that's not all I did after graduate school I kind of changed to careers for a while so I was an independent record label owner and traveling deejay this is my label partner J haze at Club milk in Tokyo on our tour in 2003 and this is really what got me into a human-computer interaction because note what's happening here so I don't know how many of the people in the room have ever played records raise your hand if you have so small number so this has been the user interface for DJ since the oh I think 1970s it's to direct drive technics turntables and a multi-channel mixer in in the middle to mix between two channels of stereo sound now what is this laptop doing in the picture that's right the laptop has all the music on it but Jay is not playing the laptop he's performing with the turntables because there's a there's a round box up there that's a system called Final Scratch so the turntables are not plugged into the mixer they're plugged into Final Scratch which is plugged into the laptop which is plugged into the mixer and the records the records he's playing are not normal records there's no music on them all that's on them is a timecode so it's some time code encoded as an audio signal the software here reads that timecode and uses it to control playback of mp3 files so instead of travelling with a case of at most 100 record records that's all you could carry a record weighs about half a pound so 50 pounds that's already a lot to lug around here I can now have thousands of songs with me but still use the rich interface that I've gotten used to over years of practice to perform so this was really fascinating to me and then when I decided to go back to grad school add an insignificant University on the other side of the bay I decide to really look into user interfaces so I did my PhD in computer science at Stanford in the human-computer interaction group and I finished last fall and so since January I'm now here teaching both the undergraduate and the graduate human-computer interaction courses so what is human-computer interaction well let's just take it from the top we clearly have three terms here we have the humans which are the end users of your application traditionally many people have thought as it's the person sitting at the keyboard and using the mouse but it also encompasses others right the people you interact with online the people you collaborate with over applications over shared whiteboard drawing applications or your friends on on Facebook's on Facebook so these are all the humans that human-computer interaction cares about then of course we have the computer the machine that your application runs on that traditionally was a desktop now more and more the form factors look very different and also the programming models start to look very different because more and more applications live on the web and are split into client parts and server parts so this machine side of human-computer interaction is also constantly evolving and then interaction is just kind of the ongoing dialogue think of you forming some goal you have to express that in some way to the machine the machine has to interpret that give you something back which you have to interpret form the next goal so it's kind of an ongoing cycle now user interfaces are then the part of your application that enables this ongoing dialogue so clearly we have traditional user interfaces with widgets so user interface components such as the ribbon in Microsoft Office but that's not all human-computer interaction is also very much concerned with the hardware you use to interact with your computer so the Wiimote introduced acceleration sensing and has a totally different number of buttons than your your mouse or your keyboard and there are other more experimental interfaces such as the react table which is an interactive music synthesizer that uses these tokens on a horizontal display so you put these tokens down which represent different musical operators and the way you move them and arrange them on the surface controls the the synthesis of sound now if there's one thing to take away from today it's that HCI encompasses all of this is it encompasses the design the prototyping the implementation so the engineering and the evaluation of user interfaces if you want to look at that as a different diagram you can really see three big strands of influence so we have people and we have to know what people are good at doing what their cognitive limits and capacities are what they like to do what they don't like to do then certainly were an engineering department so there's technology involved right what are the technologies we have at our disposal to build new interfaces but we're not done with that you also have to consider the task human-computer interaction is much more of a design discipline than other parts of computer science and design is always very specific so you try to solve a specific task for a specific type of user now and none of this happens in the void so there are organizational issues right things you can do at a start-up are very different from things you can do at Microsoft just because the size of the organization which also means that the software you use at a start-up will be different than the software you use at Microsoft all right one more way to slice human-computer interaction if you look at where people who are active in human-computer interaction today are coming from computer science is really only one of the major directions and the other two are design so kind of the the professionally trained designers which mostly came from product and interact and industrial design who really know how to define the look the feel the aesthetics of an artifact and they're now increasingly moving into the digital realm and then there's applied psychology which gives us a science of the user they give us models of what we can hold in working memory how fast and efficient we can be when when interacting with computers but really what it comes down to for me is this this is why I love to work in HCI I can wear all of these four hats on any given day right there's engineering and I love to build complex systems but there's also design and art which get into the aesthetics and the dynamics and the look and feel and then there's science in the end especially if you're in research you want to make some claim that what you created is better than what was out there before and how do you do that well you borrow experimental paradigms from cognitive science and psychology this is how building user interfaces happens in practice which is different from many engineering processes it turns out that this human ingredient in building user interfaces it's just very different very difficult to quantify and to build complete models of we don't really know how we think and what we like and user interfaces and what makes us perform better or worse we have little kind of tidbits of knowledge but there's no complete theory of what people like in user interfaces that means you cannot build a successful user interface by just sitting down in the library and thinking really hard about the problem all you can do is build a concrete solution that embodies your hypotheses of what you think may work and what you think people will like and then test it and see if how people respond is how you thought they would respond and then you take that feedback and make that the input on the next iteration of your design so you improve the interface you test it again you take what you learned and improve the interface again and you do that until you run out of time or money and this fact that really the only way to be successful in user interface design is through iteration it's summed up in this quote by David Kelly so David Kelly is one of the cofounders of IDEO which is one of the most successful product design consultancies they're right here in the Bay Area they have offices in Palo Alto and in San Francisco and his quote is that enlightened trial-and-error which is just experimenting trying things out but not trying things out randomly right you have to have some intuition which is guided by theory and models and your own expertise outperforms the planning of the flawless intellect alright so how does this look like in practice how does the practice of enlightened trial and error look like well whenever you try to build a new user interface that makes someone's life better you have to understand who that someone is first so you really have to understand your target users and that is much easier if you build an application for a very targeted group of people it's very hard to design something like an operating system or a abstract communication platform for everyone in the world because people are so different however if you zero in on user interfaces for students who take CS 10 right there are certain commonalities that you know all of you share for example Wednesdays at 11:00 a.m. you're all in this room I can use that to think about whether certain design decisions are better or worse now we have borrowing from design very structured processes for how to observe users so first of is you have to get out of you're out of your dorm room out of your lab and you have to go out into the so the top shot here is an example of a group of students I led their design exercise was to build a user interface for farmers who sell products at the farmers market so we went to San Francisco on a Sunday morning to the Ferry Building and they just interviewed the people who had fruit stands who had vegetable stands and try to understand what are the opportunities here because we're on a higher level it doesn't seem like that farmers are natural users for new technology yet there are all these different opportunities around you know how they handle payment how they want to build lasting relationships with their customers that you only uncover by going out and talking to them now once you've talked to people you then build scenarios so you make up stories that aggregate all these different data points that you've observed into kind of coherent holes and then you also try to build these abstract models that summarize your insight and usually that involves lots of sticky notes now once you have this insight and a particular point of view of how you want to improve these users lives you don't go out and just build the final thing because that is risky and very time and resource intensive it takes a long time to write production-ready code now to make sure that you're building the right thing you want to get remember the cycle you want to get to the part that you can get feedback from other people as quickly as possible and you do that by building mock-ups and prototypes so these are approximations of your user interface that just seek to elicit what it would be like to interact with the application before writing a single line of code or before writing thousands of lines of code and only writing dozens of lines of code so there are a variety of techniques for example paper prototyping so here someone is prototyping on in the upper picture a mobile phone application with a certain menu now instead of trying to program an embedded device I just took a sticky note drew the menu on it and said here pretend this is the menu what would you do next so you can build one of these prototypes in half an hour an hour two hours and instantly give it to your friend your classmate and uncover really high-level successes and problems in the way you're structuring your application know you can go one level up in fidelity and build interactive prototypes using HTML web design tools flash you know there's a wealth of applications that let you wire together user interfaces with limited functionality fairly quickly and that now lets you get add a little more issues of fit and finish how it feels like using the application because if you're doing paper prototyping someone has to play the computer because there's no program yet right so I would give you the interface and say what would you click on you take the stylus and say I click on the file menu and I said Oh hold on I'm the computer let me draw the next sticky this is the next screen what would you do next so there's there's just a what I think the high-level point here is just that there's a range of options you should you should take into account yes yes yeah absolutely so tools like scratch or your version VOB are exactly at that right level of abstraction where you can mostly think about what you want to get done and not worry about what's the complexity of the algorithm what's this API look like to do network communication that can all come later because that's a different set of concerns altogether so here just some other examples of prototypes this comes from film you can draw a storyboard that just shows frame by frame what a user would do as they go through one of the key interactions in the UI you want to build prototypes are not just software only so this is a prototype that I do built back in the early 90s when people still used 35-millimeter film cameras so when digital cameras were not around one of the questions they were faced with was what do you do when you have an LCD screen on the back of your camera so how would you answer that question their solution was well we have some mechanical engineers that know how to build boxes so we'll have them build a box we'll just disassemble an LCD screen put it into that box and then run this long cable back to a desktop computer where we use whatever software is the fastest to mock up the user interface but then type the graphics PAP back out onto this display where you're now pretending that you're holding the camera so this gets lets you get at the experience of using the LCD screen it says nothing about how the viewfinder works or how to set exposure right but that's kind of the key to building a successful prototype you have a key question that you want to answer and you build the cheapest possible thing to let you answer that question all right once you've built a prototype it's time to get some feedback so if you think about evaluation there are really two different categories of questions that matter the first is formative evaluation so the formative just indicates evaluation while you still have time to change what it is you're doing and the key questions are there are we even building the right thing because if not then we should switch course as quickly as possible and find the right thing to build and concretely what should we change for the next iteration we're going to face now this is different from summative evaluation which is we've build our interface it's done it shipped how successful were we in the marketplace how successful were we at achieving our performance goal how happy are the users with it and there are many many different techniques you can use for evaluation this is kind of the traditional high cost one which is you have a special usability lab where the user is on one side of the lab behind a halfway mirror they interact with an application and you stick as many cameras and other instrumentation in that room as possible but you let the user completely just interact and you film all their failures and successes and communicate that later on tally them up run statistics on them this is the right way to get to do science in human-computer interaction however it's also very resource intensive and slow so if you guys are for your project building a user interface you don't have to rent out one of these labs it's already incredibly useful to pull someone in from the hallway and say I'm working on this application pretend you know you're trying maybe it's a Bart application pretends it's 8 p.m. on a Friday and you want to get to San Francisco how do you find out when the next train is and then you let them run through your interface and ask them to talk aloud ask them to tell you what they're thinking where they're getting stuck and sure enough half an hour later you have this really useful data of how people think your application works which is usually different from how you think that people think your application works all right time for a quick poll all right so the question is you know do we actually care so how hard is it to write these user interfaces how much of an applications code on average across operating systems Microsoft Word your scratch program is devoted just to getting the user interface widgets on the screen in interpreting the mouse input and how much is all the backend logic the database the business logic the networking we have five options and thirty seconds alright it looks like we're stabilizing so we have 0% think it's 10% 14% of you think it's be about 20% see 50% D and 17% E so may have biased you a little bit given that I'm talking about the value of HC I hear someone studied this wrote a paper about it and said in today's application 20 years ago roughly 50% of the code is devoted to the user interface portion now there's a separate question right you can ask how long does it take to write that 50% are those easier 50% or harder 50% and it turns out is also roughly about 50% of the time on a project that's spent on writing UI code so if it's really half of your project you should maybe invest half of the time of your project on getting that part right all right let me tell you a little bit about the history of human-computer interaction this is one of the very first computers in the u.s. the ENIAC at UPenn in 1946 the user interface to this computer where patch cords and dials and some individual not LEDs those weren't around so individual light bulbs fast forward to another poll when do you think the mouse was invented okay I think we have everyone if I had to interpret your answers I think it would look like this which tells me you're probably not that sure okay with someone let's see we have half the class thinks it was in 78 anyone want to venture a guess why'd was 78 or late seventies early 80s oh I'm not telling you yet that's a very good intuition and overall you got your timeline right there and that it was the mid 70s to early 80s when personal desktop computing really emerged anybody want to argue for for another choice yes okay so Woodstock was in the 60s there were computers with mice had to be the 60s so so the first Apple Macintosh was what 84 and it certainly was the first really successful computer aimed at at consumers that shipped with a mouse and actually the design for that was also done here in Silicon Valley by frog design a company like like idea who imported designers from Germany to to work on on the industrial design of the mouse but actually turns out the first Mouse this is the first Mouse it was built in 1963 by Doug Engelbart and Bill English at SR I which is Stanford Research International a spinoff from Stanford in Menlo Park and it was built as part of an early research project into new user interfaces that aim to augment our collective intelligence and the mouse was kind of a side project that just fell out of this really large research agenda this also illustrates another point that's that you find very frequently it takes about 15 to 20 years for a research idea to make it into the while so you can buy it off the shelf so how did the first Mouse work well there were two wheels set at a right angle if you move the mouse let's call this the Y direction one of the wheels would spin the other wheel would just kind of scrape and hop along yep if you move the other direction the other wheel spins and the Y wheel scrapes along and if you move diagonally then hopefully both spin proportionally to to the direction the next insight was to put these wheels inside the mouse and use a ball that that would move both so you don't didn't have that that scraping so the cool thing was that last semester Doug Engelbart and Bill English came to visit and Doug gave he gave a brief address because he was a Berkeley attendee I'm not sure he ever graduated he was in the ph.d program and Bill English is also still around and he was you know showing up how now he does everything undestroyed she brings us to the next question when was pen input to computing invented so anything that's a pen on a screen okay 30 seconds are up let me draw these results does anyone have a strong opinion on on pen computing I heard something about a light pen you know tell us about the light pen so clearly this has been around since about 2004 this particular device so kind of having touch sensing screens they've also been around for a while but in consumer products they've only really come to our attention in in the last 10 years or so right around 2000 you could buy a lot of tablet computers then those went away and now you can buy a lot of touch displays it turns out the first pen computing interface was done by Ivan Sutherland once again a research project this time on the East Coast and this is to me is just so inspiring this was 1963 at which point I talked to Doug Engelbart awhile ago and he said back in the early 60s the longest time anyone had ever gotten a computer to run without crashing was about half an hour and at that time along comes Ivan Sutherland who says oh I'm just going to build a 3d CAD system using this light pen which is basically a light sensor that that reacts whenever the the cathode ray beam of old CRT displays kind of passes by it so this is something that's why hardwired into the computer and there were no there weren't even any good displays back at the time so this is basically a hacked oscilloscope I said oh yeah we can just draw directly on this oscilloscope and well we're not that good at drawing so let me just build a constraint solver into this user interface can make things parallel even if I didn't draw them that way so I would say this is still along with drug angle Bart one of the two greatest demos of all times of HCI because clearly decades ahead of its time all right moving right along so someone said back in the mid to late 70s was when you could buy personal computers and that is right on target so this is the Xerox star so the Xerox star and the Xerox Alto were the first machines that basically had the whole vision of what a personal computer was in one package so you have a graphics display not a text display and you have this desktop metaphor which is graphical objects on the screen are like objects I like physical objects on my desk so there are file folders there are papers there's a trashcan and I can drag things around this seems completely obvious to us now right if you go back to the papers written in the 70s there is a whole paper that just explains what this metaphor is so people would understand there's a completely new concept and I have this link here let's see if we can pull this up so I just quickly want to show show this to you oh and missing a meeting so I just found one of these these web 2.0 startups that allow you to build timelines of computing history and that link will be in the slides that are online and I kind of picked some of the most interesting moments in HCI history so it actually starts all the way back in 1945 when the director of the National Science Foundation wrote a paper about the Memex did you guys hear about the Memex in class yes so he basically envisioned the web it was all on microfilm but still it was high documents before computers existed and then we have this big jump to the 60s where we have sketchpad which I just showed you the first Mouse the first GUI widget the mother of all demos by Doug Engelbart where he showed hyperlinking word processing hierarchical document organization video conferencing collaborative document editing all in like one video going through the 70s you then have the invention of Ethernet at Emacs the editor has been around for a surprisingly long time the first painting programs came in in the mid-70s the first visual programming languages of which scratch is you know kind of in that realm also came in in the mid-70s and the first version of the notebook was written up in 1977 by Alan Kay who now teaches at UCLA so I'll let you explore that more after class and we'll just move on so what has changed since that since the 70s or 80s in terms of what we do with computation that effects how we build user interfaces who's Gordon Moore exactly founder of Intel the guy after which Moore's law is named so this is now also already seven years ago so we can quadruple this estimate or maybe it's eight fold now so for every ant in the world there are 100 transistors what does that mean I would say it means something like this where we have a we have time on the x-axis and the number of processors per person in the world on the y-axis and if you look at the history of computing you had many many people using one computer in the beginning mainframe computing and then processors became cheap and plenty enough that it seemed entirely reasonable to have one processor per person so you bought your own desktop and that was your computer you didn't have to share it with anyone else well now we're starting to be way over here where the power of a desktop PC of the mid to late 90s you can buy for like $2 and 50 cents in in a chip that large so what does that mean if we have many many processors per person well that gets us to research directions so this is where kind of HCI is headed now so stuff earlier on these are all the things you'll learn about if you take an introduction to human-computer interaction the stuff I'm going to talk about from now to the end of class is all the stuff you'll work on if you decide to go to graduate school and study human-computer interaction all right so let's go back to the personal desktop PC have you guys in class discuss anything about mental models probably not all right so a mental model is my understanding of how a piece of software works right and a good piece of software a good user interface will have very carefully thought about what the users mental model of it is because of these has the wrong mental model they will frequently run into errors and not know how to get something done with the application now let's turn mental models around if this is your computing platform what is the computer's mental model of you so what does the computer think you look like when this is your user interface this is what artist Dan O'Sullivan came up with he teaches at NYU in the interactive telecommunications program so your computer knows you have a finger for clicking and maybe for hunt and peck typing it knows you have an eye to look at the screen but there's no mostly no 3d it knows you have two years because they're stereo sound but it doesn't know that you where's the rest of your body it doesn't know you have arms and legs and Anna torso and what would happen if we actually leveraged the rest of your body as a source of physical input here a couple of projects this is our Guitar Hero and colleague Scott's Epona's who was a student at the University of Washington so he is playing air guitar without any controller at all and that is because he's wearing a set of electrodes on his forearm these electrodes sense when he is contracting different muscles in his forearm so that looked a bit cumbersome but you can imagine having this whole thing just miniaturized into a sweat band and wirelessly communicate to to your PC and using pattern recognition algorithms he can then detect what are you doing this or this or this ah so that's that's a that's a very good question so this points towards that one of the dimensions of designing an input device clearly is latency so how quickly after you do something does something else happen in the user interface now this is where applied psychology comes back in because from experiments we know how quick something has to be for you to feel like it's a responsive interface and it turns out that threshold for normal applications is about a hundred milliseconds if the time lag between you doing something and something else happening on the screen in return is longer than 100 milliseconds we feel a disconnect between our actions and the result now for games that has to be a lot shorter right this particular approach is fast enough to play guitar hero decently you're going to play a first-person shooter with it probably not but that's the same trade-offs you kind of have with with we games or Kinect games right you get something new which is a richer engagement but you also have to give something up sometimes that's performance sometimes its accuracy so it just depends on what you want to design for here's another example from my friend David Merrill he was at the MIT Media Lab and now has a startup company in San Francisco his company is called Scipio and the question is well in the physical world were really good at organizing stacks and piles of stuff with our hands with both hands at the same time in the digital world we only get the equivalent of a four finger right how easy is it to shuffle deck of cards if all you have is a four finger so what if we take our user interfaces and instead of having one displays just having many many small displays and so here's an early demo of this system basically these blocks all have LCD screens and they can detect when they're next to each other and right now his company is building for example maze games where you see the next piece of the maze by rearranging the block and putting that at the end of the existing maze and I think they're starting they're aiming to launch this product and to sell it by the end of of the year or early next year other research directions we've seen a big shift away from who uses computing away from electrical engineers and computer scientists to everyone else who's working on other interesting projects and uses computation as the most powerful tool they have to get their work done so who in the room has been to Maker Faire Oh only three or so four of you all right this is this is if there's homework that I can assign go to Maker Faire next year so this is an annual convention at the San Mateo fairgrounds where you have the whole world of do-it-yourself hackers and amateurs who bring fire spewing fire trucks virtual reality bike helmets guitars that play themselves as just a vast landscape of 3d printers that use sugar and cornstarch people who make who who print metal sculptures it is so inspiring and there are 50,000 people who were there it is enormous and I would argue that you know scratch is actually only a few years ago scratch was an HCI research project out of the MIT Media Lab and it's it's a project so research in new user interfaces for programming squarely fit into human-computer interaction there are other projects at the Media Lab such as the lilypad arduino which allows you to make computational fabrics and computational fashion so you can start to sew LEDs into into sweaters and scarves and put and embed sensors as well and it just really opens up a completely different class of users who start to experiment with hardware who have completely different goals from electrical engineers another area of research is clearly social computing so an interesting part here is that actually most of the really successful social computing applications tend to come right now from the startup space because they're you can in research in in the university you try to get at the kind of conceptual core at things in the startup space you try to get as many users as possible which is a great map maps very well onto social computing so we have Friendster and Facebook Wikipedia or crowdsource applications for restaurant recommendations like Yelp now there's also a research component in here and the research component right now tends to be on the analysis side so this is much closer to social science so you can ask how does this large-scale online participation work in practice can we detect any regularities any laws in what's what's happening online so let me just give you one quick insight and that is whatever social you community you look at online when you try to characterize how active people are how many people participate and how much those people participate you find these power laws everywhere so these are basically these exponential fall-off curves where the first user contributes n posts the second user contributes n over two posts the third user contributes n over four posts the fourth you and so and so and so on so you have this really long tail that most people online lurk and do almost nothing and there's a short head of people who are enormous ly active so these are statistics for Flickr the same thing holds when you look at Wikipedia so the top people write more than a million posts but already if you look at user ranked 500 out of many thousands they've only written like a dozen and here's a little bit of math for this lecture how do you find out whether the data you look at is a power law because there are many different curves that roughly look like this well it's a very simple logarithm so you take this equation you take the logarithm of each side so you end up with something like that where K's your parameter and this sure looks like an equation for a line right y equals M times X plus B so you take your data data from an online system you take the log of it plot it and if you find a line you've found a power law and a lot of social science right now is moving in this direction of doing really large-scale statistical analyses of what people do online because for the first time we actually have this record of what people do in the trace of their interactions which all gets saved to the database which we didn't have before so where before you would study what a group of 12 people do while you follow them around yourself with a research assistant now we have this trace of what 5 million people do online for five years all right to quickly sum up finish up let me just show you a couple of things that are going on here in HCI at Berkeley so I myself work on user interfaces that allow you to rapidly write programs that use sensors and actuation so I developed hardware platforms as well as visual programming languages that allow you for example to build here this is a prototype that investigates whether using accelerometers is a good idea for text entry and the kind of tools I develop make it possible to build a prototype of the system in about an afternoon instead of drawing up a circuit board getting it manufactured writing the embed code for it etc we've taken these systems to maker faire have people create their own game controllers with frying pans and staplers and garden gloves so here are some examples of someone who's navigating a spaceship by banging on the walls and stomping on the floor or firing rocket thrusters by ducking down or shooting rubber bands by flicking a bend sensor that they taped to a table and so these tools make it possible to explore this space I think the bend sensor flicking that was all done in less than thirty minutes the game existed before we also work on collaborative applications so this is a large-scale multi-user interactive surface where multiple people can interact with the same display the same application in tandem and they can use both multi-touch as well as regular keyboards and and mice and then finally I'm also working on end user programming so specifically for people are not professional programmers how can we help them debug their programs in a more efficient way and one thing we're using right now is an approach suggested by recommender system so if you like book X you might also like book Y on on Amazon or on Netflix and we're taking that same approach to basically suggest to you how you should change your code if your program breaks so we have a recommender system that says people who had this error whose code looked like yours next made the following change all right it's 12 o'clock so time to wrap up this is the one slide summary so HCI is concerned with design implementation and evaluation of user interfaces right now is a super exciting time to work in this area because of this explosion into mobile debate Curtis and social computing if you're interested in taking an HCI course I'm teaching CS 160 next semester don't be discouraged by the wait list there's always a wait list for the course we'll make the decision on the first day of class who gets into the course and there is a course thread in human centered design which is a certificate program that just lists a whole bunch of courses that are relevant to HCI and other forms of human centered design in all different departments across campus thanks "
xCv2uSvh570,28,Playlist: https://www.youtube.com/playlist?list=PLAuiGdPEdw0j6VNxfbY-FNlbAjlWIVNnO,2020-10-22T16:53:21Z,"Human Computer Interaction, Lecture 14. University of Vermont, Oct 22, 2020.",https://i.ytimg.com/vi/xCv2uSvh570/hqdefault.jpg,Josh Bongard,PT1H15M46S,false,59,0,0,0,0,okay good morning everyone i hope you're all uh doing well and staying safe um just a reminder you are working your way through deliverable seven and we'll have a hands up this morning hands up if you have a k n that's recognizing at least two numbers better than fifty percent k n that's recognizing at least three digits better than fifty percent four digits better than fifty percent five digits better than fifty percent six digits better than fifty percent seven digits better than fifty percent eight digits better than fifty percent nine digits better than fifty percent ten digits better than 50 percent okay amanda is there and the rest of you are working on it uh you've got a couple days left damien's there uh you got a couple days left to work on this uh just a reminder um today i have my office hours which are which are usually 11 to noon i had to move them today to 2 to 3. so if you're still struggling with your k n and you don't know what to try next please be sure to try and come and see me or amanda during our office hours for help okay so um any questions about deliverable seven quick questions otherwise come see us during office hours oh okay all right so uh we are going to finish our section on uh cognitive psychology today where we swept through just a few aspects of human cognition and we're going to spend all of today most of today talking about uh the rule of affect or emotion in human computer interaction and we will probably start in today on uh crowdsourcing we'll see uh we'll see how we go okay we ended last time uh by visiting with an old friend uh hal who uh in the short span of about 30 seconds seems to express a wide range of emotion uh anger jealousy regret uh you contrition and so on for most of us we're pretty convinced that hal may be able to express these emotions but probably doesn't have them so we're going to obviously distinguish today when we're talking about emotions in machines between the exp the outward expression of emotions and the internal uh subjective feeling of emotion okay we ended last time by talking about this concept of anthropomorphization which is the attribution of human behaviors or properties to non-human animals or in our case machines in the information age these days we do this all the time here's a random email i got from uh from uvm uh years ago something was wrong with the software when the software was feeling better they would send out all the delayed reports right we do this all the time even though we know that the software itself is not feeling ill okay this aspect of anthropomorphization is going to come up three times today it's going to come up in three different branches of effective computing so effective computing itself is a branch of human computer interaction and inside effective computing researchers are often trying to create interactive systems that recognize emotions in their human users or the flip side of that enabling the interactive system itself to give the impression of having an emotion giving an outward expression of an emotion like we saw hal doing uh last time and then finally assuming that we can get our interactive system to recognize the emotion of our human user how do we modulate or modify the behavior of the software to make sure that we're evoking positive human emotions rather than causing negative human emotions like frustration and anger okay so we'll look at each of these uh in turn today we'll start with getting computers to recognize emotion uh in the reading for today there's kind of an amusing uh statistic which is that three-quarters of people have admitted to swearing at their computers i don't know about you but i'm definitely in that camp why would we want to enable technologies to recognize emotion well obviously whatever it is that the system is causing the whatever the system is doing to cause the user to swear it should probably not be doing that anymore so there's a pretty obvious reason why we might want to recognize emotions in human users but if we do want our technology to recognize emotion how should we go about getting it to do so we can create adaptive interfaces that recognize and modulate their behavior in motion and of course first they have to recognize it if they do what are the ways in which they might be able to adapt you can probably write down some ideas of your own if the if the user is expressing anger or frustration then the system should sort of tone down whatever it's doing become more passive and let the user assume more control over the direction the system is going most people become angry or frustrated with their system because they're not able to they're not there they've lost control they have minimal control you get the spinning wheel of death and you can't reset restart slow down speed up change direction in the system we might want to in the opposite case provide more information if the person seems curious or interested in what's going on at a given point in time in an interaction with the system if the user is confused provide less information and especially if the system is able to elicit a positive emotion to remember what elicited that positive emotion and be able to come back to that particular mode or function again okay so we're going to get computers to recognize emotion well what are they that's obviously a big question and again could probably be an entire psychology course in and of itself so for our purposes we're going to focus on obviously the outward expression of emotion our interactive systems at the moment do not have access to our internal emotional state that might change in the uh when as brain computer interactive devices become more widespread bci which we'll talk about when we talk about cyborgs towards the end of the course but for the moment we're assuming that most technologies are not under the skin they are on the skin or at a distance from the user and they're going to have to try and recognize emotions from a distance humans have to do this with other humans as well most of our most of our society relies on our ability to recognize the emotional state in others and adapt our behavior accordingly so uh a very uh there's a very um there there was a very uh profound study back in the early 1970s by ekman and ellsworth and what they found or what they claim to find in this study is somewhat controversial is that by studying the facial expression of large numbers of humans across very diverse cultures they found that there were few cultural differences in the way in which explosion express expressions uh were expressed on the face so in most cultures when someone is feeling happy uh they smile and when they feel sad they frown the magnitude of the smile and frown may be slightly different depending on your your culture but generally speaking ekman and ellsworth found that there were few cultural differences in the way emotions are expressed for most aspects of human uh behavior if there are little cultural differences that suggest the root or the cause of that behavior is much older than culture or might at least much older than the currently existing cultures and might therefore be an evolutionary response and there is an uh a whole literature on um what are the darwinian pressures or what are the evolutionary pressures acting on the expression of emotions why why did we evolve to express emotions uh through facial expressions we're not going to touch on that today but from an hci point of view that's this is a promising result because it means we can hopefully try and recognize or infer emotions from facial expressions regardless of the cultural background of our human user the other good news here is that um facial expressions can lead to the quantification of emotion what ekman and elswood actually found was that certain face muscles are tensed during the expression of certain facial expressions which correspond to certain emotions uh eight years later plutchik put together an emotion wheel which is he used uh similarities in the subset of muscle groups in the face that were being tensed to arrange uh related uh related emotions so sadness and dis and disgust are closer to one another on the emotion wheel then sadness and joy are most of us would intuitively expect that sadness and joy are further from one another than sadness and disgust but we might be hard-pressed to explain quantitatively why that's the case in this case the emotion wheel uses a dis this distance of uh overlap in the subset of muscle groups that are being tensed during these facial expressions so to illustrate this i took i took halves of faces from the internet and put them together for these various facial expressions so uh this is sort of my own attempt at an optical illusion here your brain obviously knows uh your prefrontal cortex the logical part of your your brain immediately recognizes that this is not the same face this is not an individual person but hopefully the other parts of your brain can recognize particular facial expressions and what those facial expressions are communicating so what are some of the common features between neighboring facial expressions here obviously we can't see the muscle groups directly in these images or when we're looking at another human being but we can infer it from the deformations of the skin of the face what are some of these common features what are shared by neighboring wedges in the circle between certain wedges in the circle that are not present in the other wedges of the circle if you have an idea go ahead and type it into chat similar mouth groups right so you can see between disgust and sadness we have a frown a downward a downward pulling uh set of lips in anticipation and joy we have an upward curved uh upward upwardly curving lips what are some other features as you remember from or as you'll now know through your use of k n we need as many features as possible the more features we have the better chance machine learning algorithm is going to do at recognizing whether a current face is expressing sadness surprise joy and so on eyebrows so khan what is it about eyebrows that are similar between neighboring wedges and different compared to other wedges so raised eyebrows yeah exactly so raised eyebrows shows up not just in the case of uh of anger anticipation as well or surprise when humans are engaged in social interaction most of the time we are looking into the eyes of the interlocutor the other person that we're speaking with the eyes communicate obviously a lot or they advertise much about internal emotional state what is similar about the eyes in these images and different about the eyes compared to distant wedges in the emotion wheel the faces that i've constructed for the pair of wedges joy and acceptance these faces at least to me seems somewhat similar what is the common feature between joy and acceptance or perhaps satisfaction is a better term here relaxed eye muscles right so you can actually if you think about it you can you can actually pick out in these images which parts of the face are tensed and which parts of the face are relaxed so the fact that we can visually interpret some of these features is promising it suggests that we may be able to construct a machine learning algorithm that can similarly observe faces and infer emotional state from them okay so how would we do this well we would do this in the normal way that you set up your k n or the way you would normally set up a machine learning uh process we have to start with our input data what is our training data um we could of course actually instrument the face or the head of the human user there are many other things other than just facial expression that correlate with internal emotional state our your heart might race your the movement of your body may be different for different emotional states electromyograms emgs which we talked about a couple lectures back they measure the tensing of muscles directly could measure body movements respiration rates heart rate skin conductance a very important feature in a lie detector of course these days now with the improvements in machine learning perhaps we don't actually need these these are probably not going to be acceptable from the point of view of our users remember the hci non-functional requirements one of them is that the user has to accept or be willing to use the system if the system can only recognize emotions if you wear certain instrumentation on the face or head that's probably not very acceptable so we can probably do quite a bit with a webcam just just visual images of the face itself okay once we have our input data then our machine learning algorithm has to go looking for features furrowed browse smiles fro brows smiles and frowns and so on and then our system needs to reason about those features needs to combine them in certain ways to determine to make a prediction about what emotion is being expressed so remembering in your in the case of your k n this reasoning part of the k n is finding another face in the in the training set of faces that has a similar tensing of muscles if we're focusing on facial expression and returning the class label associated with that image which in this case a class label is going to be something like angry happy frustrated uh satisfied and so on so i i there are obviously many different kinds of machine learning algorithms so just for today's purposes i've represented this as a decision tree so assuming that our system can extract features from the raw pixels of an image of a face it might detect teeth if teeth are being shown that corresponds to a subset a subset of emotions if teeth are detected and lips are curving upward then possibly the system will predict that the the face in the image is expressing joy if teeth are detected and the brow is furrowed maybe that's anger someone seems extremely uh extremely upset and so on okay so we might start by creating a decision tree like this at random putting different facial features into the uh into the conditionals here and putting different predictions in the clauses themselves and we might then measure for a given decision tree how good its predictions are so imagine we have two stimuli two separate images of two separate faces we put the first image in and the system predicts joy but the user was actually expressing anger so minus one point for the decision tree second image second face we feed that into the decision tree the decision tree predicts joy that user indicates they actually were pleased so plus one point given the fact that this decision tree got one out of two correct that's not very good so maybe we restructure the decision tree a little bit this would be like learning or adding additional training data to your k n making some change to the machine learning out uh the machine the learner and then hopefully it will do a better job at making predictions okay all right so i again we you can imagine this has come a long way obviously we now have very good facial recognition on our phones and our technologies not only can we recognize the faces of who is in the image but most machine learning algorithms these days that work on this problem can do a pretty good job of predicting at least the expression at least the emotion that's being expressed on the face of the user in the image okay so let's flip this around now and think about the converse problem in effective computing which is trying to get machines to give the impression of having a motion but this should immediately raise the question in your mind of not only can we enable technologies to do this but why would we want to do this in the first place one reason is is a very interesting quirk of human psychology which is that people tend to respond very strongly to objects that seem to express emotions even if we're not sure whether the other that we're observing has those emotions uh if you casually scroll through your favorite social media feed i am sure you will see images of cute pets and cute animals where the owner writes in in the in the post my cat is it seems pretty uh displeased this morning now whether the cat is actually displeased or not is a matter of debate but most of us instinctually feel drawn to the image because of the of the fact that this animal seems to be expressing this emotion this is often an act of anthropomorphization right we are attributing human emotions to animals and in the case of interactive systems we might also be willing or unable not to attribute human qualities to uh interactive systems so what you're seeing in the image here this is the kismet robot built at mit a number of years ago you can see that the construction of this robot was uh deliberately characterized various features of the face so that the facial feature the facial features associated with certain emotions can be exaggerated by kismet i'll show you a short video and in this clip you're going to hear kismet you're going to hear kismet uh repeat the same phrase over and over again and inflect that phrase with different uh facial really think expressions do you really think so do you really think so do you really think so do you really think so okay so there's kismet repeating the phrase do you really think so over and over again why does kismet have ears most of us don't waggle our ears when we are trying to advertise our emotional state to an interlocutor was the purpose of adding ears to kismet so in this example we've moved from human computer interaction hci to human robot interaction hri [Music] we are trying the the creators of kismet are trying to induce or create this feedback loop between kismet and the observer henry says it expresses a motion similar to most animals which humans recognize there's a an interesting argument in uh animal studies which is that cats and dogs domesticated cats and dogs evolved to become protected by humans and the best way to do that was to elicit positive emotional response from uh from humans and dogs at least especially cats to some degree use their ears to amplify their advertisement of emotional state right so kismet is not just represent is really not a strict representation of a human here it's some combination of human uh and animal and like our domesticated uh pets is trying to exaggerate emotional expression to really pull on our heart strings to really trigger this anthropomorphization response uh willem says we're familiar with animals like cats and dogs that do this as well more total input to receive right more total input for us as human observers for the moment to receive right it is pretty clear what emotion kismet is trying to uh trying to communicate in one of the clips in one of the repetitions kismet was very sad do you really think so how did kismet advertise sadness [Music] kismet hangs its head and its ears and its eyebrows everything droops why humans do this to some degree when expressing a sad emotion [Music] why let's go back to dogs for a moment dogs especially exaggerate this particular uh notion when dogs know that they've done something wrong they will often advertise contrition or sadness and hang their head in their ears why do dogs do this there's an important behavior in the canine and canine species that's related here dogs are evolved from wolves wolves hunt and do pretty much everything as a pack and for a pack to work well there is a very clearly defined set of dominance relationships and one way to signal sadness or contrition is to literally bow down before someone else or something else and exhibit sadness or contrition okay so there are a lot of subtleties going on in the social advertisement of internal emotional state and we can create machines that replicate those behaviors which humans tend to expect when they're interacting with other humans or other domesticated animals remember that throughout hci we are trying to match and support humans expectations that they're bringing from the physical or social world into the informational world uh so this was a research project uh kismet was a research project carried out almost 20 years ago now at mit one of the graduate students working on that project cynthia brazil several years afterwards founded the gebo corporation and the jibo attempt was to try and to take all of the mechanical complexity of kismet and boil it down to the simplest possible device something that is almost a cell phone the simplest possible device that still is able to easily advertise a range of emotional states back to its human user i'm going to play you the promotional video just before the gebo launch which was about i think five or six years ago now unfortunately last year maybe it was two years ago the jibo corporation filed for bankruptcy there was a competing chinese company that produced a related product that was slightly cheaper that's how things go but anyways i'll show you gbo now as you watch the promotional video i want you to pay attention to the clever uh hardware and software and how they were exploited to try and communicate emotional state what are the actual features that you can you can recognize in gebo this is your house this is your car this is your toothbrush these are your things but these are the things that matter and somewhere in between is this guy introducing gebo the world's first family robot say hi jibo hi jibo [Music] jibo helps everyone out throughout their day he's the world's best cameraman by intelligently tracking the action around him he can independently take video and photos so that you can put down your camera and be a part of the scene take the picture [Music] he's a hands-free helper you can talk to him and he'll talk to you back so you don't have to skip a beat excuse me and yes you both melissa just sent a reminder that she's picking you up in half an hour to go grocery shopping thanks dubo he's an entertainer and educator through interactive applications gebo can teach let me in or [Music] i'll blow else house in hey where'd you go there you go he's the closest thing to a real-life teleportation device he can turn and look at whoever you want with a simple tap of your finger check out my turkey dinner mom maybe she wouldn't eat that baby pizza i want turkey pizza and he's a platform so his skills keep expanding he'll be able to connect to your home welcome home eric hey buddy can you order some takeout for me sure thing chinese as usual you know me so well and even be a great wink you have a voice message from ashley want to hear it absolutely hey call me when you're home better make that take off for too gebo you've dreamt of it for years and now he's finally here and he's not just an aluminum shell nor is he just a three-axis motor system he's not even just a connected device he's one of the family [Applause] this little bot of mine [Music] okay when i first showed the gebo promotional video about five years ago somebody asked if it was actually an advertisement for a black mirror episode i don't know some people feel cheap is a little bit more frightening than than hal what is your house what features what features does gebo advertise to cast the illusion or attempt to cast the illusion that it has a particular emotion at a given point in time differing facial expressions and use of emojis yeah so obviously uh jibo was was built for our decade unlike kismet he's able to to exhibit or advertise emotions in a wider range of ways the circle changes color as emotions change right that's kind of interesting what expectation is being supported by the change in color that's being drawn from people's expectations of social interaction if you go back and watch this video you'll notice that the circle meant to represent jibo's face reddens when it's happy or expressing the emotion of happiness why is redness associated with happiness abstractly red is often associated with anger but if we're talking specifically about facial expressions face-to-face interaction why is redness associated with happiness there's a physiological connection here blushing right so blushing perhaps if you're embarrassed or you're blushing because you're it's a mixture of embarrassment happiness when you are really happy and you laugh that requires a lot of energy and blood rushes to the to the face right so there's some very subtle physiological cues that are simplified and abstracted but still present in jibo so the circle meant to represent jibo's face changed color it also changed shape in what way and what was it uh what which particular aspect of social interaction was being supported by the deformation of the circle or anything else you noticed in the in the video if you watch it carefully there's 10 12 14 different very simple movements or behaviors that gbo generates that are mimicking various social exchanges between humans between humans or between humans and domesticated animals okay leave you to think about that and we'll move on so assuming that we want to do this and we can enable technologies to do this and we can elicit this aspect of anthropomorphization how could that be useful for forwarding whatever it is that the whatever activity the person wants to carry out with the interactive technology by creating this anthropomorphic bond we're trying to create an emotional bond at least from the point of view of the human user so in the somewhat cheesy video you just saw about gebo the idea was or the hope was that gebo would be able to create an emotional bond between itself and members of the of the family that would be particularly useful for interactions that are painful or difficult or can be boring at times interactions that uh go on for quite a long period of time and maybe this emotional bond or this anthropomorphic engagement will help to make that interaction a little bit more pleasant for the user this is mostly true this is true in a lot of different kinds of educational software so for any of you that have tried out duolingo which is a language language teaching software there's duolingo's owl this little mascot that that sort of cheers you on as you as you go right so learning something can often be frustrating boring requires lots of repetition if you have something that it seems to be engaged with you on a social on an emotional level during this learning process often the emotions being expressed by the system are sort of messages for i know this is difficult for you i feel your pain i'm bored too i'm frustrated too let's just do one more exercise and so on there's also an application for these kinds of technologies in physiotherapy and therapeutic software we're going to talk about robots in a couple of weeks you'll see some examples of physiotherapy robots and therapeutic software where at least on the point of view of the user as they're as they're rehabilitating there may be some actual physical pain and physical pain is obviously associated with extreme emotions and it may be useful if there's a interactive technology involved that is actually asking you to carry out an action that's that's physically or emotionally painful that the system itself is advertising that it recognizes your emotional state and it feels sorry that you have to go through this uh this process i know this is emotionally difficult but it is also these kinds of technologies are also seeing application in exposure exposure therapy which is used for um used for therapy for anxiety and for ptsd one of the very first applications of this was for veterans returning from the gulf war in this case they were placed inside a simulated vehicle a simulated humvee and they were looking through an empty plate glass window at an empty wall even sitting in a vehicle for a lot of returning war veterans that was an extremely anxiety inducing experience they were in this case instrumented with uh with electrodes that could measure heart rate heart rate and skin conductance and the system would measure the emotional state or at least the physical arousal of the of the patient sitting in the vehicle and would do nothing the system would just wait until they gradually acclimated to sitting in the simulated humvee as they did the system would would increase the virtual reality or the augmented reality of their experience by projecting an empty desert scene onto the quote-unquote windshield of the simulated humvee which for many patients would immediately cause anxiety to increase again the system would wait until they acclimate to what they are seeing or what they're experiencing as they do the simulated vehicle would start to rock back and forth and provide uh the physical sensation that the vehicle was moving so one of the challenges of exposure therapy is reaching just that limit that induces a little bit of anxiety but not too much anxiety just enough to allow the patient themselves to gain control over their experience and be able to calm themselves down and convince themselves that they are not in any clear and present danger system like that as you can imagine is extremely emotionally fraught for the human user and a calm interactive system that is demonstrating that it knows uh what the emotional state of the patient is and possibly expressing its own emotional state it is also feeling what you are feeling may be useful in such situations obviously this has to be done very carefully but there is an application for affective devices in in such a domain okay so uh last part here is obviously uh in some cases we would like to not just minimize negative affect uh anxiety fear frustration and so on but maximize positive affect so designing for pleasure joy satisfaction and so on like many times before in this course we're now dealing with something that is very subjective what does it mean for the user to have a pleasurable experience with their interactive technology that's a very joy or pleasure satisfaction is a very vague uh subjective term we're going to as usual try and unpack this into specific aspects of pleasure in this case and ways we might be able to actually measure these things okay as usual we'll start with some of the more obvious aspects a pleasure like physio pleasure so this arises from the observation or the actual physical manipulation of an object so apple got this right right at the beginning they recognized that computers certainly desktops back in those days had to not just be functional but beautiful and people would be willing to pay more money for a beautiful and functional machine over just a functional machine and clearly if you look at the product line over the history of apple all of its devices take this idea of physio pleasure into account in some way what are some other technologies apple or otherwise that elicit physio pleasure physio pleasure they're beautiful to look at they're satisfying to manipulate or hold or plug together snap together you think of some other examples here right at the beginning of the course we talked about uh ergonomics which is how to design things that are beautiful or elicit this kind of physio pleasure from from their users so mac track pads right the the the size of a trackpad on a mac has varied over uh over the lifetime of that product line and getting that size right size itself is an important feature of physio pleasure cell phones for a long period of time got smaller up until recently and they got a little bit larger again there's something about uh holding it as simple as holding an object in your hand that it takes quite a bit to get that right for that to be a pleasurable experience what is the what is the best size for a cell phone given the the advancing technology now we can probably pack quite a bit of functionality into an extremely small phone but if it's too small it's not exactly pleasurable or or convenient to hold in your hand okay social pleasure obviously having a great a great discussion with a friend or a challenging argument that you win at the end there is clearly much pleasure that humans derive from social interaction and we can all feel the inverse of that during the pandemic era right lots of our social exchanges like in-person social exchanges have gone away and most of us are feeling the adverse emotional impact of that most most technology most computer technologies or a lot of computer technologies have been designed to try and facilitate social exchange or create new forms of social exchange whether that's actually improved your social relationships with friends and families or allowed you to build a bigger social network than you had uh before hard to say okay i think that one is is obvious right all social network uh all social network apps are examples of trying to maximize social pleasure psychopleasure this one is a little bit more elusive a little bit difficult more difficult to measure whether it's working or not which is the cognitive or cognitive and or emotional satisfaction you derive from achieving something that was challenging so learning a new programming language learning math learning a new language learning to get things done more efficiently there is a pleasure that you can derive when you overcome those but in the process of learning a new programming language or a new natural language it can be extremely frustrating boring at times so this one is very difficult for an interactive system to track because the vast majority of the process the user the human user is likely expressing uh furrowed brows and negative emotions swearing at the computer and so on so how do you know how how would an interactive system know throughout that process whether those negative emotions are actually useful they are bringing the user closer to their goal which is learning the language getting things done more efficiently and so on so think about perhaps your asl educational game that you're working with or duolingo if you've ever used it or any other educational software or tutorials imagine that you were to connect a webcam to that educational technology and it's inferring the facial expressions of the user over time most of those frames of data are going to register negative human emotional state on the face what might it be looking for in that data to indicate that at least the user is making progress we're on the right that we the interactive system and the human user are on the right track are they frustrated uh because they're trying to learn a particular concept in the language are they frustrated because the system just isn't working they're not they're not making any progress on learning what they want to learn it's a very tricky set of emotional states to distinguish between how might you go about doing that if you have an idea go ahead and type it into chat imagine you're watching someone who is trying to learn a language they're learning it by using an interactive technology or textbook doesn't really matter what is happening what is fl what is uh flitting over the face of uh of a student as they are attempting to learn something that's difficult what hints might you get along the way that despite the fact that it's mostly frustrating and boring they're making progress they are deriving psycho pleasure from the process what do you do as you're working your way through several hours of homework but intermittently feeling excited that you've learned something that you've made progress that you've moved on to the next lesson now let's go back to the emotion wheel for a moment most people most people they're learning something that are difficult they're making they're frustrated and they're bored but suddenly they make progress and there may be a very brief flash of joy or happiness there's often a lot of anticipation i wonder what's going to happen next i'm making progress what's coming next i got past this difficult part there may be certain things you can look for there are behaviors beyond the face right the fist pump of i finally figured that out in the case of your asl educational game you could imagine that your system is active and periodically queries the user to see how they're doing how might the user very quickly indicate to your asl educational game that they're doing well they're enjoying what's happening they're making progress it's okay keep going so as khan says in programming when you finally get something to compile and run successfully that's great absolutely that's easy enough for the system to detect because it's happening within the computer what is happening on the face what is happening on the user's face the moment the compiler reports zero errors that flash of a millisecond the system might focus on exactly that moment the system knows at what moment in time this is your code compiled correctly and at that moment it may take a photograph of the face and see something like acceptance or relief as julian says absolutely right as a coder i know sometimes the best cycle pleasure is exactly at that moment where when a buggy is some buggy code finally works right so the system can make use of context in this case the context around encoding to focus in on particular points in time where there's going to be a very informative signal if the system finally compiles and the user is still frustrated or bored that's usually a good signal that um they just they have to do this there's homework that they have their their homework is an hour late it doesn't matter anymore i just need to push through this i'm no longer deriving psycho pleasure from this process it's just something i have to get done there are these very subtle emotional cues that can be very useful from the point of view of the interactive system about how to alter the interaction to support the activity of the user the activity might be i just want to learn this for fun i'm enjoying what's happening or i have to learn this i have no choice those are very different activities okay as usual we move on to the last aspect of this this aspect of human behavior which is ideological pleasure much more difficult uh to measure which is pleasure derived from doing things or achieving things in a way that conforms to our core values so in hci this is often a user choosing to use open source software rather than commercial or closed software using green technologies running machine learning algorithms that have a smaller carbon footprint than other machine learning algorithms this might not be something that the interactive system can detect directly it may simply be a choice made by the user they choose to use this system over another system but if you're working for a company that's developing interactive systems that's an important aspect of your human dem your demographic your user base to to keep in mind okay that concludes our discussion on affective computing and it also concludes our section on cognitive psychology as promised this is very short we left a lot of aspects of human behavior that are relevant to hci untouched we're going to switch gears again now and enter into the longest segment of this course which i've nicknamed looking outward so in the the the majority of the history of computer interaction it was one desktop one human no internet just one-on-one between one computer and one human but of course one of the most exciting things about computer science and in our society in general which is that over the last 10 15 years we have been developing generations of technology which are moving out into the world and this outward movement takes many different forms and we're going to look at those different forms in this lectures in this in this sequence okay we're going to start today with looking at a particular way that technology has moved outward which is that a technology is now not interacting with just one user it is interacting simultaneously with many users or from the point of view of the human user they are working alongside many other humans and collectively contributing to a collective task the activity here is some collective activity that would be beyond the ability of any one human user so that human is going to want to participate in a team and use software that supports that team and makes it easier for the team as a whole to solve that problem and that's crowdsourcing okay so we're going to do we're going to do this lecture chronologically starting in 1999 does anybody did anyone ever work with seti at home it's still going i think no maybe a little bit too old for for most people in the room here so seti at home uh seti is the search for extraterrestrial uh intelligence the city organization got access to a number of radio telescopes here uh in the us and drew in in back in the late 90s so much data that there was no way that the computers that seti had available to them was going to be able to crunch through that radio telescope data looking for regular patterns so as the name of seti implies they're looking for little green men out there in the universe and they're going to do so or they still are doing so by looking at the background radiation of the universe and trying to pick out regular patterns that are probably not natural they would have to be made by an external uh intelligence so uh uh the people at seti came up with this idea which is at least in the late 1990s most people had desktops and for a lot of the time those desktops were unattended people were doing other things and the computer would flash up while the computer was not being used a screensaver so they figured they could exploit all of that unused computation to distribute the task of looking for artificial signals in radio data to those computers the uh screen the the um the the screen the screen saver had obviously had these very engaging graphics they were meant to communicate to the user you're doing something very complicated something you know very very difficult to understand but if we're lucky we'll actually find alien intelligence out there in the universe this was clearly pulling on people's ideal pleasure right people's ideology which is some of us would are have a burning need to know whether we are alone in the universe and set it home for the very first time made it possible for people to directly participate in that adventure so if you ran city at home you felt as if you were an active participant in this process but unfortunately it was an illusion you you the human user were not actively involved your computer was so seti at home was exploiting distributed computation but not distributed cognition they weren't using the brains of their human users they were just using the cpus of their computers regardless it was extremely successful they recruited a huge user base again as i said i think this is still ongoing today a year later um berkeley uh some researchers at berkeley realized that you could use this idea not just for looking for little green men but you could subjugate this uh this technology to support other scientific endeavors like the protein folding problem so the protein folding problem if you've ever take a bio class as you know is that you put together a series of molecules in a one-dimensional chain creating a protein and depending on uh and depending on the nucleotides in that in that in that chain that chain will fold up into a particular three-dimensional shape and the three-dimensional shape of a protein dictates its behavior what it does it turns out however that the problem of predicting the 3d shape from the sequence of uh sorry peptides on the on the protein chain is known it can be done but it's extremely computationally intensive to predict it because the computer has to simulate the folding process itself so the berkeley team created folding at home which would allow as a screen saver your computer to fold various uh proteins this has been extremely successful uh project it's been ongoing for 20 years there's been almost 140 research papers that have come out of folding at home folding at home today is now being used to fold proteins and i think it was last week or as early this week that the berkeley team announced they found a protein that folds and partially suffocates covet so the berkeley team generalized this technology to the berkeley open infrastructure for network computing or blink in the boeing system which allows other researchers working on things other than protein folding to encapsulate their research program and embed it into boink and then a user who downloads boink basically can select from a menu of different research projects which ones uh which ones they would like their computer to work on so in this example here i don't know what w can usually guess what these are washington university i don't know what that is prime grid so computing prime numbers for those that have a mathematical inclination you might derive some ideological pleasure from contributing to the search for ever larger prime numbers or helping to try and prove the collats uh conjecture someone has their mic unmuted does somebody have a question or comment yes i'm sorry i it's hard to hear you could you repeat your questions i'm not sure if that was a question directed at me okay let's uh let's carry on okay um let's talk about the uh esp game uh which was released to the net in 2004 this was a combination of gamification and uh crowdsourcing just give me one moment here i think somebody's mic is unmuted no okay uh sorry this was a combination of gamification and uh crowdsourcing and uh the game here was uh two users were men randomly matched up uh on the internet and uh they were both shown the same image such as this landscape that you see here they were given a number of taboo words words they weren't allowed to type in but then they were asked to type in in with single words what do you see in the image and the game was not just to type in words that you see in the image but to type in words that you think the other person will type in when they see this image so in this cartoon example here the pair of users both typed in the word tree and so there's a match on the word tree they got that correct they were shown a second image and asked to type in words that described what they saw in the image and so on why was the esp game created what was it uh what was it designed to do why ask a very large number of users to describe a large number of photographs absolutely so this was uh this was the very first attempt to try and create labeled and labeled image data set for the emerging uh deep learning machine learning algorithms so machine learning back in the early 2000s it was becoming clear that machine learning algorithms could work but it was hard to tell which machine learning algorithms were better than others because they simply didn't have enough data so a group set a group from carnegie mellon uh sat down to figure out how do we create a data set with a million or more images in them and for which there are words associated with the images so uh so now if we feed an image into a computer and a computer spits out a word the computer's prediction about objects in that scene how do we know whether that prediction is correct or not we have to have words associated with those images from humans that's what the esp game was designed to do uh david says now we just do the same thing to prove to our computers that we are not robots that's true uh the person that created the esp game also created uh captures that's true okay so part of the part of the machine learning revolution owes its uh owes its thanks to uh crowdsourcing and gamification we just talked about the folding team at berkeley um what was noticed back in uh when this was launched in the early 2000s is that the berkeley research team started to get emails from frustrated users who said they were watching their this screensaver and they were watching how the computer was trying to fold this string of beads and the users claimed in the email that they could do a better job than the computers they could see where the computers were making mistakes or having a hard time figuring out how to actually fold this thing into a compact shape and they claimed that they could do better the berkeley team got a few emails like that then hundreds of emails like that then thousands of emails like that and realized maybe it would be worthwhile actually testing the question test the hypothesis of whether non-experts these are not biochemists just random people on the web could actually do a good job of protein folding so they created the game called foldit which again i think you can download and still play today where basically you're showing a beginner puzzle a relatively simple short polypeptide chain and you're asked to fold it into a compact shape you can wiggle things which represents brownian motion to allow the system to lock into a slightly more compact configuration and so on the berkeley team solved some of these folding problems already and put them on there to solved problems and if you also solve those problems you got points you moved on to the next one and so on until you got to comple sufficiently complex proteins in which it was not known what the 3d shape or the conformation of that protein was but if there were many people unfolded that came up with the same solution the chance that that would happen by chance is extremely unlikely so you could actually look at the crowd itself and take the consensus to determine uh that that is the correct fold for that protein they could then take the super computers operating at berkeley and actually fold that protein and see whether the computer did any better than humans and it turns out that at least collectively large numbers of non-experts can do a very good job at the protein folding uh problem kind of an interesting result okay so uh these various crowdsourcing and gamification programs started to capture the attention of darpa which is the defense agency research project uh i'm missing something here defense agency research project agency uh anyways darpa is the research arm of the department of defense as you can imagine the department of defense is interested in trying to recruit large numbers of people to solve a collective search task in a very short period of time if some unexpected event happens how could you quickly recruit a large number of people and ask them if you see an x y and z please report it to the authorities as soon as possible darpa was interested in developing technologies that would help people spontaneously coordinate their action to solve this problem so in order to test this idea darpa created the darpa network challenge which became nicknamed the red balloon challenge the red balloon challenge works as follows darpa announced that they would give forty thousand dollars to the team that was able to locate ten eight foot red balloons that were going to be placed at 10 random locations in the continental united states and the team that reported the gps coordinates of those 10 balloons first would win the 40 dollars uh so darpa uh announced this challenge they announced the date that they were going to place the balloons um darpa prepared uh to leave the balloons up for about a week and to accept submissions up to two weeks after the competition on the day of the competition um in nine hours a team from mit successfully reported the gps coordinates of all 10 red balloons much much earlier than anyone was expected would be possible so some of you may know about the red balloon challenge you might know about the mit team and how they solved it if you do don't type it into chat yet for those who haven't seen the answer i want you to take a moment and think about how you would solve this problem let's imagine you read about this challenge on the internet and you get together two of your friends who are good coders and three of you decide you're going to go about trying to find these 10 red balloons how would you go about doing it what set of technologies would you pull together what code might you write how might you recruit other people to your cause to help you find and report the positions of these balloons remember that this was carried out in 2009 at this point social media existed but you couldn't really go to twitter and type in big red balloon and instantly find people reporting hey in my backyard or in the park near my house there's this weird large red balloon today that would probably be sufficient to find all 10 balloons in nine hours that it wasn't possible back in 2009 i want you to think about how you might go about solving this and if you start to have some ideas no matter how rough and unpolished they are feel free to go ahead and type them into chat just remember if you know what the actual answer is how the mit team solved this problem do not type that into chat okay go for it and see what you can you come up with okay you can go ahead and continue to type in your ideas if you have them i'll start with alex's comment here searching through online news sources around the same time they were put up to see if they got any coverage that would probably work today but in 2009 um not ever there weren't as many people posting to social media as there are now that that wasn't the answer it wasn't sufficient to find it not all the balloons were were reported by someone publicly on the internet willems at the top says make a webpage offering 39999 to someone else to find them for you so willem's solution is to incentivize with money someone else to find the balloon what is the probability of any one person finding all ten balloons it's pretty much close to zero so if i saw willem's advertisement and i understood what he was asking that he'd give me 39 000 uh if i found all 10 balloons and if i understood the problem i would realize the chance that i'm going to find all 10 balloons is close to zero so it's not worth my time even though 39999 would be a nice thing to get so incentivizing people to find the balloons is unlikely to work william says you could also get an extra dollar by giving them to darpa instead of me okay okay so henry says maybe fifty dollars for the first person to find a single balloon okay so what's my chance if i if i see henry's advertisement what's if i say oh i could get 50 bucks if i find one of these balloons again if i understand the problem what's the chance that i'm even gonna find one balloon assuming i live in the continental united states still pretty pretty slim so it doesn't make sense to incentivize people financially to find all 10 balloons it probably also doesn't make sense to incentivize them to even find one balloon what what should we incentivize people to do would be easier for them to do where they would still get a financial reward you can tell someone i'll give you x dollars if you find the balloons are eight feet in width they put them in parks and uh i think mostly parks yeah you can financially incentivize people not to find balloons but to spread the word so as henry says here spread the word to more people so henry might offer a financial reward if i see henry's uh advertisement and i advertise and i pass along the advertisement so work together with people physically near the other balloons okay if you know where the other balloons are we're out of time for today do not look at the next slide in this slide sequence if you're interested in this problem see if you can actually sort of write down a simple algorithm that you might deploy on a social network to get this to work i will see you next tuesday uh remember that you have deliverable seven due this coming monday evening you have a quiz due tonight and i wish you a good rest of your day thanks very much everybody bye bye 
WWBH56Rd2-o,27,"This lecture continues our discussion of social HCI and social computing. We also begin discussing artificial intelligence and machine learning.

This lecture is part of the course Human-computer Interaction (CSE 4663/6663) taught by Zack Henkel at Mississippi State University in the Fall 2020 semester.",2020-10-20T20:09:53Z,HCI 2020 - Lecture 18:  More Social HCI and AI/ML,https://i.ytimg.com/vi/WWBH56Rd2-o/hqdefault.jpg,Zack Teaching,PT1H12M54S,false,19,0,0,0,0,hi everyone you able to hear me okay great oh yeah like you were early nice um all right let's go ahead and get started so we'll start with just a couple of course logistics things most importantly exam 2 is due tonight so be sure to finish that up and get it in tonight before midnight on canvas are there any questions about exam two i've seen a couple people have submitted it so okay good there's a question let's see in questions three and four do we need to sketch a low fidelity prototype or explain the low fidelity prototype's purpose has six points how long should the answer be so um you don't need to sketch it if you if you want to to sketch it out that's fine you could you can write a description of you know what the low fidelity prototype would be like oh it'll be index cards with this and that and you know this is how we'll use them as a prototype um your answer doesn't have to be super long there you know just just enough to describe the prototype fidelity that's a good question other questions okay another one okay so when we have a robotic bartender prototype design should we talk about a mobile app for the bartender robot or talk about the hardware of the robotic bartender or the station app design so you can really choose any of those i would probably focus more on either the mobile app or the kind of kiosk station design and talking about that but you could definitely discuss you know how the robotic hardware might be involved in that so if you're you know thinking about ways it's going to deliver a drink or something you might say like oh i've got a counter set up with you know numbers and different areas or something you know whatever is relevant to the design they're probably focused most on the software side of it it's a good question anything else how do i want number nine answered let me see what number nine is i want it answered correctly it's probably let's see okay so [Music] uh number nine is the uh designing a study for two versions so for that just be sure that you address the the numbered items there what you would measure how you would analyze the results in any potential confounds or biases that might exist so it doesn't have to be a super detailed or lengthy answer but you would need to describe kind of how you would set up your study and then what you would measure and how you would look what results you would get and how you would analyze those so what they would mean if if they came out one way or the other is that helpful is compare and contrast okay yeah that that would be fine um so you know it's up to you um so the question is just that you've got these two versions to compare so it's up to you to decide like if you want um it to be within participants where a person would try both versions of it or if you want it to be where a person only sees one version of it those details are up to you so yeah however you want to do it kind of lots of right answers as long as you kind of address these points it's a good question any other questions okay if you have questions uh throw them in the chat um as we go and i will ah yes okay how detailed does number seven need to be let me see number seven okay so seven is the design task um so it doesn't have to be um incredibly detailed and polished like so it doesn't have to be artistically a masterpiece but it needs to have at least the functions that are described so placing an order tracking a status of an order and seeing past orders those all need to be represented and then there needs to be some interactive element to the prototype so even if that's you know just clicking on tabs and it navigates to the page of you know past orders or current orders um you can kind of you're you're free to go as far as you want to make it a nice uh polished design but um as long as it captures the concepts and shows the user flow then that's that's uh perfectly fine okay if you have other questions just let me know and i will get to them um so let me go ahead and transition quick announcement um i just saw this before class uh so adobe's having their max conference right now where they update all of their adobe things and uh xd is has some updates available so it seems like the main thing they're pushing is this 3d transforms so everyone should you know if you're using xd go make 3d transforms um it's kind of interesting but there's a few other features that didn't seem that great anyways um oh and before we get started um i am working to get you all uh feedback on your projects um so i'm hoping to get that all done tonight and i'll post it on canvas um that'll be my feedback and then the peer feedback from the peer evaluations um i know that i owe you guys several things that are in the grading queue right now so i'm trying to get through those over the next couple of days so we'll um we'll talk more about kind of what's next on the project path um on thursday um but i'll uh i'll try to get everybody feedback by tonight um let's just have to go through all of the projects and uh there are a lot of them so but they're very good so um overall everybody's uh is doing a good job so all right last time we talked a little bit about visualization of you know study data or just data kind of out in the wild and how to make sense of that and good and bad visualizations um and then we started on social hci um so these are just some of the high points of visualizations that we talked about just that they're helpful to you know explore data and see what's going on to find weird patterns and kind of investigate further pointing you to a couple of examples of of good visualizations you'll get a chance to do some visualizations on the third exam um that's kind of the main part of it um we'll be doing some visuals but we can think about that later um and then we started talking about uh social hci um and that's where we're gonna we're gonna jump back into social hci today we're gonna finish that up and then we're going to look at uh ai and machine learning um and how that relates to hci um so let's go ahead and get back into social hci so um kind of started off by saying you know obviously humans are social spend a good amount of their time engaged in social activities and we kind of introduced this idea of technology mediating social interactions and so i'm going to use the word social computing some today that's not anything super special that's kind of just referring to using computing for social reasons or having it mediate social interactions in some way so some of the topics um that we talked about um last time we began by talking about facilitating social interactions um you know with technology so whether that's something face-to-face or something remote we talked about kind of all of the different aspects of communication when people speak face-to-face and how it's quite an involved and detailed thing but luckily we have a lot of data about that we've had social scientists who've studied people and observed patterns and behaviors and quantified and described all of that for us and that's helpful when we start to design things that interact with that process so we had started to talk about remote interactions using technology so you know text audio video and we'll pick up there and then we'll talk a little bit about telepresence um social media networks and then um co-presence and then we'll talk about uh social agency of machines so let's jump back into remote conversations um so there's a lot of research uh in this area of you know supporting people um having a conversation with each other um when they're physically separated um physically at a distance so uh you know some of the historic ways we've done that you know we've had well i guess just writing letters more modern emails or text messaging audio calls video calls all of these things that somewhat mimic um conversation that you would have face to face but also usually add something or constrain something in the process so we talked about some of the you know common examples that we see today that most people are familiar with so things like skype facetime zoom being video chat applications that are kind of available widely and we discussed a little bit about kind of that visual channel obviously adds a dimension to the audio channel and that can be both good and bad um and that there's you know things to think about um this example here of um so young children um it can be especially helpful to have a video feed so you can kind of see that nonverbal channel um and so that's that's helpful in comparison to just the audio feed um so then we we started to talk about um kind of something that goes beyond just the person to person or you know small group to small group conversation and that's looking at uh social networks um which have seen widespread adoption um and uh are used um widely so kind of one of the major examples is twitter twitter's used for a ton of things today and you know it often breaks news it's used to gather data about you know maybe how people feel about something or to see who's affected by a disaster or a storm or all kinds of things and then we we started to talk about how social networks are a little bit different in that they tend to uh most of them and we'll go into a little more detail today tend to allow you to broadcast out uh to a broader group um and that they're kind of subject to inaccuracies biases and and just you know straight-up attacks attacks in terms of people spreading information that's maybe not true or that's not fully informed and and certainly in the past few years that's been kind of a major mainstream issue that people have discussed and you know with looking at how these networks distribute information and how they should behave within kind of society so last time i mentioned this story about twitter had had imposed in facebook as well um some blocking on a particular new york post story um that is related to joe biden um and since we we talked last um i believe twitter at least has completely reversed their stance on this so they kind of took a a strong approach to banning this particular article and then they kind of you know they got a lot of pushback for that obviously this is in a political context in an election year in the u.s so it's kind of the worst of everything coming at them but this is kind of uh really taps into that thing that we've seen you know so many people focused on whether people in news media or people in government about the content that's shared on on these social networks and how that should be moderated or regulated and and i suppose we'll continue to see that process play out and that that's an interesting that's a place where hci is very much at kind of the intersection of you know these very important societal issues so you start to have um not only kind of ethical and moral things come into this but also the the legal side of things so if you're you know looking at the u.s context right you've got to you've got to take into consideration what laws apply what constitutional rights might um you know be impacted by some of these choices um so and and this is you know um obviously people could without you know technology or computation could find a way to to do things let's spread information like this but definitely in this case the technology is a huge facilitator of making this possible and so that kind of puts it into the realm of something that uh people who design technology like this need to think about um and so uh you know kind of what that looks like for hci like in practice is you know um think about some of the the social media networks that now will um they'll do fact checking on you know an article that someone shares right um and if it meets certain conditions you know based on however they do their fact checking they may apply a label to it you know they might add a an element in the user interface to alert people that you know our fact checkers you know think this this information is not accurate or maybe you should look at this too and consider it with it and we've seen a lot of uh a lot of kind of iteration and approach uh development in that area um over the past few years as as these social networks really take on this area and and what's the best way to do that so in terms of a user engaging with your interface how do you tell someone that the thing they're about to share is you're assessing it to not be true or you want to urge them not to share it um what what effect does it have depending on the way you do that right um and and we've definitely seen reactions from both users who have been affected you know by things like that and users who see things like that when you see something posted and you see that kind of editorial information has come in from the platform or the the interface people certainly have a reaction to that um and it's kind of become a big area and so it's interesting to see the different approaches that have been taken and it's it's something that i don't think we're anywhere near um anywhere near an answer or that will necessarily ever be near an answer but we certainly are evolving our approaches and how we how we deal with that kind of thing so i think this is where we left off last time we were going to talk about um a study that facebook did i think in 2014 and um so you can see the headlines here of you know facebook emotion study breach ethical guidelines deliberately made people sad and so this was uh an article that um facebook published in one of the bigger social science journals um and it got a lot of uh backlash so this was this was in 2014 and i remember this happening and it was really upsetting to me um as as somebody who does um social science research the kind of the the way that this played out so to give you kind of a high level summary of of what happened um facebook um wanted to see if they could um manipulate um well they wanted to see if they could have this concept of emotional contagion is how they they described it so what they did was they selectively manipulated people's news feeds to have more or less um emotional content um and negative or positive post um basically um and then they they looked at what people posted and how they behaved in each of these conditions and they they found that they were able to actually have people kind of post you know more negative things or to be in more of a negative state when they kind of flooded them with just the negative and then the opposite with the positive and kind of the big outrage here with this was that facebook didn't tell any of their users that they were manipulating their their feed in this way for this purpose um and you know at first you kind of think well okay facebook they have this feed of activity that's happening how they arrange it you know that's their business but so they said that you know the terms of service that they have with any user is enough to protect them so to give you an idea if we wanted to do this study um you know like at a university lab or something we'd have to go through an ethical review and it would be problematic that we're going to try to make people sad or unhappy and we'd have to talk about a way how are we going to mitigate the risk or how are we going to make sure that there's not too much harm to anyone in this study um and so a lot of the kind of the upsetting part of this was that facebook didn't do that um they just said that their terms of service were enough and that they didn't have to do the ethical review so the question why did facebook conduct and publish the article so facebook does a lot of research there they do research on a ton of things that you wouldn't even imagine but in this case um this was just you know one of their research projects um looking at how emotion uh can spread through social networks um and so from a from a business perspective you know i'm not entirely sure what they would hope to gain from this i'm sure there are things that that this is useful for but you know they do with their research they um the people who work there who do the research like to publish their work and that's something that facebook uh you know does and a lot of companies do but and in this one they were also working with the university collaborating on this project so yeah it's just kind of a regular thing that they would do is to publish this kind of work i don't know if they realized how controversial this would be and you know the we've kind of largely moved on from it at this point but it is you know it's something worth noting right um so you know facebook being software um that's you know able to have an emotional effect on people is able to moderate how people feel and and influence kind of their social uh interactions that they're having so definitely you kind of see that these platforms do have uh social power um and that they can you know manipulate things if they'd like to um which is interesting um so just to talk a little bit more uh broadly about um what we'll call social network sites so sometimes in literature you'll see um just any kind of social network twitter or facebook or instagram snapchat whatever those they fall under this category as a just a sns a social network site um so some of the questions that we're interested in from an hci perspective um one big one is you know how does um use of these um platforms affect people's interactions um face-to-face interactions you know um are people more or less likely to interact face-to-face when they have these platforms available um do they you know do this in addition to their normal interactions things like that and so kind of a spoiler alert here um when we talk about the effects of of social networks on on people there are studies and data usually to support both sides of the picture to say that you know they're both harmful or help and helpful so that's you know that's good people are still investigating it and and trying to um fully detail it um and there are upsides and downsides as far as uh we can tell uh to using these platforms um so for example uh this next question of how does you know social network use affect well-being um so kind of an interesting study um and this is just from one article so you could probably find you know other data that that may say kind of the same thing or may uh give you something else to think about but um this article looked at um the difference between composed and targeted communication so like if you use like the messenger app or something to have a conversation with somebody um and it's a very one-to-one um very interactive um communication with them over a platform um then that's generally a has a positive effect on people's well-being whereas if you spend more time just doing kind of wide broadcast you know like say you're going to tweet something out to the world and you only get you know you might get heart feedback or you might just consume things that were broadcast and provide that one-click kind of feedback that actually if if you kind of spend all of your time doing that that tends to be a little bit more harmful um for well-being so that's kind of an interesting way to divide it and look at the effects um another question we see um is there's this idea of a filter bubble right and that people using the different social platforms uh either through the ai or the machine learning or just through the connections that they have um kind of create their own landscape and and are in this bubble of only um receiving tailored news um that that fits them they don't hear things that are outside of of their viewpoint or outside of their interests their connections um and there have been a lot of studies into this um and kind of some of the the details of it matter um so in general there's a pretty recent study showing that in general people using the social networks do get a varied online news diet now you have to take into consideration what we're comparing that to is you know compared to what they would get news online if they weren't using the social networks and of course individual results vary here but on average this kind of recent study is showing that you know it's it's more varied than we thought um but there's also um the finding that uh the user's own choices tend to play a larger role than any recommendation algorithm or any feed algorithm so like whatever facebook's doing to decide what to show you in shaping kind of what you're exposed to so that's not necessarily you know it could be that the social networks themselves do um reduce the diversity of the news you're exposed to in comparison to say i don't know like watching a news broadcast or uh reading like a newspaper uh website or an actual newspaper but in this case you know when we investigate the question of specifically the algorithms we find that they don't play as large a role as the user themselves so that's kind of kind of good um so another aspect related to social networks is this uh online disinhibition effect and so this is kind of um the concepts that we are familiar with that when people are online they're kind of um depending on you know what the platform is um and certainly if they're anonymous um it it can lead to um more um just expression self-disclosure um especially if that's not linked to their identity necessarily um they're more likely to take actions that they might not take in the real world we certainly see this come up when we deal with anti-social behaviors online so any kind of harassment online or anything like that trolling or flaming or you know kind of whatever aggressions you want to talk about that occur uh primarily or originate on virtual platforms um this effect is at play when that's going on so people are at least somewhat it seems uh kind of recognized being disconnected um and and connected in a way that's different from kind of face-to-face interactions or real-world interactions i guess um and that certainly like i said it really becomes the case when their identity um they don't pursue their identity to be linked to their actions um so that's that's something to think about too um in terms of when we design uh software hardware that's going to facilitate social connections so i want to move on from from the social networks and and take start to look at telepresence so when we think about telepresence these are any kind of technology that's focused on making a person feel more like they're in a remote location and allowing communication to occur so that that can be you know as simple as a video chat kind of thing or that can become more involved in terms of doing something like you know vr or looking at having an embodied robot that allows them to move through the environment physically or combining video feeds to show people in a shared space when they're actually in remote areas so this is kind of the area that we consider telepresence this is an example of just kind of one of the classic business examples of a cisco uh telepresence conference room um and the idea here is that you kind of um you're you've got like half a table um and then you've got all these screens right and then the other people have the other half of the table and it's meant to make you feel like you're sitting you know across from each other in a meeting at this table um from you know remote locations and so you can see this is a lot of specialized hardware and software to make this happen to try to improve the the realism of being somewhere else i'll show you another example this is from the late 90s this is called hyper mirror and it's a a system that some researchers created to create a shared virtual space so the idea is that like you look at a screen um and it's like looking at a mirror but it has added into it um composited in uh the people at the other remote location and vice versa so they you know you all see each other on each other's mirror okay so you can see in this top picture um that you can only see you see three people in that picture and then on the bottom picture you see a fourth person um standing there who's not actually at that location but is is being composited in so i'll show you a quick video of a modern example of have you this come here hey dad you can see from there do something wait i want to find you the facebook page i just hit your head papa before i hit you go go flying okay so that's just an example um of of kind of the the whole concept that's you know what the hyper mirror system did that's like a museum installation version or something that's just some someone's video off of youtube um but i think it's it's a nice example because it it shows you kind of um how people go about using the system um and you know immediately wanting to kind of do the thing that you really can't do which is to like interact with each other because they're in different locations right so they're trying to punch each other or whatever and that's kind of where you run up against these boundaries um on the technology right you start to see that you've got this image and it may overlap in ways that don't make sense um spatially um and that you can't actually have the physical interaction that it might encourage um so it's just it's kind of an interesting example that's you know quite old at this point but but is one idea about kind of bringing people together remotely so this is another early example this is bireality and this is a robot so you can see here on the left that's the the robotic platform itself um it's pretty tall pretty big um and it's got you know screens on each side of it um that are showing the remote person's face and the remote person um stands inside of this uh what they call the display cube um and they've got screens all around them um showing kind of what the robot is seeing wherever it's at so they feel very embodied right like they're they're moving through that environment and communicating with uh whoever's on the other side so that was an early example now we see commercial products um so you know this is a little less immersive right um this is um pretty much video chat with wheels this is a beam robot and it's kind of the same idea so you start to have to design for things like how can you make the user the remote user aware of their surroundings more you know things that you might not think about until you use it for the first time so something like you know how do they know that they're not speaking too loud right how can they get a good sense of what their their volumes like or even how can they get a good sense of how far away someone is um and especially when they start to like navigate the robot right how do we let them do that um in a way that's both safe but also intuitive enough for them from a distance so there's a lot of design problems there and then of course you've got you know all of the um the other aspects just the hardcore technology aspects of you know you need to get the video feed and that requires so much bandwidth and you want the latency to be low and you know there are a lot of requirements right the robot needs power it needs to charge it's what does it do if it loses wi-fi does it just freeze where it's at does it try to find wi-fi those kinds of things um you've got to think about all of that when you design this um so it stretches beyond just the user experience of you know i'm using the telepresence robot from remotely and you're interacting with it you know in person you've also got to think about you know how does the person who takes care of the telepresence robot interact with it you know how do they know when to tell it to go to its charging base or you know how does it inform them that it needs this or that um so there's that whole other side of the design right um and that's um really interesting you also get some some interesting issues like you know okay you can drive around with the robot but it doesn't have any arms or anything so like you know you're not going to be able to push an elevator button or something right um so you might have to ask someone to do that for you um that kind of thing comes up so kind of whether it's a robot or or just like the cisco screens or the hyper-mirror kind of thing telepresence focuses on realism and really trying to transport you from wherever you are to somewhere else but i think a question that that some people have looked at is kind of does the is pushing the realism really the most important thing for the interaction or should we be focused more on kind of what we can add to the interaction um that wouldn't be there um if you were in person so you know what other kind of features can we add that might be helpful to people using some kind of telepresence system because a lot of the realism features we come up against kind of just hard barriers although we can do really amazing things with it kind of the idea is you know we might not be able to do amazing things at scale yet and so what can we focus on that brings a benefit widespread but you know maybe not as much realism so that that's an area to look at kind of related um to this this idea of being remote is a concept that um i want you to know about is social translucence is what it's kind of been called in literature um and this is talking about you know if you're let's say working collaboratively and remotely with someone or with a group of people there's these two dimensions of this it's about finding the right levels of them so the first one is awareness so how you make the status of one person known or visible to others that they're working with and the other is accountability and that's how you let the person who you're telling others about know what's being shared right so you can kind of imagine like if we're working on a document together or something um you know how much detail do you is it appropriate um or useful to share with um other users right you can kind of go from like sharing nothing really um to maybe saying like oh okay you know this person on your team you know updated this file um 10 minutes ago right um or you might say you know they're currently online and they're working on this part of the program or you know whatever you're doing but you can also push it to the extreme where you're saying oh they are on this line of code and they are typing exactly this right now and you can watch them in real time so thinking about something like a more collaborative editor right um and all of these levels may be appropriate um but uh you've got to be sure that people have an awareness of what's being shared and so kind of the hci the design of that is important right how do you communicate um first of all how do you communicate people's statuses in a way that's not obtrusive or distracting and is actually helpful but then how do you let people know what's being communicated to others so that's a big area and we're talking about remote collaboration another thing that we also look at kind of a fundamental thing is just how people coordinate with each other so when people are physically co-located they have a lot of coordination mechanisms verbal and non-verbal and you know they'll use things like pointed things they have shared groundings and that becomes a little more difficult when you're remote so that's you know that's more of a kind of even domain specific thing of looking at how people do something uh together when they're co-located versus when they're remote so i want to touch briefly on co-presence this is the idea of people are in the same physical space but we're going to use technology to kind of help them collaborate or work together so one thing that this can be about is kind of just capitalizing on people's sense of awareness so we generally keep some awareness about what's going on around us so we have a peripheral awareness that's of kind of the social context around us um so we do keep track of you know who's in our environment and kind of some social attributes about them we you know we keep track of uh if our boss is in the room versus um if a friend of ours is in the room right or you know if we're um in a you know somewhere with our parents or our grandparents or uh someone we know versus someone we we do know uh or don't know um these all affect kind of how we behave and we keep an awareness of that as it goes on and we also keep a situational awareness which is you know just knowing what's moving where in the environment and kind of having a a little bit of a prediction loop going there in case something uh there's something we need to respond to so those are things to think about as we think about these kind of technologies that people will use together when they're co-located so this is an example from the interaction design book and this is a an interface that shows a floor plan of an office and kind of gives the status of the people in the office where they are can show you you know who's got a conference room uh booked um maybe the agenda for the day that kind of thing so this is a tool that's for the people that share this environment right to help them organize that and kind of keep track of what's going on um another example that's quite different um from from the office environment um this is a an augmented reality sandbox this is also from the interaction design book um and so you can see you've got you know multiple people can interact with this exhibit which has got sand and it's got a projection on it and it's sensing you know how people are moving the sand to change the projection um kind of a collaborative environment and so we see other things we've talked about um in other lectures you know shared uh interactive displays um we've looked at things like the table that keeps track of you know who's talking yeah this does look like fun i i've never um i don't think i've played within one of these in person i don't think i've seen like things being projected on sand anywhere but i would i would like to mess with that but yeah so a lot of different collaborative technologies that you can come up with and kind of you know we follow the basic pattern right we see what people are doing how we can support them best and how does the technology really enhance that so now i want to move on to a much different aspect of social computing and hci and talk about social agents so up until this point we've kind of mostly referred to the technology as either being a tool or a utility um a service you know to help us communicate with each other to help us uh distribute a message um or to bring us to a remote location um but we really need to think about um the technology itself um being considered a social social agent um being considered um an entity that you can interact with socially which might sound crazy but it's not um so this is a book called the media equation and it's it it sums up quite a few research studies and papers um with the kind of headline being the computers are social actors and kind of the go-to heuristic here is that people will try to treat things in a social way with just the tiniest cues and this applies to technology computers and machines it's kind of an unconscious thing we tend to use our social heuristics and our you know social actions by default um so we kind of quickly adopt it with technology we do it mostly unconsciously it's not that we aren't aware that you know the computer is is not a living social thing it's that it's easier for us to think of it as one and use it in that manner um so i'll give you a couple example studies that are uh discussed in this book um so uh something as simple as there's a study that they did where they had people um evaluate they had them come in and do a task on a computer um and then they had them evaluate how helpful the computer was and the only thing they varied was did they take the evaluation of the computer on the computer itself or did they take it on another a separate computer that wasn't the computer that they you know did the task with and the finding is that although no one can tell you why um they are more positive towards the computer if if it's the computer they did the task with and they're completing the evaluation on the same computer they'll give it a more positive rating so when we talked about doing studies right we talked about how people do this with other people right we want people to be we want to please people we kind of want to you know give them the outcome they're looking for right in this case this is transferring over to just the computer itself in a very simple way right very simple manipulation as we go on and look at this more another simple study that this group did was to have people come in and they gave them either a red or blue wristband and then they would pair them with either a computer that was red or blue had a red or blue sticker on it right and so just by pairing people with the same color this concept of a team gets created and people actually give more favorable ratings although the task was the same the outcomes are the same to the computers that if they are on the same color right if they feel that they are part of a group part of a team and that's a that's a very social finding so we can abstract this um in a way and there are a ton more studies so one of the uh people who who did this this line of research um was actually uh got to work with him on the survivor buddy projects so um a lot of his work is in this book and there's just a ton of studies that you can basically say if it's some social finding that holds true when two people interact there's a chance that it's probably going to hold true when a person and a computer or a person in a robot have a similar interaction so thinking about that um i want to think about some of the more explicit kind of social actor machines that we kind of invite into our world so i'm going to show you this research project video and this is for a system called buddy [Music] wow [Music] so [Music] oh [Music] okay so kind of interesting yeah okay so a comment there uh what a whimsical tune for a kind of depressing concept yeah so you know are you too busy at work uh to interact with your child well here here's the answer you know um so there are good things and bad things right we can we can talk about uh technology like this um but um it's also kind of on my list of um people trying to find a purpose for a social robot in the home right oh i can do everything it can take pictures it can create a baby journal for you you can talk to your child whatever in a more realistic sense if we think of things even as simple as like interactive toys the kids have which are more and more complex today um you know a lot of them are pretty much just a proxy to a cloud somewhere we see um kind of this interesting landscape where you might have a a companion that is kind of a an artificial computation device and so we start to have a lot of questions about that and if that's helpful or harmful or you know what it's good for what it's bad for and can we even you know do it well um we can't do it well yet but there are a lot of ideas out there kind of similar to the the sentiment of of what we saw here um and so it's it's something to think about um you know how that that works going forward and people have certainly done work looking at this um there's uh sherry turkle um at mit has done a lot of work looking at like children using furbies um and kind of their interactions just with furby uh toys um and and that's a really interesting um set of data that she's captured about that if if you're interested in that kind of thing um some of her books are very good um but we certainly see that uh children and adults treat these things socially and even more so when you start to give them faces and eyes and you know social purpose so i want to show you one more video this is about a technology designed to help people kind of bolster their social skills so this is a project from mit a few years ago so play that [Music] when people talk to each other the majority of the information that's conveyed comes from the way we say things rather than the words we're actually saying eye contact smiles voice modulation speaking rate pauses and emphasis on certain words often add an extra layer of information in our interactions many of us want to improve these interaction skills but don't have the resources to do so imagine if you could practice your interaction skills with an automated system in the privacy of your own living room a program designed at the mit media lab lets you do just that hi i'm mary i'm looking forward to doing your interview my automated conversation coach consists of a 3d character on a computer screen that can see hear and make its own decisions based on its interaction with a person and it works on a personal computer now let's get started using a webcam the system can analyze facial expressions for example it can measure where in the interaction you are smiling and can recognize your head gestures such as a nod or a shake the system also analyzes your voice it not only understands what you say but how you say it using real-time speech recognition and prosidy analysis it can capture the nonverbal nuances of conversations and display it in an intuitive format when you're done it gives you a summary of the information when you smiled how fast you spoke and so on and it can show how these measures change over multiple sessions it even allows you to watch the video of your interactions with various measures of your behavior displayed alongside the video such as when you smile how the volume of your voice rises and falls and what words you emphasize it even shows when your attention wanders in a study with 90 mit undergraduates the subjects went through simulated job interviews before and after receiving this training those who got the feedback from this automated system were rated as better candidates for the job than those who did not besides job interviews the researchers say this system could help with public speaking dating learning languages or helping people who have difficulties in social communications so that's kind of um an interesting use of technology there and it's you know it's social but its whole purpose is to be kind of a trainer for somebody to improve their skills in this case the the study they did was about interviewing for a job and you could see why you might want to improve your skills um particularly you know interviewing skills right um and so in this case they're leveraging the technology that we have in terms of sensing what's going on to give feedback to a person to help them um adapt um their behavior um so this this can be really useful and you could see that you know they had like a an avatar person on there interacting with them giving them that feedback live kind of um to practice the interviewing process so that's kind of another application we look at so it's more of a therapeutic application almost or just a self-improvement application where we're leveraging technology to improve our own social skills the question are these ideas still being worked on yes definitely so i don't know in particular what followed this mit project i imagine it was someone's dissertation probably but i'm not sure who followed them and if or if that person you know continued the research but we certainly see a lot of these kind of coaching applications and you know um we we do see some products right that are kind of self-improvement products there are various apps you know they're kind of focused on psychological well-being um and fitness in that area there's kind of this um either self-guided or a remote person guiding people through like cognitive behavioral therapy through an app or an interface so it's a big area that's still expanding and and looking at things like you know does it make a difference if it's just an avatar on a screen versus having like a physical robot there or something that can you know be in the environment um so there are a lot of questions that are still being studied in this area um but it's certainly an interesting area so just to touch on a few um kind of things to think about when you're designing for social computing and this is um a lot of this has to do with you know like think about if you're designing um like a social um feature as part of an app so you're essentially like you're working on kind of social network like features right um and some type of uh community where people are going to you know connect from multiple places um there are these different aspects um that you really want to think about and you know this is its whole own course really um to talk about kind of designing social computing but i just want to touch on some of them so you know establishing norms so this is important when you set up like a new community or something you how you present what the default or norm behavior is is important we kind of get that in in face-to-face interactions in life you know depending on where we grow up or where we're at we kind of look around and see what people are doing and we follow those norms right it's really easy to do so you've got to think about the equivalent of that um when you're designing these social computing software um you've also got to think about you know kind of the thing we've talked about quite a bit is the anti-social behavior that might occur um on social platforms um you've got to think about how you're going to moderate or govern the content um kind of what the activity that occurs on the network and you've got to think about um you know the different users the different groups of people that are going to use it how your algorithms and approaches are going to impact people right that's incredibly important um and then there's this whole other area that could also be kind of its own course so like talking about how do you get users how do you attract users and retain users and you know whether it's a social platform with just user submitted content that you're trying to build or you know it's something where you're maybe it's like a a fitness app that's trying to keep people uh you know meeting goals and and you know expanding um what they can do um and and healthy um keeping them healthy um and you want to keep users engaged you want to attract users and you want to keep users that's often also a big business point right and so when you get into that you can start to look at different types of motivation that people have um so there's kind of intrinsic motivation that people have um more out of the goals that they have the interest that they have things that are come from their own desires versus extrinsic motivations which are not really based on your relationship with the goal but maybe you know something like money or a grade or a degree or you know things that people need and it's a goal they want to meet but the motivation is different from you know i'm really interested in you know painting landscapes of mountains or something right i just like that so that's you know and more intrinsically motivated goal so you've got to balance that depending on what you're building so you know the wrong mix of that uh can be can be difficult so um there's definitely like a a tendency that we have today to gamify things so like to give badges or points or you know things like that if you overdo that um for something that's an intrinsic motivation that's something people do for the joy of doing it but the you know they like the thing they're doing and that it's kind of a self-reinforcing thing those badges and points and things can become kind of annoying they can they can harm the user experience now there is a point where you do want to transition somebody even if it's they're mostly intrinsically motivated you may want to add some extrinsic motivation as you go so you have to transition that so that's that's its whole own thing but it's it's things to be aware of um kind of a few more important areas um we've got these concepts of strong and weak ties um so you can think of strong ties as people like close friends family people that you have a relationship with that you're going to have outside of whatever software or system you're using right and you'll probably use multiple platforms multiple pieces of software to interact with them um and you'll communicate directly with them whereas the weaker ties which you can think of as just acquaintances or you know people that you know of or kind of have seen those are people that you usually may only interact with over a particular platform it may be more of that broadcast style messaging less intimate and direct feedback so you've got to kind of consider what is your platform or your product which of those kind of ties does it really focus on so you can think of something like facebook right it's pretty much about people's connections just based on social connection alone right whereas something like um like linkedin okay people have a kind of a professional um interest right that is shared with other users to connect them um or something like um like a you know user submitted content like reddit or something right you you join reddit and you add subreddits there are things that interest you um you don't necessarily go and add people um that your your have close strong ties to so that's something to to keep in mind another small thing to remember is that you can actually shape you know how users engage with other people socially over a platform over software which we saw with the facebook uh article that is perhaps not ethical um but like a simple example here of um there's this you know social reciprocity principle if you do like favors for someone they're more likely to you know do something for you it's just kind of a hardwired script that we have socially you can leverage this in an interface and there are a lot of other social kind of hacks you can use to keep people engaged and and to augment people's experience with technology and so i'll kind of leave it with this is to remember user autonomy is kind of important if you're if you truly want to avoid um you know trapping someone or getting someone addicted to your platform in a way that's harmful to them a lot of times you can just think about the level of user autonomy so would the user you know do this on their own if i didn't promote it this way or do this or that and you can think in more in more detail about that but that's an important aspect of social computing is to be responsible with your designs so that's kind of a summing up social hci for us some other concerns which we'll talk about um a little bit in a different lecture um so social hci interacts with things like equality and fairness here's an example of a netflix algorithm perpetuating some racial biases so that's a really important thing to think about and we'll talk about that in detail and we'll also talk a little bit more in detail about things like government and politics and and how hci fits in there so we've got about 10 minutes left so i'm going to start talking about ai and machine learning we won't get through it that's okay because our lecture on accessibility is not that long so we'll we'll start ai and machine learning uh we'll do a couple minutes of it now and then on thursday we'll we'll pick it back up and we'll do accessibility so i will start us off with a video more ai tools more ai tools deploying new ai tools deploying ai tools deploying new ai tools ai tools that we deployed or weren't ai tools build ai tools building ai tools building ai tools we will have ai tools ai tools sophisticated ai tools ai system we will have ai tools you know in general i think over time we're going to move towards more proactive review with more ai tools okay i just always i think that's funny so that's just a bunch of clips of um mark zuckerberg having to talk to congress um about ai tools um so facebook's really into ai tools right now um and that that's kind of a great intro and segue into um you know where is does ai and machine learning uh fit in terms of these platforms that we're making or these interfaces that we're building and hci principles and how does that fit together so we'll start i'm going to kind of give a brief primer on ai and and specifically where machine learning fits into that and how that all plays out and then we'll look at it in the context of hci so if you haven't taken ai here i would recommend that you do it but this is kind of the textbook to go to for any questions about ai so artificial intelligence a modern approach it's a really good book um kind of has all of the basics of ai in it but we'll start uh with defining ai so it that turns out to be kind of a tall order it's a little bit difficult to define ai so here i've provided some of the definitions that um the this textbook which is kind of a important one gives in terms of talking about artificial intelligence and what it actually is um but i'm going to go ahead and just kind of give you a definition that i've kind of settled on for ai and we can all kind of come up with ways of describing ai there's no hard set definition but it's it's definitely a broad area of study that focuses on enabling computers or machines to operate um at levels that previously require human intelligence and i think what's important to remember is that it is a broad moving target things that we considered ai previously we may not think of so much as ai now that they're regularly used so ai is kind of a flexible area so to give you just a a brief high level summary of what's kind of included in artificial intelligence and these are not all of the areas but these are just some of the most important ways of dividing it so problem solving and reasoning this is a big area so things um even as simple as satisfying constraints so like if you have a map and you want to color uh different boundary or bounded areas on the map you know whether those are states or countries or counties or whatever you can use ai algorithms to use the minimal amount of colors to color all of the areas on the map right i don't really think of that as ai so much but but it is um that's kind of where where that comes from um things like if you wanted to figure out where you should put cell phone towers optimally so optimization problems right um and looking at that that's an ai problem you can use ai algorithms to solve something like that also important nai representing knowledge so this is something we don't see as much right now since statistical methods with machine learning are very very popular and very powerful but representing things in terms of symbols and reasoning over those symbols has historically been a big part of artificial intelligence things like planning so route planning planning to get from one place to another how you should get there weighing different options certainly learning so pattern identifications classification prediction that's where the learning really comes in and then perceiving perceiving the world so things like natural language processing computer vision so scene understanding understanding human speech things like that all fall into artificial intelligence all considered ai problems so thinking of examples that are what i call non-ml examples so um things like just straight up search of a state space or optimization like for games like predicting all the possible game states right and then analyzing those and making a choice based on that that's kind of classic ai and then uh things like expert systems which take like um a series of rules and apply them um to inputs um to decide on you know a decision to make a decision or to produce an output early search engines were certainly um non-ml as we as we know it today um and then there are even some basic conversational agents um that that don't use um machine learning uh but use um more just kind of ai techniques early ai techniques to to interact with users so i'm going to pause this here and we'll pick up and we'll do a demo of this conversational agent on thursday but let me jump to kind of our calendar so um you've got the exam due tonight so please do that please get that turned in and then the rest of our week looks like this i am going to start aligning the quizzes and making them available longer since uh that seemed to be kind of common feedback like to have the due dates kind of all the same not have something due every day so hopefully you'll be able to adapt that and kind of keep things lined up and give you you know more time to do the quiz so that'll be good um but that's all we've got going on this week we'll talk about project more on thursday but no deadlines right now as far as this schedule is concerned um and i that's pretty much everything we've got going on now so we will meet again virtually on thursday um and we'll talk about ai machine learning and we'll do a little bit of accessibility as well so that's everything i've got i'll hang out for a couple minutes if you have questions just let me know otherwise have a great day and please turn in your exams tonight you 
XetRWdqygRo,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-26T16:45:13Z,"L21: Gestalt perception. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/XetRWdqygRo/hqdefault.jpg,Josh Bongard,PT50M35S,false,150,1,0,0,0,okay let's get started as always let's talk a little bit about deliverables before we get back to lecture deliverable 8 is due tonight how many people have finished livable 8 hopefully it's relatively painless right at least compared to six and seven okay hopefully things are kind of downhill from here six and seven are usually the monsters I'll try and make it up for you in eight eight nine and ten so let's talk about deliverable nine for a moment so deliverable seven you are adding states to your program deliverable eight you are adding a database to your program deliverable nine you're going to be adding scaffolding to your program we mentioned scaffolding last week I believe scaffolding is an idea that comes from developmental psychology so another aspect of psychology and HCI developmental psychology is the study of human development or the growth from children to adults scaffolding is the rather intuitive idea that if you want a learner to learn something like a child that's trying to learn how to walk you scaffold their environment you'd scaffold the learners environment to make things simple once they start to get a handle on the task then you gradually start to remove the scaffolding so child is learning to walk the child stands on mom or dad's feet mom or dad holds the child and walks the child through the process of walking so the child's nervous system actually experiences the sensor motor processes involved in in walking other examples of scaffolding holding the bike while they learn to bike or even better than that once you get tired of holding the bike trying to move training wheels right so training wheels are the canonical example of scaffolding it exists in lots of aspects of human lots of human domains right so education hopefully the first test of the first deliverable for this class was relatively easy and I try to ramp it up to six and seven right so in interface design this is an important aspect of HCI often you're presenting your user with an interface or a software hardware combination they've never seen before like leap motion and you want them to try and learn how to do something in this case they're not only learning how to interact with leap motion but once they get that under their belts they're also trying to learn the ten digits of the ASL language how can you scaffold that experience for your users in deliverable 9 you're going to be shooting some videos demonstrating that you have added some scaffolding to your program and you're going to be adding three different kinds of scaffolds all three of these scaffolds rely on your programs ability to detect that your users are are increasing and proficiency they're getting better at signing the ten digits so as your program recognizes that they're getting better your your program is going to gradually remove the training wheels okay so what does this look like it's only one page so a little bit shorter than than usual but again this is a little deceptive because most of the onus is on you to figure out the details here the first scaffold that we're going to be looking for is that you're going to start to present the user with an increasing number of digits so maybe you you roll a ten sided dice and you choose the digit seven you show seven to the user along with a picture of what the gesture 47 looks like but you've already forgotten what it is what's the gesture 47 this one okay so you show this the user tries to do it and you're and they get it wrong right the can and learner consistently outputs a digit other than 7 the user is not getting it they're not signing that digit correctly so once they have failed however you show that to the user they're presented with seven again and again and again and again until they get it once they get it now you roll a 9 i dice and pick one of the other remaining nine digits you add that to the set and now the next instance the user is either shown seven or the new the new digit right and on and on we go so you're expanding the digits that you show to the user based on the fact that you've detected that they've gotten better so you're going to be shooting one video to show us this particular scaffold and in that video I want to see the user which is probably you failing to do what the program is asking and you keep getting shown the same digits eventually you get it right once or twice or three times or whatever you decide and now we see that the program detects that and starts to show you a broadening set of digits make sense ok that's scaffold number one scaffold number two will increasingly challenged the user to remember the gesture associated with the digit as they improve how what might the scaffold look like and what would it mean to gradually remove this scaffold okay so right so this is the black art of scaffolding so we could do what you propose which is detected they've signed correctly a few times and then the next time they see it they don't get to see the image of the gesture right you got the training wheels on and you just take them off completely right what we would like to see in deliverable 9 as best you can is demonstration not just of the removal of the scaffold but gradual removal of the scaffold so for scaffold number one we're increasing the number of digits that you show to the user that one's pretty easy right one digit then two digits then three then four and so on how might you gradually remove this second scaffold sure they're not personal so my question is absolutely right so flash up the image for a shorter and shorter period of time until you remove the training wheels and you don't show the image at all that's one way that you might go about doing it right you might come up with other other ways it's up to you what the scaffold is and how you remove it but that's what we're looking for right that's your grant you found a way to gradually remove scaffolding okay the third one is to increasingly challenged the user to sign the gesture quickly what might this third scaffold look like and how would you gradually remove it public accountant crease each iteration through the program and the fact they do it I don't know maybe not let them sign if they don't do it as much as they previously switch to everyone perhaps so might we might increase the threshold at which we consider success so if you correctly sign a digit the second time but it takes you longer we might count that as a failure it's one possibility how else might we do this you're trying to increasingly incentivize your user to sign more quickly have a countdown right that makes sense remember our discussion on Monday about perception and attention and memory right you can try this yourself if there's a counter that's counting down your eyes are going to be tsikata ng22 the counter which which is fine right but that might add a little bit of stress to the user and make things a little bit more difficult how would you introduce a timer without actually having seconds that are counting down to zero what are some other things you might do that allow the user to continuously attend to the wire frame of the hand how might you do this so you don't have to actually look away from the hand to the counter which is in the top right you could have to hand fade away or the color of the lines might change or the lines might thicken or there might be you could change the wireframe visualization to communicate the fact that there is some timer on the other side that's counting down and if you do it in the right way the user will start to get it pretty quickly they'll build up a mental model and they'll be able to predict at any instance in time about how much time they have left to sign the current sign right okay okay so you're shooting three videos demonstrating these three scaffolds and we want to see the scaffold applied when the user fit in each of the three videos we want to see the scaffold applied when the user is failing and as the user starts to succeed we want to see in that video evidence that the scaffold is being gradually removed make sense okay now performance detecting improved performance again that's up to you that might be how many times they sign correctly the time it takes them to sign correctly it's up to you we don't we don't care any questions about deliverable nine no okay so back to lecture we are working our way through lecture 13 we started a discussion last time about Gestalt perception what does Gestalt mean aside from a word that's fun to say oh the hole is the is greater than the sum of its parts why does that apply to perception so your users when they're using the ASL system they see a set of lines but they don't see a set of lines they should pretty much instantaneously see a hand because the gestalt tests taught us that most of our perception our brains are wired up to take a whole bunch of disparate detail and immediately see the pattern or the hole that's hidden inside of all of those parts okay so I pretty sure we're going to finish lecture 13 today and we may move on to lecture 14 on affective computing okay so we started last time by looking at gestalt and then one of the first rules or laws of the gestalt approach to perception which is this idea of reification which is as close to the philosophy of gestalt perception as you can get right there's your brain is reifying or making real the triangle and panel a and the bar and panel b and the sphere and panel c and champ in panel d okay we ended last time by talking a little bit more about aside from trying to synthesize holes from perceptual parts what is the brain really attending to we finish the discussion about attention attention last time there's lots of controversy in the field of psychology because again we don't really know how the brain carries these things out some psychologists argue that our brains focus on the constants ease in our world and others argue that our brains focus on the changes in the world and as you can probably anticipate they're both right okay let's see some examples of this we're going to see some examples of perceptual constancy and perceptual changes and in both in both cases and the examples i'm about to show you i want you to pay attention to the context which is the other perceptual things that are going on that help you make sense of the constancy or the change who has seen this optical illusion before tell me about the colors or the gray scale of a or b absolutely right the light the light square which you said light squared dark square which is which this is a trick question ok so the lighter one in the shadow which is panel be there compared to the darker panel a outside of the shadow oh sorry you can't see that from back there there's a or a or b if you can't see it a is the darker panel and B is the lighter panel there was something incorrect in that statement what is it they're the same light intensity right neither is darker than the other I see a lot of concern looks right a and B are the same color you don't believe it right that's the whole point none of you should believe it I promise you that it's true and I'm about to prove it to you trust your professor for a moment it's true your brain is militating against it why why is your brain convinced that B is lighter than a because there's a darker squares around in the shadows like it's lighter shadow is surrounded by one sweet right so the shadow is making is is influencing your perception here right that's the context the context is the shadow there's also that right so this panel here is actually lighter than this one but if you pay attention carefully these two are the same all right here's your proof this is not unlike the Necker cube right hopefully with these added bars your brain should be able to grasp for a moment that these are the same color but the moment your brain loses focus of those two bars it flips back to be is lighter than a let me grab a colored object from up here okay so hopefully most of you can see the medium green on this eraser if we were to turn off the lights in this room which is good because there's no windows in here there would still be a little bit of light filtering in from the to open doors what color would you see this room is mostly dark a little bit of light filtering in what is the color of this label it'll be dark green but your brain will know that it's not actually dark green why absolutely so the minute we turned off the lights the early part of your visual stream the part at the back of your brain will tell the front of your brain the conscious part you're looking at a dark green label but the front part of your brain knows that I didn't magically change this the color of this label the instant we turned off the lights it's focusing on the constancy of this particular object this label regardless of the light levels that fall on it you know that this color is probably not changing right even if I change the orientation of it maybe some self shadow falls on it the actual color of this label is changing according to one part of your brain but the other part is correcting for that change its compensating based on its predictions about overall light levels right you can imagine why our brains evolved to do that if you're if you know that Barry's of just this color of red are safe and berries that are a slightly different color of red are deadly and you see those berries in different light levels your brain needs to be pretty good at knowing the actual color of that that barrier so this optical illusion is designed to try and foil that right your brain is constantly lightening the color of panel B because it knows it's in shadow and then when I ask you to compare these your brain says B is lighter than than a right so this optical illusion is a great example that shows that our brain is always working to compensate for other things that are going going on ok what other kinds of constants ease exist here's another gestalt law of perception which is the law of continuity I've taken 5 semicircles here and I've laid them into the slide and your brain knows that there's probably there's a very low probability that these 5 semicircles are lined up at a corner to corner assuming I didn't do it on purpose so instead of 5 semicircles you probably see what some single object here right so a twisting piece of paper or ribbon or something like that right so based on the way that these two separate objects are touching one another it makes it seem to your brain again that there is one object there rather than rather than five ok why does this matter for HCI remember our discussion about visual design we want to try and communicate as much information as possible with as little ink as possible let's imagine that I wanted to create a visualization for our users to communicate some hierarchical structure there is some top-level object followed by two underneath it and two underneath it and two underneath it so there are 15 objects in total and in order to communicate the hierarchy or the relationships between them I might connect the objects that are related with lines which gives me another 14 perceptual objects the 14 lines and a total of twenty nine visual objects if I remember the gestalt law of perception and continuity that things that are close to one another seem to be part of the same hole I can throw the lines away and there are actually probably more colored pixels here than there are here but the idea here is this is somewhat simpler than this hopefully by looking at this I can convince you of the relationship between these objects if you look carefully this pair of objects down here these two objects are actually closer to one another then either object is to its parent node but hopefully you get the idea that this one is more related to this object than it is to this object makes sense so we can rely on some of these perceptual biases to simplify the way we communicate information to our users okay here's another one so we just looked at we just looked at organization of objects what about shapes so here we have a bunch of objects and hopefully you see three rows here rather than six columns I cheated a little bit because the circles actually are physically closer to one another then the circles are to the Diamonds I meant before class to try and equalize these distances you can try it yourself I think you will still see three rows rather than six columns because our brains tend to group together similar up similarly shaped objects there aren't actually three rows here are six columns it's what is your brain tend to see right perception is an active process ok that's the gestalt law perception things that are similar similarly shaped are similarly colored we tend to see as a group closure another rule we tend to see objects that are visually ambiguous our brain tends to focus on the closed rather than the open there's another optical illusion for you what do you see yep there's the vaz in the two faces right so like the Necker cube your brain should probably flip flop between seeing the vods for a little while and seeing the faces if you pay really close attention for most people they tend to spend more time in the phase where they see the vaz than they do the two faces because the vaz is a closed object and the faces although there are not closed objects they're open objects so again if you display something to your user and they're trying to make sense of your visualization they will attend to or recognize closed objects more than open objects which is again something that you can exploit okay we're still talking about Constance ease here so what other kinds of constancy zar there we we have color constancy so we tend to see the same color even in changing light levels and hopefully in this case when I ask you what is the shape of this object you won't say an oval although that's really what you're seeing what is your brain seeing instead something that's round well an oval is round also a disk right so there's a circle there your your brain is taking this object and pushing it through a mental model and that mental model mentally rotates this coin until its face on and your brain knows that when this object becomes face on it's going to be a circle even though you can't see the circle you see an oval because you have 20 or 30 years of experience of coin shaped objects that you see is an oval and when you rotate them face on there they're a circle there's another version of this optical illusion where you where you narrow this image a little bit and most people will also know that when you rotate it it's now not a circle but an oval ok so we see objects in different light levels at different orientations and different positions but we know most of the time that that object is not changing when the light levels change or when the object rotates or when the object moves that's not always true we know that's true of rigid objects right so your brain is doing a lot of that work for you already ok let's switch now from the way in which our brain tends to enforce constants ease and let's switch now and focus on how our brain deals with change we've already mentioned this a few times already there's some obvious cues that we pick up from things that are changing in our environment if we see multiple objects and some of those objects are moving faster than others then usually we infer that the object is closer the object that's moving faster is closer to us that's not always true it's true in this cartoon when is that not true you have a bunch of objects some of them seem to be moving faster than others but your brain does not think that the faster objects are moving are closer to you than the slower moving objects let's imagine I should make an animation of this one of these days let's imagine I showed you a whole bunch of objects they were moving at different velocities but there was no correlation between the direction they were moving try and mentally simulate that what would you what would you take away from that animation what would your brain tell you at CA would you feel like would you feel like some of those objects are closer to you yes no usually it's no why no so in this cartoon here these two trees are moving in the same direction there's a perfect correlation between their movement if you ever see a in a sci-fi movie the star field and they jump to work drive and all the all the stars move away from the center of focus they're moving in different directions but their directions are correlated they're all kind of moving away from the center of the screen so sometimes it looks like some of those stars are closer to you than others if everything is moving in different directions that illusion falls apart why because what's that absolutely right so if you look out the window at a bunch of trees and all the trees are moving in the same direction your brain usually knows that it's not then that's moving its you and so that the things that seem to be moving faster are closer to you right the stars are moving are actually not moving it's usually you in the spaceship that's moving forward if everything is d correlated then it's probably the objects that are moving themselves and you can't then rely on velocity as a cue for which ones are closer to you than others for those of you that are are proficient now and matplotlib or pygame you could probably write such a simulation pretty quickly and try it out yourself okay so remember that all of these perceptual biases come from our decades of experience in the physical world right here's another one that I just mentioned right so objects that are flowing from the center of the screen out towards the corners of the screen are usually implying that you're moving in towards the center of the screen this has been a simple graphics trick in video games since the beginning okay okay so we've been working our way through memory attention and perception and we've been looking at some of the biases in human perception that we can exploit for HCI let's turn things around now remember in human-computer interaction humans perceive the computer and react to it but more and more of our apps are also perceiving the human user and reacting to them and your users usually know that so they are building up a mental model of the software the app or the robot so we need to think carefully about how apps and combinations of software and hardware perceive the world this is easiest to do when we think about a robot which is basically just a computer that can act and perceive the sensory repercussions of its actions and it's able to physically move as well okay so in order to think about this let's write a few lines of pseudocode for a very simple robot which is our lego robot that we have here in this in this cartoon here we're looking down on the robot from above let's imagine that this robot has two wheels and all this robot can do is stay still turn on turn on the spot and if it detects that it's in danger it can drive away it can run away so it's that's all it can do in terms of its motors we're going to give it just one sensor which is an ultrasonic sensor so this is the Lego Mindstorms robot kit this is what an ultrasonic sensor looks like the ultrasonic sensor gives information about distance let's imagine that the ultrasonic sensor is on the front of the robot so as it turns it's sort of scanning how far away any objects are from it in this little cartoon here let's imagine that there is only a single moving object in its environment so if it doesn't see the object it returns two feet or the sensor returns two feet if it turns and it sees the object it returns a single number which is how far from that sensor to the object so we now know how the robot can move we know how it can sense we're going to give it one more item which is a timer so it's got a stopwatch when we start this robot it starts the stopwatch and it has timing information so it can record when it detects certain sensory events so far so good ok so this cartoon here is to sort of show you what this robot should do if there's an object here and the robot somehow detects that the object is moving away from it it should stay still it's not in danger in this case the object starts here and approaches the object the robot and the robot should run away in this third case which is tricky the object is here the object starts moving and actually gets closer to the robot but it's not in danger this object is eventually going to miss the robot anything so I want you to turn to your neighbor and I want you to sort of just sketch out a simple algorithm for this robot that will use movement timing information and sensation to be able to distinguish between these events so in essence the robot is just going to return a binary value zero means I know I'm safe and one means I'm going to run away because I'm in danger this case should return 0 this case should return 0 this case should return a 1 i'll give you a few minutes to talk that over and then we'll see what you came up with oh sorry there's a little bit of a confusion here so when the robot decides to move on the spot its whole body and obviously the sensor which is on its front moves so there are two wheels underneath this robot and it can choose to turn on the spot all right behind a horse of this tonight I know you're the high protien of the time Harry Selfridge invades to where the Coast regions of memory function at under management doesn't matter how fast is it ran let's say they move about is the same speed as the robot I don't think it really matters for this example ok it's gotten kind of quiet so you must have all solved this problem do we have some solutions out there how does the robot distinguish between these three cases yep it senses the object Lindsey a certain radius and then how does it search how does it sense the object the ultrasonic sensor so you know it affects how far away the object is from it ok but in all three of these cases so far the robot cannot detect these objects so let's get a little more detailed here how does it detect the object sorry pivot right so it's got its got to start moving so a robot moves and then what happens it moves in minimal effect on sir how how does it detect the object the ultrasonic sensor what about the ultrasonic sensor will tell it that it's detected the object distance absolutely right so it's turning the robot starts turning the robot knows it's turning two feet two feet two feet BAM less than two feet right so when you started your description here you said the robot detects the object which is shorthand for what's actually happening right so let's try and use the ladder kind of language sensor motor language the robot acts and it gets this sensory repercussion which means this ok so the robots turning turning turning suddenly the distance drops to less than two feet what does the robot do now it doesn't know whether it's in danger or not ok so the robot could start the timer the moment that it sees the object and what's it looking for at that point D a decrease distance right it was less than two feet and now it's looking to see whether it's less than two does that is that working there's a detail that's missing here ok in this third case here ok so maybe it's got to take a few readings right over time how does the robot know it's me the object is leaving its field Division haha so the robot is moving moving moving distance drops to less than two feet and at the next time step the reading goes back to two feet what does the robot do in this case sorry it could but that's not a good idea in this case is it we might in the first case yes this one here so we might have already stumbled on a solution right so I start moving two feet two feet two feet suddenly the distance drops to less than two feet and then it goes back to two feet ah I'm saved is that good enough sorry did it pass the objects you tell me what from the robots point of view this is with a idea we're trying to investigate here you're trying to put yourself into the robots point of view right put yourself in the robot shoes the robots wheels all the robot ever gets in its whole life is just a single number from one time instant to the next and possibly timing information it's got to try and use that information to figure out something about its external world protein the entire time all of you I understand what ok so the capabilities are that the robot can choose to move so the robot could choose to just keep turning in a clockwise motion on the spot at constant speed if it wanted or it could choose to rotate in the other direction or it could choose to rotate until its sensor is less than two feet it's up to you you're programming the robot it can change its actions based on nothing it could just keep doing the same thing you can change its behavior based on the timer and or the sensory information to keep this simple let's assume that it could just turn on the spot for now it's got a distinguish between these dangers and save situations how does it do so maybe start with the first case which is a little bit easier the robot is turning turning turning faces the object distance drops to less than 2 feet the robot chooses at that time to stop moving and it continues to see the distance dropping that case the robot is pretty sure it's in trouble right oh I'm sorry I yet you're right I got the wrong one so maybe it stops here and sees that the distance is less than 2 feet but whatever that distance is it's increasing the robot knows it safe in this case it also stops and sees the distance is decreasing and knows that it's in danger so this seems to be a pretty good solution move until distance is less than 2 feet and then pay attention to how the distance is changing don't even need the time you know what direction it's actually going yes so in these two cases it can write it can figure out whether the object is approaching or moving away from etsy but if the object is moving and bleakley to the robot meaning the object is here and starts and moves in this direction if the robot turned clockwise and pointed at this object and stopped moving it's in trouble right so this simple little program we've sketched out for the robot is not sufficient why would it the second is the x-division good question right so the robot says ok I'm going to modify my program a little bit move move move see the objects stop and at the next time instant my vision goes back to 2 feet therefore I'm safe is that good enough or let's assume it doesn't curve straight lines it's not quite sufficient it could we just haven't added that to the program yet so it's definitely not safe right so this oblique angle is safe but this oblique angle is not safe so just because the object has left its field of view wouldn't it you could turn towards that would it okay so maybe the robot turns to towards it how does the robot know to turn towards it all it knows is that it got two feet and then it got less than two feet and then the next third time instead it went back to two feet this robot does not yet know which direction the object has left its field of view so which way does it go well through my was continuously spinning ha if it reads distance at one point in time okay and then I would read the distance another point in time and then maybe it can stop spin move away possibly right so again there are lots of solutions to this problem when I'm trying to impress upon you is it's tricky because it's hard to put yourself in the robot shoes right we say ah it should turn towards the object and track it to see whether it eventually the object starts to move away from it but we haven't yet figured out how the robot knows which way the object is even moving in right all it knows is two feet less than two feet then back to two feet here's one possible solution which is that the robot just turns in a clockwise manner at a constant speed and as it does it's running the timer and in this example it wasn't two feet it was 30 millimeters 30 millimeters 30 millimeters eight millimeters 30 30 30 6 30 30 30 30 40 and so on this is what we might see we might see the object approaching the robot but this is what the robot sees so depending on the time interval between these two readings so this is like the radar beam that's moving around and pinging whenever it hits something if that time interval is constant the robot keeps turning around and around and it keeps getting a less than 30 millimeter signal at a constant time interval what does that mean it's getting closer more specific than that well we don't know if it's getting closer on this example here yes it's getting closer if at every 32 at every 1.5 seconds it always gets a less than 2 feet reading what does that mean about the object that's that the object is moving parallel or in towards the robot right so it's always hitting the robots sensor is always hitting the object at the same angle of the clock right the robot is turning at a constant velocity the robot has decided what to do it's moving at a constant velocity it's timer is just running and it sees every 1.5 seconds I get a reading that's less than 30 millimeters that means whatever that object is it's always at the same angle relative to me yes we could that's all maybe not a practical solution yes that is true yes perhaps ok so it's seeing this object at a constant time interval and it notices that every time it sees the object the distance is less it knows it's in danger right the object is always at two o'clock relative to me and every time I see it as I spin every time I see it it's closer I'm in trouble right you just do like like it have it turn to the left and turn to the right and check to see where the object is yes yes we could we could track actively track the object right in this case if it sees the object at two o'clock and every time it sees it the object is further away then the robot also knows that it's safe if the robot turns at a constant velocity and it sees the object at a non constant time interval what does that mean it's it's not angled directly towards the robot or away from it it's moving right I see it at two o'clock the next time I come around I see it at three o'clock and four o'clock and then five o'clock and i also see that the distance is different if you were to plot that as a line on one side of the line the robot will know that it's always safe and on the other side of the line that it's always in in danger that's one possible solution the other one is to actively track the object this is in the field of robotics a pretty trivial example but at least for me this is hard to do because you have to think about what the robot has at hand right so this is often known as the frame of reference problem here it is the frame of reference problem the frame of reference problem is a problem because when we observe an animal or a robot or a nap we're seeing it from our point of view oh the robot can see that the object is behind it the robot has no idea about behind her in front of me all it has is distance and timing information we have a particular view onto the problem which is very different from what the robot sees right when your users looking at your app they say oh the app can see that I'm doing da the app may or may not be able to see that the app is probably seeing something like this and may or may not be able to infer what the user is doing it's a very difficult bias built into the human cognitive system which is this frame of reference problem i'll come back to the super bots in a minute I want to just end with the frame of reference problem today we've run out of time but this works with people as well if we had more time I would get you to stay where you are and turn in your seat to look at the students behind you then I would get you to come up to the front of the room and stand where I'm standing and figure out which students here are occluded from my field of view so i know which students are occluded from my field of view you don't the students in the front row will usually do a much better job than the ones in the back row why you're closer to my position right so your frame of reference is more similar to my frame of reference then your frame of reference is to my frame of reference right it's hard to put yourself in my shoes even though we're both humans much harder to do if you're trying to infer the frame of reference or what a robot or a nap can can see okay we've run out of time so I'll stop there deliverable eight is due tonight you have a quiz due tonight and i will see you on friday thank you 
UwAZDqqT0NA,28,Playlist: https://www.youtube.com/playlist?list=PLAuiGdPEdw0j6VNxfbY-FNlbAjlWIVNnO,2020-09-08T10:35:34Z,"Human Computer Interaction, Lecture 02. Recorded at the University of Vermont, Thurs Sept 3, 2020.",https://i.ytimg.com/vi/UwAZDqqT0NA/hqdefault.jpg,Josh Bongard,PT1H10M31S,false,151,4,0,0,0,okay uh good morning everyone uh again i apologize for adding further confusion to i'm sure what is a confusing week uh i know we said we would start this class uh we would be teaching this class through zoom but i've been asked to switch to teams so we will meet here on teams every tuesday and thursday morning just a couple of reminders about how this course works at the beginning of class or a few minutes before class i will add an attendance sheet for today so if you arrive a few minutes or early please be sure to click over to the spreadsheet which will be editable during class time and please type your name in here so we know that your you're present um and speaking of your presence again if you have the internet bandwidth um i'd like to ask you to turn on your video so we can create the illusion that we're all together uh in a classroom as best we can it definitely helps me instead of talking into an empty screen so thanks for that um other reminders uh you had a quiz that was due at 11 59 pm last tuesday night we have a quiz uh every day that there's a lecture if you've just joined the lecture and you missed the quiz or you have other questions about how to catch up uh please email me or the ta and we'll get you oriented as promised i record every lecture so you can now find the video lecture from friday's class up on youtube at the link here if you're unsure about any of the details about how we do this class please uh click through to there okay um i assigned you uh deliverable one in which you'll be uh installing and starting to play around with a leap motion device uh any quick questions about the elite motion device you can either type something into chat or raise your hand and unmute yourself and ask your question so far so good no questions so far okay all right so uh carrying on uh with the lecture material we're gonna finish uh we're gonna finish the uh will office hours be on teams uh or zoom for the moment office hours will still be on zoom if you go to the syllabus you'll see there's remote office hours just as a reminder if you click on my office hours you can see that you can sign up you can sign up for office hours and during office hours i will link a zoom link to your name and that's an indicator that you can click on the link and come join me in zoom if that becomes overly onerous we will switch to to zoom but for now just hang out in the waiting room here for office hours and i'll send you a link where you can join me one on one for office hours sound good okay okay so uh again just back to lecture uh material we're gonna finish slide deck number one today just sort of an overview which again is logistics and expectation and then i will finish our discussion about why study hci and then today we'll spend most of our time talking about the basics of human computer interaction okay any other questions before we jump back into lecture material all good okay so uh we were talking about two uh two reasons so far about why you might want to take hci first of all there's some very high paying and interesting uh jobs in hci and we ended last time by talking about a second reason which is as we all know uh we're in the moment we're in the current data flood there is more data out there than we know what to do with it there are a lot of challenges out there about how to extract meaning from all the raw data that is being generated every day and that requires both technical acumen the ability to program and wrangle that data there are data science classes here at uvm about the technical side of dealing with large amounts of data but as the gap minder website makes clear they're also creative challenges how do you actually visually present that information in such a way that the user is not overwhelmed by the amount of information that is there so it's just an example of this we ended last time with with this gap minder example there is a lot of information here however there is relatively little text so because this is a good visualization it doesn't require a lot of text uh textual explanation about what you're seeing on the screen and hopefully with relatively little effort you can extract some of the patterns that are inherent in this particular data set which is plotting an economic indicator gnp against a health indicator life expectancy in years for most of the major uh countries on earth we ended last time by looking at one particular dynamic that you can see here i've highlighted two particular countries china united states and you can see that the economic improvement of china seems to be increasing at a faster rate than that of the united states you'll notice that in each of these trajectories we have a number of circles and some of these circles are occluding other circles what does the size of the circle represent again you can either type it into chat or unmute yourself and just offer an idea what does the size represent so size represents population although i believe nowhere on this plot does it actually say that how do you know that the size of the circle represents population what's the clue [Applause] china has the biggest circle the assumption of the person that created this website is that most people would know that the population of china is greater than the the united states so whether you may have realized it or not there was an assumption on the part of the designer and hopefully most of you most of you realize that uh the colors match the map that's true so obviously color represents geographic region and uh size represents population and obviously as you can see in this plot the larger the circle uh the smaller the population in that country right and vice versa the larger the population in that country the smaller the circle just testing that everybody's awake this morning of course it's the the opposite the size of the circle circle represents population why is that why did they choose in this case to represent larger size with greater population it seems like such an obvious choice it might seem strange that i'm even asking this question it's intuitive somehow why is it intuitive what we're going to spend a lot of time doing in this class is addressing some things that may seem obvious on the cert on the surface but they rest on assumptions that are it's not always obvious we instinctively associate larger with bigger right so uh as amanda just said here there's a metaphor here which is larger the bigger the circle the larger the population of that country you'll notice that it's a metaphor between something that's relatively concrete and explicit which is the size of the circle you can see that directly in the plot but it's representing something that is abstract we can't actually see the entire population of a country so we're using a metaphor we're drawing something explicitly that represents something that is abstract or implicit in the case of population size and circle size that visual metaphor seems uh very obvious there are some less obvious visual metaphors that are also represented in this figure there's some aspect of the way things are drawn in this figure that are meant to represent a more abstract concept can anybody see what some of those additional visual metaphors are that may be less obvious than population size equals that circle size equals population size [Applause] could be color they may have chosen color to represent something about the culture of that of that uh of that part of the world uh jillian says opacity and so does joseph so let's focus on opacity at the moment what is it about the opacity that's being represented why are some circles um filled in and some are partially partially translucent some are opaque and some are translucent they're using size to represent one piece of information as we just described they're using opacity to to explain something else prasita says comparing them together exactly so if you play around with gapminder you can click on a subset of countries and then drag the time slider bar at the bottom of the screen back and forth to draw a trajectory and focus your analysis on a subset of countries what else what other aspects of this figure are representing uh are representing something abstract some are more subtle than others we've talked about size opacity color uh line weight yeah possibly i think in this figure all the line weights are the same width but that's a good observation we could use that visual detail to represent some additional piece of information so ethan mentions time for the repeated circles so if we focus on the the trajectories for a moment we see obviously that there are multiple circles in the trajectory what does each circle in the china trajectory represent something about time but what what aspect of time again it doesn't say [Applause] seems like a year right so again there's tags here uh for 1975 and 2004. there look like there's about 30 or 40 circles for each of the two trajectories here it's probably per year which makes sense [Applause] i cheated a little bit because the tags are close to the bottom left of each of the trajectories which is indicating that the bottom left of the trajectory is where the trajectory starts and moves to 2004 you'll notice that in each trajectory all of the circles are occluded except the topmost circle in the trajectory so occlusion the fact that certain circles are occluding or covering up others is another visual metaphor what is it representing why isn't the top right red circle occluded and the bottom left circle is the one that is not occluded so david says the lower you are in the stack uh the further back in time you are exactly so just like circle size equals represent population size here occlusion represents further into the past or more occlusion represents further into the past less occlusion means closer to the present like with circle size why are why is it that circles that are closer to us in the present are less occluded and those further into the past are more occluded why not the opposite occlude uh data points that are closer to us here in the present why that choice two ways we could have used occlusion to represent the passage of time why isn't the top right circle here which is 2004 the one that's furthest towards the back and the one down here which represents 1975 why is that not the one that is non-occlusioned so sarah says the natural progression why is it natural your brain is probably suggesting to you that that is natural that's the obvious choice but again in hci we're going to drill down into human psychology we need to understand why most of us feel that this is the natural way to do that nolan says because it probably has to do with the user's intention what they want to get out of this picture which is to see all of the information about the most recent objects right so generally speaking we're probably interested in health and wealth in countries that are closer to us in time than further away [Applause] david says the occluded ones are harder to see which mimics how we forget the details of the past exactly so that choice of occluding things that are further in the past is again exploiting some aspect of human cognition which is obviously things that are further into the past are more occluded from us if i asked you what you had for breakfast this morning assuming you've had breakfast already you could probably tell me if i asked you what you had breakfast for what you had for breakfast yesterday uh you might be a little more hazy last week last month last year last decade things that recede into the past are more occluded in our memory than things that are closer so another visual metaphor this one resting also on some aspect of human memory and human cognition okay uh sarah points out something else which is obviously if it was covered up or if it was included we wouldn't be able to see the objects that come come after it right so that's also related to this idea of the fact that memories involved in this particular visualization okay we'll spend a lot of time talking about visual metaphors in this class we'll also talk about sense metaphors other than visual ones like auditory metaphors and tactile metaphors we'll get there okay so let's carry on obviously the way that we plot information if we're plotting data are trying to extract patterns from that data why are we trying to extract patterns from that data because we're hoping that that becomes actionable we're hoping that there is a pattern we make it easier for our users to seek patterns in the data and hopefully come up for explanations or causal mechanisms why things happened and that might suggest what we might want to do next here's an example of a particular pattern that that you can draw out of gapminder relatively easy again where it's a trajectory of actually two different health indicators for china children per women woman fertility rate how many children are born to each woman and the life expectancy in years as we saw before what's happening in this particular trajectory [Applause] what is the pattern that is being shown here [Applause] so we're starting in 1960 as before and moving forward in time to 2004. you can see that there was a particular pattern occurring in china in 1960 61 62 63 which is increase in fertility rate women were having more children per woman life expectancy was was increasing and then there was a sudden reversal fertility rate started to drop and has continued to drop up to uh 2004. as ethan mentions this is the uh this is the china's one child policy which was instituted uh in uh 1963 64. easy on the details so relatively quickly we can see the uh the impact at least the uh at least the reproductive impact that that policy had on this country did it have an impact on health as represented by life expectancy did the one did the one child policy alter the life expectancy for members of the for for chinese citizens possibly we have to look at sort of the rate of increase on the vertical axis along here and along here as willem says it's somewhat unclear from this graph one of the differences between hci and just traditional visualization a data plot is that most good hci visualizations are interactive as i mentioned if you go to the gap minder site you can drag this horizontal time bar back and forth you can actually interact with the data in real time and you can by manipulating the visualization you can look for evidence for or against a given hypothesis so i've posed a hypothesis to you or a question to you which is did the one ch did the one child policy impact health of the population as a whole it's not exactly clear from this however we might be able to click on and bring up other variables here to test that hypothesis uh david's dropped a link here to some animations that you can find in gapminder one of the one of the other things we i like about gapminder which we'll talk about later is again this interactive uh this interactive process right human computer interaction as we're going to talk a fair bit about today is an interactive process it is not a one-way process it is not meant to be something that is projected onto a passive uh observer how do we create interactive visualizations to make it easier for the participant the human participant to pose hypotheses to the data and then interact in such a way to look for evidence for or against that hypothesis i'm going to spend a fair bit of time talking about that when we get to visual design okay um if we're talking about visualizations here's a more recent uh example i will draw many examples throughout this course from the pandemic i'm sure that will be true in other courses just a a proviso here we're going to analyze various aspects of the pandemic from an hci point of view i want us all to keep in mind that this is a very real very dangerous and very upsetting ongoing event for many of us so just keep that keep that in mind okay you've probably seen plenty of covet 19 dashboards by now hopefully towards the end of this course you will be able to look at those dashboards with a more professional eye and be able to assess whether they are doing a good job or not again it depends on the user it depends on the participant if a participant comes to a covid19 dashboard what is it that they most want to do understand and or see that question should drive your decisions about what to include in a dashboard and whatnot okay a lot of challenges there okay uh last reason why you might want to study uh hci which is that hci or interactive technology will soon be everywhere all the time that's the 2019 version of the slides i've updated this to say now hci is everywhere all the time here's my little cartoon of this idea way back in the stone age about 15 years ago 15 20 25 years ago most humans did not did not have a computer or a smartphone few people had computers in the 80s and 90s moving forward many more people had more computers and then started to have more cell phones you can go into gapminder and actually plot the number of computers or the number of cell phones per individual by country and you'll see something interesting which is in most of the developing countries most individuals have a cell phone or more than one cell phone and few have a desktop or laptop okay that was kind of a surprising event a few years back as i mentioned in this class we're going to spend relatively little time talking about traditional technologies like desktops and laptops we're going to look at other kinds of interactive technologies that require or challenge the user to interact with them in different ways one class of technologies we'll look at are embedded devices so these are this is any device that is directly sensing the environment rather than a desktop or a laptop which is usually passively waiting for the user to type something in or move the mouse so embedded devices are in a fundamental sense more active than a desktop or laptop they're actively drawing information directly from the physical world simplest example of this are intelligent light sensors in the davis center for example the room is dark you move in it immediately detects movement and turns the lights on when the davis center was first built they had to go back and alter some of the parameters inside the embedded devices because the designers of the the light sensors in the davis center did not take into account the kinds of activities that were going to be carried out in some of the rooms of the davis center which are classes so you have an instructor at the front who may or may not be very very active as he or she is lecturing and you have a lot of students that are sitting quietly in the classroom or around around a set of tables if nobody moved for a minute or two all the lights in the room would go out [Applause] okay so um that was kind of an interesting aspect and again as in hci we need to think carefully about uh we need to think carefully who is going to be using that technology interacting with that technology and in which way okay finally i will spend some time talking about robotics partly because that's my area of research expertise but also because robots themselves are an interesting class of interactive technologies like embedded devices they're able to directly sense the environment using sensors but unlike embedded devices they are also active they are able to move themselves about their environment so uh we're not quite here yet we the world basically looks like this there are relatively few autonomous robots in our everyday environment with the exception of maybe the robot vacuum roomba but we're moving towards a world that looks like this in this world humans are interacting with not just more devices but more different kinds of devices and that complicates things in an interesting way for an hci designer okay okay so that finishes uh lecture one uh we'll move on now to lecture two we're gonna just talk about the basics of hci what is sort of the the historical foundations for this aspect of computer science okay so we'll start with the name of the field itself human computer uh interaction hci is basically in one sentence about how to allow humans and computers or maybe we should say humans and interactive technologies to interact towards some common goal how can a technology have a goal we'll talk about that also but again i want to focus on this issue of goal the user is trying to do something and they are trying to use this interactive technology to do that thing better faster more consistently with more people and so on there's something they want to do they may or may not be able to do that to do that task without the technology so they are adopting it with the hope and the expectation that that technology will allow them to do that better like we saw in the gap minder example that might just be trying to look for patterns in vast amounts of data okay so let's focus now on humans and computers i'll use computers at the moment just for short form for any sort of interactive technology humans and computers are similar at a very high level but the similarity will stop at this high level what do they share in common both humans and computers receive input in some way and produce output in some way and then in between they process information right information comes in we cogitate computers compute and then we act on the world what are the input devices let's start with humans to begin with what are some of the input devices for humans quote unquote input devices go ahead and type it into chat eyes you'll notice that eyes is usually the first sense organ that most people mention ears are usually second we will see in this course uh uh over and over that humans are primarily visual creatures most of interaction with technology is visual okay eyes ears nose so we're mentioning um anatomical features let's focus on oh i asked for input devices so that's fair okay skin touch did we get them all eyes ears nose skin what are we missing mouth for for taste right okay as we were taught in school the five senses i think we have them covered you have a sixth sense and a seventh and an eighth what are we missing you all have it one again what you'll see over and over again in this course awareness of body position right the kinesthetic sense exactly which is provided by uh which is provided by sense organs or sensory cells distributed in your muscles and ligaments though they're inside the body we're going to spend a lot of time in this course talking about interop interocep interoceptive sensors interoception internal sensation traditionally interactive technology has nothing to do with your internal feelings because obviously we don't have interactive technologies inside our bodies at least not yet but with the rise of cybernetic prosthetics and implants that will be that will be changing okay um what about output devices how do we cause change to our environment let's focus on the output device what aspect of your body allows you to cause change on the environment voice and your voice is produced by vocal cords vocal cords are vibrated and they're vibrated by [Applause] your mouth is moved by your hands are moved by your legs are moved by ultimately the brain yeah that's that's that's a good point robert says muscles here so we're going to focus on muscles as sort of your output device we your brain supplies uh and electricity the muscles causing them to contract which causes the various parts of your body to move which causes an impact on your environment so again at a very high level humans have five or more input types of input devices but strictly speaking only one kind of output device which is muscle muscle drives uh almost everything we do there's a few exceptions and we may talk about that later okay let's talk about input and output devices for computers what are input devices for computers how does a computer draw in information about the outside world keyboard right now again usually number one keyboard and mouse the the historical uh bread and butter of input devices more recently cameras what else how else do computers draw in information about the outside world network adapters exactly lidar that's a good example yep so using lasers to to pull in information about objects that are close or far away from you scanner microphone touch screen leap motion exactly so for many many decades there were basically two input devices keyboards and keyboards and mouse before that there were punch cards but in just the few last few years the number of types of input devices has proliferated and seems to be accelerating okay what about output devices how do computers project information out into the world hopefully that's picked up by human users screen screen is usually mentioned first because again we are primarily visual creatures speakers is usually second because our secondary sense that we rely on is uh is our auditory sensation vibration exactly okay uh haptics that's a good one we'll spend a fair bit of time talking about haptics networking again okay okay so as promised throughout this course you're going to see these red boxes which are empty on your version of the slide deck and are filled in in mine this is not meant to be a prod for you to madly type in or write down everything you see in my version of the slide deck this is just meant to be a reminder that you should be annotating these empty red boxes as we go in any way you see fit or doing so after after class when you watch the lecture video some uh quiz questions may be drawn from material that are inside these red boxes again it doesn't have to be exactly what's in mine okay so we've just talked about the similarities between humans and interactive technology which is they both have quote unquote input and output devices but that is basically where the similarity between humans and computers ends we're going to spend most of this course talking about the differences between these two types of agents what differences exist just if we talk about input and output devices or channels what's different about the way that input devices work for humans our input devices work compared to input devices for computers or output devices sampling rate okay so your sense organs sense change in the environment at a certain frequency or a certain rate in time which is often different from computers [Applause] what other differences exist so more biological for humans of course um because we are biological what is it what is it about biological input devices biological sense organs like eyes and ears and smell that differs from input devices for computers analog versus digital this is an important one right most of the information we type into a computer is digital either somebody pressed the return key or they didn't not the amount of pressure although that is changing a touch screen some touch screens can record the amount of pressure feedback rate again yeah some of these some of these input devices for computers are relatively slow some quickly ability to repair that's a good one right so luckily most of our uh most of our biological components are self-repairing not so for for computers i'm gonna change this question slightly um humans use their uh sense organs to extract information about the world and computers use their input devices to extract information from the world what differs in the way these two types of agents use their input devices to extract information from the world not differences in the devices themselves but the way that humans or computers use their input devices [Applause] computers almost exclusively taken from data from humans that's that's true yes exactly that's not necessarily true for us autonomy winbow what do you mean by autonomy if you can expand on that in chat that would be great difference in processor yeah okay so the brain the brains of these two different types of agents are different ethan what do you mean by agency talk about the differences in the ways the way in which we use these ah free will we'll talk about free will a little bit later humans can focus on one input device we will talk about uh the human capability for attention so um some of you may be wearing a watch right now but you may not be aware of the pressure that that watch is imposing on your on the skin of your wrist you you are able effortlessly to ignore it but now that i'm talking about the watch on your wrist on your wrist you may be aware of the pressure of your watch on your wrist right we can change or direct our attention to various signals that are impinging on our sense organs what other differences exist so computers can obviously also change their attention but that requires us to write some code computers almost have to wait for input so as nolan mentions here this is one of the most profound differences that we're going to focus on in this class most interactive technologies are passive they have to wait for information to arrive at their sense organs versus humans which are not they are active which you may or may not agree with most of you are sitting relatively comfortably i hope and passively listening to this lecture so you may feel as if you are passive and information is passively arriving at your sense organs but as we will see in this course that is definitely not the case that is an illusion that your your mind is casting for you uh there are things that's exclusive for humans so we can alter the way we respond to incoming information based on emotion or her our hormone levels that is that is definitely true what does it mean to be active with our input devices how can we actively ex how can we actively exploit our input devices to extract more information from the world what do we do using our output device our muscles to exploit our input devices to broaden the net that we cast with our sense organs [Applause] change position to gain more information exactly so if we were in a classroom those of you sitting in the back row you would be occluded by students that are sitting closer if i want to create eye contact with those sitting towards the back of the room i will try and move my head to see you most of you are occluded by the fact that you have your video turned off such as life what other that's a good example thomas what other examples exist what other kinds of actions do you perform to gain more information from your sense organs humans are active creatures most interactive technologies for the moment are somewhat passive they're mostly passive let's focus on skin for a moment uh uh this should say skin not touch skin is sort of the uh the the black sheep of the five sense organs which tends to be last or close to last however your skin the largest organ you possess is able to extract a wide range of physical phenomena like touch so binary information is something in contact with your body or not pressure a continuous value your skin can tell you something about temperature it's not very accurate but within a few degrees how does skin give you information about texture how does your sense organ tell you when it's in contact with something friction friction comes about when when you move your skin across an object right so the object may be in motion which will give you texture information but you can choose to rub an object in a certain way to extract texture whether you're aware of it or not the way you choose to move the particular actions you take the direction and velocity rotation of your hand in contact with an object your brain is generating that action in a particular way to extract to maximize the extract extraction of information so yes the information is coming in through nerve endings under your skin but that only works if your skin is in motion relative to the touched object what other kinds of actions can you take to broaden the net of your skin there are things other than pressure temperature and texture so braille is a great example absolutely you do this all the time you act in certain ways to expand or broaden the net of your sense organs although you're not usually aware that you're doing so you will have to be aware of those processes if you're going to design interactive systems to allow users to do that as i mentioned skin is not very good about telling temperature it's also not very good about telling wind direction i realize we're all indoors at the moment what aspect how can you affect your skin so that it is more sensitive to wind direction you go outside and you're wondering about which direction the wind is blowing what can you do lick your finger and hold it up right so there's a pretty non-obvious when you think about it action that is exploiting some specific aspect of a specific aspect of your sense organ what is it about dampness on the skin that tells you something about wind direction again you may not have ever thought about it heat change uh he change in evaporation right so a thin layer of moisture on the skin will rapidly change temperature if there's wind buffeting it and it is also evaporating within a few seconds um if the if the layer of moisture is is thin enough right so the more you think about it it's a pretty sophisticated uh interaction with the environment that humans actively exploit all the time every tool that any human has ever built starting with stone axes is also a product of our actions and those actions are meant to cause change on the world which alter the kinds of inputs we receive back maybe that interaction from output on the world through a tool through an interaction with that tool with the environment back to the sense organs is very indirect but in essence that is what most a lot of our tools are designed to do is to allow us to see or sense the world uh better okay okay so um up till now in this cartoon here we have uh input arriving at uh we have input i'm going the wrong way we have input arriving at the human or interactive technology some cogitation or computation and then output on to the world it seems so obvious a way of thinking about humans and also computers that again most of us don't ever think about it but we're going to start our historical journey today with uh with bf skinner which is who is one of the reasons why we tend to think this way before skinner the study of animal behavior and human behavior was basically uh natural history people would go out into the environment of the animal observe the animal in its natural environment pose hypotheses about that animal for example is that animal capable of learning or not and observe that animal and try and answer that question however the environments of most animals and humans are very complex there are a lot of variables there are a lot of physical phenomena impinging on the sense organs of animals they have very complex musculatures very different and complex behaviors and ways of impacting their environment so bf skinner was a psychologist who in the 1930s said why don't we try and simplify the environment of an animal so we can restrict the kinds of inputs that animal receives and restrict the kinds of outputs they can produce and based on what we observe of the relationship between input and output we can deduce something about the behavior or behavioral capabilities of that animal okay some of you may have seen a skinner box before this is sort of an idea that's entered into popular culture here's an example of an experiment we might wish to carry out with the skinner box let's imagine we have pigeons and we want to know whether pigeons are capable of operant conditioning which is a fancy word for a type of learning how do we use the skinner box to test that hypothesis is this animal capable of learning well we could for example intermittently flash a light inside a darkened skinner box and the pigeon can either peck lever one lever two or do nothing if the if the pigeon pecks lever two we will put a little bit of food into the box if the pigeon packs lever one or neither of the levers we provide no food if we do this over and over again shine light if lever two then release food if we do that we collect information from not the animal itself but from things happening at the input layer and the output layer what data might suggest to us that this pigeon is in fact capable of operant conditioning or learning what would we expect to see in the data imagine i gave you a data set which is the times at which we turned on the light the times at which lever one and or lever two went down and the times at which we input the food and you were able to see those uh events occurring over time what would you look for in the data to convince you that this animal is actually learning [Applause] light and lever to converge right exactly so uh if this if this pigeon is smart every time it sees the light it should immediately peck lever two to release the food right pretty straightforward okay makes a lot of sense very appealing to someone of a scientific mindset and as we can see we've greatly restricted this animal to input the animal does something and has an effect on output we can of course put a human in a skinner box as well and ask the same same kind of question which again suggests or reinforces this idea that humans are animals and ultimately the smart technologies that we created like computers should take input think about it then do something and have an impact on the environment six years after bf skinner introduced behaviorism or the skinner box alan turing introduced the turing machine which very different from the skinner box but has something in common which 1936 was still just a thought experiment there wasn't a physical machine yet the turing machine also would receive some input do some information processing and then some output let's try and be a little bit more specific of that in a couple of sentences what what does a turing machine do [Applause] most of the computer scientists here should be able to answer this relatively quickly the turing machine doesn't exist in a vacuum there is this external tape what does a turing machine do what is its input what is its processing and what is its output what kinds of outputs is it capable of it reads and writes a tape yeah so reading uh reading some symbol from the current position on the tape that's the input and writing to the tape is the output what's going on in between how does the turing machine know what symbol to write to the tape state transitions so internal state of the turing machine it has some internal state that internal state can be influenced by the symbol that it just read from the tape if it transitions to a state that new state may dictate what symbol it writes to the table what other action aside from writing to the tape is the term is most are most turing machines capable of it moves the tape left and right exactly right so even in computers or even before computers even existed there was this idea of movement or action so for many people the turing machine was not a passive machine that would sit there and receive a tape for some people it was the turing machine that moved relative to the tape it might seem like an arbitrary distinction but given what we've said over the last few minutes a passive machine sits and waits for the tape to pass by an active machine moves relative to the tape it's interesting to think if you're uh of a historical mindset whether turing was influenced by skinner in both cases in both animals or humans and machines there was this emphasis right from the beginning on input cogitation or computation and then output okay again this is just meant to show you sort of the level of detail we're looking for in the description of the turing machine okay basic idea some input some output and some internal processing okay so we've been moving sort of slowly through the beginning of the 20th century we're now going to take a step back in time to the 19th century and talk about the philosopher john dewey john dewey is famous for many things he is arguably one of the most influential philosopher american philosophers he also had a lot of important things to say about education some of his thoughts on philosophy and pedagogy or education came together in an article called the reflex arc in in psychology that he wrote at the very end of the 19th century in dewey's own words we begin not with the sensory stimulus we begin not with the input but with a sensor motor coordination in a certain sense it's the movement which is primary and the sensation which is secondary in dewey this would have meant that the sensation came after the movement the movement of the body head and eye muscles determining the quality of what is experienced this was a very profound statement that was given long before this idea of focusing on input first then output for dewey it was opposite output comes first output influences the environment and that change on the environment is then sensed by the human and it is that action the movement of the body that determines the quality of what arrives on our input channels that arrives on our sense organs he refers to this as the arc which maybe wasn't the best choice of words what we're really looking for is what he called coordination here or a circle a human all of us since the day we were born we are acting on the environment whether we're aware of it or not we are pushing against the environment literally and figuratively and we are observing the results of our actions the environment pushes back i'm going to use this turn of phrase many times throughout this this semester we push against the world and we observe how the world pushes back that loop or that cycle or that arc if you want is the building block for everything else we do that is how we learn about the world that is how we adapt to the world that is how we survive in a challenging world however thinking that way requires us to break or break our bad habit of thinking of behavior as linear or passive input passively arriving at a human or a technology and being translated in to output in everything we do from now on in this course we are going to try and think of this as a continuous loop in a continuous loop there is no real beginning and end although although dewey did talk about action first then sensation again most of us have 20 30 40 years of experience of this continuous loop running all the time regardless of whether we are awake or asleep based on the raw data that we have generated through that process over the decades we have built up expectations of this reflex arc i expect that when i send commands to my muscle to raise my hand and i feel strain sensors or strain cells in my body that tell me the enteroceptive signal inside my body telling me that my arm actually is rising i expect i have an expectation about what's going to happen which is that i will see my hand enter my visual field if i send commands to my to my arm to raise my arm in front of my body i feel the sensation in my arm that my arm is actually rising and i do not see my hand appear at the bottom of my visual field i am going to be very surprised and probably pretty upset there are expectations we have all the time about our interactions with the environment luckily most of the time those expectations are met and we calmly go about our day sometimes those expectations are not met and we become curious we focus our attention as we just talked about we focus our attention on that surprising event or that expectation is broken in a pretty serious way and we run for our lives or uh we are very upset okay so again in this continuous loop we have built up huge expectations one important or probably the most important aspect of our external environment are other people so as somebody was mentioning earlier uh in the chat um we also have social expectations um when uh when you're talking with another human being assuming that you can see the face of that of the listener you have certain expectations from them and uh if those expectations are broken you might change your mode of operation if you're trying to engage your friends in conversation and as you speak you see their eyes wander away from your face to something else that's usually a telling signal about how to change this loop okay so a lot of what we're going to do in hci is try and understand the less obvious aspects of these expectations drawn from the physical world or the social world and build them into our expectations with our devices humans whether they realize it or not they want their devices to be to act more or less like the physical or social environment when you move your mouse and there's the cursor on the screen if it does not respond immediately that's pretty upsetting or frustrating i expect my hand to immediately appear in my visual field when i feel my hand in front of my face if there was some lag in that process i again would be pretty upset okay it might take you a while to wrap your mind around thinking about interactions especially with technology where movement is primary and then sensation is secondary you can you can convince yourself of this process again if you have any uh roommates or dorm mates you mentioned the leap motion device plug the leap motion device in turn on the visualizer put the leap motion device on the desk in front of your friend and say nothing and see what they do they may sit there for a while maybe a few seconds waiting for something to happen but after a few seconds movement will become primary they will grab the device pick it up touch the top wave their hand over it and expect something to happen okay all right so let's think about this uh coordination obviously this this kind of coordination can take very different forms depending on what the person is trying uh to do or what they would like to use the technology to do because it would be uh difficult or impossible to do that task otherwise here's a very traditional thing you might need to do some research you jump onto the web you have a certain idea in mind like you want to think about new energy policies for vermont you already come in with an idea about biofuels you start googling biofuels uh you learn a little bit about it and based on what the computer gives you back you alter your idea maybe now you're thinking a little bit more about wind and solar you google those terms you get some information back and around and around this loop you go if you're googling something the iterations through this loop may be a few seconds or a few minutes you type something into google you read what you get back type something else read what you get back it is a discrete uh and more or less digital interaction the information that's coming back most of hci was kind of focused on this idea for a long time web designs search engines and so on now we are moving much more into a very different kind of interaction and most focus in hci is on continuous ubiquitous and environmental interaction we just talked about embedded devices your smartphone is obviously the most important embedded device any of us uh have your smartphone sometimes it acts like a computer and waits for you to type something onto the screen other times it acts like an embedded device where it is directly sensing the environment whether you know it or not it may be sensing uh position and that position may be changing because you are interacting with the environment so the feedback loop um in a smartphone assuming you're carrying your smartphone is piggybacking on this interaction and a lot of the smartest apps that are out there are exploiting assumptions about this loop okay so again i used to give this as a hypothetical example in class i don't know if there's an app that does this but we're getting pretty close to this you're walking down uh the street and your phone is detecting change in your gps position and it detects you're moving really really relatively close to a starbucks um it's able to infer that maybe that's your destination even if you haven't typed this into google maps because maybe you tweeted something about caffeine or you visited that starbucks multiple times before it infers that's where you're going the phone or i'm sorry it assumes that you're heading towards a starbucks but you may not be aware it infers you don't know that there is a starbucks nearby so it buzzes your leg it politely uh interrupts you and indicates that there's a starbucks nearby and the minute after you read the notification from the app the app notices that the change the change in position has changed so you've deflected your trajectory and you're now walking towards the starbucks without you having to tell your app it's going to infer now you're headed towards the starbucks however now your phone realizes that based on your usu your usage history and how long you tend to spend in starbucks for example you're not going to make it home before your battery flats out so your phone buzzes your leg again to let you know you alter course and so on so there are these periodic interruptions that look a lot like these periodic feedback loops between the human and the smartphone in this case but there are also a lot of continuous interactions going on here you're continuously moving through your environment as this interaction uh is playing out your phone is directly or indirectly altering that continuous interaction of you with your physical environment and drawing conclusions about your behavior or your desires based on that as we know with technology technologies often assume wrong and when they do it's extremely frustrating so again the phone has to also decide whether it makes sense to interrupt you or not okay i've been doing a lot of talking i'll just take a break for a moment any questions comments ideas additions all good [Applause] okay so up till now again i've been talking about computers i'm going to try and stop talking about computers and talk about interactive systems instead because we're going to study things other than computers in this class just as a reminder an embedded device is any kind of computer or computational device that has sensors it can directly sense the word the world with or without a human as an intermediate an intermediary and finally robots so a computational device that can sense the world directly without having to wait for input from a user and can act directly on the world okay in order to qualify to be an interactive system obviously it must interact with a person and it must be able to internally process information in some way so we want to make sure that this admittedly broad term interactive system is not too broad a knapsack also assuming you're wearing it maintains a continuous interactive loop with you with the human wear the human does something with their muscles they put books into their backpack which immediately changes the weight of the knapsack which sends directly a signal a physical signal back to the human which is an increased load on the shoulders we are not going to consider a knapsack an interactive system because he's not doing any internal uh data processing okay a cell phone is an interactive system okay all right as i mentioned last time what you'll find is that this course is relatively broad not necessarily very deep hci has one foot planted in psychology or the human side of things and the other foot planted in technology and because of that hci draws on a lot of different fields of study and human endeavors as promised we're going to spend a fair bit of time talking about people uh we've already started talking about human psychology the brain is a prediction machine whenever we interact with the physical environment or other people or technology our brain has an expectation or prediction of how that interaction is going to go and based on how the world pushes back the br our brains assess whether that prediction was supported or foiled and decides what to do next we'll look a little bit at social sociology what are some of the unspoken expectations we have when we interact with other people we'll talk about ergonomics so this will take us into the subjective or the aesthetic side of hci what is the literal or metaphorical fit between people and the things they use so there's been a lot of work on the ergonomics of smartphones we want to make sure that smartphones fit into the daily lives of people what are some physical aspects of smartphones that have changed over the years to create a better fit between people and their phones not the software the hardware itself the size right for a long time smaller was better up to a point and now for some smartphones we've reversed yeah weight is important we may put up with we may allow larger phones as long as they provide more functionality rounded edges that's a good one that's a great aspect of ergonomics why rounded edges why does that matter what was the design decision behind rounded edges it's not immediately obvious what it is so selfie camera obviously you find that uh you find that on all devices um but it's not quite about the fit it corner corners are ouch yes right so maybe something about uh being safe there's another reason that cell phones have rounded corners or another design decision that influenced why uh why cell phones have rounded edges more futuristic maybe yeah exactly maybe it maybe it appeals to some aspect of human psychology doesn't get caught on things like pockets or bags right so um it was immediately obvious with cell phones that these were things that people were gonna put in their pockets and you probably experience this with your keys it gets frustrating when you have to reach in when you're pulling something out of your pocket 20 or 30 times a day for better for worse which we do with our phones if they keep snagging on the inside of pockets that's such a great thing right again maybe a minor detail but again it suggests this gray area about things that are not so obvious but are very important it has to do with thinking carefully about the physical context of people people have pockets and things can get snagged on them okay culture also influences design of interactive technology we'll talk about that later obviously technological considerations also matter we're going to talk about lots of different kinds of hardware platforms it's going to require us to delve into electronic software engineering multimedia databases sensors and motors when things can actually move some coding networking these things we're all pretty familiar with on the other hand of the objective straightforward technological considerations we're also going to have to spend some time thinking about design and the subjective or the aesthetic sides of technology rounded corners on cell phones has an ergonomic reason but it also again appeals to our aesthetic sense someone said maybe it's more futuristic to have things with rounded corners what are aspects of design that appeal to human psychology and human aesthetic sense we'll look at interactive design 3d design design considerations drawn from engineering if we're building physical things graphic design if we're building graphical things like gapminder product design if we're building things like cell phones or leap motion devices we've got one minute left so let's finish with activities and contexts we're not just thinking about people but we're thinking about what people do and we're going to really focus on things that people do although they're not aware that they're doing it so soft systems are things like businesses or universities inside uh organizations like organizational psychology a lot of the uh the way in which it works is unwritten so a lot of the rituals or the things you're used to at the university have all been disrupted because few of us are now on campus so a lot of our our communities of practice or our tribal knowledge has gone out the door and we're going to have to build new social norms around remote instruction again a great object lesson in hci okay i think we will stop there today just as a reminder you have a quiz due at 11 59 pm uh tonight you are working on deliverable one which is due this coming monday 11 59 p.m and i will see you back here on teams tuesday morning thanks very much everyone see you later 
JMzYmEl2cWM,28,"Tuesday at 5:30 p.m.: #womaninscience Dr. Elisabeth Churchill from Google will help you understand people’s social and collaborative interactions in an emerging landscape using Artificial Intelligence.

Special thanks to our Platinum #WEP2018 sponsors: Saudi Aramco and Sabic",2018-01-16T16:03:01Z,"WEP2018 TV: The Past, Present and Future of Human Computer Interaction",https://i.ytimg.com/vi/JMzYmEl2cWM/hqdefault.jpg,KAUST Official,PT1H16M31S,false,3070,30,0,0,0,"good evening good evening welcome everyone welcome everyone to this evening's winter enrichment program keynote lecture my name is Jeff Shama professor of electrical engineering here at causa and I'm very pleased to introduce our speaker tonight as dr. Elizabeth Churchill who is currently the director of user experience at Google Elizabeth is originally a psychologist by training she received a PhD in cognitive science from the University of Cambridge her research has been in the areas of human-computer interaction computer mediated communication mobile computing and social media with a focus on understanding people's social and collaborative interactions in everyday and in physical contexts her prior experience before joining Google includes director of human-computer interaction eBay research labs and principal research scientists at Yahoo research where she founded the Internet experiences group among her dr. lives us many distinctions she's a distinguished scientist of the ACM for those of you are not from the cs and computing community ACM stands for the Association for Computing Machinery it is the world's largest scientific and educational computing Society where she is also the secretary treasurer she serves on the advisory board of a number of university departments and she is a distinguished visiting scholar at Stanford University's Media X Institute we're very pleased to have her this evening the title of tonight's lecture is the past present and future of human-computer interaction which is a topic that touches all of us and so in recognition of this topic I'd like it all to put away your smartphones and join me in welcoming dr. Elizabeth de cows [Applause] thanks so much thank you for having me here I'm very excited to be here and although everyone's put their phones away I'm gonna put this down here I'm gonna be talking quite a lot about phones and about interfaces on phones so for the purposes of looking at your phone screens at the right moments you're allowed to bring them out so human-computer interaction as a field of study it's a discipline concerned with the design implementation and evaluation of all interactive computing systems that are for human use and with the study of major phenomena surrounding them this is my favorite definition it comes from actually the early 90s but I still use it a lot and human-computer interaction as a set of courses and a set of practical exercises and a way of thinking about the world really has started as that have become part of computer science curricula in the States and that's in part because of various associations like the ACM and the work that we've been doing to make sure that when we build technologies we think about the people who are going to be using them and the societies that are going to be adopting them so it's a discipline that's it's more of a sort of field of endeavor with multiple different disciplines contributing including you know things like the technology the devices and the interfaces that we see every day and I think some of you are building some of the infrastructures that lead to the devices that we use all of the time interaction and service design so there are aspects of using computational technology which is very much more like a service than a quick interaction you're connecting to your bank or something for example and there's a whole service infrastructure there HCI scholars think about the physical environment and indeed a lot of the history of HCI human-computer interaction came from ergonomics the sort of physical devices that people use every day and the layout of things like cockpits so the physical environment in which you are makes a huge difference to how you can use something a great example would be has anybody been out and you are looking in this bright sunshine and you look at your screen and you can't see the screen anymore the physical context of use makes a huge difference people in practices so at the center of human-computer interaction as you would expect our people is thinking about the people's perspectives and the things that they do every day their practices what do they do with technology and why and finally a lot of scholars in HCI draw on understand to understand societies and the policies that you have because any society you might have a policy for example around the storage of personal data something that I think is very close to all our hearts and concerns and the Society's policy the governmental restrictions on how data is stored and can be shared will have an impact on the experiences you have of services and of the computational devices and services in your life so human-computer interaction draws from all of these different areas and contributes to all of these different areas and I'm going to keep trying to explain that as we go along and I thought to to just situate some of the work I'm going to talk about I'll talk about some of the other kinds of work and kinds of conference that are sort of human centric that an association like the computing machining machinery Association sponsors so we sponsor many many conferences and these are some of the ones that really deal with people and technology you've all heard the words user experience I'm sure this is a sort of field of endeavor that sort of came out of HCI and various other fields but it really tends to focus much more on technical technology devices and interfaces and the interaction and service design and not the other aspects that I mentioned and as Jeff said in my introduction I'm one of the directors of user experience at Google and Google is investing more and more in this area so there's actually quite a few a few of us now and we tend to focus very much on the devices and also on some of the interactions and services that we have for example Gmail maps and so forth another area of endeavor and investigation is user interface and software technology this is a conference that the ACM sponsors and that tends to focus very much on the technology and trying to think about where the future is with those sort of new interactions that are being built new kinds of for example pen based interaction or gesture based interaction ubiquitous intangible computing this is these are other terms you might have heard of and these scholars tend to think about the physical environment a lot more they try to think about how computation devices is ubiquitous it's starting to become embedded in our physical environment think about for example smart homes and we'll talk about that a little bit later remembers areas such as computer-supported cooperative work and studies of social media which tend to look at the technology but they're really focusing a lot more on this people and practices aspect because they're looking at the cooperative work so think about your work environment and how you collaborate with people every day whether you're distant or together and how you share resources in real-time or asynchronously there's a large amount of research being done in that area so there's HCI and HCI touches on all of these and contributes and draws from all of these different kinds of areas and you'll find HCI scholars going to these other kinds of conferences as well to specialize in particular areas but to contribute to the bigger conversation so if we start to think about this in terms of you know society and culture I just wanted to show a slightly different representation to give you a bit of a framework so if you think about the infrastructure at the bottom all of the kinds of things that you know we do to make the internet work to have devices any anything that we interact with the internet or any kind of you know computation has a some kind of surface and I'm working a lot right now on like the glass surfaces and our phones I work on phones and tablets and so forth on a design system called material design which is in fact a system for screens but draws on physical materiality and I'll illustrate that but you can also imagine an interface being you know voice interface more and more voice interfaces are coming into our homes so that's another kind of interface and so if you're interfacing to resources through you know touch or vision or voice or typing the interactions that occur are basically the logical sequence of things you do in order to get something done so as we're going up here you can think of those as sort of micro interactions you start to go up to some of the interconnections like I was just talking about in collaborative work between different devices and different people and you starters go up into organizations and so the kinds of collaborations the kind of communications you have fit into sort of organizations so if you're collaborating it's in your department it's with your fellow students an organization could also be a family organization if you use social media to connect to people with whom you have some familial relationship or it could be you know your neighborhood and then you start to go up to more macro structures which is like you know what's how do you communicate with the government more and more government resources are coming online and as you're going up you're starting to think about these are sort of economic and social and socio technical large-scale systems and so this is one of the ways that scholars have talked about these different these different sort of you know dimensions if you like the sharp end is where you're actually touching something on the device here you're interacting with your favorite favorite social media site right that's the sharp end of interaction at that micro scale but that interaction is affected by all of the things that come down from the top end from the economic pressures from the guidelines and from the restrictions that might be in place around as I say the sharing of personal data so whenever you're designing for the users who are people you know in their fullness you're always inheriting something from that bigger set of structures and the question is you know how do you make sure you take those things into account think of it as sort of design constraints so where I'm working most of the time is at this level and that's where I'm going to stay very much today I'm always thinking about the bigger picture and always working with people who are building infrastructures but my particular focus is right there around interfaces interactions and interconnections between people but as I say you always having to take this into account a good example of why that or how that can be the case as if you're in a different live in a different part of the world where there might be different kinds of services available restrictions and or cultural norms about how technology gets used another thing that we should think about is as you're studying all of these different levels as people are participating in these different levels you know how do you start to communicate the insights you have and I'm gonna illustrate later how a design system can illustrate can embody knowledge because you've got technical innovation at the bottom and you're often sharing that engineering or computer science or whatever innovation in the academic context through conferences and peer review maybe patenting maybe prototypes maybe startups but if you're at the research insights level which is where a lot of my work is and my team's work you know again I'm publishing in academic contexts but I'll obviously sometimes be pushing things out on blogs or I'll be going and giving talks or I'll be trying to talk to product teams about how to take up the research insights into products I'm giving talks internally now as you start to go up the knowledge that you have from these different research areas gets codified in different ways and this is how you scale knowledge right so we scale knowledge in the academic context through peer review and publication and replication and collaboration but you can also take principles and you can create guidelines and I'm going to illustrate that so that even people in you know vastly different areas can read the summary of what it is that you've found out for example about human perception through things like principles of information grouping in interfaces so those principles are derived from psychological experimentation but go into guidelines and best practices so that anybody anywhere can take them up and put them into practice and of course at the top of all of that would be something like standards so you know you get standards bodies who are very concerned to provide guidelines for the development or the design of interactions for people and this is a good example this is the w3c standard on basically legibility right so it's ensuring that a contrast ratio of at least four point five to one exists between text and images of text and background behind the text that is a guideline that has come through years and years and years of research and discussion and experimentation but it's out there so that if anybody who hasn't got a background in visual perception wants to create an experience for a user that is perceptible this is a good guideline for them to follow so there you have you know a best practice codified in a guideline that is really summarizing a lot of research and the talk I'm gonna give today is really about how we're trying to do that through this design system called material that I'll tell you about so with that as a sort of framing and a background to try and illustrate how you know the big picture and the the very small effort can scale I want to talk a little bit about the past of HCI as a discipline and you know this is the ACMs special interest group again the Association for Computing Machinery this is a special interest group on computer human interaction I think it's about I think it's the second biggest group special interest group of the ACM the ACM is about a hundred thousand professionals and I think sig Chi is currently sitting at around for between four and five thousand and these folks are interested in human-computer interaction and this is a picture that was taken from the first conference of human-computer interaction and that was in 1983 and so if we sort of go back in time to 1983 this is a little bit before 1983 but you know a screen a surface and people sitting at a desk and you know typing and I wanted to put this over here to emphasize that point that I mentioned earlier that a lot of HCI really did sort of come from ergonomics physical ergonomics a lot of scholars had that as a background but the work that was being done was to understand how people could perceive on a screen and understand the information on a screen which was not the beautiful graphical interfaces that we have now so how do you help people understand and one of the fields that came out then was this idea of cognition thinking cognitive ergonomics so this is ergonomic of the body how do you design something so that it fits the way people think that fits the way human information processing occurs so that's a sort of thing that was really driving people to that first conference and that first conference they didn't expect very many people to show up and I think they ended up with something like 900 people and they were expecting like a hundred and what that told them was that there was this giant entry out there in making these amazing devices accessible to people because people understood the power of bringing these devices computers into everyday lives not just working lives but it's like how do you make them usable so this is what was going on in 1983 and yes it makes me feel a little old because I remember a bunch of these so you know there's these are from the Computer History Museum so if you ever have a chance and you want to sort of think back into the past of you know computation and devices and consumer electronics the Computer History Museum museum is a great one to go and look at but I remember sort of cd-rom starting to arrive in the post and so forth but one of the things that sort of consumer electronics sort of evolution one of the things that people were really thinking about then a lot in trying to get an understanding of how people how could we can get people to use these devices was the distinction between discretionary use consumer use and required use and so required use might be you have to use these devices for work so automation systems at work were already starting to you know computers were appearing on desks and people were using computers and the required use of course came with a lot of training so you were trained how to use these devices and there was like you know support internally so you'd start as a novice and you'd get training and if it was really critical for your work you'd get a lot of training so you know in safety critical systems which were using computers there just be a huge amount of training that went along before you could pick something up music you certainly couldn't pick it up and you know just get away with it straight away so you would develop expertise discretionary use how we started off with our phones and so forth it was discretionary use and it's increasing you're decreasing ly discretionary because this is how we connect with each other and as I say to think you know governments we could've go online and that that line around mental models is really important because as you go from a novice who knows nothing to an expert you're building a mental model of how the system works and I'm gonna try and illustrate how we have scaffold it if you like given people ways to understand a very complex system through metaphors and you help people sort of onboard to think about how to use something through metaphor so you can train people but you can also leverage what they already know and a good example will be something like email a lot of the early email systems try to use the metaphor of posting letters and mailing things so that you would send stuff off and you know that can work so far and then break down so you're always thinking about how brittle using a mental model is but the early work in HCI was really focusing on this it's like how critical is the system and how do you build expertise and how do you leverage what people already know so that they can quickly start using something that is in fact blank hey wait why is this blank all right okay this seems to be working all right this is the thing you never want to have happen right as I was saying human-computer interaction I'm just gonna plug this back in here see if I can get this to work some reason oh here's another great human-computer interaction passwords how do we feel about passwords in the audience we're gonna come to that one later I win thank you I win all right now no I don't win isn't this fantastic all right I'm trying to understand why this is happening isn't this always the way I'm gonna do that performance art thing okay doing a V so that's on but can you plug that in to something else what I'm going to show you so that idea of the metaphor I'm going to show you the actual pictures but I'll whiz through them quickly so one of the first things that people did was they created the GUI so the GUI came out in the sort of late seventies you know early 80s was really being sort of tried out so the graphical user interface the first model that people used in your seer was using a metaphor of a desktop so you'd have piles of paper and you'd have a little trashcan and the idea was that people who had no idea about how to do computer science or to use a computer would be able to just use their knowledge of what it was like have a desktop to file things or put things in the trash so I've got a few old examples of that and there was a fantastic paper that was written in the early 80s by a couple of scholars who have severely well known at the beginning of HCI and it was called analogy considered harmful and what they started looking at was when does a mental model when does something that's a nice model like the desktop actually break and I'll give you a personal example which was the first time I used a Mac and you used to have to drag the image of the floppy disk how many people in this audience remember floppy disks oh thank goodness used to drag the floppy disk into the trashcan and I remember sitting there utterly frozen because I thought if I dragged my floppy disc into the trashcan would it wipe the floppy disk it wouldn't just eject it right so there was these ways in which the analogy was breaking down for me and as you see you'll see as we're going forward how those kinds of analogy worked effectively for some situations and not others and there are some spaces in which you just can't get away from doing the training what is happening oh you're kidding why is it not coming on here it was on so the the big thing you can't do without with human-computer interaction is even got a battery it's not working I mean have you got your mic charger you don't have a charger it was working when I plugged it in earlier because when I went over to the loo it was working this is it a new map but if you've got maybe gotten of these have you got one of these you think's that's blown always have a backup right okay so you think that's blown does anybody want to know what happened you laugh when you see my last slide I promise you know so you know what I was saying about computer-supported cooperative work that uh nothing works without an expert what's that oh there we go the lights are coming on yes [Applause] is that gorgeous all right now where were we here we go all right Dada yes so 1980 so this is what it sort of looked like you know to sort of do programming and this is what I was saying the Xerox star in 1981 was the first attempt the graphical user interface to start laying things out to use people's existing mental models so that we could much more quickly get people up and using the technologies and we'll come to direct manipulation in a minute but this was general magics magic at which the communication device and you can see how it's so directly trying to use the knowledge people already have of these different elements in their lives to get them to take an action so in each of these icons is inviting an action and when I sort of you know upside that you could look at your phones take a look at the icons that you have right now and start to think about which ones you know are really obvious to you and you obviously know what to do next and which ones are less obvious like for example the hamburger menu do people know what I mean by the hamburger menu those little lines along the side usually at the top left or something where you click on it and you get more information this was one of the great innovations the hamburger menu and I would love to know how many people were totally confused and who were still like looking at it and wondering eh why it's called a hamburger menu but be who came up with that so this is Alan Kay and 1976 this was at Xerox PARC this was the first desktop model that was really sort of tried out so you know there's your contacts or whatever in your calendar so it looks like a calendar that you would have on your wall so you know everything old is new again or something now this was work in the early 1990s so you had the graphical user interface but now you started to get direct manipulation and people like ben shneiderman and others were saying oh look you know all of this it's like this sort of physical it looks physical but what you have to do is drag a mouse over and click on it let's start to actually use direct manipulation in the system of the beautiful work that he did with Catherine Plzen and they were looking at real sort of toggles you can see up there they started to do experimental work to look at the toggles and to try out to see this was to get somebody to try to manage their air conditioner which I'd still find to be almost impossible personally but yeah so they tried out all of these different looks and feels of this these toggles to see which one's worked best and you can actually start see I think that's slide to unlock looks a little familiar to a lot of people maybe and you know we still have the direct manipulation things down here which were really inspired by a lot of this early work and you know I was saying about that idea about principles and guidelines many folks who are implementing these kinds of interactions right now may not know that the roots of the work was back in these HCI and human factors people way back when because the ideas have been bubbled up into best practices and guidelines and that you know if you're in a peer review committee might feel like oh it's not great because my world didn't my work didn't get credit but if what you want to do is scale and get really great human centered practices out into the world this is a perfectly fine way to go to get ideas out and disseminated and you know the Android ecosystem for example is full of a lot of developers and designers who are building experiences for everyone in the world and so getting really good guidelines out there is very important to a company like ours and I just wanted to put that in there because that's slight swipe to unlock was very much I think you know part of the experience that people thought was brand new when they got their iPhones so how did all of this knowledge all of those experiments like Katherine's and you know other people's experiments how will we disseminating that knowledge originally well we were building models like this this is the model human processor this was the first book which had the title human-computer interaction the psychology of human-computer interaction but drew explicitly on experimental work with people to try and understand the cognition of how people do reasoning and to try and make predictions about what would be hard for you to do and what would be easy what kind of icon what kind of programming tasks would people find difficult because of the way human memory and human information processing works and what could we do with a model like this to make predictions about time taken to succeed about errors and about confusion so this was a sort of model which you would sort of run as a designer to make predictions and there was a great study that was done which was looking at telephone operators for 9x I think it was and they basically built models based on this kind of analysis of human information processing and made predictions about what was going to be easy or hard and then they demonstrated through empirical evidence that the model played out and they ended up saving a huge amount of time for the operators on the basis of this the other way we codify knowledge is through books and that's a great way you know if you're in class and your professor says you must read these books but it's not necessarily a great way to get all of the ideas out to a general audience so as I was saying summarizing all of this research guidelines and principles can be a really great way to take all of that rich knowledge codify it and share it and this is a great example that still exists today this is the Apple guidelines and you can see that a lot of that work which a lot of people did to demonstrate you know how to make something usable ends up being codified in these really really simple rules and guidelines for designers and then you could take each one of these and unpack those this is one of my favorites this comes from 1996 this was apples Newton which did not succeed it as a product but I still think the guidelines are absolutely fantastic for a couple of reasons one is because the guidelines incorporate human information processing insights particular recommendations for design and code and so this was you know one of the first ones that I came across where you could actually see all of that in one place it wasn't just a make it beautiful it was and here's how you can do it and here's why you want to do it and here's what beautiful means in this context so design systems so all of that was to give you some background in thinking about how design effects interaction and experience and how we can codify that knowledge into design systems and so I want to start talking now about material design and material design is Google's design system and Google's design system material to i/o you can find more information and so there are many many many products that Google has but also as I say the Android ecosystem plus iOS a lot of experiences and apps out there where what we want is for people to have a sort of beautiful lovely experience and the story goes that Larry Page several years ago asked us to address making Google's products more consistent and more beautiful and Google was not known for design that has since changed in a large part because of material so in about 2014 a bunch of folks started to think about what should we do for a design system this is the website you can go to material to i/o and you'll see guidelines and resources and also tools tools to help you sort of understand how to make to apply some of these specifications so the material design team was sort of organic and it built up and material design everybody gets confused because it isn't actually material it is inspired by material as a metaphor it's a philosophy a metaphor some specific guidelines some specifications components and it drives an experience but also brand how you can express brand and there's a community around material so the specific metaphor here really is on tactile reality and I'll show you some examples and the idea is it's the visual interface it's the typography the visual it's the interaction design and it's the motion design animation in the interface and when you take an action what happens and the idea here is that you know the innovations in our personal technologies and we can take advantage of those but you really want to bring that digital interaction and computation to everybody so how do you get everybody you know I talked about the novice to expert in the mental model how do you get people on boarded feeling expert really quickly and how do you allow people to not just do discretionary and recreational things but productivity activities as well and these are very different kinds of app experience if you look at your phone you're going to have a load of different apps that have different purposes and different levels at which you want to sort of learn you know spend the time to really understand so the idea that was material design would be a single underlying system that allows for a unified experience across different devices and platforms because of course we move across from tablets to phones to desktops now so you can go and have a look in the spec but you know this idea is that you've got this material metaphor you've got bold very very beautiful you've got a bunch of fantastic designers who really have thought from almost like you know the oak couture of design and color schemes to render something that's aesthetically beautiful but also where information can be presented in ways that are salient and this motion idea is baked in so here are the sorts of experiments that the designers were doing and continue to do before implementation occurs so materiality is represented through for example layering and using shadow to suggest what is at the top and what is at the back what I'm hoping is that if you haven't got a design background as you go away after today you'll start to look at the apps on your phone differently and start to think about how the designers and the developers are trying to suggest to you how to take an action and what an action will do so here you they're using shadow the idea of bending so material design was really there was a lot of inspiration from paper and so how does how do you imply that in a rendition like something like this and inspired by designers of the physical from the Bauhaus there were a lot of workshops that took place where people actually started to work with physical artifacts and physical paper to say are we just making it up or how do we really render this design beautifully in the interface so here are some of the design studies and you might start to recognize icons and so forth here so what does it look like if you have light from different angles what shadows are created and of course you're starting to see how that ended up being this do people recognize this icon so you know it was really inspired by this idea of of creating a sort of depth in the interface to make it visually appealing so again more experimentation with shadow and perspective and a little bit of color there for salience you sort of draw the eye here's some sort of device layouts and what does it look like in context so really trying these things out and this was you know trying to think about you know materiality and hovering and space in other ways so these are all the kinds of experiments that are being done again to do that kind of scaffolding using visual perception to get people to see things in a different way and this idea of natural is really interesting so here you've got the layering this is illustrating the layering and it should be natural that this thing that's right in front of you is actually the dominant foveal visual activity and what's behind is secondary so the color the surface iconography are all used to emphasize different kinds of action and a lot of experimentation has gone on to thinking about dimensionality of fording interaction and that concept of affording is suggesting and interaction to you and inviting you to it so that you have a sense of what's important in this particular interface that you see so this is the elevation model that what's on top and what's behind and how is a shadow cast and if you start to look at the apps that you're using start to think about how the designers might have really tried to render this so there you can start to see the layering and you can start to see again I hope what an app might look like so we're looking right now at the design work but behind this is a lot of principles from design that have come from a lot of experimentation this I hope you'll recognize start to see maps how information is layered on a map how what is at the top and what you can touch and interact with and what the response and action is going to be so you know the visual thing at the bottom is providing the pallet if you like and then you start to see these different layers of what's important based on what we understand that people want to do with this information and the idea here is that you use these kinds of buttons so the icons are designed very specifically to initiate change to to get users to invite it's like you press that microphone and you have an idea that that is going to be something that you can take an action on what is it going to do so the animation also so it's getting more and more complicated right because there's layers of information which is the action moment there's motion occurring and how do you choreograph motion to happen so that it's not discordant and things all going in different directions so you confuse the user visually now this might all look sort of easy or beautiful or whatever I hope but these are actually really really difficult to bring about and to try out with people it's a different scale but a lot of people talk about VR making them nauseous so there's a huge amount of work goes into how the visual perception will not affect the physiological reaction here we're trying to do the visual perception will not confuse the cognitive processing it'll be clear to you what to do and it will be designed so that you understand what's going on and that your actions have feedback of the right kind so again you've got this motion and it provides meaning but the motion might be different for a music app than a calendar what kind of motion are people expecting as something moves will it occlude information that's actually very valuable to the task how do you want to make more information salient so it's also really hard when you think about adaptive design so I said you know you've got your tablet you've got your phone you've got other things so how do you adapt across all of these different devices I'm a designer or I'm an engineer I've got this specification that you've got this beautiful motion that's gonna be sort of choreographed and not occlude information it's not gonna move so fast that the users are like oh goodness me it's not gonna go so slow that they think that their device is somehow broken and that might be different across different amounts of surface area so something that's on your screen that's this big on your phone how it traverses on a tablet is likely to have a different trajectory but perhaps a different speed and that has perceptual consequences design consequences but engineering consequences so how do you engineer to make sure that the experience feels the same even though the vision layout is different the amount of eye gaze the you know that you can look the target area is different so there's a huge amount of work has gone into also looking at different devices because there's different devices of different sizes but when you are working with people in the Android ecosystem of many many many manufacturers and many different kinds of just phone for example starting to look at all of these different phones and all of the different dimensions and also the density and you know the how if you like you know how refined the particular phone is you've got a different set of experiences so how do you actually create something that feels like it's the same well you have to have this the engineering specifications to know and to try and test things out so this is one of the tools you know I said the design system has guidelines it has specifications it has suggestions for visual but it also has suggestions for engineering implementation across different kinds of platform so that's why it becomes a system as opposed to just a set of guidelines because you're really trying to give people the tools they need to engineer something and these are some of the different things so I have not been working in this particular area for that long only a few years and I had no idea and so I started working to it just how many different kinds of elements there are that go into making a very simple feels like a very simple interaction you know you've got action buttons scrolling techniques you've got dialogues typography I want to show you some examples data tables how do you lay data tables out so if you look at spec there are all of these different kinds of elements that you want people to be able to draw from and because it's a giant ecosystem people are building new elements all of the time and also contributing into that broader community but you know buttons notifications how does a notification pop up what's the animation for it does it interrupt what's its visual look I'm hoping like I say you'll look at your phones later and you'll be like I had no idea there was so much if you're not in this area you're already in this area you know so you know here are just some of the things trying out the different interaction elements I mentioned permissions and notifications you know trying different things out you know do you have like that here are some different ideas sort of you know swiping here's some different layouts this is back for 2014 I I dia of redlining and how you have things that are the right proportions for the screen size and how you do layout this is drawing on a huge amount of perception but also typography and layout so you've got visual design elements coming in and you know helping a lot of very smart engineers to understand this is why this matters here's how to implement it this is the latitude you can have and this is more of the work of my team very specifically so my team is the research team that starts to look at some of these elements so you'll have for example here we have text fields and I talked about affordances earlier there are like sort of seven or eight different kinds of text field that were being tested out and so what we did was we did a series of studies where we use the Mechanical Turk and had hundreds of people literally try out the text fields and all of the text fields fit within the general guidelines for material but some of them were more explicitly you can see some of them used a lot of shadow some of them didn't some of them were just lines this is just a couple of the conditions and then we looked to see where people clicked and we got their subjective responses and we had them do tasks and then we generated a lot of data to say you know here are the ones that perform really well in task and here are the ones which may look aesthetically pleasing and be part of the system but it seems that they perform less well for people more generally so here we've got design which is based on potentially years of looking at you know people reacting to different kinds of elements and now we've got experiments which start to validate and/or give some kind of more nuance to those designs and so as I say we've been looking at these and making recommendations and as part of the design system we're asking to start publishing our results and our methodology for studying these so that again folks who are out there in the border Android ecosystem who may not have user experience research on team can actually learn a little bit about some of the best practices for evaluating how their interfaces are working for people so we're gonna start publishing some of that soon here's another one I mentioned legibility and I showed you the standards earlier this is you know just one of the studies it's trying to understand how text whether it's legible or not over very rich images and over different kinds of saturation and we've literally got the contrast ratios up there we looked at what the guidelines said and then we started to do different kinds of treatment over the image to see whether the image was obscured if it was was that important was it not but could people read what the text said and we've just started getting some really interesting results through that so again one of the things we're starting to do more and more of is include accessibility and sort of diversity in terms of for example in this instance you know sort of visual perception to really put that into the guidelines as well and this has consequences for the devices I mentioned earlier because if you have a sort of low resolution device you might have a very different experience from if you have a very high-end device so we're gonna start really trying to test about what are the what are the sort of parameters if you like around which you can play around with some of this stuff icons I mentioned earlier there is a huge resource for icons and we'd be started to do some tests around which are the right icons which ones are perceptible back to the mental models which ones are easy to understand and which ones are confusing and here's something I'd love to ask of this audience if anybody knows any research or has any in science into cultural differences in terms of icons and what works and what doesn't please let me know because I'm really interested in starting to find out which which icons actually work across different cultures and which don't again not if you're an expert and you know what a hamburger menu is but if you're more of a novice you know what strikes you is just utterly confusing because we don't know and I mentioned fun SCI happens I just happened to be a little obsessed with fonts I love fonts google fonts has many many many many fonts and we have a huge amount of use of these fonts and there's some really lovely studies that are being done at the moment which are like how does how does a font suggest a personality for your site you know if you're a bank what kind of font will you use you know if you're more playful what kind of font and and there's a lot of evidence that shows that people's subjective and emotional reactions to fonts are much stronger than you would think has anybody ever had an email from somebody instead of like you know bold 32 all caps read I mean that's not just a font but it's also just a bit jarring or comic sans do we know what Comic Sans is Comic Sans was like was so big and then 1990s and now I see Comic Sans and I'm like [Music] although I actually kind of still like it a little bit but these also have perception issues so one of the sets of studies we've been doing is looking at what fonts are better for reading at distance on large displays and you know it's just it fascinating what's working for different things this is something else that some of the folks on the material design team did which I thought was really lovely frankly they did some analyses and they said that there were in the emoji sets there were very few women represented in any kind of professional capacity and so there were a bunch of studies done and a bunch of these were produced and then we tested them out and tried them out and here are some of the emoji for professional women because they just weren't enough so the other thing I said was that material design is not just it's not just a system and a set of guidelines and a set of inspirations material design isn't just an invitation to kind of think about consistency and the sort of human user experience it's also a community and so span as a conference that Google runs for designers and for engineers to really think about the broader community of design and these kinds of activities have been very much part of putting Google on the map for design and there are design awards which were kicked off at Google i/o in 2015 and this is really recognizing the great work that designers are using and the great kinds of contributions producing and the great kind of contributions that they're having to the design conversation more broadly so here are some examples so this was the Google web interface in 2011 and this was the sort of next iteration as material started to come in and you'll see that it's actually changed even since then and this idea of space and this idea of color and this idea of these different buttons and here you're starting to see the evolution a little bit of some of the shadow with the compose button here are just some of the external folks who have been using material design guidelines to some extent and material design spec and these were from the first a real big refresh a couple of years ago but I think you'll see these are the kinds of things that people said they wanted and that they were getting from using material design so they'd have their app they'd use material design and then they'd take a bunch of metrics and measures and here are some examples they've found more engagement with that floating action button which was one of the sort of innovations material brought in you know more revenue so people had you know I said about taking action if you make an action salient more people taking action so there's the aesthetic experience of liking using the the app but then taking an action becomes easier because of the salience in the interface delight people saying reporting that it just felt better the color schemes were better and I haven't done this research but I'm really intrigued about this idea that these kinds of aesthetic experiences just make people feel calmer or happier or whatever the fire began that floating action button is the fab animations so you know people using animations and trying to think about how animations will bring the eye to the action or give the feedback that the action has been successful understanding features this one was boosting use their engagement with more features so anyway so those are some of the examples so material design has really started to create conversations and give people some practical solutions and a lot of the focus especially out in the bigger world really are you know they might have they might be small and scrappy they might be startups they might not have a lot of resources for designers and user experience researchers so that's that amplification and that sort of impact factor that I was I was talking about so we're thinking about how shadow and elevation works in more complex sites like this we're thinking about super apps from China and you know does material design fit there or does is it not open question material design already has this idea of elevation how do you take material design or something like it into augmented reality what does it mean to have a design system and a design spec around oriented reality here are some of the other devices someone was asking about Google glass earlier what kind of design system for interaction do you have when the surface is a brain or skin or under the skin what kinds of guidelines do we need for the future for people we've already seen something like material is working very nice on these four screams what about what do we need to do in this space as this as these get taken up more I've been talking about accessibility and usability and we're increasingly making sure that we bake diversity into every test so that we can really understand what a range of different users with different kinds of potentially impairments how can we design for them devices that have no screens what does it mean to have a design system for conversation and actually there's a whole bunch of effort at Google right now which is you know sort of conversations on Google which is how you design a really great conversation so these are the interconnections of ubiquitous and tangible and wearable and embedded embodied practices for which we don't have good design systems yet we don't know how to design for these interconnected devices very well yet and the complicated smart-home how do these all fit together if you've got five different ecosystems with five different kinds of standard for their interface and interaction design I don't know about you but if I go from one ecosystem to the next I'm often pressing the wrong button or saying the wrong thing so those are just some interesting challenges for design systems and for best practices as these devices come into the world and we need to understand more about how they're being used by people in the world so if that's still at the interaction level more of that pointy end the sharp end there's also a lot of work to be done in human-computer interaction at that blunt end and I just wanted to show you this because how many people know about Maslow's hierarchy a few of you yeah great well this is a classic old thing from psychology right and it really talks about you know at the top you've got growth and at the bottom you've got survival and your needs right so when I was thinking about what we've done with interaction and human-computer interaction but with technology in general I'd say with you know we're doing pretty well on the aesthetic and the cognitive especially with something like material and these designs and all of these years of understanding what how to present information to people I put a question mark around esteem and belonging and love because people tell me that social media makes them feel loved other people tell me it doesn't and I think the jury is out so I put a question mark but it's an orange question mark and the red question mark at the bottom is I think we're really at the beginning of thinking about safety and physiology and these these tools safety for example I mentioned data safety earlier you know I think we're at the beginning and that is part of human-computer interaction we've got these gorgeous devices you know we've got these self-driving cars and all the rest of it this was a few years ago were a bunch of people hacked into a car while somebody was driving and basically took over control of it while he was driving doesn't feel so safe and we're hearing more and more about hacked homes and ransomware you know these don't feel safe there's a bunch of stuff that's happened more recently and this is my favorite quote from William Gibson the future is here it's just not evenly distributed so I put that blunt end we've got there's me maslov's hierarchy again some people don't have this and I just want everybody to laugh with me at the next slide when your power thing blows you get to sit while I fumble for a while right down there finally artificial intelligence we had a lot of conversation around that and algorithmic bias we really need to challenge this because all of those interactions and interfaces and all that gorgeous stuff that's happening information recommendation we don't really know where this is going and we have to really investigate it because folks who are really at the sort of infrastructure layer are affecting the pointy end and the blunt end and it's giant amounts of bias so here's the big picture and as I say HCI human-computer interaction deals with all of this if not directly then in partnership and we really really really need to think about all of these different aspects and no one group or one person can do it all but understanding that they affect each other I think is really important for making decisions about what one does in one's research or in one's practice before I stop I also wanted to say that you know going forward into the future the ACM is sort of a you know there are many professional bodies ACM is the one that I happen to be very involved with and we have a thing called a future generations of computing the future of computing Academy this started last year the first cohort was 48 people who are early in career applied and got grants to become this first Academy and to work with each other and this was a cross computer science not just HCI although there were five people from HCI and the idea here is to provide a platform and a forum for folks who are thinking about everything from the sharp end to the blunt end from infrastructure and heavy technical to interaction and social to think about where we are going in the future and so if anyone wants to know more about that please let me know because we'll be getting our second cohort fairly soon also I wanted to put a shout out for the women in computing F it's at the ACM because another part of bias is often that you know it's not just women but diversity in general in computer science we're trying to really address that finally I wanted to give a shout out there is Ereb HCI and organizations start in 2016 we're trying to think about the the specifics for this region in the human-computer interaction arena in terms of sort of cultural needs and different perspectives and bringing a different voice to the table but also about who is participating and who is not and how can we get a diversity of voices because of course we all you know unconsciously bring bias on our own cultural bias when we are building in designing systems and so the more diversity you can have in your sort of collegial group the more we can address those biases and you know design for everybody and have equitable computing and equitable human-computer interaction and I think that is also the future of HCI and the future of technology design and so with that I want to thank a number of people the team who brought me here in particular and Jeff thank you for your lovely introduction and also to the material design team more broadly which is about a hundred and fifty people about 50 engineers 50 designers and a bunch of other sort of collaborators and my team in particular the material experience research team who are really bringing research and grounding out of the design and I wouldn't say validating so much as extending and expanding you know our knowledge of how to apply the design principles so with that thank you very much [Applause] okay Thank You Elizabeth and we do have an opportunity for questions there's a mic going around and okay what would be the main argument of creating this Arab as CI in the questions like do you have an Asian s CI for example yes now I went to be clear I did not set it up and we did not set it up but one of the great things about the human-computer interaction community generally whether ACM are not is that there are a lot of local chapters who self-organized to come together and so there was a bunch of folks who basically felt that there were scholars in the sort of Arab world who would potentially have their own ideas and their own issues about cultural specificity but also would want a community so that there are people who are close who they can talk to about you know particular designs or just build community so I was involved in helping set up a Chi chapter out in Indonesia and so there's a lot of local area chapters but a lot of the ideas will bubble up to the bigger community there's a couple of chapters in India so ACM SIGCHI has a bunch of like different chapters I can put you in touch with but this particular community I don't know them personally I just know that this is a groundswell of activity of people who want to locally discuss you know what's special and what's not and just build a community a good friend of ours has brought us a workshop to the so the computer human interaction General Conference that happens under the ACM once a year it's called CAI Zhi and that's about 4,000 people and we have workshops before the main conference starts and there is one which is specifically focused on just for HDI in the Arab world this year organized by a friend and actually ex PhD student I was her advisor my question is the following what is the reason that people switch from this there was a thing called skeuomorphism like ten years ago when objects on the screen they were like try to make them as realistic as they are in the physical world but now then goes to more like simple minimalistic kind of style like material design and Apple goes the same direction what is the reason for this why designers I would like to speculate and why designers do what they do because I'm not a designer but I think there were a couple of reasons that I would speculate it around firstly there's a sort of fashion reason right so you know design moves in fashion right so I was thinking about the old Couture and Main Street but I know that the idea of a lot of white space is that it's sort of aesthetically pleasing calming in that you can actually make certain things more salient than other things so that if there's a lot of clutter it's hard for a user to know which action to take what they should do next and so making everything much simpler is an aesthetic desired aesthetic if you like now where that doesn't work so much typically it seems and this is a hypothesis that we're testing is that in that expert zone around for example work tools where you've got someone who's an expert information density what you perceive to be dense information is very different when you're an expert for example looking at a spreadsheet which you understand then when you're a novice trying to see what's salient in a very complicated spreadsheet so the specific example you give which is of white space it could be that that aesthetic is really really pleasing to people and invites actions that are really clear and clean and crisp in certain situations but that in other situations that might be the right design so I think it's sort of open it's open but I think that there is partly fashion and there's partly this aisle here of creating beautiful salient and simple elegant interfaces so that the human visual and cognitive processes are not overloaded that would be my hypothesis but we should ask a really good designer which I am NOT I feel like you know when you're on the plane and say is there a doctor in the room is there a designer in the room we need one so thank you for a presentation there's there's been a lot of work on user experience you should know a lot of good examples about what's being done at Google but also software is changing a lot right so so we're moving to this space where a lot of the software is actually built automatically automatically through machine learning so in a certain way the user and the Machine are interacting in a way which is maybe different from what it was five 10 years ago yeah so does it also make sense to start thinking about machine experience yeah meaning that maybe you want to design in such a way that helps the Machine learn faster or the Machine in the case of failure because machines are starting to fail a lot more this makes sense it absolutely makes sense and yeah and there are people working on that and it's really a very important area I'm actually running I'm Co organizing a symposium under a triple AI which is going to be at Stanford in a couple of months and it's the user experience of a machine learning is what we're calling it but one of the aspects of is the machine learning machine experience of the human and trying to kind of unpack that so that the it's dialogic in an interesting way and you know it's very early days on some of that stuff but I think it's really important the other area I think is really exciting is the idea of applying certain kinds of machine learning techniques to exploring design spaces so for example if we had a series of for example if we took the design space of clutter - no clutter and then we just sort of set up a whole bunch of permutations along and set of parameters or scales and then we had you know some kind of interaction over them could we make predictions about knowing what we do about human visual Samians can we make predictions about what would work for people and what wouldn't work for people so that's sort of using sort of different kinds of techniques more as an exploration we've also done some work where we got screenshots of apps you that used you know material design or not and then we just did like a huge analysis over them to see which ones seemed to be the most popular and which ones weren't and what people were looking out and what we could predict people were looking at or not so I think there's lots of spaces to really kind of start to apply machine learning techniques and or analysis techniques from both directions to really push the experience yeah thank you for the talk I'm wondering how the company receives feedback from end users about like their users their experience well each product has a huge amount of research and so there are like several different levels of getting feedback from people one is that of usage so there's you know log analysis and there's use analysis of any product all the way through to usability studies in lab and two more ethnographic studies where people go out and researchers go out and study how people are using different you know applications or products and Google has this great thing called the Google van which literally has a bunch of researchers in and they travel around to all kinds of parts of the country and set up usability and interview labs in the van to find out from people what their experiences of Google as a brand or a set of products or whatever what we also do with material design as I say that material is a community as well as a set of you know design guidelines so forth and so there's always this constant feedback from the material design community of engineers and designers who are trying to design and they're sharing stuff in you know various forums and you know tweeting at us and so forth so there are so many different kinds of signals different ways of engaging with the different users but you know it's like it's number one priority for every single product and every service at the company thank you we're going to end the evening there thanks again Elizabeth thank you thank you thanks so much and thank you for bearing with me thank you and we have a thank you for you "
hS8t56DXEHg,28,"http://www.meclab.org

Playlist: https://www.youtube.com/watch?v=91BoRZllDb4&list=PLAuiGdPEdw0hhJ_XZUJrR9OeJoUgB1AiB",2019-10-29T15:08:13Z,"Human Computer Interaction. Lecture 16. Oct 29, 2019.",https://i.ytimg.com/vi/hS8t56DXEHg/hqdefault.jpg,Josh Bongard,PT1H14M29S,false,19,0,0,0,0,"all right good morning everybody let's get started we are partway through our second-to-last theme of the course looking outward thinking about designing technology that we deploy out into the real world that is sensing the real world in real time not unlike how we are we are trying to instrument design that technology in a way that it disappears into the background and helps us carry out tasks that would be different helps us to carry out tasks we difficult or impossible to carry out otherwise we're gonna finish lecture 19 today when we were looking which we'll finish our three-part series on looking at ubiquitous technology that tries to help us answer questions about human or social behavior we looked at how people tend to interact face-to-face and how their position within that group alters their behavior in those face-to-face interactions we looked last time at this what's now grown into a subfield of HCI which is activity tagging tracking your behavior throughout the day and trying to add some machine learning to predict given those raw measurements weather whatever it is you're doing is positively or negatively affecting your physical or spiritual well-being and we started last time by looking at the somewhat controversial human speech on project how do children acquire language it's a very mysterious topic very difficult to measure and understand up until recently psychologists have tackled this problem by bringing the child and parents into a laboratory setting which is by definition an artificial environment so anything you do learn in a lab setting may or may not tell you something about how children actually acquire language in the wild and by in the wild we mean usually their home right so we're going to continue on looking at Deb Royce speech on project today where he and his wife actually instrumented their own home and tried to record much as possible about the first few years of their child's life and then use that raw data to actually start to answer these questions about how children acquire language well finish that up and then we'll turn to a four part lecture series on robotics why are we talking about robotics and HCI class we've spent a lot of time in this class talking about interaction in the sense of an intelligent being usually it's a human user that is pushing against the world or pushing against a technology literally or metaphorically and observing how the technology pushes back we've spent some time talking about embedded devices and ubiquitous technology which also observes the real world but has limited ability to push back against the world to actually cause change in the world we can think about wearable technologies like a Fitbit in the case of activity tagging where the technology is sort of passively riding along with the human operator and picking up information along the way what's going to distinguish robotics from the other ubiquitous technology we've talked about before is robots have their have motors they are able to act on them on the world independently of their human operator or owner and observe how the world pushes back so we're gonna switch from talking about human-computer interaction to human robot interaction robot robot interaction and all of the more complicated interactions that arise when we start instrument or world not just with sensors but also with active technology that like us can also push against the world and observe how the world pushes back okay that's where we're going you've made it we're at deliverable ten the tenth and final deliverable so let's talk about deliverable ten for a moment you're gonna be filling in the fourth and final panel here in the lower lap some of you may have put material from deliverables eight and nine in different panels doesn't really matter for our purposes which panel you place your visualizations in in this final deliverable you're going to be creating additional visualizations but these are visualizations that are running continuously throughout the user interaction like the skeletal hand up here and these visualizations are going to give real-time feedback during a single user interaction so you're going to be creating and deliverable ten three different real-time visualizations the first one is going to show is going to show how the user's performance during the current session differs from their performance differs drink from their performance from the last session so how am i doing now compared to the previous session that's deliverable one and again you can choose however you wish to do that shoot a video of that visualization second visually visualization how does the users performance overall sessions compare to the performance of other users how am i doing compared to other people that have used the system so far third and final visualization is real-time feedback about a single-digit presentation so as you know but as your user may not know it takes the can and algorithm a few iterations through the infinite loop to detect whether they're signing the digit correctly or not what should your user be doing during that period so you're going to be creating some kind of warmer or colder visualization at this point so the system asks the user to sign the gesture one the user signs the gesture one but for whatever reason the leap motion device is detecting an occlusion and it's not predicting one so the prediction of the knn algorithm is not matching the digit that the user was asked to sign so the visualizations should say colder the users should change what they're doing somehow or they're getting warmer the predictions are correct give the can add a couple more cycles to ensure that you're actually signing the correct digit and that it's not just a hiccup in the KNN or the leap motion device and then you sign the digit correctly sound good pretty straightforward any questions about that okay next week we will talk about what you'll be doing post deliverables which is these interim videos one per week until we get to our final exam period we'll talk about that next week any questions about deliverable 10 all the other nine deliverables No okay so back to the human speech ohm project so just as a reminder this was a question about trying to answer the question how do children learn language and to think carefully about the physical social and cultural contexts that surround that activity and hopefully in this case it's quite a bit of instrumentation in the home but trying to do this in an unobtrusive manner as possible this was work done back 10 years ago we already talked about this a little bit okay so we watched the video last time in the first part of the video this was this was Big Daddy right the parents being able to observe everything about the children part two of the video was sort of Big Brother how do you monetize that process by watching all channels and all social media feeds maybe that's not so important for our purposes today okay so let's talk about the technology first they mentioned this throughout the TED talk they instrumented 14 microphones and 11 overhead omnidirectional camera so the cameras are up there and the ceiling and hopefully the parents and the children forget about the cameras and the nanny there were three caregivers now they probably didn't forget about it all the time so they were probably subconscious or unconscious influences on their behavior around the child knowing that video and audio was being recorded so again this isn't perfect ubiquitous technology it's not perfectly invisible 11 overhead cameras 14 microphones and you could see from the video the kind of video and audio they received they collected data over the first 9 to 24 months of age for the child which is known to be the most sensitive period of language acquisition this is when the child acts like a sponge and soaks up a huge amount of language and for most children within this period starts to regurgitate those words back to the caregivers during this period they were they collected over 4,000 hours of recording over four hundred and forty four of those 488 days so they didn't get every single day you could imagine there were days when the video or the audio went down or something else happened obviously they're not capturing all hours in the home what else is this massive dragnet missing we're trying to capture as much data as possible about the child during this sensitive period of course it's not perfect when the child is outside of course right so it's not not perfect there's lots of missing parts here but but again as Deborah I mentioned this is the biggest family album of all time right the most data that's ever been collected about a human child during this sensitive language acquisition period about 10 hours of recording per day they captured it they think between 70 and 80 percent of the child's waking time and five to six of the cameras were active at any time right you could imagine the child in room one the parent in a room next door there's an open door between the two rooms the parent is saying something and then a few seconds later possibly the child says says something else there's yet another caregiver in a distant room so there's obviously lots going on in the hall complex social dynamics going on that we're gonna try and capture and analyze okay so just as a reminder we mentioned this last time most work in psychology is Theory Laden I have a theory about how children acquire language I bring them into the laboratory and can design and conduct an experiment to test that theory positive or negative right why do we do this because of course we can't instrument every waking hour of the child's life or a human subjects life but with the advent of unobtrusive ubiquitous technology we may be able as scientists and psychologists to free ourselves from the theory trap we might be able to just let humans do what humans normally do and assuming they're okay with it capture as much data as possible and then try and ask questions about human behavior from a data driven approach rather than a theory driven approach capture the data and then dive in the data from different theoretical perspectives so I can come to the data then with a theory and look to see whether the data supports that theory and hopefully we're doing this in a much more natural setting and what we do learn from the data might be closer to the reality of human experience in human development that's that's the idea where the hope for ubiquitous technology in the psychological sciences okay so back to the human speech home project and break this down into three phases first of all there's the the raw capture right we're trying to capture as much audio and video at least indoors for the child as possible given that massive raw data set can we then distill it down into a number of features so you remember back to your K&N algorithm we had four features in the case of the flower 120 features in the case of the human hand but those 120 features about bone position came from a much larger raw data set which is the raw infrared pixel values coming from the device itself so the leap motion device distills down raw data raw infrared data into relevant features for inferring hand position which is bone position gonna do the same thing here but at a much larger scale we're gonna take the raw video and audio and try and distill it down into relevant features such as language from audio people from audio and video activities and objects and if you are paying close attention you'll notice during this first transition from raw data to features theory is already starting to creep in right we just said we want to try and keep this as purely data-driven as possible but we're already how they we're already introducing unmentioned assumptions about language acquisition which is that language acquisition is most dependent on language maybe that one's not so surprising but also people activities and objects so there's certain things that are going to be left out of the raw data that may influence language acquisition that aren't captured by these features what might some of those missed features be most likely a child's first word is going to be influenced by hearing that word on the television or hearing the the caregivers mention that word who said the word what were they doing when they said the word where which room were they in what objects were they holding or what objects were they looking at when they're when they said that word what else might matter here okay so it doesn't it's obviously not very specific here what about people right is the video are the the inference of people in the video is it going to be sufficient to see whether the child could see the face of the caregiver speaking the word when the caregiver spoke the word maybe yes maybe no are there features other than people activities and objects that might influence the child's language acquisition that are that may be in that raw data but are being thrown away here are we missing things or losing things you're all excellent English speakers but you're also learning a lot of new things think about acquiring new knowledge here on campus what aspects influence whether you absorb class material well or not time of day right you know they have that it's not it's not captured here right how much sleep you got now you might be able to capture that from raw data data of the people they were capturing they were capturing 70 to 80 percent of the child's waking hours what about the child's non waking hours do they have that in the raw data do they have it but they're throwing it away theory is already creeping in right we could of course assuming we were able to get access to the raw data dive into the raw data with new theories such as time of day and sleep influence language acquisition and try and distill those features out of the data and try and answer questions about language acquisition and that way this is again all ongoing work right how can we capture as much of the features as possible that might influence a particular human behavior such as language acquisition okay so again once they have those features they're going to focus on these four classes of features can they prove or disprove theories of language acquisition so from the audio they're gonna try and pull out who is saying what and how that's prosody remember it's very important not just what is said but how it's said and we'll look at how they do this in a moment did a lot of speech transcription speaker identification so not just identifying English words in a raw audio stream but who said it prosody features again how is it said and from video they're gonna try and get a people and activities and objects right who was where what were they doing and how with what kinds of objects so going back 10 years ago person tracking and identification was not as as sophisticated as it is now but it was still possible to some degree what were they doing so like we just saw in the previous activity they're gonna try and the previous lecture and try and do some activity classification we we're assuming that the caregivers and the child did not have a Fitbit at this at this point so there's no acceleration data we don't actually know how they were moving or what they were doing we're gonna try and infer it indirectly from the video and object tracking and classification so we're going from the raw data to machine perception we're gonna try and automate the distillation of raw data down to features as best we can but it was again very much a black art back in 20 2009 they're also going to incorporate some human analysis into this loop once they have the features they can start to do some machine learning and make predictions about child learning so for example we could propose the following hypothesis that when a word was spoken by the child for the first time versus how often was the word spoken by the caregiver I hypothesize that frequency of the the word being spoken is going to influence when the child speaks the word for the first time seems kind of intuitive right the more the child hears the word the more the earlier they're likely to utter it themselves it's a theory but I'm gonna try and you is the data or the machine learning and look at these features pull out of pull out of these features frequency with which the word was uttered by caregivers and also the time at which the word was spoken by the child for the first time okay okay so let's look at let's look at audio now so this work was carried out in the MIT Media Lab the Media Lab is well known for doing a lot of HCI a pioneering work so on top of this raw data they had to create some HCI interfaces to be able to perform some very challenging tasks like scan through nine months to 24 months so 18 months worth of raw audio and pull out of that words which obviously exists in that audio for just a few seconds right they're going to look for needles two or three second utterances in a haystack an 18-month raw audio feed how do you do that well one way is to start doing a visualization and designing a particular visualization to help with a particular task remember our pact analysis so the task here is finding needles in the haystack finding words in 18 months worth of raw audio you can visualize audio well using a spectrogram so what is a spectrogram well as you can see here on the horizontal axis we have time on the vertical axis we have frequency from low frequency to high frequency and the amount of ink that's dropped at any one point is the power the amount of that frequency in the signal you can get very good at reading spectrograms and you can see speech versus non speech white noise versus a repetitive pattern like a song you can actually see the things that your particular types of audio by visually scanning images why would we want to take audio and turn it into video when we're looking four words in the audio why this particular visualization decision why the decision to visualize audio at all exactly so again back in 2009 you couldn't just pull words out of a raw audio feed you had to either listen to 18 months worth of it and pull out the words or compress that time into spectrograms to allow someone who's good at reading spectrograms to zoom in on a 30 minute period in which there's lots of word footprints right so you could look at if you're good at looking at a spectrogram you could look at a four or five or six hour stretch and see a lot of sound but focus in very quickly that's where there was a conversation right okay so good reasons for doing that and obviously they need to do this across the parallel microphones that are that are picking up sound throughout the house they put in lots of nice interface features so that you could zoom in to a few seconds and actually identify and then listen to that audio to see if there actually is a word uttered during that period scan back out to a few years months weeks not an easy thing to do right a very challenging task a lot of interesting HCI challenges there once they started to pull out shorter audio tracks someone had to actually listen to those audio tracks to identify yes there's a word in there and yes the automatic speech transcriber got it correct not an easy thing to do so about 25% of the audio recordings when there actually was sound going on in the room of the home was speech that already is kind of interesting they would then select a segment of speech or potential speech to be transcribed listen to the segment and type what was heard this was a very time-consuming not that interesting task they tried to automate it as much as possible so it's a little hard to see here see if I can zoom in on this for you so here's a little snippet of time in which either the parent is singing to the child or the child is possibly watching television and the automatic transcriber thinks that the wheels on the car goes so fast was mentioned during that time period the transcribers listening it ticks off yes that's correct part of the speech was clipped was it taught was a television was it actually one of the speakers it's not clear I don't know so sort of annotating and correcting the automatic speech transcription okay using today's technology this would probably be easier but if you've ever used to be automatic speech transcription it's still not perfect okay okay so again there are a lot of other HCI challenges when they try to help distill all this down to features once they do then we can move on to the third and final phase which is the science part of this and start to ask and try and answer hypotheses about language acquisition here's one particular graph that they pulled out of these features we've got the age of the child on the horizontal axis here and for each month they asked how many word births were there during that month was there a word spoken by the child an adult an adult form of the word spoken by the child for the first time in at a at age nine months the child uttered two or three words for the first time at seventeen months of age the child issued thirty five new words during that month and so on tell me about the language acquisition for this particular child during this period what happened assuming that our feature detectors and machine learning algorithm is relatively accurate how is this child acquiring language during this period the speed is decreasing right so this part of the curve is not surprising to most developmental psychologists this is the canonical sponge phase they are absorbing words and and regurgitating them back at an ever-increasing rate a lot of words in the English language it was a little more surprising is this sudden transition where acquisition drops off the rate of acquisition is still positive the word child is still uh during new words after 20 months of age but at a much slower rate okay so one hypothesis leads to another what's happening here you only need a few words okay you'll need a few hundred words to be able to communicate okay right so why does a child observe and regurgitate so many words so that they can articulate what they want so that's a pretty common hypothesis across the developmental psychological realm drops off because but I aspire because if you have enough words to communicate the spike is a little mysterious right possibly this is an artifact of the data maybe this is not quote-unquote real we have to take all of this again with a grain of salt we know there's a lot of noise in this process right the raw data itself is not capturing everything it's not capturing everything perfectly our distillation of raw data down into features is not perfect so again we need to trait treat this with a grain of salt but definitely we can trust that there is an exponential increase and definitely a decrease during this period you mentioned vocabulary what about this decrease what explains that what could explain this decrease okay moreover also what what is that what might the child be learning above and beyond just new words what is it that they're doing during this period social interactions of course right there's a lot for a child to learn during this period grammar right the power of language of human languages is it's combinatorial power right I want more more is a good word you can imagine that ones in there I want more water a lot more food a lot more attention I want more sleep I want more of being left alone it's the combinations possibly that the child is experimenting with during this period possibly at this point the child has enough words where now the child is possibly subconsciously realizing that you can get more of what you want not by uttering new words but combining them in new ways again that's just a hypothesis we could imagine diving back into this data to see whether the data supports that hypothesis we could for example instead of plot word word births we could plot word pair births when does the child say more water for the first time okay okay let's look at another hypothesis this was I mentioned this one a few slides back does the frequency that a word is heard predict how early the word is spoken so on the horizontal axis again we have a age in months of the child and on the vertical axis here we're going to have the frequency or how often the word was heard how often the word was spoken by a caregiver the mother of the father or the the nanny within earshot of the child it's log frequency for our purposes that won't matter too much so for example a point down here in the very bottom right represents what what is this bottom right most point represent exactly right that word was uttered by the child only for the first time after 24 months and was never heard by the microphones the child probably heard it otherwise how did the child utter it but it was either spoken by someone else in a room outside the the times during which the microphone was on or outside the home maybe it was on the television but it was out of earshot of one of the microphones who knows versus a point up here in the top left so here is a here was one of the first words that the child ever spoke and that was uttered very frequently by the parents if we then linearly regress this red line against the data so we try and place this red line in the plane so that it is as close to all the dots as possible you can see this is kind of almost an impossible task there's quite a bit of spread in the data however it is a pretty good fit and we won't go into the p-values but it's a good enough fit to give a negative slope which means obviously that the lower down or the less frequently the was heard further to the right then later it was for the child to utter that word for the first time right kind of makes sense again it's not a good fit because of course language acquisition is an incredibly complicated phenomenon and it's doubtful that any one feature like how often the word was spoken by the caregivers is gonna perfectly predict when a child uh ters the word for the first time but it looks like it's at least part of the story that makes sense what else besides frequency might predict how early the word is spoken no there's a few parents here the simplicity of the words right what words did the parents speak what else we've looked at a few language applications now what are some of the other aspects of speech that might predict how early the word is spoken by a child think about again the physical and social context going on in this house between nine and twenty four months of age for the child now verbs yeah possibly yeah we could split the words in a noun nouns and verbs would be one or the other is easier for the child the child utters it earlier what else so those are features of the language what about what happens in the seconds or minutes after a child utters the word for the first time what kind of rewards make the child expect to receive food praise attention the caregiver looks at the child immediately after the child utters the word for the first time the parents eyes widened in delight that they have the child is mentioned a new word for the first time lots of those subtle social cues are obviously not obviously but potentially underlying language acquisition let's add just one additional feature which is prosody how the word he's spoken so we're gonna look now at F and P F is gonna represent the frequency with which parents uttered the word and P is gonna stand for a Prasad ik feature so some feature of how the words are spoken in this particular plot they're gonna focus on what's sometimes known as mommy ease so when you speak to a child some people will instinctively elongate the vowels so we're going to measure this particular prasat ik feature of mummy ease where a P of 0 represents you speak the word in the adult form and the longer the caregiver along gates the vowel the higher the value of P so far so good you can imagine that this is not something that's naturally captured by an automatic speech transcriber so that these p these different kinds of P values were extremely painful to collect from the data luckily they do indeed have an influence on how early or how late the child utters a word for the first time here's how we're gonna look at this we are hypothesizing that both F and P influence language acquisition but we don't know in what proportions so we're gonna combine F and P together and we're going to introduce a third parameter here alpha you can think of alpha as a slider bar then we're gonna slide back and forth between a value of alpha equals zero and alpha equals one alpha equals one is we're going to combine F all of F and none of P so this is actually alpha times F plus one minus alpha times P so alpha is going to dictate the proportion that we're going to combine F and P an alpha point five means mix F and P together in equal amounts and an alpha of one is it's all P so in any setting of alpha we have some combination of F and P and we could then ask how well if at all that combination correlates with language birth right the time at which light the words are born question so we can see that in this case so for the value of alpha equals zero we're looking whether there's a correlation between F which is shown here on the vertical axis here's F on the vertical axis and we can ask whether it correlates with the time at which the word was spoken for the first time so a negative correlation value means negative slope right higher frequency means later acquisition lower frequency means earlier acquisition right so we've got F on the vertical axis time on the horizontal axis the higher the F the earlier the t the lower the F the later the T that's an anti correlation they're related there definitely is a relationship but it's an anti correlation right a positive correlation of an r of plus point to nine would mean the higher the F the more frequent the later the child spoke the word for the first time right so we're looking really to see whether there is any correlation large negative or large pause if a correlation near zero means there isn't much of a relationship between F or some combination of F and P and the time at which the word was spoken for the first time makes sense okay so here we go so remember so here's frequency on the vertical axis so frequency is always going to have a positive value frequency of zero means that word was never spoken by any of the three caregivers throughout this nine to 24 month period of time a maximum frequency at least in this picture is nine so in this case F is going to range between zero and nine okay mommy ease here P is also going to be a positive value and it is also going to range from zero up to some positive number doesn't specify here what it is a P equals to zero for a given word means that word was spoken by the caregiver in an adult Pam very high value of P would be how long they along mated the vowels if we imagine a hypothetical transcript a theoretical transcriber that could actually take a raw audio and give you back the number of times that a given vowel in the word was duplicated that could be P P is the number of times that the vowel is repeated for example right the more the longer the vowel is elongated in the audio the more O's or azor eyes are captured by the transcriber the higher the value of P I doubt that's how they actually did it but that's the basic idea so P is basically how much the word the vowels are being exaggerated so the frequency zero shouldn't that be a special case he is also zero since it was never spoken there can't be any elaboration yeah good point so for a given word that the child utter if we go back and look for the parent uttering it and we find that F equals zero is never heard by the parent then P would also be zero yes that's right so far so good okay so if this plot on the Left we're comparing F against time and asking whether there is a any correlation in this case there's a negative one but of course on the vertical axis we could put instead of F we could put F plus P for any given word how frequently wasn't uttered the F Plus how often how much were the vowels elongated whenever that word was spoken by the caregiver we could put F plus P on the vertical axis and ask whether that correlates with time that example correlates corresponds to alpha 0.5 so an alpha of 0.5 mixes P F and P together an equal amounts you can just think about it as a weighted sum and then asks for that particular admixture does it correlate with time and if we go to this plot now on the right we can actually find that admixture so for an alpha of 0.5 we're asking is there a correlation between F and P combined and equal amounts to time and it turns out yes there is there is a correlation of negative whatever that is negative 0.26 correlation is a little bit less a little bit closer to zero than the correlation for just F so far so good so the way to read this plot on the right is on the horizontal axis now we're not plotting time we're plotting different mixtures of F and P I could make a hypothesis that says I think word birth is most sensitive to 80% frequency and 20% prosody frequencies more important than prosody it's a very specific hypothesis I could ask that and then look on the vertical axis of 0.8 F plus 0.2 P and ask if I fit a red line to that scatter of points is it a better fit if you do that for it for a number of different values of alpha every point on this plot corresponds to a different weighting of frequency and prosity and asking how well that weighting predicts the word birth and we get this check mark pattern what does that tell us exactly exactly right so at this value we can see there's a minimum in the correlation the correlation is most negative at an alpha of point two and we are looking for points that are as far from zero as possible the more negative or the more positive our value of R is our correlation the more that feature predicts the other thing which in our case is word birth so the correlation is strongest at an alpha point two and as you mentioned that corresponds to 80 percent frequency and 20 percent prosody that correlation is more than than frequency alone or prosody alone so for this particular child whatever is going on neither F nor P explained language acquisition but a particular combination of F and P do a better job of predicting so what do we learn about this family from this example we know that the caregivers were elongated the vowels to some degree and obviously admitting and saying some words more frequently or less frequently than others and that was in love those things were influencing language acquisition for this child again that hypothesis or that finding suggests 10 more hypotheses such as we know that word acquisition for this child is influenced by frequency and this particular kind of ferocity what is that what new directions does that take you in and thinking about language acquisition absolutely right so thinking about social and cultural context obviously what goes on in the home of any given child is very different from one child to the next how consistent is this signal across children assuming we were to repeat this experiment which I don't know if we want to do how similar is this is it all human children tend to acquire language as some function of how often they hear it and how the caregivers pronounce it maybe yes maybe no at the moment no one knows if F predicts word birth and P predict word birth and both together predict word birth better what else does that suggest to you there's other factors right here's two of them there's probably dozens or hundreds or maybe thousands who know us right okay this is not a class in linguistics so we're going to leave that here and we're gonna finish with this last plot which already we saw in the TED talk this is just a good plot to remember this is probably my estimation the best example of scaffolding you can actually see it in the data so on the horizontal axis here we don't have the age of the child we have the months from word birth we'll come back to that in a moment on the vertical axis here we have change in mlu the mean length of the utterance so for every word that the child uttered for the first time between nine and 24 months of age we have that point we can then ask when the child when the child uttered that word for the first time and then move backwards in time in the data set and find all the times before the child uttered the word that a caregiver uttered the word and for every time find that word how long was the sentence that that word was embedded in the apparent says do you want water length of utterance for want water enough length letters of - 4 + 2 / a2 is 3 mean length of utterance for water given those two captures is 3 right okay so if we then line up all the word births so obviously the child uttered words for the first time at different times we take all those words and we line them up at 0 on the horizontal axis we get this very interesting pattern which is around the time that the child uttered the adult form of the word for the first time the parents were emitting 0 on average zero length utterances around 1 so short very short sentences water or even half a sentence wat wat water water very very short utterances 15 months before the child uttered the word for the first time the parents were emitting sentences to the child that had a length of on average 2 or 3 want water you want water and so on after the word is spoken there is a slight increase in the length of the utterances again from the caregivers so definitely caregivers are changing their speech patterns in anticipation of the utter an utterance of a word for the first time why what's going on here so it's definitely scaffolding but scaffolding how without needing for the elongation yeah okay possibly but why didn't the parents just why isn't this line flat then why don't the parents just have very short sentences the whole time why is the line curved I sorry the number of words in a sentence so a length of utterance equal to three is you want water length of utterance four do you want water that's the length of an utterance but we're capturing those sentences many many times before the child says water for the first time so we're taking all those sentences and taking the mean length of them correct so let's look at just this red the farthest left red dot the farthest left red dot represents the fact that for all words that the child spoke for the first time fifteen months before the first utterance of those words the parents were uh Turing sentences with two or three words in them not just water but all those words that the child utters for the first time we get down here they're very very short one to zero I'm sorry I misspoke it's not ml you it's change in ml you I'm sorry okay let me start this again so the dotted line represents zero change in ml you if we take all the sentences that the parents ever uttered with the word water in it let's assume that that average over all those utterances was three for example okay up here the change in ml U is two point five which means fifteen months before the child ever mentions the word water sentences that contain the word water fifteen months earlier our five point five right there two point five more than the average water sentences reaches three make more sense yes right so remember that around here around zero it doesn't necessarily mean that the sentence is uttered by the caregiver is zero or sure it's the mean whatever the mean was it could actually be seven or eight doesn't this plot doesn't tell you anything about that but it's definitely shorter than those sentences were long before the child uttered the word maybe so word birth right is the birth of the word from the mouth not the brain of the child when very difficult to say when the child knows the word right you may be right much more difficult to get at that phenomenon but why the curve what so why are the parents omitting longer sentences if children learn better from shorter sentences because the child uttered the word closer to enclosure in time two shorter sentences why doesn't why don't parents do something similar to mum E's here why don't they just use very short sentences they're not they're doing something else the grammar alright so obviously we're trying not just just to teach the child individual words but how to use the word in combination with other words presumably right water is a powerful word to know but want water I want water you want water that's more powerful right I can do more things if I combine words together maybe again we hypothesize maybe that's what the parents are doing afterward birth which is helping the child understand how to put words together in combination but what about before word birth this is harder to understand why is that why are the parents using longer utterances long before the child's even uttered the word for the first time they hone in on the word on the child acquiring the word water right so they're the parents are starting to utter are shorter and shorter sentences that contain the word water and it gets to its shortest point or the acts are the average length of the utterances right when the child utters the adult form of the word for the first time but how did the parents know to hone in on that word at that that rate so during the talk professor Roy says the parents swoop in or some meet the child at the word earth nobody knows right but it seems that this is what's going on there's definitely some scaffolding right the parent sees that the child cannot ride a bicycle puts training wheels on the parent is watching the child ride the bicycle with training wheels and is seeing something in the way the child is riding the bicycle in the way that the child is possibly emitting child forms of the word water which for this child was Gaga there's something about what the learner the learner is starting to signal competence and as the parents start to see or observe or infer growing competence they reach in and they gradually sorry they gradually are adding scaffolding and then the child utters the word and starts to use it and they're gradually removing the scaffolding again so this is a pretty interesting finding it seems intuitive that parents teach their children language but what's more interesting is what seems to be going on is the child is teaching the parents how to teach it language the learning is bi-directional the teaching is bi-directional here right somehow what the cues are that are going back between the caregivers and the child who knows but this is one of the benefits of data driven approaches right it's unlikely that someone would suggest out of the blue the hypothesis may be children are teaching parents how to teach it language that seems to sort of jump out of the data okay again not a linguistics class but one of the a demonstration of what ubiquitous computing and technology can help us with ok any questions about lecture 17 18 and 19 before we move on to our last class of technology ok all right so at the beginning of the course we made this sort of cartoon of humans interacting with computers human-computer interaction and the idea was to think not really about where the interaction starts or ends but to think about a continuous interaction where the output of one participant like for example the movement of a mouse the output of the human becomes the input to the computer the computer senses change in position of the mouse and then vice versa the computer outputs some visualization to the screen which falls onto the retina of the human which alters the behavior of the human and around and around we go we are now going to start to look at a particular kind of technology robots which are active they are able to actively act on the environment and the robots can sense the repercussion of their actions so we're going to get into a much more complicated loop here we're going to speak in lecture 20 here about this particular feedback loop and understand how this works in robots and then we're going to look at human robot interaction where humans become part of the environment of the robot the robots are hopefully not physically pushing against the humans they are metaphorically interrogating the humans and observing how the humans push back so we'll look at social robots next time okay so how does a robot go about interacting with its environment there is this continuous feedback loop which is difficult to think about we talked about this at the beginning of the course it's much more intuitive to think about a passive being like a human or a robot that sits there and waits for some input from the world cogitate some processes on it and that does something it's not that simple there's a continuous feedback loop one is always pushing both are always pushing and the other participant in this feedback loop is always receiving that that push because that's non-intuitive in the history of robotics we've tended to look at animals as being overly complicated than they actually are we talked about this at the beginning of the course so we looked at BF Skinner's box which was a way to put an animal in a box and try and simplify the behavior of the animal to really understand this feedback loop one of the repercussions in the field of robotics is because we tend to overestimate the complexity of what's going on in the head of the animal or the human we've tended to try and create overly complex robots that fail so one of the reasons that robotics has not moved at the same pace as computer technology is because of our inbuilt bias to understanding behavior most robot assists would like to take human or animal behavior and build it into a machine but if we overthink that behavior you think that behavior is overly complicated we build an overly complicated behavior into our robot and it fails miserably so the HCI approach to robotics is to think not so much about what's going on inside the head of the animal or the human but to focus instead on the interaction if we can get the interaction right we can get away with simpler robots that still do what we want them to do so again just a reminder of this idea of mental model if you observe an ant moving along a beach it will carve out a very very complex path and you might be led to the assumption that that that ant is doing some very complex navigation instead of what it's probably doing which is interacting intelligently with the beach which is if there's a stone to my left go to my right if there's a stone to my right go to my left if there's nothing in front of me go straight right we're talking about stones and the ants immediate environments we think about the way in which the ant could interact with its environment we come up with a much simpler explanation of what the ant might be doing and that simpler explanation turn right if you're blocked on the left and vice versa much easier to place in a machine okay so we just said that this idea was first articulated by Valentino Brayton Berg Valentino Brandenburg wrote this great book called vehicles in the 1980s Brayton Berg was not a roboticist he was a neurophysiologist he studied the architecture of brains in fruit flies you've all lived in student dorms and student housing you've probably seen lots of these things very very small and they will find any rotten fruit or any any food sugar products and he where in the room if I had a fruit fly here there was a rotten apple in the back corner let it go it would fly around and gradually find the Apple in the 60s and 70s when brayton Berg was most active and research in the literature there were theories about how fruit flies actually find rotten fruit and people had ideas about how the fruit fly was performing differential calculus in its brain to follow gradients of smell and find the fruit meanwhile Brayton Berg was dissecting the brains of fruit flies you know how small the food fly is imagine how small the brain of a fruit fly was Brayton Berg grew increasingly doubtful that there was some differential calculus built into these very small brains like the example of the ant if you're a flying machine and you're moving about an air and three-dimensional space if there is more smell on your left turn veer to the left if there's more smell on your right turn right if there's more smell in front of you go forward if there's more smell behind you turn around that's that's enough to follow a gradient to find the fruit so brain Berg realized by looking at the literature that people kept overthinking animal behavior so he wrote this book called vehicles and he used the word vehicle on purpose although for us a vehicle suggests an artificial machine vehicle in this sense is meant to represent an organism a human robot it doesn't matter it's something that can act on its environment and sense the repercussions of that action in the book it's written like a series of fairy tales once upon a time there was a vehicle known as a vehicle one vehicle one had a very simple body we're looking above a vehicle one it has a single wheel at back and a single temperature sensor in this case the temperature sensor is connected with a single wire to the wheel which produces a pretty simple machine if the temperature sensor sensors senses higher temperature the wheel spins at a faster velocity lower temperature the wheel spins at a lower velocity so imagine now what happens if you put this vehicle into a pond and you saw it swimming around in the pond you might say it's restless because it does not like warm water it is quite stupid since it's not able to turn back to the nice cold spot that it overshot in it's restlessness anyways you probably if you didn't know what was going on inside the vehicle you couldn't see whether it was made out of metal or cells you might say well clearly it's alive since you've never seen a particle of dead matter move quite like that this was a highly respected neurophysiologists saying we would think something this simple is alive strange why I use this kind of deliberately controversial language we'll come back to that the book continues on with a series of increasingly sophisticated vehicles there's I think twelve and all we won't look at all of them just look at the first few let's look at series - so there's 2a and 2b and these two robots are capable of fear and aggression we now have a robot with two wheels back left and back right and two light sensors front left front right and we have its lateral connections if some eating the same and lateral on the side so we have same sided connections left sensor two left wheel right sensor connected to right wheel imagine now that you were to come at this robot from its front right with a flashlight what would the machine do turn to the would it turn towards the light turn away from the light so it's gonna turn away from the light why let's talk about sensor motor here what's going on the right side yeah exactly so at this instant in time there's more light falling on the right hand light sensor meaning the right wheel is turning faster than the left wheel and it turns a little bit away from the light so let's sort of run our mental model of this machine it turns a little bit away to the lights let's imagine it just turns an inch or so at the next time instant assuming we do not move the flashlight what happens now it's turned a little bit away from the light the robot has acted based on what it's sensed which means the relationship between the robots sensors and the flashlight have changed this interaction has changed just because the robot moved right this is what distinguishes robots from passive embedded devices how does that change the robots sensors it's moved and there's been a sensory repercussion what is that repercussion it's getting less light right it just turned away from the light a little bit so now given the change in the sensory circumstance change in the sensory circumstance of the robot how does that change its action it's gonna turn away from the light a little bit more slowly because there's less light which means it's gonna be even further away from the light at the next time instant it's gonna turn turn turn and slow down assuming that these light sensors are aamna directional it can still see the light behind it I start chasing it with the flashlight from behind what is the robot going to do as I start chasing it so now the environment is fighting back how does the robot reality he will run away right because he's afraid he's the coward if you read vehicle and you read this chapter in vehicles Bradon berg tells us that of course this robot is the coward it's afraid of the light is this robot afraid of the light why would a very serious and renowned scientists use such controversial language where's the fear circuit in this machine it's not there all right let's leave that there for a month to be same thing two light sensors two motors but now we have contra lateral connections we're going to literally cross the wires contra processor against left sensor to right motor right sensor to left motor and let's assume the flashlight is here pointing towards the robots front left where does the robot can do in this case move towards the light there's more light on the left sensor which means the right wheel turns faster which means the robot turns towards the light what happens at the next time instant let's run our mental model of vehicle to be both sensors can see the light so maybe it evens out its turn and starts to head towards the light so the the curvature of turning is increasing it's moving increasingly straight towards the light what else is happening about the behavior of the robot it's not just slowing its turn it's it's speeding up right it's now closer to the light which means the wheels turn faster if instead of a flashlight you put a naked bulb on the ground it will accelerate and smash the bulb because the aggressor hates light with the passion and will do whatever it can to destroy any light source that is naked and unexposed and at its level does the aggressor hate of course not right that's ridiculous there's two contralateral synapses if we're talking if we're thinking of this vehicle as an organism or wires or circuit lines if we're thinking about a robot why would professor Brayton Berg talk about aliveness and cowardice and aggression he wants to maybe not necessarily explain these complex emotions using simple mechanisms what's the point he's trying to make aside from just vehicles simplifying it in general but how he's making now a philosophical point what is the philosophical point here we put we overthink things about animal behavior imagine that vehicle 1 & 2 a and 2 B were not we're not animals are not machines but organisms right if you saw an organism doing something like this you may or may not tribute an emotional state to it the moment you open the hood of the vehicle and you see what's inside well of course it doesn't have aggression or fear makes sense right it's just a machine it can't possibly have these emotions remember our discussion about affective computing there's one final point or one final step professor Brayton burg wants you to take right oh of course this is ridiculous machines don't have this and maybe we overthink this in organisms lower at organisms at least we do though right humans do of course we love we hate no question no question that these don't and no question that we do what is the final philosophical point that Braden Burke is trying to make here right we talked about this a few weeks ago anthropomorphize ation right we project human values or in this case maybe human emotions onto things like at least this vehicle that don't have them what's the final point beyond anthropomorphize ation that Brayton burg is trying to push you towards what's going on we anthropomorphize our selves right we project back onto ourselves it feels like I have aggression and fear in the very same way that it feels like I decided to move my finger when I saw that moving red dot in the lipid experiment the final point that Brayton burg was trying to make in vehicles is we should check our assumptions about our own emotional state right do we actually have them what do we meet what do we really mean by fear and aggression we overthink the behavior of animals including potentially ourselves okay as usual we'll end on a philosophical point today you have a quiz due tonight and I will see you on Thursday "
7_vNEO65Mn8,28,"http://www.meclab.org

Playlist: https://www.youtube.com/watch?v=91BoRZllDb4&list=PLAuiGdPEdw0hhJ_XZUJrR9OeJoUgB1AiB",2019-10-22T15:08:16Z,"Human Computer Interaction. Lecture 14. Oct 22, 2019.",https://i.ytimg.com/vi/7_vNEO65Mn8/hqdefault.jpg,Josh Bongard,PT1H15M19S,false,64,1,0,0,0,okay good morning everybody let's get started I hope you enjoyed Dan Hardy's a Q&A session last week who wants to go in on a startup all right great so let's make sure we understand a little bit about HCI before we start our start out back to lecture we're gonna talk about deliverable 9 in a moment we are approaching the end of the deliverables we are working our way through this theme on looking outward so thinking about not just creating pretty websites but thinking about designing technology which is interacting in real time in the real world alongside their human users and we're looking at some of the unique challenges and opportunities that's such a big ubiquitous computing affords just a reminder today noon to 4:00 p.m. I'm sure you've heard this many times now already but if you haven't this afternoon on the fourth floor of Davis Center if you're looking for a job that's the place to be ok so we're going to finish the last couple of slides in 115 which is sort of broadening our discussion about interactive technology where we're looking now not at gooeys or graphical user interfaces but thinking about how to bring into account the other sense organs like audition and touch and we'll finish our discussion of touch today we'll talk a little bit about what ubiquitous computing actually is we will finish this lecture today and start in on the first of three lectures we're going to have that look at deploying a particular kind of ubiquitous technology to ask and try to answer a scientific question that would be difficult or impossible to do without technology which is threaded into the physical environment ok so that's where we were and where we're going I'm going to start lecture and then we're gonna come back to deliverable 9 about partway through the lecture because I want to introduce the concept of scaffolding which is what you're going to be tackling in deliverable 9 ok so just as a reminder we ended last time by talking about tactile user interfaces can we create technologies that communicate back through us through touch and what would be the advantages of communicating through touch rather than projecting a visual feedback what were some of the advantages we talked about last week we talked about tu eyes why bother creating a moving pin display that we can actually feel feedback physical feedback from the device right so this idea of physical interaction if we're if we're interacting with someone or something else a lot of that communication is through touch and through skin right we can imagine through tactile user interfaces that multiple people and multiple computers or technologies can collectively manipulate objects and create or do something together and there is an important aspect of tactile feedback in that process I skipped over last time ultra haptics which i think is a particularly interesting example of a tangible user interface this one is going to as you'll see in a moment provide tactile feedback but also visual feedback and combine them in an interesting way so let's have a look at ultra haptics and then we will talk about the technology itself this one pay particular attention to the feedback loop remember we talked at the beginning of the course about the output or the interaction of the user becomes the inputs of the device and vice-versa that feedback loop exists here but it's a little bit difficult and a little bit subtle to see what is involved in that feed okay let's have a look directive surfaces are now common in interactive surfaces are now common in everyday life they allow users to walk up and use them with no instruction however current methods for providing tactile feedback require the user to cover up the visual content by touching the display or attached devices to their hands we present ultra haptics a system that provides mid-air haptic feedback and requires no contact with either tools attachments or the surface itself ultra haptics uses a phased array of ultrasound transducers to create tactile focal points in midair the array is driven by a stack of five driver boards which received emission patterns from a PC the user's hands are tracked by a leap motion controller and the haptic feedback is projected through an acoustically transparent display directly onto the user's bare hands there are four steps to our unique focusing magnet first we define a large volume around the transducer array within which we will model the ultrasound fee order then we position positive control points where we want to form focal points these tell the system to generate the highest intensity ultrasound possible at these locations they are then surrounded with non control points these have the opposite effect telling the system to generate the lowest intensity ultrasound at these locations finally the phase delay and amplitude are calculated for each transducer in the array to create an acoustic field that matches the control point this simulation illustrates the acoustic field as we move up from the transducer a red color represents a phase and brightness represents intensity at a height of 20 centimeters a focal point is formed above the ultrasounds D focuses once more similarly this simulation shows five discrete focal points being formed at the same time by varying the tactile properties of focal points such as the frequency they can be made to feel different from each other in this scenario a tactile information layer is created above the display by moving their hand over the map a user can feel the population density of a city the frequency of a focal point represents the density in that area here focal points are created above elements of a music player interface this allows a user to locate themselves on the interface without looking tapping the focal point above the button starts and stops the music the focal point above the volume slider can be grabbed at this point it pulses to inform the user that the system has recognized their grasp the focal point can then be slid up and down to change the volume these are just a few of the applications that become possible with the ultra haptic system okay so let's write let's talk a little bit about the feedback loop in ultra haptics what what is involved in this feedback loop a motion sensor so a sensor that's detecting motion it's tracking the hand location so we've got a familiar friend here as part of the feedback display the leap motion device as you know is capturing information about where the hand is and hand orientation that's the input to the ultra haptic system what is the input to the user so yeah and I apologize for the sound you can go back and watch the video at your leisure so these phased arrays that are underneath the hand or in some cases in this case projecting from above downward or instead of phased ultrasonic emitters so they're emitting ultrasound which is not unlike the infrared in the leap motion device is a physical phenomenon that sends out a wave and in some cases detects the the the feedback from the wave but in this case the wave that's being emitted is felt by the users hand how absolutely they mentioned these positive and negative focal points so these phased arrays are all emitting a series of ultrasonic waves and the device itself can control the phase offset of the waves and the frequency of the waves so you've all you've all probably seen a body of water you have two waves that are approaching one another and if they hit in just the right phase if they come together at the same time they produce positive interference you get a wave that is larger than either individual wave that's in this case the positive focal point imagine these waves that are coming out at the bottom of the phased array or if the phaser aims is placed above the hand pointing down these waves are coming down and these waves are hitting one at an exact position in three-dimensional space and if your fingers happen to be very near that posit point of positive interference you feel a vibration which feels something like something is there that is resisting your grasp and you can see in this example of the music player interface you could simulate something like a knob or a button or a or a key press sure their sound waves absolutely at a certain frequency yep no it is beyond human hearing which is kind of interesting you could imagine also adding in an auditory feedback you can feel these waves rather than hear them okay so that's a positive interference point you can create a negative control point where you set the phase offset so that they are out of phase and again you've seen in a body of water two waves that approach one another and they're off by a little bit and they sort of cancel each other out and you get mean height right so in this case they create if they're trying to simulate a knob that can be grasped they project or they set the frequency and phase offsets of these ultrasonic emitters to produce vibration at this particular point and everywhere around that point they're trying to cancel out these waves minimize their frequency so it doesn't feel like there's anything there right that's creating a discrete point in space and then they gave this sort of example of a map where you have some continuous increase in interference which feels like something that's increasingly rough obviously there's nothing there but it gives the perception that as you move your hand over the map you're feeling increasing population density or some distribution of data distributed over a two-dimensional plane that's the idea kind of interesting and neat but why would one want to do that what's the advantage of being able to simulate a dimensional object that someone can reach in and manipulate even when it's not there they showed you a couple applications where else might this be useful absolutely right so there's a visual component to this particular application but you don't necessarily need a visual component other ideas they mentioned in passing in the video that this might be useful for applications where the user doesn't actually need to be looking where they're grasping absolutely it could be useful in a case like that there are a lot of domains where they pay very careful attention to heads-up display like working on the space station or other offer earth applications in military applications there's a lot of potential information and you need to think very carefully about providing enough information for the human user to make intelligent decisions in real time but obviously not overwhelmed overwhelm them one strategy is not to simply produce more and more visual feedback but to take feedback and shunt it to different sense organs so there's visual display and tactile display if I'm focusing on something visual I might be able to reach outside of my visual field and feel something and manipulate it without having to take my attention off whatever it is that I'm looking at yeah absolutely that's a great example right a social context where you prefer people aren't actually touching an object I'd be kind of interesting to be where you're reaching in and manipulating arbitrary objects imagine that you have control over the frequencies and phase offsets of this array how complex and objects could you project that would project the illusion of that object actually being there imagine trying to do that and ask your user to reach into this field and describe the object that's inside the field of the D array could be kind of interesting imagine a lot of potential applications okay so I introduced last time but we didn't have a time a chance to look at it the second application this is the actuated workbench if we can create a feedback loop where the device is also providing physical feedback to the user you can imagine users and the technology itself collaboratively manipulating in this case a real object the actuated workbench consists of 8 by 8 array of electromagnets it uses magnetic attraction and repulsion to move magnetic objects to dimensionally on a flat surface here we can see a magnet moving in stepwise manehattan motion here we see an object moving in a smooth circular pattern although the array of electromagnets is fixed the system can create smooth motion by varying the strength of the electromagnetic fields in addition to magnets the electromagnetic array can move any small ferromagnetic object such as a paper clip here the user controls the pucks motion with a trackball smaller objects can be moved much faster though their motion is not always so smooth magnets of different sizes and shapes behave differently in the system's magnetic fields this stack of small magnet saucer on musically we use computer vision as a preliminary object tracking technology here the user records a path by moving the puck on the surface and the system then replays that path through magnetic actuation the blue projection around the puck is a graphical visualization of the strengths of each adjacent magnetic field here's an example application intended to teach users about physics the read projected area on the surface represents the zone of attractive force well the blue area represents repulsive force the user can feel these forces by lightly holding the puck in different areas of the board when the user releases the puck it flies to the red zone of the board to which it is attracted magnetic drawing toys are effective for visualizing the actuated workbenches magnetic fields a Magna doodle allows us to see the fields used to trace the smooth circular path shown at the beginning of this video the dapper dan toy lets us see the magnetic activity in the movements of iron filings on the surface the actuated workbench can be used to control the planchette in a Ouija board game [Laughter] like other robust systems such as the diamond touch presented by Mitsubishi Electric research labs in Wis 2001 the actuated workbench works even when set on fire the last very important demo there of course that's a nod to robotics demos whenever you create a video of your robot you should always set it on fire at the end to demonstrate it's very robust okay actuated workbench what's the feedback loop here so we just saw ultra haptics where they're using ultrasound to create this tactile feedback loop what's the physical phenomenon being exploited here electromagnetic waves are fields right okay so what does this feedback loop afford we saw a few examples of feedback between human users and the device what are some of those collaborative actions that the two can perform using this technology might this be useful sure right so an educational or training application you could imagine a lot of educational applications here how might these applications if you were to create an educational application or a training application with this device what advantages would it have over a more traditional way of teaching electromagnetic fields of interactions of large bodies with electromagnetic fields moving objects in real time right you can see the object itself and if you hold on to the object you can physically feel the effects of an electromagnetic field which we can't see on objects one of the biggest challenges in education is taking an abstract concept and trying to make it concrete in some way right so you can imagine the actuated workbench here as providing unique opportunities to concretize particular abstract concepts like electromagnetism okay forget electromagnets for a moment what else might you be able to teach better using this technology than others you are all in the process of building an educational system and hopefully the leap motion device will provide your users with you and your users with opportunities to learn ASL better than if you just showed them a whole bunch of YouTube videos and they practiced in response to the video there's something about the interaction with the leap motion device that facilitates learning what here might be able to facilitate learning over doing it into more traditional sense okay so you could program Chris lankton zant which is a simple agent and learn a little bit about AI and path planning what other domains might it be useful to teach using this device chess there's an interesting idea yep how many of you have learned to try and write in another language learned kanji or some other language that requires fine motor skills imagine someone an expert in kanji signing various various symbols using this device and then the user holds the device and their hand is pulled through the process of drawing kanji characters right could imagine for teaching languages or written expression perhaps that would be useful one of the biggest challenges in learning a sport or chess or anything that's tactile writ writing a language obviously it has a lot to do with the hands you can watch the hands of the teacher from a distance and try and replicate that with your own hands very difficult to do because you're going from the tactile sense to the visual sense and translating back into your own tactile sense would be nice to cut out the middleman vision in this case and allow in a sense the teachers hands to become your own hands you could imagine whether that would work or not but that's the way of trying to think about this right we want to try and come up with unique learning opportunities that would be difficult or impossible to do without technology itself okay so that concludes our discussion of tactile interfaces we're gonna switch now and talk about this general fields of ubiquitous computing which is sort of a subfield of HCI we're gonna stitch technology directly into the physical world and what are some of the unique challenges there I started the beginning of the semester with this little cartoon here way back in the Stone Age we had all these humans and a small minority of them had desktop computers and within a few years there were more many more cell phones out there than laptops and desktops cell phones and computers have lots of differences but for our purposes our phones are interacting with us but also interacting more directly with the real world than our computers our modern smartphones have a large number of sensors in them which unless you turn them off they are recording various information in real time we're going to start in the applications in the next few lectures to look at embedded devices so these are devices that also have a lot of sensors and are also direct interacting directly with some field of view around them a simple example would be the motion sensors over in the Davis centre the sensors themselves don't move but they are sensing in real time something about the real world independent of whether there are people around or not and towards the end of this section on looking outward we're gonna look at robots which from an HCI perspective are nothing more than self moving embedded devices robots over embedded devices are able to push against the world and observe how the world pushes back ok so this is the world that we're moving towards and there are some important HCI challenges and opportunities in doing this right that's what we're talking about in ubiquitous computing which you can think of as putting computation out there into the world and if we do it well the technology disappears into the world how many CPUs are there in this room now a lot that's a good guess who knows right hopefully I'm luckily we're all not aware of each other cell phones we know they're out there there's a bunch of CPUs out here there's one in here there's one in there it's out there and a lot of those CPUs are doing active work for us at the moment and are helping support this activity of learning about ubiquitous computing but hopefully most of the time the technology is not you're not aware of the technology and instead you're resting on that technology and using it to learn about ubiquitous computing how do we create ubiquitous technology that is invisible this is an idea that goes way back to the early 1980s Ken's a camera at the University of Tokyo was one of the founding fathers of ubiquitous computing and he wanted to put computers everywhere even back in the 1980s when personal computers were just getting started professor Sakamura had the the insight that even if every single person on the planet bought a desktop and eventually a laptop probably not going to have 10 or 20 computers but we may have many many more devices which are computerized this was a pretty insightful observation way back in the 1980s so the number of embedded devices a human uses is limited only by the number of uses you can think of them so inside your laptop there are a number of CPUs in your day-to-day life among the objects that you own where else are their CPUs everything your car your phone kitchen appliances absolutely Wi-Fi router okay if you have a laptop you're running you're running Mac OS or you're running Linux or you're running Windows what are all those other CPUs running a lot of them are probably running the real-time operating system nucleus memorably known as Tron how many of you have heard of the Tron operating system a couple of people it is the most popular operating system on the planet at least as of last count thanks again to Professor Sycamore on ok we're not going to talk about Tron itself the point here is that among advanced computer science students were not even aware of the fact of all of these devices and the operating systems that they're running hopefully those devices are in the background and are helping you drive to drive to campus in the morning helping you prepare meals and so on there in the background and they're because they're working well they're invisible mark Weiser at the Xerox Palo Alto Research Center known as Park this was the Google of its day back in the 80s and 90s coined the actual term ubiquitous computing ubiquity means sort of everywhere and Mark was the one who wrote our computers and he meant here ubiquitous technology should be like our childhood it's an invisible foundation that we rest upon that's quickly forgotten after you learn about it but it's always with us and effortlessly used throughout our lives and we can call it into the foreground when needed he was inspired by the name from one of Phillip K dicks novels called Ubik if you're a science fiction fan I highly recommend this book ok so that's the idea of ubiquitous computing which helps to orient us to think about how to go about designing this technology if they're going to create a large number of devices that are sensing and acting in real time in the a world alongside us it would be great if we're not aware of what their they're doing let them do their up their job so we can get on with us so how do you make this invisible how do you create technology that supports us like in our childhood well we're gonna draw on this important concept of scaffolding and through the rest of this section of the course we're gonna come back to scaffolding several times this is an idea that comes from developmental psychology or the study of child development scaffolding is a very important and intimate process that goes on between parents and children where parents either physically or metaphorically support the learner and as the learner starts to signal increasing competence at whatever it is they're trying to learn which in this case is to walk on their own two feet the the instructor or the teacher or the parent or the caregiver is sensing the fact is somehow inferring that the learner is getting better at the task and gradually removes the scaffolding until there is none left that's the idea okay tricky thing for the teacher to do how do you detect growing competence okay if scaffolding is done right it induces a learning gradient so a slope up which the learner is climbing and they're getting gradually better and better at the task right a common example of scaffolding from child development our training wheels on a bicycle if you if you've never ridden a bicycle you get on a bicycle without training wheels there's no gradient 99% of the ways that you apply force to the pedal pedals are gonna end in disaster and there is a very small minority of actions that the peddler can perform that will keep the bicycle upright and headed in the right direction how do you find that action that needle in the haystack if everything you do causes the bicycle to fall over you add training wheels and now 50% of your actions cause the bike to move a little bit forward and the rest cause the bike not to go anywhere there's now much more opportunity for the learner to start to experiment with this device and learn about it and when they finally figure out how to induce motion and balance on the bicycle you can remove the training wheels what are some examples of technology that do something similar right so a lot of software at the beginning says do you want to be walked through the tutorial or the tips and you have to click on something to say yes or no which is fine but now obviously we've costs the user having to click and say yes or no good technology doesn't have to ask the user but like a good parent is sensing growing competency in the user and removes scaffolding and tips and tutorials as competency increases clipping could Oh clipping right we failed miserably at this task so a lot of Technology if it senses that you're struggling will pop something and ask if you need help which is okay other examples ok let's try in for a moment mentally design some technology to do this scaffolding you've all probably used GPS on your phone you turn on GPS and it tells you how to get from point A to point B if you turn on sound a voice tells you in a thousand feet turn left in five hundred feet turn left and 250 feet turn left if she tells me one more time to turn left I'm going to throw my phone out the window right it's all good it's fine but could you imagine a GPS system which is sensing growing competency so I traced the same path a couple days in a row what is it that is going to tell the GPS whether I remembered the route and I don't need to be told anymore to turn last or I know I was talking to a friend and I I wasn't paying attention the last time we traced this route so I still don't know how to get from point A to B I'd like some verbal help of course GPS could ask and I could say yes or no but could it in fur growing competency how would a GPS system know whether I'm learning the route or not okay so we have an accelerometer in the phone this is probably going to be useful but how does acceleration what features of acceleration will will tell the GPS system whether I know the route or not okay exactly we're getting closer right something a change in speed which is my definition acceleration might be able to tell the system that I know I'm going to turn left soon or not but it's tricky what feature of acceleration might tell you that there's a great example right so perhaps she asks a probing question and it causes me to change my acceleration if she's giving verbal hints and I'm ignoring her and when she speaks my acceleration does not change that's a sign that maybe I know the route and I don't need any more verbal hints that's a great observation right paying very careful attention to in this case the physical context of driving either aided or unaided by a GPS device and using that to allow the GPS system to remove scaffolding when it's no longer needed because especially in a driving situation I prefer not to have to look down at my phone and turn off or mute the sound right it would be good if this was just intelligently in third absolutely right so we could we could start to use facial recognition lots of other features going on in the car that we could exploit to figure out how to intelligently tune the amount of scaffolding provided during route taking okay so as I mentioned we're gonna come back to this idea of scaffolding several times throughout the rest of the course and scaffolding is the focus of deliverable 9 in deliverable 9 you're going to be adding three scaffolds to your system educational software in particular education it's all about scaffolding first scaffold you're going to be you you're going to be applying he's challenging the user to sign an increasing number of digits as their performance increases so instead of creating a system that says I think you're pretty good at digit zero would you like to move on to digit one we're going to try and intelligently infer increase in competence and expose the user to more digits as their competence increases how can you detect increasing competence given your code base so far absolutely right so your KNN is gonna come in very handy here your KNN will be able to tell you whether the user is signed incorrectly or not we might average over multiple demonstrations and take some average is the average number of correct gestures increasing like the example of GPS we just talked about there's some other additional subtle cues that you might be able to pull out that indicate growing competence exactly absolutely so time is going to be very important as we move forward with this technology I can't remember if we've introduced the time library yet or not if you haven't it's very useful you could measure using the time library the number of seconds that elapsed between your system flashing up the digit 3 and the number of seconds that elapsed until the K&N detects or predicts that the user is signing the digit 3 how long does it take the user to figure out what to do in addition to how often do they get it right so there's a double challenge and deliverable 9 which is not just thinking creatively about the scaffold itself but more importantly knowing when to apply the scaffolding and when and how to gradually remove it ok second scaffold you'll be adding will increasingly challenge the user to remember the gesture associated with the digit as they improve so at the beginning we're going to tell the user how to sign 3 4 5 and so on but we would like to gradually remove the scaffolding of showing them the gesture and forcing them to remember it simplest way to do that is obviously to flash up an image of the gesture itself for a shorter period of time how else might we help our users gradually memorize the gestures associated with the digits one of the one of the challenges and scaffolding is not to remove the scaffolding immediately take off the training wheels and now we have a learner that's completely unaided we're always looking for ways to include gradients to make this process of learning or memorization in this case as smooth as possible the gesture we have the image of the gesture the image or a figure or whatever it is you're showing the user what they should do you can remove it after a shorter and shorter period of time what else could we do absolutely you could we could put up an image of the gesture but you also have the virtual hand itself you could draw the virtual hand that corresponds to whatever the user is actually doing it could draw an additional skeletal hand which is showing what they should be doing and they are trying to match their virtual hand with the demonstrated hand okay so go from do this to pick which of the two is correct so recognition rather than remembrance we talked about that several times already like three and six I still get confused after all these years if you put up two images I can usually remember which it is unless the two images are three and six then I'm still confused but I can usually pick up the sign for three compared to the one compared to one or zero and so on so we could go from do this to challenging our user to recognize and then finally to challenging them to remember right that would be adding in a gradual learning gradient okay again in this assignment and all the rest of the assignments it's up to you to come up with your particular solution the third scaffold will increasingly challenge the user to sign that gesture faster as they improve how might you induce a learning gradient here okay okay there's a great idea I could get into sequences of numbers and you need to sign them as quickly as possible maybe the sequence grows longer as again they're able to succeed in this this task ok any questions about deliverable 9 I think it's pretty straightforward okay okay so that's scaffolding let's have a look at another application this one is is one of the primary applications of ubiquitous computing another reminder here of visual design what are these three population pyramids on the right telling us populations are getting older right the graying of the global population these are three population periods at three different points in time for a particular country which country is it United Kingdom very good how did you know thank you the Ministry of Health and Labour spelled properly okay okay so one of the challenges obviously in manipulating slides which I'm clearly not very good at how do I that's not going to help us much let's try this let's try this okay okay so we're gonna look at an application where we're going to think about trying to deploy ubiquitous technology into the homes of the elderly so we can keep them as independent as possible most people would prefer to stay in their homes than in a care facility and from an economics point of view this is also useful for from a social point of view so here's a little cartoon example here we have miss X who lives alone in her home and her mobility is declining family members would like to call and check in with her every once in a while if you call and the phone keeps ringing do you jump in your car and drive over or is everything okay one potential application would be - would be to stitch a number of sensors into various carpets or flooring throughout the home and write a little bit of code to support this distributed sensor network if the phone rings and we detect pressure sensors we detect pressure on the carpets and its misse-x moving about we give a message saying this X is approaching the phone if she's almost at the phone just let the phone ring normally if the phone continues ringing and there is no detection of movement in the home perhaps you send out an alert - to the caregiver I've drawn this using something known as amia or entity relation modeling for information artifacts it's a bit of a mouthful but if you work in industry you're probably coming up against the hermia notation in erbium notation we're gonna use rectangles to represent physical objects in some way they could be people they could be non intelligent devices they could be intelligent devices we can have attributes associated with those devices we can draw edges together objects and numbers in the corners of connectors represent the one-to-one one-to-many or many-to-many relationship so in Miss ex-home she has anima carpets and in each carpet there are and pressure sensors again this is a little bit cartoonish it allows us to pretty quickly sketch out an idea for a distribution of people objects and technology what else might me deploy into Miss X's home to increase her independence assuming that she is challenged by decreasing mobility any ideas what's the context here that's most maybe motion motion detection okay so we're trying to detect motion okay so maybe motion detectors in the rooms to detect motion if we want to detect X miss X's motion what's the easiest way to do it have her have a wearable sensor maybe miss X is not willing to wear that technology right the easiest solution to keeping people independent in their homes is to turn their homes into big brother right sensors everywhere on the person in the person in some cases however again that may be the most that might be the easiest way to solve the problem but from Miss X's point of view it may be unacceptable right that someone is watching at all times again we've talked about acceptability this is a difficult thing to get right in this domain but again from a healthcare perspective very very important and increasingly important okay again just something to think about okay lecture 16 is a little short so we're gonna move on now to lecture 17 in lecture 17 we're gonna look at the first of these three applications where we're going to try and apply ubiquitous technology we're going to look at some investigators that deployed ubiquitous technology to pose and try and answer a scientific question that would be difficult or impossible otherwise the first one we're gonna look at is social network inference so if you give me all your your Instagram accounts I can use those to know your social network at least that part of it that's on Instagram assuming I don't have access to your social network and I just interact with you physically in an informal manner on campus how might I infer the social networks that exist among this class so have a look at an application this in this case the investigators wanted to ask the following question could you use ubiquitous technology technology to extract face-to-face information so not tweets and Instagram posts but look at face to face information from social groups and learn about how people behave towards one another in this social network okay that's the starting point let's talk a little bit about the experimental design we're going to just talk about this at a high level for those of you that are interested you can go and have a look at the research paper itself there were 24 graduate students who were not involved in the study they were graduate students from a different lab these were very accommodating graduate students they were willing to wear back in the in 2008 this sensor pack which you can see is on this strap going around the back and tellingly this sensor this sensor array is close to the speaker's mouth and also close to their ear they agreed to wear it for a six month period the sensors recorded what the speaker's spoke but it threw away all the information of the content of speech we're going to see this several times in many of these applications for important privacy reasons we are not going to be recording what the person wearing the sensor said what the person they were talking to said or most importantly a third person walking by who knows nothing about this application or this project it's also not going to record what they say we're gonna just record in this case the volume of their of their speech and what's known as prosody the way in which they spoke and will talk more about porosity in a moment okay given the speech data we're gonna collect how people spoke how these 24 people spoke over a six-month period can we infer from that first of all can we learn a social network among the 24 grad students which grad students know each other better than others or more friendly towards one another and if we can infer this social network can we ask unique questions that would be difficult or impossible to ask in a social science setting or questions that would be difficult or impossible to ask even if I had complete access to all of your Instagram and Twitter and Facebook accounts which is given your position in the social network does it alter the way that you behave or the way that you speak in that Network this is a question that's been on social scientists minds for a long time where are you within a social hierarchy do you know where you are in that social hierarchy and does your position or your knowledge of that position alter how you speak to others at different positions in the social hierarchy among a social group so we're gonna focus on the technology but most importantly we're trying to think about how to use this technology to try and answer a question that's difficult to do otherwise okay so what's in this sensor pack there was a number of sensors accelerometers infrared and visible light digital compass temperature barometric pressure humidity most of which probably are irrelevant to social interaction we're gonna focus on just the microphone and only focus on how people are speaking to one another so right at the point of capture the microphone itself the moment that the text the speech is captured we're gonna throw away the contents of speech so that we're not recording speech putting it on a hard drive and then throwing away the speech later so it's an important part of these scientists being allowed to do this experiment in the first place and we're gonna store on the device itself just the speech statistics the statistics of prosody the way that they speak okay importantly these subjects we're not wearing the devices all day every day for six months they were only wearing them during working hours one week per month for six months okay let's dive into of the data maybe I should maximize this so it'll be a little easier to see okay we've got five panels here which as you can see top to bottom represent the days of the working week the horizontal axes range from 9:00 in the morning to 8:00 at night and in this time span we're gonna break this time span into five minute buckets and inside each of these five minute buckets we're gonna look across the speech statistics that we obtained from those twenty for graduate students and within that bucket we're gonna measure P SS or persons seconds speaking imagine that within that five minute interval all twenty-three graduate students are sitting quietly in the library they're not speaking and they don't hear anybody speaking but one of the graduate students is involved in conversation with someone outside this social group and they're speaking for three minutes out of that three minutes out of that five-minute period three times 60 is 180 180 person second speaking seconds assume all twenty four graduate students are speaking for 10 seconds only 10 seconds during that five-minute period 20 for graduate students each spoke for 10 seconds 24 times 10 240 person second speaking so it's not how many people were speaking just the total amount of speech that was going on among those 24 students during this period okay just given that data alone you can already start to infer something about this group what is actually you can infer several things about this group remember as always in HCI we're starting to think about context right what what are these graduate students doing how are they behaving how are they speaking what can you tell me about the context here why the tall blocks Tuesday and Thursday morning maybe they're all in a meeting with one another maybe they're all in a meeting apart but then why this regularity of Tuesday and Thursday morning if they're apart generally they share a schedule at least in some ways for some reason they're all which again from context you'd you'd sort of imagine right so I said 24 graduate students graduate students is a much smaller pool than the undergraduate pool so they're probably taking similar courses and seminars and they're probably together Tuesday and Thursday morning right it would be unlikely that they're all in different places and it's just at this particular time that there's a lot of speech let's assume they are together Tuesday and Thursday morning are they in a lecture were they in a seminar seminar why a seminar absolutely so importantly as I mentioned before the microphone is very near their mouth and near their ear so this device is able to distinguish between are they talking or are they listening to somebody else talking and PSS is only measuring them talking it throws away for the moment information about somebody else talking so the 24 graduate students themselves are doing much more talking Tuesday and Thursday morning than they are as a group on average the rest of the working week right so we're already starting to infer something about this this group not necessarily about social dynamics yet okay so let's have a look now at starting to zero in on social interaction let's imagine an example here so in a particular point in time imagine we have three of the speakers that are in a classroom or they're together somewhere they're in a coffee shop and the three of them are involved in a conversation outside that room in a neighboring hallway speakers four and five are also in conversation imagine further that the conversation inside the room just before it finishes the conversation out in the hall starts up so we have two separate conversations among two different groups with different numbers of members in each of those two groups and the time at which those conversations occur is overlapping we don't know that however we just have the microphone data right so what might that microphone data look like if we collect the data from these five speakers we might notice that for a certain period for a certain period speaker ones volume is high and then their volume is medium same thing for speaker two and speaker three and we might notice that this nonzero volume or medium and volume covers the same period of time it suddenly grows silent for all three or at least there's no more speaking they go into they go into a quiet study session or something else happens and the conversation ends same thing for speaker four and five how do we take this raw data and discover that in fact the first three speakers were in conversation and then speakers four and five speakers three and four possibly they're definitely speaking at the same time there are conversations where people are speaking at the same time it's a good start they match up right so I arranged these five plots vertically as a hint right we're looking for temporal coincidence things are occurring at the same time right we might have people that are actually speaking at the same time but are still doing it as part of a conversation face to face with one another so how do we look for this temporal coincidence we're going to use or the investigator to use something known as mutual information we're not going to go into the mathematical foundations of mutual information for our purposes like correlation it tells us about the mutual dependence between two variables so let's apply this idea of mutual information let's start to march from left to right along these five panels and at each point in time we're going to pick one of the five panels and take the the amount of volume and compare it against the amount of volume in the other four panels among the other four speakers and we might notice for example that there is high mutual information actually just during this period here during this period whenever speaker two's volume is high the speaker's for the other two volumes register medium volume for that period so another way to think about mutual information is whenever I measure one thing in this case high volume for speaker two I can be confident if there's high volume here I can predict medium volume for speaker one and speaker three and at least within this time interval I'm always right that's mutual information so there's high mutual information here there's also high mutual information during this time period I can be confident that whenever speaker threes volume is high speaker ones volume and speaker twos volume is medium so among these three speakers as they march from left to right during just this period there is high mutual inform the particular correlation between whose volume is high and whose's medium changes but mutual information is high if we look at this time period here and we look at speaker for speaker for volume is always high but I cannot be confident that the high volume for speaker 4 predicts the volume of speaker 3 there is low mutual information between speaker 4 and speaker 3 during this period so you can imagine as I'm computing mutual information during this time period the distance between speaker 3 and speaker 4 is increasing the more mutual information I detect between volume levels of different graduate students for the same slice of time the more confident I can be that they're in a conversation even if they're speaking over one another so far so good okay so that's how we're starting to go from raw volume information to social network inference so if I take just these five graduate students I can be confident that at least for this time period these three speakers were in a conversation so there's some we're gonna set some threshold and if me if mean and mutual information goes above that threshold I predict these people were in a conversation and I'm going to represent it using a social network so in a social network we have nodes and edges the nodes represent people and the edges represent some social relationship between them in this case in our purposes an edge is going to represent that that pair of graduate students was involved in a face-to-face conversation with each other for some period of time we don't know whether they're friends we don't know if they're competitors we don't know if they're arguing with one another or agreeing we don't know anything else we only know they were together in the same place and they're involved in a conversation together if we repeat this process over lots of these slices throughout this six-month period we can detect multiple conversations among different subsets of the grad graduate students at different points in time every time we every time we predict a conversation we're gonna add an edge to our network of 24 nodes if we go to add an edge like between s1 and s3 and there's already an edge there then we assign a number to that edge which is 2 so we're gonna now have a weighted social network so the edges have weights the weights are integers and the integers represent the number of distinct conversations that that pair of graduate students were involved in face to face ok so now we've gone from raw volume information to a final network which is our prediction about a particular kind of social dynamic how often were they involved in conversation with one another ok that's social network inference that step one now that we do that can we start to ask questions about social dynamics that would have been difficult or impossible to do before we're going to look at now is we're gonna look at now not just not just we used volume to infer the social network we're now going to use for other features of prosody for other ways in which they spoke but not what they said and we're going to see whether they're positioned in this social network that we just constructed if that correlates with the way that they speak to other members of the group we're gonna look at rate how quickly did they speak pitch did they speak high frequency low frequency turn length how long did somebody speak before they seated the floor to somebody else and within a given conversation what was the turn frequency how short were the intervals during which one members volume was high and the other volumes were medium okay so let's start with the hypothesis people change their normal way of speaking more when they speak to strangers than with good friends okay so there's several nouns in the sentence that we're going to have to pin down using metrics or data so what do we mean by their normal way of speaking and what do we mean by strangers and good friends let's start to test this hypothesis by computing a B I / J what that mean B is gonna represent one of these four features of prosody and we're going to ask about I the person that's speaking so among these twenty four graduate students we can pick one at random that's graduate student I when we can then look at this speech frequency this speech feature such as rate how fast did I speak how fast did I speak when they were speaking to everyone else in the social network all the other twenty-two graduate students how fast did they speak and compare that to our sorry just take the mean of that rate so how quickly did they speak to the other twenty two graduate students then we can ask for the same graduate student I how fast did they speak when they spoke to Jay the twenty-fourth graduate student so how did they speak to a whole bunch of other people in the group and Jay we can then also compute s sub I which is the standard deviation of the speech feature so for a given graduate student I if they have high standard deviation sometimes they speak slowly sometimes they speak quickly if another graduate student I has low standard deviation they tend to always speak at the same speed humans differ in lots of different ways we all have different ways in which we change how we speak okay we're gonna put these three variables together to compute D IJ and D IJ is gonna represent the amount how much eyes speech changes when speaking with J right so we're taking their mean rate of speech we're subtracting out their rate of speech when they talk to J if they speak to J at the exact same speed as they talk to everybody else these two bees will be equal cancel each other out will be near zero and we're going to normalize we're going to divide by standard deviation we're gonna try and ask how the rate of speech changes for all twenty four graduate students but some of those grad students their rate changes more compared to other graduate students and that part we're not interested in it's a confounding variable so we're gonna get rid of it by normalizing by S sub I so far so good okay so we know that D IJ if it's zero that means I doesn't change the way they speak when speaking to J compared to everybody else if d IJ is high is positive the higher it is the more they change their speech compared to J okay we're then going to compare D with C the fraction of time that I speaks with J okay so now we've got C and D the amount of time they talk to J and how they change their speaking behavior when speaking with J and we can then go look for a correlation between C and D and it turns out that for all four if we repeat this process for all four of these features of prosody there is a negative correlation the higher C is the lower D is and vice-versa does that prove does that provide support for the hypothesis or evidence against the hypothesis it proves it right how so how does C and D relate to this hypothesis that's right so good friends is a bit of a subjective term here right we should really replace good friends with people they often speak with during this six-month period exactly right so let's take that example the more you speak with someone that means CIJ is higher you speak with them a lot and D is lower we're looking at there's a detected anti correlation here hi-c Lodi I know someone very well I talked to them a lot and my D is low the way I speak to them is the way I usually speak to other people in the group may be more tellingly is the opposite when C is low I'm speaking to one of the other 23 graduate students that I rarely speak to during this six month period C is low D is high however I normally speak I'm now speaking differently to that person than I do on average to the other people in the group okay as I mentioned this correlation is negative 20 correlation so I changed my rate of speech when I'm speaking to a stranger in the group I change my pitch I changed turn length doesn't say whether I speak more or less I just changed my turn length same way thing with turn frequency we're gonna end today by just talking briefly about P P is statistics probability that our result is false the null hypothesis so maybe we're wrong maybe when C goes up D actually doesn't go down the probability if C and D relate to rate is extremely low it's near zero so I can be pretty confident that rate changes when I speak to a stranger P is also low for pitch I can be pretty sure that the pitch of my speech changes when I speak to a stranger I can also be confident of it for turn length P here is above 0.05 which for most statisticians makes them nervous I would probably reject that one turn frequency by the grad students are probably not changing that one in the response in in conversation with strangers quick question possibly they might be where they might be related that's a good point okay you have a quiz due tonight you're working on deliverable nine and I will see you on Thursday 
DUc2Vqkmohc,28,"A guest lecture in Yang Liu's course ""Topics on Computing for Society"". Gives a broad overview of research in the Tech4Good Lab that cuts across diverse methods and application areas.",2019-12-11T05:10:12Z,"HCI and AI for Society: Education, Work, Civics, Governance, and Human-AI Collaboration",https://i.ytimg.com/vi/DUc2Vqkmohc/hqdefault.jpg,Tech4Good Lab,PT1H26M24S,false,35,0,0,0,0,"we are very happy to have professor David Lee so guest lecture here he's gonna tell us something about AI post or show good I like collaborations he's a professor at a computational media and it came here a couple years before me it was it was really impressive I know Dave's work several years ago and even at that time AI was not super hyped he was thinking about this like AI human collaboration how humor I can attract so that was pretty remarkable there's not so many people were thinking about this this business popular like everybody jumped on this topic but at that time it was not anything to do sure yeah thank you yeah thank you so much for having me and really excited to be here and and we're not that old yeah I know but but yeah so I'm going to be talking today about eight ten AI for society and basically and I'm gonna try to kind of tank together and I just basically through all the stuff I'm interested in and hopefully it will be interesting to you let's see okay so this is Jonathan Brown and he describes these three different era for how we live work and learn he says that you know when he was growing up his father was advising him on his career and and he said hey John um think of yourself as a steamship and set a goal for yourself and just like go for it don't let anything get in your way and John said well when he was actually building his career which was very successful um he he said that his crew was more like a sailboat it was more you know instead of powering through obstacles it was more getting a sense of the different winds of change and tacking along to get to the the place he wanted to be and he says that now we're in a whitewater world so you're not just tacking around to get where you want to be but you're constantly being buffeted by new rapids and you have to adjust to even stay afloat so what's the reason for this whitewater world we're in well if you think about the digital infrastructure that's changed in the last couple of years there's been a lot of changes and you obviously have internet which connects us to information connects us to each other and it's created this digital world that we can tap into and but you also have our devices right so the things that our smart phones of Internet of Things the things that take our digital world and kind of plug it into our social and physical worlds that we're connected to um so that it's not just us kind of logging into this digital world it's actually it sending push notification to us and then you have the cloud you have the API economy and all this infrastructure that makes it really easy to develop deploy and scale new technologies and then bring them into this digital infrastructure that then affects our social and physical worlds that we're living in so what does that mean it means that we have this connected digital social physical world and that there's these new technological innovations be introduced in it and that are not just affecting our digital world but also how we live and work right and we have to figure out how to adjust to it on how we live in in such a world so my research is in social computing that means I study the intersection of computational systems like these and social interaction and in social computing we like to study what we call socio technical system so these are there used to be you know isolated systems think Facebook or Twitter they're in the cloud and we call them socio technical systems to remind engineers that they're not just technical systems and they need to be designed for the people in it and the properties of that system also depend on the different social dynamics the social norms and the human factors well I think we're in this place where we're not just dealing with these individual isolated systems this connection between this global digital world and our social one has created a socio technical world that we're living in I like the way that my past PhD adviser put it Shh go Alex from Stanford and he said algorithms used to be what made computers run faster now they're creating social and economic systems and they're creating political systems and educational systems does anyone relate to this so now what does it look like to live in a whitewater world well our economic system is is kind of organized around expertise and professionalization right you go to school study for many years you come out with some area of expertise and you enter the economy and contribute to society right that's kind of how it's supposed to work at least that's how it's supposed to work in a steamship or a sailboat kind of world but when you're in an environment of constant change and that changes the way you learn you have to continuously learn and you have to learn and ways new ways that can cross knowledge boundaries but it's not just learners that need to be adapting our educational systems they also need to provide ways for students to keep up with that pace of change and our work institutions need to support on-the-job learning so there's all these different things that we need to do to adapt to a changing world and it's not just individuals but it's also the communities that need to do this adaptation right so people are you know asking the government asking nonprofits how are we gonna regulate and create new policies for all the new change that's happening in our world when you have to deal with dragula scars blocking technology right all of these things have impact and how are we providing services for the people who fall through the gaps these are all really challenging questions but they're important for thinking about this socio technical world that we're in it's not just technological Rapids there's a lot of different Rapids right natural Rapids political rapids we're in a whitewater world it's hard to imagine how how to live in it so what I want to do in this talk is to explore this question of living in a whitewater world and really you know these are big questions so it's really about exploration not answering asking questions doing a little bit of dreaming and I'm gonna be focusing around these three topics so the first is how do we support communities in navigating a whitewater world through large-scale societal collaboration and then how do we help individuals navigate a whitewater world by thinking about how we can reinvent education for continuous learning and then finally um how do you work towards human AI collaboration that supports individuals and communities in navigating this whitewater world okay so the first two are gonna center around kind of past projects I've worked on it's going to be you know much more at a high level paints the big picture of some of the things my own journey is through these topics and then the last topic we're gonna go more in depth in talking about some work that I'm thinking about right now and and yeah details around that cool okay so let's start kind of roughly where where I start at the beginning my PhD I was working on voting and decision making one of the closest ideas to kind of this concept of large-scale collaboration is what's called participatory democracies there's an idea from political science and that advocates greater involvement of people in political decision-making so the idea is you know if you involve more people that can promote civic engagement it can foster accountability transparency leadership and then hopefully allow the government to tap into the knowledge and expertise that already lies within their community and there's a lot of examples of this kind of thing already being done in real communities in real political settings so from local to to national government when the more successful widespread models of this is called participatory budgeting and how it works is that a government sets aside some of its money and it basically lets the community members decide how it gets spent there's also examples around things like policy reform in Finland crowdsourcing the off-road traffic law and even on constitutional reform in the Icelandic experiment so all sorts of different types of experiments around this topic and roughly speaking all of these experiments unfold through roughly four stages okay so this is patisserie budgeting in particular and after the process designed community members they start by brainstorming ideas these ideas are developed into proposals in partnership with different city agencies and they're voted on by the community and then finally the top projects or policies are funded or implemented now the major challenge in these kind of processes is that they don't scale very easily everything requires going in person to a town hall meeting that has a certain room size and and then there's a challenge of managing all the ideas right so if you have a large number of ideas how does any person actually look through all ideas much less rank them or disgust them or talk about them so those are came to the challenges and when we got involved in working on this and we mainly did two things at first we created a simple voting platform that helped to bring some of their processes online and then the second thing we did is start thinking about algorithms for voting at scale so if you have a thousand ideas you know no one has time to look through all of them I'm much less rank them so how do you find the top ideas and this is an area of research called preference solicitation so basically instead of asking the user to rank all a thousand ideas and algorithm they might you might just pick two ideas and ask a participant well which of these ideas do you like better this is obviously a lot easier for the participant but the question is can this find the true output ranking or something close to it that would have resulted if everyone had given their full ranking and hopefully can it do it in a small number of comparisons per person so I told you that the first part is high level so the answer is yes okay I'm not going to the details thread to ensure answers in practice yes and you really only need a small number of comparisons and there's like all sorts of theorems like this one that say you know four different types of voting rules and essentially you need on the order of you know something like n log n comparisons where m is the total number of proposals that's great because even if the number of proposals is as large as a number of people that that means you only need around log m comparisons per person right which is just good this log n in practice extrapolating from the work we did in Finland if you have around like a thousand proposals ten thousand people roughly this means you know after you get the constants around like 10 to 20 comparisons per person so I thought that was oh yeah you can achieve some kind of large-scale collective outcome with just a little bit of contribution per person right so you're approximating what would have happened if everyone had ranked a thousand proposals which is completely impractical yeah yeah so so there's different ways to do it in the case of the this border thing so so one of the key things is just random sampling but um you can sample in different ways and then there's a bandit version for kind of eliciting and there's all sorts of other methods outside of what we used I think yeah so things like minimizing max regret that's something that Craig what Leo did so a lot of different types of techniques for this and there's also a lot of different types of goals depending on the type of voting rule you're trying to achieve because there's not one function for how you go from a set of rankings to to end end ranking I'm kind of glossing over all this right um so I thought it was really cool and then I talked to Jonathan reconcile who's that it was a cio of paltrow at that time and he asked this question well how do you know that the top ideas are actually better right I are the most voted for proposals really the right ones and I had to admit that like I didn't really think so right so you know this wasn't an election for representatives this is actual decision-making the number participation we have here is not you know even close to the level of participation in elections which is already you know not that great so you know I found it hard to argue to myself that you know to the people who are actually think about the policies that you know the top ideas are necessarily the right ones yeah so well so I think what he's getting at is she wants some way to Agra the ideas based on you know real deliberation on the pros and cons of the different types of proposals and maybe some evaluation of based on some normative properties of their impact on the community so I think that's kind of ways getting ideas like if you just have if you just have a ranking of and you're just saying these are the top ones well that's kind of like well I guess these are like old political questions right like you're in your majority all sorts of things like that but especially in the context of of these participative budgeting or you know participate democracy kind of events um I felt like a lot of the you know arguments maybe for like representative raucous II well it's like a little bit weaker because um you know it's happening if you're thinking about applying it like all sorts of these different things and most people don't have time to participate you know does that really make sense to just aggregate vote um and and this made me think right can we go beyond voting because the interesting part is not just the voting itself in participate democracy but actually the process of coming up with ideas right or or maybe it's the process of deliberating and learning right because those are the parts we're engaging people to like you know get new information and you know that intuitively makes sense because people can do a lot more than just vote right they can talk with each other they can make compromises alright so how can you incorporate that type of contribution into these algorithms what would it look like if there were is like thousand people here kind of talking to each other making compromising decisions well that's it's also not practical just like it's not practical for any person to look at a thousand ideas so I thought you know what if just like we used a sequence of comparisons to approximate a large-scale vote could we use a sequence of small group deliberations to kind of approximate a large-scale deliberation whatever that means okay so that's kind of basically what I was thinking about and you know what would it look like if I were to have an algorithm that said hey young and yeah tongue and what's your name Sayid right like why don't the three of you like get together and talk to each other about what you think the right proposal is for XYZ problem and you can do whatever you want but at the end you have to agree on something and then report that to the algorithm cuz you have to come up with some compromise decision and let me know and then after you do that then I'll pick another three people and I'll repeat the process okay so you know what do you guys think the decision should be and you can talk and whatever and could we create something where you take a sequence of these things and you get to something interesting and then go into the details but it turns out that roughly speaking you can and what I mean by roughly speaking is well this is a mathematical model so what does it even mean to do that mathematically so how we modeled this is we said you know suppose that people are represented by points in space okay I'm closer to people who have closed their preferences to me and and suppose that every group of three people is able to converge to a good compromise decision so what is good compromise that doesn't mean um let's say it's like the point that minimizes distances to each of you and and then we showed that if that's true if groups of three are able to come to their compromise decision then you can use a small number of these small group deliberations it's also n log N two find the top compromise decisions across the whole population in other words the the points that minimize some of distances to all points and so again n log n is good because you have n people so an average that's log n small group conversations per person so I thought okay that's great that's really cool but but again real deliberation is not just you know let's get to the median opinion right that's that's just compromise and it should really capture changing opinions so now you state a really interesting new piece of information and I'm like wow that's interesting way of thinking about it I changed my opinion right that's kind of like what you'd want to see and participatory democracy getting all the perspectives and aggregating that together towards some better outcome or how about ideation like coming up with new ideas and so I thought that's what I really want to work on but how do I work on that using mathematical models and so I had done some work on opinion formation and dynamics in the context of polarization and but I really struggled with like how close are these models to thinking about ideation or deliberation or the things that I really want to work on so that pushed me into the space of soul-searching it's like basically what am I doing with my life or for my research and you know is my research useful how many of you guys are PhD students any of you like towards the end of your PhD okay so you know if you're kind of getting towards the end that's play what your feeling is like oh I don't know what I'm doing with my research it's completely useless and and so I started thinking about that and I'm thinking well like you know is academia really the right place to work on questions like these and if not then where so I started doing all this you know random exploration of different things and um see the thing is the research community that I was working in in in my PhD was really focused on mathematical modeling right which was really great at understanding and designing collective properties but not so great when I'm thinking about like human factors or motivation or ideation or deliberation things like that and so I was like okay this this doesn't make sense and I thought well I'm at Stanford I should I'm not gonna go into academia so I'm just gonna explore entrepreneurship that make sense and so I was working on some different startup ideas as very lucky to participate in some fellowship programs took some courses in business school I mean I really liked it and because well there's you know I had I had a really negative view of entrepreneurship before because it's like you know it's all about just making money and people who want to control other people anyways so and you know one of the things I really enjoyed about explain entrepreneurship is I realized that that's not really what it's about it's really about how do you bring ideas from my head into the world and at scale and for impact well there's that there's also that other part but um so they start working on on these ideas still related you know still in my mind working towards the end goal of like bringing communities together to work on problems and and I was working on this I started wrestling with this tension between monetization and the mission I really wanted to work on and at some point I just realized it wasn't gonna it wasn't gonna work and at this time Jeff Raikes he was the he was the ex CEO of Gates Foundation he came to talk to some students and he said something really interesting he was talking about his philosophy towards philanthropy and he was saying well the private sector is great used to be an executive at Microsoft and so he says what's really great at providing services for things that are monetizable and the government is also great it doesn't have that monetization constraint so it can provide services that don't have that but it's not good at doing things that are high-risk or require innovation had this gap in the middle things that require a lot of risk and innovation but maybe not clearly monetizable in the short-term and he was talking about you know kind of philanthropy but when he said that it was like a light bulb came into my mind I was like oh well maybe academia could also play that role maybe no academia doesn't have monetization constraints supposed to innovate and do high-risk things right of course academia has that publishing thing knowing publishing thing and so it needs to create new knowledge um and so I was thinking okay well maybe this could work maybe I could you know go back into academia and I never officially lost my advice is very nice just let me play around and and and and and one thing I realized is that there's these different research communities and each of them valued different things and they value different types of knowledge that are produced at different points in bringing AI an idea into the world so I realized that there's actually people out there who work on human factors and motivation and design that was HCI so I started thinking well maybe I could go into academia and couple a process when I'm building things with communities and deploying them while also kind of publishing across different communities that was my idea everyone was like that's stupid I don't do that wait till after tenure and so that's kind of the rough frame of mind that I had when I decided to jump back into academia and now I was like okay I'm gonna try to get into the design world and you know take a hold on the voting decision-making algorithm thing and what I really want to work on is how to support people who are on the ground providing services nonprofits governments volunteering and right around this time was when the Syrian refugee crisis was like erupting to over 4 million people displaced and I was just struck by the scale of suffering like a third of a three displaced nobody wants to take them in it's a hard it's really hard challenge and my research was in crowdsourcing but I didn't if you like anything I worked on was actually useful so it's like so I started thinking about like how can we organize people to to do things to support kind of volunteering and nonprofits and want to start local which is important for design for iterating and so I started thinking about refugee resettlement in the u.s. at that time was the largest resettlement destination takes around 70,000 or it took around somebody thousand to a hundred thousand refugees a year and that process kind of goes through eight different agencies the largest as Catholic Charities around eighty percent of that and then there's also our C Jewish Family Services so I started talking to them want to understand are there any ways that maybe technology can help organize volunteers to help them more that was my basic thought and and I thought that well you know in the digital world we couldn't have examples of this and we have Wikipedia a bunch of people creating an encyclopedia we have things like fold it it's a game where people learn how to fold proteins a lot of people like playing it for some reason and the top players they actually have made scientific discoveries and then there is duolingo which is a language learning app I think some of you might have might have used it before but how it works is as you're learning a new language you're actually helping to translate the web so it's actually all these different examples of how in crowdsourcing people have been thinking about how do you organize a large number of people to do something I thought maybe I could apply that to the volunteering setting I initially wanted to start with something simple but whenever I was talking to typically the volunteer coordinator our conversation somehow would lead to things like building a web app for them that would kind of change the way they delivered some sort of service and and that's a hard problem for them because it's hard for nonprofits to develop internal expertise they can't compete with Google and Facebook in terms of salary and it's actually pretty hard to also cultivate a volunteer community around things that are complex and interdependent it's easier to do like packing boxes sorting clothes things like that things that don't require expertise so this is really a challenge that relates to complex crowd work right how do you coordinate short-term volunteers around complex interdependent projects it's hard because volunteers typically don't have that much time they typically don't have that much experience and so our thought was well what if we created a platform that teaches people how to code another one but as they learn they're actually automatically contributing to building real projects for nonprofits so again I'm not going to the details right now but the short story is that it didn't work many times and then it worked kind of and the first couple of times basically we found there's a lot of interest people are excited but it didn't work for many reasons first even when people said they were experienced well they were experienced in different ways of building websites there's different stacks there's different languages write different best practices and if they're gonna be building a common website well everyone kind of needs to learn the other pieces that they don't know and so everyone is pretty much a novice and so we tried having these like really detailed workflows that describe exactly how it's supposed to work that was way too overwhelming and convoluted and finally we got to work for a simple case so just we just started by focusing on creating the view of a web app so just HTML CSS um given some design and we recruited participants who didn't really have prior background in HTML CSS so it kind of worked right people were able to take a design and build at least the view but one of the interesting things that struck me in this process was how much students wanted to participate I'm even before we got it to work right we found that 85% other participants said they wanted to join even if they were just recreating existing web sites for the sake of learning and it made me realize that students really want opportunities to learn skills in real world settings oops and that's what got me to start thinking not just about kind of volunteering community governance and but also how that volunteering participant democracy stuff intersects with learning and work and experiential learning a lot of knowledge is tacit knowledge that means it can't be learned through lectures right you have to learn it experientially and the way this used to happen is through apprenticeships or internships but internships need mentors and mentors need time right so you probably have experience before that getting your first internship is really hard right because before you have the initial amount experience no one really wants do nothing they don't want to but they can't really spend all that time with you if you know in the end there's not going to be some some results or at least some contribution right so it's this vicious cycle well you can't get experienced until you have experience um I think that's gonna be a really important question in the world of AI and automation Heather McGowan puts it this way she says in the past we learned in order to work that's the steamboat model right and now we must work to continuously learn so how do we do this is it possible to scale opportunities for apprenticeship learning let's go back to how this worked in our nonprofit volunteering project so how it worked is like I said we built another website for teaching people how to but instead of going through like topical learning modules usually mostly the websites you learn HTML then you learn CSS then you want JavaScript right you learn all the different techniques how it works in our case is you work through micro rules so what what are micro rules well at the beginning you might be an elements developer so you have no background and you're given the design for small component and you learn to identify the elements in that component to declare them using HTML in to style them using CSS but not the layout just the individual elements and so you have this elements developer role and you have these series of steps maybe you start with a conceptual step and then you have these action steps where you are doing some work but it's actually doing real-world work as opposed to you know some practice exercise and then after you finish the LMS developer role then you move on to B being a layout developer and you're given the output of a elements developer and now you're learning how to structure elements into a hierarchy and a Dom and you're learning to lay them out using CSS and position them relative to each other and again you have some conceptual stat you have some action steps you might have loops or branches right so basically you have some workflow for doing your role and again it's working on some real real goal and then after that you know in our simple case you would move up to becoming a page manager so this is someone who manages the page right there in charge of decomposing the page into smaller components they delegate them out to elements and layout developers and they kind of stitch it back together so each of these roles they're small roles take a couple of hours and they're not just learning roles so like I said they're the contribution is something they're coming from this hierarchical structure some opinionate hierarchy that's kind of describing one way for how you might build a website or in this case create the view of a page and the top role is kind of responsible for the overall goal and they can delegate out pieces to other people um so it's a way of taking a process for getting things done and coupling it to some kind of experiential situated learning pathway right a pathway of micro rules and we can steadily work towards learning tacit knowledge and you also have these nice and mentorship properties because as you work your way up the hierarchy then you naturally mentor those below you because you have experience in those roles a lot of times we talk about learning by doing right learning by doing says that in order for people to really learn we need to give them opportunities to do things right to learn the context of real work but is learning by doing scalable so I think that maybe it's only scalable if doing by learning is possible so what's doing by learning I made that up it's doing real-world complex goals by knitting together the output from learning by doing because if it's not possible to do that then you're always gonna run into this challenge of sustainability mentorship costs but if it is possible then there's going to be incentives for people to provide opportunities for learning by doing so so I've been exploring this idea in other contexts I'm collectively it's another project in the lab that originally just started because I had a bunch of undergrads and I was like maybe they'll would like reading research papers because of course everyone loves reading research papers and and it turns out that they actually enjoyed it let me started evolving it growing it to people outside the lab and the reason is if you pick the right papers and the right people then it kind of gives students the opportunity to explore the creative side of computer science and to learn how ideas they're learning apply more broadly and they also get to build relationships with peers you're splitting the work of reading so it's a very low time commitment and most of the groups that we've ran so far they're exploratory groups so it's kind of focused on building intuition creativity we started to also think about an experiment with these small groups as building blocks for learning pathways just like the micro rules are kind of building blocks for learning pathway and you and you can have different types of these groups that introduce students to different parts of the research process like ideation I you know literature search and that also involves them in action actually contributing so one of the interesting things about this kind of building block is that it's in person it's a relational kind of goes back to like the earlier thing I was working on like small groups right you can get richer dynamics and it's one thing that's missing from the online world and so there's other work that we're doing I'm not going to go into kind of exploring what does it look like to build communities or organize collective behavior that's built on these building blocks of in-person relational interactions could this help with mediating polarizing discussions because it's impossible to have discussions about polarizing things online right so I've been kind of going all around and talking about some of my design work and really enjoying that but I also like the mathematical side of the work they used to work on and so recently I started thinking about well how do I revive my mathematical work and kind of bridge these together which is the original goal I'm how do I add a stronger data component blend it with design in a synergistic way um well I'm not gonna go into this but on the kind of one obvious immediate direction is to think about how we can model and augment different aspects of micro rule hierarchies and small group networks um to kind of leverage the platform's our building and the data sets that we have or are going to have and right how do we how do we argument different forms of human interaction like mentorship or management or group work and teamwork the bigger theme that all of this falls into is is trying to think about and work towards better human AI collaboration so what is human AI collaboration it's a recent hot word that people talk about a lot one of the things people often are referring to when they talk about human AI collaboration is they're talking about AI that is also having humans in the loop right so they say human in the loop yeah algorithm and then they kind of bring in the humans when they need to write so oftentimes this is related to humans labeling data I'm accomplishing other types of simple tasks that AI just isn't good at another way people think about human AI collaboration might refer to things like smart devices eye things that prosthetics that adapt intelligently to feedback provided by the user maybe they provide feedback to the user so some kind of integration of AI and humans one important question in thinking about human AI collaboration is like what kind of collaborative future are we working towards we are working towards a future that's very these are both human II I collaboration I'm working towards the future with a trait automation centric where humans are kind of low-cost labor and or are we working towards a future where AI is augmenting and supporting kind of human human work for human communities I think yeah actually it doesn't fall cleanly like this like I kind of want to say because automation can really contribute to both sides but and sometime I think I think that sometimes the automation of you focus on automating away holistic because hopefully you're automating wasting things that's kind of helping you men but sometimes they're like kind of this when people talk about it slims they'll kind of map these together so I think one useful way to think about these issues is to consider this access of humans and AI interacting together and playing different roles that are either focused on I'm working on a task or on coordinating multiple agents whether they're AI or humans I'm towards a goal so I would say that most human-in-the-loop work falls in this upper left quadrant right it's mainly like AI is orchestrating right and they're delegating simple tasks to humans to kind of work on like label this image for me thank you right and or transcribe this audio right but AI is kind of orchestrating or you know the smart devices example kind of falls in this bottom left quadrant right it's kind of it's closer to a tool right that's particularly suited and responsive to and in to an individual in carrying out a task so what's what's here on the right side well the bottom right is just what we've been doing all the time for ages right it's just humans coordinating organizing themselves and in organizations and leveraging AI to do different tasks that are annoying like you know in Excel spreadsheets I want to get this linear regression or whatever right that's like some simple form of AI um but it could just be you know any humans organizing themselves and then using AI in the course of our work right how about this top right quadrant something where you have both AI and humans kind of coordinating with one another and passing things on and I don't think there's a very many examples of this and I think that one of the reasons there aren't many examples of this is that we don't have very good models of human coordination and collaboration because that's really complicated like how do humans collaborate with one another well I can't really model it mathematically but since AI kind of ultimately relies on some models is kind of classic this is I need to have the citation copy/paste sitters but means some underlying model and if it's gonna participate in or augment human collaboration it needs some way of modeling that collaboration so it can kind of jump in and participate so how do you how do you do that like that so that's one of the things I've been thinking about how do you model a complex work and how can we work towards a model of human computation that's more suitable for thinking about augmentation of human collaboration currently our models of crowd work kind of look like this right they really focused on this AI is orchestrating and then humans are doing little tasks for them it's really centered around human the loop crowdsourcing and the focus of these models if you are cynical you're basically thinking about one of these questions are you thinking about how do I manage those error-prone humans and how do I do it without paying them too much you're thinking about how do you handle people who are lazy who are strategic and manipulate manipulatives right and then hopefully how do you get rid of them once you know once you've used them and collected all that data and refine your model that's kind of a cynical way of putting it but that's kind of roughly what's happening and it's not really the fault of the models or the researchers it's just that we've been viewing human computation in a very silicon-based way in Silicon you're focused on primitives that are fast that are cheap that are reliable and then compose them together to get some computational goal right hopefully turing-complete you know kind of thing here's that here's an example of how we think about human computation from a very silicon-based point of view alright so here's a description of a challenge problem for something called a crowdsourcing compiler so basically this is a quote that's what is a crowdsourcing pet compiler it's some kind of high-level programming language that compiles down to the primitive code that orchestrates machines and humans towards doing some tests right so in the same way that today's Java programmers of low-level machine leave many of the implementation details to the crowdsourcing compiler and what they're basically trying to say is that you know this is a challenge promise a really hard problem but they're saying well probably there's a way to get at this goal of creating consistent compiler saying well we know that the organizational schemes in most successful crowdsourcing examples share a lot in common the tasks to be performed are obviously paralyzed 'el and furthermore the basic unit of human contribution required is extremely slow takes some punctuation furthermore they're usually very little for the National Park between the contribution so it's saying like when we think about human computation we should be thinking about very small human contributions with very little coordination between them the presence of these commonalities is a source of optimism for the crowd system compiler right specifically painting this picture right see silicon competition is about designing devices that are fast cheap and reliable for simple primitives but humans are really sucky devices we're not fast we're not cheap we're not reliable alright so none of those things however even though humans aren't good devices they're also not limited to small primitive tasks which is what we're focused on in silicon humans are creative they're collaborative right humans can take some large under defined tasks like take a man to the moon and back write a very undefined undefined test and they could actually complete it and maybe with some probability maybe after a really long time and so you know the space of human executable tasks or the primitives is actually much larger it's really wrong to think about them as these small primitive units given sufficient time and motivation they can learn and innovate plan their way to making it happen now the outcomes that you know that you want to be measuring it's not just gonna be error quality typically what you know in the loop conne surcingle cost I guess costume matters but you know things like likelihood of success how long does it take and maybe the more annoying thing is that creating a model for this depends on so many human factors right like the ability to recruit a strong team cast a vision delegate work design effective communication and learning structures right these are all parts of the pieces that determine whether someone is able to take an undefined task and actually accomplish it so what would it look like to design a model of human computation that's not centered around composing simple computational primitives but it's centered around decomposing delegating and coordinating under to find complex goals yes yeah it is it is kind of like that yeah so how do you design a model of a company or organization or all the other things the teams market you know all those things that we use to organize human to carry out work because we need a model of that for to think about how a I can augment it that's one way of thinking about it so that's what I want to start working towards and a model of human competition that can reason about large under defined tasks that can capture critical human factors and that can explain some of the challenges that we see right now in complex crowd work so let's take a stab at this um first what we need is a big task ok there we go we have a big task and we need a worker at least one and we need to characterize these in some way right so the task has a size the worker has some available time and maybe some expertise now if it's just one worker then there's not that much to talk about because basically it boils down to does this worker have enough time to complete the task right if they have higher expertise they work faster so maybe it's just expertise times time is greater than size of the task right that's pretty much all there is to talk about and for most of this talk I'm going to assume baseline expertise just e equals one so one unit of time accomplishes one eating at work just simplify that so what happens if you want to think about many workers all contributing towards some big task um well we don't really have any structure in this task it's just some big task right so for now let's just think about these workers working sequentially on this task if someone works on it they hand it off someone else works on it etc well intuitively if the work is very independent then basically they're able to complete the task if the sum of their available times is at least the size of the task right if it's very independent but what if the work is very interdependent this is where it gets tricky how do we capture the challenge of interdependent work in a mathematical model let's start simple so you have at a suppose you have a task someone works on they take ten units of time to complete it let's say ten days okay now suppose that you have the same task and you have two homogeneous workers okay so in other words they have the same expertise like everything is equal about them in terms of their experience expertise whatever as that original individual so what happens if the first person stops halfway and passes the task to the second person how long will this second person take is it also you know is it no five has five days and the first is it five more days right well if you think about it in your experience it'll probably take longer right think about if you're working on a research project and you finish half of it and you pass it on to someone else exactly the same experience it's probably gonna take them a lot longer because you've built out a bunch of context from working on the problem right so a bunch of intuition a bunch of you know all those sorts of things the second person that comes in they need time to pick up that context right and the time they need will depend on how interdependent this task is so we're gonna capture this intuition by giving tasks and interdependency parameter this parameter is just a number between 0 & 1 it captures the extent to which context can be passed from one person to another so specifically it describes the amount of extra time that someone would need to spend due to lost context maybe like reading the documentation or you know whatever it is right so if gamma work has been done so far then a new worker that's starting at this point they would need to spend gamma times the extra time to pick up the context yeah if this is like one unit I'm at T equals zero versus like what you can tie my teeth of ten they are equally sorry second oh yeah you're saying that the expertise thing is that's a great point keep that thought we're gonna handle that later um so so so if you go back to our original example um the second person they would need to spend five times DV time to pick up context and then an additional five units of time to complete the rest of the work okay if D is zero that's basically saying it's completely independent there's no interdependence e if d is one this is saying that's impossible to pass on context basically the second worker has to start from scratch redo the work in order to build that intuition to kind of continue so I would say you know like a research is kind of like that so really intermittent eyes if someone leaves halfway kind of have to start over mostly at least it's close to one so this is essentially the model in the full model we have a task graph a directed acyclic task graph which allows you to represent the structure less units of work in relation to other units of work right so for example if you want to model and have different expertise for these different notes and just like there is an interdependence parameter for a node there's also entered dependency parameters for each edge and the edge ones are basically the same they describe the context that an individual needs to pick up from task you before they can work on task V yeah so right now the model of this task graph is actually this this represents the entire goal in terms of and then you assign people to different tasks like say like scooby our goals they're not connected directly yeah it's a graphic fully connected okay so when it's not connected I mean oh I see so you might we're both saying there are two tasks connected to J yeah still very depend on represents defining so we're independent each other given yeah so the reason why they are basically grass is we're kind of thinking about it also as like the order the test the work gets done so like you know you first have this past and then this open like this follow of this this all of this right so you would only depend on the stuff before now probably you know what you might want to say is like what the simplest Nonie's like the graph would be like two notes they're both dependent children right and you would say okay well if there's actually tasks that that had that property right both depend on each other right it's kind of what you're saying no no it doesn't by the tree yeah but I think you are getting at something which is like no for a lot of tasks we think of they both have context to each other but the way to think about it is like if those tasks are broken down to their like basic units then it would be usually the tasks we're referring to are actually collections attacked so so now we can if we have this model we can start asking questions like how scalable is complex crowd work that's a great abstract question so how we formalize this as we say is with this notion of work capacity so what work capacity is is is this okay and this is [Music] [Music] that kind the same interdependence structure as G but they have different sites that graph and you multiply the sizes by different sizes the same interdependent structure so so you can ask like if you have an infinite number of work of workers with let's say for simplicity we're saying they're homogeneous okay so we have the same expertise and time what is okay so s is the large Graff in this family that this set of workers can complete that is feasible so that's kind of like the capacity of that so that workers right and we can ask questions like if you had an infinite number of workers what's the capacity right I mean hopefully it would be infinite right infinite workers of course anything at work right and well if you think about it as long as the interdependence a parameter D is nonzero and you can show that the work capacity even for an infinite number workers is always bounded and so in this case this is saying okay homogenous workers they have the same expertise for all paths and then the work capacity is just going to be expertise times time times this thing which is basically the task granularity divided by D okay so this gamma G is just the relative size of the entire task graph to the largest subtest but to the largest subtest plus it's available time for one okay so it's basically depending on these four things right into dependency expertise time and task granularity yep that's good and it's interesting to think about this in the light of the trends in HDI in the design community and trying to get towards more complex tasks right so you know in the early days of crowdsourcing it was just like paralyzed Abul task everything was paralyzed well and then people started doing micro tasks workflows thing like okay and so we always like have very small tasks for people and orchestrate them together using workflows to get complex things essentially and then after that that started reaching a bottleneck where people are like okay these aren't able to solve harder things like building a website right um so people started to work on say okay well let's have workflows but each unit is a large macro task so it is the same thing and then well once we have macro testing we start saying okay well we can't use turquoise anymore we have to use expert workers like from freelance platforms this kind of a trend and you kind of see that reflected in in this equation right so um if you if you have these constraints then basically what you need to do is either make the task granularity really small it's also interesting to kind of reflect on different things like this is a quote related to social mobilisation so given all we have learned about social mobilization why isn't social media a more reliable channel for constructive social change social media has been much better at providing the fuel for unpredictable bursty mobilization than a steady thoughtful construction of sustainable social change so basically people are saying hey we've done all this work related to how to crowdsource things through social media like open call right spread through Twitter get a bunch of people to help undermine things and basically well the reason why we're not able to do steady thoughtful construction is sustainable social change is because sustainable social change is a large complex problem a very interdependent problem and when you're mobilizing people through social media the vast majority of participants are short term novices right there's going to be a fundamental bottleneck and that's serum I'm generalized if you vary the node expertise so instead of so these are still in my ecology workers but now they have different expertise for different subtasks then you get this equation and T is still the same T the time of people have s G is that's total size of the task graph so I call this a path and this term if you look at it then you realize that what this is is this is the maximum amount of time that any individual might have to spend absorbing content because this is the this is the amount of time that you have to spend absorbing context of the task you're dependent right so su is the size of that the prerequisite tasks time dependency divided by expertise neighbors and is your prereqs of papers yeah and then this is the amount of time you're assigned to the last delta sliver of your task but you please think we have to spend X V I'm taking in context write maximize this so this is essentially saying that really the bottleneck is this max time on context and that's encompassing all the other things so I think it's for me it's interesting to think about because it helps to think about well what other directions could we go in to continue increasing the complexity of tasks that we can support that people have or ease the chance for a context and actually if you think about micro rule hierarchies it's essentially doing that it's decreasing the time you need to pick up context a process related context right there's there's other results that you can use this model to to prove I haven't put it all in this in this presentation um but um you can think about you while they think about how do you recruit for complex CAD work you can derive a feasibility boundary like when is it when is a set of workers assigned to a to to note feasible and basically what it turns out to be is for each node if you look at the set of workers that have signs that don't remember they the carrying out tasks sequential okay so you can assign them non sequential a different set paths but for Givens at a statistical so if you look at the workers to sign into one sub task and you order them from the person with the the least amount of time least amount of expertise weighted time the highest turns out that's the optimum pace you should you should start with the prism of at least and most extreme person box and and the equation the brief is that you take the person with the most expertise waiting time and down and you wait this by a geometric distribution with parameter D and this is truncated so if you if you kind of average people's expertise weighted times based on a geometric distribution then if that value basically is greater than the the amount of context the maximum amount of context required for that node then it will be feasible and the closer it is to that limit the more workers you'll need so as you get close to that then you'll need infinite number workers if you get farther from that it will drop down and it turns out that um you can show that roughly the top this number workers approximate the class so this is a number of notes this is the harmonic mean of the interdependency parameters for all the nodes and this is natural log of epsilon so so if you have this number of workers essentially they approximating the work capacity so another if you give me any number of workers any assignment I can say I'm just gonna choose that number of workers from it from that set of workers for Jimmy and that will achieve a work capacity of 1 minus epsilon of the work capacity you initially had yeah you can also have theorems that kind of discuss the role of generalists where's a specialist and when should you recruit one or the other when you have you know introduced in your work or when you are adapting to different when you need to recruit a set of workers that can adapt to different tasks with different interdependence and structures you can kind of see that in in a formal way yeah yeah yeah yeah that's very interesting yeah very cool good observation so we've also you know so far what I've talked about it's still not really touching on the original goal which is the human capacity for taking a under defined task and kind of discovering structure delegating it coordinating it out right that's kind of what I originally started motivating from and we've started to think about this and and the way to think about this is just it's actually pretty straightforward to take these task graphs and to extend them to not just representing like the underlying true structure of this task graph but to also represent the partial knowledge someone might have about a particular task so maybe like some task has some true underlying task structure but what an individual sees of the task or knows at the task is is may be completely structuralist maybe they just see one node that comes through the whole thing or maybe they see a subset note so you can represent that by a partition and then there's a way that you can you can model you know if someone's working under partial knowledge then basically what they're doing is they have to pick up all the context for so they're assigned to a partition right so instead of being assigned to one subtest they're assigned to a collection of the past because they don't we can't distinguish between them right and when they're assigned to that collection they first pick up the context for all of those tasks right because they're working on it together and once they pick up the context for all this house then they can start working and the work they do because they can't distinguish between any of them is just proportional on all the tasks they work on you know all the tasks proportionally proportional to the remaining work that needs to be done so you can define a model related to like working under partial knowledge and you can also start thinking about like if someone how do you model someone discovering structure so probably if you have more expertise you're more likely to discover to kind of recognize a particular task right if I'm given something I can be like Oh like this mathematical side I can kind of break it down but the lawyer side of the legal side of this I have no idea I need to find someone whose expertise who expertise there to me it just looks like a big block right of work so you can start modeling these things and start discussing like how do different protocols for distributing work compare across across different people this stuff we're still working on and finalizing so I'm gonna wrap up by going back to this original question and kind of doing some more speculation and on how do we navigate in a whitewater world what I'd love to see is to redesign learning so it's more experience right where students are actually learning by working on real-world community projects I think that you know we've started that experiment in Cosway right mostly working with university students building nonprofit websites and I think it's possible not just at universities but potentially kind of going down into K through 12 maybe with simpler type of work but basically coupling learning with doing and thinking about how do you also incorporate that into work on the job learning and how to enable people to transition from industry to industry there's a lot of people who are discussing how we're gonna deal with all the job destruction due to AI and when the ideas is universal basic income actually is becoming you know more discussed in the presidential election and basically you know everyone should get paid enough to survive I think that maybe along with this idea or some combination of it um there should be some form of like maybe universal basic learning in the Great Depression the government what they would do is they would create these big government projects like building bridges or infrastructure right it's like Works Progress Administration basically a way to give people a job while there aren't weather me where the why the markets aren't doing well right and that doesn't work so well when things are changing right because if you give someone a job building a bridge after they come back they still haven't made the transition to the new the new types of work that are needed that are needed in the changes in the economy so I think it would be really interesting I mean this I don't know anything about macroeconomics at this level I just again the speculation dreaming um but you know what if it were possible to actually enable learners to contribute to real-world work they obviously won't be as efficient as private sector employees right they can't compete with a Google employee building website but they can still complete work so maybe right if we can figure it out and so maybe you can imagine something like the Works Progress Administration which was you know the stuff during the Great Depression but focused around building different types of government projects using new technologies right so a way that people can learn as they're contributing to government projects so they can get paid through the government at a lower rate than the private sector and but it can also help people transition I think it would be really interesting to think about whether volunteers could provide kind of operational capacity for nonprofits in a similar way that capital financial capital provides operational capacity for corporations if you see you know I think I resonate with this because when I was kind of explaining entrepreneurship and and you're thinking about the tension between monetization and mission and exploring different types of models for a social entrepreneurship and social enterprise that B corporations all these different types of things it's actually pretty hard for for someone to keep those two things aligned the mission and the monetization because the operational resources you need to run even a social organization especially focus organization is financial capital right so you still need money to run your organization and it is hard to to bring it together now if you know the problem with volunteers is that they don't they don't build on each other in the same way that financial capital right if you like a million people donate a dollar I have a million dollars yes right you'll take over the world but if a million volunteers give me a day's time that kind of doesn't add that's not equivalent to a million days right because of exactly what we talked about right interdependent work so so is there so if there was a way for volunteers to kind of as they're learning contribute or something like that then you might be able to create some kind of like market dynamics that are more mission aligned because you can also have like you know Justin just like in an entrepreneurship you just need convince one crazy investor so believe in your idea and and then you can kind of get enough operational capacity so you prove it out to the next set of investors right so maybe in this case right you just need to get the first set of out crazy volunteers so believe in you to prove out the idea enough to get the next set of crazy volunteers right and maybe that's more aligned because people who are volunteering will care more about the mission then the financial operations what in what sense oh I fall into your labor yeah yeah so that's interesting yeah that's an interesting observation I haven't really really thought about that I know that there are people who are working I know some people who are working on different types of things in prison population to provide educational like startup entrepreneurship type opportunities and that's a great observation yeah yeah we didn't get to really talk about polarization today I think that's a really important and challenging issue I think that two parts of of that issue is that you know most of us don't know very much about the issues different you know complex social issues because they're complex right I don't know almost anything about most issues and and most of us even though we'd like to think otherwise are great at letting other people know our opinions but not at listening to other people's opinions or incorporating them right um so I wonder if you know if you could actually get that lar doing by learning thing to work right where people could early on be learning in the context of being engaged in their local schools neighborhoods cities whatever right that might enable people to start learning about different issues and to start learning different civic skills like how to disagree with each other in a productive way and then kind of we ended with talking about how can we think about how a I can play AG menteng role in all of these different pieces can we get to some vision of human collaboration where you're blending design and models and data to think about how to support different types of complex human dynamics so that's all I have um initially Young had told me that you guys are working on projects and I if I wanted to I could propose some projects I just finished this talk literally right before so I didn't get to think about that but anyone's interested in in in talking about some project then email me and I can try to articulate something cool yeah thank you so much thank you so much yeah I can hang around we're gonna be here [Music] "
vDjVC5Dw1pY,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-08-29T16:31:38Z,"L01: Introduction (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/vDjVC5Dw1pY/hqdefault.jpg,Josh Bongard,PT42M34S,false,1104,9,0,0,0,okay let's let's get started welcome to CS 228 human-computer interaction if you're not signed up for 228 now is the time to leave no you're on the right place at the right time okay we have 42 students signed up for this class and this room says that it seats 42 people I see four empty seats up here at the front so maybe some people didn't make it today that's fine everybody's seen the screen okay okay awesome so what we're gonna do today is we're just basically to do a lot of housekeeping logistics what you can expect for me what you can expect from the TA what we're going to expect from you and if we have time we will get into the interesting part of the course as you know obviously we're here Monday Wednesday and Friday 10:50 to 11:40 somebody please remind me there's a clock in the back there once I get going I might forget please stop me five minutes early and each of you is going to get to take home one leap motion device today there's 42 of them sitting behind the screen here so I think with five minutes we should all be able to grab one and sign it out so somebody remind me we're going to stop at 11:35 today okay all right let's see other boring logistics stuff I'm Josh Palmer you can call me Josh that's fine my email is up there office hours are sort of right after this class my office is in Farrell Hall which is on the Trinity campus so if you have a short question we can walk and talk from here back towards my office if you have a longer question you might want to walk over to Farrell for noon to 1:00 and then I office also have office hours Tuesdays four to five is there anyone here who cannot make either of those office hours one two three four anyone who could not make either of those times okay I will put up my fault my total fall schedule so if you need me we might be able to find it time to meet outside of my office hours yeah okay obviously though first line of defense is the teaching assistant oh and Marshall is the TA for this class he's an undergraduate he took this class last year so he will know the pain that you will be going through there's Owens email and on blackboard I put up his office hours I don't remember them off the top of my head but he will hold office hours and he's gonna hold them in 332 which is the computer lab up on the third floor that worked pretty well last year that gets too noisy in there during his office hours we might find another time in another place we'll see how things things go okay and then again there's lab meetings available for you and Bodhi three sixty nine and I saw some computer labs down here how many of you have a laptop okay how many of you are going to be using lab computers which is fine if you do okay all right we'll talk about that okay so before we go on with logistics just to keep things interesting I want to try and do a live demo we'll see how well this goes the heart and soul of this course is going to be a series of programming projects that you're going to be doing with the leap motion device as I mentioned you'll be getting one at the end of the class today when you get this box and you take it out you will see something that looks like this you plug it into your computer and let's get this going here okay bear with me a second here okay so once you have your device here it sort of sits on your desk and you can then wave your hand above it and the leap motion device is meant to capture the three-dimensional position of every bone in your hand you can see it's a little jerky because my hand is shaking so I'm going to put this down on the desk and here we go okay it can capture zero hands one hand or two hands how the leap motion device works is basically there are two cameras in here you might not be able to see the two red lights from here these are just infrared LEDs so it's basically capturing reflected light and there's some very sophisticated machine learning that is going on inside the leap motion device which is taking an array of pixels which is reporting the amount of infrared light that's arriving at the camera and inferring as I mentioned the x y&z position of every bone in your hand that's what the leap motion device does for you you're going to be doing in this class a series of weekly programming projects and we'll talk in a little more detail about the first project today your task is to then take the three-dimensional positions of every bones in the hands and wrap that into an increasingly sophisticated program that you're going to be that you're going to be using to teach your roommate or your boyfriend or your girlfriend or a friend how to do how to learn American Sign Language so the leap motion device takes camera data and turns it into coordinates for you you are going to be writing some software that grabs all of those coordinates and tries to figure out is the person trying to sign 0 1 2 3 em and and so on and broadcast that information back to the ASL learner any questions so far ok that's the leap motion device let's get back to logistics here as I mentioned the blackboard blackboard is going to be our main point of contact for all material and deadlines and submissions and all that sort of thing I'm sure you're all familiar with blackboard you'll see that there is a I put up a syllabus for the course so let's just spend a couple minutes going through the syllabus so that we're all on the same page okay I think I've talked about all this so far there's ons there's Owens office hours Wednesday and Thursday 2:45 to 3:45 p.m. here's the little blurb for the course it's a little bit dated but it sort of gets to the point of things in this course we're going to be talking about the design of user interfaces how to implement them and how to evaluate that they're working well you're going to be creating some ASL educational software and all you're going to do with your user is put this on the desk in front of them and say nothing else and you're going to be assessed on how well your software teaches the person that they need to do this not this not this that if they do this that the saw this thing whatever it is is recognizing their hand a little bit later it's going to recognize that they are or are not signing the number 0 and so on how do you actually evaluate that that's working well your code might run perfectly fine with no bugs and nine out of the ten users who've never seen this before when you start up your software they look at it and they grab it like this your software works perfectly well but in terms of a user interface it's failed completely that's what we're going to spend most of the time talking about in this in this course ok so how do you design such an interface that's intuitive and so on you implemented how do you evaluate it with lots of different users and we're going to talk we're going to start by talking about traditional interfaces like browsers and so on and we'll get into increasingly complex and more exotic electronic equipment topics that will touch on our interface design human factors cognitive psychology so if you're trying to design an interface that's in a two intuitive for a person moment a person looks at the screen or looks at this they have a whole bunch of hardware up here and they have expectations about how they're expected to interact with the software or the hardware so we're going to spend quite a bit of time in this course talking about psychology right when someone looks out at the world what do they expect to see when somebody interacts with the world where do they expect to happen when they interact socially with someone else how do they expect that person to react all of those expectations about the physical world in the social world users bring it to bear on hardware and software often without them realizing that they are what are those expectations and how do we design interactive technology to make sure to support those expectations rather than frustrate okay so we'll talk a little bit about cognitive psychology I do research in robotics so I had to find a way to shoehorn robotics into this course somehow we'll get to robots a little bit later we'll talk about wearable technology augmented reality pokemon go Google glass we'll get to all of that fun stuff in a little bit again this course includes a significant project there's no midterm there's no final project you're going to be developing software for this and then the final project is going to be you using your ASL educational software to teach people ASL okay alright when you finish this course what should you be able to do you should understand how HCI is different from traditional software engineering how do we go about trying to design software that's not just bug free but software that people actually to use understand the challenges and implications of putting the person first so we're gonna spend a lot of time already as I mentioned thinking about how people think I've already told you that the leap motion device takes a camera image and turns it into the coordinates of the bones in your hand you are then going to be taking that and trying to write software that allows your computer to recognize did somebody sign one of the gestures in American Sign Language we need to think very carefully about how people might try and sign a sign from ASL who heard me give a pitch for this class in cs50 last year the year before okay a couple of you have heard this already so if you know the answer to this don't answer if you've heard this for the first time you can you can answer it's gonna be a lot of challenges and trying to put humans first when you develop this ASL software what might some of those challenges be aside from getting the code to run without bugs I think you were in my cs50 class right okay no cheating if you know the answer to this everyone put up their hand for a second raise it high so everyone in the class can see it okay now with your hand raised this might be hard for people in the front turn around and look at all the hands in this room okay you put your hand stop what can you tell me about what you just saw that's gonna make things tricky as we go all hands look different exactly did anyone notice how many lefties just put their hands up there's a few okay lefties please identify yourself if you're here we have one two three four five six lefties this year that's that's good okay so putting people first everybody's different right everybody's hand is different that's great when you start to write your machine learning code to try and recognize ASL it's not going to help you to say people's hands are different different how so we know that recognizing this isn't all we is going to work for someone who's gonna sign 0 like this right it's gonna be that's gonna that's gonna matter what else aside from righties and lefties is going to matter yes some people may not have a right or a left hat or both what else what else differed among the hands that you saw just like the different style absolutely right so if the if you write your code it flashes up a big digit 0 which means try and sign 0 someone does this someone does this someone does this those three different ways of signing 0 are very different in terms of the leap motion device some are easy to recognize some are not turns out actually this one is easy this one is very hard why now we're thinking not so much about the human but we thought about humans they might sign it at different orientation we want to work back from the human to the interface why might leap motion device have a harder time recognizing this than recognizing actually should be like this why is this why is this harder than this absolutely right so the three fingers are occluding the fact that my thumb and forefinger are trying to sign zero do you think a user who's never seen a leap motion device is going to know that absolutely not right so already when you start to think about your system you're going to have to signal or advertise to the user certain things right like don't grab the leap motion device don't put your hand on top of the leap motion device hover it somewhere about a foot above the device if you're going to sign things make sure that the leap motion device can see it right okay let's think a little bit more about trying to put people first so righties and lefties matter orientation is going to matter what else might matter johnathan with that hand size is going to matter it's going to matter particularly for this class one of the things we're going to talk about in HCI is some of the surprising repercussions of cultural and social context so think about this particular class that has to do with hand size what's different about this class than some of the other UVM classes you might be taking what's different about us yes you're all computer science majors yes what's difference between a lot of the CS classes you take and a lot of the non CS classes you take there's a lot of males right look at the gender ratio in this class what do you know about male and female hands generally speaking guys have bigger hands right so we are going to create we're going to create a few weeks a data set that is collected from all of you signing above your leap motion devices so in that combined data set we're going to have mostly right hand gestures we're gonna have mostly male sized hands machine learning algorithms tend not to do very well with the minority data left to its own devices most machine learning algorithms will say 80% is good enough I'll just ignore the 20% okay let's keep pushing on this for a moment I told you that the leap motion device has infrared cameras and it detects reflected infrared light what else might it have a harder or easier time recognizing we talked about righties and lefties males and females absolutely any jewelry that is reflective is it might be an issue what else skintone makes a difference as well I look around at your peers think about that as you're designing your your interface all of these things are the heart and soul or the black arts of HCI right in this course it's not so much about writing code that works correctly it's making sure that you write code that takes into account the diversity among your user group have you thought carefully about who's going to use it and have you come up with innovative hardware and/or software solutions to handle the diversity in your targeted demographic okay obviously you're going to gain a lot of practical experience designing these kinds of interfaces and implementing them and we're gonna spend quite a bit of time trying to think about the likely near-term future of interactive technology leap motion devices are here Google glass is kind of here drones are coming autonomous cars are coming augmented reality virtual reality these are all very different kinds of interactive systems hopefully the things that you learn in this class if you go out and work in this particular industry will help you think carefully about what happens when half the population is walking around and looking at the little camera in their their glasses it's gonna be an interesting it's going to be an interesting world okay course materials there's no required textbook for this class the first third of the class more or less I going to draw a few chapters from designing the user interface by Schneiderman a tile you can go and get a copy of the book if you like it's not in the UVM library I will put one on loan that's not in the bookstore I'll put one on loan in the library I haven't done that yet but I will put all of the the readings up in blackboard anyways some optional reading will be drawn from a book I wrote a few years ago with my colleague Rolle Pfeiffer again all of the readings will be posted directly to the schedule in the syllabus and you can find them find them there okay I also put up the lecture slides on blackboard please download either print them out and bring them in in dead tree form or electronically on your devices what you'll see as we go forward is a series of PowerPoint slides and from time to time there'll be a big red empty red rectangle that's a signal to you that you're supposed to annotate something inside that that rectangle I know how PowerPoint eyes can glaze over so I will try and not kill you with PowerPoint they're just there as a template for you to annotate as we go if you like to do that pen and paper that's fine if you want to do that electronically that's that's also fine but I'm expecting when you're in class you're annotating the slides as we go you won't have to do that today we'll start that on on Wednesday okay as you might have noticed I started taping the lecture at the beginning of class when class finishes and I get back to my office I'll push it to YouTube and I'll put a link to the video lecture into the schedule so if you miss class or there's stuff you want to go over again you can go and have a look at the video lecture slides the video lectures I taped the first eleven last year they're already up there in a YouTube playlist I'll create a new playlist for this semester and I'll fill in the videos as we as we go okay let's see there is an online multiple-choice test after each lecture it's due at 11:59 p.m. the day of so your first quiz is due tonight by 11:59 p.m. again when I get back to my office I'll make the quiz live on on blackboard shouldn't take you more than about two minutes if you came to class and did most of the reading it's just there as a prod to make sure that you keep up with the lectures and the reading material sound good okay let's see software as you probably figured out this is a pretty programming intensive course who has not programmed in Python before okay a couple people if you have not programmed in Python before I highly recommend code atomy it's a browser-based Python tutorial for those of you that haven't programmed the Python before get started on that immediately everything in this course is going to be done in Python so make sure you're you brushed off the rust flakes from your Python programming okay we are going to be making use of four Python packages in this course numpy is numerical Python which allows you to manipulate vectors and matrices we're going to be doing quite a bit with numpy matplotlib is a visualization package this is going to be used to project all the graphics to your user for your a ASL system scikit-learn is a machine learning package for python we're going to make use of that in a few weeks time and pygame is a way to sort of create fun graphics that you can add on to your ASL system as we as we go forgot to put up here there's actually a fifth Python package which is the Python SDK or software development kit for the leap motion device so in the first programming project which we're going to talk about shortly you're going to be using the leap python package and mat plot lip just those two this week okay grading participation is worth 5% the beginning of every class I'll pass around an attendance sheet so just give it your signature there pass it on to the person next to you if you come in late or you don't get the sheet for some reason make sure you come up to the front at end of class and add your name to it you can miss up to and including three classes if you find that you're missing more than three classes come and talk to me and let me know let me know why okay as I mentioned these very simple quizzes at the end of class we have about thirty classes they're worth 1% each every week you're going to be submitting one of these deliverables or these programming projects with the leap motion device each one is worth five percent so you can see where the bulk of your is going to come from these weekly programming projects are cumulative they build on the one that you did the week before so make sure not to fall behind in the programming projects if you don't finish one and you don't submit it obviously you don't get the 5% you're gonna have to finish it anyways the following week so that you can implement the next the next deliverable once the first ten weeks of the semester are out there have done your ten programming projects and you will have a functioning system that recognized that recognizes ASL signs from pretty much everyone here in the class because we're gonna use a machine learning algorithm to train on data from everybody's hand in this class that's what you'll have at the end of the ten weeks the remaining four weeks you're going to be turning that simple recognizer into a full-blown ASL educational system it's up to you how you want to do that we'll talk about that later any graduate students here know grad students this semester okay let's see student responsibilities late policies your doc 25% if you submit one day late 50% on the second day after that you don't need to submit you can cooperate and help each other but obviously you're gonna be working more or less alone on the ten programming projects and then on your educational software it's super easy to cheat on the ten programming projects you can probably get the code by now off the web somewhere or from someone else in the class if you do you're going to be completely lost in the last four weeks about how to turn that simple recognizer into an ASL educational system your choice okay we talked about participation let me know if you have any exceptional needs okay let's go to the schedule now so that's the syllabus which is up here on blackboard if you click on course materials you'll see at the moment there's a schedule up here we will start with this schedule at the beginning of every class let's wait for it to form this for okay so here's the schedule here's today here's a link to the PowerPoint slides for the first lecture which we're probably not going to get to today that'll be two on Wednesday that's fine there's the required reading for today the quiz for tonight will draw on what I've said today and from this first chapter some optional reading here bad designs it's great to have a look at bad design to get a good feel about how to do good design that's completely optional nothing on the quiz will come from the optional reading you can see that I've already put up the first deliverable the first deliverable will be due next Monday at 11:59 p.m. so you got a little over a week to do the first deliverable for those of you that are learning Python this is going to be a pretty busy week okay so we've got about 10 or 15 minutes left so I want to just walk through the first deliverable with you okay all of the deliverables as you'll see are a set of step-by-step instructions that guide you through the programming of the leap motion device what you're going to be doing in let's try and can we see this a little bit better what you're going to be doing in deliverable one is establishing a very basic feedback loop between the user and the leap motion device deliverables 2 through 10 are going to just be complicating this feedback loop so make sure in this first deliverable that you understand this feedback loop so that you can add stuff to it in the weeks to come okay so how does this work you turn on the leap motion device the user hovers their hand over the leap motion device as I've already told you the leap motion device device outputs three-dimensional coordinates of all the bones in the hand you're gonna throw away all the coordinates except the x y&z position of the tip of the index finger and that XY and z coordinate is going to move a black dot around the users screen so at the end of deliverable 1 when you run your code you should be able to hover your finger over the leap motion device and move a circle on the screen you see the dot moving and that influences how you move your finger right it's just that simple ok so how does this work well it's I've broken this into three parts 1 2 3 here what you're going to be doing is implementing these 3 parts in turn at the end of each part you're going to shoot a short five-second video you can just do it with your phone of your screen no points here for video quality you're just demonstrating to us that you were able to implement that part so you're shooting three five-second videos you're gonna push those three videos to YouTube stitch them together into a YouTube playlist and then when you submit next Monday all you do is submit a URL that points to the YouTube playlist the TA will just go and watch your 5 second videos yes you got part 1 part 2 and part 3 correct it's the same for all the deliverables you're not submitting any code you're not submitting any PDFs you're only submitting a URL that points to a YouTube playlist that demonstrates to us that you implemented this week's deliverable correct correctly sound good okay so step one is pretty straightforward here's my video this is just demonstrating that I've plugged in my leap motion device this program that's running here the visualizer it comes with the leap motion device so this video is just demonstrating that you're able to download and rodding leap motion fine on your machine thank you thirty seconds okay okay so there's no real coding in that one in step two you're going to be using the matplotlib Python library which is the visualization part so you're going to put your leap motion device to the side you're gonna write some matplotlib code in Python that just draws a black dot and moves it randomly around the window pretty straightforward so that'll just get you familiar with the visualization side of things and then in step three you're going to be connecting your leap motion software with your visualization software so now when you move the tip of your index finger it moves the black dot okay any questions so far pretty straightforward the biggest challenge this week for those of you that know python is going to be installation headaches right we have how many people are Mac people how many people are PC people how many people are Linux people okay so we've got an even split between Mac and PC a few Linux users you're gonna get a leap motion device some of them are this year's leap motion device as some of them are last year's leap motion devices we've got different operating systems luckily we all have one version of the leap motion SDK that you're going to download from their website so we're going to have lots of different combinations and permutations of hardware and software some of you installation will take 60 seconds unfortunately for some of you and I don't know which ones you're going to be installation is going to be more painful nothing I can do about that all I can do is urge you that when you get your leap motion device today make sure you do today's quiz and then start working on deliverable one once you get all the software installed things will go a lot smoother on deliverables 2 and onward ok any yes yes yes you might have to create a user account or it'll be briefer yeah just go ahead and create a user user account I think there's a download link there for Orion which is their attempt to integrate leap motion with oculus rift unfortunately I don't have an oculus rift for all of you we're just doing v2 which is version 2 of the leap SDK maybe next year see ok ok so let's go back to blackboard for a minute I walked you through the syllabus we had a look at the schedule you can see now that there's a link that's live for the first deliverable there's all the instructions for the deliverable when you've created your YouTube playlist come back to Blackboard click on deliverable 1 and you'll see here it says tech submission right submission just click on that and just type in or copy and paste your URL in there that's it any questions let's do next Monday at 11:59 p.m. we're all good ok we got about 10 minutes left so let's go back to the schedule for a moment ok I broken the course material up into five sections the first section obviously is kind of overview an introduction what actually is HCI and how does it differ from software engineering and all the other computer science things you've learned so far the second of the five themes we're going to tackle is design how do you actually sit down and think about design an ASL educational system that uses this device that most of your users have probably never seen before how do you actually go about designing and then implementing such a system September 19th and the 21st I will not be here Jeff Springer is a former student of this course and he is now the CEO of Zemo Zemo corporation they're developing robotic educational games they're going to be publicly launching their game on Steam this year so they're in sort of their alpha testing phase you are going to be some of the alpha testers so you're all going to get a copy of Zemo on the 19th and Jeff is going to be here to introduce you to Zemo and he's going to take you sort of behind the scenes of his company and sort of show you how they developed this educational game which is maybe more sophisticated but not that different from what you're going to be doing in this course okay all right the third of the five themes we're going to be tackling in this course is cognitive psychology right the minute someone sees a novel piece of software or a novel app or a novel app coupled with a novel piece of hardware they bring a lot of expectations to bear on that system they're going to expect it to have certain features if it's something that looks anything like a browser they're going to expect something like search up here something like a home icon up here and all sorts of other expectations where do those expectations come from it comes from our decades of experience of interacting with the physical world and interacting with the social world we'll talk a little bit about about that theme four of five I call looking outward most computer technology up until the last few years has been pretty standard right something projected on the screen you got a mouse and a keyboard and away you go right in the last few years computers have started to thread their way into the everyday fabric of our everyday lives most of you probably have one of these in your pocket or on your desk or in your your knapsack right computers are now out here in the world with us when they're out here in the world with us they are interacting with the world your computer is interacting with the world in a very different way than a traditional computer sitting on your your desk right how does a computer deal with the real world so we're going to talk about looking outward and all the different kinds of technologies that exist and those that are coming and the fifth and final segment I call looking inward so computers are coming out into the real world but they are also coming inward towards us so virtual reality reality augmented reality will talk about wearable technology elsewhere in the course and cyborg technology so cochlear implants that allow deaf the Deaf to hear retinal implants that allow the blind to see already exist some of you have seen 3d printed prosthetic arms right our computers are actually physically entering us and they are interacting with us in very intimate and unexpected ways right so think about human-computer interaction taken to the limit you have a device that actually becomes part of the human being and is interacting directly with your nervous system there's some interesting challenges there okay so that's the schedule and again as I mentioned we'll start with this at the beginning of every class and I'll upload slides as we go so here's the lecture slides for today which will just start on today finish tomorrow and lecture slides for Wednesday as well so on Wednesday I either bring these lecture slides in electronic format or print them out for annotation okay so we have five minutes left so let's start on lecture one here let's start with a little bit of a history lesson here I it's a potted history of computers I picked a few computers here which computers do you recognize here iPads iPads pretty obvious about the gray one in the top center there someone's nodding their head back there what is it dial ah it did have a dial-up modem that's the Commodore 64 that was my first computer I'm old but I'm not that old that's the ENIAC back there one of the very first computers we're not going to do a history of computers what I want you to pay attention to here is think about all the different the ways in which people interact with these computers have changed over the years right we go all the way back to the Eenie act the way of interacting with the eating act was plugging in and out plugs or feeding in a deck of cards it didn't take too long for the mouse and keyboard to arrive which we still more or less have then we had to touch pad now we have the touch screen the way in which we interact with computers has changed greatly over the history of computers and again that rate of change is increasing right the diversity of computer technology that's out there is increasing so there's our leap motion device there's Google glass which was made public for for use about two years ago so anyone's seen anyone walking around with a Google glass a couple people but not many probably why not anybody know what happened to the Google glass in the last two years or so why did they scrap the google says it's on hold indefinitely so they haven't scrapped it anybody know why why would you possibly scrap such a world-changing technology massive privacy concerns there was the beginnings of a huge amount of legislation legislative action Google said we're way let us rethink this a little bit right there are huge political and cultural implications to HCI right again something to think about here's a cochlear implant which we'll talk about later here's a 3d prosthetic arm this is where we're heading and we've already seen how you have to think kind of carefully about who is going to be using this and when you start thinking about who is going to be using this it changes your thinking about how to write code for this right you may or may not be working in this industry you may or may not be working directly on the hardware you probably will be writing software for pretty exotic computer technology in the not-too-distant future again my hope is that you take away a lot of lessons from this course that make sure that the software you write for this HCI interactive technology is inclusive of everyone who wants to use it and not exclusive okay I think this is a good place to pause come and collect a leap motion device you'll notice there's a number on the box sign your name next to the box you've got a quiz due tonight and I'll see you on Wednesday 
U8CXBn5ZwVo,22,"HCI is one of the most important subject of ATHE Level 4 & 5 in Computing. Human-computer interaction (HCI) is a multidisciplinary subject that focuses on computer design and user experience. It brings together expertise from computer science, cognitive psychology, behavioural science, and design to understand and facilitate better interactions between users and machines",2021-04-18T19:20:01Z,Human Computer Interaction ATHE Level 4 & 5 Computing,https://i.ytimg.com/vi/U8CXBn5ZwVo/hqdefault.jpg,Horizon Navigators Institute of Technology,PT34M,false,24,4,0,0,0,[Music] okay so we talked about user okay we must keep in mind the uh identity of user from which we part of the world the user belongs okay this is very important second is about computer it sounds so easy like what is computer we use it daily what's important to read about it but you must we must know the techniques and the science behind this interaction okay these are two complete entities and there is a strong relationship between human and computer and psychologically it plays very important role okay if your laptop is not working you cannot work properly okay yeah if your laptop is not speedy enough it's not it's do not have good speed you cannot work properly on it is it so yes so it means that it means you have a relationship with your system that affects your mood it can affect your sleep okay so it's actually a very vital part of our life now so when we talk about the computers we are referring to any technology ranging from desktop computers to large-scale computer system for example if we are discussing the design of website then the website itself would be referred to as the computer okay computer can be any application any technology use okay devices such as mobile phone or vcr can also be considered as computer so computer is anything any digital technology we are using any digital technology okay your vcr your mobile phone the websites google everything we will call it computer so when we say we are discussing human computer interaction we are actually interacting with the new technologies and what is the interaction there is obvious differences between human and machine definitely there is obvious difference there is a language barrier there must be a translator okay between them between two entities human and computer in spite of these hci that means human computer interaction attempts to ensure that both get on each get on with each other and interact successfully okay human computer interaction hci is responsible for the smooth interaction between human and computers by developing strategies in order to achieve a usable system you need to apply what you know about humans and computers and consult with likely users throughout the design process okay in real system the schedule and the budget are important and it is vital to find a balance between what would be ideal for the user and what is feasible in reality okay in whenever we want to design anything anything so we make its feasibility report in in the feasibility report we see what are the ideal condition how good we can make create the system but keeping ourselves in a limited budget so human so if you want to develop a website a website must be good enough but it must be within the budget there is no use of making a very good website which is the best among all but it's so costly that no one is going to buy it okay same like it is no use to make uh to design a very very very amazing mobile phone which has very high speed but no one in the world is able to buy it so so we must see the market that are if we develop an hci human computer interaction or any app or any um social media platform or any website people must be able to use it easily that will be our success okay for example if a book is so good it's so well written but it's not very cheap and it's not available to all the public it's of no use that's why pirated editions of the different books are made so that people can buy it and people can read it that that is as successful kpi of a success of any book okay what are the goal of sga okay what by human computer interaction if i want to develop a system in which by which a human and computer can interact what are the goals what can i achieve understand the factors that determine how people use technology okay this is important i must understand that how people use technology old age people use percy or perceive technology differently young people perceive technology differently and the babies they are very fast learned they perceive technology differently i must i i must consider these things okay second develop tools and techniques to enable building suitable systems okay my s whatever techniques i am going to design my hci that must be building suitable system for building suitable system means they must be appropriate and easy to use for all the public and they must be in budget achieve efficient effective and safe interaction okay but what i want i want to design a hci which can develop efficient effective and safe interaction between computers whenever i want to give any instruction to computer it can easy my system can easily decode it and my system easily understands what i want to say for example if your mobile if you're using any mobile phone and your touch got and or you if you drop your mobile phone and you crack your screen what happened touch is not enabled if you just touch the home screen it starts the back back the screen and you get nothing so on your mobile is iphone 12 x pro okay what will you do it what will you do from it are you going to use it without repairing it no why needs to be why that's iphone 12x pro that's the best phone in the world why you're not going to use it maybe because it's expensive okay it is expensive but it's not efficient okay it's not effective so my system must be efficient and effective you're getting my point yes yes okay the fourth one is put people first you must know what people want okay you must know what people want from your system okay underlying the whole theme of hci is the belief that people using a computer system should come first computer system is not using people people are using computer system so computer system must be modified according to people people must not be modified according to computer system okay you cannot say okay i have made the system this is very difficult so what people will learn or people have to learn no you must modify your system according to your audience okay their needs capabilities and preferences for conducting various tasks should direct developers in the way that the design that the design systems people should not have to change the way they use a system in order to fit in with it instead the system should be designed to match their requirements okay this is a goal of hci okay what is usability usability is one of the key called key concept of hci it is concerned with making system easy to learn and use a usable system is easy to learn it should you you should easy easily learn it okay the uh company if a company buys your app any application or any website your website should be easy to use they should not hire a professional to get its staff trained to use your website no you see facebook facebook is very easy to use that's why it has more much larger amount of audience in contrast twitter is not very easy to use that is why it is comparative it comparatively have less audience than facebook second easy to remember how to use okay you must not forget the steps they should have small number of stuff okay less number of step steps to post anything to design anything effective to use efficient to use your system must be effective and efficient safe to use okay your privacy must not be breached enjoyable to use you must enjoy how you are using it okay it it must not be a trouble for use whenever you think about a website and you say oh no that is very difficult if i have to use it i must be like mentally prepared because it is so difficult to use and i must have to remember the steps no you must not for example um a normal person logs in facebook about 11 times a day okay 11 times a day a normal person logs into facebook so facebook is very easy to use every person knows how to share how to write a post all the features are on your home screen okay within attach you can do anything that is very easy to use second it is efficient and effective it's it's it does the task very easily very fastly okay it's easy to learn you can see many illiterate people who doesn't even know how to write are using facebook very easily even the old age people are using facebook so it is easy to learn as well okay actually these are the components of hci usability acceptable accessibility and adoptability we learn about usability and now what is accept accessibility assess means to reach anything okay people will be excluded from assesses interactive system for a number of reasons okay why people cannot access our system why is this feature important there are some constraints physical conceptual economical cultural and social how see them these are the physical constraints you can see in the picture a person is on wheelchair and his hand cannot reach the atm machine or this machine so this is his physical constraint he cannot reach there then how can we use it the designer or system must keep in mind that a dis that disable or specially able person also have to use this machine so this this part of machine should be of such length that a normal or specially abled people all can use it okay inappropriate sitting of equipment input and output devices making excessive demands in their abilities for example it can be difficult to use it are so many information that a person with mental disability cannot go through it for example an atm may be positioned too high for a person on a wheelchair to reach a mouse maybe too big for a child's hand or mobile may be too fiddly for someone with arthritis to use okay these are physical constraints i use a designer must be keep these things in mind second is conceptually for example a person cannot understand the instruction what does it want to say whenever you go to atm it gives simple instructions but at some in some other atm the instructions are very difficult to handle so you avoid going if you say no i i will go to some other atm which is more easy to use okay obscure commands and cannot form a clear mental model of the system sometimes we may get confused in different websites you know somewhere else are very confusing so we avoid using them for example i never understand pinterest whenever i get any link related to pinterest or i suddenly turn it off because i know i cannot understand it that is very difficult to use okay otherwise economically okay you have designed something and that is so costly that people cannot buy it what is the use of making or designing such high price you know gadget there's no use so this is economical constraint third is cultural exclusion as we as i told you earlier designer making inappropriate assumption about how people work and organize their lives for example a interface or any app you're using in america you cannot use the same in uae or you cannot use the same in africa for example using a metaphor based on american football would exclude those who do not understand the game exactly okay yes okay social exclusion what is are the social equipment is unavailable at an appropriate time and place yes equipment is equipment has some time bounds it's not a it's for example it is designed designed by american time and in america from morning nine to evening five it's available at the same time in other countries that time is maybe of midnight so this is social exclusion people cannot interact with each other people are not member of particular social group and cannot understand particular social modes or messages okay i'll give you example you see memes on facebook okay yes yes okay if you ever had any experience here that you showed these memes to an older person they do not get it okay before facebook we do we we didn't know that such meme exists okay so if someone shows us before that we say okay we don't know what is it what is it this is this is just picture or some random words we do not understand it that is actually the social gap now we have a specific state of mind that that is why we are able to understand those means otherwise if if a person is unaware of all these things he is unable to understand all these okay approach to designing for accessibility okay design must be accessible for everyone design must be simple and easy to use you see big brands they do not use you know different groups of colors facebook is facebook just have two colors blue and white you see and many big bridges for example what's up what's up what's up is what's up earns a lot of money by used by this simple interactive app it just have two simple colors and very easy to use so the design is very easy in facebook as well the design and the all the graphics are very easy overcoming these barriers to access is the key design to consideration two main approaches what are the two main approaches designed for one there must be universal design it it should not happen that when if you are using facebook in ksa the um interface is different if you are using in pakistan the interface is different no because if you travel from one place to another what will you do are you going to learn how to use that app every time when you travel no it should not okay the design for and the design must be inclusive we are going to discuss more about design okay the design must must this okay we are going to learn about varying ability varying ability is the ability to change it is not a special condition of few but a common characteristic of being human and we change physically and intellectually through our lives if a design works well for people with disability it works better for everyone the design must be so that disabled people also can use it at any point in our life personal self-esteem identity and when well-being are deeply affected by our ability to function in our physical surrounding with a sense of comfort independence and control usability and aesthetics are mutually compatible i'll give you example okay we see i i see a person who don't have legs and is on wheelchair and up another person who is completely fit okay so they have to reach on a roof it it's clear that a person the person who is fit he can go to the roof by using the stairs or the ladder and the other the other person is disabled but let's take an example what if i remove the stair or the ladder there is no way to reach on roof now both are disabled although one have legs and one do not but both are disabled why because the mean the transport is gone so disability is just a condition where a person is unable to do a task it most of the time doesn't have to do with physical disabilities if we pro if we provide enough resources a physical disabled man can do exactly the same output as an able man so disability is nothing my design must be so that a person with disabilities can use it if a person can with disabilities can use it then obviously an enable person can use it as well because they have more stamina and other things okay what are the factor of hci there's a large number of factors which should be considered in the analysis and design of a system using hci principle okay in najwa i want you to read all these please done okay let's come to environmental factor okay let me give you an example i am designing a mobile phone okay i am but i for its body i am using a metal which cannot tolerate heat okay which cannot tolerate about for example 40 degree centigrade okay i have designed the mobile phone and i send it to for example dubai okay you know the temperature in dubai is very high most of the time so what will what will happen maybe maybe the my system will not work good there may be my the body of my my mobile phone get d-shaped so this is what i ignored the environmental factor while manufacturing i i face the consequences after its making and during its usability okay same as let's take the same example i designed a mobile phone okay and i have used some uh like um material in it which gets sparked when the system is heated okay so for it it is it is not a safety measure it is not a safety me i have used any metal which can which gets so hot when it's when it is opened or exposed to heat so it may can burn hands so what is that i did not take care of environmental factors so these there are environmental humans for example if you have designed an atm machine and if it is exposed to rain or lighting it get blast or it get you know it stops it is of no use because an atm machine is placed outside it must have to go through different seasons you getting this yes so these are the environmental factors okay moving to the next slide okay what are the principle of universal design okay what is what a universal design must have a universal design is an optimal design because it's it it an optimal is like it's a best best position an optimal position is the best position best for both the computer and the user budget friendly as well so it must be equitable the design does not does not disadvantage or stigmatize any group of users okay all group of users can use it with equal efficiency flexibility in use the design accommodate a wide range of individual preferences and ability it must be flexible to use like if a feature is not good enough it must be so flexible that the designer can change it or modify it simple intuitive use use the design use of the design is easy to understand regardless of the user experience knowledge language skills or current concentration level okay the design must be easy to understand for example when the mobile phone was invented so it it was kept in mind that old older people are going to use it as well and young people and the kids so you must take care of all the mental level of people language skills okay precipitable information the design communication necessary information effective effectively to the user regardless of ambient condition or the user sensory ability okay the information must be delivered delivered easily to all the users okay i have once uh i have read about a person who was blind who was totally blind and he first introduced the app which can read like if you put your mobile phone near any text the system will automatically detect the text and read it for the person so it was very useful for the person who who are blind it and it is now used in most of the mobile phones and it that app app is also available on play store as well okay okay okay tolerance for error the designer minimize hazards and the adverse consequences of accidental unintended actions okay the your device must be able or your app or any computer i will use the word computer from onwards for websites application or any technologies okay so your computer must be tolerant for error it it can it can handle the error if any error comes your system must be able to copy with it no physical effort okay the physical effort should be low you know the first computer was introduced by charles barbage and after that the first computational computer that was manual like if you had to perform any task you or the a person had to get up from its seats go to the back of the system and put on the switches and then came back and do all the procedures and if he has to done another task he again go back to the main system and change the switches plug in and plug out and came back and do all the calculations this was done in early times with the computer and now we don't have to use it because it it was very much physical effort and now we have very low physical effort okay size and space for approach engineers yes this is very important your system your your computer must be size friendly space friendly and user friendly for example i want if you want to buy why okay tell me why the tabs are not as much pop tablets are not as much popular as mobile phones a major reason okay you cannot easily handle them everywhere you can you you can grab your mobile phone in your hand and go anywhere but you cannot grab a macbook or any tablet and you you can run freely it is very difficult to even hear a call and listen up all on a move you know then you will have to connect the headphones and it's like a long process yeah it's it's a very long process appropriate size and space are provided for approach reach manipulation and use regardless of the user body size posture and mobility you must have seen desktop computers yes yes they are they have big size and they are very difficult to move that is why they are not very preferred people usually prefer to use laptops anywhere except the computational work they are not very friendly with the computational one so the size of the system matters a lot okay ensuring an accessible system okay the system must be accessible to everyone and a designer have to ensure it as a way of ensuring an accessible system the designer should include people with special needs in requirements analysis and testing of existing system as we discussed before my system must be accessible to all the users like i show you here yes okay this was not accessible for everyone so my system must be accessible to everyone consider whether new features affect users with special needs positively or negatively and note this in specifications okay you must know if you are adding any new feature in your system either it's effect it will definitely affect people the designer or a team of designers behind the computer must know how that change is affecting people and is it affecting people positively or negatively their feedback is important okay if the feedback is negative the system must be flexible enough to change it default take an example of what's up privacy policy we had heard in previous days so everyone was like no we are not going to we will stop using whatsapp and they found some alternatives of another of other apps who will not use their data what whatsapp up showed must much flexibility and delayed the process so this is how the feedback of the people works and pressurized on different systems of different computers take account of guideline including evaluation against guidelines okay every system your computer must contains guideline okay guy how to use it include special need user in usability testing and beta test okay we will learn about beta test but during the testing your system must test the usability of special people people with different disabilities people with you know speech disability physical disability or any other disabilities my sister they do this for systems yes nowadays sorry they do this nowadays like for any system they're making yes yes it it is a very important part of any procedure okay let me let let me tell you uh an example it was i heard it when i was in masters so you know the inventor of iphone he was when he was student he used to learn calligraphy okay calligraphy is is a is a piece of art okay you write different verses of poems in a very beautiful pattern so he learned perfection in calligraphy calligraphy is a very is a task of perfection so when the first iphone was brown was brought to him that this phone this is the first model of the phone we had made that person had a glass of water he put the phone in a glass of water and few bubbles came out so he said he said this is not perfectly integrated as you you see there are some air spaces in it make it so compact that there is not even a single air air space in the system in this mobile and now um apple the iphone from apple is one of the mobile phone which do not have air spaces in them i don't know about the recent designs but it was it was true for the previous models that it they do not have airspaces in them and that person said that there that i have learned such perfection from calligraphy okay this is these are the example of testing when this is mobile systems are used are designed they have gone they are put in heat ovens and cold temperature zero degree temperature below zero zero degree time temperature all these testing are done with all the mobile phones and all the gadgets all the computers why because they have to sustain in very difficult environment they they are going to be sent in different part of the world okay so this is actually a testing techniques in different processes so now coming back to the lecture we have read about hci it's we have read about hci okay it's definition it's part there are three parts of hci what are the goals of its you have what i want to achieve i want to develop a system which is safe and easily efficient okay then we have read about the components of hci there are three components visibility accessibility and adaptability we have read about usability it must be easy to use friendly to use for example for like whatsapp it must be accessible to everyone people with abilities with disabilities needs special needs it must be available to all the people belong to different culture different languages different okay so culture or such barriers should not limit the use of technology okay then we see different examples of physical constraints or physical disability than the conceptual disability okay conceptual disability differs from place to place okay it has been proven that people belong to different parts of the world have different mental caliber economically it should not be very costly because if it doesn't matter how good a system is if no one is going to buy it there must be we must take care of business as well then we read about cultural exclusion that the designer making different part different computers must take care of the cultural gap or cultural constraints social exclusion it must be like people of every group and every age can use it okay what was the approach we read about the approach to designing for accessibility a system can be accessible and good enough if it is universal and the people from every part of the world can use it effectively we must design a site design for all because if a disabled person can use it a normal person can use it as well okay we read about the factors which can affect hci your organization factor environmental factor user factor comfort user interface task factors constraints system functionalities and productivity factors then we read about the principle what are the principle of universal design on which points the universal design must consider okay these are the universal designs seven universal designs then how can we ensure that our system is accessible so we see there are three points four points we must take take care of and if if these four points are good then our system is accessible and easy to use for everyone okay i have shared the slides with you in the easy post the this course and the the course introduction to technology these are very easy course most theory based okay you just have to read it carefully and there's no need you know to get it memorized this is very easy you just thank you so much okay you can leave the class laughs 
VfnIgZsnzAQ,28,Playlist: https://www.youtube.com/playlist?list=PLAuiGdPEdw0j6VNxfbY-FNlbAjlWIVNnO,2020-10-29T15:56:11Z,"Human Computer Interaction, Lecture 16. University of Vermont, Oct 29, 2020.",https://i.ytimg.com/vi/VfnIgZsnzAQ/hqdefault.jpg,Josh Bongard,PT1H15M29S,false,82,1,0,0,0,okay good morning everyone i hope you're uh doing well um not too much to talk about in terms of deliverables uh you're all working on deliverable eight which is due this coming uh monday november the 2nd uh in deliverable 8 you're going to be putting your k n on pause for this week and and hacking together a database which as i mentioned last time is not the most elegant way to integrate a database into javascript but it's good enough for our purposes in this class any questions about deliverable 8 okay my apologies about the video from uh from tuesday there was some copyrighted material in there i fixed that now so you should have access to the video lecture from from last time okay so um we are moving slowly into the second last theme of the course this idea of looking outward trying to create interactive systems that are interacting more or less direct directly with the physical world and their technologies that are being stitched into the physical world and interacting with it alongside humans there are lots of different ways we can interpret this outward spread of interactive technology we looked at one dimension of spread last time which is social spreading so creating technology that is used by many people simultaneously and specifically in the case of crowdsourcing systems trying to allow people to work together to solve a common problem that would be difficult or impossible for one or a few people to solve on their own we're going to dive back into lecture 15 in a moment which is a different approach to to looking outward which is looking at touch we've spent a lot of time looking at talking about visual and some discussion about auditory feedback and interaction if technologies are going to be deployed out into the world they are able to feel the world and supply feeling literal feeling on on our skin back to us which is a different branch of human computer interaction known as tangible computing so this is the sense in which the thing that we're interacting with is physical in an important way of course your laptop and your cell phone and your desktop are also physical objects but their physicality has relatively little to do with how you interact with them intangible computing we're paying very close attention to physical forces that are being passed back and forth between the human user and the interactive technology we'll uh we'll probably finish 15 today it's a pretty short lecture and move on to lecture 16 where we're thinking uh we're going to think a little theoretically about this concept of ubiquitous computing so ubiquity means basically everywhere so obviously it's a very exciting time for hci in particular computer science in general our global society is in the process of basically digitizing or computerizing everything on the planet and connecting everything together into one large internet first of all and now the internet of things what is the internet of things going to look like what do we want the internet of things to look like and what should it not look like in lectures 17 18 and 19 which we'll get to next week we're going to look at the application of ubiquitous computing in in scientific endeavor so we can also view ubiquitous technology as a new kind of scientific instrument a new microscope a new telescope a new tool that allows us to pose new scientific questions and test those questions using ubiquitous technology in ways that have been difficult or impossible to do before okay the week after that we will probably start to talk about robots which are self-moving interactive technologies they're able to move themselves and literally and figuratively move themselves out into the physical world okay so that's a little bit about where we're going back to tangible computing which we started in on last time we looked at we looked at the moving one type of moving pins display where touch is the sensor modality in which information is being communicated back to the user most of the information you get from your devices is visually from a screen or auditorially from ringers and buzzers and and so on so i won't play this video this was the video that's copyrighted and triggered the copyright infringement in uh in youtube but what you saw you if you saw the video last time uh you saw some of the ideas about how this application might be used what are some applications that could be useful where the computer is raising and lowering these pins and constructing uh 3d patterns like the one that you see here that are changing over time and a user can either feel those pins or like you see with the red ball there place objects and interact with this moving pin display what might be what might be some interesting applications if you take this course a couple years from now and we use the moving pins display rather than the leap motion device what might you want to program it to do give you a minute to think about it if you have some ideas go ahead and type it into chat or feel free to unmute yourself and describe your idea verbally one of the killer apps that the demonstrator showed in the video was allowing their phone to be moved by the moving pins to them when the phone was ringing i don't know if that's exactly a useful use of this technology but who knows this is often true of very new kinds of interactive systems it's not always clear what the what the killer app or the usefulness of that technology would be any ideas okay i think on that um if you have an idea go ahead and type in a chat but we will carry on i'm going to show you another tangible interface now this is ultra haptics i'm going to play an older video this was the prototype when this was being developed uh when this was being developed this is now being sold as a commercial product so if you google ultra haptics you can see a more uh more recent version of this technology but those more recent videos kind of gloss over the technical details we're going to watch this video it's going to go into the the technical details of this and what i want you to do while you're watching this video is to remember our discussion about the actual hci loop the output of the user the movement of the user becomes input to the interactive technology it works on that that input for a while and then it provides some output back to the user which become which is sensed somehow by the user if you remember all the way back to the very beginning of the course we listed all of the quote-unquote input and output devices for a human what are the various sense organs or sensor modalities sense modalities that humans have available to them what technologies project onto those various sense modalities and alternatively what are the standard input and output devices for uh computers but that list of input and output devices is growing and you're going to see some additions to that list in this video so one way to do this is to write uh write computer or write ultra haptics and human and then input an output for both of them and write that down some of these so what is being projected from who from the human to the technology from the technology back and in what way what physical phenomenon is being used they allow users to interactive surfaces are now common in everyday life they allow users to walk up and use them with no instruction however current methods for providing tactile feedback require the user to cover up the visual content by touching the display or attach devices to their hands we present ultra haptics a system that provides mid-air haptic feedback and requires no contact with either tools attachments or the surface itself ultra haptics uses a phased array of ultrasound transducers to create tactile focal points in mid-air the array is driven by a stack of five driver boards which receive emission patterns from a pc the user's hands are tracked by a leap motion controller and the haptic feedback is projected through an acoustically transparent display directly onto the user's bare hands there are four steps to our unique focusing method first we define a large volume around the transducer array within which we will model the ultrasound field then we position positive control points where we want to form focal points these tell the system to generate the highest intensity ultrasound possible at these locations they are then surrounded with null control points these have the opposite effect telling the system to generate the lowest intensity ultrasound at these locations finally the phase delay and amplitude are calculated for each transducer in the array to create an acoustic field that matches the control point this simulation illustrates the acoustic field as we move up from the transducer array color represents phase and brightness represents intensity at a height of 20 centimeters a focal point is formed above the ultrasound defocuses once more similarly this simulation shows five discrete focal points being formed at the same time by varying the tactile properties of focal points such as the frequency they can be made to feel different from each other in this scenario a tactile information layer is created above the display by moving their hand over the map the user can feel the population density of a city the frequency of a focal point represents the density in that area focal points are created above elements of a music player interface this allows a user to locate themselves on the interface without looking tapping the focal point above the button starts and stops the music the focal point above the volume slider can be grabbed at this point it pulses to inform the user that the system has recognized their grasp focal point can then be slid up and down to change the volume these are just a few of the applications that become possible with the ultra haptic system okay so um the idea here is to create this illusion is to create the illusion of the fact that the user is grasping an object hovering in three-dimensional space over this uh over over this system how is that illusion created so as the name implies ultra haptics they they're making heavy use of ultrasound here so they have a bunch of these ultrasound emitters which are built into this base plate and these ultrasound emitters send pressure waves into the air above them and by setting the relative phase offset and amplitude of the signals being sent out as those signals start to move as those pressure waves move upward from the base plate those pressure waves interact with one another and at certain points in space and time above the plate there are instances of positive interference and negative interference so these are pressure waves if you think about actual water waves water waves in the ocean if you have two waves that are coming together depending on how they collide with one another if they're in phase they're both rocking together when they collide they will produce a wave that is much taller than anyone than either wave alone that's positive interference if the crest of one wave hits the trough of another wave they cancel each other out and that's negative interference so the control points that they mention in this video represent points in space and time when there is positive interference waves come together and create a more intense or higher amplitude wave that is actually felt as vibration on the pads of of the fingers which is not quite the same as the amount of pressure that would be applied to the pad of a finger if you grasp a button but it's close for people that have used this device they report it feels like they're actually grasping some type of physical object in the world okay how is the hand recognized how is how is input provided from the human to the device exactly our old friend the leap motion device so uh so hand position and orientation is being provided by leap motion depending on where the hand is for example if the hand is in the posture of grasping a knob and that and that grasp event is where about where the knob should be they want to give a signal back to the user that they've actually managed to successfully grab this quote unquote not volume knob even though that knob isn't there so that information is sent into the ultrasonic array and the amplitude and phase offset of the transducers are changed a little bit to increase momentarily the positive interference at that point which from the user's point of view feels like the knob giving a little bit of a vibration the knob is quote unquote telling the user i know that i've been grasped by you okay kind of an interesting uh technology again seems to be somewhat practical uh it is it is on the market at the moment okay so why tangible user interfaces or tui's you know you're all very familiar with gui's which project to the visual sense why would we want to manipulate objects and get tactile information back the largest sense organ that you have is your skin and your skin gives you huge amounts of information about the world around you and especially about the objects that you are in physical contact with right when you grasp an object rather than passively looking at it from a distance you can learn much more about that object than you can just by looking at it obviously if you grasp it with enough force you can tell whether it's a hard or soft object what else can you learn about an object through physical manipulation that is difficult or impossible to do just by passively looking at that object imagine the ultras uh the ultra haptic system is projecting the illusion of a particular object into space in front of you the user can't see it but they can reach into the ultrasonic field and feel and manipulate that object what might they learn about that object what might they be able to learn about that object by manipulating it most healthy uh young human infants if if objects are placed nearby infants will not simply sit there passively and look at those objects they will reach out and grab those objects and if you've ever watched a young child interact with objects nearby you can get a pretty strong sense of what they're trying to learn about that object right so obviously petting an object or a pet tells you as sarah says about the material properties of it or as matthew mentions the texture difficult to often tell the material or the smoothness or the outward surface of an object just by looking at it obviously much better to manipulate it what else can you learn about an object thinking is this is another instance of how thinking about thinking is misleading as adults most of our time is spent passively observing objects at a distance text video listening to your instructor's voice at a distance but young children are not as visual or auditory inclined they learn about their world through touch touch provides rich experiences shape obviously yeah we are out of practice because we do not physically manipulate objects to learn about them very much anymore we did when we were young how else did you learn about physics the the everyday behavior of objects by physically manipulating them you can learn whether they're hard they're soft we can learn something about their material properties shape is another dropping them as bryce says is an action exactly what action what mysteries of objects are revealed by dropping them what do you learn by dropping an object weight yep absolutely lift just grasping it itself tells you something about the weight or the heft it's fragility can it be broken or not how things move based on their material we can figure out relationships between material weight uh hardness softness shape by manipulating them we often learn their relationships there yep huge huge amounts of information to be learned whether it separates from other objects so if i pick up a seeming object as one piece come away and the other piece stay behind what is the connectivity of objects young children as they manipulate objects they are also typically observing their hand and the object they're manipulating and so they're learning not only relationships between the physical properties through uh tactile interaction but also relationships between the visual and the tactile relationships so uh something that is separable if i pick it up one piece comes with me and the other stays behind what did it look like before i physically separated it what are the visual clues that this object is not just a single object but multiple objects okay we bring our spatial intuitions to bear on objects that we that we manipulate for most of us it is easier to reach out and grab a 3d object than rotate it compared to using a cad program and trying to use a mouse to click on various views of an object and rotate it in three-dimensional space if you google uh online 3d drawing program you can try and play around with this with on your own very difficult to rotate an object in three-dimensional space to a desired uh orientation be nice if we could use something like the leap motion device or the ultra haptics to allow people to manipulate or build objects with their quote-unquote bare hands okay for most of us at least it's easier to sketch with a pen than it is by uh painting in a paint application using a mouse we have years and years of experience of manipulating objects and manipulating them delicately like a pen or a brush paintbrush with our hands than we do at a distance so to speak by grabbing a mouse clicking a virtual moving a virtual cursor which that cursor can grab and rotate and move and so on okay here's another example of exploiting the fact that it's easy to manipulate objects or it's intuitive for us to manipulate objects with our bare hands this is a project from the ishi group at mit and as the name implies of this technology the primary object the focusing object in this technology is literal clay malleable material as you watch this video i want you again to pay attention to what is the feedback loop what which sense organs are being engaged by the interactive technology and what aspects of our output devices which in this case is going to be our hands and fingers are at play here this is also an example of another branch of hci known as computer supported cooperative work or cscw which we've talked about sort of indirectly throughout this course these are particular technologies that are directed at allowing people to work together so crowdsourcing is one form of computer supported cooperative work github is another one what happens if there is a particular task where it makes sense for a bunch of people to stand around a common object and collectively manipulate that object in the case of illuminating clay to literally manipulate it and reshape it in to demonstrate ideas and brainstorm and so on illuminating clay allows users to simultaneously interact with both physical and computational representations of the landscape here we see two collaborators preparing a landscape model to be analyzed with the system the vivid 900 minolta laser scanner allows the topography of the model to be captured at a rate of 1 hertz a mitsubishi lcd projector casts the results of the landscape analysis back onto the surfaces of the model the work table comprises of a smooth white surface and a rotating platform onto which a landscape model is placed we experimented with different types of landscape modeling materials plasticine with a ductile fibrous core allows the model to maintain the required topography the area around the platform is illuminated with the library of analysis functions that can be selected at will the remaining edges of the work surface are used to project cross-sections of the model terrain the scancast mode projects analysis functions onto the model these include variables such as slope variation and curvature shadowing and solar radiation water flow and land erosion the cut cast mode allows users to add surfaces for projection without affecting the simulation results cad cast allows the user to construct three-dimensional topographical models this system allows any object from the user's work environment to be used as an input to the system without the need for tagging tethering or demarcation the interface provides a simple means for three-dimensional display where the tangible immediacy of physical models can be combined with the dynamic capabilities of computational simulation okay what uh what information is being collected by the interactive technology by the illuminating clay what is the input to the device or the system in this case [Applause] how does the system know to update the visual display that it's sending back so david mentions a point cloud so the the system is able to know is able to indirectly infer the local heights of the clay using a point cloud i think actually they derive a point cloud from the video display but i could be wrong basically it's getting back curvature of the clay and then using that it updates its output which is a visual output onto the clay so this is also an example of a multi multi uh multimodal projection so this system is projecting back uh onto multiple uh sense organs sends sense modalities for the humans the humans are seeing changes in visual output but also feeling changes in the clay which are either being caused by themselves or by others manipulating uh the object okay so again here's a technology it's a little bit dated at this point this one did not become did not make it out of the lab into a useful product but again it sort of suggests interesting ideas about how to combine visual and tactile feedback i think we will move on last one we're gonna the last uh tangible computing application we're gonna look at here is the actuated workbench again i want you to pay attention to what is the feedback loop what is the input to the system what is the output of the system and what is the input and output from the point of view of the human user the actuated workbench consists of an 8x8 array of electromagnets it uses magnetic attraction and repulsion to move magnetic objects two dimensionally on a flat surface here we can see a magnet moving in stepwise manhattan motion here we see an object moving in a smooth circular pattern although the array of electromagnets is fixed the system can create smooth motion by varying the strength of the electromagnetic fields in addition to magnets the electromagnetic array can move any small ferromagnetic object such as a paperclip here the user controls the puck's motion with a trackball smaller objects can be moved much faster though their motion is not always so smooth magnets of different sizes and shapes behave differently in the system's magnetic fields this stack of small magnets jumps around musically we use computer vision as a preliminary object tracking technology here the user records a path by moving the puck on the surface and the system then replays that path through magnetic actuation the blue projection around the puck is a graphical visualization of the strength of each adjacent magnetic field here is an example application intended to teach users about physics the red projected area on the surface represents a zone of attractive force while the blue area represents repulsive force the user can feel these forces by lightly holding the puck in different areas of the board when the user releases the puck it flies to the red zone of the board to which it is attracted magnetic drawing toys are effective for visualizing the actuated workbench's magnetic fields a magnadoodle allows us to see the fields used to trace the smooth circular path shown at the beginning of this video the dapper dan toy lets us see the magnetic activity in the movements of iron filings on the surface the actuated workbench can be used to control the planchette in a ouija board game like other robust systems such as the diamond touch presented by mitsubishi electric research labs in whis 2001 the actuated workbench works even when set on fire okay i think the grad students who worked on this project went a little overboard with their demos towards the end but uh other than communicating with those that have crossed over to the other side with the ouija board what might be some applications of this kind of technology the developers had a few ideas and built them into this video assuming you had access to the actuated workbench you could program it what kind of applications might you think about creating here remember our discussion from a few minutes back about how it's much more intuitive to grasp and manipulate an object than it is to passively view it from a distance how might we exploit that fact to make something easier for a user where they can physically manipulate something rather than having to look at something from a distance and learn about it that way any ideas there were several suggestions in this video about creating educational software so they had an example about trying to teach about electro uh electromagnetism which is an abstract concept for most people by concretizing it making the user able to actually feel the pull of a magnetic force as they moved an object across the top of the actuated workbench most of you have had the experience about trying to learn to uh play an instrument or uh or print learn calligraphy all of these fine motor tasks that require very careful control over the hand or the manipulation of an object with the hand the best way to use learn this without technology is to is to observe an accomplished instructor so call up a youtube video let's say you want to learn a japanese kanji characters best thing to do is to call up a youtube video and watch someone do it but when you watch someone do it you have to watch what they're doing visualize what it would feel like to perform that action and then carry out that action yourself so you're going from a tactile interaction the instructor's tactile interaction with her pen that's transduced into a video the visual sense the student absorbs that visual scene and then has to again translate the visual scene back into a motor program a way to to move their hand to reproduce what they saw the instructor do imagine instead that you were to grasp a pen that is connected to the actuated workbench and that pen would pull your hand through the drawing of a kanji character the instructor might record that using the actuated workbench so the instructor would grab would grab the object and the actuated workbench would passively detect the changes in magnetic forces in the magnets underneath the plate or movement across the plate and then the system would then the system would play that drawing back i think in this video actually you could see an example where that someone performed a motion and then the actuated workbench played it back that could be a potential use for this technology difficult to learn something by watching an instructor at a distance better to in a sense hold the hand of the instructor and allow the instructor's hand to pull your hand through the actions you're trying to learn okay i'm sure you can think up some other potential applications for this technology we've looked at lots and lots of example hci systems in this class to date this one the actuated workbench has a particular capability that none of the others have had up to this point of course this system uses magnets a lot of other systems did not use magnets that's not the important difference what is the important difference here can the actuated workbench do that none of the other technologies we've looked at so far are able to do the hint is in the name itself of this technology no the actuated workbench is able to actuate or move physical objects in the world we saw jibo and kism the g bone kismet robots which move themselves a little bit so they are strictly in a strict sense moving physical objects this one however it's the system that is moving other objects in the world and if a user grasped that object both the system and the user can collectively move or manipulate that physical object whether or not the actuated workbench ever becomes a useful technology it demonstrates again a potential application area in future hci systems which are computer technologies that are able to actually move objects out in the world and we're going to see much more of that when we come to the robotics section of the course bryce says i work at a bike shop and the immediate thought for me was a larger scale system that auto organizes all my tools there's a great idea absolutely right so if this workbench were an actual workbench and you're working on something with a lot of metal tools that might be that might be a great example yep good good catch there okay so again short lecture today on tuis or tangible user interfaces and again the idea here is to think carefully about pact what do what would people how how would people prefer to carry out an activity some activities it's much easier to manipulate an object and learn something or do something through physical manipulation than it is to passively look at a screen from a distance okay so uh we're going to move on now to lecture 16 where we're going to think generally about this social enterprise that we are involved in which is stitching technology into the world technologies that can directly sense the world and as we saw in the case of the actuated workbench uh interactive technologies that can literally manipulate the physical world alongside human beings okay so this is my cartoon from the beginning of the course president says you could play chess with someone across the world where the pieces move for the other person as you drag them across the board right if you love chess or other games you can obviously play on the computer but there's something even in chess which is a very abstract game there is something about the tactile element of chess that is often lacking in online online games okay so ubiquitous computing um here's my cartoon history of hci back in the stone age all of about 30 years ago there were lots of humans the ages and a small minority of them had computers the internet was invented and these computers started to be connected up then the smartphone was developed most people in the uh in the developing world if they don't have a laptop they definitely have a smartphone there are many many more smartphones out there now than there are laptops and desktops the internet continues to grow and spread and become increasingly interconnected we are seeing the arrival of embedded devices so these are stationary devices that can sense and send information to devices that are nearby so an obvious example would be a wi-fi a wi-fi a wi-fi projector we talked about smart light sensors in the davis center so systems that can take input directly from the physical world they sense things but they have limited ability to influence the world directly we're going to talk shortly about robots which are also embedded devices they can directly sense the world but they are also able to move uh themselves so this is the world that we're heading towards how do we make sure that as we continue to architect and build the system that we think carefully about pact and we react appropriately when unanticipated consequences arrive to create this uh increasingly diverse set of interactive technologies that the world of the future whatever it's going to be is going to be not just apps they're going to be technologies that physically directly that interact directly and physically with the world hopefully with humans and not uh at cross purposes to humans okay that's the idea of ubiquitous computing putting computation out there in the world and when it's done right computers disappear into the fabric of the real world so one of the most important things about getting ubiquitous computing right is that it should be invisible right or you're not aware of it obviously we're not all together in classroom but if we were i would ask you that hopefully you have your cell phone but you've forgotten that you have it in your pocket and hopefully most of the time your attention is focused on the slides and my voice if your phone is constantly notifying you and vibrating it is not invisible it is pulling your attention towards it um as we move around in uh or when after the pandemic when we get back to moving around in physical environments we can see in certain places like times square the the technology that's there is not invisible and that's the point it's meant to be very visible but useful ubiquitous technology that is stitched into the world and is there to help you is hopefully most of the time invisible and you can effortlessly quote unquote call it up and use it when you need it and then when you don't need it anymore it fades into the background the most common example that that arises in most people's mind when they think about the invisibility of ubiquitous computing is gps right it's there if you need it you pull out your phone you turn you plug your phone into your car and it tell there's a voice that tells you where to go you don't even need to look at your phone your car knows where to go or your phone knows where to go it will be quiet it will not project to your visual sense while you're driving it will only speak up when needed with the press of a button it disappears okay this is an idea ubiquitous computing is an idea that's been around for a long time dating back to the 1980s but it's only in the last few years obviously that the uh practical technology has caught has caught up and made ubiquitous computing a reality there are typically uh two uh founders of ubiquitous computing uh one in japan one here in the us uh ken sakamuro at the university of tokyo referred to this as computers everywhere his insight way back in the 1980s before before there were cell phones is that most people were and this was right around the time of the beginning of the personal computer revolution he realized that even in the distant future when everybody had a personal computer they would still probably only have one there'd be little need to have two personal computers and it seems seems silly for someone to need three personal computers however so therefore the number of processors the number of computers is limited by the human population however even back then professor sakamura realized that the number of embedded devices a human uses is only going to be limited by the number of devices they serve so people in the future may have just one personal computer but they're going to have lots of other technologies which was unknown at the time and those technologies were going to have a cpu inside and some operating system was going to have to run that cpu so professor sakamura created the real-time operating system nucleus known as tron why real time well as you you've uh as you've realized now with the leap motion device the the software supporting the leap motion device is real-time software so the moment that something happens you can have the system speak up and call and call a callback function that says i have a fresh frame of data when you're working with embedded devices in the real world they're events that happen in in the real world and those events are recognized by the operating system and they trigger something to happen which is different from traditional code where you write a loop and it just does its thing internally regardless of what's going on in the outside world so that's the idea behind real-time software and and professor sakamura realized that embedded devices like cars and refrigerators and airplanes these were all going to need software that is real time in an airplane or a refrigerator a car there are events that happen that the software has to deal with immediately no matter what the software is doing when that event occurs you deal with it um so starting the 1980s you started to deploy this operating system into devices that were being shipped with processors and even back in the early 2000s windows was being shipped on about 200 million personal computers a year at that time already in the 2000s there were orders of magnitude more uh devices that were running the tron operating system running uh then so back then in the early 2000s if you ask somebody what's the most popular operating system out there at that time most people would say windows the apple apple was not ascendant at that time but they were wrong the most popular operating system at the time was tron and that is still true today so tron itself no longer exists it's been folded into a parent company that develops other real-time operating systems but there are many more instances of real-time operating systems running on the globe at the moment than there are windows and linux and mac uh uh operating systems running okay around the same time uh mark wiser wrote a research paper called ubiquitous computing so he's the one that actually named this idea of stitching computing into the world mark worked at zero palo alto research center or park park was uh was sort of the google of its time xerox the corporation that made xerox machines or copier machines had uh this research branch known as park where researchers were allowed to work on anything that they thought was interesting it was an amazing place to work back in the back in the in the 80s at the bottom of the slide this is a quote from mark's paper our computer should be like our childhood so mark created this important metaphor which is that like our childhood ubiquitous technology should provide an invisible foundation that is quickly forgotten by us but it's always with us and effortlessly used throughout our lives anytime you climb on a bicycle and immediately start riding the bike your childhood is there with you what you what your body learns when you learn to ride a bike you don't consciously call up remembering how to ride a bike when you hop onto a bike it's there and you can effortlessly call on it when you need it and the minute you hop off the bike it's gone again okay mark uh also refers to a book by uh philip k dick called ubic most of you probably know do androids dream of electric sheep which became made into a movie called blade runner um hollywood has a love affair with philip k dick at the moment ubik has not been made into a movie yet but here's hoping over the winter break i am going to assign ubik as required reading for all of you that haven't read it yet okay what are some examples of ubiquitous technology that's already out there that has this property it's invisible and it's quickly forgotten after you stop uh using it but it's always there in case you need it obviously most of us are spending much less time walking around in physical environments anymore but when you do what are some examples of ubiquitous technology that you can effortlessly interact with without giving it much thought and then forget it again when you're done wi-fi is the uh yep absolutely most common wi-fi and gps are kind of the most obvious examples these days what are some ubiquitous technologies that are provided by wi-fi that are supported by wi-fi what are some apps that are particularly useful when you're mobile when you're out and about and trying to do get things done in the world rather than passively watching and scrolling them on your phone no no no it is does anybody still play pokemon go the pokemon characters are out there in the world they are invisible but if you bring up your phone you can locate them quickly and then play the game for a bit and put it back in your pocket okay maybe a bit of a facetious example gps and wi-fi at the moment are kind of the only examples but you can start to think about some other technologies you might start to layer on top of cell phones as people move about in their environment okay one of the most important things that is built into most ubiquitous technology again is that it is being stitched into the world it suffuses the physical world rather than sitting as a separate information network within it the internet is not ubiquitous computing the internet is ubiquitous it's available on any computer anywhere in the world but it is somehow separate from the world right it takes some effort to get stuff into the internet you need to type it in or do whatever you move a mouse provide information or do something to draw it back out again wi-fi and gps of course they are supported by the internet but those applications are directly interacting with the world when ubiquitous technology is done right it should act as scaffolding scaffolding is another concept that we're going to draw on from psychology and as the name of scaffolding implies humans particularly will spontaneously scaffold the learning of their children as you can see in this example of learning to walk scaffolding should help people learn new skills obtain information be aware of new opportunities scaffolding is particularly useful when it's difficult for the learner to get started there's a particular task they'd like to learn or a function they'd like to carry out they don't quite know how to get started they need they need some help how do you provide that help in the case of a human parent or in the case of ubiquitous technology and then if it's done right that scaffolding provides a learning gradient meaning it provides the learner a place to start and it suggests baby steps either literal baby steps or figurative baby steps on how to make gradual process at that task how to take small incremental steps to move towards mastery of a skill as the learner starts to demonstrate in increased competency a an accomplished scaffolder will gradually remove the scaffolding to allow the learner to continue on on their own two feet literally or figuratively that's the idea of scaffolding we mentioned affordances many years many years many weeks back feels like years sometimes talked about affordances a few weeks back these are the two sort of uh the two foundations of hci that are brought in from psychology affordances and scaffolding we just we've actually talked about lots of different examples of scaffolding already we just talked about the actuated work bench where uh a student may want to learn kanji they don't know how to do it yet they have no idea how to get started but by grasping an object and allowing the actuated workbench to pull their hand through the motions of drawing kanji characters that's scaffolding right it's giving them a place to start as i mentioned the other the other side of scaffolding is detecting growing competence in the case of the learner and knowing how to remove scalpel scaffolding when they signal greater competence in the case of the actuated workbench how would you code how would you program the bench to gradually remove scaffolding as the user becomes better at signing kanji characters on their own what is the scaffolding and how would you remove it decrease the magnetic field strength right exactly so in the same way that a parent will grasp their child's hands with less and less force until eventually they let go altogether the actuated workbench could decrease the magnetic field strength until eventually the user is moving the pen on the workbench by their own and the workbench is completely passive the other tricky thing for the teacher in this case or the instructor is detecting increasing competency how would the workbench know that the user is getting better at signing kanji on their own what is the input to the bench what is the sensory signal what is the sensory signature that the workbench is looking for to know the user is growing in confidence at drawing kanji characters how does the system know when the user is starting to stand on their own two feet remember that in the actuated workbench there is a camera that can see the position of the pen and based on the position of the pen it knows what forces to apply to move the pen but also the user is moving the pen by themselves how does the system know the user is getting better at signing country this is one of the trickiest parts of scaffolding seems intuitive as the user gets better at something you gradually remove scaffolding but thinking about thinking is misleading it's not immediately obvious how to detect growing growing learner competency and remove scaffolding humans for very good darwinian reasons we are natural teachers we're instinctive teachers even if you've never had a child if you have a small cousin or a nephew or niece if you're not a parent you can still immediately grasp hold a child's hand and sense when they're getting better at walking and know exactly how to apply forces to help that child out it's something that is inbuilt in us but in the case of the actuated workbench or any other in interactive technology it is not an instinctual teacher you need to program that system to know when to gradually remove scaffolding you're going to have some time to practice this in deliverable 10 where you are also going to have to detect growing competency on the part of your user who is getting better and better at signing assigning asl digits how do you know how will you be able to detect given all you know now about the k n and the leap motion device how will you know when the user is getting better at learning what you want them to learn okay i'm not going to answer the leap motion one for you i'll give you i'll leave you that one to think about in the case of the actuated workbench it the the workbench quote-unquote knows the kanji character that is that is in the process of being signed as being as drawn so given the current position of the pen which the workbench can see the workbench knows what the next position is the workbench can feel the the motion of the user so there are sensors sorry the visual sensor can see that the user is trying to push the uh pen towards where the next point in the kanji character should be they're trying to move in the right direction and a tenth of a second later the magnets actually move depend to that position so if the user is pushing and trying to anticipate the movement of the pen that's the signal that demonstrates growing learner competency okay what are some other examples of scaffolding either in the physical world so forget technology for a moment how do humans scaffold other humans experiences or other examples of interactive technologies that also provide scaffolding throughout your lives your parents and your various teachers and instructors and your friends and family members have either directly or indirectly or consciously and unconsciously scaffolded your world provided opportunities for you to get started on learning something difficult or accomplishing something that would be difficult without scaffolding provided by other humans thinking about thinking is misleading you have been surrounded by scaffolding for most of you as university students most of that scaffolding has now been removed not all of it but most of it what is it what are some examples of it we've mentioned bicycles a few times in this lecture how did you learn to ride a bike training wheels exactly so two examples are usually given of scaffolding from the real world the parent child interaction that you see in the in the photo here and training wheels what are some examples of interactive technologies that that provide training wheels to the user and then remove that scaffolding as the user gets better so a lot of games or complex software if they detect that you're a new user they'll provide a pop-up notification or helpful hints or a tutorial and if you swipe them away or better yet you don't have to distract yourself from what you're doing sakad to the notification click on it to get rid of it you demonstrate through your actions that you don't need that particular scaffolding anymore okay i'll leave that for you to to think about let's uh let's think about another potential application for scaffolding that is becoming increasingly urgent and this is the scaffolding of older older people's uh homes and environments uh ubiquitous technologies tend to appear in indoor environments first rather than outdoor environments so we're going to focus in this example about trying to think about instrumenting the homes of the elderly to allow them to remain independent for as long as possible why is this so urgent well this particular visual design or this particular figure shows why as most of us know most populations at least in uh in first world countries are becoming increasingly older so back in the 1950 in this particular country which has not been named yet in this particular country in the 1950 in the night in 1950 you can see that the majority uh the majority of the population was middle-aged but there were many more young people than there were people 65 and older in 2005 it was about 50 50 uh as well slightly more people over 65 than under 0-14 and the projection for this particular country in 2015 is that there will be uh if you sum 35.7 plus 10.8 it's almost greater than those that are working so in japan for example there are now more people that do not work than work they do not work because they're retired or they have yet to enter the workforce how do you keep a country going socially and economically if the majority of people are not eligible to work and the minority are working this is not this figure is not drawn from japan what country is this figure drawn from and what's the hint that tells you what country this is it's not the united states the uk how did you know that ethan that's correct exactly there's your there's your hint okay not so relevant to our discussion today but especially uh especially during the pandemic era this is becoming increasingly urgent how do we keep the elderly or in some cases the sickening firm out of hospitals and health centers and allow them to remain happy and somewhat healthy in their homes can we provide technology that is stitched into the home of the elderly or the infirm that supports them and provides scaffolding in their daily lives in their home okay lots of different applications you can think of this here's one particular example we're going to stitch in some cases literally pressure sensors into the carpets of miss x's home so i'm drawing a graph here with nodes and edges um this graph is drawn drawn according to the ermia specification this is entities and relationships so the entities are obviously the rectangles here these can be these can be either human stakeholders like miss x herself who lives alone in her home physical objects in her home like carpets distributed throughout her home telephones but also sensors and network devices and so on relationships are represented by edges connecting objects and the numbers or variables that are labeled at the tips of the edges represent the one to one or one to many or many to many relationship so we have one stakeholder in this case miss x in her home in her home she has m carpets throughout her home for every one carpet she has n pressure sensors that we're going to stitch into the carpet i mentioned ermia here because for some of you that go work in industry this is a common notation that's used for creating visual requirements or visual use cases remember our discussion early on about design processes for hci it's a common notation or a system used for sketching out these kinds of use cases okay so what is the packed analysis here we are supporting miss x but we are also supporting miss x's caregiver so this may be uh miss x's children who uh periodically call on the phone and this could be the phone or the smartphone doesn't really matter for our purposes and it rings and rings and rings if you have an elderly family member a parent or a grandparent you probably know this feeling where you call and there's no answer if you call an elderly person at home and there's no answer what do you do at that point there are many explanations for why miss x did not answer the phone if you live anywhere nearby that the safest and most humane thing to do is to assume the worst and always hop in the car and drive over wouldn't it be nice if there was another way to get a better sense of what's going on in miss x's home imagine that we create these pressure sensors we hook them up to a sensor network device which is drawing information from all the pressure sensors and sending an automated uh text message to the smartphone which is if i call and there's no answer and i hang up the phone that call may trigger a text message which is what's up with miss x so for example if there is a particular pattern in the pressure sensors miss x is up and moving around a message might be sent saying miss x is approaching the phone please wait or or please call back in a moment she should arrive at the phone in 30 seconds two minutes five minutes if there are no peeps from the pressure sensors then perhaps miss x is not moving around or at least has not stepped on any of the carpets um she can't hear you or we haven't detected any movement in the home in response to your phone call please send a message to her vibrating pager or drive over or or what have you there's just one example so obviously this might support the activity of the caregivers for miss x it may be acceptable to the caregivers because it provides a little bit more information but it may not be acceptable to miss x herself this is clearly a pr a pretty strong invasion of her privacy and her home this is a particular uh a particular aspect of context in that that arises in health or other support systems for elderly yet independently living individuals most elderly individuals are particularly uh are particularly resistant to in intrusive technologies and beepers and phones and so on so navigating the acceptability of health support technologies for the elderly is a particular challenge and growing challenge in hci okay uh that concludes our discussion of ubiquitous technology we're making good progress today so we've got uh four minutes left so we'll just start in on lecture 17 now as promised we're now going to look at three different kinds of ubiquitous technologies uh three different applications these are all scientific applications so trying to pose a scientific question and test it using ubiquitous technology where it would be difficult or impossible to test that scientific question without technology being stitched into the world itself okay i'm going to start today by looking at ubiquitous computing for learning about social networks we're going to be focusing on a particular aspect of social networks which is face-to-face communication so obviously we can analyze social network data drawn from your favorite social network uh platform but what about uh face-to-face human interactions what what can we learn from a social science point of view about how humans interact face to face that's difficult to do without ubiquitous technology okay that's what we're going to look at in lecture 17. so let's just introduce this experiment first it's a quite a few years old now but this is one of the first attempts to use ubiquitous technology in the social sciences so um some very willing graduate students 24 graduate students agreed to carry around sensors which were attached onto the the belt of their their backpack here the sensors as you can see here sitting about where their lapel would be and it's relatively close to their mouth and it's going to record what they say again you can see this is a pretty seemingly a pretty intrusive technology one of the big acceptability or hci challenges for ubiquitous computing is is privacy issues and intrusiveness this sensor is going to record the volume of their speech how loud or how quiet do they speak and it is also going to record how they speak this is known as atrocity do they speak quickly slowly do they speak with clipped tones or did they slur their speech do they have a low or high pitch so the the uh the quality of the speech but not the content of the speech the sensors are going to at the point of the sensor they're going to extract out actual words and throw them away so we're going to anonymize what's being said and this was a nod to the acceptability of this technology these graduate students have agreed to use this technology for six months but their friends and family members haven't so even if the graduate students are okay with their words being recorded and used in this study other stakeholders have not okay the first thing that we're going to look at next time is from this raw speech data just volume and prosody the way they speak can we learn a social network among this group of 24 grad students who are friends and who are not uh i think actually let's pause here before we get more oh sorry last thing i wanted to mention here given the fact that we can infer the social network directly from the speech data is there a relationship this is the scientific question they're going to tackle is there a relationship or a correlation between where you are in this social network and how you behave among this group of 23 other graduate students is your face-to-face interaction modified or is it different corresponding to where you are in this social network answering this question would be extremely difficult to do without recording in real time information about how they speak who they speak to and so on i think we'll stop there for today remember that you have a quiz due tonight you're working on deliverable eight i wish you a good rest of your day and a good weekend i have my office hours at 11 a.m today otherwise have a good weekend we'll see you back here next tuesday thanks very much everyone 
nnvyGo7rLZc,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-19T16:28:27Z,"L18: Mental models. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/nnvyGo7rLZc/hqdefault.jpg,Josh Bongard,PT48M27S,false,130,0,0,0,0,you just can't okay let's uh let's get started let's spend a couple minutes talking about the deliverables I can hear that people are starting to diverge in their solutions for deliverable seven that's fine I can also hear that some of you are now realizing the warning that i gave you which is we're now trying to get Matt plug lib to do something for which it was not designed to do right so that's why whatever solution you come up with is is fine for us I hear that some people have come up with some certain solutions I'm happy to help facilitate that conversation so if people have found hacks or ways around Matt plot libs failings send me your solution I'll put it up on blackboard and hopefully on the announcement page there if you're stuck there will be a solution that that works for you I heard some people talking about the wireframe hand that's fine you don't necessarily need to draw the wire frame and all the time the wireframe hand was myit my simplest thing I could think of to get you started so the person can the user can see what they're doing if you can come up with something simpler that's more lightweight on matplotlib that's that's perfectly fine let's see some other possible hacks you can go back in matplotlib to 2d so again you can draw the 3d wireframe and in 2d and if you do it in the right way it's still kind of obvious to the user what whether leap motion is capturing their gesture or or not you can switch to PI a game that's that's fine other solutions that people have found on yet okay I've been these wireframes what's causing my issue okay go around an integral switch state since Forge images for me okay just when I introduced because I'm closing the images okay i introduce a new plot the event is not working with that fought because of how the variables are set up functions I say okay okay I'm trying to switch it back to object-oriented because I can then just do self meant ok but that that was better issues ok ok so again whatever works is fine with us I mentioned again a few times now this pipes option so you might want to try and do things in parallel right so you want to try and keep your infinite loop running tight meaning that it's going through that pretty quickly or else you start to get significant lag and what the infinite loop can do is use a pipe and start up a separate Python program that draws whatever it is write an image of video a wire frame and then it shuts down but the infinite loop that's capturing the data from the user is still running running tight so again it's up to you what solution you you hide ok I've also heard that people are dedicating quite a few hours to deliverable 7 my apologies about that deliverable 7 is a bit of a beast I hope when we talk about deliverable eight today this one's a little bit more lightweight so hopefully by the end of deliverable 7 you now sort of have the structure in place where you can switch and draw things based on the internal state of the program once you find a solution that works for you on your platform hopefully you can stick with that for the remainder of the course any other questions about deliverable 7 ok again if you found workarounds email them to me i'll post them to blackboard as an announcement ok so let's talk about deliverable 8 so in deliverable 7 you are adding state to your program and in deliverable 8 we're going to be adding data persistence or in other words we're going to be adding a database I've thrown a lot of different concepts at you in the last seven deliverables so we're not going to actually set up a database we're going to do this in a simpler way which is using Python dictionaries so you're not going to be setting up a mysql server and communicating with it you're going to have an internal database and all the data that you capture from all of the users that use your system that data is going to go into database okay who has worked with Python databases before one person okay so this will be new for most people again there's a little short tutorial here that you can follow along with a Python dictionary as the name implies a particular kind of data structure that has two parts there's the key and the description so obviously like a dictionary if you think about a dictionary the key is the word and the are sorry the key and value kyun values makeup dictionaries for a real dictionary the key is the word and the value is the description so key and keys can be any data structure and values can be any data structure you want so I'm going to just walk you through deliverable ate a little bit show you how this is going to work so you're going to create a new variable called database and curly braces always indicate Python databases sorry Python dictionaries so it's step one here you're creating an empty database so you've captured no data from your users or leap motion it's simply empty at the end of your program you're going to be dumping your database to a file so when your final quits just before it quits it's going to write out this data structure database which contains all of the data in your program and then at the beginning of your program before you enter your infinite loop you're going to be loading that database back in so when you shut down your program and start it back up again the data is going to be persistent because we're just going to shove everything into this database data structure save it to a file and read it back here right so that will save us having having to get into actual databases and so on okay okay we're going to be using a database we're going to be using dictionaries and we're going to actually use dictionaries of dictionaries so we're going to actually create this empty dictionary and inside that dictionary we're going to store entries and an entry is made up of a key value pair the key the first key is going to be the name of let's see here if I can find it here we go the key is going to be the name of the user and the value associated with that key is another dictionary and that dictionary contains all the data about this particular user named Josh so we had initially this is line 6 D here's printing out this data structure called database it was originally empty now it contains one key value pair the colon shows the key on the left and the value on the right the key is a string in this case the name of the user and the value in this case is a dictionary tell me about this dictionary inside the dictionary what data does the system have so far on user Josh nothing right it's an empty dictionary ok so we're creating these dictionaries of dictionaries after a little while we now have the database looking like this still a dictionary so the outer curly braces there are two entries so you can see the entries are separated by a comma and each entry again is a key value pair so we now have two entries at the top level of the database which are the two users that have used my system so far and we haven't captured anything about them yet so far so good ok so in deliverable eight we give you a little bit of guidance about starting to build this hierarchical structure halfway through deliverable a it's now up to you you can create any of any key value pairs you want what information do you think you're going to be storing in this database or in this dictionary should named it something else getting these confused what data might you want to record about your users progress okay so maybe we want to store whether there are lefty or righty so maybe inside this database we make a new entry called righty and the value what do you think the value might be so again the value can be any data structure we what we want true or false right so you might have an entry in here where the key is called righty and the value is a boolean value which will just indicate whether they're all righty or lefty right what other information might you want to store their accuracy so how might we record their accuracy in this data structure what kinds of entries might you want to create so prediction accuracy or percent of prediction accuracy of the learner okay so the prediction accuracy of them or of the KNN learner your previous cannon learner with their so there's only there's just one learner underneath right all that learner is doing is grabbing frames of data and outputting whether it's 0 through 9 yeah actually how much time it takes them to sign the number right so your program knows which digit is being projected right so the system is asking the user to sign three and the KNN learner is spinning out at every frame a digit so we can count the number of frames that pass until the can and learner starts out putting the digit 3 right so that information would capture how long it takes the user to correctly sign to the digit so how would we encode that in this hierarchical data structure that's probably some relevant data where would it go so we might want to record how long it takes the user to sign the digit 3 but also how long it took them to sign the digit 4 and 5 and 6 and so on so we can record global information about the user like whether they're a lefty or righty we can record the number of times they've logged in for example and then finally we could start to pull out user records so in this case we've got our database and if we use square brackets and put something inside that square bracket that means we're requesting the value that's associated with that key so remember at the top level of the database I'll scale back here a little bit the keys are strings which are the name of the user so online 9a here I'm putting in a string here requesting all of the information associated with that user so if username is equal to the string called Josh then user record will equal the value associated with that key which in this case is a what data structure another dictionary right so now I'm I've grabbed the dictionary associated with this string and it's now being pointed to by user record and so now I can go into that record and set a new key value pair which is the number of times that the digit 3 has been attempted so I see that josh has logged in I roll a ten sided dice and it comes up three so I display the number three to the user and I run this line which records the fact that josh has been shown the digit 3 and has been requested to try and sign it right so I could add another entry to josh's user record which is digit 3 successful right did they did the can and learner's start to output three indicating that the KNN learners recognizing that josh is signing digit 3 correctly did you have a question I'm like a purse is in the room x can be like image of that number just years of that that's a very good question and that's coming up in deliverable nine or ten and that's called scaffolding so make things easy for the user we show the digit 3 we show up this picture and then we see how long it takes them to sign it if they get better at it if we record performance accuracy and on average they get better and better at signing the digit 3 then the next time we pick the number 3 to show to the user we only show the digit itself and not the image associated with it we're removing some of the scaffolding and making the task a little bit more challenging yes we will definitely get get to that as I mentioned you can use any data structure here for the key so in past years some people have done the following so instead of writing out user record i'll just write you are you can use the number itself so this might be a dictionary initially empty that contains all the information about how the user has done in terms of the digit 3 so i can store whatever I want in here I could for example then do the following once I've created this the first time that I show the user the digit 3 that's the first attempt so I create again a new dictionary inside this which is all the information about when the user was exposed to the digit 3 for the first time if I then create this key value pair this I create this key value pair when the user is shown the digit 3 the second time and it contains all the information about how well they did during the second presentation of the digit 3 so we can creat creating these deeper and deeper levels of the hierarchy and you can reference them with square brackets like this or a comma between them this also works I could keep going so I could say all right on the second attempt during which the user tried to attempt digit 3 how long did it take them to do so what was the time or the number of passes through the infinite loop until they started to sign it correctly so now mikey is a string and my value is going to be what data structure I'm going to record the number of loops it took until they successfully sign the digit 3 an integer right so it took eight passes through the infinite loop for them to to sign the digit 3 so back to your point about progress right let's say this is the hierarchy I start to build up in my database about my users how would I then query this database or go through this dictionary of dictionaries and compute whether they're making progress or not what would I be looking for how are we going to quantify progress in your system how many times they've done it versus how much how many times or how many times have had to go through the loop how long it took them to do a correct that we could divide it by that right so if we're interested in progress in general it's probably how quickly they correctly sign the digit over all the digits and over all the presentations of all of those digits so I could create a nested loop here for I in range 0 through 10 remember in Python it doesn't use the last value here so I can iterate i from 0 through 9 inclusive and I'm then going to ask for all the presentations for 4j in you are I through the following so let's think about the logic here for a moment I'm going to iterate over all nine over all 10 digits and in the inner part of this loop for each value of I I'm going to pull out J and J is the the key here right so if let's say in you are three here there have been two attempts so there's 30 and 31 so the user has been shown digit 3 2 times when I is equal to 3 and we get down here this for loop is going to realize that there are two entries in you are three so it's going to set j equal to zero for the first pass through this for loop and then it's going to set j equal to one on the second time through this for loop let's now we iterate we come back out of the inner loop we come up here I is now equal to four at this inner for loop is inner for loop looks inside of you are four and sees that for has been presented to the user three times so now jay will iterate 0 1 and 2 so the nice thing about dictionaries is we could just ask for all of the keys that exist so in this case there are two key or two entries that exist in you are for our sorry three entries that exist at you are four and two entries that exist in you are three so we're going to entering over all the digits we're going to iterate over all of the presentations for each of those digits and we've been my inside here do some computation using you are i J and time so if I reference this it'll return an integer which is the amount of time it took the user to correctly sign the I a--the digit during the jf presentation of that digit so far so good ok so again let's continue this example for a minute I want to try and estimate Josh user Josh's progress through the system so far so I'm iterating over all the dishes and over all the presentation of those digits what computation do you think I'm going to be doing in the innermost loop here what am I going to be looking for how are we actually going to quantify progress here averages so let's say I know the ASL alphabet already I come and use your system the minute it starts up I start signing everything correctly right off the bat so mean is not going to be sufficient here the average their last if it's less than or equal to I guess if it's less than absolutely right so as J iterates from j equals 0 1 2 3 as the number of presentations of a given digit is given is this number decreasing right if it is that's progress right the first time j equals 0 the first time I've ever shown the digit 3 i takes me a while to figure it out i eventually get it right the next time digit 3 is shown to me i equals 3 j equals 1 i'm a little bit faster and this integer is a little bit lower right so i could perform a computation to see whether i'm getting better or not and then i might flash something up on the screen to tell the user they're making progress or maybe they should spend some more time on this particular digit or what have you ok so that's just a sort of crash course on dictionaries of dictionaries which is going to store all the information of your system so far across all users across all digits and any other information you want about specific events in your system like the jf presentation of the iaf digit ok what you're going to be submitting for deliverable 8 is a walkthrough of your user using the system so you're going to show a user logging into the system so this is important we're going to add some code above the infinite loop that does a little bit of text at the command line I know I said we're supposed to assume that our users don't speak English we'll assume that they speak English at least for the beginning part who says please enter your name your user types in the name and then it enters into the infinite loop once matplotlib starts up it's very hard to capture text from the command line so we're going to do all that before the infinite loop you're going to show somebody logging in you're going to be showing them doing their thing you're going to show them stopping the program starting the program back up again typing in their username again so they're logging back into the system and you're going to show some persistence so you're going to show when the user logs back in for the second time before they do anything there's going to be a visualization to the user showing how many presentations of each of the 10 digit and 10 digits they saw in previous sessions so this is obviously for your users but also for us to show us that you're successfully capturing data from one session to the next and when an existing user logs back in you successfully read that database back in and you're displaying that user to that data to the screen make sense ok little tricky in this case if you're logging in and trying to phone to film the screen with your phone will give you a little bit of poetic license here doesn't have to be a perfect video just make sure that we can see these seven steps in the video ok any other questions about deliverable eight ok if that was a little bit confusing again there's a tutorial you might want to just walk through creating dictionaries of dictionaries before sitting out on deliverable eight ok so back to lecture finally we're starting in on the net theme of the course which is cognitive psychology they're going to be four lectures in this theme here we're going to start with the most objective one which is mental models and we're going to start moving through different aspects of psychology that are increasingly subjective or hard for neuroscience to get at at the moment but are important for HCI such as affect otherwise known as emotion ok we're going to talk a little bit about mental models today the reading for today is based around a research article that i published about a robot which you'll see in a moment so what do robots have to do with HCI i'm going to show you this robot in a moment and this robot builds up mental models by interacting with its environment and the reason why we're going to talk about robots making mental models rather than users making mental models is because one of the nice things about robots is we can break them open and actually see the mental model inside which we can't do with humans we can infer that you're building up a mental model because when I show you a new app you immediately know how to interact with it in the right way because you have meant a mental model of how apps tend to tend to work so we're going to look at the nitty gritty details of how interactions with the physical world lead to mental models and once you have a mental model what is it good for the main thing that a mental model is good for is making predictions ok so the optional reading for today is the actual article itself this is pretty technical so I don't expect you to absorb all or any of the article itself the required reading today is from a colleague of mine who wrote sort of an opinion piece about the article trying to explain this article for for a general audience so hopefully after today's lecture you should be able to read what do robots dream up and understand a little bit about this this robot the title is a play on words or it's a nod to something else everybody know what it is they're in the back do androids dream of electric sheep okay otherwise known as Blade Runner right ok there's the very good ok what do robots dream of let's see what robots dream of here's the robot when we first built this robot and turned it on and undergraduate sought and turned around when it started flailing around and said dude that's the evil starfish so for better for worse that name has stuck this is the evil starfish here when I'm going to walk you through an experiment we did with the evil starfish here what's important about the evil starfish is that when we turn it on it has little to no understanding of itself so it's not unlike a human infant who is born in this crazy chaos and gradually starts to build an understanding of self where do I end and where does the world begin and like human babies the starfish is going to do that by interacting with its environment how does the robot do that well as you can see the robot has four legs and it had four sensors we're going to disregard two of them here and just focus on the to tilt sensors so whenever the robot moves it can sense how much it tilts to the left and right and it can also sense how much it tilts forward and back so every time the robot pushes against the world meaning it moves the world pushes back and the robot can sense how the world push push back by seeing how these two angles left and right and forward and back changed okay so those are its sensors we tell the robot that it's made up of this main body this green cube here and it also has these additional eight parts but we don't tell it how those eight parts are put together and that's what the robot is going to try and do so in this little cartoon here here are three guesses that the robot may make about how its put together each of these three guesses is a mental model so in this case we're focusing on a particular kind of mental model which is a self model the human brain continuously updates and maintains and grows different kinds of mental models mental models about self mental models about others mental models about the physical world mental models about technology mental models about apps we're going to focus on just one type for this robot which is models of self so far so good okay okay so as I mentioned the starfish robot here starts its life without a mental model of self and the first thing we asked it to do is build up an understanding of self it's going to start to interact with the world I will try and actually play this out of power point here will make it a little easier okay here we go so the first thing the robot did was to perform this random action it doesn't know anything about itself it knows it's made up with these nine parts it's been told that has to figure out how they put how its put together so in the absence of any knowledge it just moves randomly it knows there are these nine parts and these nine parts are connected together with eight joints so there's four legs the leg is attached to the main body and the lower leg is attached to the upper leg so two joints or two motors per leg four legs four times two it knows it has eight motors so it just sends electricity to some of these motors and the result is that some of them rotate upward and some of them rotate downward and the main body in this cartoon tilts and this in this video here tilts backwards a little bit so it knows that when it sends these commands to the eight motors the tilt sensors respond in a particular way so i mentioned that we're talking about mental models here we're talking about a specific kind of mental models which are self models they're also known in neural science as forward models this is important for us because a forward model implies forward in time when I do something my model makes a prediction my mental model makes a prediction about what I'm going to sense in the future forward in time when I click on this button in this app I have a forward model of that app so I make a prediction about what I'm going to see half a second or a second later when i click that button if I see something different from what the forward model predicts I'm either frustrated and I delete the app or I'm interested because I got a new result that I never got before I mentioned last time the brain is a prediction machine and it's a prediction machine in a specific way the brain is making predictions all the time and the brain has been wired up to pay attention to things that disagree with our prediction right if I send commands to the muscles in my right arm I predict that I'm going to see my hand in the right side of my visual field if I send those commands to my muscles and I don't see my hand appear in my right visual field I'm going to be pretty surprised and I'm going to pay attention to what's what's happening right okay so that's a forward model so as I said the robot doesn't have a model yet all it's done is performed this action and recorded this data now our robot is going to stop moving for a moment and it's going to run a machine learning algorithm so one is much more sophisticated than the KNN learner we're not going to get into the details of this machine learning algorithm what is what this machine learning algorithm is doing is iterating over a whole bunch of possible mental models and for each one of these mental models or simulations of itself it's sending electricity to the same eight motors all right so each one of these mental models that you see flipping by the robot is sending the same motor commands that it did to its physical body and what is the result of those actions what happened the same tilts right so if you watch carefully you'll see that the green cube is tilting back right so at this point in this robots life lifetime it's succeeded it says okay i know that in reality i did this and I got this sensory repercussion I've now created a whole bunch of forward models that when I do the same thing when move slightly forward in time at a moment in the future I move in the same way so these models are explaining what I sensed in the real world how good are these forward models in actual reality from our point of view not very good from the point of view of the robot they're perfect they predict exactly the sum of this robots experiences in our world right so far so good the robot does know however that there's a bunch of them there's a whole bunch of these different mental models that are all perfect and the robot knows when I say the robot knows put the scare quotes around it it knows that this something's funny right there's no way that it can be this robot and this robot and this robot and this robot so it knows it has a bunch of mental models not just one it has a whole bunch that are very different so if you were this robot what would you do would you keep trying to refine these models absolutely you do another experiment to add to your data set so now the robot does this and now it tilts to the left says okay uh-huh I have to training instances right remember our discussion about KN and learning it's a very different machine learning algorithm here but the same principle the robot is building up a training set on its own right so now it says okay I've got these two pieces so now I'm going to take for example this mental model and I'm going to evaluate this mental model twice once with the first action I performed and again with the second action I'm going to perform and I'm just showing you the second one you can see that now the main body is tilting to the left right so at this point in the experiment the robot has now is getting better it's getting closer to an understanding of itself you'll notice it says down here the eighth cycle of 16 so we're actually skipped ahead to the eighth experiment not the second one so what this robot does is it performs an action with its physical body stops moving and tries to improve these forward models using a machine learning algorithm and the machine learning algorithm is looking for models that explain all of its experiences in the real world so far so good okay if you watch carefully we picked the eight cycle because our robot has a little bit of a Eureka moment here okay at this point in the machine learning algorithm its stumbled on this forward model which explained seven of the eight experiences it had had so far but not the h1 and you can see why I got seven out of eight correct right it's almost stumbled on the right answer a couple more iterations of the machine learning algorithm and it hits on this and now explains eight of the eight experiments from our point of view how good is this model not very good okay so bad it's got the pieces more or less right they're bent right it's not perfect this points out another important aspect of a mental model it doesn't need to be perfect it only needs to be good enough for whatever we want to do right so you get your new yep thank you so what do you want to do once it has sense of self like do you want to delete job today you'll see where one third of the way through this video so I'll show you the latter set two thirds in a moment okay mental models for models don't have to be perfect you get a new smartphone it's got yet a new version of the operating system on it you have to get a go yet again or I have to do yet again go in and figure out how to turn off the ringer for new text messages right I have a pretty good mental model I've had for android phones already I predict it's probably going to be in the settings sub sub directory or submenu not the text submenu it's good enough I don't know where exactly it's going to be in the new menu hierarchy but I kind of know where to go right it's good enough okay okay so we keep going with this experiment here's the 16th experiment and at this point if you look inside the brain of the robot this is all you see it has a whole bunch of slight variations of this model which for new experiences already predict the right result so if we were to take this mental model and the physical robot were to think about performing a new action it could take that new action and instead of carrying it out using its physical body it could submit it to this forward model and the forward model will or the main body the forward model will tilt and that tilt is a prediction forward in time the ford model says if you do this I think you're going to tilt like this right this is very important skill to have there's a good reason why all mammals have forward models in the brain i'm outside and I see that there looks like something like a huge cliff in front of me I could walk to the edge of the cliff and confirm that yes this actually is a cliff or I could use my forward model to mentally simulate approaching the cliff and sort of seeing what the situation is is there there's a famous saying and neuroscience that Ford models are good because many of them died in our stead right before you actually try out a dangerous action you might want to consider or mentally simulate that dangerous action and work through what the consequences of that action might be forward in time okay so now the robot is armed with a bottle of self the question is what is this robot supposed to be doing well we've asked the robot to do something pretty simple which is just move from the left side of the table to the right side of the table the robot may fall off the table so before it starts actually thrashing around on the table hold still and uses its mental model its forward model to find a way to move so in this experiment there are actually two machine learning algorithms one which you just saw the robot using to build its forward model or its mental model the second machine learning in the second machine learning algorithm the ford model is fixed the robot says okay I know how I'm put together it's now trying to generate a movement pattern that will get it from the left side of the table to the right side of the table and in this experiment here I've skipped to the end of this second machine learning algorithm and this is what that second machine learning algorithm has found so the physical robot hasn't even tried this out yet it just says I think this might work my mental model says this will get me from the left side of the table to the right side of the table so it tells us the investigators it says okay I'm ready to go I think I have a plan we let it try its gate out in reality you can see why I got nicknamed okay so up until this point most robotics most robots did not have a forward model they would thrash around on the table and get better and better and better at whatever we ask them to do what's the problem with that could fall off the table that's one big problem what else what happens if you ask it to do something else now it's got to start thrashing all over again start start from scratch it you'll see there's no wires connected here this robot is running on a battery this was a battery back in two thousand six which didn't last very long it doesn't have the energy budget to be doing that not only is it dangerous it's very energy inefficient and it's not general right it can't learn something something else this robot armed with the forward model we can ask it to do something it'll sit quietly back in 2006 it would sit quietly for three days today it would probably sit quietly for maybe five minutes and then it would do something which may not be perfect but it would be an approximation of what we want it to do what I'm showing you here is actually the first half of a longer experiment so in the second half which we don't have time to get into but you go watch the videos on my website we pulled up a leg off of the robot so we simulated damage this was a project was funded by NASA NASA's always worried about what happens to their robot probes if the probes break or the rover's break when they get to wherever they're they're going the robots leg breaks it doesn't it can't sense the fact that the leg has broken off but the robot does know that when it performs an action it performed before now it's getting different sensory repercussions so it's it used to have a right leg and when it performs a certain action it tilted 40 degrees to the left now it no longer has a right leg it sends motor commands to the motors on the right side of its body and it doesn't tilt to the left at all so it knows something has changed question like as it goes through all this it takes up memory good qualities does it to the Ford models that that aren't writing absolutely it does right so when the leg is broken off the robot knows it's a four-legged robot and as it gets new experiences that don't match the old ones the training set is changing right we're adding new trick new experiences and that old forward model for which it had four legs cannot explain all the new data so eventually the machine learning algorithm throws that old one away and adapts it to explain the new experience and that new forward model is usually a virtual three legged robot the robot has figured out that the only explanation of self that explains all of these new experiences is that I'm no longer a four-legged robot I'm a three legged robot the mission the NASA mission hasn't changed however the mission is still to get from the left side of the table to the right side of the table we don't care whether your four-legged or a one legged robot figure out how to do it so the robot says okay so it stops it's now it knows it's a three legged robot so it starts up that second machine learning algorithm and tries to come up with a new movement pattern that wins supplied to the virtual three legged robot gets the virtual three legged robot from the left side of the table to the right side of the table when it done when it's done that it puts up his hand and says okay I'm ready to go and even though it's damaged it's able to still fulfill the mission it's a little slower and a little rougher than it was before but it carries on okay this isn't a course on robotics I wanted to just use us as an example to explain first of all what a forward model is I do something and I make a prediction using my model about what sensory repercussion is going to occur in four in time in the future where do these models come from they come through interactions with the world I do something and I gets sensory repercussion that becomes a training instance in my training set and I develop forward models that explain that training set once I have a good forward model I can use it to make predictions or mentally rehearse things before having to try them in reality if I download a new app and I don't have a good forward model or a good mental model I've got to physically navigate through all the sub menu items to try and find what I'm looking for kind of doing it at at random right I want to use my model to guide me to find the specific function that I'm looking for in the new app okay the evil starfish has taken up all the time for today so we will start on lecture 11 on friday you have deliverable 7 do tonight along with the quiz good luck 
MMDWvgxXH4I,28,"Instructor: Josh Bongard

Institution: University of Vermont

Recording Date: September 2015",2015-12-07T22:07:52Z,"Human Computer Interaction Class, Lecture 09 of 27",https://i.ytimg.com/vi/MMDWvgxXH4I/hqdefault.jpg,Josh Bongard,PT1H18M5S,false,206,2,0,0,0,okay so let's get started we're going to be shifting gears today somehow the timing all lined up we're switching to a new a new module in the course and we're also going to switch gears in the fourth deliverable now which I'll talk about shortly but just so we have sort of a big picture of where we aren't where we're going we've finished lectures one through eight now where we've been talking about HCI design so we borrowed a lot of ideas from software engineering how to write code but hopefully by now you have a good understanding that there's a lot more to HCI to getting your code to to run without crashing so we're going to switch gears now and starting in lecture 9 we're going to start to talk about cognitive psychology so the particular aspects of psychology that have to do with cognition which is the part of your brain that is being used to make predictions about what's going to happen in in the future so obviously we could spend the entire course or several horses talking about cognitive psychology I'm going to pick and choose a few a few topics to touch on which are particularly relevant to HCI and I've organized our lectures on cognitive psychology going from the most objective to the most subjective so it's relatively easy to pin down what it is about the brain or how it is that we make predictions about what we expect to see next on our phone memory attention and perception what do we really mean by memory and attention and perception things get a little a little hazy here we're going to talk about Michelle perception and frame of reference and finally affective computing or effect which is the emotional side of of your thinking so again as we often do an HCI we're going to go from things that are objective and easy to pin down to the-- that are subjective have to do with aesthetics have to do with the right brain left right okay so that's that's where we're going in probably the next two weeks or so so we'll start lecture 9 in a few minutes but again I want to start by talking about the deliverables I've seen that 25 of you have submitted deliverable 3 that means that there are 12 of you that have not yet submitted deliverable 3 again if you have not submitted deliverable three yet make sure you get it submitted as soon as possible all of these deliverables are cumulative so you don't want to fall behind a couple of things about deliverable 3 that may be holding people up seems that a lot of people had problems saving and loading back in matrices who had problems with the load and save function okay quite a few people some of you found a workaround using pickling this is a nice a nice feature of Python if you don't know what pickling is google python pickling a lot of the different libraries and python if you use a particular libraries load and save commands it has its own particular format for loading and saving things and sometimes it's it's problematic pickling is a way just say I don't care what's in this data structure or in this object just pickle it into a pickle it and save it out to the disk I don't care what the format is and then you can read it back in using I think it's unpick ille right so that this might work if you're still having problems a load and save use pickle and and pickle did anybody use pickle to get them out of this particular pickle okay good if you've already figured that okay if you're having problem if things are holding you up in deliverable three other than loading and saving come and see the TA or come and see me and let's get that resolved as quickly as possible so we can move on to deliverable for any quick questions about deliverable three that I can try and answer okay so just as a reminder I'm sure it's fresh in your mind deliverable 3 the bulk of the work here was capturing the data from leap motion and storing it into these three dimensional matrices right this three-dimensional matrix has all the information your system is going to need in future to be able to recognize whether a student has correctly signed one of the ten ASL digits so in deliverable for we're now going to switch gears and focus on how we can actually get your computer to recognize whether the student has signed the correct sign or not so deliverable for unlike unlike deliverable 3 is relatively short 36 steps and seven pages but for most of you there's going to be a lot of new concepts in deliverable for that you might not have have seen before so again get if you haven't yet gotten started on deliverable for do so as soon as possible okay I'm going to walk you through I'm going to spend the first part of today walking you through what's new and deliverable for which will hopefully help you we're going to do things the old-fashioned way I'm going to work mostly at the board and you're going to take notes these aren't in the lecture slides so we're going to be talking today mostly about machine learning so I'm going to keep a running list of terms here which you may or may not have heard of before so let's start with machine learning so machine learning is a particular field in computer science that deals with how do we actually find patterns in data how will your system know that this particular collection of numbers corresponds to 0 or that this collection of numbers corresponds to the number one or two or three or what have you so machine learning is all about trying to find patterns in data ok so in deliverable for you're going to be implementing a particular machine learning algorithm that does classification so classification refers to any machine learning algorithm that's trying to predict a particular class so I'm going to just use as a cartoon our little cube here so whenever I draw this cube this is supposed to represent a collection numbers that we've collected from need motion and given all of those numbers we want our machine learning algorithm to be able to predict the class which in our case is going to be 0 or 1 or 2 all the way up to tonight which of the 10 ASL digits is the user signing or is it none of these 10 digits okay so classification is when we're trying to predict an integer some of you might have heard about or done regression everybody worked with regression area for linear regression so regression is a different collection of machine learning algorithms where instead of predicting an integer or a discrete class you're trying to predict a real number so we're not going to deal with regression in this class we're just going to focus on classification we're going to focus on a particular classification algorithm known as k-nearest neighbor and I'm going to walk you through the k-nearest neighbor algorithm so basically what the k-nearest neighbor algorithm is going to allow us to do is clarify this arrow here how do we go from this collection of numbers to an integer a correct prediction about what the user actually did has anyone seen the KNN algorithm before a couple people okay so this will be new for mostly okay so I'm going to describe the k-nearest neighbor algorithm to you geometrically with pictures so that you could have an intuition for how this algorithm works and that will help you greatly with deliverable for so let's imagine that we want to create we want to be able to predict not hand gestures but which particular species of flour we found in a field so let's imagine you go out in a field and you start picking flowers and for each flower that you pick you measure two different features so we'll call this f1 for the first feature and f2 for the second feature so maybe when you pick a flower you measure the length of the petals which could be the first feature and you count the number of petals at that flower has so for each each flower that you pick you're going to record two numbers the value of each or lung and the value of feature too and if we then plot an individual flower that's going to tell us where this plot it's it's based on these two features ok let's imagine that we take a flower expert along with us and for every flower that we pick we measure f1 and f2 and our flower expert tells us that's an iris that's a daisy that's a rose that's a particular species of rose and so on they're telling us which flower it is which corresponds to the particular class we eventually want to be able to predict on our own so we're going to try and use Cain years neighbor where we can go out armed with our machine learning algorithm pick a flower and the computer will tell us if we measure these two things what flower it is we don't need the expert anymore this is where we're going ok so features are numbers which describe an individual flower in our case and the class is the thing that we're eventually going to predict so this would be roe's daisy iris or in our case the digit 0 the digit 1 the digit 2 and so on so far so good ok so we picked one flower let's say that this equals rose hopefully you can distinguish Monroe is from another type of flower but let's assume that you can't we pick another flower measure f1 and f2 and our expert also tells us that that's a rose you take a third flower we measure the number of the length of the pedals and the number of petals and our expert tells us that particular flower is an iris okay so we have two different classes now rose and iris and we keep picking flowers we keep measuring the features and our expert keeps giving us back the class to which that particular flower belongs so we've measured this many flowers and we've gotten back this many number of classes now we have a data set in machine learning this is called our training set because we're going to use this particular collection of data to train our nearest neighbor algorithm to be able to predict flowers on its own so that no longer we no longer need the expert okay all right so now we've got our training set how exactly does Kate how exactly does the k-nearest neighbor algorithm learn to predict for a new flower which of these two classes if any it belongs to well let's see how well you do let's say I pick a particular a new flower so the experts gone home we're still out the field I pick a new flower it has this particular petal length and these number of petals which flower do you think it is rose why do you throw I'll put it in as a dashed line to represent that this is our prediction we don't actually know what it is because we don't have our for the roses are that area okay there in that area they seem to be clustered in a particular part of the feature space so you could probably mentally draw some line that last Sue's these these points and that represents a particular part of the future space that seems to be filled with with roses okay you can probably see where this is get going you pick another flower and you measure f1 and f2 for that flower which of the two classes did this flower belong to call the play sorry the IRS right it's probably an iris let's let's predict you pick a third flower here which flower is this hard to say why is it hard to say given this geometric interpretation of our algorithm why is it difficult to know whether it's a rose or an iris absolutely it's in the area where both flowers start to overlap so if we look here and we look in the neighborhood of this flower so what are the other flowers for which we know the classes that happened to be nearby the flower that we're trying to predict there are two roses that are close by and to iris's that are close by so who knows which which one this is right hard to make a prediction in in this case ok so the k-nearest neighbor algorithm as you might be able to guess from now says for any given point that you want to try and predict look at the k-nearest neighbors so we need to define a distance between this point and all of the other points in this case if we say k is equal to four so we for any new flower find the four flowers that could find the four closest flowers and look at which classes those flowers belong to and within that set of four flowers look for the class that's in the majority meaning which has more which would which has more of that particular label and that's our guests okay so we're now basically talking about a model and we're going to come back to this term many times and the rest of this course so in the case of the k-nearest neighbor our model are in all the algorithms the model is the thing that makes the prediction so this is why I wanted to write down all this terminology the machine learning algorithm which in our case is the KNN algorithm is the thing that makes the model and then the model is the thing that makes the prediction is this rose is this an iris so in deliverable for you're going to be creating a k-nearest neighbor model and that model instead of predicting roses is going to make predictions about whether the user just signed 0 for 1 or 2 or 3 and so on so in the k-nearest neighbors algorithm the reason I chose this particular algorithm for our classes it's as simple as possible one it's pretty straightforward it's very intuitive in order to make this model we need to provide the training set so the model is made up of the training set itself if we didn't have the training set how does how does the model going to be able to make a prediction what else do we need in our model for the k-nearest neighbor algorithm to make predictions am i missing in this model so far the distance so that's that's a good point actually won't get into this too much i'll just write this as DM for distance metric so how do we actually measure the distance between a particular point and another point luckily python is going to hide that detail from you you don't need to choose a distance metric and choose one for you that was a good point what else are we missing here what else do we need before our model and start to make predictions like we did in our cartoon example here data to compare it to the training sets yet actually yes so that I forgot to mention that so we also have another data set which is our testing set so in our cartoon example here are testing set is made up of this flower this flower and this flower right so our testing set is our ability to be able to test how well our model is doing how many out of these three flowers does it get does it get right so I'll put this in here as well as T lowercase STR testing set our testing set what else do we need so let's see we have our we have our training set so we have our squares and circles we haven't we have a distance metric so we can measure how far this flower is from this flower and how far this flower is from this flower we want our model to make a prediction what else we missing the classes themselves okay let's assume they're in the training set so the training set contains actually maybe this helps make yourself a training set contains all of the features and all of the classes so that's already in our model it's got the values of f1 and f2 for each of our flowers and the classes for each of these flowers a number of neighbors which is cave ka nearest neighbors okay so armed with all of this our model should now be able to assign a class to each of our to each of our individuals in the test set and when it does it's going to make a prediction so in this case the dotted the dotted circle indicates that our algorithm is predicting arose in this case and an iris in this case and maybe it returns a class saying I have no idea in this case how do we know how well our K n algorithm did in this cartoon example so if I collected all this information about the flowers I put all that information in there i defined a distance metric i pitched three new flowers and i don't know what those classes are and ok how am I going to know how well it did your distances are small or smaller than because if you're comparing the neighbors to your testing distances between them or small that maybe we're pretty good right so if I picked a flower that's out that's out here who knows right might not might not work very well ok if I really want to know how well this model is doing I got to call up my flower expert and have him or her come back in and tell me what the actual labels are for these three flowers right there's no really good way that I can know how good the model is unless I can compare the models predictions against the actual classes assigned by the expert so far so good ok one of the things that makes machine learning so difficult is what should be the value of K so there's some parameters here or in our case it's keep it simple let's just imagine there's one parameter what values should be assigned to K how many neighbors should we look at in order for our model to make good predictions so let me just erase this circle here we looked at K equals 4 what happens if K is 100 given this training set how well is my model going to do I pick some new flowers and call up the expert and he or she comes back and labels those signs classes to those flowers what's that why is it always going to be an iris there's more irises right so we don't have a hundred you're never going to have a hundred neighbors so it'll find all the neighbors that it does have which is all of them and there just happened to be more irises so what if our model is always going to predict iris no matter what right even if something falls right here in field of roses it's always going to predict irises so Ches a little bit too big what about k 1 let's go in the opposite direction as small as possible how's it going to do now probably better definitely better than this right at least it's not always predicting irises right so we happen to find a flower right here we're probably going to do okay assuming we're in the region of irises what if we find a spot that's right here we're very close to our one nearest neighbor which happens to be a rose but I tried to pick that point to be as close to a rose as possible but still kind of in the field of irises right so it's it's going to definitely going to do better but still probably not very well so the black art of machine learning and the black art of using the k-nearest neighbor algorithm well is picking a good value for K you want to look at enough of your neighbor so you're sort of getting a statistical sample but you don't want to look at too many or else you're looking at neighbors that are too far away from okay one last one last thing I want to define here which you'll which will be measuring and deliverable for is error which is how many predictions did you get wrong out of all of the individuals in your your test set and we're trying to get that error as low as possible okay so in this cartoon example I used flowers and that was a non arbitrary choice so in deliverable for you're actually going to be working with a very famous training and testing set which is an iris dataset that was actually collected in the 1930s so they were picking flowers that they knew belong to my biology's rusty I don't know if iris is the genus or what have you but they were picking flowers that they knew her irises but they didn't know what particular species of iris it was so they had an iris expert along who actually identify which iris family these particular individual flowers belong to and they measured not just two features of each flower but four features and you can and you can read up and deliverable for on this data set so why did i draw the picture for you using only two features rather than four features could I have drawn it for you with all four features what's that it's two dimensional right so you'll have to do this in your head now so the actual data set you're going to be working with in deliverable for is a four dimensional feature space f1 f2 f2 f3 and f4 right so we're actually working in a four dimensional space and inside that four dimensional space is a sprinkling of points for each point corresponds to a flower and the shape of that point if you like represents the class that that flower belongs to as I mentioned you're not using leap motion and deliverable for I want you to just build k nearest neighbor algorithm in Python and in order to do so you're going to be using a final python library called Sai kit learn python is wonderful in trying to come up with the maximally cryptic names for their their libraries right scientific scientific kit of algorithms which happened to be some machine learning ones scientific it of learning algorithms scikit-learn psychic learned has built into it a function called KNN and that's the function you're going to be using so a lot of the effort that we just went through is hidden inside of the algorithm you don't need to worry about the internals but you need to be able to know whether you've implemented things correctly or or not if you train or you create your k NN model following the instructions and deliverable for and the predictions that come back is every single flower belongs to iris family number two something is probably wrong right what might that be so you're going to play around a little bit with finding a good value of K for the iris dataset that's what you're doing into the group of four so far so good new questions about KN okay in deliverable five you're going to be bringing together what you did in deliverable three with what you are going to do in deliverable for so in deliverable five you're going to take your canon algorithm which was predicting irises and deliverable for and now it's going to start to predict ASL gestures and deliverable five things are going to get really tricky really quickly why so now we're dealing with leap motion data and not measurements of flowers given this picture what's the first big challenge we're going to come up against yes lots of numbers lots of features right so in the iris dataset we have four features so we're dealing with a four dimensional feature space what is the dimension of the feature space you're going to be dealing with in deliverable five how many numbers do we have we did we did part of this country we did this calculation last week I think more than four all right how do we do this calculation how do we know how many features there are here at least to start with five times four five six thank you five fingers for each finger we have four bones and for each bone we have the x y&z of the base of the bone and the x y&z or the tip of the bone for six numbers per bond giving us a total of a hundred and twenty dimensional feature space if you're not scared by now you you should be yes you brought up I'm sure no problem I can't imagine four dimensions good luck with with 120 expert okay the big challenge we're going to face and deliverable five going forward is reducing this number right why what does it matter even if we can't imagine it as long as we have an intuition for a few features and we bugged our hand and algorithms so that it correctly predicts using the iris dataset we pull out the iris dataset and plug in the data that we're generating with leap motion who cares if it's in 120 dimensions compared to four does it matter okay there's another very famous problem in machine learning you won't go into it it's called the curse of dimensionality the curse of dimension is called the curse because things work nicely in low dimensions by things become more difficult in higher dimensions and every few years a machine learning person comes along and says I've solved the dimensionality problem things behave nicely in 120 dimensions rather than four and their colleagues look at their research paper and they say about on how you made a mistake here it's hard to get get away from the curse it always seems to come back and get us what do you think happens what's what would make our model do well or poorly we know that choosing k poorly we'll make our model predict poorly what else might hobble our model why did it why does it work relatively well in this cartoon example here for a cave of four doesn't always work it works most of the time absolutely right so they're geometrically sup separated right when you start to go up to very high dimensions imagine now so in deliverable where we know in deliverable three you're capturing three or four or five gestures right each one of those has a hundred and twenty numbers if you imagine each gesture that you captured as one point in this 120 dimensional space and you captured a whole bunch of zeros and you captured a whole bunch of ones do you think that the zeros and the ones are going to be clearly separate in this hundred and twenty dimensional space who knows I don't have an intuition about a hundred and twenty dimensions and I imagine you don't either so without this intuition the best thing we can do is try and reduce the number of dimensions we're going to spend a lot of time in the next few weeks trying to reduce this number we brainstormed a little about this last time the first thing we can do is throw away redundancy so there's a lot of numbers in here that are the same what are they the tips and bases that's right so wherever the tip of one bone overlaps the base of another one those two not those two sets of six numbers are redundant we can throw one of them out right so I can't do the calculation on my head but if we do that we compress things pretty nicely and we'll spend time compressing down and down and down we're not going to get down to four but at least my experience last year was we were able to impress enough assuming that we collect a training set from all of you who all have differently sized hands and you're going to capture your data in rooms that have slightly different lighting some of you are going to capture with your right hand somebody you're going to capture with your left hand later deliverables we're going to put all of those cubes together and if we have enough of them there should be a good separation between all the zeros all the ones all the tubes and so on and if we give that combined training set to psych it learns KN n algorithm it should be able to predict for new people whether they're actually signing 0 1 or 2 yeah yes not if the lefties in the room are willing to capture a lot more data for us than the righties we'll get there but yes our algorithm probably no matter how much effort the lefties do for us our algorithm is probably always going to do a better job predicting for righties than lefties right we're already excluding one of our demographics which we said we should never do in HCI why why does it not not matter how much training data are lefties generate for us where are the lefties in this room there's only a couple of them so we give them 10 times as much work as everybody else still not going to be enough why not hey exactly right none of our lefties can grow or shrink the size of their their hands so we're just going to hope that however many lefties we have in the class there's a sufficient spread in the size of their hands that our can and algorithm will be able to do something with with the lefty data okay so you've correctly identified why we're spending so much time in an HCI class about talking about machine learning right if we really want to be careful about not marginalizing some of our users we have to think carefully about what is our machine learning algorithm doing what can our minority demographic give us that will help with this process and so on so is one of the beautiful things about HCI we're thinking about the technology in this case the machine learning blurs into thinking about our people our users you have to think about both carefully to successfully produce an ASL educational software system okay all right just a little bit about what you're going to be submitting for deliverable for and then we'll jump back to lecture here is the actual picture of the iris dataset so however many flowers they actually picked again I can't draw things in four dimensions so I've only plotted two of the four features that are associated with each flower the color now rather than shape represents which of the three irises those flowers actually were that's just the training data so that's the data we use to train the k-nearest neighbor this picture now is my attempt to create a visualization to show the training data which is all the dots that are circled in black and all the dots that are not circled in are the testing data and you'll notice that in the testing data some circles are solid and some of a differently colored page I'm trying to maximize the data to ink ratio what do you think that's going to tell you why did I visualize the testing the testing set in this way okay how is that visualized here yes okay so what does it mean that some have a differently colored edge and some are sold in the fill color is what is most likely so much and then the border very cool it is the game so the color on the inside of the dot is the actual class of the flower so the expert came back and labeled all of the irises in the testing set so the inner color here represents which iris it actually is and the color of the edge represents the prediction remember that our cannon algorithm is always giving us back an integer as a prediction right it's 0 if type of iris or the first type of virus or the second type of virus that's why we have red green and blue girly the three colors so what do the solid dots represent and what do the multi colored dots represent multicolored are Arabs right so these are actually in circled in red as well this particular flower was a red iris and RK n algorithm correctly predicted that it's a red iris this iris down here was actually a green iris but our KN and algorithm failed and predicted that it was a blue iris so in this visualization you can see the training data you can see the test data and you can see the predictions which are the colors and the edges and you can see the error which is the difference between the inner color and the outer color ok given all of that how well was my KN algorithm doing in this case perfect not bad it did better with certain irises than others which irises that into well predicted in fact that a perfect one typewriters which one the red one right so all of the tests data plotted here all the red irises work correctly predicted to be red irises why did it do so well with the red irises and it didn't do so well with the green and the blue irises exactly so here's a blue iris we look at the k-nearest neighbors and if you look at the inner colors kimber what the k actually was in this case it actually is the case that there are more there are more green irises in this set that there are blue irises in the training set the ones that have the black on them so this is always true of any machine learning algorithm if there is not a good separation between the features it's not going to do a very good job being able to predict which class they belong to so this is again one of the things that we're going to spend a lot of time on in the next few weeks and the deliverables is collecting gesture data getting as much of it as we can to help our model and also helping it to separate these data points trying to do it as good a separation as as possible okay so a lot of the steps in deliverable for are going to lead you up to recreating exactly this picture but you're not submitting this picture because this picture reports it reports the first in the fourth feature remember that for the IRS data set there's four features so I picked to it before one in four you're going to be submitting a single image which is the same picture but using features two and three make sense we're being able to tell from your picture whether you're can and algorithm is working correctly so another difference between deliverable for and the previous three is you are comparing your image against or your video against what's in here so here you're submitting an image where we're not giving you what the actual image should should look like so we're really sure that your your K in and out so all of your KN and algorithms are working before we unplug the IRS data set and plug in all of our big motion there make sense ok most of the details of that are written in there there's a lot of pointers in the deliverable two URLs that tell you more k nearest neighbors and the iris dataset which you can read at your leisure okay okay all right back to the schedule for a moment this is a as I mentioned the timing worked out very well this week of this year because we just finished talking about models and now we're going to start a discussion of cognitive psychology by talking about the models that are that are in here there's required and optional reading for today I'm going to draw some examples from a research paper that I wrote a few years ago so the research paper itself is the optional reading it's pretty technical you can have a look if you want we're going to talk about this robot in a moment the required reading is a more popular science description of what we did which somewhat tongue-in-cheek was called what do robots dream of so we're going to talk about mental models for our robots before we talk about mental models for for humans ok that's the required reading for for today ok so let's start with our robot here which when we built this ten years ago we turned it on for the first time we turned it on in a research lab and there was a student sitting like a nearby who turned around and said dude that's the evil starfish and the nickname is stuck so this is the the evil starfish robot and so I'm going to talk you through how we built this robot why we built this robot we built it because it's going to develop the robot is going to develop a model of its own and use that model to make predictions how does this work well we turned on his robot very much like most of us when we were born we didn't have any real mental model we weren't able to make predictions about what was going to happen to us a second or a minute or a day into the future hopefully now you can a little bit better about making predictions what's going to happen to you in the future so it took us growing up from infants into adults to build up these mental psychologists are still not really clear about how we do that but we tried to do it for a robot so at least we could make this explicit so I'm going to show you how this robot builds up a mental model and how the robot and uses this mental model to make predictions and get around in the in the real world okay so that's what the robots looks like we're going to focus on just two sensors so this robot is two sensors on its main body which record how much it tilts to the left and the right and how much it tilts forward and back so every time the robot moves there are these two tilt angles which change right remember when you're talking about John Dewey you push against the world or in our case the robot moves and you sense how the world pushes back right you do something and you see the sensory repercussion of that action our robot does the same thing our robot moves and those two tilt angles change okay we told the robot a little bit we told it that it was made up of these nine pieces look kind of like Lego and what the robot was trying to do is figure out how these nine pieces are put together so our robot doesn't have a camera you can't see itself all it has are these to tilt numbers whenever it moves how does it know whether this is a good description of itself or this or this so our mental models for this robot are basically just how are these nine pieces put together very simple mental model okay so how would it know whether any of these given models is correct so here's one mental model here how does the robot know how good this particular model is at making predictions about what's going to happen next well does so in the following way so the physical robot up here receives motor commands so it comes up with an idea of how to move John Dewey said movement is primary move and see what happens so the robot decides how to move it take it away and I'm going to play a video for you in a moment you can see this in action the robot decides to move in a particular way and that generates some sensor signals which are just these to tilt angles the robot begins by generating a random mental model so it puts these nine pieces together randomly and then execute the same motor commands it did in reality so up here the robot has eight motors and down here it also has eight lawyers for motors at the shoulders at the four shoulders and an additional four motors at the 40 works ok so the simulated robot sends the same commands to its eight motors that the physical robot sent to its feet motors and the simulated robot gets back to tilt angles which is represented here as sensor signals by armed with these two pairs of numbers how does the robot know our good this particular mental model is given this particular mental model what do you think the relationship is between sensor signals and sensor signals Prime that it's good right exactly so if this set of tilt angles east to tilt angles match these two tilt angles that's pretty good the mental model made a prediction that matches reality right not unlike we just talked about here with our cave nearest neighbors our robot is not running the k-nearest neighbors algorithm it's running a more sophisticated algorithm that we're not going to talk about today but it's the same the same idea let me try and play this see look more large so you can see it all right here we go ok so the robot is going to start by generating internal models so here's our robot in its infant form the robots just been born and it chooses to move randomly this is how a robot moved at this point our robot doesn't know anything about itself so what you're going to watch in the next few seconds now is the robot very quickly sifting through lots of different kinds of mental models how are these mental models are they matching with reality you don't look anything like the robot whatsoever right turns out however that these terrible mental models are actually producing similar sensor data it's actually matching what the robot got in reality how is that possible I just finished telling you that if the mental model doesn't resemble the physical robot then there shouldn't be a match between these sensor signals that's it our physical robot has only been alive for three seconds right it just did this and got back to tilt angles with just this one piece of three second experience in the real world there's an almost infinite number of mental models that that can reproduce that sensor data ok so our robot it basically has all these different mental models that are all completely different and they're all matching its physical experience so the robot knows wait a second I only have one form and the fact that all these mental models are different means something's wrong so if you were the robot what would you do next move again right so you'll notice the text down in the bottom left so what our robot actually does is move for about three seconds and then think quote-unquote search through all the possible models that can come up with looking for models that produce the same sensor data that it experienced in reality move think move think move think goes back and forth I just showed you in this video the first movement and the first round of thinking instead of boring you with the second through the seventh I jumped forward to the eighth so this is the eighth time that the robot has moved and when I unpause the video we're going to go back inside the brain of the robot and you're going to watch what it comes up with okay so now after eight experiences with the real world this is its best guess about how its put together how is it doing how is its mental model pretty bad if you watch carefully at just this moment in the experiment it hit on this model and said oh wait a second i just found this mental model and this is explaining much more of these eight experiences that I've had than any of the models I've seen so far this is really good it's not perfect it's explaining most of the eight experiences and half a second later it hits on this one and says this is the best I've seen so far I've jumped for it again now to the 16th and final cycle so it's now moving for the 16th time and now this is what's going on inside its quote-unquote brain it's converged on just this one kind of mental model that explains all the experiences has had so far so by interacting with the physical world it's generated enough data and used a learning algorithm to learn what this model is it has an understanding of its own body well who cares right it has a model but what good is the model the reason we've gone to all the effort of creating a robot that can generate its own model is it's going to use this model to make predictions so now that the robot has learned something about itself we're going to ask the robot to do something and in the video I'm going to show you in a moment we've asked the robot to move from the left side of the table to the right side of the table not a very difficult task and before the physical robot move the robot just SAT there and said okay I know what you want me to do I'm going to think about it so it's taken its mental model and it's now mentally rehearsing if i rotate motor one then rotate motor three then seven and two then three then two and three if I rotate my motors in that way I think about moving in that way I will actually move from the left side of the table to the right side of the table so our robot is mentally rehearsing or predicting what set of movements of its motors will cause it to do what we want it to do which is moved from the left side of the table to the right side of the table okay so it took a few minutes to come up with this way of moving and now tries it in reality okay so what I've just walked you through is a relatively complex experiment the details of the experiment for today are the optional reading but I want what I want you to take away from this experiment as you've just seen a relatively simple machine that builds a model of itself and then uses it to make predictions so in HCI when you create an interface and your user starts tapping buttons or swiping the screen they're doing something not that different from what the robot is doing I wonder what the system will do what sensory repercussion I will receive if I do this and is that matching my expectation yes or no as it does i start to build up a model of what i think the system will do if i do this or if i do this and I can start to without having to actually do it I can think ok I want to place a call and I want to attach a photo to a text message I can think about how I'm actually going to do it by using my mental model of the smartphone or the app or whatever this is what your users are doing all the time and this is one of the most fundamental building blocks of cognition push against the world and see how it pushes back build up a mental model and then use that mental model to make predictions before trying it out in reality ok that's all i have to say about the evil starfish any questions ok all right after all that back to back to HCI ok people differ and because people differ they build different mental models and the kinds of mental models they built depend not just on who they are but what they want to do so you can kind of tell how data this slide is by me talking about an mp3 player your grandmother wants to buy you an mp3 player she goes to best buy and ask somebody does this thing play songs a person says yes it's an mp3 player if it doesn't it's not right your grandmother might build up a very different mental model of what an mp3 player is from what you consider an mp3 player you might place certain devices into an out of the mp3 player class differently from how your your grandmother might might do so okay I love this example here of a discretionary user you get a new phone and you don't want to learn all about the phone but you want to know some basic things and hopefully if your student one of the first things you want to learn to do is turn off the new message alarm right where is that particular function you've used smartphones in the past so you already have a mental model that predicts somewhere there is probably something that allows you to turn on and off the new message alarm where is it is it in the is it in the menu hierarchy for messages where is it in the menu hierarchy that's rooted at options in some phones it's under messages on other phones it's under options where might your user what why might you use your go looking in messages rather than options or vice versa when you get a new phone where you go looking for that function if you get the new iphone and you had a previous iphone where you go looking I don't have an iphone so I don't know options okay imagine that in the new iphone that option was not an options but was in messages right you have a lot of irate Mac people emailing that Apple so your mental model is obviously based on your experiences right in particular on the systems that you've used in the past we've talked about this quite a bit already right people predictions bait and they're looking for consistency and often it's conceptual consistency so if I'm using an Apple phone I expect certain things to be under options and other things to be under messages and so on and so forth be be aware of those things if you have an early adopter you have a geek they'll go through and find every function they'll systematically go through all the functions in the phone right different people are doing different things with the sister your new HCI system and they're building up different mental models okay we just saw this with the robot how do you build up a mental model well you push against the device and see how the device pushes back so i put my smartphone in this box what was this box called I'm singings box a few times by the Skinner box right so we supply some stimuli or our device receives some stimuli from outside which is usually from the user and the device provides response and like skynyrd in in the early 20th century collected a whole bunch of stimuli and responses and tried to use this set of sim you lie and responses to describe a model of the animal's behavior those in the Skinner box users instinctually do the same thing I'm going to click on all these different things with my new smartphone and see what responses I get back okay and usually it's rote learning that this function produced this response this function produced this response and so on and hopefully after you've done that you start to do some conceptual compression instead of memorizing all of where the functions are you're building up probably without realizing you're doing so an understanding of where things are so if you go looking for a new function you'll be able to make a good pretty good prediction about where that function will be in the menu menu hierarchy unfortunately some users never get beyond the stage several years ago my father got his first computer and was learning a word processor he hates computers with a passion he told me just write down on a sticky note if I want to save a document where do I go if I want to load a document where do I go if I want to print something so my poor father his study is now littered with these sticky notes right and as you can imagine every once in a while I got a call from my dad saying how do I do this it's not on the sticky notes right because he's never done any conceptual compression he has no way to generalize right if you do rote learning you can't generalize and you can't make good predictions about novel things if you want to do something new with the word processor if you've just memorized all the functions you're you're at a loss right so why do we do this conceptual compression and again you might not be aware that you're doing it but if you do it well you turn on your phone and there's something new you want to do with the phone you're able to do it correctly the first time because you're making a prediction using your mental model of the system last week we were talking about user evaluation if you want to know whether you user actually built up a good mental model let them play with your system and then ask them to do something novel with your system and if they do it correctly the first time without referring to any help pages you they built up the right mental model and they're using their mental model in in the right way most people when you corner them and really ask them to describe why they like this interface over this one they'll end up telling you it's because I knew where to look or I knew what to carry out that function so one of the most important things you're doing an HCI design is building a system that helps your user build up a mental model to take all of the raw data that they collect by interacting with your system and compressing this down into the right mental model hopefully as the designer you already had a mental model we're going to create a new interface for the phone and all the functionalities for this new phone are basically going to be lumped in these two different things placing calls and sending texts that's the basic clumps right hopefully your user experience in your system will build up a mental model very similar to yours if it's not then they probably have the wrong model and they won't be able to make predictions but go looking in the wrong place for novel functions okay so this is a great example i hope to drive home this point so your user your user builds up a mental model by interacting with the system by providing stimuli to the system observing the response of the system and on the slide here it says building hypotheses that it could be mental models or predictions and it's hard for you to introspect and catch yourself building a mental model and the Necker cube is a very famous optical illusion from psychology that was designed to help you catch your brain in the act of forming mental models so I want you to in a moment ignore my voice just focus on the Necker cube relax and let your brain do what it does best which is build up mental models or build hypothesis alright let's start with the question on the slide which squares closer to you the top aha we have a categorical answer you know you meaning the front of your brain your cerebral cortex knows that neither square is closer to you right you know this is a two-dimensional image but the rest of your brain will not accept that answer right so as you were staring at the Necker cube what was the rest of your brain doing it was trying to translate it into the 3d object right your retina is a two-dimensional surface and there are photons falling on it all the time so you are also actually even though your embedded in a 3d world you're receiving 2d images one of the most important functions of the mammalian brain is taking two dimensional inputs and trying to interpret it as a three-dimensional seen how so your brain was trying to interpret this as a three-dimensional image hopefully it had a hard time doing so why the Necker cube is designed to give that part of your brain a hard time exactly right so given this picture there are two mental models that your brain is trying to validate right either this square is closer to you and this one is further away or this one so which of those two mental models is correct neither all the angles here are chosen very carefully to make sure that there's just as much evidence supporting mental model one as there is supporting mental model too so hands up if you saw both cubes were their split seconds where you actually thought it was one or the other right that's the back of your brain winning out over the front of your brain knows it's neither most of you might have experienced them alternative anybody feel that the cube was doing this ends up if it did okay that's good how quickly was your brain going back and forth between these two mental models is it doing it every tenth of a second every ten seconds about every second most people answer about every second so mystery why turns out that one second is seems to be this magic frequency of the human brain there are certain things that happened the brain at one second intervals and so when you present people with the Necker killer without trimming so there might be some psychology students here but assuming that you haven't been asked that particular question about the Necker cube before most people experiencing experience their brain going back and forth between these two mental models at about one every every second right so your brain is basically saying it's mental model one there's some evidence supporting it wait a second there's a second mental model and there's some evidence supporting that one no wait it's this it's this is this so the Necker cube is designed to allow you to experience your brain trying and failing to form and validate one mental model over over another ok so again why does this matter for for HCI I'll be aware that your user is doing this and they're probably not aware that bit of doing this this has been put into a cognitive architecture so an explanation of petit one particular process of human cognition be aware that no one really knows what the correct cognitive architecture is so when I say your brain is doing this and then it's doing this put mental quotation marks around all those things but it feels like your brain was doing that who knows what your brain is action ok so your users doing something that feels like this or results in predictions and now how the brain is actually doing we're not going to usually that process of forming mental models is in service of a particular goal your users trying to figure out where is the turnoff messaging alarm option there's different things different goals if your user might be trying to achieve given your interactive system and given those different goals there's different ways they're going to push against your system and observe how your system which is bad so this particular flow that you can find in your textbook is named after Donald Norman's seven stages models activity you can also imagine six or eight doesn't doesn't matter too much but the basic flow is here right in this cartoon here we're assuming that the users trying to build up a mental model the same way that your brain is trying to build up a mental model of the system in order to build up the mental model you're going to have to interact with the system so you form an intention to act or push against the object and you come up against the Gulf of execution which means if I want to learn about the system what should I do and choosing what to do or how to interact with the system can be non-trivial a lot of people will just give up and do things at random or do things systematically it turns out that there are things you can do that are much better than that we'll come back to that in a moment so you want to build up a mental model you decide in order to do so you're going to have to interact with the system you decide on how you're going to interact with the system you do and then you see how the system pushes back you interpret the perceptions you receive and now you come up against the second gulf which is known as the Gulf of evaluation so you'd show you push against the system and it pushback does that result tell you anything or take you anywhere any closer to your goal so if our goal is to build a mental model or whatever our goal is the Gulf of evaluation is what should I do I just turned on this new app and the screen is blank what should I do next that's go over that aye sir that's the Gulf of execution what action should i perform what is it that I want to do with this app when I did something it gave me back a result am i learning about the system or not am I getting more confused or am I starting to build up a mental model this is just a more detailed way to get one of the challenges of HCI which is presenting data to the user or supporting an interaction which allows them to cross these two gulfs the moment they turn on their app you should at least give them some information about what they should be doing should they be tapping the screen swiping the screen pressing a button and if they do show them something so they say oh I get the system I see how this works I get the I get the idea right I know what the interaction is going to is going to be okay so I'll just finish with this slide for today let's come back to the robot for a moment so the robot had the same problem right it was trying to build up a model in this case was trying to build up model of itself so it also had to cross it had to cross these two gulps so let's start to let's start with the gulf of evaluation how do I know whether my actions in the case of the robot moved and kremser to explode we already talked about that we told the robot you can cross the Gulf by comparing these sensor signals so see if you can find a mental model that gets these two sets of sensor data coming back from the physical robot in a simulated robot to to match if you if the physical robot is moving and it's collecting new sensor data and it's looking through new mental models and those mental models are getting closer and closer to reality they're matching the robots physical experience then it knows that it's doing the right thing whatever the physical robot is doing it's getting closer and closer to its goal we got one minute left this is a this is a tricky one Gulf of execution so the robot moved once and came up with all these mental models a whole bunch of them are all different but they all matched the robots physical variants what should the how should the physical robot move the second time this is very tricky to cross this particular Gulf the robot could do something at random and it would get some new data turns out that the robot can do something much better than random and so can humans what you do is not to move randomly but move in a way that causes your current mental models to disagree this cartoon here let's imagine that the robot moved once and it's now come up with these two different mental models that both explain the first action robot says ok I get to move a second time how should I move so it takes the second action and before the physical robot moves in reality it feeds that action to both of these robots this robot tilts to the left and this robot tilts to the right this model is predicting a physical robot if you move in this way I predict you're going to talk to the labs the second mental model says no no no you're wrong if you perform this particular second action I predict you're going to tilt to the right it can't both be right the road the physical robot is going to carry out this action and it's going to either toptable after terms on the right or tilt in some other way and these models are going to be won at least one of them is going to be wrong which is good because it's trying to find the correct one it helps it mete out the good models from the bad ones so in the video that you saw the evil starfish was not moving randomly it moved once produced some mental models and then shows another way to move to distinguish between these models I didn't notice it so much today but a lot of people when they watch the Necker cube they move their head subtly you're actually moving in a way to try and figure out which of your to mental models is wrong and unfortunately there is no way you can move your head to figure out which is right because right okay I think we'll stop there we'll talk about these two goals again on Thursday there is a quiz that's due at midnight tonight get a jump start on deliverable for because there might be a lot of new material for many of you thank you 
-1wZbed60Ws,27,"taster for course at CHI 2017

http://alandix.com/statistics/course/",2017-05-15T09:54:08Z,Making Sense of Statistics in HCI: From P to Bayes and Beyond,https://i.ytimg.com/vi/-1wZbed60Ws/hqdefault.jpg,Alan Dix,PT31S,false,262,0,0,0,0,p-values confidence intervals Bayesian stats what does it all mean this course will help you make sense of stats we will focus on understanding concepts and ideas you will learn how to make the most of your empirical effort and avoid misleading results most important it will put you in control of your data 
xZyt2ADdUzA,28,"The timeline of computing history is marked with innovations—vacuum tubes, the mouse, the device in your pocket—that enabled new ways for us to interact with our computers. The language we invent to interact with these innovations is the essence of human-computer interaction (HCI). Today, technological innovation happens at a pace that makes it difficult to wrap our minds around the long-term implications for society, politics, and ethics. 

In this talk, Irene traces the history of the HCI field and explores what the future might look like given the latest advances in computing technology.

Watch more talks with design leaders here: https://www.invisionapp.com/webinars/",2019-05-07T16:01:40Z,InVision Design Talks —  The Future of Human-Computer Interaction with Irene Au,https://i.ytimg.com/vi/xZyt2ADdUzA/hqdefault.jpg,InVision,PT45M35S,false,3889,89,2,0,2,okay hi everyone thank you for joining design talks feature of UX design this is the fourth and final session in our week-long webinar series on trends shaping a field of UX my name is Chloe gray and I'm on the envision marketing team and I'm really excited to host today's webinar with I read out before we get started I want to go over a few housekeeping items so we'll have ten or so minutes at the end for Q&A so make sure to send your questions through the questions box or post them on Twitter um if you're posting on Twitter please use a hashtag hashtag design talks we're actually going to be giving away a free t-shirt a free envision t-shirt to one person who tweet it's their favorite insight or quote from the webinar using hashtag design talks so make sure you are sharing on social as well without further ado I'd like to introduce a speaker Irene Oh Irene is design partner at Khosla Ventures where she works with CEOs from early to late stage startups she is dedicated to raising the strategic value of design and user research within software companies through better methods practices processes leadership talent and quality previously Irene built and led the user experience design teams at Google Yahoo and Udacity and authored the definitive O'Reilly book design and venture capital Irene I'm gonna let you take it from here and turn on your video I'll just pass you the presenter mode okay great hi everybody we're just gonna do the handoff here yeah one sec okay hi everybody um so thanks for joining this morning just to recap so my name is Irina and I'm a design partner at coastal adventures and as Chloe mentioned I started my career at Netscape and went on to build the human centered design practices at Yahoo and google Udacity my background is in human-computer interaction and so I thought today I'd like to share with you kind of how the whole field has evolved and really been defined by advances made in computing technology so in order to see where we're going I just wanted to kind of go back to where we where we've come from and look at that evolution I'm going to turn off my video so that you can see the full screen hopefully this still works okay so what were the advances made in computing technology that have kind of defined human-computer interaction see so innovation in hardware and software has enabled us to interact with our computers in different ways and when we define new languages to invent with these innovations that's really the essence of human-computer interaction technological innovation is happening right now at such a pace that we can hardly wrap our minds around the long term implications for society ethics politics but but let's look at where we've been to see where we're going so in the early days of computers the first electronic computers used vacuum tubes as switches to represent and control the routing of this so here at the language that we use to interact with the computers was literally physical switches that were manipulated for computation these computers occupied entire rooms this is a photo of ENIAC which was among the earliest electrical general-purpose computers that was made this is another photo and I just want to note that most of the computer programmers at the time were women and this is how they programmed the computer we eventually moved from vacuum tubes to punch cards as a means of input into punch card computers which would execute the programs this is an example of such a computer that my own mother used to process payroll data and coding was literally done by hand on sheets like these which was still a physical endeavor and a manual process during this phase of computing the goal of human-computer interaction was simply to get the data processed we weren't interested in usability or desirability and then in the next phase there were three critical inventions that led to this next phase of computing really it was the transistor the integrated circuit and the microprocessor and these inventions enabled computers to be much smaller faster and cheaper and more efficient than the computers that were built with vacuum tubes and this began the trend towards miniaturization that continues to this day these inventions drove the birth of the personal computer the first personal computer was introduced in 1975 by IBM it had a 16 line by 64 character display that ran a computer language called basic and command-line interfaces where the primary means of interacting with computers first on terminals like these the deck vt100 and then later on PCs running UNIX and ms-dos the interface would consist of a command-line shell which would accept commands as text input and then convert the commands into corresponding operating system functions the advances made in computing technology created an opportunity for people to interact with computers in a different way we moved away from physically manipulating bits to interacting with computers in a digital space on screen and then with the invention of the mouse by Doug Engelbart and the graphical user interface or GUI this revolutionized HCI and it was really during this phase that the field was born early attempts at graphical user interfaces were still morphic and literally tried to replicate the physical world even in its most inefficient ways the most successful style of interaction design has been what we refer to as wind which stands for windows icons menus pointer wimp interfaces are so ubiquitous that this is really what comes to mind when we think of the term graphical user interface although not all GUI czar wimped systems so in the previous slide I showed you these are screenshots from general magics magic cap interface and you can imagine like if you need to send an email you need to sit at a desk if you need to go access some application you go into the hallway to get into a different room horribly inefficient and really didn't work for people the the language of HCI in a whimp world is direct manipulation complete with nouns which are represented by icons and verbs which are represented by menu items and buttons and with a GUI and a mouse people could under up they could operate under a paradigm of hey you icon go there into that folder or app and that was really the mental model that was established the Macintosh which exemplified the ideal GUI launched in 1984 with principles that emphasized directness user control and real-world metaphor the goal of HCI at this time was to leverage the pre-existing paradigms of the physical world without taking it too far and make it user friendly now with smartphones the birth of the iPhone and the Android based smartphones put the power of computing in everyone's pocket and by using our fingers instead of a mouse direct manipulation is even more direct than with a whimp interface we swipe to scroll we literally drag objects by dragging our fingers it's still direct manipulation but the language has evolved to meet the form and capabilities of the computing device and with the power of once used what what once used to be a supercomputer now in the palm of your hand we're able to enable people to access computing technology from anywhere now whereas in the first phase of computing we were bringing the physical world into a digital space sensors enable us to use natural movement to bring a digital world into a physical space the Xbox Kinect made playing video games a physical endeavor the Fitbit digitized our physical activity ring allows us to talk to visitors at our door even when we're not at home and nest made our thermostats smarter by adapting to our usage patterns in the home I'd like to highlight a few more companies that are doing interesting work in this area particularly in health tech so there's a company called siren care that has created smart socks that use temperature sensors to detect inflammation in real time for diabetics diabetes patients are prone to foot swelling and it can lead to infection or amputation if the foot is not checked so with siren care they we've conductive thread into the fabric of the sock to detect when there's inflammation and all that information is then uploaded to an app on your smartphone to alert you if there's an issue a live core offers a mobile fda-cleared EKG monitor targeted towards patients who have atrial fibrillation afib patients have an irregular and often rapid heart rate that can increase their risk of stroke heart failure and other heart related complications a life court offers two ways for people to monitor their heart they have kardia mobile which is an attachment for your phone and cardia watch which is a replacement wristband for your Apple watch a notification is sent to the user to record an EKG if the heart rate appears inconsistent with the user's activity level now theoretically it could go further all the data is stored in the cloud and a live core can mine all that user data to make predictions based on what they see so for example if your EKG reading mimics the pattern seen in other patients hours before they have a heart attack a live core could alert you to the danger and advise you to take necessary precautions to avoid the heart attack so as computing moves beyond screens beyond devices with screens the language of HCI similarly evolves how do you design without a screen as we bring the digital world into a physical space how do we deal with the ethical and privacy issues surrounding constant instrumentation and mass collection of personal data as sensors become ubiquitous we increasingly track every aspect of our lives we may reach a point where computers know more about us than we know about ourselves already researchers have been able to successfully infer what kind of personality a user has just based on their keyboard and mouse use and another team has shown how just by looking at mouse cursor motions they can read the users emotions already Google and Facebook algorithms know exactly how you feel and countless other things about you that you may not even suspect Facebook only needs ten likes from you to outperform predictions from work colleagues about your preferences personality and dispositions and if you have clicked three hundred likes on your Facebook account the Facebook algorithm can predict your opinions and desires better than your own life partner now as much as we need government regulation to help consumers protect their data and privacy the cat is kind of out of the bag and even if tech companies change their financial models governments will use these same techniques to monitor and control their citizens imagine if some company or government entity read every email text message or post that we ever wrote every photo we ever took and combine that with knowledge of our heart rate blood pressure hormone levels all in real time as we move through everyday life this entity would be better equipped to tell us how to vote what to choose how to spend our time far beyond what we consciously are aware and in the same way that we relied less on our sense of direction and navigation because we have GPS we may decide to stop listening to our own feelings and start listening to these external algorithms instead and this could lead to a scenario where algorithms would be better equipped to kind of vote based on your behalf instead of having elections in China the government is testing a new way of social control that ranks citizens based on their social and financial behavior this is an example of the facial recognition system used there the social credit system uses facial recognition to evaluate individuals day to day behavior and generate a credit score for each person so that minor violations like jaywalking or jump at the transit ticket line could decrease one's social credit score in the system thereby making it difficult to get a loan or travel abroad and even manspreading on the subway in Beijing might reduce one Social Credit under a new proposal that's being considered but it's not just China that's using technology to create an authoritarian state with surveillance in the u.s. gaggle a company that sells and creates products for schools screen students electronic documents communications and calendars for things like bullying violence and suicidal ideation and the ACLU and the ESF have expressed concern that surveillance leads to false identification of students as safety threats which exposes them to harm and an erosion of civil and intellectual liberties a strict privacy laws in the u.s. may protect us from this black mirror inspired future but conversely they may also hinder us from making the best predictions for customers when it comes to making decisions around our health based on a genetic database and since this is all based on statistics the size of a company's database is key to making accurate predictions so in this sense Chinese disregard for privacy may actually give them an advantage in the area of health and medicine over the u.s. now AI researchers and robot asses are testing the limits of how much machines can learn to mimic and respond to human emotion these are the big questions the world of HCI seeks to answer now how do you get a machine to think and act like a human in 2011 Apple ruled out Siri and Google now launched in 2012 marking the beginning of a new phase where apps use natural language to answer people's questions we now have fast none of computers and data where we can actually train large neural networks and as we train them with more and more data the performance approves improves accordingly in 2015 a 31 year old man named Roman mother ANCA died while crossing the street in Moscow and his death was a devastating loss for his best friend Jan Yokota and at the time Jenna had been building a messenger bot that could do things like make restaurant reservations when Roman died she fed her text messages that she had exchanged with Roman into a neural network and created a bot in his likeness and as a memorial to her friend could have replicated his personality and could talk to him again via a chat bot and thus this company called replica was born this chat bot uses a deep learning model which learns to mimic how humans speak in order to simulate conversation the version of replica that exists today is used by people who seek emotional support it's not a substitute for Siri or Alexa or Google assistant people are using replicas to talk about their feelings in their lives what does it mean to be human when we have technology like this our organisms just algorithms and is life really just about data processing is consciousness valuable what about love or human connection what kinds of relationships do we want to have with machines are they our friends are they our assistants are they sentient beings are they meant to be a long horse or pleasure partner what is the role of the machine and what is the role as a human as machines are increasingly trained to think and act like humans their abilities surpass ours and many of the startups I work with are using human users to train the machine learning algorithms so the machines can do the same work much faster at a much larger scale and as this technology becomes realized the goal of HCI is no longer even a goal unto itself but a goal to be removed in fact one of the executives I worked with boldly proposed a quarterly no you my goal where the goal would be to launch at least one feature per quarter that requires no human user let's talk about augmented reality and virtual reality these systems to date are still mostly concerned theis much like the personal computer was in the 1970s before useful software was invented right now augmented reality is mostly used for specialized applications like flying a drone or for business use such as on the factory floors or in the operating room but there is increasing consensus that artificial that augmented reality will be the next new platform in computing in these contexts of use the goal of HCI is to connect the physical world with the virtual world how might this data on demand overlaid on top of the physical world help people do their jobs more effectively how might these systems be designed so that they can be used in a hands-free environment the software for AR and VR systems continue to evolve and thus the language for interacting with them varies but all of them leverage natural human movement mediated by a screen and sensors and whereas AR aims to connect the physical world with the virtual world VR aims to bring humans into the virtual world virtual reality stands to occupy people's time and attention either as entertainment or as an escape from reality or both and in worlds where robots have taken over most jobs virtual worlds may be the place where people seek a sense of meaning and purpose for their lives with current AR and VR interfaces the user sees the world through a screen and the computer overlays an interface on top of that view I'm particularly intrigued by the converse where a computer that can see and understand your movement can augment your experience of the real world without a screen so for example there at home workout in devices like mirror which is on the left and tonal which is on the right here they currently stream workouts at home but imagine if these devices were equipped with sophisticated computer vision where they would be able to see the user working out and coach the user on their form and tailor exercises for them I believe experiences like these are more representative of augmented reality interfaces where your regular everyday experiences are in half by computing technology that is ambient it can see you it can hear you and it can communicate back to you maybe not even necessarily with the screen now no other invention has the potential to radically change human-computer interaction than brain computer interfaces today all the interaction with a computer is done via a device between you and the technology whether it's a mouse keyboard joystick screen or sensor or microphone by removing the device and directly decoding the signals from our brains to devices we break through to a new kind of interaction between humans and machines there's one company called control labs that is doing just that the first thing they want to fix is the experience of texting on phones imagine not needing a keyboard to enter text into a machine what if we could send a message to our loved ones from or Pacus with our hands still in our pockets at MIT Media Lab there's a project called alter ego in which electrodes attached to your jaw and face pick up neuromuscular signals triggered when you say words in your head so just thinking about saying those words triggers the device and the device can understand what the user wants to communicate and can talk back to the user via bone conduction making it possible to interact with a computer or a digital assistant or AI without ever having to speak aloud or fiddle with a smartphone it's kind of interesting that they've chosen to detect the brain signals of the thought around speech rather than the thoughts themselves this was a deliberate choice that they made to protect the users privacy and just last week a team at UCSF announced that they have found a way to decode brain signals into speech giving voice to paralyzed people it's a step towards a system that would let people send texts straight from their brains the proof that you are seeing is saying we'll only lose there is literally no interaction with a machine today whether it's a phone a computer a robot that this technology doesn't ultimately turn side down we often engage in research around human augmentation with the goal of curing disease or helping the sick or disabled for example alter-egos creator envisions his technology to be used by paraplegics are those stricken with ALS but momentous inventions are never restricted in its used to only healing inevitably they're used to upgrade normal healthy humans as well human-computer interaction then becomes human computer integration and we may very well see the beginning of a new superhuman caste or a class of people who have access to this technology while others do not and while the thought of a superhuman cast that is altered through nanotechnology and brain computer interfaces can be troubling at this moment we may think if only we could be so lucky as to use technology to help people realize their greatest potential but instead the combination of advertising based business models online social systems overwhelming AI have kind of exploited the worst sides of human nature and led to the downgrading of human behavior and humanity we have seemingly separate problems such as tech addiction teen depression shortening attention spans political polarization the breakdown of truth outrage if ocation of culture the rise of influencer culture these are all actually not really separate issues they're all related these companies have gotten really good at understanding how humans interact with computers and they have figured out how to exploit human nature for their financial gain now in the early days of human-computer interaction and human factors we study we focused on studying the individual human we looked at physiology emotions attention perception and cognition and through human centered design we have figured out how to extract attention from people for company's financial gain we've learned how to manipulate people's beliefs and worldviews and now we're reckoning with what we've built and human centered design is no longer enough the problem with human centered design is that understanding the individual human is not enough we also need to protect what we can't see we need shift from human centered design humans that are designed to humane center design so what does it mean for design to be humane centered that means the design is characterized by compassion and sympathy for people especially for the suffering or the distressed we act in a manner that causes the least amount of harm to humans with humane centric design we go beyond the individual human and into how people relate to each other and how that ecosystem works as a whole we go beyond human physiology emotions attention perception and cognition and consider more broadly how the individual human relates to other people in society as a whole such as sense making decision making social reasoning group dynamics and social environment historically we have viewed AI systems as passive tools that can be assessed purely through their technical architecture performance and capabilities increasingly we are seeing the rise of machines that have their own agency machines that are actors making decisions and taking actions autonomously as a result they are active actors that have their own behavioral patterns and ecology as they change and influence their environments and people and machines around them we need to acknowledge that we are all one giant human machine system and started studying it that way humans and algorithms are both actors and they both relate to each other so for example a machine behaviorist might study the impact of voice assistance on a child's personality development or they might look at how online dating algorithms have changed how people meet and fall in love or they might look at the efficacy of mobile apps and their ability to treat depression without a human intervention ultimately this area study would investigate the emergent properties that arise from many humans and machines coexisting and collaborating together now you may be wondering what you personally can do given the advances in technology and the dominant role it plays in our lives first as this is mostly an audience of designers the champion and agent for humane centered design we need to consider not only how individuals interact with technology but how the human machine system affects humanity as a whole as we design and develop products and experiences and for individuals B&O should be aware that the notion of free will is a fallacy we believe that we're acting freely without understanding that we are prone to manipulation in ways that we may not be consciously aware of so know thyself as now you're in competition with AI that may be developed by governments or companies that know you better than you think you know yourself or that you believe that you know yourself as consumers in a capitalist society we help decide what is work using what is worth our time and attention we all own that and a simple thing that you can do is just to be intentional about where your attention goes take a few breaths anytime you're transitioning from one activity to another when you're using technology Linda stone coined a term called email apnea which refers to the common occurrence of people not being aware of how they're breathing when they're checking email it happens so use this moment of pausing and taking a breath to check in with yourself ask yourself how you're feeling and whether this is actually how you really want to be spending your time okay thank you thanks everyone that was awesome we can move into the Q&A portion let's just turn on my camera and you can turn on your camera as well that was really great super interesting we have a lot of good questions and feel free to continue to send questions into the questions box so first question do you believe that free and open source projects have a potential to be more humane centered than classic business driven companies um potentially because it's built by the people for the people theoretically um so there is sort of a wisdom in the crowd also but the one thing I would worry about is that the people contributing to open source are generally as it is now a pretty homogeneous population and so there is the potential for bias to be built in those systems even if they're open-source projects because not necessarily all populations are contributing to the creation of this open-source project so that would be one thing I would be really conscious and aware of and you mentioned some really interesting concerns that could come about as we continue to blend with technology and blend technology into our lives and someone asks like if you could someone else here like could you just give a maybe like a summary of the top five things that you think people need to be aware of and really cognizant of as we keep advancing with human-computer interaction and do you agree with Nick Bostrom's views and worries about human computer singularity mm-hmm I think that would be a while before that happens I think we're kind of overestimating how quickly that change is going to happen I think the more immediate concerns right now that I think everybody should be aware of are things like bias even the top AI researchers and scientists they will say that the first thing they worry about every single day is bias and how to build systems that because you know because the way these systems are trained is that they're taking data from before and in putting that into training system and so naturally there's gonna be some bias in there but so how do you create these systems so that they're they're trained based on past data but also not inherently building in whatever stereotypes and biases that we perceive so that's one I think another more immediate issue is really an issue around attention and where we're placing it so this is for the individual there's the other which is companies and even some government entities that are manipulating how people are behaving and for the in the case of the companies there is a financial incentive for them to do so and so we may need like new fiduciary models to incentivize companies to kind of align what they're doing with how people really want to live and behave in the meantime like what we've seen now is we're reckoning with a rise in teenage suicide and depression pervasive sense of loneliness polarization all these things are related to the fact that these algorithms are kind of trained to serve up content that gets clicks and the things that get clicks are the things that are like most provocative most polarizing most controversial most salacious and so yes we need some regulation and we need the companies to take charge but it is sort of like an arms race and so there's only so much the companies can do as long as this is kind of my model they have so it's really then incumbent upon individuals to pay attention to how they're spending their time and like how many of us have kind of gotten ourselves lost into the universe of the internet where you click on one link and then on another and another and then you and death watching some videos and the next thing you know you've just burned a couple of hours playing video games or watching videos or reading stuff on the internet so we need to build an awareness around that there's also the issue of privacy so that's the the third big issue that I think is more immediate is individual's data protection right now most of our data is owned either by companies in the case of the US or by the government in the case of China and so like you know is that a good thing should consumer protections be put in place to protect our privacy and like I said in the presentation the cat is sort of out of the bag like even if you have regulation of these companies trees will figure out how to do this as they have and it's sort of an arms race it's like even if you have companies or countries saying well we're the good guys we're going to protect user privacy and data and we're not gonna use this to monitor people or to serve up that content or things like that there will be other countries or entities that will do it and so it's it's you know for things like human augmentation synthetic biology nanotechnology brain computer interfaces like this is gonna happen it's not gonna be stopped and it will become a reality and in that sense it's sort of like a nuclear arms race and so it really requires global cooperation and consciousness across all of humanity to come to terms like what-what is really within bounds and what what is what are we going to pledge to do and not do to preserve humanity in the same way that there were there was mass collaboration cooperation across countries around nuclear disarmament it's kind of in the same situation now thank you rianne's there neo some great questions coming in here so thanks everyone who's sharing these a couple of people asked about the right way to get corporations on board with humans under design and how how can we balance or kind of gets empathy and compassion to take presidents of our profits and market share if you know it seems like they might be competing interests how can we get those to work together in my work with startups conversation that first of all the venture capital firm that I work for very rarely invests in companies that make money off of advertisement advertisements I mean sometimes it happens but usually the focus is on creating some service that is of value that people would be willing to pay for and so that obviates the need for you know this race to the bottom of the brainstem where and you know you serve up content that gets people click on stuff and your extracting attention from people so the first thing I would say is like try to explore alternative ways to make money and create a sustainable business that also offers value to people and I'm hopeful because whereas in the early days of the internet that the expectation was that everything would be free increasingly we're seeing high value services that people are willing to pay for so increasingly people are willing to pay for high quality software you know like people are willing to pay for like Spotify Netflix you know these are content delivery platforms that are by subscription and the quality is so much higher than the the free kind of models so so that would be one is to explore alternative fiduciary incentives for companies and I think that's really key the second advice I would piece of advice I would give is to really invest in user research I there are larger companies that have certainly invested a lot in user research but when I work with startups this is one of the most underinvested functions that can really pay off for a company and be like rocket fuel in delivering insights to companies around what's the best way forward there's nothing like bringing the voice of the user and insights about people to life inside a company that can motivate an engineering and part development team to move in a certain direction or another so and again it's not you know it's not just about doing the research but if nobody hears it and if nobody really understands it then it didn't really happen so one of the biggest challenges around research is not necessarily just around doing it but it's also about communicating it back to the organization in a way that's really effective and and heard great and none this one is about skill set so if if one is currently immersed in an industry that largely involves user facing interfaces how can position themselves as designer to adapt to the future of human-computer interaction as it becomes less UI oriented hmm I don't think that user interfaces are going to go away completely I do think that the nature of design work may evolve this has already started to happen over the past several years as like for example when when the internet when we started having web browsers and websites and web applications like this had never been designed before and people had to figure out what's the best way to design web applications or websites that would be useful for people and there was a lot of user research done around that and then eventually we developed guidelines and then there was like a shared mind around what a what create what constitutes a usable web site and how do you get there and then when smartphones came out same thing happened the design of mobile applications and websites on mobile devices is kind of all over the place and then eventually it kind of converged and now when you look across the design of mobile apps there's a pretty standard way of offering functionality there are only so many ways you can offer a button or menu but a bunch of icons things like that and in the same way I think it's new technology comes on board we're going we'll figure it out as we go now this whole push to maybe remove the UI altogether as I said in my presentation what happened there was that it shifted the activities of the designers to be more like facilitators of the experience so they weren't necessarily designing screens or wireframes but they were leading the cross-functional development team through a journey of understanding the end user and what their goals were what was motivating them and what would make them feel really amazing in their jobs because it's an enterprise company and then taking those insights back to the product managers and the engineers and and then through those insights the engineers came up with the ideas around how to autumn certain features and when would be the right touch points for involving humans to approve whatever the machines were doing things like that and then and then building it from there so it wasn't like the designers didn't have jobs but the nature of what the designers were doing evolved it was really about understanding people bringing those insights back and then facilitating cross-functional collaboration around what's the best way machines can support users in their efforts to meet their goals um we have time for two more questions so how do you recommend that we design for Humanity when creating something novel where the impact on humanity may be unclear for example at the very start of social media I would guess it was not initially clear how it could eventually affect so many aspects of life like depression or attention or cyberbullying now and I think it's really not feasible or practical to try to anticipate every single possible outcome whenever any kind of new technology is rolled out there's a process of learning as you go for any kind of new endeavor but I think that the moment we begin to see and understand that there are mass impacts like companies have to take responsibility and I think that's where we've seen some companies fall short because they have defined their success in certain kinds of terms and metrics and it's a lot easier to stay the course especially when things are going really well to kind of focus on user growth or engagement or time spent things like that especially for media companies these are very classic ways that media companies have measured themselves even predating the internet like radio stations and TV stations have kind of measured themselves in this way and so you know I think going back to my earlier comment like we need to look at what are the financial incentives like whenever a company's is created they should think about where's their money coming from how are they making and are their success metrics aligned with human success metrics and if not maybe that's a chance to kind of re-evaluate what are we building and is there a chance to like push ourselves to do something even better and so that's where it's one thing to like make stuff and experiment and put stuff out and see how it's received and define it but as like as a start-up becomes a company and even as a company matures I think it is incumbent upon the leadership to kind of take another look at how they evaluate success so like just somethings as an example when I worked at Udacity we had enormous debates about whether to allow people to move together as a cohort where everybody starts and ends of course at the same time versus let everybody work at their own pace and a very common metric for massive online open courseware at that time was completion rate and and that was kind of like how Coursera Udacity udemy all of these companies would measure themselves and brag about engagements and things like that we knew from our own internal logs and then also from looking at what other MOOCs were doing that when you move people through as a cohort where there's a definite start and end time for everybody that the completion rate would be higher and and so there was a strong argument for creating cohorts within the Udacity experience but our founder Sebastian Thrun felt really strongly that we should support an experience where people can move at their own pace because there are a lot of people using Udacity courses who were working full-time and they were taking these classes at night there were people who just were high school students and we're learning at a different rate than people who were like mid-career professionals and he wanted to create an experience that would support all those kinds of users and so ultimately it came to a point where he as the founder and CEO said I am willing to sacrifice a high completion rate for being able to offer an experience that is asynchronous and you know we could have created an experience either way we could have moved people through as a cohort or we could have created an a secret learning experience but what was important was that we decided on a certain path and that we were very valued we were very principled about it we knew why we were doing it and that the founder the CEO is making these deliberate trade-offs on the success metrics and maybe redefining what defines success maybe it's maybe you shouldn't be completion rate maybe you should be something else like your ability to find a job that ultimately led to that path so that's just a very specific example of how we put that into practice great and can you share some tips or your favorite resources for learning about humane centered design where you recommend people go if they are either starting from zero or it's something to just want to brush up on or should they or should they start there's no brushing up because it's still emerging and it's just being defined now and it's really a term that's being coined and put out there in into people's consciousness so it's again we're making it up as we go I would say the organization that's probably on the forefront of offering thought leadership for this really is Tristen Harris's Center for humane design and interestingly his co-founder aza Raskin is the son of Jeff Raskin who was one of the inventors of the Macintosh and and Jef Raskin wrote this text book called the the humane interface so that was kind of come full circle there I think that people really need to invest in design research and not just high usability testing but really kind of a call for studying like using methods employed by psychologists sociologists anthropologists emphasis to study the ecosystem the people and how they work with each other and collaborate and make decisions and how they think and again I said this is like the most underinvested area that I see many companies have if they have it at all and it's something that I think is incumbent upon every company to invest in even if they don't have user researchers maybe it means that the product are doing this research or designers but everybody kind of needs to invest in this there are emerging academic kind of research groups that are looking at this and I would say Stanford and MIT are on the forefront of this so at MIT there are now researchers coming together to study the human-machine system and at Stanford we have the Center for forget what they call it exactly the Center for humane AI or something like that led by Fei Fei Li so these are really the two institutions that I look to to see the you know where is the cutting-edge research you know that's happening same thing with things like brain machine interfaces I see that really happening at MIT and Stanford as well great thank you those are some really good tips so that's all the time we have for questions so thanks everyone for attending today and thank you every this is a great presentation we all really learned a lot we will send out a recording of this presentation next week so definitely keep an eye out for that in your inboxes and I hope everyone has a great day 
U7n57Bh6QeU,28,Playlist: https://www.youtube.com/playlist?list=PLAuiGdPEdw0j6VNxfbY-FNlbAjlWIVNnO,2020-11-10T17:32:09Z,"Human Computer Interaction, Lecture 19. University of Vermont, Nov 10, 2020.",https://i.ytimg.com/vi/U7n57Bh6QeU/hqdefault.jpg,Josh Bongard,PT1H14M50S,false,69,2,0,0,0,okay good morning everyone i hope you're doing well um so uh before we dive back into lecture uh congratulations you reached the tenth and final uh deliverable i'll just give you an idea for what you're in in store for uh this week in this uh final deliverable uh you're going to be incorporating uh some scaffolding and as you remember from our discussion uh a few weeks back this idea of scaffolding comes from psychology and it's the idea of helping a learner get started at a task and as that learner starts to gain competency in the task as we observe that growing competency we gradually and incrementally remove the scaffold until they're able to master perform the task without our help so you're now moving into the stage with your final project where your help you're thinking carefully about how to help the user learn and be able to sign asl digits on their own so you're going to be implementing three scaffolds this week and as you'll see deliverable 10 is only a page long because most of the ideas are going to be up to you this week rather than me and the attendance sheet is currently in view mode thank you very much let me fix that for a moment um so you're implementing three different kinds of scaffold this week and uh the way in which you implement that scout those scaffolds uh are up to you um so each one uh is up to you you're gonna be figuring out how to apply it and then gradually uh and then writing up a one or two sentence description of it that you're going to be submitting along with your video that demonstrates that scaffold in effect and it's also going to demonstrate the gradual removal of that scaffold as the user signals or competency okay so what are the three scaffolds the first one is going to challenge the user to sign an increasing number of digits as their performance improve so at the beginning you'll show just the two digits or one or two digits over and over again but as they start to sign it correctly you need to be able to detect that and then you might fold in a third digit they practice with the three once they signal competency with all three you add a fourth uh and so on the second scaffold will increasingly challenge the user to remember the the gesture associated with the digit as they improve so you're going to flash up the image and the digit itself and then over time as they get better and better at signing the digit you're going to show only the digit and not not the image not the image of what they should be signing so they need to remember memorize the gesture associated with each digit and the third and final scaffold will increasingly challenge the user to sign that gesture faster as they improve so once they see the digit 4 we want them to be able to sign that digit as quickly as possible okay so that's uh that's deliverable 10 which as usual will be due next monday and uh the following tuesday the uh next tuesday we'll talk about the interim reports and the final project which you'll be working on for the last two weeks of class before the final exam which for us there is no final exam you're going to be giving a two-minute and 30-second oral presentation summarizing your final project so we have 60 students in the class we have a little less than three hours for our time together uh and so you'll be you'll be presenting your work orally i apologize for the time i realize it's exceedingly early in the morning i don't set the time for the exams but we'll all meet uh very early on tuesday december 8th and we'll be able to sit back and and learn about all the various ways that you've thought up for teaching your users asl any questions any questions about that all good okay so back to our back to our theme on looking outward we're going to finish sort of the first part of this theme this morning with the human speech home project and in lectures 14 through 19 we've been looking at a large number of applications different kinds of technologies and just sort of surveying all the different ways in which we as a society are gradually stitching technology into the world where that technology can directly sense what's going on out in the world we've looked at lots of wearable technologies in the second half of this looking outward in lectures 22 through 23 we're going to be looking at technologies that are embedded out in the world that can directly sense the world but they can also act directly on the world they can push against the world and because they have sensors they can observe how the world pushes back and that technology of course is robotic so we're going to introduce this concept of robots today and we're going to come at robots from an hci perspective we're going to view robots as a technology that takes as input input from the world conjugates on it and produces output back on the world and we'll talk about the nature of that interaction in lecture in more detail in lecture 21 then we'll look at robots that interact with other with people and finally we'll end with robots that interact with other people so robot swarms okay so that that's where we're headed but uh before we get there back to uh the human speech home project which just as a reminder just as a reminder was as is this interesting application of ubiquitous computing inside the home of a researcher who's going to use this technology to study uh his young child and how this particular young child acquires language and this is clearly a very different way of coming at the problem of understanding how young children acquire language because it is not being done in the laboratory setting they are going to passively observe uh their child's uh first few months month nine through month 24 which we'll talk about later and as uh as deb roy mentioned in his ted talk here um the data set that was produced by this is uh was and still is the world's largest family uh photo album our home uh home album okay we got halfway through this ted talk last time and we ended by listening to just an audio transcript of every time uh professor roy's child said the child's version of the word water which was gaga and eventually the adult form of the word and these were uh short audio clips that were taken from different rooms in the house at different times you might have heard in a few of the short clips you could hear running water in the background so the child was either in the kitchen or the or the bathroom so he didn't just learn water over the course of the 24 months the first two years that we really focused on this is a map of every word he learned in chronological order and because we have full transcripts we've identified each of the 503 words that he learned to produce by his second birthday he was an early talker and so we started to analyze why why were certain words born before others this is one of the first results that came out of our study a little over a year ago that really surprised us the way to interpret this apparently simple graph is on the vertical is an indication of how complex caregiver utterances are based on the length of utterances and the vertical axis is time and all of the data we aligned based on the following idea every time my son would learn a word we would trace back and look at all of the language he heard that contained that word and we would plot the relative length of the utterances and what we found was this curious phenomena that caregiver speech would systematically dip to a minimum making language as simple as possible and then slowly ascend back up in complexity and the amazing thing was that the that bounce that dip lined up almost precisely with when each word was born word afterwards systematically so it appears that all three primary caregivers myself my wife and our nanny were systematically and i would think subconsciously restructuring our language to meet him at the moment of the birth of a word and bring him gently into more complex language and the implications of this there are many but one i just want to point out is that there must be amazing feedback loops it's not of course my son is learning from his linguistic environment but the environment is learning from him that environment people are in these tight feedback loops and creating a kind of scaffolding that has not been noticed until now but that's looking at the speech context what about the visual context we're now looking at think of this as a dollhouse cutaway of the of our house we've taken those circular fisheye lens cameras and we've done some optical correction and then we can bring it into three-dimensional life so welcome to my home this is a moment one moment captured across multiple cameras the reason we did this is to create the ultimate memory machine where you can go back and interactively fly around and then breathe video life into this system what i'm going to do is give you an accelerated view of 30 minutes again of just life in the living room that's me and my son on the floor and there's video analytics that are tracking our movements my son is leaving red ink i'm leaving green ink we're now on the couch looking out through the window at cars passing by and finally my son playing in a walking toy by himself now we freeze the action 30 minutes we turn time into the vertical axis and we open up for a view of these interaction traces we've just left behind and we see these amazing structures these little knots of two colors of thread we call social hot spots the spiral thread we call a solo hotspot and we think that these affect the way languages learn what we'd like to do is start understanding the interaction between these patterns and the language that my son is exposed to to see if we can predict how the structure of when words are heard affects when they're learned so in other words the relationship between words and what they're about in the world so here's how we're approaching this in this video again my son is being traced out he's leaving red ink behind and there's our nanny by the door all right she offers water and off go the two worms over to the kitchen to get water and what we've done is use the word water to tag that moment that bit of activity and now we take the power of data and take every time my son ever heard the word water and the context he saw it in and we use it to penetrate through the video and find every activity trace that co-occurred with the instance of water and what this data leaves in its wake is a landscape we call these word scapes this is the word escape for the word water and you can see most of the action is in the kitchen that's where those big peaks are over to the left and just for contrast we can do this with any word we can take the word by as in goodbye and we're now zoomed in over the entrance to the house and we look and we find as you'd expect a contrast in the landscape where the word by occurs much more in a structured way so we're using these structures to start predicting the order of language acquisition and then that's your ongoing work now in my lab which we're peering into now at mit this is at the media lab this has become my favorite way of videographing just about any space three of the key people in this project philip decamp ronnie kubat and brandon roy are pictured here phillip has been a close collaborator on all the visualizations you're seeing and michael fleishman was another phd student in my lab who worked with me on this home video analysis and he made the following observation that just the way that we're analyzing how language connects to events which provide common ground for language that same idea we can take out of your home deb and we can apply it to the world of public media and so our effort took an unexpected turn think of mass media as providing common ground and you have the recipe for taking this idea to a whole new place we've started analyzing television content okay i'm going to uh pause the video there in the second half of this ted talk professor roy goes on and describes how he's taken this idea of connecting language with context who what where when and how and applies it to to media which of course is the age that we're now uh living in with social media analysis but we're going to focus today on the specific application of using ubiquitous technology to understand the origin of language at least from the point of view of a new human being as david mentions their their decade old 3d visualizations still look pretty good you can imagine how how shocking this was at the time so this uh study was carried out in uh 2009 um so uh a lot of ubiquitous technology was still in its infancy uh at that time so this was an unprecedented data set we're going to go back now and have a look at we're going to dive through this data set in a little more detail and then come back to some of the scientific questions that deb mentioned during his talk okay first one as he mentioned um is they instrumented their house with audio and video feed so they have 11 overhead uh cameras that give a fisheye lens a view of each room so they can capture pretty much all the action in the room 14 microphones as well they collected data over the uh over the following period for their child nine to the uh 24 months of age which is known to be the most intense period of language acquisition if you've ever been around a child around this age it's amazing how many new words they absorb and emit during this period and we'll actually see a visualization of new word acquisition over this period in a moment during this period they collected over 4 000 hours of audio and visual which spanned uh 444 out of the 488 days that make up this period so a pretty good coverage they got most of the time 90 000 hours of video 140 000 hours of audio 200 terabytes of data again very impressive for 2009 they got an average of about 10 hours per day so from the child's point of view they got most of that child's waking hours the cat they think they captured about 70 to 80 percent of the child's waking hours of course they're recording not just the child but they're recording the three other caregivers so professor roy his wife and the nanny um they're recording their behavior as well so given the behavior of these four individuals in the home typically five to six cameras were active at any given time here's a quote from the paper that's the reading for today in which professor roy distinguishes his approach from the traditional approach which is theory laden meaning that if we want to understand a given phenomenon which in this case is how children acquire language we might start our scientific investigation with a theory in the case of deb roy's approach they're going to start their scientific study not with the theory but with the data why is it better to have a data-driven approach than a theory driven or theory-laden approach well in a contrast to diary studies which are necessarily theory-laden if we go in with theory since the diarist cannot record everything he or she must rely on theoretical biases to decide what is noteworthy at the time of observation so if you have a theory and you're going in to test that theory you have to decide what observables what are the things you're going to measure who what where when and why in deciding on what to measure you're usually being guided by your theory observations that you think you're going to be irrelevant to the theory or extraneous to the theory you're not going to you're not going to measure so i gave you this example earlier imagine that someone has an idea that children learn language through joint attention the parent looks at an object says the name of that object the child looks at the eyes of the parent infers where the parent is looking the child changes their gaze they secad away from the parents eyes to the object and then they say the name of that object if your theory is that joint attention scaffolds the acquisition of language then you're probably going to measure direction of gaze and audio about who says what when but maybe pointing is also an important point part of language acquisition gesture but in joint attention we're only focusing on the eyes and what is said so we may not record uh who is pointing or what gestures are being made by what individual what time we might lose important information if we come at these kinds of questions laden with a theory alternatively in the human speech-owned project or the speech on corpus the data that makes up the speech home project may be re-analyzed multiple times guided by different theoretical perspectives so if we think that joint attention is important we could dive into the video and try and infer uh who is looking at what or whom at a given time if instead our theory says gesture is important caregivers point at an object and say the name of that object we might dive back into the data again and now look for hands in the data and what gestures are being made with the hands the existence of high resolution video provides opportunities to study the role of various aspects of non-linguistic context so what are all the things that are going on other than language that may be important for the acquisition of language who is looking at what when who is pointing at what and so on so he mentions joint attention here to routine activities and and beyond okay all right so uh that's the that's the reason why we're starting with data rather than starting with a theory what's the data we have on hand well they have raw audio and video data we are going to try and distill out of this raw data uh some some uh some subset of phenomenon so we are a little bit guided by theory here we're assuming that in order to acquire language language itself matters we need to listen for words in the audio transcripts people are probably important professor roy was showing the social hot spots of people uh him and his child that were close together and interacting he was saying something the child was saying something so who is nearby the child when the child says a word or a word is uttered is important activities are important objects are probably important and then from all of this data as we dive into this data and look for correlations between when the child says words for the first time so language acquisition the birth of new words from the point of view of the child correlations between those events and these events will support or disprove various theories we could dream up of language acquisition okay so from the audio we can pull out who was saying what and how so did the child say something where when were they when they said that word did one of the three caregivers say a word and who and what uh where and when did they say it in order to get from raw audio to this we need to do some word level speech transcription speaker identification who is speaking we also as you'll see in this study in a moment they also focused on prosody remember we talked about prosody before this is the way in which things are said how loud was a word spoken how quietly was a word spoken what was the pitch of the voice that spoke that word uh and and various other aspects of the way in which something is said from the video they're going to pull out who what uh where what were they doing how are they doing it and with what so objects as well so we're looking for people activities and objects in the video feed and here's just sort of a flow flow chart of how this goes once this arc is completed then we can pose particular theories or particular hypotheses and test it with the data for example here's one theory when was a word spoken by the child for the first time so within that 9 to 24 month period is the did the first time that the child say the word water was that late was it towards the end of the 24 month period or was it early was it close to that nine month period you could for every new word mentioned by that child during that 9 to 24 month period so we have the the time of onset how early did they acquire that word and we could uh compare it to how often that word appears in the raw audio feed as a whole so basically how often was that word spoken in the home before the child said it for the first time if there is a positive if there is an inverse correlation meaning the earlier the word was spoken the more often that word was heard if we can find such a correlation then in the data set that would support this hypothesis if there's no correlation between the time that the word is spoken for the first time by the child and the frequency with which that word was spoken in the home then that that lack of correlation would be evidence against this hypothesis what are some other given given the fact imagine you had access to this data set what other hypotheses about language acquisition might you propose and what aspects of the data set might you go looking for to test it i'll give you a minute to think about that if you have an idea go ahead and type it into chat so i mentioned joint attention maybe that's important look at looking at where the individuals are looking at the time that a word is spoken or detected in the audio stream you may you may not be able to remember how you acquired language because you were pretty young when you did for those of you that have learned a foreign language and some of you are learning asl at the moment what are some of the things that help you learn the words or the gestures in the case of asl that make up that language who do you learn from what are you looking at when the person is trying to teach you some language what are they doing what are you looking at obviously you might have learned some language from a book so you might not actually be interacting with a person the way that adults learn language is probably different from the way children learn language what are some other aspects what are some other things you might look for that are going on in the home that you might measure in this data set and look for correlations with words spoken by the child for the first time what are some events you think might cause a child to say a word for the first time television right so they have the audi they have the audio transcripts i don't know how good those audio transcripts are if they're able to pick up if they're able to pick up language in uh being language from the television broadcast but yeah that one's that one's probably a big one you can definitely tell from the video feed where the television is in the home so this is the pre uh 2009 this is the pre-youtube era so the child is not watching youtube on a laptop at this point or at least that's pretty unlikely we might be able to detect whether the child is sitting in front of the television when they speak a word for the first time if that happens quite often that would be evidence that would support ethan's hypothesis here that tv helps teach children language if children if children utter words for the first time very often away from the television that would be evidence uh uh against this hypothesis any other ideas what are some other triggers in the home that help the child to generate language for the first time their own basic needs right so what is the point of language at least for children at the beginning it might be to ask for things if all you can do is scream and cry that's fine it'll usually give you your basic needs but as you grow a little bit older past nine months there may be specific things you want that it would be easier for you to ask for them by name rather than to throw a temper tantrum so their own basic needs that's an interesting one how could we pull out from the who what where and who is doing what that would tell us that the child is emitting the word because they want something any ideas if the child says a blanket for the first time let's take bryce's example of blanket we can determine where and when in time the child emits the word blanket for the first time we might be able to then go to the video feed and see a short time later a minute or two later does a blanket actually appear does the child actually get what they want does the object appear shortly after they emit that word if if there is a coexistence between the utterance of the word and the appearance of the object in the word or the object itself that would be evidence in favor of bryce's hypothesis if those uh co-occurrences are rare that would be evidence against this hypothesis okay i think you get the idea okay again this is uh 2009 uh so audio automated audio transcription was not very good at the time um so you have 488 days of raw audio most of that most of that audio is silent or is uh non-speech background background sounds so how do you actually pinpoint the important moments in the audio feed and the video feed which especially in this case in the audio feed are words they did this mainly by creating a whole bunch of interactive tools that allowed the grad students to sift through this vast amount of audio and video looking for who what where and when to support or disprove various hypotheses with the audio they visualized the audio so they made pictures of the audio feed using what's known as a spectrogram so a spectrogram works as follows and i'm sorry for the poor poor image quality here you can go have a look at the paper itself see more details the spectrogram you have time on the horizontal axis here and on the vertical axis you have a whole bunch of frequency bands so high low frequency middle frequency and high frequency and in this case a white pixel at a given point represents there was no amount of that frequency at that point in time if you take a vertical cut through a spectrogram you'll get a whole bunch of colored pixels so colored pixels in this case men colored pixels here represent strength of a given frequency so you can see for example in this little screenshot here there was a burst of sound over this time period it was initially a a low low frequency sound rising to a higher pitch and then cutting off so you can see these squiggles and you can actually uh you can actually analyze a spectrogram and there are particular squiggles or particular signatures of words that you can go looking for so they're basically throwing away all the non uh the non-sounds and then eventually the non-verbal sounds and just focusing in on the words themselves this audio visual browser that they made allowed them to look at all of this data sort of over uh over they over very broad time scale so they could zoom out to year zoom into two seconds not an easy thing to even start to sift through such a massive data set automatic speech recognition in 2009 was terrible uh recognition error rates were about 25 percent so accuracy was 25 basically unusable so they did manual speech recognition they used spectrograms to sort of zoom in on particular points uh in time and they grabbed small periods of time that seemed from a spectrogram point of view to be words they put all of those little snippets into this fast speech transcription application and then the grad student could click click play and listen to that clip and then they would have to try and indicate whether the transcription uh actually got it right or not and some so it's not speech it is speech so basically the graduate students are trying to clean up the automatic speech recognition which was which was pretty terrible you can imagine this was an incredibly painstaking operation for one hour of um possible speech it took an hour and a half to transcribe into actual words a lot a lot of manual effort here today this could probably all be be automated okay i'm sorry they typed oh they typed what was heard it wasn't necessarily automatic speech recognition so it was basically 95 percent grad student effort and a 5 automatic speech recognition here okay so a lot of a lot of effort went into this uh these early stages of distilling raw data down into words where were the people activities and objects and so on once most of that dirty work was done then they could move on to the science part so let's start with a simple question which is what is the rate at which this particular child acquired language so they're going to use this term word birth as a shorthand for when in time was a particular adult form of a word mentioned for the first time so you heard in the ted talk last time the child saying gaga gaga water so what was the moment at which they issued the word gaga we can record that here on this vertical act uh the horizontal axis going from nine to twenty four months you'll notice in month nine the child mentioned maybe three words the first three words this child ever uttered the next month month 10 here they mentioned no new words month 11 a couple of new words and then in the succeeding months the child was exponentially uttering new words for the first time what happened at month 20. any ideas so before this study it was known that that uh new word acquisition grows exponentially in around this period for most children give or take and then of course it has to tail off at some point because if you're exponentially acquiring words there's only so many words in the in the english language so the the rate of acquisition t was known to sort of tail off but no one really knew what the shape of that tailing off was at least for this child there was an extreme tapering off of word acquisition this extremely dramatic event that occurred around month 20. any ideas about why this might why there might be this sudden drop what's what's going on here the first thing you might worry about if you don't know this child is that something has happened and they've stopped they've stopped speaking or they're speaking much less as ethan says perhaps they found a core lexicon for expression if you want something you can obviously ask for it by name most children realize pretty quickly if you want other things the best way to get those things is to issue words in combination right hot water cold water warm blanket red blanket favorite toy and so on so once you learn grammar at least the combinatorial power of language you don't necessarily need to acquire lots and lots of new words you need to figure out how to combine those words in combination and that's probably what's going on around this period here but this was particularly shocking to most people in linguistics that there was this rapid a drop off in the acquisition of new words so one of the nice things about approaching a scientific question just from the data point of view and trying not to bring in your biases your hypotheses is that often the data will present whole whole new ideas right so this shows not just a drop in rate of increase which would be a sigmoidal curve meaning the length the child is still acquiring more words per month but there is actual there is an actual discontent a discontinuous drop here which might suggest new hypotheses about how child children acquire language or as ethan mentions here how they master a core lexicon and move on to the grasping the combinatorial power of language kind of interesting okay here's another question and some data that attempts to support that does how often a word is heard by the child how frequently is a word issued in the uh uttered in the house does that predict or correlate with when it will be spoken for the first time by the child and it seems that the data for this particular child suggests the answer is yes so again on the horizontal axis here we have the age in months of the child each dot corresponds to a word spoken by that child for the first time so here are the two first words ever uttered adult forms of two words uttered by the child in month nine the vertical axis represents how often that word was heard in the raw audio stream before this period so uh you'll notice that for these two points those words were heard relatively frequently for this word down here whatever this word is this word was only spoken for the first time by the child at month 24 and that word had been heard zero times it wasn't it hadn't been it either wasn't in the audio transcript or the grad students didn't manage to capture it even once in the raw audio feed you'll notice that there is a big cloud of this data so there isn't the correlation here if there is one isn't immediately clear and this is not unexpected this is raw data obviously the audio and video capture of these phenomena word births or word utterances not perfect a lot of noise in this system nonetheless if you take a straight line and you try and fit that line somewhere in this plot so that line is as close to all the dots as possible this is linear regression you get this red line which has a negative slope and that the slope of this line is minus 0.3 the negative tells us that there is indeed an anti-correlation here the later the word is spoken the further to the right you go in this plot the less that word was spoken in the house the further down that point tends to be okay so there is a fit here there is an anti-correlation but it's not great is there something else going on here that might uh produce a better correlation can we can we capture a set of phenomenon that better predict when a child will speak a word for the first time if we could find such a phenomena we're getting closer to understanding what causes a child to utter a word for the first time turns out that you can turns out that word frequency is not the only thing that predicts word birth so i'm going to unpack this slide for you but let's start by going down to the bottom of this slide we just saw in this plot here on the vertical axis we were looking at f or frequency how often a word is heard let's take every word that was ish that was uttered by the caregivers or the uttered by the child and let's also measure for that word not just frequency how often the word was spoken but it's prosody how that word was spoken on average by the caregivers in this study they looked at one particular prosodic feature which is uh what's often known uh sort of tongue-in-cheek as uh mommy ease so this is sort of baby talk um in baby talk at least in english we tend to take a word and exaggerate the vowels mommy daddy so you can actually take if you capture a word from the audio feed you can measure the length of the vowel sound again by using these spectrographs and the longer the vowel sound the higher the value of p we're going to mark it as being having more of this prosodic feature which in this case is more of mommy okay so for all the words all the points here we have two were uh we have two numbers associated with it f and p how often it was spoken and how much mommy ease was used when that word was spoken on average so far so good given those two features we can then ask the following scientific question does how often a word is heard and how it's said in combination predict when it will be spoken okay so how do we test that okay so what we're going to do is we're going to take this figure which had f on the vertical axis and imagine i'm going to re-plot this data but now on the vertical axis i'm going to plot f plus p i'm going to mix i'm going to add the two numbers frequency of the word and the amount of mommy's and that that sum of those two features will give me the height of this word we still have the time that that word was spoken so that'll give me a second plot where now i have f plus p on the vertical axis and i could then fit a new line in that second plot and ask is that line does that line represent a greater correlation a stronger correlation if we're using f and p to predict when a word was spoken then the relatively weak correlation we got when we used only f so far so good okay all right uh however in this hypothetical second plot we're just mixing f and p but what how much f and how much p should we mix together to look for a stronger correlation in order to to test that they introduced another variable alpha and you can think of an alpha as a slider bar if alpha is set to zero we're going to only plot f against a that's alpha equals zero if alpha is one if we slide alpha all the way to one we're going to look at we're gonna mix all of p and none of f together and ask does just p and a correlate so in essence we're creating a new variable which is going to be alpha times p plus one minus alpha f the more alpha the more we're mixing in p with f makes sense okay so if we take this alpha and we slide it slowly from zero to one that's going to produce a whole bunch of these scatter plots with that different ad mixture of f and p on the vertical axis and for each of those scatter plots i can get back a single number which is the slope of the linearly regressed line yeah okay so obviously plotting all of those scatter plots is going to be a lot of visual clutter remember our discussion about edward tufta when we talked about visual design we want to maximize the data to ink ratio if we drew all of those scatter plots for all of those admixtures of f and p that's a lot of ink the only thing we're looking for in the data to ink ratio in this case is how does correlation change as alpha changes so instead of plotting all of those scatter plots we're going to plot one plot where now on the horizontal axis we no longer have time we have all of the different values of alpha so what is the left hand what is the left hand of this figure represent or what does this dot represent at the very left edge of this figure and obviously sorry i forgot to mention on the vertical axis here for that dot we're plotting the correlation that we obtained for that value of alpha what does this dot tell us right here any ideas so this dot clearly corresponds to an alpha of zero if we look down here an alpha of zero means we're plotting f against uh a or age of the child when you first spoke the word which is actually this plot here if you're really paying attention you'll notice that this point does not seem to correlate to a minus 0.29 so i don't know if this is an error in plotting or maybe just a little bit of noise in the data but this single point here corresponds to this entire plot here you'll notice that we get all of these different correlations for different values of alpha and we get when we get out to an alpha of one we're plotting porosity the amount of mommy ease in all the words we're plotting that against how old the child was when the child uttered that word for the first time and we get a correlation of minus whatever that is minus point seventeen a weaker correlation so mommy ease itself is not very predictive of of how early the child will say the word so if a lot of mommy ease is used that doesn't necessarily mean that the child will utter that word for the first time earlier if we go to the bottom of these points around here at alpha 0.2 we have a very strong anti-correlation what does that mean so if we work backwards to an alpha of 0.2 what is us what is that telling us about how this child acquired language how does an alpha of 0.2 translate back into the amount of f and p and how that amount of f and p predicts how early a word is spoken what is this point what is this set of points down here telling us about our hypothesis it's telling us something pretty specific actually so let's start with the alpha of 0.2 how does that translate back into f and p what's the value of f and p associated with an alpha of 0.2 so an alpha of 0.2 means 0.2 p plus 0.8 f meaning there's there's more f than p in the signal that best predicts uh word birth so as khan mentions here this is telling us that frequency has more influence on predicting when the child will utter the word but not perfect but not all the influence so an f an alpha of zero is just frequency so mommy ease has something to do with it the amount of mommy ease in the words influences how early they're spoken but it has less of an infant in influence than frequency does as you can imagine when we're dealing with humans the the story is always complicated there's probably going to be no simple explanation of how children acquire language the way they acquire language is going to be a mixture of lots of things so as ethan was mentioning it's probably a mixture of television assuming this child was allowed to watch television um their own innate needs at for asking things as brice mentioned uh as ethan mentioned it's probably comes from combinations of words it's going to be a lot of factors that influence the language acquisition one of the exciting things about this data set is it allows us to actually get at what combination of phenomena predict language acquisition of course in this data set it's only going to allow us to uh test that for a single child if we were to repeat this if we were to repeat this process with multiple children we could ask how those combinations of features sorry i think somebody is unmuted here okay uh we could ask uh is this phenom are the are these ad mixtures of features predictive for just this child or how much do those admixtures differ for different children are there very different stories about language acquisition for different children okay okay last the last one the last uh the last set of hypotheses we're going to look at here was actually mentioned by uh professor roy in his talk which is do caregivers scaffold the learning of new words so here we see the appearance of scaffolding again so this is kind of an interesting idea in language acquisition which is of course the children are learning language another way of asking this question is do do caregiver do the does the child teach the caregivers how to scaffold the child right are the caregivers altering their behavior in support of language acquisition and as you can see in this plot the answer seems to be yes again at least for this child okay so this plot takes a little bit to digest so let's as usual go through the horizon the horizontal axis and then the vertical axis so on the horizontal axis we have time again but not 9 to 24 months we have at the center here the moment at which the word was birthed by the child obviously the child is going to emit other words for the first time at different times during this 9 to 24 month period we're going to take all of those times and shift them horizontally so they all line up at zero here so for all the uh this this corresponds to various points in time which is when the child issued a word for the first time so if we move to the left here for a given word we can ask what was happening before earlier in time months before the child uttered that particular word for the first time if water was emitted late we can go backwards and look at when the caregivers uttered the word water if another word like mama was issued earlier we could again go early even earlier and ask how often was the word mama mentioned by the caregivers okay the vertical axis here represents change in mean length of utterance so let's go back to zero here and take a particular word water we can march backwards in time and find in the raw uh find in the audio transcripts occurrences of the word water and then look at the sentences in which the word water was embedded and count the number of words in that sentence do do you want water that's a length of utterance of four want water that's an utterance of length two so four plus two divided by two is three so if we found three two sentences that contained the word water the mean length of the utterance is uh three and we can then average the length of those utterances month by month so at the moment at the moment that the child uttered the word water during that month what was the average length of the utterances that contain the word water we're going to normalize it might be two so around that time maybe the caregivers were saying want water water want water a lot and then the child said water for the first time a mean length utterance of length two so we're then going to march backward in time and ask does the mean length of utterance all is it different from two and you'll notice that in this case uh and for all of the words most of the utterances that contained the word that was birthed at this time were longer than the uh utterances during the month at which the child birthed the word so there's a lot of normalization and shifting here does this this make sense it does not mean that at the moment during it does not mean that in the month that the child uttered the word for the first time that all the utterances were linked zero that doesn't really make sense actually it means that the sentences containing the word were longer but months and months before that word was spoken for the first time the caregivers were shortening the utterances or the sentences that contained that word might not be surprising that that happens you know a few weeks or a few days before the child utters the word because the child is uttering um the child version of that word gaga so the child is giving probably hints to the caregivers that they're getting ready to say the adult version of that word and that might trigger the parents to scaffold the acquisition of that word they might start shortening the sentences that contain that utterance one of the amazing things about this plot at least from my point of view is that this is occurring in some cases over a year before the child utters that word for the first time you'll notice that after the child has uttered that word for the first time the mean length of the utterances start to go up what is that telling you what's happening what does that tell you about the behavior of the pair of the caregivers at least it tells you that they're using longer sentences that contain that word what does it suggest about the interaction between the parent and the child the parents and the child remember we're talking about scaffolding here the drop in length of utterances is the addition of the scaffolding right the parents are making it easier for the child to utter that word for the first time water gaga water gaga water gaga water water after this period as the length of the utterances are starting to get longer again this is the parents removing the scaffolding they're demonstrating i should put this in the form of a question this is a hypothesis are the parents helping the child master the use of this word how to use this word in combination with other words as ethan was mentioning are parents helping the child to form a core lexicon and how to combine words together in more complex ways it's not immediately clear but you can clearly see the addition of scaffolding and the removing of that scaffolding from the point of view of the parents and when that scaffolding is added and when it's removed is predicted by the month in which the child utters that word for the first time so it may be not so much that the parents are teaching the children language it's that the children are teaching the caregivers the children are teaching or signaling to the caregivers when to apply scaffolding and when to remove it kind of interesting it was known it has been known uh in linguistics for a long time or developmental psychology that the learning of language by a young child is not unidirectional the child is not purely passive and taking in information there is some give and take but again this data-driven approach to this question makes this interaction much more clear and and suggests how this interaction is unfolding uh over time yeah so the mean length of the utterance so if i mention if i utter a whole bunch of sentences with the word water in it what's the mean number of words in each of those sentences okay okay uh that concludes uh lecture 19. any questions about this particular study ubiquitous computing any questions before we move on to lecture 20 where we're going to start to introduce this third class of technology robotics okay okay so why are we talking about robots in an hci class as i mentioned part of the reason is because that's uh my home area of research but robotics is a very important part of hci in its own right because a robot is an interactive technology it takes information in from the world and produces actions and those actions influence its environment which influences the robot's relationship with its environment which influences what it does next a robot like humans pushes against the world and observes how the world pushes back so as we discuss robots today and thursday i want you to keep the interaction of the robot with its environment at the front of your mind this is the most important thing the internals of the robot how it transforms its sensory input into motion or output is less important what's more important is what's coming in from the outside world and the influence of the robot on its environment if we have a robot that is that is turning towards a light source for example it's emitting some behavior a person who is observing that robot is taking his input the person is seeing the behavior of the robot so the robot's behavior becomes in input to the person the person may be holding the flashlight and based on what the person senses they may come up with an idea about what the robot is doing and move the flashlight what is the person doing the person as always is creating a mental model of what they see we've spent a lot of time talking about this one of the challenges historically in robotics one of the reasons that the development of robots has lagged behind the development of computers and smartphones is that our mental model of a robot or our mental model of an animal which is usual and animals are usually our models for building robots our mental models are wrong if we observe a robot or an ant that is following a light source we might imagine that that robot or that ant is uh getting an image of the world around it recognizing a flashlight or in the case of the ant it's recognizing the sun in the sky turning towards the sun uh figuring out how to move how to turn towards the light and then moving towards the light to say it may seem simple but this is actually a relatively complicated explanation of what may or may not be going on in the mind of the ant if we have an overly complex mental model of an animal if our mental model of the animal is wrong we might build that overly complicated mental model in the form of code into a robot which is what this cartoon is trying to show down here and we've programmed this robot with an overly complex controller or brain and the robot doesn't work this is sort of a a one-minute summary of what is sort of stymied robotics process for for many decades you might remember when we talked about kismet and jibo the interactive robots when we look at animals uh we tend to anthropomorphize them or we may attribute more complex behavior or we may we may attribute overly complex cogitation what's going on inside the head of the animal that is actually there that that uh that tendency to anthropomorphize animals means that we take those complex models and build them into our robots and they don't work okay so a lot of things we've already seen from hci now which raises a question robotics which is if we want to make robots that can interact with the environment maybe we need to uh we need to program them more simply we need to put less in there and instead allow the robot to exploit its interaction with the world to learn about the world and get along in the world safely and efficiently okay so in order to explore this idea of rethinking robotics coming at it from a simpler point of view i'm going to show you a series of hypothetical robots that are taken from a book by valentino breitenberg called vehicles why is the book called vehicles well for most of us vehicles uh connote an an artificial machine right an autonomous car or something else breitenberg meant vehicles as a term that was meant to span animals and robots so when we're talking about vehicles here you can think about these as a hypothetical animal or a hypothetical robot why did breitenberg write this book well valentino breitenberg was a very accomplished neurophysiologist meaning he studied the physiology or the anatomy of the brains of animals he particularly focused on the fruit fly most of you probably live in student housing so you're probably very familiar with fruit flies if there's any rotting food over a couple of days suddenly you get these very very small flies these fruit flies which as you probably know from first hand experience they're extremely good at detecting the odor of rotting food stuff and flying towards that that food how do they do that there were many theories back in the 60s and 70s about how fruit flies do this which is that they smell local odor gradients as they're flying they perform differential calculus in their head and orient themselves toward the smell and head in that direction braedenberg who actually studied the brains of fruit flies you know how small a fruit fly is you can imagine how small the brain of a fruit fly was breitenberg was pretty sure that all these theories all these theories about what was going on in the head of the fruit fly were overly complicated there was no way that a brain this size was performing complex calculations about detecting gradients and then computing change in heading based on that gradient to head towards the rotting piece of fruit so braedenberg started doodling with other hypotheses other ways in which a self-moving animal or ultimately a robot might be able to detect uh might be able to detect signals in the environment and be able to exploit its interaction with the world rather than rely on internal complexity to find the rotting fruit move towards the light and so on these doodles eventually became a book again called vehicles um it's an interesting book it's not very long less than 100 pages and it's written like a fairy tale broken up into chapters in chapter one uh breitenberg introduces vehicle one which is about as simple as you can get vehicle one has a body we're looking at this vehicle from above as a body it has a single wheel has a single sensor uh on the front and in vehicle one we're going to assume this sensor detects temperature the higher the temperature the more strongly the sensor fires the lower the temperature the less frequently or less strongly the sensor fires imagine that this single sensor is attached by a single wire to the wheel if you take this very simple vehicle and put it for example in a pond braedenberg said imagine what you would think if you saw such a vehicle swimming around in a pond you might say it's restless and does not like warm water why does it not like like warm water because if it's in warm water the temperature sensor is firing strongly which causes the wheel to turn quickly and the vehicle moves quickly through the warm water the moment it hits a cold spot a colder patch of water the sensor fires less strongly which means the wheel turns more slowly which means the vehicle moves slower through cold water so it's restless you might say it doesn't like warm water but it is quite stupid since it's not able to turn back around to the nice cold spot that it overshot in its restlessness so this vehicle will move slowly through cold water but eventually leave that patch and enter an area of warm water again anyway you would probably say that this is alive since you've never seen a particle of dead matter move around quite like that okay so what brayton berg is inviting us to do is to imagine imagine we're seeing the just the behavior of this we're just watching the behavior of this creature assume we don't know what's going on inside that creature as he points out here if we don't know what's going on inside we might start to anthropomorphize already it's restless it doesn't like it's stupid it can't turn around you remember our discussion about hal maybe maybe a vehicle is uh upset that it it can't turn around maybe it regrets the fact that it's now no longer in that nice cold patch that it enjoyed a few seconds ago okay as breitenberg is pointing out here we might overshoot what's actually going on inside this creature which we know is simply a single sensor attached to a single motor by a single wire that's vehicle one okay in chapter two of vehicles uh braydenberg introduces two different vehicles vehicle 2a and 2b let's start with vehicle 2a slightly more complicated than vehicle one we have two sensors on the front of the vehicle two wheels on the back we're gonna assume now that these two sensors are light sensors you'll notice that there's a little plus sign next to each of the two wires which represents a a positive connection so the stronger the sensor fires the faster the wheel turns if we came at this robot with a flashlight turned on from the front from the robot's front right so if the robot detected a light source in front and to the right of it how would this robot move now we're assuming obviously we can see inside the robot if this robot detected light to the front and right of it how would this robot move in the next time instance if you have an idea go ahead and type it into chat with this robot go forward turn right turn left stay still the ro as thomas says this robot would turn left why would it turn left what's happening here can we dig down into the sensors and motors a little bit if you think about it the two uh the two light sensors at this point in time there's more light falling on the right sensor than there is on the left sensor because the right sensor is closer to the light since the right sensor is more activated than the left sensor the right wheel is going to be turning faster than the left wheel which means the robot turns to the left remember that uh this a robot is very different from all the other technologies we've seen so far because like us like animals and humans the robot is self-motile it can move itself the moment it moves itself it alters its interaction with the environment or it alters its relationship with the environment which means through its own actions it's going to alter its sensory perceptions at the next time step so imagine this robot does move a little bit forward and to the left what does the robot do at the next time instant after it's moved forward a little bit to the left what's it going to do next the values arriving or the the amount of light arriving at the two light sensors is going to be different the moment it moves one centimeter up and maybe one centimeter to the left what's going to happen next let's imagine that this robot moves forward into the left enough so that now the light is is here and the two light sensors are here so they're sort of parallel with the direction of the flashlight what does the robot do at that point what can you tell me about the two sensors the moment this particular robot moves a little bit up and a little bit to the left how are the values at the light sensors now different from the values of the light sensors just previously does it keep moving in the same direction one light sensor will be even more activated so if you think about at this point in time think about the difference in light values falling on the sensors here these two sensors are now here so that they're parallel closer to and parallel with the flashlight there's going to be way more light falling on the right light sensor than there was before and the difference between those two light sensors maybe even more it's going to turn more to the left okay i want you as homework to think this through as this robot continues moving how does it continue moving and we will start up here on thursday you have a quiz due tonight and you're working on deliverable 10. i wish you a good rest your day and see you all on thursday thanks very much 
kBWyZWBEoO4,22,"Degree Programme Director, Jan Smeddinck, gives an overview of Newcastle University's MSc in Human Computer Interaction. 
 
 For further details, please visit the Newcastle University School of Computing Taught Degrees overview page at: https://www.ncl.ac.uk/computing/postgraduate/taught/#mastersdegrees",2020-05-20T09:03:36Z,MSc in Human-Computer Interaction at Newcastle University: Programme Outline with Dr Jan Smeddinck,https://i.ytimg.com/vi/kBWyZWBEoO4/hqdefault.jpg,Open Lab,PT1M45S,false,184,2,0,0,1,hello my name is Jan Smit Inc and I'm a lecturer in digital help at the School of Computing I am also the degree program director for the new MSC in human-computer interaction the MSC NHC is a one-year 180 credit program and that will include a number of basic modules and then also some electives basic modules in HCI of course include HCI itself introductions to user experience and deep course and interaction design on top of that you'll also be learning how digital systems impact in different systems and societies and of course you'll be learning about the technological foundations of digital systems core curriculum closes with a dissertation project that often allows you to engage with ongoing projects and one of the research groups at the university or sometimes even with companies the program also includes exciting optional modules that allow you to deepen your software development skills in a specific area or to engage with physical prototyping and making or in a new development area such as human artificial intelligence interaction the MSC in HCI will give you a really good understanding of what it means for people and societies to be interacting with digital technology and then of course most importantly how to make the technology that really works for people the programming will give you a highly actionable skill set of skills that can be put to good use of course in software development houses but also a broad range of creative industries and content production houses as well as a growing number of business in all sorts of application areas that nowadays are working with digital technologies you 
QljA8OAKRp4,27,"In this webinar you have the possibility to get to know more about Tallinn University overall, the admission procedure, student life in Tallinn, Tallinn Summer School and more specifically about Human-Computer Interaction master's programme that prepares specialists that understand and create user-friendly technology and Digital Library Learning master's programme that provides its students the skills and competencies necessary to navigate through the rapidly evolving world of digital libraries.",2015-03-26T15:34:15Z,Webinar about Human-Computer Interaction MA and Digital Library Learning MA (2015),https://i.ytimg.com/vi/QljA8OAKRp4/hqdefault.jpg,Tallinna Ülikool I Tallinn University,PT58M40S,false,300,1,0,0,0,"so welcome i am very international marketing specialist here at Valley University and what we're going to do today first of all I'm going to tell you briefly about tile university overall then my colleague will continue with the human-computer interaction program and then makes teach the library learning program then I'm going to tell you about the mission requirements students life and the bit about our campus and if you have any questions pimpri dress them in the chat window because my colleague mella and jana will answer to all of your questions in the chest and they throw on you can ask directly from us so we can answer it here online so first of all few quick facts about selling niversity Valley University as if each day was established in 2005 as a merger of but different Institute's at the moment we have approximately 10,000 students studying here on ba ma and PhD level among whom approximately 700 are foreigners most of all foreign students are from Finland from Russia Turkey latvia lithuania and so on we have approximately 1200 soft members here at a university among whom nine point four percent are academic foreigners which makes it makes it it's the highest percentage chills we have here in estonian university we have 250 partner universities which means for example when you come here to study and you want to go abroad with Erasmus exchange studies you have a variety of universities to choose from and at the moment we have 26 academic duties here at a university we define ourselves as the leader and developer of smart life span in Estonia and we have directed our resources into five broad focus fields educational innovation cultural competencies open society and government digital media culture and healthy and sustainable lifestyle we believe in individual approach to students by that i mean our study groups are horses are it's small which means our students have always the possible excuse to turn the lectures of the professor to ask questions discussed some topics and matters our campus our main campus is located in the heart of the city center old town which belongs the unesco world heritage is only 10 minutes walk away Harper at least five minutes and also the bus station and Airport are close by and our 21st century facilities include library study rooms modern classrooms computer classes and for example we even have our own cinema hall here but now i will give the floor to my colleague and she will tell you more about the institute of informatics now what everyone around the world we have a viewers from Australia to 13 today i will call it up the world and i really happy to present you the study opportunities at the institute of intimate exert the force and I'd like to tell you that darling universities actually undergoing major restructuring you know that you'll come up with them better a structure for interdisciplinary research and study and most likely I'm Institute of informatics will let you in together with two other Institute's institute of information sciences and the Department of Mathematics and our new name / 2015 might become identities of digital technologies so engage you with them you our slides later on this year and search our website besides the title entities of informatics it will also look for that title of digital technology or similar Oh introduce them has currently free structures around which all our studies and research our centers and their most of important that is that the center of educational technology which is the leader of its field in Estonia and the surrounding areas countries and then I would trade the road scheurer or the one who shows directions were to go in digital education and the digital information society in Estonia and the neighboring country acosta estonian government is very I'm keen on establishing the world leading digital society in Estonia and we are up here also enjoying their support of governments in teaching gas ICP related fields and errand girl research and therefore in case you're planning to come and study in Sirenia this is definitely the place to be a little bit about the newer King digital technology institute the mission is to implement them create the solutions which support them develop bingo information society which is centered on peoples peoples need everyday life and taking the maximum use of modern technology helping everyday life more enjoyable and easy and our new Institute or the information of the UN citrusy refocusing on three sorry five knowledge areas and I would like to point out right now the first two if you can learning at the distance and human-computer interaction Syria there in the learning ecosystems research group I mainly focuses on taking digital technologies in in learning situations formal at schools universities elementary schools or also informal learning like workplaces and everyday life the social dimensions and so on at the other area i'd like to point out some idols as team opposite direction is researching and focusing running smoothly on them all the assets related to people using computers interacting with IP technology and them nowadays it's not just only am using the computers and ID technology tools but it is a feeling and experiences which people go through when they are actually using modern status and tools so we total sustainable design of user engagement and other aspects with what we are focusing here and when you are thinking of coming to research and study at Alinea 50 then within been to my computer interaction area you can specialize on the on the f aspects most interesting to you and we really do everything to support you researching of motor filter practiced here you see a list of our major keywords in the Institute which has been collected based on my later researchers activity to develop my last and then our little risk them about our study programs first of all to give you an overview of the entire academic setting then we are teaching programs on every academic level bachelors masters or doctorate level and also giving additional courses for those who are mainly coming to university and interested in one of your subjects if there are summer school especially sympathize and also people are welcome actually do come and study one or two single portions of interest at Valley Community Theatre the first of all our bachelor program which is sought in estonian at least mainly work with certain information technology and informatics we have few portions which also a seen you dance or international students can take in English but we would actually like to see the most visiting and analysis international students coming to our master program and the first folder will establish the most program of human interaction which has a special focus and interaction design area it is two years master program we expect students to come here and spend their two years in the Allens focusing on theory and practical studies the program is fully in English so there is no language issues if you speak English language better than PQ level and then from this year actually we have a new offer for those who put some reason aren't able to come to Tallinn we are together with them cyprus cumulative technology we are going to offer a new master a lot of them fully online and the disposed interaction design and compared to that local human computer interaction programme PA italian then this some online interaction design these are mainly focusing on the interaction design aspects and they are not covering all the white other aspects and choices what we have you entirely so we would term success is only are not able to come to darling then it would consider interaction design online program which we are doing together with cyber of new stuff technology as an alternative option sir and they look good like you also mentioned that we have a doctorate program here for those who are more research oriented and them would like you to further on the program mr. four years program and it's titled information / technologists also possible to recover in English or in a sony electric as well so shortly about the human-computer interaction is amor HCI as we are to mention astre in the short form and the main goal is to prepare expert who actually understand them does know how to break new currently technology which helps people to live there a delight advantage the people capabilities and and goals and desires so that using technology is not something of its own but it's teacher symbolizing know everyday life and be easy do you ever think that I you think you sent without it the program is two year program lasting four semesters and giving 120 European credit points the semester 3 is a 1100 euro and the tennis we have four semesters so we start thinking how long it less than we would say two years and four semesters the program has its own myth files the address is AC i Justi utah vs you may in a ticket out later or abscess knowledge that slideshow as well and then i will point out a few specifics of the program and especially we are engaging our students on our research programs and them projects so every students are joining in our program are expected to actually in very early on stage know what they are interested in them select them on a research topics also take part of the research project similar to their interest and we have also where IAM international staff and student body so most likely will be exposed to ideas from all over the world and we also enjoyed a company off by playing students here from all over europe so broadly speaking half of our staff and half of our students are international and then I believe it makes 72 of them put forth changing the ideas and coming up pizza with innovative solution here you see a list of compulsory courses if you also thinking go the only line the program of interaction design then these compulsory courses are roughly similar post on our local HCI program we will also have a bunch of your group of as collective courses which here is one kinetic in addition to the compulsory subjects and the pier with the list of electives if you can see some of them of course spare tire list although if you would them like you examine further how the studies field and I put on the slide a overview of the courses which form the mainland studies of iti and them in addition to this slide every students have a roughly 16 traded points of elective studies or free studies which began to entirely according to their own interests there is no restriction to that and well let's take a quick overview of the study process as well the topics or the courses are listed before I would now on the scale of two years you can see in 10 starting from the left you can see that them yellow box is representing compadres cautious I mostly taking place in first and the second semester in this year and on a second year students are free to focus on elective courses which didn't much research English but also parallels you're the person taken years they are expected to want to their independent course selection of free elective santa and continuously find out that supported courses that besides the Cabal three months to to come out which are the most a Sicilian here i put the one schedule come next if a week our studies are organized not every week but in every second week and not in from monday to friday but actually from thursday's to port authority which means that they're one risk between I study with info trooper independent studies independent research activities and you might find this a little bit different than in general but this actually is helping to distill a strong focus on one area of interest in the other firm increasingly different subjects and we also have a yes sorry we have on the schedule plant in such way you know that you have maximum benefit from visiting international lecturers and the students have enjoyed that so far here are a few of our maintenance professors and lecturers you can google and tell me more about their research lecture but damn if i put the names here and my SMS courses they are teaching so that you could talk going to further student on your cell phone your graduation all our students before has i have enjoyed being employed then blowing rate is very high and probably close to a hundred because you deserve early work will paralyze studies as well and the main man purpose is to give the students from all the goals or them skills necessary to support the local IT companies in designing user friendly environment and tools but also being able to develop the general understanding how things especially our city's software tools should work in a in a society and becoming their identity performing the leaders of the first things in our country or any other country where they are coming from a little bit all about admission and in general in general requirement our applicants think you have who the bachelor's degree or currently qualification the earlier studies and we also expect our quickens you have some experience from the piece of informatics shall computer science of design or social sciences which gives them good starting point synonym of the study there are also some hundred specifically admission requirements which you might have to reject their family unit it is website to make sure that that's what else what applies to to you and to citizen from your country that you wouldn't eat those and I would like to point out that the deadline for applications are different depending on what your what country you are coming from those who are coming from outside of European Union here with your european economic area for them the application deadline is first of May especially do I give the time for visa process and and the certification of documents and for the you EU your new Union citizens for citizens from Russia Turkey you train the deadline is first of July and and everyone is welcome to you apply already starting from the date only the deadlines are the ones with close application period but you can start already today and I would like to point out the one more aspect about human computer interaction programme if we have an entrance exam which is composed of them free part first of all and we expect you to create your digital portfolio and upload it in internet or email it to our contacts absence of issues then also please a droid a motivation letter which explains your interest in research studies have found our program and then these are different deadlines than their mission deadlines would like to point out testers for example if you are not coming from you EU Europe for example coming from and Ukraine or M any other country so actually I would say that if you've gone from other continents and your chasing deadlines interest of Maine then you are still able to compose your portfolio and provide your motivation letter until May fish at the same time if you are applying from within your opinion and or russia turkey and ukraine you have more time to compile your portfolio because we needed before they actually interview and then the interview times are not perfect yet we will announce it on our broken flip i stopped a bit later depending how many candidates are exploring this yet there are also several scholarships depending on which country you're coming from or what are your circumstances to the surface i skidded here some links please simply to click on them later on them and read up or down on the website the road tolls on the human public interaction system side and we also have facebook them where I suppose please join in get the latest news what's going on and pestered discussing goals already taste a current student so that you would be well prepared on teams on the joiners and there are other options I mentioned these online master's program in interaction design but for those who are not able to come to Estonia say the website is ID master d you please have defined to $65 three separate day out and then the very very of courses which are offered or now on my master's program answer for that is spelling mistake up there in a title and that's doing term also a 1 deadline for applications if we do surface of july because in this program you might learn least we'd have given to that country actually and therefore you can apply latest and human prostate rocket program and an hour is that I would have any questions regarding human computer interaction field I would be happy to answer and then I will turn the floor with you my colleague from basic yourself information sorry one man question which has been presented in chest but I would clarify where the study burgers are actually taking place and that will last explain further than we have to master programs one is fully online and one is taking place that darling anywhere 15 Estonia the one taking place here silence is titled human-computer interaction to your master program and the other program was still interaction design if it's teaching basically focuses on one year for of courses and is carried out oddly over internet and so on if you are able to come over to Thailand to Estonia then you have a possibility to enjoy our program of school with interaction in case you cannot come here then the other program is starting to interaction design thank you very much and I release and let my police come and continue prepare hello everybody I am going to talk about digital library learning master program and I am happy to be here and introduce this program to you and Jim to start with a first I would like to say some words about the Institute which is offering the master program and i should say is that our institute of information studies that have had different names during the years and would have been in universities in 9065 and we have belonged to different faculties for example for the faculty of culture for the faculty of philology for the faculty of social sciences and those faculties have influenced the development of our spherical and since users who tasted we are the independent institute of information studies positive such after already issued before we have large structure before in our university and from the 16 by 2015 we probably would be yes in them I mean the new Institute and we are not quite sure about the name yet but probably it will be the Institute of digital technologies and our other departments or other units there are automatic and mathematics that units and and actually I would like to say is that it has inquest natural as to it belongs to the Institute of digital technologies because you know Institute we have had all the time quite strong focus on technology and whatever I have put here on the slide some signpost for example because we can journalist as internet came to a failure in 1992 and already in 1993 we started to teach engineers internet in our theory school and in 1995 to 90 97 the first I debased distance education program for school librarians and in 1996 to establish the eternal info forum which was actually the first officially Jonah in estonia in 1996 Oh yeeah experimented quite riveted with audio and video conferences technologies and in 2003 we started to provide on by mastering information management and also a defined information literacy even tutorial which is still used in our higher education institutions are also at school level and in 2007 we impersonated that error component of joint master or digital library learning it means such a request support from a smoke bomb booster to deliver this program and initially magician dude ethnology we have also very very strong focus on internationalization and we have a very you boy international network and at the moment we have 24 hours of agreements with with many countries in Europe and we have also participated in many many international projects almost all European Commission purchase for example some framework cost network or attempt to turn out as Olympic Emilio's Minerva and and so on and distance and already in 2003 even together with my colleague from my university we started to develop this is a library learning program and my main idea who why we started to do below if you will develop this program horse that we believe you to such in library and information science education in Europe is quite traditional walls and we wanted to create something in a way users which is a connected with modern technology and then only started with people through this program and in the beginning there were only two partners but but our University and some will give us the van we decided to invite also colleagues from oslo university college and in 2007 because thatís Perkins with support for the program and then we delivered the program together with also university college are my little scamp animals but since then we 2015 a year theory by only the program pyar par my university and stirling university because the auto universities can't join with different in different races and if the main aim of the program is to provide knowledge and skills to navigate in the complex world proxy to the library and mr. librarianship and what makes the program unique is also specially still focus only one some it in one dimension but we try to connect a technical dimension economic and social dimension and how the program is organized I should still say yes even also it's not anymore in the program with us follow the same structure interpersonal a semester we will have a summer school before importing oslo mounted on a school is in cali and then Austin with tallinn university and furman university deliver oneness module i will talk about these modules with each baker in the second semester courses are delivered by italian university in the system except is former University and also internships but it included to the farmer modules it means that students have to make to the internship in very facetious digital library centers into Europe and the in the first semester students can accuse should choose our masterpieces and they can choose a place where they would like to do it sir in towing or in pharma and it depends what is a concrete topic and body the interest of the supervisor and before we are also aware modules or the contacts but not be kinda like this theory it is something what we have things that users in the eighth and we have found that it is very very beneficial and we do it before the course is there or module start and because we try to find out what are the expectations and and and meet the furnace for example related to that content content what is their previous knowledge and skills in the area but is what are their expectations what are their learning needs learning site how they like to learn what are there is the follicle requirements and and so on and then we can adopt the program that this way to the students that needs and expectations then again about the structure I guess I see the first semester both stunning University and former University deliver it um well your stress both modules are 50 etds and one more Uli's research methods and theory of science / Donnelly and digital knowledge organization is delivered by far my illiteracy in second semester of foster modules are delivered by jumping found 15 pts information and knowledge management and human resource management Lynn is a semester we have photos to more news posts 50 need gifts and post modules are kilobytes I farm and the names of him of the modules are successful digital libraries and users and horses of citizen libraries and as I said before in the fall semester the students with you dare ignore 5256 and what is the unique is this program further very unique is effectively we are very flexible oh hi all possibilities students to learn you can learn at the campus students you can learn completely as an online student the students and we also offer blended learning model the posted a particular culture you very much photo constructivist approach to learning it means that you as a student you are responsible for your own learning and constructing yourself for your interpretations and meaning of things and we don't support the knowledge transfer model what we use at the parachute be constructed by owner and you also believe very much that challenges the social activity and beautiful and construction knowledge together with your fair students and your tutors we also try to provide you very much very broad perspective and multiple perspectives then it means to look the same a gentleman from multiple point of views and we integrate in for example different research view for example to some phenomena we have teachers from from Asia we have issues from karma from united states from australia and we try to integrate with the practices then we very much encourage our students that you make research and also present and promises to their research and we have had experience for example in our deal program since 2008 but during some of the semester one feels pretty are fair and masterpieces that such we will go to the conference called qualitative and quantitative methods of libraries would have been that all years and our human presence are every search results which will be published in the proceedings and also in the journal of and qualitative and quantitative methods of life a library it we encourage very much also and our students probably published or already during their study done we have experiences that students have office in Paris Review Journal or already during your study time we use a very much technology in our TV education and even when intervening we had there are so close montes program and it was meant at the template program we already then use heavy d information and communication technology to support students use of social software if you skype will use learning management system moodle or your soup multiple media or your conferences audio and we also have people especially for our his using open educational resources as a student and successful we also encourage two decisions today Pemex's which are related to our offices but to make a sauce summary and even there some courses are taught in the polymer and some intervene when we disable program as an integrated whole we integrate theory and practice we use constructive approach to learning for years we encourage multiple perspectives and representations of constant for using that indicated intestinal approach we integrate media and research or digital does not be penalized it as I stated before that we very much interface Mont appear multiple perspectives and representations of constants and there are many many famous library and information science researchers and even outside of library information science who have contributed to our program and who are ill contributing to our program and here are some pictures of them and if you have you are familiar with library and information science then please people still need any deduction everybody know cool tell everybody knows Peter investment on williston and Ronde yes a rocker faith and so on and outside library and information circles I also think that the name of a PN bangers you'll need in this introduction why you should choose still programs I already say about the flexibility of program we have no different program for online students or campus students or plenty students which I'm let's try to do all this within one program you can choose the temple mount or online mode or plenty fast then you can you can get your opportunities you studying international goals and in multi cultural context and you can study if you want to send back a to European cultures it's Union culture and and Italian culture and after that's it you can also continue at PHP level expanding university or power minus 2 and to give you some overview what has happened where my heart would have heard during superiors user 7225 2014 127 students from 55 countries would have had this 24 female users and 53 may use and the young students have been doin teach you and the Saudi students 50 skills then we have 20 a variety of ages and and it's possibly only server merely see something services the 30 and here is one example of the country's what president still program and to have ever had in many students from from Les Paul or for example 10 nepal's and students from Italy a student from egg yolk clear and another country has to leave these lips and it's such a silly 5,500 have been better than get here I have here some illustrative examples of different field courses that you can you can get some visual c licious for example here in the we're still over til 10 you see our kin second coat and we'll have here till third full scope and fifth group yet we have a killing sister and there is some students from there till seven and in this picture we have two solid eight and if you are going to employees and you repeat that is nine and opportunities what kind of opportunities you have after graduation but the program is designed to prepare information professionals for her all who have orbital responsibility for managing digital conversation group called version program and implementing digital library for productivity to the library education program and if our target have knowledge and skills necessary to function in the complex words of digital libraries in different sectors of society and before I give the floor to our Eunice tourists who will talk about the admission requirements I would like to ask if you have some quick I can see if you are any question some of the most common thief Thank You Patrick 15 success there is no questions at the moment and life is an oratory says thank you so high again about the admission requirements first of all and this applies for both for the human-computer interaction and the chief digital library learning program you have to fill in the online application if I owe you can find on an application at the Sonia that stream of live.com and there is one question about the only knife I spat the online admission exam we're going to tell about the deadlines a bit later but the overall deadline for digital library learning program now then you have to visit a line when you have to have applied everything all the documents it's the receipt of March 2015 for human-computer interaction is was the first of May and verse of july party to the library thousands of march then after you have a building online application you have to send pi by post and if you when you have paid application fee which is eight euros you have to send I post the online application cover printed in science and copy of your bachelor's degree certificate and transcript of records the translation is requested instead of payments are not in English and both of the copies and translations must be adjusted by an opera after that you need to prove your English proficiency you need to have an English at least some be two levels you can prove your English with the most well-known test against or twice well for example you can carry out the test as language centre and table sense research directly to tell University and also there is a possibility to carry out the test that here at Valley University out there for that you have to be present here in terminal the audience but there is a possibility after as you cannot go to the online application system copy of your identification page of your password and the receipt of payment of the application fee and the scottie already also said there are some countries specific requirements for example students coming from Finland if you have graduated become louder you don't don't have to sorry there are some comments in the chat window and my focus goes there at the moment but about the country specific requirements once again for example if you are from Finland and you are graduated with whom lauda you don't have to prove your English proficiency you can go and check our website for the country specific requirement if something that applies for you or not and after that you need to pass the program specific requirements and for the deal program it's good if you have knowledge and skills with computers or information use you have to give in the statement of purpose in English what is your motivation why you want to study the program what are your plans then a reference letter I want to three pages and last but not least you will have an interview with that mission committee and if you cannot come to Estonia of course there's a possibility to carry out the yellow sky for that you need to have a webcam to a few words about the student life here in Tallinn a lot of our attic can't ask about their study allowances the answer is yes we have study allowances for students who have been already admitted to tell university one possibility is to apply for the need space study allowance for students staff and financially disadvantaged backgrounds and the allowance is 75 up to 220 euros per month during the whole study period so masters two years there is also a study allowances based on your study resource if you have a good average grade and there's a possibility to get an allowance 100 years per month there is one semester that means that you have to apply each semester again and if each to your crazy chicks and it's a greater average greatest groups and most probably you will get the allowance here at our university we have our own student union under the Student Union there are different units and organizations such as international club that organizes events for international students is in Thailand for example organizing events for exchange students we even have our own student TV that broadcast you the latest news to the week the our own culture club sports club Photo Club other cultural collective even our own students kept in the child care about your accommodation a lots of a pic can't ask us if we have dormitories yes we have dormitories but in order to get a place you have to deal in that occasion rather ratifies the return there are only dormitory time university dormitory that's just around the corner of the main campus so again in the heart of the city centre there are double rooms for the price of one hundred and seventy euros and fifty cents per month vendors chief restore mystery there are double and single rooms the price for double room is 170 rose the fight for single rule is 340 Rose deco dormitories there are double and triple rooms the price for both is 150 rose of course you always have the possibility to rent an apartment by ourselves with your friends with your course mates and the price varies they can start from 200 euros can go up to 400 500 600 if you want to know more about the possibilities of accommodations and goalies to tlu ze / housing there are also listed different style and platforms where you can see what kind of apartments are available what's the price of those at the cost of living here in telling the Monte living costs are approximately 300 to 500 euros not including accommodation of course it depends on yourself how much can you spend how much you want to spend but here are just a few examples of surprises for example students now here at university are three to seven euros the public transport is free of charge for the residents of selling cheaper tickets are 12 18 euros contradict a day for 13 years and so on this challenge the cockroach a few examples birthday my internship there is a possibility to work during your studies you can you can work even if you're studying full-time you have the possibility to go and see what are they what are the options without the job offers if you go to study in estonia taavi e / workings there are also listed different online platforms with organizations can upload their job offers so you can see how's the job market here at the moment also you always have the possibility to ask directly from the institute some context or from our character and counseling center at the first about their campus campus here in the city center all of our buildings have a Latin names so our students rejection I swear they have to go for the class first of all cetera and the earth represents academic traditions it is the oldest building of Sally University then ostrom the stars represents that achieving their goals this is the newest addition Italian umer city built in 2012 then Silva forests represents researchers also known as the language buildings Nova new represents innovation north the houses for thick film and media school there is also located to cinema hall I told you before they're all the students of tell University can see fields for 30 some a bear represents defending your ideas and it's the houses Institute of Fine Art modest the C represents open at sentara different Institute's classroom seminar rooms is so us just few pictures of our kids this year then few words about that planning summer and the winter school those short courses during the Italian summer and winter schools that summer school takes place in July from 13 to 31st the winter school a huge in January from or two 20 second of January during the summer school we have usually about 350 participants turn to intercalate list your show every time eighty-five percent cell data cans are international students so when you come here you meet a lot of new people from lots of different countries additional different courses what we offer during the time in summer and winter schools during the summer school we offer approximately 25 different courses language courses creative courses spicy courses and so on you have the possibility to attend at the cultural program which means that the participants are taken to the museum's to study trips outside of skyline and so on if you want to know more about the summer or winter schools then I suggest you to go to summer school tiu dusty or winter school that tngo does see now I will give the floor to my colleague once again and she will tell you more about the information and knowledge management in digital environments it is one program in the summer school yeah hello everybody and as already is my colleague told i would like to invite you to take part of the information and knowledge match versus Easter environment summer school and this is the summer school we are we believe the colic strong misuse of information studies are offering already second year and the year this cesspool will take place in the middle of july in the middle of nine sicilian summer from 13 to 17 sofa july and this course information and noise management in digital environment actually is based on our digital library learning master program by colleagues iliacus told you about it couple of minutes ago and we decided to develop from our modules which we are offering the report of digital libel learning monster program as an information and knowledge management and the human resource management course so we decided to use let's say our knowledge and skills to deliver those two courses and we created this five days summer school of course it's always very difficult to choose what kind of topics you may include to the five days sports and we decided to have the topics related to the change management in digital environment Moodle standing as the producer change then of course the pink frameworks which are supporting their change models another aspect is the framework for the information and knowledge management in an authorization human resource management its external context organizational information culture and leadership and people so those are the topics em what we will cover during our vibration and summer school and we would like to invite you to take part please consider about this opportunity to see sterling to see Delhi University and to be part of our international summer school team thank you as I really keep the floor to cotton again I like you wanna give you a little information about the courses in our summer school program which are sold by this is us in cosmetics and the first of all we have four course dinner together but then i will start with experimental interaction design course which is a two week course and intended for those who want to get to know the basics of interaction design want to try something that directly with their own hands it's going to be very intensive and very interesting hands on the coils and with many lectures about the theoretical theoretical background and then right out everyone can join in teamwork and start developing their own as prototypes and learn as they go and the earth next course which would like Georgia point out is a mentor for advanced level it is about research methods in human-computer interaction we are hosting offering it for the great students who are prospective doctoral students but also master 2,000 people rearranged rested in research and are welcome to join in and that it is a one week course both courses are taking place in that you I and they are to keep lying to the people of enough background you will continue their own research and discovering the fields and who would like to take if your academic level then these people can get their first step idea what is the research feasible and then they can enter our study program and there is 20 more courses one is called the sign of serious games it is a one week course and in connection with one of our local research projects here theses about games and gaming and creating games which actually helps people learn something or do some serious business that besides having fun the course is very popular interesting and I'm recommended it there is one brand new course this summer which is about the free and open source software for a so called regular users but it is meant to be everyday people who know how to use a computer in the everyday life but would like to take advantage of all the freeware and software of open source software out there and this is an overview of course and they're giving us the possibility to try different oceans and all by yourself and I'd also if it is that one week course so we have one sports for two weeks which is an experiment and interaction design and then we have three courses which last one week and you can see the emoji taste at them summer school website yes Oh last but not least i suggest you to contact their students and only in case you have any questions regarding the overall students live here italian university about university or specifically about the study programs if you go to TL UAE / ambassadors did you find the context of our student and aluminum represents our english space degree programs to go ahead and ask i rested on and i suggest you to follow us say social media ocean in facebook you can find that to type in tally diversity we are also in my contacts yet instagram my sister used to linkedin and sodium at last but not least in case you have any specific questions about their mission procedure then I suggest you control to contact our mission specialist admissions at CI yo te and meanwhile you can go and check up our quest hele Oct "
Swzpn3o5c88,28,"Instructor: Josh Bongard

Institution: University of Vermont

Recording Date: September 2015",2015-12-07T19:18:45Z,"Human Computer Interaction Class, Lecture 03 of 27",https://i.ytimg.com/vi/Swzpn3o5c88/hqdefault.jpg,Josh Bongard,PT1H13M17S,false,420,2,0,0,0,it's free that's right something like okay let's get started how many more degrees is it in here today than it is about five feet out there I don't know how they managed to do it right ok a couple more glasses hopefully and then we'll be complaining about the opposite problem so I'm hot you're hot bear bear with us here with us here just a couple of housekeeping notes again everything's on blackboard somebody mentioned that the tests are posted when the tests are posted it puts an announcement in blackboard but it doesn't send an email I from now on when I post the test when I get back to my office after class I'll make sure that the blackboard announcements also send you an email so if you keep up to date or email hopefully that should that should work any questions about blackboards the quizzes so far so good okay a quick word about deliverable one just to jog your memory right we're establishing this very very simple feedback loop between you and the leap motion device your Python code which draws something to the screen which is seen by your user which causes them to move their hand and around and around we go everyone managed to get Python installed matplotlib so far so good leap motion installed it does if you install canopy so canopy is the overarching system that includes all of the libraries are going to need for this class so if you successfully got cannot be installed you should be fine anyone actually managed to finish this assignment a couple people okay so you got a week left that's fine again I know some of you are rapidly catching up and brushing up on your Python skills so keep on top of that just as a reminder next Monday 1159 p.m. those three video deliverables will be due in in blackboard so you go to blackboard click on the assignment and submit URLs that point to a YouTube playlist that contains your three video deliverables yeah okay so let have a quick look at the schedule just to orient us to where we are and where we're going we're going to finish we're going to finish lecture to today moved on to lecture 3 which is packed analysis which is people activities context and technology so this idea about how to think about your users going to talk a little bit about that will probably finish lecture three today and we'll probably start in on part of lecture four so again the quizzes are going to cover whatever material we finish today so if we just do lecture three the questions tonight will be just on material from lecture three if we get through the first few slides of lecture for that might be a question in in the quiz okay so as you can see lectures four through eight we're going to start to talk about design how do you actually go down how do you actually sit down and start designing a piece of interactive technology or piece of software for leap motion that not only runs and doesn't crashes but your user would prefer to use your ASL educational software over someone else's what are the additional considerations we need to think about when we're designing when we're thinking about creating a good interface we're going to spend a bit of time talking about that there's some brackets in here about guest lecturers I'm going to be doing a little bit of traveling next month we'll see where we get to but you will have a guest lecturer at some point probably beginning of October but we'll talk about that when we get there okay then on to psychology and onward from there so lecture two we were talking about the basics of interaction and I showed you this very simple cartoon of a human interacting with a piece of technology the human does something the piece of technology takes that output from the user as input does something and project something back to the user which the user sees or census right the user acts and there's a sensory repercussion of that action when I act in the real world there is also a sensory repercussion when I act by asking a question there's often a social repercussion which is you answer my question and around and around we go we talked about John Dewey in the context of this feedback loop why the heck would we talk about a 19th century educational philosopher in an HCI class what was the point that John Dewey made over a hundred years ago that really matters for for HCI I'm saluting to Dewey right so think about the action first which causes something to happen out in the world and then we observe the repercussion of that action why would that matter well if you show your leap motion device to some of your friends not tell them what it is just plug it in and say this is a wireless mouse watch what they do some people will sit back and passively wait for something to come up on the screen to tell them how to interact with this technology other users will immediately start to wave over the device or bang on it or grab it or or move it around right certainly users will just instinctually start with action and see how the technology reacts others will sit and wait right this is part of another issue that we started to talk about in HCI that's peep that people are different right people are going to bring different assumptions to bear on your interface and you're going to have to think about supporting that diverse group of people using your your interface okay okay I think it's easier for the people in the back if I leave this in edit mode can you all see the bottom of the slide okay all right okay we got just about to the end of lecture to where we were talking about things that may HDI unique compared to other branches of computer science HCI we're going to draw on psychology for a theoretical base and some of the principles of software engineering for the design approach of course we're talking about human computer interaction what do we mean by interaction well there's lots of different kinds of ways that your user is going to be interacting with your system some of them are obvious like physical interactions does your user wave their hand over the leap motion device or do they grab it but there's going to be increasingly less obvious kinds of interactions that are going on and as a good HCI designer you need to be aware of what those are or influence your root user towards the kinds of perceptual and conceptual interactions that you want them to pay attention to so I gave you this example of the Wii Remote last time and we right at the end we walked through the three different kinds of interaction that are possible with this device so simple physical interaction is i move my wii remote and I see something move on the screen right so I'm aiming my aerial that I'm about to release from my bow when I actually do release the arrow I hear different sounds coming one from the remote one from the screen they're separated or the delayed in time which was a clever hack put in by the creators of this video game to create the perceptual illusion or create this perceptual interaction that I released the arrow from my remote and it hit a target in the screen a short time later right once I start to realize that's happening in the game I'm building up a conceptual interaction and conceptual interactions have to do with mental models and predictions right I push against the world and I have a prediction about how the world is going to push back so once people play around with this a little bit and they move away from the screen they're going to have either consciously or unconsciously the prediction that now when they shoot the arrow the time delay between the sound at the remote and the screen is going to be longer okay if that expectation is not Matt you're going to confuse the user right okay so we're thinking about these three different kinds of interaction as we as we go okay we're going to switch now as we move into lecture three about this idea about trying to think about our human users first and only then start to think about how to create our interactive system given those those users so putting people first sounds pretty obvious and simple as we go through this course we're going to gradually keep unpacking this term and what do we mean right so what does it mean to put your users first as your design and interactive technology well obviously first you want to think about what does the user want to do they want to learn ASL using your leap motion device they might not actually be that interested in the leap motion device itself how can I learn ASL as quickly and effortlessly as possible given your interface and work backward from that user goal to what your interface should look like and what it should present to the user good examples of other interactive technologies out there is designing new ways to connect people to people so if you look at the history of computers most of the killer apps have been ways that allow people to work well together better and actually create new ways for people to collaborate that were difficult or impossible without computers or interactive technology more specifically again this idea of creating synergies and herring and collaborative work so making it easier for people to brainstorm or work together rather than to make it more difficult other ways you can put people first is involve them in the design process itself so are there ways that the user can easily reach in and modify the interface so it's more appropriate for them right it's not a prepackaged piece of software but something they can actually modify and gradually get further further down into the guts of that interface if they want to and again designing for all of the different ways in which your users may differ okay what are some examples of technology where these five things are done well and what are some examples of technologies in which this is not done so well ID knows what are some what are some pieces of software out there that clearly by their design put the user first and what the user wants to do rather than what the technology can do are there examples of technology that advertise how great they are and all the bells and whistles that they have that completely overwhelm their users and confuse them rather than draw them in Adobe products okay which side of the ledgers that are those going on the latter thank you yes okay okay right here all the amazing things you can do with your your videos right rather than starting simple which is usually people want to cut and paste videos publish it to YouTube right ninety percent of users probably that's all they want to do those two features should probably be front and center and everything else is hidden until the user advertises they want to do more what's that I was trying to buy a sandwich okay absolutely sure right so there you go there's a great interface right signs or menus or options right here are all of the possible things in a flat list that you can do go right where do you where do you start later on we're going to spend a lot of time talking about navigation right where am i I'm trying to order a sandwich what's what am I supposed to do first how do i how do I know ok Google right was brilliant right put a empty bar type in words and click go right I want to search for things that have to do with this this keyword right start simple and all the machinery in the background is is hidden designing new ways to connect people to people what are ways in which people can interact now given interactive technologies that were difficult or impossible before social networking who would ever want to write messages to each other in what is it 120 characters or less it's ridiculous no one's ever going to want to use such a such an interface okay other examples of connecting people to people first killer app was arguably a spreadsheet doesn't really connect people to people but the second killer app email right people still use email or less maximizing synergies inherent in collaborative work have you worked with software that actually allows you to work better with members of a team or software that actually made it more difficult I see some nodding heads back here do you have an example sorry what's so great about gotomeeting or what's not so great about it what's that like waking up a lot it breaks up a lot okay I'll be the same place okay why because I I was just using it today and I was editing something while someone else was editing something and it was like we were sitting the same sink me a favor right right so that was a tricky thing to figure out for a long time right how do you allow two or more people to simultaneously edit a document right there are all these post-it notes an Adobe Reader and all that sort of thing right and Google figured out the cursors right you can see where we're each other where the where the authors are right again pretty simple but once you figure it out you can two people in two different continents can work on the same document simultaneously example involving people in the design process itself you don't quite like the interface you go in and and change it or make it better yep okay yep sure I de editing editing software documents changing the interface itself the wiki revolution right that web pages didn't have to be something that was fixed you could actually go in and change the wiki page without having to know HTML made a relatively simple interface to go in and change and edit and add which paved the way for Wikipedia and so what are some simple things on your desktop or on your laptop that allow people of diverse physical abilities to easily use the software interface sorry voice to text okay yep good example other examples some people provoked to prefer to speak rather than type or depending on your current situation which of these two are you want to use yep in the opposite texts these text to speech okay what else some of you are wearing glasses some of you aren't exactly right relatively easy on most platforms to very simply increase or decrease font size right you get on a new computer or you projected on the overhead here and you're very rapidly change font size you're not this example okay again our red boxes there's just a few examples that we touched on you can write down some of your your own right okay all right so we're going to move on now to lecture three maybe okay so we're going to talk now about packed analysis so we're good HCI programmers we know we have to think carefully about people but how do we actually do this so I even come up with examples where this is done well and answered ideas where this was done poorly but what were the creators of Google Docs thinking about when they were trying to solve the problem about simultaneous authorship allowing two or more people to write a document at the same time what was the design process they went through to lead them to to that idea so we're going to start today with packed analysis or analysis in general and then in lecture for we're going to move on to this idea of design actually sitting down and trying to create these kinds of interactive technologies that are easy to use okay so packed an acronym just for people conducting activities in a context using technology and what I want you to take away from this acronym is then again it's not just about people right so what is it that the person is doing what is the activity and what is the context in which they're doing it so as you were going about your day and you've got your smartphone there are certain contexts that you're going to be in when you're going to prefer to type rather than dictate and hopefully in this context right now you're going to choose to type rather than speak right and at other times it'll be the opposite so it's not enough just to think about the person but what is that person doing in their everyday lives where and when and how are they going to use the technology so again we're going to talk a little bit about design how do we actually sit down and design software that supports all the people we want to support in all the activities they want to carry out in all the possible context or circumstances where they might do so we're going to obviously talk a little bit about technologies but we're going to do that last we're going to think about P&C carefully first and once we've done that we should have some very specific ideas about t ok so people who exactly are we designing our software for who is going to be affected by it so we're going to spend a little bit of time talking about users and stakeholder so a steak there might be someone who is affected by a piece of technology but never actually uses it themselves can you think about a piece of technology and stakeholders people that are affected by technology but aren't actually using it sorry okay right so it's the company but what is the technology maybe the company produces the technology whether it's right there going to either make money or they're not going to so maybe you do decide to dictate into your smartphone in class right we're not using your smartphone but you're definitely affecting all of us okay again this is something that's often missed you think very carefully about all your users you support them perfectly and then it turns out that there's a much larger group of stakeholders we're being affected by the technology an important aspect of thinking about people when designing technology okay and again activities in context this one can be a little bit more subtle and difficult to think about it's not just the person but where and when are they going to be using the technology okay so let's try this here's three groups of people and activities teenagers who are texting each other people holding a video conference and someone who turns on their robot floor cleaner which is the iRobot Roomba that you see there okay so we want to create technology to support these groups and I'm going to now add context so teenagers sending text messages to friends while driving there's the context people hold a video conference somewhere inside somewhere outside again this used to be a fictional scenario but now if you've got skype on your smart phones you can hold a video conference inside or outside maybe not so fictional anymore someone turns on the robot floor cleaner but they happen to have four cats at home okay let's focus on the first one how does your thinking about the technology immediately change when I added the context of while driving imagine you're going to write some additional software for a smartphone to try and support the recent law against texting while driving right it's being enforced but is there a way we could help enforce this law through technology allow it to interpret voice okay okay right so my bluetooth breaks and I say okay forget it I'm not going to use the voice I'm just I really kind of tell my friend about this it's super important so I'm going to text anyways okay so you have an accelerometer in your smartphone if the acceleration is above some third certain threshold text is turned off it's three lines of code is this a good solution could be yeah okay that could be let's come back to the acceleration threshold turning off texting you're a passenger right you're a stakeholder you're not actually driving the car you just happen to be in the car and you're affected by this this software right it's good first approximation but now there are these these ripple effects right so maybe that's not a good solution maybe we go with having to turn your car on with your cell phone or there's some authentication detail there maybe it's not a good solution big part of HCI is trying to poke holes in all of these solutions because there's some detail of the context that doesn't hold up what's that let the cars drive themselves right and then we context as we go that may soul thing that may solve everything I'm like yes okay I have to say like okay I'm not driving haha not sure if that's actually a good solution because that's right I'm trying to text you now notification comes up which draws my attention to the phone I think about what they say and have to pray Express yes or no right again we're going to spend a lot of time talking about cognitive psychology and this perception action loop right your screen flashes something because acceleration is detected above some threshold if something is flashing or moving or making a sound at least for the first tenth of a second you can't help but turn and visually attend to that stimulus right you could try and ignore it as best you can but you're attending to it and it's drawing your attention away from something else okay yes maybe driverless cars will make this all academic but again a good example of thinking about how thinking carefully about context or changing your thinking about context immediately changes your thoughts about what the technology should actually be okay I'll leave the other examples for you to fill in at your leisure ok so we've already talked about one feedback loop in this class which is user waves their hand over leap motion and they see something on the screen which causes them to change motion and around and around you go right that perception and action feedback loop the John Dewey sensor motor coordination loop that we talked about last time happens at a very fast time scale right milliseconds or thousandths of a second there is also a longer feedback loop that occurs with all technology which is the moment you deploy a technology you're going to change or increase the number of activities that people can carry out and maybe change their the context in which they do so so the moment that texting the first texting app was deployed for a smartphone that immediately created the novel activity in context of texting while driving which now might create a new opportunity for another piece of software that suppresses that activity what are some other examples of technologies that once deployed created new opportunities that were capitalized on by new kinds of technology smart phones themselves right the internet the smartphone right they generated all kinds of new new opportunities you want to get ahead in computer science and HCI be aware this much longer maybe not that much longer feedback loop so we're going to spend some time again when we're talking about design is thinking very carefully about our people activities in context pa-c and we're going to boil these down to requirements they're going to let us think about the technology with the technology should do and what it shouldn't do once we deploy the technology that might create new opportunities that we want to capitalize on okay so very quickly let's try and spit out some requirements for a video conferencing system that can be used both indoors and out of doors what is one important thing that such a system must be able to do video audio balance absolute 0 white balance why auto white balance in this context absolutely right so if you don't have experience with video you might not or sensing technology in general you might not be aware of how extreme differences and light levels are inside and outdoors if you're a photography expert then that you are we're going to talk about the human visual system which cancels out all of these differences right the light levels in here compared to light levels out there don't look that different to you because your brain is normalizing those those differences and your software better do the same thing Jeff and I do the fact that I'm fixin happy absolutely right so you may be inside there may be external noises but for sure when you're outside there's probably a lot more background noise than that inside has anyone tried to Skype with someone who's outdoors skype does a pretty good job at cancelling it out but not not perfect it would be extremely disruptive okay I'll leave the robot floor cleaner again as an example okay let's assume that we create this outdoor video conferencing system we deal with the light level issue the background noise what sorts of new activities are Mike this create besides holding a video conference with some people indoors or outdoors how might just be useful researcher actively what they're doing instead of think a video of it so I'm going to ask a question and ask investigating absolutely right so rather than embedded media you have embedded professors or field researchers right you're teaching from the field to students in in real time right you can almost do this now with skype and smartphones but not not quite there yet it's a good example okay so in the rest of lecture 3 here we're going to march gradually through p and a and c and then eventually to t let's start with people and forget about activities and context for the moment we've already mentioned some of this obviously people differ and again some of the ways in which people differ are more obvious than others wait so if you're dealing with leap motion very quickly realize that the size of your hand matters and if you think about it a little bit more matters whether your lefty or right-handed people differ in all different kinds of ways as well okay pattern recognition software is getting very mature now you can usually find faces and people in a video stream automatically but it's not perfect right it obviously depends on the person whether they're recognizable or not imagine that have some person recognition video surveillance software that you deploy it's pretty good but not perfect it doesn't do so well with people that wear certain color clothing but not other kinds of color cloven not such a big deal imagine that your software works better or worse for people with different colored skin from the computers point of view it's algorithmically it's almost the same problem but culturally and politically it's going to make a huge difference if you try and deploy that software right so now we've just jumped from physical differences between people to cultural psychological political differences between groups right certain errors in your human recognition video software might be acceptable to the group as a whole and some might be absolutely unacceptable ok so again obviously people differ psychologically different cognitive biases some people look like to look at pictures rather than listen to sounds if they're trying to work their way through understanding patterns in a data set so some people are better about thinking spatially some people tend to think better temporarily over time let me hear that again and I'll tell you if there's a difference in what I heard cultural differences an obvious example of cultural differences is language differences so if you look at the keys on the left there right we all recognize them as first or to the beginning previous next and last or to the end is that always true if those labels weren't there would everyone assume that you left hit the arrows that point to the left our first or previous are there groups of people that would assume otherwise okay looks like a play button in this part of the world why absolutely I could be next chapter but most right hand pointing arrows we assume means forward in time right mice might be next chapter all the way to the end or forward by five seconds but most of us will share that assumption right so we read left to right so words that are to the right in our visual field are words that are coming up on a page of text which led to this convention where right word for facing arrows tend to mean ahead in time and left word back in times if you're a young person growing up in Japan or Israel you don't have those assumptions right so you might guess and figure it out by again playing with the interface but if you want to skip to the next chapter if you grow up in Israel you might click on the left facing arrow right and suddenly you jump backward a chapter if you're a very young person that could be extremely confusing depends on who you're developing your software okay I you since is again you might have same people in the same culture but they have very different experience levels right most novices tend to work with the mouse most people if they're using the same software over and over again we'll learn keyboard short short cuts and work with it very differently discretionary users these are users who are being forced to use the technology against their will I hope no one in this room has ever used PeopleSoft Oh some people have I can tell right okay let's say as little about PeopleSoft as possible for those who don't know it's used for most human resources and admin features at UVM faculty faculty didn't have a say in it I'm pretty sure you guys didn't have a say in it right a decision was made and PeopleSoft it is it's hard to use already but now on top of being forced to use it changes the way that people tend to use it okay ok so again we want to think about all the different ways that people differ now we're going to think about the activities themselves what is the person trying to do what is the activity Mike the way in which two different people carry out this activity be very different so in the way that we just tried to think about all the different ways that people may differ let's think about all the different ways that a particular activity that someone's trying to carry out with your technology differs ok broke this into a list of 10 you could probably break this into lists of more or less but the first floor gonna have to do with temporal aspects or time related aspects how the activity unfolds in time is this an activity carried out by a single person or more than one person how well defined is this activity does the person actually know exactly what they want to do from one step to the next how safety critical is the activity how are you going to deal with mistakes on the point on the part of the user or mistakes on part of the technology and then only then ninth and tenth do we get into what's the nature of the content or what are the building blocks that are needed for a person to carry out this this activity ok so I'll jump backward forward and backward to this slide a couple times let's start with the temporal aspects how regular or infrequently is the activity taker taken place is it something they're going to sit down and do it all in one go is someone going to use your leap motion software and try and learn all of the letters in the ASL alphabet and one go are they going to come back to your software from time to time are they going to come back once a day maybe they'll use it every day for a month and then not use it for six months and then you come back again cognitive psychology people forget so what happens what should you show the user if they come back to the system an hour later a day later or a month later it depends on the person usage history again smooth there's going to be something they use continuously over time is your phone always on and vibrating and telling you whether you're getting text meant judges or is it something that you're doing at particular points in time if it's something if it's a task or activity that's interrupted how does somebody find their place again so what's an obvious at a widget in interactive systems that let you find your place again if you come back to an activity after some time has elapsed just yes okay Codecademy how does Codecademy help you find your place absolutely right to save function I got this far save whatever that means when I come back don't take me to the first page take me to my saved place right pretty pretty straightforward okay that's pretty straightforward but let's think a little bit more deeply about what the activity is is it linear progress can I start here and just go forward in time or is there more internal structure and to set up this discussion going to walk you through it the parable of tempest and forum this comes from Herbert Simon a Nobel Prize winner back in the 60s we thought very deeply about human behavior and economics okay in the parable of tempest of horror both of them are watchmakers who made very fine watches however as they were making their watches the phones in their workshop rang frequently new customers are always calling them hora prospered while tempest became poorer and poorer in the end tempest lost his shop or was the reason behind this well the reason behind it was because they made watches in different ways watches consists of about a thousand parts each the watches the tempest made were designed such that when he had put down a partly assembled watch and answer the phone immediately fell into pieces and had to be reassembled from the basic elements right not a good way to do things if you have to get through all of thousand pieces before you're interested hora on the other hand designed his watches where he designed his activity which was building watches in a different way so horrid designed his watches so that he can put together sub assemblies of about ten components each so he these sub assemblies of ten pieces then took these 10 sub assemblies put them together into a larger sub assembly and so on and so forth so that finally ten of the larger sub-assemblies constituted the whole watch each sub assembly once done and it was only made up of ten actions would us would not fall apart when when put down so here's my little cartoon to try and show you how this works so here's for tempest here who adds a piece of his interrupt if it breaks adds another pieces interrupted in rates or on the other hand make sure that he has a stable sub-assembly builds these sub assemblies and only then puts comes together so at any point in time and there's an interruption still has the component pieces what is the abstract principle that herb Simon was trying to get at here it's not about watches it's about the design process itself modularity right so when you save and come back to something it might not necessarily show you in a linear sequence where you are but if it's a learning piece of software you've gone through these three modules and your ten percent of the way through module 4 i'm designing this course to be hierarchical right the course is about HCI there are six themes within each theme there's a couple of lectures each lecture is made up of a bunch of slides each slide is made up of a bunch of bullet points and so on ok although this is a lot of powerpoint i try to organize things hierarchically so it's easy for you to chuck together the major issues we talked about and remember things better right if you're studying and you forget where you are at least you remember you were in lecture 7 and lecture 7 was about design in general right you can more easily find your place when you stop studying HCI you come back to it a few days later ok response time this is going to be something we talked about quite a bit so again back to the loop of sensor motor coordination that do we mentioned the moment I send commands to the muscles in my arm I see my arm enter my visual field almost immediately right throughout my life that expectation has never been broken if I move my mouse and I see the cursor move a tenth of a second later I'm going to be extremely frustrated right we have an expectation that when you move the mouse and something moves on the screen it should at least from our point of view be simultaneously can you think of examples where your expectations about these kinds of timing events were broken so we talked about the spiral of death right that means put all your expectations on hold I might get back to you in the next second I might never get back to you when your arm falls asleep okay how's that an example it's you you like try to send a signal through okay and it's not responsive or it's possible okay that's true right it's very it's very jarring right if you ever had that happen what about with software or technology you expected a particular rate of response but it wasn't met okay right and again our expectation about that is coming down every every year a very ever used a friend's laptop and the sensitivity of the mouse was much more or much less on your system right it's you kind of pull back for a minute you're a little bit surprised and then you gradually get used to it or you slow down or speed up the mouse response rate you have an expectation from your own system that doesn't hold up on on others okay again we'll spend quite a bit of time talking about that cooperation again we're going to spend a lot of time talking about groupware or software that allows people to work well together complexity what is the person going to do is there a very clear set of steps that they're going to carry out or is the activity kind of vague right are they going to surf the net or they're going to do some very specific research and wikipedia right how big or how well defined is the task then we get into safety right are we going to catch mistakes before they happen or catch mistakes after they happen are we going to do Proactiv error handling we're going to make sure that no one makes a mistake in the first place or do retroactive arrow error handling will deal with a mistake after it happens how do we decide which of these two to use here's an example where this went catastrophic alee wrong this is the case study of the ther act 25 I was a piece of medical therapy technology from the 1980s this is arguably the worst software bug in history killed five people injured many many more just going to give you the summary if they're at 25 and you can go read about this at your leisure okay there was an earlier device called the fair act 20 and in the tharok 20 and etheric 25 it was used for providing x-ray irradiation to someone who's already suffering from a cancerous tumor somewhere just under the surface of their their skin there's an electron gun electrons in red I apologize there's a typo with your slides it says x-ray gotten should say electron gun electron gun emits a beam of electrons these beams hit an X ray shield which was basically a lead block and this very thin focused beam passes partly through the lead and creates on the other side a cloud with a very specific geometry that provides when it's working just the right amount of radiation to just the tumor and nothing else pretty simple machine and again it was designed to be very simple because again as things get more complicated there's many more things that can go go wrong alright specification are the requirements for the third act series is a command is sent to move the shields into position and their positions slightly different for different patients given where the tumor is the shield's then move in response to that command the command is sent to the electron gun to fire the beam and the gun fires okay worked perfectly for the ther at 20-under these specifications it failed catastrophic ly in their act 25 using the same for specifications so without flipping to the next slide anybody want to hazard a guess as to why what could what would go wrong here absolutely right there's no explicit specification here that says fire the gun only when the blocks have moved into position right it seems like an obvious thing to put into the specifications easy for us to say after the fact okay so the actual events that occurred gave the third x 25 your command was sent to move the shields the shields started moving but hadn't reached their target position before the command was sent gun actually fired and in many cases patients were directly radiated by an electron beam that hit the surface of the skin and they received orders of magnitude more radiation than they were supposed to even a very very simple machine like this if you don't get your specifications right you don't think carefully about the hardware side of things in this case this wasn't really anything to do with the person or the activity but the context these these shields themselves okay clearly in medical technology we usually want to be proactive about safety right much harder to fix things after the fact so how would we know whether use a proactive a retroactive security system okay what is it that we're going to present to the user now we get into the nature of the content itself again when we start to talk about psychology we're going to spend a lot of time talking about visual perception and auditory perception how should you communicate information to the user when the user does something with your leap motion device what are you going to show them on the screen do you have large or small data requirements is your data streaming or chunky it's a terrible way of talking about data but captures the idea streaming you have a real-time video or speech stream chunky you have icons or text or images streamy has the added advantage in that you're able to communicate a lot of information pretty quickly an image is worth a thousand words and a video is literally a thousand images right you can actually present your user with a lot of information if you have a video stream or you have an animation might be too much you can also in a real-time auditory track often detect changes or rhythms or regularities much easier than if you're looking at the same data presented visually so we'll come back to that again so how you choose to present your data or establish the sensor motor link depends on thinking carefully about how the visual system or human visual system works and how the human auditory system works okay then we can make decisions about 2d or 3d are we going to use text or motion or sound or combine them and go from there okay let's move on to context now so again let's start with the most obvious things that move on so the less and less obvious kinds of contexts that are going to matter physical context pretty obvious is something that's going to be used indoors or outside is it going to be used on a smartphone with a whole bunch of people around is it going to be used in private does internet matters is something that's going to be used where there's lots of Wi-Fi hotspots is it going to be somewhere that's remote these are things you can probably answer pretty quickly once you start to think about your technology social context gets a little bit trickier how supportive is the environment are there a lot of manuals shows have dated this particular slide is right it was the last time somebody read a manual for their new smartphone or their new app they downloaded on their phone not too much that's evidence that people are getting better at HCI if you design your interface well you shouldn't need a manual and you should definitely need a minimum of texts okay assuming you do get confused how can you get support right on most web pages there's a question mark somewhere and it's usually in the upper right of the page right it's relatively easy to go find support or get help if somebody plugs in your leap motion device and you tell them to use it to learn ASL and they say I don't know where to get started there's something wrong right how do we know to show people where where to go okay social context again this is kind of overlaps with physical environment where they using it organizational context right what's the bigger picture here again thinking about stakeholders what's the impact on not just the person who's going to use this technology but all of their friends on social media or stakeholders that are never going to be are never going to see the technology directly okay if we deploy a technology that's useful you could be sure it's going to be disruptive and it's going to change the way people do things it's going to create jobs destroy jobs and change jobs okay so only after we thought carefully about P&A and see now we can sit down and start thinking about technology but I want to start with T you want to start with the other three these are a little bit more obvious what is the input to your leap motion ASL software what is its output and why what else is your software going to be communicating with and then finally the internals now you're finally ready to sit down and actually write the code itself we're not going to talk about that part in this course there's a sister course the HCI which is software engineering which is all about creating the guts of your software that the user probably will never ever see we're going to focus on the interface in this course which is really about input and output and who else or what else does your software talk to okay so let's have a look at a few examples now okay the infamous torrent who are the people who are the activities what's the context and what was the technological solution well in the 90s there was an observation that someone would suddenly have a very very large file at a certain time and a whole bunch of other people wanted exactly that file that only one person had all the pieces up so here's a very large file that's broken up into a bunch of pieces so the moment that you have one person that has a very very large piece of data which is suddenly very very popular to a large group of people you have a bottleneck problem right everybody is going to start to draw down that large piece of data from from that person BitTorrent turn things around and said how do we get past that bottleneck well the easy way to get past that bottleneck is to take that huge piece of data chop it up into small pieces represented by the colored dots here and give different people different pieces of the data don't give everyone the red dot then everyone the orange dot and so on right because everybody is then missing the other piece so once you give the red dot to somebody else that reduces the demand on you for the red dot right so BitTorrent basically blurred the distinction between servers and clients this technology was a response obviously to the beginning of sharing of illegal content okay so let's shift gears for a moment now here's again for more examples each one of these describes particular people and activities and contexts I want you to turn to your neighbor and just spend about 30 seconds or a minute on each of these for sketching out the ideas of technology that addresses the particular kinds of people and activities and contexts that are mentioned here you don't have to get into the details but just an initial sketch of technology to support stores in a mall that want to scan shopping bags of people walking by and show in the storefront ads tailored to whatever is associated with that particular kind of shopping bag people want to visualize bittorrent across the internet we talked about visual design how would you visualize who has what and could you guess from that that visualization what kind of data is homeowners who want not the Roomba that cleans your floor but another indoor robot that paints the walls a setup wireless sensors deployed in a rain forest to collect environmental data so wireless sensors as the name implies very cheap sensors that have a local battery source and they collect sensor data where they're placed and broadcasted intermittently back to a piece of software that collects that data okay turn to your neighbor think a little bit about the kinds of technologies that would support these people activities and contexts and we'll see what you came up with so okay let's let's do some very very quick brainstorming here let's start with number one who has an idea of Technology sketch takes into account this particular kind of context could visualize any kind of data than anyone has on their machines was specific to bit too right why would you want to visualize who has what Colleen able you things aren't I guess we folks downloaded in the current moment on ok ok bye this one just as a file ok the map of active file transfers that's a good idea right who traded what with whom right which would be probably important for people that are trying to figure out where and with whom a particular file originated again it depends on the context some of which is explicit some of which is implicit how about number two here anybody remember from minority report a few years ago Tom Cruise is walking through the mall and it starts to advertise things specifically for him which was a problem because he was trying to stay under cover we're pretty close to that not quite there yet yep make sense so let's make a smart shopping bag which makes the job easier right happened on the internet because it's it's easy to know who bought what at what at one point at target ads towards them right commits only change the context now we're talking about shoppers not online but in the mall changes the changes the kind of technology how would we go about making a smart bag let's pursue this idea from the engineers here thank you ok so RFID chips are going to come up quite a few times in this course do you want a hazard a summary of what an RFID chip is that's it so rfid tag collects radio waves and extracts just enough energy from them to broadcast something back right if we're going to attach hardware to bags they should be cheap and probably not have a massive battery where you go you could like along with container receipt printer QR code or something on the side of the bag okay okay right so we put you our codes on the receipt and stick it on the side of the bag right there's a cheaper option than RFID tags okay all right we'll leave that one for a moment let's do robots painting indoor walls right so all you got to do is look around this room and you'll start to realize there's a whole bunch of exceptions to just blindly painting the walls right walls aren't flat some parts of this room we may want painted some we might not we don't want to have to write some complicated software to let the robot recognize blackboards and moldings and so on again like the example can QR code let's just put up some magnetic tape that says don't paint any further than at this point okay wireless sensors deployed in a rainforest for environmental modeling what are some specific aspects of this technology we have to think about given the specific context has to be very energy-efficient right so they're going to probably be somewhere that's quite remote from a Wi-Fi hotspot or a source of power okay let's play a similar game which is I'm going to take the same for examples and now again change the context so in number one now we know a little bit more about the user group we're trying to support and they would like to be able to distinguish legal from illegal content and this is always a cat and mouse game because the moment they figure this out the tour enters we'll figure out some other way to disguise illegal content as legal content Joe question okay we want to deploy our system in the mall but maybe we don't want to be or maybe for legal reasons we're not allowed to target ads to shoppers that are under the age of 18 how would we deal with that change in this case people we don't want robots that paints over posters maybe our magnetic tape idea already deals with this situation how does your thinking about wireless sensors change when we're a little bit more specific we want the spins of the sensors to be deployed in the canopy in the rainforest all right let's just do this in an interactive manner pick any one of these for how did your thinking about the technology changed change the moment you saw the change in context or our target users how could you tell by looking at the dynamics of file sharing on BitTorrent whether someone had just shot a home movie that was large and was sharing that on the internet compared to the latest Hollywood movie I give ridiculo I know movies are formed in a lot so probably working some sort of software video sharing services have done just exams serve audio components or video components that are recognized yes we could dive into the internals of the data itself and look for certain signatures of the vehicle content but we could try and do it without having to do that maybe for privacy reasons and so popularity is one of them that would be one thing but cute kitten videos are also very popular on the Internet what I don't know what a torn tracker is okay mostly okay okay yes there's specific content again we could you could look at think about the context here new Hollywood movie is released someone pirates it and shares it on BitTorrent length length you'd be surprised yes maybe maybe length right there might be a whole bunch of these lines of evidence that we bring together right maybe you keep track of when something is deployed based on release dates of movies that are likely to be very popular on bittorrent sure right again we can get right into the content are there indirect cues that we could use if that's not available okay we don't want to personalize ads to shoppers under 18 okay so we could deploy some face recognition software Microsoft has this product now that tries to estimate your age directly from facial features could go that route what's the disadvantage of that approach absolutely right so back to safety what happens if it makes a mistake face recognition software is getting better but it's certainly far from from perfect if there's simpler solution than deploying face recognition software so again think about our initial sketch of this technology people who stores and get out the receipts or bags simple right so again it doesn't have to be a technological solution just not everyone is going to get a smart bag or a QR code right you show your ID and if you don't agree it's assumed you're under 18 and you're given a non-smart bag could be it could be something very simple just takes thinking about the technology okay I think we solved number three here already what about number four how does your changing how does your thinking about the technology change you want to specifically deploy to rain forest canopy okay right so it either has to latch on to something we're going to have people climb up and deploy them how are these actually going to be deployed what else changes actually I already not very expensive laughs absolutely right so physical context is important here we can't assume massive batteries somebody mentioned power already probably want to go with solar if it's a rainforest there's probably plenty of sunlight around it's not the rainy season but you specifically want to be in the canopy sollars out again right this one has a lot to do with the subtleties of the physical context in which you're going to deploy the technology number two has a lot to do with political and legal aspects of context okay that's the end of lecture 3 so this is probably a good place to stop for today thanks for bearing with us in this particular physical context of massive heat we'll see you on Thursday quiz number three will be doing 1159 tonight I'll go back and put it on blackboard thank you 
DDE1b9fDars,28,"All lectures: 

https://www.youtube.com/playlist?list=PLAuiGdPEdw0iLnUFP7kALZf3SbGIokPKt",2018-10-30T15:33:11Z,"Human Computer Interaction lecture 17: Activity tagging. (Filmed Oct 30, 2018)",https://i.ytimg.com/vi/DDE1b9fDars/hqdefault.jpg,Josh Bongard,PT1H14M3S,false,59,2,0,0,0,"okay let's get started a few housekeeping notes before we jump back into lecture today first of all I'd like to advertise an upcoming talk this thursday by todd lee tom is the CEO of Emotiv corporation and towards the very end of the semester we're going to talk about cyborg technology and brain-computer interaction so BCI is the idea of creating computer technology that reads your thoughts and interprets your thoughts and acts on your behalf so one of the most interesting the most recent and the most controversial form of human-computer interaction both corporations leading the way in developing a particular form of a EEG headset that you wear and deed it can read your thoughts and an interesting and controversial technology I realize we're all very busy at this time of year but I guarantee you this will be a very very interesting lecture he will present this technology which again we'll talk about towards the end of the semester the talk is this Thursday I IRA Alain Chapel 5:30 p.m. she will also be conducting a live demonstration of her EEG headset she will invite someone to come up from the audience wear the headset and within about 180 seconds the device will train on that person's particular brain activity and be able to read their thoughts this volunteer has to be someone who can remain relatively calm on a stage in front of several hundred people why does that matter if you're nervous in front of a bunch of people it's gonna make it very difficult for the device to read your thoughts right physical and social context yet again you also have to have relatively little hair in order for the EEG electrodes to be able to record just under the scalp and this volunteer is going to be a very senior member of the UVM administration this person does not know that they're gonna be the volunteer yet so keep that to yourself I've given you a couple hints you might be able to figure out from that who this person is going to be so as I said I guarantee this is gonna be an interesting talk please do try and make it if you can there's a link there and RSVP should be should be a lot of fun I will be hosting the question and answer period at the end I will have an additional sign-up sheet with me if you come and sign up to the signup sheet after the qat there will be an additional 3% extra credit for this course so please please do come I was the one that suggested to UVM that we invite Tom leaves definitely an interesting HDI application extra 3% credit if you attend and sign the signup sheet so after QA when everybody's filing out come up and just sign your name 3% yes it's the artist in P just for a good number I'm not sure whether you actually get a ticket or whether it's just for a number count but please do RSVP I think it just helps get everything ready for format okay so that's Thanh Li again I was away last week at a DARPA meeting and professor bagra was here and I believe he walked you through some of his crowdsourcing research hopefully you found that interesting I was also a timely lecture because in this look at the beginning of this looking out section we're thinking about stitching technology into the world around us and as we do some of that technology is being used simultaneously by many people so how can we farm out a task via the technology to a large number of people where they can collaboratively do something together and allow them to do something that would be difficult or impossible for them to do together or allow or allow us as the creators of the technology to study something about social behavior among humans that would be difficult or impossible to do otherwise okay again I was away last week in Chicago and as usual I brought you back something to bribe you with given absence I have Chicago Cubs and Chicago White Sox chocolate as far as I understand there are no nuts break yourself off a piece and pass it along to the next person okay plenty for everyone this morning thanks for thanks for bearing with me with all the absences I will be here until the remainder of the semester okay so where were we where are we and where are we going again we're working our way through this section on looking outwards thinking about creating technology where it isn't a traditional one-on-one interactions between one person and one piece of tech we've got a lot of people going about their daily business in the real world carrying technology moving through an environment in which there is technology stitches stitched into the environment so on which is this idea of ubiquitous computing which we covered in lecture 16 in lectures 17 18 and 19 we're looking at three different research projects very different research projects all of them are using ubiquitous technology to try and study something about people that was difficult or impossible to do before so beginning of last week we looked at social network inference so given the fact that someone people are walking around with a microphone relatively near their mouths can you infer from their conversations among other people what the social network is once you infer what the social network is who knows whom and who tends to enter into conversations more often with whom can you tell something about how they modify their behavior given where they are in the social network get a very difficult thing to do without ubiquitous technology we're going to finish our lecture today on activity tagging so it can be actually infer from this thing that you carry around in your pocket pretty much all day every day can you infer something about what someone is doing from moment to moment and use that to pass that raw data collected from the phone through Senate models that makes a prediction about whether today was born less healthy compared to your days in the past so we're talking about activity tagging but we're looking at we're looking at that in the larger context of mobile health monitoring some of you wear a Fitbit if you're carrying one of these things around as well it may it may already or it may in future be able to infer something about how much sleep you're getting how much activity you're getting how much social interaction are you engaging them from day to day and then use that to make an estimate about how how your mental and physical well-being is going we're gonna probably finish lecture 18 today we might start in on lecture 19 today which is the human speech aroma project with speech on here what is it Omer anomic s-- the development of human language we know the human speech on project is about the development of chip of language in particular how a human child goes about acquiring language what do you think the word speech alone means what other words do you know that ended over the genome right so the genome was mapped out back in the 1890s which was to take to take our key moment and understand or be able to record from every single gene on the genome so basically recording the entire thing anything else sorry the chromosome right exactly there is also that now the connectome which is a project a set of projects to try and understand in as much detail as possible the connectivity in the human brain so map all the way down as far as we can go all of the details about the human brain that's the connectome so the speech omits a little bit of tongue and cheek but it's an attempt to try and understand all aspects of speech or all aspects of speech acquisition you all can speak English relatively well you most of you learned it when you are a very young child how and what caused you to acquire language in the way you did it's a great that you did short answer is the moment nobody knows because it's a very a difficult thing to measure in the human speech on project it's a somewhat controversial project where someone's home was instrumented to record as much as possible the first three years of a human infants life and then use that entire dataset to try and understand how that child acquired language during their first three years of life okay we'll come back to that in a moment let's go back to lecture 18 and before we do it just talk a little bit about liberals hopefully most of you are on track to submit deliverable 9 Wednesday night and then Thursday we will talk about the tenth and the final deliverable next week when deliverable 10 is due we will talk next week about what you will to be doing between the submission of deliverable 10 and working on your final project through the rest of the semester until you present it orally during our examination period talk about that next week all good any questions about the deliverables ok all right so back to lecture 18 and activity tagging again this is the idea of not just trying to tag activities but do this within the larger context of mobile health monitoring this is developing into a huge industry what are all of the things that go into our physical and mental well well-being there are obviously lots of aspects of our behavior that do how all of these things combine to actually predict your well-being again a very difficult thing to do so we're gonna focus in this lecture I'm just trying to use your smartphone to predict some of these aspects of behavior okay so in the paper that we're talking about today there were three goals the first goal is to try and measure correlates of well-being so one of these things like sleep physical activity social interaction measure one of these things continuously and automatically right we don't have someone continuously having to log how many hours of sleep you think you got how many hours of exercise you think you got how many conversations for a given day you paid it in and so on we want this to be automatic and we want it as best as possible to be continuous once we are collecting these raw measurements and we then push these measurements through a machine learning algorithm that will produce at the far end a prediction of your well being so if you get four hours of sleep hopefully that machine learning algorithm would output a negative number it's probably not so good for you if you got 13 hours of sleep or spent 13 hours in bed hopefully it would also output that maybe that wasn't the best way to spend your night but seven to eight hours it would produce a positive number right we might not be able to get much beyond that we're just going for an approximate prediction about how these correlates are contributing to your well-being and then finally take these measurements or these predictions and create some visualization that we can present to the users that it's relatively easy to understand right so imagine you can glance at your phone and it gives you sort of a picture about how you're doing in terms of sleep physical activity and social interaction for today yesterday last week this week compared to last week and so on something that's sort of relatively easy to understand and might help you try and modulate some of these things in the coming days and weeks to improve your well-being okay that's what we're trying to do we've looked at some of the raw data last time let's actually dive in and look to see what they actually did we're gonna start with sleep when we brainstormed about this last time most of us have a more or less regular routine and that includes our phone myself I tend to plug in my phone at night I sleep at about the same time obviously my phone is not moving while I'm sleeping it's relatively quiet throughout the night and so on so we might be able to take several measurements from a phone again assuming the user is okay with us doing so and combine these measurements together and push them through a model to predict how many hours of sleep we got the researchers use more or less the idea they found that for the human subjects that participated in the study that the phone recharging lack of motion and ambient sound or lack of ambient sound was enough using a single model they didn't use the KN algorithm but they use something else that was relatively simple that could predict plus or minus ninety minutes how many sleep you got how much sleep you got it Stickle off by a pretty big margin but not bad so we have raw data which we're converting into number of hours of sleep and then we're gonna take numbers of hours of sleep and push it through this model which is gonna give us back a number which says whether the amount of sleep you got last night is positively or negatively impacting your well-being we're not going to go into the details of this you'll notice there's an exponent here and a fraction here what is this fraction well on the numerator we're taking the actual amount of sleep over a 24-hour period as predicted by your phone subtracting the ideal amount of sleep which according to the researchers is seven hours there was an interesting meta study that was published this week that looked at a whole bunch of sleep studies published in the literature and it turns out that generally speaking most people need about the same amount of sleep a lot of people report they need much less or much more than seven hours it turns out that for most people that report they need less or more their lives they're not doing as well as those that report they need seven or eight and get seven or eight hours of sleep again an active area of investigation but for our purposes we're in assume most people need about the same amount sleep so we're subtracting the ideal amount from the actual amounts so you can imagine if that difference is larger or smaller if that difference is larger you're impacting well-being if you're getting close to seven you're doing all right okay they're dividing by H I and H are low which is the upper limit and lower limit of acceptable sleep which again is kind of interesting I'm not sure how they came up with those numbers is that something that they derived or that something they got from the literature but if you take this function and you plug in various values for these four four variables you get something that's not quite a ballot curve but pretty close to it where the horizontal axis is the number of hours of sleep and this model will give you back a prediction which is the vertical height here of well-being so we sort of have a two-stage process we have raw sensor values from the phone being combined to predict the numbers of hours of sleep you got the user doesn't have to type in how many hours of sleep they think they got and then this model is predicting a number which is positively or negatively contributing to prediction of your physical 12 you that's sleep relatively straight folk okay let's move on to physical activity this one is much more difficult because there are many different ways that you may go about getting physical activity during the day and some of these are are easier to collect with a phone than others walking running and biking pretty easy swimming not so easy right physical context so again we're gonna do the same process we just did for sleep we're gonna get an estimate which isn't perfect because a lot of different ways of getting physical activity are gonna be missed by the phone the Fitbit might capture more but still there are things that are going to be missed assuming that you're a user and you allow this app to collect whether or not your battery is charging clock accelerometer digital compass GPS microphone and camera how would you start to infer whether a user is getting activity and better yet which particular activity they're engaged possibly yeah exactly that's it right so if I leave my phone in the locker better let it yet next to the pool maybe I actually could in for swimming good point the accelerometer is probably going to be useful right so the first exercise here is sort of figuring out which subsets of our sensors are going to be most useful accelerometer is going to be pretty useful but it's also problematic possibly absolutely right this is not a trivial thing to do remember a few weeks back a few weeks back we were talking about the knn algorithm and we started with flowers and those flowers had four features who decided that those four features were the most useful features for predicting flowers there was someone who knew flowers pretty well those were actually a pretty good choice that's the feature selection problem in machine learning right we want to try and predict something from data where's that data gonna come from someone hopefully an expert has to decide what things are going to be measured which things are going to produce the data that we're going to train the machine learning algorithm on right so we're already in the business of choosing the features assuming we're going to limit our investigation to a smartphone we already have a relatively limited palette of the things we could choose anyways and any one of these features on its own or even a few can always be fooled in some way right so we want to try and not just pick a single feature but combinations of features and figure out how to combine these features so that they corroborate one another might be useful right especially for things where you're moving around assuming you're you have the smartphone on your person or it's a Fitbit and has a microphone which I don't think it does but if you have a microphone nearby enough you might be able to use ambient sound but again it depends right physical context matters here at cultural context with social context when I go to the pool and may or may not leave my phone next to the pool it may be in my bag or not so we sort of 50/50 whether you're gonna get ambient noise that would help possibly absolutely so there could be messages in the app that say if you go for a swim just make sure the phone is outside your bags somehow right so now we're asking the user to help a little bit which which may be okay ideally we'd like to do this without the user having to do much that's going to make things tricky let's focus on just this let's forget swimming terrestrial sports assume someone is actually wearing their smartphone so we're getting accelerometer data still problematic why sorry I might be in a car I might be in a car on a country road in Vermont in which case my accelerometer is going crazy and reporting it looks like I'm actually walking or running on a rough road highway maybe not very different so this is a very challenging thing to do a lot of study going into this and obviously Fitbit and other devices show that there's a lot of money to be made if you can figure out how to do this well so let's look at physical activity let's let's sketch out a basic strategy let's assume we're going to try and focus on just inferring physical exertion from accelerometer data obviously the first thing for the first thing you might want to do is try and measure human subjects actual exertion so let's get some physical data from someone when they are actually the walking running or biking while wearing a phone and read here I'm sorry there's a typo on the slides for lecture 18 you might want to go back and correct that we're gonna measure actual exertion data from someone who is walking or running at the same time they're carrying their or they're wearing their smartphone so they're also getting accelerate accelerometer data we're then going to create a model that takes as input accelerometer data and it's going to transform that data in some way to make a prediction so the solar on the model is going to make a prediction about exertion we also have actual exertion so we're going to try and improve our model we're going to alter our model so that it alters its predictions and we're gonna keep improving the model so that these predictions closer and closer match actual exertion so we're going to train the model so that it's prediction about the amount of exertion matches what's called ground truth we know exactly how much calories the human subject is actually burned okay how do we do that lots of different ways we can do that in this particular study they captured ground truth using of vo2 max or Delta vo2 which is the change in oxygen o2 and the change in volume of o2 so if you're merit wearing a mask that is able to record the amount of carbon dioxide carbon dioxide and oxygen while you are exercising the more you exercise the more oxygen you're bumpier muscles burn and the more co2 is being pulled out of your in inhaled breaths it's pretty standard practice in physiology and physical therapy and so we're going to measure while someone does this and the more oxygen that's burned the more we can infer the more calories that are being burned that's our ground truth so we're going to measure vo2 we can then think about where's vo2 coming from well we could break this down into a function of resting metabolic rate if you're in good shape you burn less oh - when you're at rest than if you are not in good shape you can then take the movement itself and think about the horizontal and vertical component so now we're gonna get really down into the nitty-gritty details how much is someone moving backward and forward and how much are they moving up and down why does that matter why would he want to break this into horizontal and vertical components we're gonna try and model these things in a moment there's energy differences absolutely one is more owners than the other why why is that absolutely so walking and running have different horizontal and vertical components absolutely right so up and down up is hard because you're working against gravity so we're really getting down now to try to understand the components of physical motion and how to use that okay okay so let's use our accelerometer data and we're gonna try and compute H first the moment we're going to assume that we know something about the person's resting metabolic rate we can maybe compute that beforehand we're gonna focus just on each and V for a moment H is relatively easy because if we take accelerometer data we're getting this bumping back and forth and if you have a tri-axial Excel accelerometer it's giving you how much someone is moving forward and back to some degree it's giving you up and down and left and right we're gonna collapse all three of those for the moment and just look at the frequency of acceleration across all three axes excuse us just this sine wave how much they're hoping we're going to assume we're gonna set some threshold and we're just going to count the number of times that this sinusoidal pattern Hacha it passes the threshold so if you're walking running or biking or even if we measured this in the pool with swimming these are all regular motions they aren't random so if the regular is going to be this regular pattern and if we measure the amount of times that this acceleration passes a particular threshold we can just count it which will give us back frequency right frequency tell us tells us how quickly someone is doing something per second so depending on the person we can tell more or less whether they're walking or running what is the frequency of your footfalls as you walk for most of it it's more or less the same not exactly the same we've taken many steps in your life you thought maybe not not actually thought about this it's it's a little bit fast one second it's around one second per footfall for most people right so if you're getting acceleration data and you're seeing that you're passing the threshold once per second you can be relatively confident that it's the person walking right if you're in a car that's going over a bumpy Vermont Road you might also be getting a value that's passing the threshold but it's not doing it at a regular interval right whatever it is it's not walking okay so again often and certain smart phones we have tri-axial measurements so we have up and down side to side and forward and back we're assuming for the moment we don't have access to that we're trying to infer this from the information okay how do we go about computing V up-and-down this one is a little bit trickier we couldn't for example use a barometer if you have one on your smartphone the higher up you go the lower the air pressure not a great choice why not absolutely right so barometric pressure changes very very little it's not probably not going to be accurate enough if you go to the top floor of this building you're still inside barometric pressures can be more or less the same so maybe that only helps you with vertical outs outside rather than inside so we're going to use we're going to try and use the accelerometer data again but we just had the single signal how would you infer V from that you could use that right so when you go up and over the top right so you can tell something about actually are they lifting their laggers they're laying falling if you're careful and if they're going up or down there's gonna be a bit of an asymmetry in this information nice rising shoes there should be a little bit more rise than there is fall so we might actually have to look at the shape of the curve itself rather than just when the curve is passing some threshold so we look at the amount of time that the signal is rising and if that's a little bit more than the time during which the signal is falling then that's someone taking longer to raise their foot and it takes less time for their foot to come down and hit and stop the oscillation and start the next one with the next foot because their foot has hit the ground higher than where it left the ground right we don't have a pressure sensor in the sole the person's shoe so we don't actually know when they're hitting the ground leaving the ground we're trying to infer all this from acceleration data which again is a tricky thing to do but with that hint you might be able to train a machine learning algorithm to do that okay like all of these things even given this heuristic there are ways to trick it how might it think how much you get a false positive it thinks you're walking uphill but you're not stair climbing machine which is fine from your body's point of view you're still climbing stairs absolutely right if you've never been in a cast your your gait is asymmetric right now we're maybe wearing a cast is just as exerting as as running I don't know but again there are lots of things again thinking about somebody's daily life where you might get a signal where it miss infers what's actually going on maybe wearing a cast is kind of a rare enough event that we don't have to think about it but in all this modern mobile health monitoring you can usually think of a large number of exceptions which are difficult to deal with tricky thing to do okay okay so back to back to what we're doing here we're trying to measure actual exertion data create a model from the acceleration data and produce a prediction of exertion and they're gonna try and improve this model like we did for sleep in this paper they kind of cheated a little bit they didn't go to all the effort we just talked about they talked about in the paper and then said that's pretty complicated let's take a step back and try something slightly easier which is first of all to just recognize individual tasks more or less like we were just talking about driving stationary running or walking they then instead of fitting their human subjects with vo2 instruments and having them run or walk on a treadmill they went to the literature and they looked up the metabolic equivalent for these tasks so how much does the average person how many calories does the average person burn when they're performing any of these four activities and then take tags so throughout the day which of these four tasks was the person involved in and multiply that period of time by the metabolic equivalent of tasks and some met all up so figure out what MIT is for walking and if you infer that someone was walking for two hours take that ma me2 and multiply it by two the number of calories per hour and sum all of that up and that gives you any G actual which is the actual an activity per day you probably want to mentally put scare quotes around actual because we're not actually measuring their exertion we're predicting it so here's our prediction down here metaball actual which again we're getting back from a model and now we're going to push this through a second model which is this one it kind of looks like the sleep model that we were looking at before but with an important difference which is there's real no there's no real upper bound on this so this assumes the more exercise you get per day the better it's probably not true if you're walking or running for 5 or 6 or 7 hours a day that might not be the best thing for your body in the long term regardless we're just assume that more of this means more more physical well-being they're multiplying it by high loads subtracting low just to try and normalize this between the maximum recommended aerobic activity per day and the minimum recommended per day so sort of where do you lie in this range so we now are armed with two models one which is looking to see whether your sleep is as close as possible to seven hours more or less the second one physical activity physical activity which is looking at physical exertion that one's giving you back another number which is basically how much high how much aerobic exercise are you getting per day per week we got two we're now going to tackle social interaction and again we can't measure that directly so we're going to try and infer it what are we going to use this case the microphone like we did in the previous in the previous experiment and we're going to measure the number of hours per day that they're involved in a conversation which is going to be the duration here the actual duration of conversations and then again we're going to normalize it by high and low and again this model looks like the one that we just looked at so the assumption here which is the more conversation you're involved in throughout the day the better again that's probably not a very good it's not very accurate good enough for most purposes if you're not involved in any conversations at all throughout the day or the next day or the day after that maybe not the best thing for your mental well-being okay they're using the microphone for this what could possibly go wrong here what are some false positives and false negatives sorry music right a false positive someone's singing and you're in the music and your phone picks it up you're not engaged in social interaction or maybe you are depending on how you define it yep watching TV right there's lots of human voices that your microphone might pick up throughout the day hard to say whether those are human voices of an actual human that you're engaged in a conversation with maybe you're talking to yourself out loud that's a good one right is that good or not for your well-being I don't know maybe that should positively count but that's an interesting one we need to psychologists for that one yes pickup absolutely right you're sitting in the library trying to study and they're two other students nearby that are yakking away and your your microphone is picking that up okay to be in a classroom you're picking up my voice unfortunately for 75 minutes right okay again it's not an easy thing to do if you go back and look at the previous lecture they were again ways that they tried to act to really dig down into the microphone data and improve the prediction that someone is involved in a face-to-face conversation assuming you have access to the raw data coming from a microphone you but you could imagine creating more and more sophisticated machine learning algorithms or you get better and better at making that actual prediction okay again we're gonna we're gonna normalize this and assume more conversations the better here's some actual data reported from the paper for given subject how is this subject doing throughout their week was this healthy weak and unhealthy week what's that they had a cheat day on Friday activity absolutely yeah maybe they were they were couch surfing all day okay remember in this case green which is activity or physical and blue is social in this simplified set of models more activity and more social is better so the dip and activity here again take this with a grain of salt red you sleep where in that case there's more can be bad and less can be bad we're looking for the sweet spot so it's a little difficult to say from this picture exactly how well this person is doing but again assuming this is you you might be able to look at this and say my gosh I didn't realize you know how little how little I got done on Friday and Saturday I'd better pick up my workout routine next week the end of the paper they introduced this cute little visualization of course this was early 2000 so this was fun because you could actually do this on a phone I think the clownfish was the orange fish there was for exercise and the school of fish was your amount of socialization so the more conversations you were involved in over the last few days the larger the school of fish and the turtle there's either sleeping or not sleeping I don't know how helpful that is the idea was to try and create some simplified visualization where at a glance you would get an idea of how you're doing in terms of these three correlates of well-being from day to day right might not be everyone who's willing to look at technical graphs can we make this somewhat interesting for the common nursery okay it's an interesting paper the reason I picked this one is this is one of the first papers to try and get at mobile health monitoring and I hope that throughout this lecture you've seen this is a very challenging thing to do Fitbit and other devices are getting better but it's extremely difficult to infer from raw data what a user is doing throughout their day and whether that is we are negatively impacting their well-being we could ask them but often self reporting is very misleading I could ask you how many hours of sleep you got yesterday and you may tell me how many hours of sleep you got me the night before but you might have forgotten that you also nodded off for half an hour at lecture during the day right how much total sleep did you get throughout the day was it a whole bunch of naps during your lectures and then for hours at night or you managed to stay awake during all your lectures and you've got a full night of uninterrupted sleep those things matter right you might neglect to mention that or to type that into an app if we had an app that was recording your activity throughout the day and night it might be able to get at that or disambiguate those two types of sleep patterns and communicate it back to you okay okay so again just to summarize here we were only or this paper these researchers were only trying to get at these three correlates of well-being they didn't tackle intellectual stimulation diet and stress even with these three which are relatively easier to infer predict than the others lots lots of challenges and difficulties there okay so that's the end of lecture 18 we're gonna move on now in lecture 19 to the human speech own project this is the third and final research project we're gonna look at where they tried to use ubiquitous technology to study some aspect of human behavior and in this case we're gonna look at speech or language and we look in particular at how human children tend to acquire language again without technologies is a very difficult thing to do one parent may happen to be around the first time that a child says their first world word they may not it may be it may be someone else who happens to hear it even if they do hear the child utter the word for the first time that parent and psychologists have absolutely no idea why the child issued that particular word at that point in time that room of the house in the presence of that particular parent it's a very difficult thing to do what is the physical context social context and cultural context that surrounds a child in the first years of life that supports them up until they utter a given word for the first time I'm sort of a mysterious phenomenon can we use technology to understand this process better okay so we're gonna look at some work that was reported back in 2009 now so again it's a little bit dated there's been a lot of work since but it was a very landmark study at the time they're gonna try and tackle the following questions how do children learn language what aspect of a child's physical and social environment influence language learning and what events in their past what event in the last second last minute last hour last day last month last year influenced language learning so if it imagines a very tricky thing to do how is this normally done so again I mentioned this before developmental psychology the study of human development as children as humans go from child to adult what are the processes that are going on that scaffold them that provides support for them to start to master various tasks that you need to you need to master in order to be a fully functioning adult one of the most important ones obviously is language okay so a typical approach we invite a parent and a child into the lab we observe the child for a short period of time usually this is a few minutes or a few hours in a controlled setting a laboratory and for example during that we might ask the parent to just play with the child in the way that the parent normally plays with the child and scientists as psychologists we might observe very carefully this interaction we might observe at a certain point in time but the parent in like you see in the picture here the parent looks at an object and repeatedly pronounces the name of that object blue block at the same time the child is looking at the parents eyes not the object it figures out where the parent is looking and uses that information to pay attention to the same object question sure that's a great question right how does how do we even know that the child heard the word blue block they heard it it landed on their ears and we talked about this a few weeks ago right the difference between sensation and perception they sensed the word blue block it arrived at their ear but they may not have perceived it it may have just sounded like a bunch of ground random syllables right it's a very good question and again there's no clear answer to that however if the parent utters the word Vblock and a few seconds later the child says blue block there's some behavioral evidence that they heard it if they don't say it you don't know that they didn't perceive it right that's that's tricky the unknown unknown okay so continuing the example here the child looks at the parents eyes and we see the child look at the parents eyes and then we see the child saccade or move what they're looking at so they're no longer looking at the parent they're looking at the object and the moment they look at that other object they say the objects they say that they say the name of the object blue block right okay that's what we might see while we're observing that we're already forming a hypothesis about how children acquire language and the hypothesis is they acquire it using something called joint attention remember we talked about attention three weeks back the parent is attending to the bleep block not the child the child is not attending to the blue block the child is attending to the parent so there is not joint attention they're attending to different things but this interaction the child looking at the parents eyes is scaffolding me direction of the child the parents eyes is telling the child where to look and they both attend to the same object I can say to you look at the photograph in the slide I'm not looking at the photograph in the slide I'm telling you verbally and now you and I are both jointly attending to the photograph in the slide right because you understand language I don't need to staff hold you by giving you a very strong hint about where I want you to look with children you'll notice that parents and caregivers do that quite a bit okay that's all well and good but this is what's known as being Theory lated we're laden with the theory about how children acquire language we're coming to this with the bias about how we think cheap child children acquire language what we would like to do is step back from theory and hypotheses and let the data do the talking collect as much data as we can about the physical and social context surrounding a child and from that raw data start to infer how children learn language we don't want to go into this process with the hypothesis which is I hate children acquire language using joint attention we observe parents and children were already kind of bias we're kind of looking for that already ha we see joint attention that's we were right that's how children acquire language starting with the hypothesis is great that's where we learned about the scientific method in high school but it has pitfalls which is that we often miss other hypotheses about how children acquire language that may be very non-intuitive for us but unfortunately that might be how things actually work okay so in professor Roy's approach that we're going to look at today instead of observing a child for a short period of time in a controlled setting professor Roy and his wife instrumented their home with cameras and microphones and they recorded or they observed through these cameras and microphones the first three years of life of their own child in its own home this is a longitudinal study went on very long that went on for three years it's extremely controversial this is a scientist who studied their own child so this counts as a human subject research study we're studying humans typically when you conduct a human studies experiment you have to obtain consent from the participants this is a very young child so you can't obtain consent from a child so in human subjects studies when you're studying children if the child can't sign the waiver form the parent does in this case the parent and the investigator are the same person there's a very clear conflict of interest it's extremely controversial however we're going to talk about this study if you have a problem with this study you're more than welcome to leave the room at this point no harm no foul oh good okay let's keep that in mind all right so I will let professor Roy describe the experiment in his own words in the TED talk we'll watch the TED talk it's about I think it's about six or seven minutes long after that we'll come back and look at this study in a little more detail and investigate the HCI components of this work watch the whole thing imagine if you could record your life everything you said everything you did available in a perfect memory store at your fingertips so you could go back and find memorable moments and relive them or sift through traces of time and discover patterns in your own life that previously had gone undiscovered well that's exactly the journey that my family began five and a half years ago this is my wife and collaborator rupal and on this day at this moment we walked into the house with our first child our beautiful baby boy and we walked into a house with a very special home video recording system okay this moment and thousands of other moments special for us were captured in our home because in every room in the house if you looked up you'd see a camera and a microphone and if you look down you get this bird's-eye view of the room here's our living room the baby bedroom kitchen dining room and the rest of the house and all of these fed into a disk array that was designed for a continuous captioner so here we are flying through a day in our home as we move from sunlit morning through incandescent evening and finally lights out for the day over the course of three years we recorded eight to ten hours a day amassing roughly a quarter million hours of multitrack audio and video so you're looking at a piece of what is by far the largest home video collection ever made and what this data represents for our family at a personal level the impact has already been immense and we're still learning its value countless moments of unsolicited natural moments not posed moments are captured there and we're starting to learn how to discover them and find them but there's also a scientific reason that drove this project which was to use this kind of natural longitudinal data to understand the process of how a child learns language that child beings my son and so with many privacy provisions put in place to protect everybody who's recorded in the data we made elements of the data available to my trusted research team at MIT so we can start teasing apart patterns in this massive data set trying to understand the influence of social environments on language acquisition so we're looking here at one of the first things we started to do this is my wife and I cooking breakfast in the kitchen and as we move through space and through time a very everyday pattern of life in the kitchen in order to convert this opaque 90 thousand hours of video it's something we could start to see we use motion analysis to pull out as we move through space and through time what we call space-time worms and this has become a part of our toolkit for being able to look and see where the activities are in the data and with it trace the patterns of in particular where my son moved throughout the home so we could focus our transcription efforts all the speech environment around my son all the words that he heard for myself my wife our nanny and over time the words he began to produce so with that technology and that data and the ability to with machine assistance transcribed speech we've now transcribed well over seven million words of our home transcripts and with that let me take you now for a first tour into the data so you've all I'm sure seeing time-lapse videos where a flower will blossom as you accelerate time I'd like you to now experience the blossoming of a speech form my son soon after his first birthday would say Gaga to mean water and over the course of the next half year he slowly learned to approximate the proper adult form water so we're going to cruise through half a year in about 40 seconds no video here so you can focus on the sound the acoustics of a new kind of trajectory now go to the water [Music] [Music] you sure nailed it did me so he didn't just learn water over the course of the 24 months the first two years that we really focused on this is a map of every word he learned in chronological order and because we have full transcripts we've identified each of the 503 words that he learned to produce by his second birthday he was an early talker and so we started to analyze why why were certain words born before others this is one of the first results that came out of our study a little over a year ago that really surprised us the way to interpret this apparently simple graph is on the vertical is an indication of how complex caregiver utterances are based on the length of utterances and the vertical axis is time and all of the data we aligned based on the following idea every time my son would learn a word we would trace back and look at all of the language he heard that contain that word and we would plot the relative length of the utterances and what we found was this curious phenomena that caregiver speech would systematically dip to a minimum making language as simple as possible and then slowly ascend back up in complexity and the amazing thing was that the that bounce that dip lined up almost precisely with when each word was born word after word systematically so it appears that all three primary caregivers myself my wife and our nanny work systematically and I would think subconsciously restructuring our language to meet him at the moment of the birth of a word and bring him gently into more complex language and the implications of this there are many but one I just want to point out is that there must be amazing feedback loops it's not of course my son is learning from his linguistic environment but the environment is learning from him that environment people are in these type feedback loops and creating a kind of scaffolding that has not been noticed until now but that's looking at the speech context what about the visual context we're now looking at think of this as a dollhouse cutaway of the tower we've taken those circular fisheye lens cameras and we've done some optical correction and then we can bring it into a three-dimensional lights so welcome to my home this is a moment one moment captured across multiple cameras the reason we did this is to create the ultimate memory machine where you can go back and interactively fly around and then breathe video life into this system what I'm going to do is give you an accelerated view of 30 minutes again of just life in the living room that's me and my son on the floor and there's video analytics that are tracking our movements my son is leaving red ink I'm leaving green egg we're now on the couch looking out through the window at cards passing by and finally my son playing in a walking toy by himself now we freeze the action 30 minutes we turn time into the vertical axis and we open up for a view of these interaction phrases we've just left behind and we see these amazing structures these little knots of two colors of thread we call social hotspots the spiral thread would call a solo hotspot and we think that these affect the way language is learned what we'd like to do is start understanding the interaction between these patterns and the language that my son is exposed to to see if we can predict how the structure of when words are heard affects when they're learned so in other words the relationship between words and what they're about in the world so here's how we're approaching this in this video again my son is being traced out he's leaving red ink behind and there's our nanny by the door she hoards water and off go the two worms over to the kitchen to get water and what we've done is use the word water to tag that moment that bit of activity and now we take the power of data and take every time my son ever heard the word water and the context he saw it in and we use it to penetrate through the video find every activity trace that co-occurred with the instance of water and what this data leaves in its wake is a landscape we call these word scapes this is the word scape for the word water and you can see most of the action is in the kitchen that's where those big Peaks are over to the left and just for contrast we can do this with any word we can take the word bye as in goodbye and we're now assumed in over the entrance to the house and we look and we find as you'd expect a contrast in the landscape where the word by occurs much more in a structured way so we're using these structures to start predicting the order of language acquisition and that's your ongoing work now in my lab which we're peering into now at MIT this is at the Media Lab this has become my favorite way of video graphing just about any space three of the key people in this project Philip the camp Ronny cubot and Brandon Roy are pictured here Phillip has been a close collaborator and all the visualizations you're seeing and Michael Fleischman was another PhD student in my lab who worked with me on this home video analysis and he made the following observation that just the way that we're analyzing how language connects to events which provide common ground for language that same idea we can take out of your home Deb and we can apply it to the world of public media and so our effort took an unexpected turn think of mass media as providing common ground and you have the recipe for taking this idea to a whole new place we've started analyzing television content using the same principles analyzing event structure of a TV signal episodes have shows commercials all of the components that make up the event structure we're now with satellite dishes pulling in and analyzing a good part of all the TV being watched in the United States and you don't have to now go in instrument living rooms with microphones to get people's conversations you just tuned in to publicly available social media feeds so we're pulling in about 3 billion comments a month and then the magic happens you have the event structure the common ground that the words are about coming out of the television feeds you've got the conversations that are about that those topics and through semantic analysis and this is actually real data you're looking at from our data are processing each yellow line is showing a link being made between a comment in the wild and a piece of event structure coming out of the television signal and the same idea now can be built up and we get his word scape except now words are not assembled in my living room instead the context the common ground the activities aren't the content on television that's driving the conversations and so what we're seeing here these skyscrapers now are commentary that are linked to content on television same concept when looking at communication dynamics in a different very DIF here okay I think we will leave it there who would be interested in knowing who is commenting on social media about which television shows would be most interested in that you bet so a great way to monetize this this approach we're talking to focus on that part let's go back and talk about language acquisition let's talk for a moment about the audio clip that you heard about his son trying to pronounce the word water in that clip itself there are a lot of clues that might suggest hypotheses you might form about how this particular human built an understanding of what the word water means right so we're going from raw data which at the moment is just that audio clip to hypothesis formulation ideas absolutely so it wasn't it was non-monotonic right so it wasn't gradually approaching the proper pronunciation of the word water there were improvements and reversals and so on right so language acquisition is already a simplification right a child might start to grasp a word and then lose it again and make more progress what might lead a child to master the word water and there was a hint in the audio transcript itself okay and that's the graph we're going to talk about that in a moment you mentioned scaffolding that might have been so we might go back to when and where those parts of the audio transcript were made and was one we're one of the three caregivers pronouncing their t's nearby and time-space right you go back and watch the video and listen to that clip there are a few clips in which you can hear water in the background right which in retrospect is maybe not so surprising as professor Roy pointed out often words are co-located in space and time with the vents where you would tend to hear that word front door you tend to hear the word vibrate more often than in other places in the house and places where you tend to hear the word water as you saw in that landscape with the mountains tends to be in the kitchen and the bathroom right so we might already start to form a hypothesis which seems relatively straightforward that words are acquired near the events relevant to that that word okay so that's just an example of this process we're trying to go from data to hypotheses rather than the other way around rather than dreaming up a hypothesis and picking and choosing the data that we collect that's obviously biased we want to try and start in an agnostic manner as possible let's start with this massive raw data set listen watch the raw data and then start to form hypothesis okay we got nine minutes left so let's just dive down into a few aspects of this experiment let's start with the HCI part right so we're trying to construct a novel HCI system here there's going to be interactions between four people the two parents The Nanny of the child and a bunch of computers in a novel way they instrumented eleven omnidirectional camera so eleven of the rooms they had more or less full video coverage of those rooms fourteen microphones as well the raw data self said itself they collected data from nine to 24 months of age which tends to be the most sensitive period for language acquisition this is when children acquire language at the fastest rate slows down beyond that point they used over 4,000 hours of recording time for four hundred forty four of those days from a total of nine thousand one hundred forty thousand of audio two hundred terabytes this was unprecedented at the time is still a pretty big data set as Professor Roy said this is the biggest home movie ever made and maybe will ever be made we'll see about ten hours of recording per day during the day so we're not capturing data at night right already there is bias creeping in you can try and minimize bias we can't get away from it altogether they think they got between 70 and 80 percent of the child's waking hours luckily for most very young children most of their waking time is spent in the home typically five or six of the cameras were active at any given time so that's the hardware and that's the amount of data that is being pulled off this hardware during that period just to really emphasize this point most of the work done in psychology to understand humans is theory lady psychologist comes up with the theory about why humans do what they do they then create some controlled environment in a lab to try and test that yes or no comes with a lot of biases okay so we're gonna try and instead start with raw data and go to hypotheses okay so let's look at a few pieces of his pipelines very sophisticated we've got the hardware the raw data taking the raw data and then we're trying to turn that raw data into who what where what and how right who was saying what and how are they saying that you remember you remember two lectures back we were looking at not just a content of language but things like frequency how quickly one thing is said volume pitch all sorts of things how things were said in order to do that we need to do word level speech transcription we need to go through raw audio and try and pull out words this work was all done just before the deep learning revolution so probably machine learning algorithms could do this automatically back then it was done manually prosody this is again just a fancy word for how things are said from the video we can try and identify more or less automatically who was where what were they doing and how and with what so we're trying to get information about people activities and objects and the interactions between them okay so here's our raw data we are trying to use machine learning or machine perception as much as possible but we probably need to augment this with human analysis it's not possible to capture pull words and who what where when and how directly from data back then at the end we want to try and get information to answer this question when was a word spoken by the child for the first time can we find the exact moment in time in which the child said water at the heart tea for the first time and how often was the word spoken by caregivers beforehand so we're already starting to form a hypothesis here okay they created a whole bunch of different applications HCI applications to help with this so professor Roy had an army of grad students that were trying to process that raw data they created this Total Recall which was a way to stand backwards and forwards through 90,000 hours of video right it's not an easy thing to do how do you zoom in on where what when and how in three basically three years worth of activity being pulled from eleven cameras in parallel huge huge data set audio is visualized with spectrogram spectrograms which if you learn to recognize these things tell you whether you're looking at speech or background noise like water like a shower running for example video is processed to highlight movement and remove static areas who saw that in the video they basically erased everything that wasn't moving and only focused on things that were moving which of the 25 audio and visual channels should be viewed at any given time and in this application it was interactive you could change the time scale from years down to seconds right there's this is very much a needle in the haystack problem in ninety thousand hours of video I want you to find the place and time during which the child rent mentioned the word mother for the first time good how do you go about finding that assuming that you don't have a machine learning algorithm that can do it for you not an easy thing to do okay here's an example of speech tagging we just looked at activity tagging so you're going through raw audio and trying to tag words so they created this blitz tribe application another HCI application which was trying to transcribe a speech from raw data as quickly as possible so at that time automatic speech recognition error rates were about 75 percent not accuracy but error right terrible terrible manual speech recognition was required so I'd like you to listen to 90,000 hours of a child babbling and find the moment in time in which they said mother for the first time an agonizing task what machines can do is pick out again erase everything that isn't spoken speech which might leave the television playing the radio playing still a pretty big ace back in which to find a needle do a little bit of transcription as best you can listen to the segment and type what was her so here's the computer's guess what we said about seven second period of time you can click and actually listen to that those seven seconds and then you would rapidly tick off you know he's a particular word present in that piece of audio or not right so relatively quick but still very painless so in bridge splits ride one-hour recording takes more than an hour to transcribe right very very labor-intensive process okay that's all the hardware the HCI I think we will leave the actual science till Thursday you have a quiz due tonight while you're working on deliverable nine and we will talk about deliverable ten on Thursday have a good day "
Jyhnlpzk6sM,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-09-02T16:56:33Z,"L03: The basics of interaction (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/Jyhnlpzk6sM/hqdefault.jpg,Josh Bongard,PT50M8S,false,220,1,0,0,0,okay let's get started we are going to spend a couple minutes just talking about deliverable one and deliverable two and then we'll jump back to to lecture let's see how are things going with deliverable 1 most people over installation headaches more or less yes no I spent like memorize that you may have noticed that I have a Mac and as you can probably imagine I wrote the deliverables on my Mac so yes people that have Macs are probably gonna have an asura but generally speaking after 12 and 13 to be anticipate more problems after steps 12 another juice I'm gonna I think famous last words I write if you manage to get the leap Python SDK installed and matplotlib running you're probably 99% of the way there I can't promise a hundred percent but though that's the bulk of the challenge after so what was the particular issue with Windows I would only draw your last point that's right so someone sent me an email about that and I put it up on blackboard so there's a blackboard announcement for those of you that are on Windows so hopefully if it's worked for to a figure that I put that code in okay perfect so excellent who put that code in scotch thank you Scott okay excellent it was happening on max - okay so if you're still having that problem try and put a pause statement in there that gives matplotlib a chance to catch up so matplotlib was originally designed for creating static visualizations so it's kind of a hack to make it run interactively in real time I apologize for that okay Scott you've identified yourself now so all PC problems go to Scott I'm just just gonna okay okay okay anyways send me your woes and at least if I can't answer them I'll put them up on blackboard or to the TA we'll get we'll get there all right okay so again deliverable one is going to be due next Monday hopefully as you all know Monday is a holiday so no class on Monday deliverable one is still due 11:59 p.m. on Monday typically I introduced the new deliverable on Monday but since we're not not going to be here on Monday I'm going to talk a little bit about deliverable two it's a sign I haven't made the link there but I forgot to add the link I will add it when I get back to class deliverables to is there if you want to get started on it now that's that's fine but as usual deliverable 2 will be due the following Monday at 11:59 p.m. okay so let's talk about deliverable 2 so in deliverable 2 let me find the screenshot here now let me find the video in deliverable 2 you are going to be translating your don't ask me this again you will be translating your 2d visualization into a 3d visualization so we're still going to be using the interactive real-time matplotlib visualization but instead of drawing a 2d plot it's going to be drawing hopefully in real time a 3d plot and you'll be able to draw your hand in 3d in real time there is I believe a couple of videos you're still making a playlist I think there's two videos in this one the same same deal as last time create a new playlist add the two or three videos and submit a URL that points to the playlist as your submission for deliverable 2 okay let's just have a look at step 11 here so the big change from deliverable 1 to deliverable 2 is we're going from 2d to 3d and also instead of grabbing just one set of 3d coordinates which is the tip of the index finger now you're going to grab the coordinates of all the bones in the hand so you can sort of see how this is going to go like a deliverable 111a you grab a frame and you assuming there's a hand over the device at that point of time you grab all of that data it's stored in the hand data structure then you iterate 0 through 4 so in Python the last element in the index there is not counted so 0 1 2 3 4 you're iterating over all the 5 fingers in the hand so you're gonna have to pull out on line see there all of the data corresponding to the if' finger then there's an inner loop J which is going to iterate over the four bones in our sorry the three bones in each of the finger so you can see on line II here we're grabbing information about the J's bone in the eyuth finger and then remember that every bone is a cylinder situated in three-dimensional space so it has a 3d coordinate describing the base of the bone and another 3d coordinate describing the tip of the bone so you're going to be grabbing those two coordinates those those two sets of 3d coordinates the position of the base of the bone the position of the tip of the bone and now you're going to be drawing a line that connects those three two points in 3-dimensional space right which will give you the wireframe hand okay in step eleven here I put a handy a link to some documentation here this is very helpful so I kind of misspoke last time when I said there are three bones in your hand there are technically four but we're only going to focus on the three outermost ones the distal intermediate and proximal phalanges okay okay so this is a good reference as you start to try and figure out which bone in which finger you're you're working with ok any questions about deliverable one or two no ok so let's let's jump back to lecture we're working our way through our first theme here introduction and overview we're going to jump back in to lecture two in a moment which we should finish today and we'll probably get partway through lecture three today okay so we ended last time by thinking about people in computers which are obviously going to interact and thinking about the similarities of people and machines both receive input along different input devices they do a little bit of information processing and then produce an output here's a list of some of the things we talked about last time just as a reminder as we go through remember to annotate these red boxes as we go we mentioned a lot of these elements last time and as I ended last time this is pretty much where the similarity between people and computers end what are the differences between these two sets how do people and machines differ in terms of how they receive information and how they affect their environments one of the obvious thing that seems to me is that humans really do have a finite set of inputs as computers absolutely right so our our set is pretty much fixed we used to have more input devices than computers that's probably not not true anymore what else what's difference but what's different between human and compute computational input devices your senses and the way in which your computer receives information absolutely right so moon all of our input is analog more or less of some signal most of computers input is digital a lot of the input devices are becoming more continuous and real-time but usually that's downsample to a digital signal in in some way right an important difference what else what about the output side of things we only have one output right mother nature has been very clever in adapting that output device our muscle to accomplish lots of different things the ways in which computers are able to affect their environment is grow it right for quite a while it was just a printer then it was printers and monitors then it was printers monitors and speakers then it was vibration and now that we get into embedded devices and robots the way in which computers can directly impact the physical world is growing right for better for worse okay okay so humans at least for now maybe have more input channels most of our input is continuous computers are digital another major difference that we're going to see as we go through this course is humans are very good at manipulating our output device or moving in certain ways to expand or focus our input devices right we are not purely passive we can act to change the ways in which we receive information what are some examples so I can see most of your faces but some of the people that are sitting in the back of the room I can't see your face too well if I want to what do I do in order to rectify that deficiency in my visual stream I move right it seems trivial right it almost it seems not we're talking about computers cannot do that right if the if the webcam is on here the computer the best the computer can do is maybe send me a signal saying can you please move me so I can see something else right computers are not very good at using their output devices to change what they can sense in the real world we do this all the time you're not always aware of it what else how else do you act to broaden the information that you're receiving from the outside world so they're very mild air currents in this room which you may be able to detect not very well though what might you do to try and figure out how air is moving in the room lick your finger and hold it up right maybe that doesn't work here so well there's sorts of those kinds of things that we do all the time and again you might not be be aware of it right so exploiting your interaction with the physical world to extract more information from it in a fundamental sense most of our traditional computational devices laptops and desktops and even these things are pretty passive right there at the mercy of their human operator to get information from the outside world and again with embedded devices and robotics that's changing we're starting to see new technologies that are able to move or act in the world to get the information that they need rather than having to passively wait for someone or something to give it to them okay so I started with sort of this tongue-in-cheek cartoon of people as sort of this passive thing that sits there and gets information from the world cogitate s-- and outputs the reason I started with that is that has has been until relatively recently a fundamental assumption in human psychology and we're gonna go back now a little bit into the past to BF Skinner where this idea came from before Skinner in psychology or the study of human behavior it was basically going out in the field and observing animals or humans in their natural environment and telling anecdotal stories about what they did right which was fine but it wasn't very satisfying if you were an empirical scientist you wanted to try and quantify and understand animal and human behavior let's say you have a particular animal species in mind and you'd like to know is that animal capable of learning or not you might send some field scientists out into the field and they would observe the animal and say well it seemed to be doing this and it changed its behavior on this day compared to this day it wasn't very satisfying right how could you try and prove or show quantitatively this species is capable of learning this one isn't this one is capable of this and so on Skinner said let's be done with it forget about the wild let's try and reduce and control the kind of input that an animal receives and also reduce and control the kind of output the animal can provide so Skinner invented what came to be known as the Skinner box you would put the animal in a box provide some sort of controlled stimulus so you could turn on and off a light emit a sound supply some food supply some air supply some water and so on in a controlled way and maybe you allow the animal to turn one or more of two levers the moment you place an animal or a human in a skinner box it changes the way you think about that animal right that animal is receiving some input and you're observing how the animal reacts to that output okay so here would be a typical experiment with operant conditioning we put a pigeon in this case in the box and from time to time we shine a light and if when we shine the light the pigeon pecks lever - and only lever - we give the animal some food that's an experiment we could conduct with the Skinner box how would that experiment allow us to tell whether the pigeon is capable of learning what would we expect from what would be your prediction of the outcome of this experiment absolutely right so if this animal is capable of learning it should learn it should be it should Peck leave or number two more often when the light is Shawn as time goes forward into the future what happens if it doesn't can we conclude that the animal is incapable of learning it doesn't respond to that stimulus right so it doesn't do what we want in this particular situation okay so again this isn't a psychology class but just to sort of lay some context here we have this sort of bias to think of animals and people and ourselves as sort of this thing that is waiting for input and when something happens the animal or the human reacts right so social psychology and anthropology sort of has followed along in the footsteps of this Canarian view of behavior and sort of observed humans in the same light the Skinner box was introduced in about 1930 and around the same time someone else on the other side of the pond Alan Turing introduced this thought experiment of a hypothetical machine which in 1936 only existed as a hypothetical thought experiment the Turing machine anybody want to hazard a two-sentence two or three sentence summary of how the Turing machine works exactly right so here's our Turing machine they either the tape moves or the machine moves so it's capable of output the machine let's assume it's the machine that's moving the machine can choose to stay still go left or go right so very very simple output it's also capable of erasing and writing a symbol to the to the segment of tape in front of it it has inputs it can read the symbol and it can decide what to do what action to take given the current input what's the current symbol on the tape compared to perhaps internal state and then act right so before computers even existed in the thought experiment that came to be known as Turing machines right there was already an assumption that these kinds of machines would take some input they would think quote-unquote and then they would do something it's interesting that the Turing machine was introduced in 1936 six years after Skinner introduced the Skinner box whether Turing was influenced in his thinking about how these automatic machines would work whether he was influenced by thus canarian view or not it's not clear it's kind of interesting to to think about okay so it's not surprising that we tend to think of people and animals as inputs processing and then output and we now have physical devices which any computer existing today is just a physical manifestation of this right it's a fancier version of a Turing machine so people and computers take input they process they produce output okay instead of going forward in time let's actually go back in time to 1896 let's talk about John Dewey the great there's a picture of John Dewey's grave does anybody know where this tombstone is absolutely so if you have time after class or if you have some time between classes today on the far side of IRL and Chapel towards Colchester Ave from us you can go visit John Dewey's grave he was a UVM graduate of eighteen something or other I can't remember went on to do great things in philosophy and one of the cornerstones of his thinking about philosophy and also about the philosophy of Education was that students should not sit passively and receive information unfortunately that's still what happens most of the time Dewey argued that we are not passive entities so we do not begin thinking with a sensory stimulus we're not sitting there waiting for sensory inputs but rather we have a continuous feedback loop with the environment right so do II tried to demolish this idea of linearity of thinking which existed long before Skinner and Turing so for Dewey we were not these passive machines that received inputs then thought and then acted that arc going from perception to thinking to acting as known as the reflex arc he said forget about the arc the arc doesn't instead it's a feedback loop so for Dewey he called this sensory motor coordination right sensors and motors or your senses and muscles you're in this continuous real-time coordination with the environment if it's a feedback loop how can you talk about sensory first then acting there is no sequence it's this continuous process okay moreover for Dewey he said it's not really the sensing which is primary it's the movement right as you exist as a physical entity in the world you're breathing you're thinking you're acting all the time and when you act there is a sensory repercussion to your action you you finish your ASL educational software you plug in the device you put it in front of someone they will immediately start interacting with it and they will probably disregard whatever is on the screen if someone has not seen the leap motion device before they might grab it and try and move it around like a mouse they will start acting or interacting with your software and hardware and they will observe the sensory repercussion of that action when you try this out on your ear roommates and so on watch what they do when they interact with your system for the first time typically they will act and observe the result of the action right so Dewey said if you want to really think about interaction or in this case human behavior flip things around think and it's much harder to do it's less intuitive think about action and sensory repercussion which is an active process rather than sitting and waiting for sensation then thinking then than that okay so we're going to try as best we can throughout the rest of this course is to take a do Ian approach to behavior rather than ask anarion approach to behavior we're gonna try and think about people interacting with our software before we've ever written it and we're thinking about how they're going to click the mouse tap on the keyboard wave their hand grab the device and how our system should respond to user to help the user understand what is the feedback loop you're trying to establish between your user and your your system okay so this is the case right we have this continuous real-time analog interaction with our physical environment we start out in the first few years of life doing a lot of that grabbing objects putting them in our mouths trying to break things hammering on our younger brother and sister learning about what happens when you act on the world how does the world respond after a few years of life you realize that you're not the only human out there there are other things out there like you that are also doing the same thing so we get these expectations about our interactions with the physical world I expect that usually if there's something far from me that I can't see very well generally speaking if I move I'll be able to see what's back there doesn't work if you're looking at a computer screen right but most of the time in the real world that's a common expectation we then as we become more social creatures as we grow up we also have expectations about social interactions right so if I ask a question and then pause I expect after a few seconds of you thinking about it you might offer me an answer to my my question right so we all bring to any new software system years of experience about what happens when we physically interact with the world and what happens when we socially interact with the world and some of those expectations are implicit right if I see something that looks like a mouse plugged into the computer and I grab it move it I expect something on the screen to move in real more or less real time I don't know if you've ever experienced it but there's sometimes if you grab a mouse and you move it and there's a little bit of a lag so the movement is like a tenth of a second off from the movement of a mouse it is the most frustrating thing possible another way to frustrate your expectations about interactions that come from the real world take your mouse if you have an external mouse turn it 90 degrees and now grab it and try and move the cursor to something on the screen it's very very difficult why because if I grab something in the real world and I move it this way I expect to see the thing move in real time along with the action right that's a common expectation we have we've built up from the physical world that we expect to see when we manipulate a mouse and a pointer right if that expectation is broken your users get frustrated pretty quickly so a lot of this course is going to be trying to identify those expectations that we have about the physical world in the social world and making sure that we're supporting them in our new software system rather than frustrating them ok alright so now trying to think like Dewey here and think about a constant interaction what are we trying to do in HCI well we're trying to create this coordination among this pair right this human-computer pair to achieve something that would be difficult or impossible through either alone so let's take a kind of classical human-computer interaction you sit down at your browser and you as the user have a particular goal which is to try and create a new energy policy for Vermont so you start doing some browsing and googling and so on you started out thinking that biofuels was going to be the way to go but after a little bit of interaction with your computer and browsing and research your ideas start to change and perhaps you focus more on solar despite the fact that Vermont is not ideal for solar and eventually the output of this continuous interaction between you and in this case a browser you get a new Vermont energy policy okay kind of straightforward think about this human-computer interaction now so now it's a person with a cell phone the person starts by acting so following Dewey right let's think about the person so you're always doing something as you're acting your device is sensing your action so your output becomes the input for the computer and vice versa right that's closing the feedback loop so as I move down the street my gps-enabled phone detects that there's a Starbucks coming up and it also detects that I sent a tweet a couple minutes ago that had the word caffeine in it and my app infers that perhaps I'd like a cup of coffee so it buzzes my leg I accessed the phone it says there's a Starbucks nearby without me entering anything to the app I change direction and my GPS enabled phone sees that I'm now moving towards to Starbucks and in furs that I'm going to get a cup of coffee it infers that I'm heading toward to go get another cup of coffee but now in furs that there's only a bar left on my battery I was heading home but now I'm heading to Starbucks the phone realizes that the phone's gonna go dead before I get to Starbucks and then get home my phone again infers that I would prefer to have a functioning phone than a cup of coffee sends me a second message letting me know that I'm gonna have to make a choice here right so again kind of a cartoon example here but we have this continuous feedback loop where it's not as structured as this one right it's not the human coming in with something they want to do and going through this process and there being a clear output where does the goal and the change of goals and sensation and action start and end right it's much more continuous and fluid so with the introduction of smart phones and more and more real-time interactive technology after more than a hundred years right HCI designers have to think more along the Dewey in line than the Skinner Skinnerian line right we are not inside a Skinner box you are continuously acting in the world and you're either you're the cloud of interactive technology that surrounds you has to try and infer what you're doing and what you want to do and enter into that continuous feedback loop okay so in that little cartoon example you might you might believe that the the app is doing the right thing or maybe the the wrong thing so if you do go out and design software for a diverse user base you're going to have users that have very different views of computers obviously if you want to put people in computers together we would like their respective skills to complement one another if you ask people that are machine centered who like computers to describe people in computers you'll typically get descriptions like people are vague disorganized distractible emotional illogical and so on in machines thank goodness are precise orderly undestructible and so on if you ask people that are not big fans of computers you'll get a very different description right the opposite one now it depends on context of course these are all true so we would like again to try and marry computers and people together so that computers are doing what they do best and people are focusing on what they do best okay this is an excerpt from a book by Donald Norman things that make us smart dr. Norman is one of the founding fathers of HCI is written a series of great books about HCI if you're interested go go have a look okay so throughout the rest of this course I'm going to try my best to not talk about computers but talk about interactive systems right we're gonna try and take an inclusive view of technology that is increasingly continuously interacting with us so an interactive system could be a computer it could be an embedded device it's a sensor that's sensing whether there are people in the room and keeps the lights on when there are people in the room and it turns off the light when there aren't people in the room and a moving embedded device otherwise known as a robot something that is able to move itself in the world and the moment that the robot moves its sensory information changes okay so again you're going to see this cartoon over and over again in this course we've got a human that is interacting and a feedback loop with technology we're going to assume that interactive systems are systems that enter into this continuous feedback loop and are also processing information somewhere right so there's some electronics in there you can argue that your knapsack is also an interactive system the moment that a person puts books in the bag and puts it on the weight of the bag changes which the person senses and does something and so on right so the physical objects in your world also enter into a real-time sensor motor coordination loop with you we're not going to focus on those we're going to focus on those that are able to sense this interaction and influence this interaction in some way okay that's an interactive system I think next week in lectures four and five we're going to start to get into the design aspect of HCI so we want to try and think about how people might interact with our hypothetical system how does that influence the way that we go about designing that system how do we design a system where we have to think very carefully about who's going to use it right we talked on Monday about leap motion for this particular demographic most of you are righties and most of you are male and most of you are fair-skinned right we've already started to think about people and that should influence the way you think about designing your software for the leap motion device right how do we make that design process a systematic process okay it's a lot of fun because it draws on a lot of different domains so if we're going to put people first we want to draw on people centric domains of knowledge so we want to draw on sociology how do people tend to interact with each other socially that's pretty important when you're developing social media psychology how do people in isolation think if I reach out do this I expect this sensory repercussion right how do we how do we get a handle on that we'll think about economics right how people tend to like to interact with physical objects or electronic objects anthropology will also become important right somebody mentioned last time that not every culture reads from left - right that's very important for a lot of the software you might develop even if there isn't text in your your system right you throw up some visualization most English speakers will scan it in Z top left top right bottom left bottom right non-english speakers depending on your native language will not scan your visualization in the same way okay so we're going to draw a lot from human centric domains of knowledge when we start to think about how to design these systems we're going to focus in this class not on the back end of the software but more on the front ends so for your the project for this class the first 10 deliverables you're going to be focusing quite a bit on the back end so we can get everything working and then in the last four weeks of class when you develop your educational software you're going to be focusing on the interface and that's what you're going to be assessed on so technologies that are brought to bear in HCI obviously this isn't purely software if you're developing something like leap motion or Google glass or 3d printed prosthetics you need to think about the hardware as well so there's a lot of engineering branches involved multimedia databases sensors and actuators right how does your interactive system get information from the world and project or influence the world in turn programming and networking and so on I listed software engineering on the list here there's a sister course to this course which is software engineering who is taking software engineering or has taken it just a couple people okay if you like this course you might want to take software engineering which focuses on how to develop the backend of systems ok we're gonna look at design in lots of different fields as we go because obviously if we're focusing on the interface we can't focus on just the objective aspects of the system we also have to focus on the subjective aspects right is it a piece of software that the user actually enjoys interacting with so there's an aspect of aesthetics ergonomics here so we're going to look at interaction design engineering design graphic design product design we already talked about visualization last time finally if we're designing software for people we need to think about what are those people doing what's the activity they want to get done and what is the physical and social context in which they're going to do it so we're going to think about business soft systems organizational psychology communities of practice tribal knowledge tribal knowledge is a good example I just added that one in red it might not be on your slides who knows what tribal knowledge is so communities of practice are groups of people that either work together or in your case study together this particular class is a tribe there's some tribal knowledge among this particular tribe tribal knowledge is usually the ways in which a group does what it does and those rules are not written down what would be an example of an unwritten rule that most of you are following right now in this class writing that down okay what else not yelling at each other okay right so general rules of civility aside from general social rules about being polite what are some specific rules that you're following when you enter leave this class or you you're in this class that are specific to being in a classroom yes okay which again the chairs are arranged for you usually facing forward again that's not written down so that's a general assumption you could if you wanted move the chairs around usually you don't what else it's tricky right because even being asked it's not quite clear although you follow it all the time you follow these rules all the time okay hopefully you focus on me most of the time at least we generally see there it is right every I've taught this class I don't know how many times every class I've taught generally speaking most of the time most of you are sitting in the same seat no one told you to do that it's a general assumes thing right it's pretty specific to classrooms in the Western world why who knows right it just it just is if I was gonna write let's say I was gonna write some software for a camera that was going to try and take attendance for me rather than doing it manually it might matter when I'm developing that software to build to design that software with the assumption that most of the time you're sitting in the same seat but not all the time it may or may not matter but it might be important right if you go and design a combined hardware and software system for a company or a non-profit or some some group some community of practice you don't know those rules right you might if you go and sit in on meetings or whatever it is that they do and observe very carefully what's happening you might be able to catch some of these unwritten rules and it's also hard to explicitly elicit them from the group right even asking you explicitly it takes a while to get the answer you tend to sit in the same place although you all do it probably in most of your classes very tricky right this is very far from sort of traditional computer science where you're trying to work out all the bugs in your code right this is a much more social procedure okay so what are people doing what context are they doing it in are they able to tell you what they want to do how they generally like to do it and and so on and how do those things impinge on the way you design software for them okay I mentioned this already but in HCI we sort of have a foot in two camps psychology we're gonna draw most of our theoretical base our concepts we're going to start and software engineering for our design approach how do we systematically develop software to support our identified group of users and the user interface is where these two things meet and that's where we're gonna focus so most of the things we're going to talk about in this class and most of the code you're gonna write for this class you need to be thinking about these two things right how might somebody interact with this what might they expect my software to do when I do this or I do this or I do this okay we're gonna as we go continually unpack what we mean by interaction we're gonna start by breaking it into three different kinds of interaction when someone is interacting with a software system they are interacting at different levels at different time scales the most obvious one is physical interaction you take your the motion device put it in front of your new user and you can obviously see what they do physically do they grab it do they wave their hand do they sit passively waiting for something to happen on the screen what sorts of physical interactions do your users enter into also somewhat obvious is perceptual interaction right thinking about what the person sees or hears or maybe feels and how they react to it or again turning that around what do they do and what sensory repercussion are they going to get or what kind of sensory repercussion should they get third kind of interaction we're going to talk about is conceptual interaction and basically you can think of this as expectation when somebody sits down in front of a web browser or the lead motion device or they put on Google glasses even if they haven't used that technology very much they're bringing lots of expectations to bear on the system so they're thinking about is the system on is it even listening to me does the Syst is the system sensing me right we build up all these these expectations they're building up a prediction about what the machine is doing right it's either preparing to show me something it's number crunching it's off it's broken it's working intermittently we have all the we're making all these predictions all the time about what the system is is doing the field of neural science is increasingly coming around to the view that what the brain is is a prediction machine again going back to John Dewey what the brain is doing most of the time is making predictions about what will happen if I do this you put a person down in front of your interactive system they will immediately explicitly or implicitly start making predictions about what will happen if they interact with the system and if what they get back differs from their prediction they're either going to go use somebody else's software they're either gonna find it interesting and want to learn more who knows right okay so obviously your interactive system is going to have to provide some sort of feedback to help people build up an understanding of what is the feedback loop here what is the what is the software receiving from the human and what is it providing back what are some examples of software you know of where visualization is done well or done poorly in the sense of visualization that's communicating to you what the guts that the system is doing what are some common visualizations that are out there I'm absolutely right so some kind of loop there's another great visual metaphor right most of us understand that anything doing this meanings hold on a second I'll give you some more feedback in a in a moment right it used to be an explicit clock now it doesn't need to be anymore right that looping signals are advertises that interaction what else it's a common one right so mousing over something is you signalling to the system I want to focus on this thing somehow and most software system these days will say I hear you I know that you're now interested in this part of the the screen or this part of the virtual world or the game or whatever it will highlight depress it'll open a text box something right it's signalling back to you that it knows that you're attending or paying attention to that that part of the screen sometimes that's great sometimes that's not you don't want to have an interactive system where you scroll a pointer across the screen and all everything that it touches immediately opens up a text box right maybe you want to be a little bit more intelligent that your system will wait for half a second if the mouse pointer stays there for half a second that that's signal from the human that they're interested in that if the pointer continues to pass over with constant velocity don't open a text box right all those kinds of subtle things are really really important that make the difference between whether someone's going to be frustrated by that aspect of your software or engaged by it okay here's a somewhat dated example but a nice one to illustrate this idea of supporting all of these three different kinds of interaction at the same time let's say we were writing a game for the Nintendo Wii Remote here so you have a remote in your hand and you can use it and you can use it to point towards a bar that sits on top of your your TV there's an accelerometer in the Wii Remote and it can also sense there's a camera in the Wii Remote that can sense these two groups of lights that are on the bar by the TV if the Wii Remote combines that light information and that excel acceleration information it can understand the relative position of the Wii Remote from the we are and it can also understand the orientation of the Wii Remote relative to gravity once you have that kind of feedback loop between the user the remote and the bar you can create some fun things so physical interaction is pretty clear that we remote with the bar is interacting and picking up this information as I move the Wii Remote because of these sensors here detects change in position and orientation let's imagine we write a bow and arrow or an archery game for the Wii Remote where I hold the bar I hold the remote and I aim at the screen and then press a button which corresponds to we can support the perceptual interaction by making a sound on the speaker on the remote which is the sound of the release of the arrow and then a short time interval later we can omit the sound of the arrow hitting the target from the computer monitor or the TV if the Wii Remote senses that the orientation is pointed away from the screen and I release the arrow what is the human user expecting to hear if I haven't aimed my arrow at the screen and I release the arrow from the Wii Remote how should this software system support my expectations about what about my sensory about the sensory repercussion of my action okay so I should still hear the release of the arrow right I released the arrow but maybe I hear a thigh right here nothing right I missed the target okay so maybe I released a few arrows and I get the idea of this interaction I move further back and point at the screen and release the arrow again again because of the thing in here the prediction machine what does your user expect to hear now should take longer right it seems kind of obvious but it might take a moment to think about you need to actually calibrate the time interval between the two sounds emitted by the Wii Remote and the monitor based on the relative position of the Wii Remote from the monitor right all of these design decisions that are now going to go into the game that we that we write are taking into account the physical interaction I'm moving this thing I can change its orientation I could change its position it's taking into account the perceptual interaction I shoot an arrow I hear it leave and then I hear it hit a short time later and conceptual interaction I expect once I've done this a few times that if I shoot off target or I shoot from a further distance I have a prediction about what I expect to hear if I move the Wii Remote further away and I heard the arrow hit more quickly will be pretty frustrating I would break my expectation of this system okay I think we will stop there you have a quiz due tonight deliverable 1 is due on Monday and I'll put up the link for deliverable 2 when I get back to my office enjoy your long weekends 
Qc6CuMwpCsI,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-11-04T16:31:37Z,"L25: Social network inference. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/Qc6CuMwpCsI/hqdefault.jpg,Josh Bongard,PT49M21S,false,53,1,0,0,0,okay so let's carry on we are working our way through the second last theme of the course looking outward and one way of thinking about this series of lectures is technology that's looking outward right so not laptops or desktops which are passively waiting for input from human users but these are devices that are embedded in the world they have sensors there directly sensing a growing and broadening set of physical phenomenon some of those embedded devices that are sensing the world directly we carry around with us and that changes what they sense but one of the general trends that we've seen over the last decade or so is that the things that we sense that we see and hear and feel as we move about the physical world is coming to overlap more and more with what the devices we produce can see and hear and feel in lectures 15 through 19 we're focusing on embedded devices so these are devices that can sense but not move themselves they're embedded they don't move in lectures 20 through 24 we're going to talk about a different class of technology which can both sense and move themselves which are robots right so our world is being instrumented at the moment mostly with embedded devices robots are still somewhat around the corner but perhaps not that that far away well we're looking at specifically in lecture 16 17 I'm sorry in 17 18 and 19 he are three different case studies where the investigator said okay now that there is some tech that is embedded out there in the world what are some things that we can do that were difficult or impossible to do in a world that did was not instrumented with embedded technology we are going to finish lecture 17 today which was tackling a particular question which is social network infrareds so assuming their microphones out there in the world can those microphones collectively determine who among a group of individuals rules are friends or at least which of those individuals enter into conversations with each other more often than with other individuals in the group if you are able to infer that this person tends to talk face to face with this person more often than with this other person you can then ask an interesting question about human social behavior which was next to impossible to study before which is how do you change your behavior or at least in this specific case how do you change the way that you speak depending on who you're speaking to within that social group right that's the question that we were we started to tackle last time we'll move on today to lecture 18 which is activity tagging how many of you are wearing a Fitbit today or an apple phone Apple phones do more or less the same thing right just one person okay i'm not a Fitbit or an apple phone or a quantified self type person myself but we will look at what can we do once we have once there a large number a large number of individuals in the population that are continuously measuring their own physiological data and then finally we'll move on next week to the third of these studies which have to do with children in this case and how children come to acquire language and that's the human speech on project any questions about deliverable 10 no okay okay so back to lecture 17 and again just as a reminder we're looking at a ubiquitous computing or Internet of Things if you like and we're looking at this particular question about how people might change their behavior depending on who they're talking to two in a social group the reading for last class was the actual research paper that we're drawing on from today just to refresh your memory this was a study in a research group they took 24 grad students and convinced them to wear the sensor packs for part of the day throughout a six-month period they were measuring how they said things but not what they said they threw away the content of speech but kept the speed at which they talked the volume the rates are sorry the pitch high pitch low pitch and a couple other aspects of speech which we'll look at in a moment we finished last time by looking at how they went from the raw data from the microphones from these 24 microphones to starting to infer the social network and then we'll finish today by saying now that we've inferred the social network can we actually see whether and how people change their behavior depending on where they are in this social group okay so just as a reminder we've got this microphone data and we're focusing at the moment just on volume right the amount of speaking that is picked up we ended with this sort of cartoon example last time so we've got this microphone data over a six-month period and they did a huge amount of machine learning where they were sweeping along this six-month period of data and looking at sort of minute or five minutes snapshots moving this time window along and during these short time windows they asked the question of were there any individuals in this group of 24 people that were in conversation with one another and if so whom in this example we had three grad students engaged in a face-to-face conversation in a classroom two of them outside engaged in another conversation here's a cartoon of the kind of volume that might have been captured by the volume of speech that might have been captured by the microphones over this short five-minute period we introduced this idea of mutual information last time which is on the surface it's at least like correlation so we're looking to see whether there are correlations between the volumes of the microphones over this five-minute period if we look at this particular time window here whenever speaker speaker tues volume was high one in three were medium right so there's a correlation over just this short period but there was a correlation so we're going to assume during that period ed speaker 2 was talking and speaker 1 in 3 were close enough to pick up that speech so we're going to assume that speaker 1 and 3 were listening so far so good right then looking at speaker for we've moved the time window a little bit and during the short time period speaker for a microphone for volume was uniformly high and during the period that it was uniformly high speaker 3 volume was changing so there is not a correlation between speaker for and speaker 3 at this time the fact that there is a lack of correlation we're going to assume that those two individuals were engaged in different conversations so by sweeping this window across and looking at mutual information between all five individuals in this example and they actually did it of course across all 24 individuals for each little time window they can take the individuals in the group and start to draw edges between them where an edge represents an instance of a conversation during this time period s1 s2 s3 or speaking together and s4 and s5 we're speaking together as we draw a little line between them so this is a different kind of social network so if we were to create this kind of social network from Twitter data the nodes might still be individuals but the edges might be what if we're to try and construct the social network from social from social media what would an edge represent in that case I'll following right mid to individuals follow each other right so here in that case they're explicitly indicating that their friends or that they want to read each other's contents right they're signalling it here the individuals aren't signalling it right we're indirectly inferring this edge based on the fact that we think given this procedure I just described to you that they're involved in a conversation and whether you want to call that friends or not we're just going to quantify the number of times they were in a conversation together so over this six-month period and breaking it up into five minute chunks they recorded lots of instances of subsets of the individuals being involved in conversations so on Monday we got this network on Tuesday we got this network and if we combine these two networks together we put them on top of one another and just count we see that there are two edges between s1 and s3 in this case and only one instance between the others so now we're taking these unweighted graphs that are produced from these five minutes snapshots putting them all on top of one another counting up a number of edges flattening it down into one single weighted graph where the weight of an edge now represents the number of conversations that those pairs were involved in at the same time make sense okay so that's we've gone all the way now from raw sensor data to this data structure right at the end of the six month period and this is our social network right we don't really know among these 24 individuals who are friends and who are not we're just going to use this conversation derived social network to now go on to the last part of the study which is to ask whether individuals act differently depending on who they are speaking with so far so good okay here we go okay so now we were before we were just looking at volume and using that to derive our social network data but remember they were also recording different aspects of speech and we're going to focus on two aspects rate how fast they speak pitch what was the pitch of their voice turn length how long was one person talking before they handed over the floor to someone else and obviously that's something that's relatively easy for us to determine so how long do they speak and in one of those five minute windows in which we were recording a conversation how often did it go back and forth between the individuals was it one person speaking for the whole time or was there rapid exchange of one person speaking followed by another one right so turn frequency okay so let's start at the top of this slide we're going to work our way through this slide step by step let's start with the hypothesis here's a specific hypothesis people change their normal way of speaking more when they speak to strangers than with good friends right now the question then is what is your normal way of speaking and how do we know whether you're speaking with a good friend or a stranger so in this example what do you think is going to count as a stranger or a good friend we're going to look at one of the 24 graduate students we're going to call that student SI and si spoke to lots of other individuals from s eyes point of view who is a stranger and who is a good friend how would we know from all the data we've looked at so far possibly so we could go back and use this additional information to try and infer something about friendship but we're not going to yep exactly so for our purposes we're going to we're going to consider a stranger as someone maybe that they were involved in a conversation with once or a few times they might have been involved in a conversation in which their volume was always medium meaning they didn't say anything they were maybe just standing next to some other friends and listening or they were sort of peripheral e involved in the conversation and on the flip side if I'm speaker I and there is some other individual in that group who is SJ it's going to be assumed that that is a good friend if si and SJ were involved in a lot of the same face-to-face conversations okay normal way of speaking is a little bit trickier so let's see if we can derive ways of speaking so in order to test this hypothesis we're going to compute a number of features from the data that we have the first one we're going to compute is may be difficult to see on the slide here B sub I /j so b is going to be a particular feature and it's going to be drawn from one of these four so we're actually going to compute be four times once for each of these four features and we're going to compute it from the point of view of I who is the individual that were interested in this case and we're going to look at the average speech feature so what was the average rate at which I spoke what was the average pitch at which I spoke what was the average turn length that I was involved in and what was the average term frequency for I whenever I was speaking that's B sub I and I /j means when they were speaking with anyone else in the group except some other individual that we pick called J so we've got one of the 24 grad students whose I we've picked another of the remaining 23 grad students whose jay and we're going to compute B sub I /j across all the other 22 individuals so when I was involved in a conversation with all those other 22 individuals and they may not up it may be a subset of those 22 we're going to compute these four quantities however out faster they speak what was their pitch and so on so far so good okay then we're going to compute I a B sub I arrow J so I arrow J means I is speaking to j so how do we know that how do we know instances in time during which I was speaking to jay from the data that we've seen so far I'll give you a hint we don't know that I was speaking directly to Jay but we do know that I was speaking in JP heard it how the correlation there's the correlation right so eyes microphone was registering high volume and Jays microphone was registering medium volume during that period right so if we have if we compute these two quantities and we're going to compute those two quantities for this then this then this and then this we know how I tended to speak to the rest of the grad students in general and we also know how I tended to speak when Jai was listening okay we're also going to compute s this should be s sub I here which is the standard deviation of eyes speech feature be regardless of the interlocutor so the interlocutor means the other person that I was speaking to so some people vary their speech more than others if I was wearing a microphone walking around my S sub I would be pretty high because sometimes i'm speaking quietly in my office and sometimes i'm speaking at a much higher volume when I'm lecturing right so we want to try and calibrate this because we're going to compute these quantities for all the into individually for each of the 424 graduate students and we're going to try and normalize their behavior based on how much their behavior tends to change in general right okay okay we're going to compare over then I'm going to compute d sub I J which is the amount that I speech changes when speaking with Jay Saudi stands here for difference what is how much does eyes behavior differ when they speak to jay compared to when they speak to any of those other 22 graduate students so far so good okay how are we going to compute that difference what we're going to take the absolute rinse between B sub I / j & B sub I arrow j why are we taking that absolute difference what does that give us forget about the denominator for a moment when this absolute difference is 0 for some individual in the group some I let's say it's let's say I am I and you compute this and you get a numerator equal to 0 what does that mean I'm just being totally normally regardless of who I speak to right so you're going to compute you're going to compute these quantities for this J then we're going to switch to that Jay and that Jay and that Jay and that Jane every time we change the J from eyes point of view all the other 22 grad students become the others and if you compute this if this comes out to 0 that means I don't really change the way I speak regardless of who I'm speaking to if this difference is high that means there's some average of the way that I tend to speak in general but for certain Jays my way of speaking changes quite a bit right it's very different from speaking to the others there's one or a few individuals in the group for which I really change the way that I speak so far so good why do we divide by s S sub I what's that doing for us calculate its in the natural very handsome how they see the natural variance right so I don't think among the grad students there would be any that we're going through puberty but if you were doing this with someone who's going through puberty there s sub I is going to be pretty high but it's for physiological reasons not necessarily for social reasons right and that's what we're trying to get at here how do people change the pitch of their voice depending on who they're they're speaking to ok so now we've got this quantity dij which is the difference in how people talk and we're going to compare it to cij which is the fraction of time that I speaks to Jay right if you do that in this case they compare dij to cij they found that over all four of these features there was a negative correlation so that meant that whenever cij is low d IJ is high and when cij is high d IJ is low what does that mean they found this correlation across all the eyes and Jays in this group of 24 that's right exactly so the less time you spend with someone which is a low c IJ the great the higher your d IJ is that whatever you said when you were speaking to that j the way you said it was very different from the way you normally say things when you're speaking to a good friend or someone that you're involved in a conversation with quite a bit you're dij tends to be lower it's closer to the way that you normally talk if you're paying attention you'll notice that there's a little bit of a bias in here or a confounding factor which is if cij is high that means you've spoken to that person a lot which is making up the bulk of the average here right so there's a little bit of a bias in this so whether you want to trust this result or not you might want to take this with a little bit of a grain of salt but it did seem to be robust across all 24 graduate students here's our which is the correlation for those of you that have worked with correlations before you might notice that these are relatively mild correlations so a very strong correlation between something is usually +1 and a very strong anti correlation between something is minus 1 things near zero not so correlated but remember this is real world data right we got noisy sensors we're dealing with people in social interactions a lot of noise here so in this case even a minus point two is a pretty strong anti correlation in this case what do the peas here represent among you remembers their statistics what's a p-value its significance right so P is a probability for the truthfulness of in this case the are so again we only did this with 24 graduate students if we did it again with another 24 graduate students or a hundred or a thousand graduate students would we see this negative this anti correlation again who knows a very very small p value of p value very very close to 0 means there's a good chance that this is real there's a very low probability that this happened by chance this is probably capturing a real phenomenon of social interaction this p value over here point 05 for most people a p-value above 0 point 0 5 that means that the probability is greater than five percent that this correlation and it isn't much of a correlation anyways is false so in terms in at least in the case of turn frequency I definitely wouldn't leave the statistic I wouldn't believe that people change the frequency at which they hand over the floor to each other to change depending on who is speaking to whom but personally I would probably believe this one that people change their rate when they speak to people they don't speak too often compared to who they they speak to frequently do people speak faster when they speak to someone they're less familiar with or slower maybe so the data doesn't tell us right all it says is they change it's different from how they spoke some people might speak more rapidly when they speak to someone who they're not as familiar with I may speak slower this study doesn't answer that question just that you do change your behavior depending on who you're speaking to okay so the data seems to support the hypothesis here that people do change their normal way of speaking when they speak to strangers than good friends let's look at a second hypothesis so I'm going to just go back and forth from these for a moment so we're still going to look at the hypothesis of whether people change their normal way of speaking but now we're going to look at not just how Jay relates to eye but how J where J is in the social network at all so do you change the way you speak when you're speaking to a well-connected person what's a well-connected person in this study it's kind of already up there but what would be a one-sentence summary of a well-connected person here person with a high number of edges and not just number of edges but number of weight right so we're summing up if we're looking at J we want to know how connected j is in this particular social network we're summing all of the numbers of the weighted edges adjacent to j and the numbers remember represent the number of conversations that they they had so we're going to compute in this case a new variable called CJ and CJ is the centrality of j in the social network so in this little cartoon here is someone who is not very well who is not very plugged into the social network they may have just a few other people that they converse with this other J over here this person has entered into lots of conversations with lots of other people in the group okay so we're going to again use d IJ so how what is the difference between the way in which I speaks when they're speaking to j compared to everybody else and we're now going to compare dij to see jay and in this case we get not a negative correlation but a positive correlation between these two variables whenever both of them are low when one is low the other is low and when one is high the other is high what does that mean let's take the first case so for a given for a given Jaso someone that's well connected all of the eyes that speak to them or sorry low CJ means someone who is not really had a lot of conversations with a lot of people in the group d IJ is low what does that mean no all right let's start with the let's try the other one then we have we go among this group of 20 for graduate students each one gets to bj in turn so we're considering the centrality of each of the twenty-four graduate students in turn we're working our way along the 24 graduate students and we found and we find that this particular grad student J as a hi CJ they've entered into lots of conversations with lots of other individuals in the group and then we look at all the eyes all the people that spoke to that person and their d those eyes those other eyes there d IJ is high what does that mean so close the less that they speak to the less they change their voice it's actually remember we have I and J here so we're now sort of switch things around and we're trying to see things from Jay's point of view so Jay has had a lot of conversations with a lot of other people and now among the other 23 we look at all the other 23 graduate students who talked to Jay this central person in the group and for each of those other eyes we calculate those other eyes di Jay and we see that d IJ for those other individuals is high what can we conclude about those other individuals the other eyes when they speak to jay who is central they change how they speak right if we march our way through the Jays again and we pick a grad student in the group who's not very central and we look at the few individuals that did speak to them and we look at those eyes who spoke to that Jay those di Jays are low what does that mean they're just speaking in their normal way of speaking it doesn't diverge or differ much from how they spoke to everyone else okay so in terms of rate and pitch and I probably wouldn't trust turn length and turn frequency the p-values are too high so for me I'm convinced that I definitely did change the rate at which they spoke and the pitch of their voices when they were speaking to a Jay who is popular who is Central and socially well-connected in the group right so again they seem to have collected some data that supports their hypothesis among this group the grad students tended to change the way they spoke more when they were speaking to someone who is well-connected in the group which is maybe not so surprising in retrospect but kind of interesting because it also tells you some other things about some of the social dynamics that are going on in that group which are they don't change as much when they're speaking with someone who is not well connected in the group right but what else can you infer from this group of 24 people beyond just the data that I've shown you so far changed your voice to be to make themself mark could be so we could get into questions about why people tend to change the pitch or rate of their voice when they perceive that they're talking to someone that's well connected that's actually the piece that I was going after which is that they perceive cool is well connected and who isn't right so again assuming this correlation is real among the 24 graduate students they generally knew who was more socially connected than who else right when they were in the presence of a Jay that was central the eyes knew it somehow or knew it on some level that they changed the way they they spoke right whether that was conscious knowledge or subconscious knowledge who knows do you think these patterns would hold up if you were to do this again with another fresh group of 20 for graduate students if you did it with a random collection of 24 people how about a hundred people a thousand people is this restricted to people that speak English does socio-economic background matter who knows right we could try it again assuming people are willing to be in the presence of a microphone for extended periods of time we could start to ask more of these questions of larger and more diverse groups right if you were to ask the grad students and if all of this data was hidden from them you showed them these two hypotheses and asked them whether they thought yes or no I don't know how they would have answered right what are they aware that this is is going on right maybe there are these social dynamics that we could get at with ubiquitous technology that we could not have got at if we had SAT them all down and gave them some very detailed social psychology survey right this is also the ethical side of this right maybe we're uncovering aspects of human behavior that the humans in the study are not really aware that they're exhibiting right okay okay so that's lecture 17 let's change now to lecture 18 we're going to take a break from speech for a moment and focus on activity the i'll i'll come back to that in a moment the material for lecture 18 was drawn from a research paper way back in the Stone Age 2011 before I think fitbit's existed at that point but there weren't a lot of them around right a lot has happened in terms of mobile health monitoring in the last five years this slide is almost completely out of date by now but i think this study points up a lot of the interesting aspects of this particular part of HCI ok so why might you consider to wear at fitbit because you want to improve your physical and mental well-being right first step in that of course is to collect raw data and then translate that raw data somehow into an inference of how much sleep you're getting per day how much physical activity you're getting how much intellectual stimulation are you getting per day social interaction how socialized are you each day what's your diet like what's your stress level like so like we just saw in the previous lecture and like you're doing with your leap motion we're going to try and go from different kinds of raw sensor data to a model which predicts on its own how much sleep you got that day what was your stress level and so on some of these things are easier to predict than others such as for those of you that have fitbit's or know how they work what's easier to get at than others among the list here of course there are lots of other correlates with physical and mental well-being but let's restrict ourselves to this list why why is that The Witcher sorry the step tracker right there you go counting steps is kind of medium level if they've kind of figured that out by now but it was it was tricky right you can fool the step counter how I guess writing on an old train lots of other things that can trick a Fitbit right so again we're going to get some false positives with all of these technologies up to a point we would like again thinking about ubiquitous computing you don't want your Fitbit to continuously be asking you did you just take a step was that another step was that another step right that's probably not going to work be very acceptable so you want assuming you are interested in mobile health monitoring you want whatever technology you're wearing to sort of fade into the background and be invisibly recording this information for you and doing it on a more or less continuous basis right it it matters right we can't take a reading every hour so we need things that are recording continuously and unobtrusively diet is the one correlate here that is resisting automation why most of these things are at least in the in the process of being automatically inferred from Fitbit in their ilk not diet why cuz it's not going to be eating yes exactly right so you still have to get one of these other websites and type in all the foods that you ate that day in the amount and and so on which again is not very acceptable to most people some people do it but takes a while to write to type in everything you ate and the content throughout the day they're getting to the point where they may be able to create a sensor that sits on the inside of your mouth and now it is at least tasting the food that you're eating how many of you would be willing to wear such a device pretty pretty intrusive right failing that it's pretty hard to see how to automate this I mean you can take a picture of your food and it tries to infer the caloric content from that tricky right so some of these aspects are resisting automation more than others okay so in all mobile health monitoring the main goal of course is to measure several correlates of well-being continuously and automatically in the background so to speak right which correlates are we going to go after some of them are easier to record than others once we do we want to take those measurements and pass them through a model which is giving us a rough estimate about how well we did that day did we get too little sleep just the right amount to much did we not get enough social interaction where we more stress today than we were the day before can it give us some sort of general reading one level up from the raw data to help us calibrate our behavior right so if our Fitbit or whatever device records that we're a little bit more stressed today than yesterday hopefully that gets to wear to think about what happened today that caused me to be a little bit more stressed some things may be obvious some of them might not be as obvious until you start thinking about it we're looking at some of the other measurements okay and again show these show these results back to the user in a continuous but perhaps unobtrusive manner okay so we're going to tackle each of these three goals in turn okay so again we're going to look at this specific application the be well approach monitor behavior is step one model well-being so go from the raw data to a prediction about general well-being and then let the user sort of see these readings in more or less real time so that they can start to generate hypotheses and ideas about how to change their behavior to improve well-being again I mentioned this papers from the Stone Age so things are a little bit outdated here it might be difficult see in the picture here but the green line here or this top plot is reporting a particular day six in the morning noon and six in the evening the Green Line is reporting whether the microphone picked up voice some unknown sounds ambient noise or complete silence so it's hopping up and down between those four estimates so there's already some machine learning going on here right there taking the raw data coming from the microphone and trying to categorize it into one of these four classes not unlike how you're taking raw gesture data and trying to categorize it into one of ten digits the accelerometer using the orange here is giving an estimate about whether the user is currently walking stationary or running okay let's start with sleep this was one of the correlates of well-being that be well tries to record you're going to use a cell phone to do this for a moment the sensors you have available is a binary sensor so the phone knows whether or not the battery is charging obviously the phone has a clock an accelerometer a digital compass gps microphone and a camera given those sensors how would you infer the amount of sleep that the user is getting per day okay so there's our first version one point oh we're going to just use the microphone and when it's quiet we'll assume the user is sleeping and when the microphone picks up sound the user is awake it's actually a pretty good start right but obviously it's going to be an approximation remember that we want to think about the users here why might that fail what's missing here yep heartbeat yes we don't have that though we're assuming that we're going to run this on a phone and of course if it's a wearable yes well you can also use the camera see like what the brightness is outside you could look if it's dark you wouldn't be doing like you more likely that you're sleeping and you use the accelerometer because okay right so station if the phone is stationary if it's dark if it's quiet we put those three things together maybe that means sleep this really like usually before going to set the alarm ok so you to you set your alarm before going to sleep not everyone here probably does so of course this is a bit of a trick question right there's going to be different solutions for different users they might need to calibrate it let's just go back to version 1.0 for a moment right so silence versus noise assuming we're doing this for students hopefully some of you spend some time in the library where it's very quiet lights are probably on so if we also combined light levels in there that'll make sure that the library is not recorded as a false positive but maybe you are sleeping in the library right we'd have to catch that one too so we're going after silence and darkness and lack of movement physical context right most of us have our phone in our pockets right and as good HCI designers by now that probably occurs to you the moment we start to fold in the camera and light levels right where where is the camera most of the time for a given user so darkness and silence and lack of stationarity is probably not enough what else do we need to add in will fold the clock in right and we'll sort of just bias things that if it's late at night we increase the probability that the system is going to predict you're asleep unless you have sleep issues and you're waking up during the night for considerable periods of time whether it's not the charger whether it's on the charger or not right again most of us probably charge our phones overnight so any one of these sensors is not going to do it right the same way that anyone coordinate from gesture data is not going to tell you what it is right you're going to have to combine a bunch of features together to improve your accuracy short of putting an EEG connection into your phone absolutely right if we really want to be accurate about sleep we're going to have to go to EEG for some of us unfortunates you might lie there in bed at night it's dark it's quiet you're not moving and you can't sleep right and it's hard to hard to catch that you might be at seventy-five percent or twenty-five percent asleep and getting that kind of information you're going to have to do something that's pretty intrusive right that being said and Fitbit is getting pretty good at this if you wear your Fitbit at night even if you're awake your movements are quite different from your movements when you're asleep if you apply some sophisticated machine learning and you have enough training data you can actually start to infer whether someone is lying in bed awake or asleep right movement data is very rich and powerful for these kinds of predictions okay so you can sort of see some of the unique challenges when we're talking about wearables or mobile health monitoring you really now need to think carefully about physical context social context differences among individuals in order to combine sensor data together or even to determine the right features for training a model to predict in this case sleep ok let's see a little bit more they actually did combine some of this information together in the be well application they combined phone recharging movement and ambient sound so total volume was sufficient to predict the amount of hours slept to within about plus or minus 90 minutes so they did some significant user studies where the user self-reported how many hours they slept per night and again self reporting is a little bit noisy but within 90 minutes is not not too bad not too bad ok let's finish with this slide so they pulled out they went from raw sensor data on the phone to a model that predicts the number of hours that the user slept per day we're now going to get add on another model which is going to predict whether you got the right amount of sleep or not this is again very tricky what counts is the right amount of sleep here they picked a very approximate model they said it for every 17 hours is ideal does that feel right to you seems low ok it seems low to me too but but there you go ok seven is ideal anything less than that or more than that you're falling off your well-being score right so there's lots of arguments you could make about this particular model but again we could refine this over time maybe we're setting like your alarm maybe you set the ideal amount of sleep you want to try and get per day or depending on the day and then we have the system this hierarchical model where we now have two models along the pipeline we have raw sensor data one model that's predicted the number of hours we slept per day and then a second model that's taking those hours and predicting a well-being score did you get too little just the right amount or too much sleep last night okay I think we'll leave it there you have a quiz due tonight you're working on deliverable 10 and i will see you all on monday thank you 
mEYTNM6CHZI,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-11-07T17:42:49Z,"L26: Activity Tagging. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/mEYTNM6CHZI/hqdefault.jpg,Josh Bongard,PT49M21S,false,31,0,0,0,0,okay let's let's get started just a reminder if you haven't signed up yet for the cs fair you have an overwhelming advantage over your student colleagues in other classes who doesn't love ASL leap motion first place is $300 second place is 200 100 and even if you're not in the top three you're at least going to come out with a free shirt so what's not to love right how many people are signed up for CS fair so far a couple people ok I highly recommend that you do there was also free pizza last year I don't know if that's on the offing this this year but anyway it should be a good time ok just a reminder if you haven't done already really easy to sign up your project for CS fair deliverable 10 is due on Wednesday by now your code is probably starting to grow just a couple of rules of thumb as you start to develop larger and larger code in Python at least this should become your best friend which is comment right so when we talked about making your program stateful and adding states it should be relatively easy for you if your code is not working to comment out the most recent states you've been at adding and you should be able to with a few hashtags roll things back to just the first state and then uncomment the second state make sure the first and second state are working add back in the third and so on so as your code becomes more complex you might want to refactor it or restructure it so that with one or two hashtags you can comment out the code that you just added and rule your system back to the point where you knew everything was was working okay just just a little hint there for you any other questions about deliverable 10 ok we will talk on Wednesday about what you'll be doing post deliverable 10 for the rest of the semester ok so jump back to lecture then so we're working our way through this theme of looking outward where we're talking about interactive technology that is somehow out here in the world with us not hidden behind the pane of glass of a laptop or a desktop passively waiting for input from the user in 17 18 and 19 we're looking at three different scientific projects in which they've embedded some technology out in the world and are trying to tackle a hypothesis that would be difficult or impossible to answer without that kind of technology so in 17 we looked at a research group that studied the questions of question of how people change their social behavior inside of a social group we're going to finish 18 today which is an application of mobile health monitoring the biggest trick in mobile health monitoring is tagging or inferring the activity that the user is currently engaged in and we're going to move on today to lecture 19 known as the human speech own project which again involved some deploying of embedded technology to answer the question of what causes a child to utter their first word for the first time what what is the combination of social and physical context or experiences that led them to utter their first word there's lots of theories out there in developmental psychology the study of children but not a lot of data to support or disprove those hypotheses with embedded technology you can start to ground some of those hypotheses about the beginnings of speech okay so let's hop back to lecture 18 we ended last time by thinking about some of the different ways of inferring the different correlates of physical and mental well-being some of these are easier than others we spent a little bit of time talking about sleep for example with a combination of whether your cell phone is charged in and it's dark and it's quiet and it's the middle of the night and it doesn't detect an emotion perhaps the users asleep some of these are much more difficult like diet absent putting a sensor actually on the inside of your mouth it's going to be hard for sensors to directly measure caloric intake and somewhere in the middle is inferring physical activity right what is the user doing and how many calories are going out how much exercise or they currently engaged in okay so we looked at sleep and I think we got here last time so just to remind you in this particular project there are two stages of machine learning there's taking the raw data and trying to create a model that predicts how many hours of sleep you got last night how many hours of exercise you got this past day once you have that prediction from the lower level machine learner then there is a second level and here's an example of a model this model wasn't machine learned it was made by hand by the investigators that takes as input the actual amount of sleep HR act the actual amount of sleep the user got over the past night and makes a prediction about how good their sleep was the night before so the model here is obviously very approximate it basically assumes that seven hours is ideal and any more than that or any less than that is less than than ideal right so raw data to predicting the actual correlates of well-being and then once you have your prediction about the correlates of well-being can you say something about whether the user is on track or off track for that particular correlative well-being okay that's sleep let's tackle the tricky ones here which is physical activity so assuming we're just going to use a cell phone and we'll assume that the person is wearing the cell phone most of the time when they're exercising which you may or may not do if you go to the gym let's assume that our users are wearing the cell phone which of these are easier to infer than others Swimming's going to be pretty tricky unless you have a waterproof cell phone what about walking running and biking why is running possibly easier than walking or biking because your leave there's right so you're falling faster there's faster input which is being picked up could be picked up by which of these sensors the accelerometer right so clear the accelerometer is going to be pretty important here especially absolutely right so your leap motion device or your cane and learner is out probably not a hundred percent right sometimes it's confusing zero one or two you got the same problem here right in this case instead of gesture data you have acceleration data and you're trying to predict at this current moment in time is someone walking running or biking what do you think it is about that data that causes the learner whatever it is to make mistakes from time to time fitbit's are using a machine learning algorithm that's much more sophisticated than K&N but still these are tricky things to tease apart yep you know what this cell phone they generally rely yes that's it right so it's not just acceleration its GPS how is how can gps help here what does GPS inferring exactly right no problem absolutely right so no sensor on its own is going to be perfect like we saw when we were doing the hypothetical example with sleep last time right you're gonna have to combine these signals together so GPS may or may not and sometimes it may be able to give you an estimate of velocity and that's going to help right there's certain velocities in which most people can't run at that's probably going to be biking but then you got another problem you detect high-velocity maybe someone's in a vehicle right so velocity alone is not going to be important how can you distinguish between a slow-moving vehicle and someone who's Vikings perhaps what else heart rate right so assuming we could get that now we're going to assume with a normal cell phone we don't have access to heart rate remember that in especially when we're talking about activity tagging here you really need to pay attention to physical and social context here what might be helpful the accelerometer where we're on their person is the user wearing the cell phone right the higher on their body they're wearing it the harder it is to disambiguate between driving and biking all depends I guess if it's on their arm you might be able to pick up that rhythmic signal right so you're looking for a rhythmic signal which which will help you infer whether the person is actually exerting effort right are one or more of their limbs moving rhythmic lanes absolutely right that might be helpful what about the stationary bike at the gym right there go velocity goes out the window some of these other assumptions we might make go out the window so maybe we need to add additional categories right on the treadmill on a bicycle running on a treadmill running normally and so on it gets pretty tricky okay so let's have a look at how the investigators here actually tackled this problem well we want what we're really trying to get out is not necessarily the activity but the amount of exertion right minute by minute how many calories are going out for most people that's wearing a Fitbit that's really what they're interested you know all right what's their level of exertion so let's start again by setting this up as a machine learning problem let's try and find out a way to measure actual exertion how many calories is somebody burning at a given point in time then if we have a way to measure how many calories they're burning and we're collecting sensor data from their phone at the same time you can see where this is going we can then start to use a machine learning algorithm that will create a model that takes as input the caloric out takes as input the acceleration data makes a prediction about exertion if you're able to measure actual exertion you can look at the difference between these two things and you're going to try and tune your model to reduce the difference between the actual exertion that you measured and the exertion predicted by the model and keep training the model so now when the user goes out in the world and starts using their device you don't have actual exertion anymore you're not measuring it but instead you have this trained model which is running on the phone and that train model is taking acceleration data as input it's not necessarily predicting the activity but it's making a prediction of caloric output okay let's start with step one how do we go about measuring actual exertion so we could measure heart rate heart rate is okay but there's lots of other things that cause your heart rate to become elevated other than just physical exercise clerk cleric output right so how much energy is somebody burning at a given instant in time how can we measure that okay this is the standard way of doing this in exercise physiology is to measure vo2 max which is the change in oxygen volume over time you can see that this is not a very acceptable option for a lot of people right you they need to actually be wearing a mask and in that mask is a detector that measures when you breathe in the amount of oxygen in the air that's flowing into your mouth and then when you breathe out through your mouth or nose it's also measuring the amount of oxygen and if that amount of oxygen goes down what does that mean you're burning oxygen right that's that's your actual usage of of energy the rate or the difference between oxygen in and oxygen out the greater that difference the greater your exercising or burning calories so we're not measuring calories directly we're getting at it through this vo2 you'll notice there's a dot over vo2 because we're looking at the change in the volume of o2 right so this is the rate of change well how do we get at vo2 so we could obviously instrument someone have them a large number of people have them engage in a lot of different activities make sure they're wearing a cell phone and we're collecting vo to and from vo2 we can measure caloric output pretty well and we're getting acceleration data which is going to give us this raw day for a machine learning exercise we could do that that's a little tricky maybe we can sort of take a few shortcuts here well we know that the amount of energy you burn is a function of three variables there's your resting metabolic rate so what is your general level of fitness to begin with this is also influenced by age gender all sorts of things we can then try and get at again from the acceleration data just H and V what is horizontal displacement and what is vertical displacement again a little bit tricky when you're on a stationary bike but generally speaking when you're moving around if you can capture H and V and you know something about someone's are you can predict their vo2 okay so let's start we're going to set our aside for a moment that one's tricky that one is difficult to estimate with a cell phone let's start with H the easiest way to do h and all the pre Fitbit devices relied on this which is just a step counter right how many steps are you taking which is a good estimate of horizontal self displacement you're sitting in a car and moving you're not getting this oscillation in you're not getting this frequency right so we're getting acceleration data and that acceleration data is following this sinusoidal pattern because someone is scissoring their legs as they walk or run or they're spinning their legs as they're on a bicycle and obviously the greater the frequency the higher the number of steps so the more steps you you capture the greater speed the person is making and you should up your estimate of vo2 max it's pretty good again these are all sorts of approximations what's the problem with this particular approximation even just for estimating H here so I walk i watch all of you leave the classroom and I see one of you take three steps in four seconds and I see somebody else take three steps in four seconds have you both crossed the same amount of distance why not longer legs right not everybody walks with the same amplitude right if it's a five-year-old and they have the exact same frequency as an adult that's also very different right so again these are all approximations we need to take into account the particular human being that we're dealing with to refine these estimates that's H what about v so is someone walking over flat ground are they walking up a slope are they walking up steps are they walking down how do you get at the V or the vertical component of displacement over time we're going to obviously go after the accelerometer as you increase in altitude air pressure goes down but that's not going to be very good unless we're talking about large vertical displacements and most of us don't have a barometer in our cell phone anyways an accelerometer captures your acceleration along three axes right actually the horizontal the vertical left to right so we want to use the vertical component of the acceleration data but how I want to know whether you're going up or down a slope how do we do it yep yes absolutely so frequency is no longer going to be sufficient right we need to look at amplitude how much did it go up compared to how much it came down before it started going up again so we might be able if we have a really good accelerometer to look at that maybe and then we start to integrate over time right we may not be able to estimate it well for a single step but over five or ten steps we might be able to do a little bit better job maybe we've been the data and we say okay in the last tenth of a second or half a second was generally speaking was the cell phone itself rising or falling and then look for any asymmetry so if the cell phone is spending more time rising than falling then the user is probably walking up a slope or walking up steps probably going to be very difficult to distinguish between those two things but generally speaking we might be able to detect an asymmetry and now start to get at V as well right so obviously it makes a difference for your calories for a given H whether you're walking downhill or uphill okay so that's what that's one way to do it alternatively and this is sort of the shortcut that the experimenters took in this paper is instead of trying to estimate all these very fine-grained things they just sort of did this activity tagging where they used a machine learning algorithm to take acceleration data and predict activities so they actually instrumented students with a mask and recorded vo2 max and trained a model to predict our sorry I'm confusing this with another experiment all they did is ask the users to report forgive a certain periods of time their cell phone would beep and say could you please tell me what activity are engaged in and the users would report back they are driving in a vehicle they're stationary they're running or they're walking or they're engaged in some other activity they would learn not to report on so there was a fifth category so the machine learning algorithm has these four categories again not unlike your 10 categories for recognizing digits and they had acceleration data so they train their model to predict to take as input acceleration data and predict which of the four activities the users were actually engaged in they then assigned to each of these four activities metabolic equivalent of task for MIT which is for most people on average how many calories they burned when they were sitting in a vehicle sitting quietly walking or running again not very satisfying because MIT is going to be very different for different people but a good first approximation okay so they now have a machine learning algorithm that can predict which of the four activities the users are engaged in and at each moment in time they just started to add up the m et for those inferred activities and just some that over the entire day of the user and that's your total amount of MIT which is MIT act here the actual amount of metabolic equivalent of task you expended throughout the day so that's again the first level of machine learning algorithm and then the second learning the second a model they built on top of that is this relatively simple model here which more or less says the more physical activity you get per day or the more any tease you collect during the day the better and they sort of normalized it a little bit by maximum and minimum recommended physical activity for a single day or sorry for a week so they average this over a week ok so again kind of hand wavy and obviously this is now being significantly refined in commercial products like Fitbit and so on this experiment i think is a good one because it's all reported in the literature you sort of see what's going on under the hood and hopefully see a lot of the challenges in trying to infer in real time and unobtrusively what the user is is doing throughout their day okay last one which we already saw in the previous lecture is trying to infer the amount of socialization you had per day were you on your own all day or did you engage in some conversation so again they're inferring quantity here called duration of social activity which in this case is just the total amount of conversation so the subjects involved in this study agreed to allow their microphone to be left on on their cell phone again it depends where on their body they were wearing the cell phone there was some machine learning that was working on the raw audio to infer first of all are their voices and are those voices coming from someone else coming from a TV or a computer or is it the user themselves that's talking and they threw everything away except the person talking so conversation here you can think of is in scare quotes it's basically the amount of time that the user spent talking throughout throughout the day okay so now we have three correlates of well-being that these investigators tried to derive the total amount of sleep you got last night the total amount of physical activity you got this past day and the total amount of time you spend talking and it's assuming that you're talking to somebody else throughout the day and trying to keep those within reasonable limits okay then the third part of this study was to create visualizations so you could look at your phone at any given time and with see a relatively a relatively intuitive visualization of these things what can you tell me about this particular single representative user through out there week they were clearly more social on the weekend that's right so Monday and Tuesday not very social activity certainly fell off on the on the weekend and sleep also degraded throughout the week sounds about right to me okay again very approximate but there's the idea and then for fun they made this very state-of-the-art real-time visualization here screensaver for their phone so the turtle was either sleeper Lake depending on how much sleep you've been getting in the last few nights the clownfish would do loops here the orange clownfish would do loops depending on whether you're getting enough activity and there would be more or less of these little yellow fish indicating socialization somehow take it with a grain of salt okay again very approximate but giving you the basic idea right can you look at your phone throughout the week and get a more or less objective picture of how you're doing in terms of sleep physical activity and being social okay how could these sensors be fooled I think we've talked about this quite quite a bit and this is the million-dollar question for Fitbit and other companies that are trying to refine these predictions right what sorts of data can we add in what sorts of data streams could you add in above and beyond your phone that might help refine these these metrics and how can we approve improve the actual machine learning algorithm that's making these predictions for you okay any questions about that before we move on okay let's move on to lecture 19 now which is the human speech own project so in lecture 17 we looked at ubiquitous computing for inferring a social network and then how people change their behavior inside that social network we just finished looking at an example of embedded technology in this case which is just your phone for doing real-time mobile health monitoring now we're going to look at some embedded technology which is going to try and help us better understand how children acquire language okay the reading for today is again a research article this is I think this one is pretty digestible this is by the lead investigator of the study sort of talks not just about this study but how technology is changing the way we go about studying language acquisition it's an interesting interesting article so it addresses this question of how do we attack this question of how do children learn language it's kind of an open-ended question well clearly how they learn language is influenced by what they hear and see and feel around them in the second the minute the hour the day the week the month and the years leading up to the utterance of a word for the first time how the heck do we know inside that huge mass of sensory experience what aspects I that influence their ability to ground or learn language right what events in the past actually influence this again if you look at the developmental psychology literature there's thousands of theories out there there's really relatively little data to go around so there's not much data to prove or disprove any of those hypotheses until now okay so what's the typical way that this is done well the developmental developmental psychologists usually will invite young children and a caregiver which is a parent or a nanny to come to the University and come to a lab be in a very controlled setting the caregiver the parents or the nanny is given a very careful script of exactly what to do and at what speed to do it and then the investigators observe the children there is a theory that says joint attention helps children learn language we're going to come back to joint attention next week or in three weeks from now joint attention is the idea that I can do things to draw you attention to something I want you to attend to I can do it as simply as just looking at something and you will most of the time if you're looking at me you can infer from my pupils where I'm looking and you will saccade or move your eyes so that we are jointly attending to the same object so there's your hypothesis that you're starting with now you want to conduct an experiment in the lab with children to see this is whether this is the case or not so the script you might give to the caregiver is parent please look at an object and when you look at it repeated repeatedly pronounce the name of that object there will be the same number of caregivers and children that are brought into the exactly the same lab under the same lighting with the same blocks and the parent will be given a slightly different script which is do not look at the block look at the child and repeat the name of the object so we have two data sets one in which the parent was trying to get the child to jointly attend to what they're looking at the other group not and the question is did the child say blue block more often in the joint attention experiments than in the control study and if that's the case that's data supporting your hypothesis that joint attention tends to help children acquire language all well and good of course except maybe that's not the case maybe its joint attention and something else how hungry the child is that day or maybe on the way on into campus the parent was playing a name game in the in the car with a child and they were looking at blue objects out the window maybe that influenced the child saying blue block that's not in the data set the investigators have no idea that the parent child would playing that game in the car on the way in right there's this huge amount of other experiences outside the lab setting that clearly influenced how children acquire language so how could we ever get at that so deb roy is the investigator at mighty who came up with this idea of actually observing a child continuously for the first three years of their life in the child's own environment this is the first study of its kind it's a longitude longitudinal study so studying the child for a long time over 33 years as you can imagine it would be pretty difficult for professor Roy to get permission to do this with someone else's child so he gave him and his wife permission to do this with their own child as you can imagine this is an extremely controversial study this is a human subject study we're experimenting with humans in order to do that you need to fill out a lot of paperwork and usually you need to get permission from an unbiased third party so if you're doing this with a child the child obviously can't read the fine print and agree whether or not to embark in the study so if you're doing a study with children it's usually the parent that signs the piece of paper in this case there is clearly a conflict of interest because the scientist is also the parent and is signing the permission slip controversial ok we're going to talk about the study if you have a moral objection to the study you're welcome to leave the class at this point oh we're all good ok ok here we go we will watch part of a TED talk of Deb explaining this experiment in his own words imagine if you could record your life everything you said everything you did available in a perfect memory store at your fingertips so you could go back and find memorable moments and relive them or sift through traces of time and discover patterns in your own life that previously had gone undiscovered well that's exactly the journey that my family began five and a half years ago this is my wife and collaborator rupal and on this day at this moment we walked into the house with their first child our beautiful baby boy and we walked into a house with a very special home video recording system okay this moment and thousands of other moments special for us were captured in our home because in every room in the house if you looked up you see a camera and a microphone and if you look down you get this bird's-eye view of the room here's our living room the baby bedroom kitchen dining room and the rest of the house and all of these fed into a disk array that was designed for continuous capture so here we are flying through a day in our home as we move from sunlit morning through incandescent evening and finally lights out for the day over the course of three years we recorded eight to ten hours a day amassing roughly a quarter million hours of multitrack audio and video so you're looking at a piece of what is by far the largest home video collection ever made and what this data represents for our family at a personal level the the impact has already been immense and we're still learning its value countless moments of unsolicited natural moments not posed moments are captured there and we're starting to learn how to discover them and find them but there's also a scientific reason that drove this project which was to use this kind of natural longitudinal data to understand the process of how a child learns language that child being my son and so with many privacy provisions put in place to protect everyone who is recorded in the data we made elements of the data available to my trusted research team at MIT so we could start teasing apart patterns in this massive data set trying to understand the influence of social environments on language acquisition so we're looking here at one of the first things we started to do this is my wife and I cooking breakfast in the kitchen and as we move through space and through time a very everyday pattern of life in the kitchen in order to convert this opaque ninety thousand hours of video into something we can start to see we use motion analysis to pull out as we move through space and through time what we call space-time worms and this has become a part of our toolkit for being able to look and see where the activities are in the data and with it trace the patterns of in particular where my son moved throughout the home so we could focus our transcription efforts all the speech environment around my son all the words that he heard for myself my wife our nanny and over time the words he began to produce so with that technology and that data and the ability to with machine assistance transcribe speech we've now transcribed well over seven million words of our home transcripts and with that let me take you now for a first tour into the data so you've all I'm sure seen time lapse videos where a flower will blossom as you accelerate time I'd like you to now experience the blossoming of a speech form my son soon after his first birthday would say Gaga to mean water and over the course of the next half year he slowly learned to approximate the proper adult form water so we're going to cruise through half a year in about 40 seconds no video here so you can focus on the sound the acoustics of a new kind of trajectory now go to water so he didn't just learn water over the course of the 24 months the first two years that we really focused on this is a map of every word he learned in chronological order and because we have full transcripts we've identified each of the five hundred and three words that he learned to produce by his second birthday he was an early talker and so we started to analyze why why we're certain words born before others this is one of the first results that came out of our study a little over a year ago that really surprised us the way to interpret this apparently simple graph is on the vertical is an indication of how complex caregiver utterances are based on the length of utterances and the vertical axis is time and all of the data we aligned based on the following idea every time when I son would learn a word we would trace back and look at all of the language he heard that contain that word and we would plot the relative length of the utterances and what we found was this curious phenomena that caregiver speech would systematically dip to a minimum making language as simple as possible and then slowly ascend back up in complexity and they made so that phenomena a just described about the utterances becoming simpler and simpler and simpler leading right up to the point when the child said that particular word for the first time and then once the child had said that word the length of the utterances that contained that word became more and more complex from the caregivers in this case him and his wife and the nanny what is that phenomenon we've seen that before scaffolding it was not known that that existed before so obviously parents speak baby talk and they speak simplified language around their children but no one realized that they were all they were adding scaffolding over time until the word was uttered and then gradually removing scaffolding by using more and more complex utterances after the child had said that word for the first time seems in retrospect that now that the child says it child must understand something about what that were is in the context in which it applies now as we start to remove scaffolding can we help the child understand how to use that word in combination with other words to signal more complex speech ones isn't thing was that that bounce that dip lined up almost precisely with when each ordered was born word after word systematically so it appears that all three primary caregivers myself my wife and our nanny were systematically and i would think subconsciously restructuring our language to meet him at the moment of the birth of a word and bring him gently into more complex language and the implications of this there are many but one i just want to point out is that there must be amazing feedback loops it's not of course my son is learning from his linguistic environment but the environment is learning from him that environment people are in these tight feedback loops and creating a kind of scaffolding that has not been noticed until now but that's looking at the speech context what about the visual context we're now looking at think of this as a dollhouse cutaway of our house we've taken those circular fisheye lens cameras and we've done some optical correction and then we can bring it into a three-dimensional life so welcome to my home this is a moment one moment captured across multiple cameras the reason we did this is to create the ultimate memory machine where you can go back and interactively fly around and then breathe video life into this system what I'm going to do is give you an accelerated view of 30 minutes again of just life in the living room that's me and my son on the floor and there's video analytics that are tracking our movements my son is leaving red ink I'm leaving green ink we're now on the couch looking out through the window at cars passing by and finally my son playing in a walking toy by himself now we freeze the action 30 minutes we turn time into the vertical access and we open up for a view of these interaction phrases were just left behind and we see these amazing structures these little knots of two colors of thread we call social hotspots the spiral thread we call a solo hotspot and we think that these affect the way languages learned what we'd like to do is start understanding the interaction between these patterns and the language of mind son is exposed to to see if we can predict how the structure when words are heard effects when they're learned so in other words the relationship between words and what they're about in the world so here's how we're approaching this in this video again my son is being traced out he's leaving red ink behind and there's our nanny by the door she offers water and off go the two worms over to the kitchen to get water and what we've done is used the word water to tag that moment that bit of activity and now we take the power of data and take every time my son ever heard the word water and the context he saw it in and we use it to penetrate through the video and find every activity trace that Co occurred with the instance of water and what this data leaves in its wake is a landscape we call these words escapes this is the word scape for the word water and you can see most of the action is in the kitchen that's where those big Peaks are over to the left and just for contrast we can do this with any word we can take the word bye and so goodbye and we're now assumed in over the entrance to the house and we look and we find as you'd expect a contrast in the landscape where the word by occurs much more in a structured way so we're using these structures to start predicting the order of language acquisition and that's your ongoing work now in my lab which we're peering into now at MIT this is at the Media Lab this has become my favorite way of video graphing just in any space three of the key people in this project fill up the camp Ronnie Kubat and Brandon Roy are pictured here philip has been a close collaborator and all the visualizations you're seeing and Michael Fleischman was another PhD student in my lab who worked with me on his home video analysis and he made the following observation that just the way that we're analyzing how language connects to events which provide common ground for language that same idea we can take out of your home Deb and we can apply it to the world of public media and so our effort took an unexpected turn think of mass media as providing common ground and you have the recipe for taking this idea to a whole new place we've started analyzing television content using this same principles analyzing event structure of a TV signal episodes of shows commercials all of the components that make up the event structure we're now with satellite dishes pulling in and analyzing a good part of all the TV being watched in the United States and you don't have to now go in instrument living rooms with microphones to get people's conversations you just tuned in to publicly available social media feeds so we're pulling in about 3 billion comments a month and then the magic happens you have the event structure the common ground that the words are about coming out of the television feeds you've got the conversations that are about that those topics and through semantic analysis and this is actually real data you're looking at from our data are processing each yellow line is showing a link being made between a comment in the wild and a piece of event structure coming out of the television signal and the same idea now can be built up and we get his word scape except now words are not assembled in my living room instead the context the common ground activities are the content on television that's driving the conversations and so what we're seeing here these skyscrapers now our commentary that are linked to content on television same concept but looking at communication dynamics in a different very different sphere so fundamentally rather than for example man okay so in the first part you saw the birth of words in the second part here you can see the birth of a very lucrative spin-off company right we will talk about the scientific part of this TED talk on Wednesday you have a quiz due tonight and we'll talk about post deliverable 10 on Wednesday have a good day thank you 
knGGVm7gH-w,27,"Human-Computer Interaction(HCI) curriculum brings together computing, interaction design and cognitive psychology. It offers a research-based approach to designing interactive, software and technical systems. Head of the Programme, Ilja Šmorgun introduces HCI programme. 

Interaction Design(ID) curriculum is 100% online study programme. It prepares graduates with marketable skills in design, technology, and theory as related to the domain of interaction design and human-computer interaction. Head of the Programme Sónia Sousa introduces Interaction Design programme.

Q&A session

30:00 – About the time schedule
34:00 – What programming skills are needed for the Interaction Design programme?
35:40 – What level of programming skills are needed for HCI?
36:40 – What is the contribution of each university towards the Interaction Design programme?
37:00 – What are the future opportunities for a HCI Master  in Estonia, Baltic states or in the other European regions?
39:40 – About admission process
40:50 – How helpful is HCI program for UI/UX Designers? What are the learning curves in the course for designers?
41:40 – How easy is it to study Interaction Design while working at the same time?
42:50 – How are the courses structured for HCI?
43:30 – Do I need a portfolio for applying to Interaction Design programme?
43:50 – What do I need to submit for applying to Human-Computer Interaction programme?
44:00 – What should I do if I have more questions?",2020-06-09T09:40:30Z,Interaction Design and Human-Computer Interaction info session,https://i.ytimg.com/vi/knGGVm7gH-w/hqdefault.jpg,Tallinna Ülikool I Tallinn University,PT44M42S,false,648,8,0,0,2,so my name is Sevilla I'm the head of the human-computer interaction study program Italian University and together here is also my colleague Sonya who will do the introduction for interaction design and also Andreas so both of them are managing the interaction design master's program and also our students gabriela Inchon who will give you a little brief personal introduction about their study experience ok so I'll try to share my screen now and we'll go through the through the presentation so I hope everything works nicely I'll try to go fullscreen so that everyone can see ok so I hope that you can see the slides if not please let me know but I really hope everything is working and briefly to introduce the HCI study program to you so in the human-computer interaction study program will emphasize technology for the benefit of people that's our kind of tagline that's our model and that's that's how we try to also approach things on a day to day basis and what are we about we focus on three main things our master's program is very much research driven it's informed by our latest state-of-the-art research that we do within our group and that's that's why I also most of our teachings most of the also the lectures most of the classes that were organized most of the material that we share with our students is not from the book it's from our ongoing state-of-the-art research that's something you cannot kind of go to a library and read about that's something that we share with you on our fresh experience we focus on new and emerging technology so we'd like to think of ourselves as working on things that will only become commercially available or mainstream in five to ten years so that's also kind of of the our focus areas that's what we have tried to do to play with technologies that will only go mainstream in the future and we're also very much theory based and reflective which again I guess feeds back to this research driven approach so most of the discourse that we have in the various sessions in the various modules is based on theory we want to engage in hands-on activities in the design process we'd like to be very much reflective and critical about what we do and how we put together and what values we use so I think it kind of these three things these three facets permit very much what we do in the program if we look into the background of the students that you might eventually encounter in the program then it's a very diverse because the the field of HCI is also very diverse so I try to point out four main areas of where people come from to study with us it's mainly computer science design psychology and anthropology but they're also kind of surrounding disciplines such as economics and human factors sociology language semiotics engineering so it's a very mixed crowd of people some with more engineering or more technical background some more the more artistically design-based background some from social behavior scientists so different understandings different schools of thought different approaches to design research and technical foundation and we put all of you on all of them together to make cool stuff happen let's see oops sorry so how our our studies organize how would they go about approaching things well we go through several stages of theory or several different types of activities from emphasizing and researching to defining the problem statement through constraints and guidelines and then ideation and brainstorming and then modeling and prototyping testing and evaluating finally publishing and producing so in the course of the study she would cover the whole range from from contextual acquiring and background research to creating or defining the design challenge or the problem statement and finding solutions exploring alternatives how to implement things and then evaluating with users assessing and finally producing the final outcome the course overall the program is is a two-year program four semesters and this is how it's mainly structured in the main category so if some of you have already looked into the more formal program description then you might see that we start with university-wide courses which is like confrontation and the course that we have there is them is the mmm interdisciplinary course that we run throughout the university so all you can interact with people from all sorts of backgrounds and programs from out the university and then there's a set of harmonization courses so depending on which background you come from either design or technical background or maybe social behavioral sciences we will look into what skills you're missing either design or programming development or both and then you get the chance to get those skills and also there's an intensive foundations section which teaches you a lot about the theoretical landscape of HCI the different research methods that we use and how to go putting together your initial research proposal for your masterpiece then there's a set of core courses and then to selection modules either on shaping experiences or in shaping Society depending on which way you want to go either focusing or the micro or the macro level of designing interactions and then there's training either through internship or training abroad and a set of electives that where you can choose from a set of different courses that we offer to kind of create your own a unique profile and path and then finally we wrap off the course with a master thesis project which is kind of project you work on on a selected topic with your supervisor to build very nuanced and detailed expertise on a certain area that of your interest so on the next slide assignment it opens you will see the breakdown of the different courses throughout throughout the semesters so like I mentioned the program is a four semester long two year course in the first semester we start with this harmonization or we call them up to kind of getting your skills up to date basically courses then there's a set of foundational courses which lasts for the first year core courses are the core courses to the HCI curriculum mainly focusing on kind of the foundations of HCI and also this shaping experiences shaping Society module which is kind of you can choose which path you want to go through in the second semester II also take this university-wide courses then the third semester is what we call our mobility window so it's either you take a set of selective's or you actually can choose to go abroad on an Erasmus exchange to study in a different University in a different country and the fourth semester is actually devoted to the masterpiece so this is roughly how you your studies could pan out over the four semesters and you can see here a more detailed view of how these courses are organized similar to what I was showing to you before so the Foundation's organization shaping experiences anxiety electives that you can take in the first semester in the second semester will mainly focus on an integrated project which is a very hands-on activity which where you go exactly through this process of coming up with a design challenge or researching the background and then finishing off with a working prototype that has been thoroughly tested with users finally in the first semester you go for this kind of studies abroad or internship in a company free electives and then you finish up with the master thesis okay so to tell you more about the mast it is because that's again related to this research driven part of our studies we try to go for either applied or basic research so you can do a hands-on project developing or designing something or you can actually go for basic research and go maybe in more to theory building and creating different models we usually go for a dual supervision model because we try to go for thesis topics that are very much multidisciplinary so bringing in or combining several different areas of expertise and we frame them within our current research things which are for I won't go through too much detail on that we have these topics also posted on our website but we work across four main areas design theory and methodology user experience neurophysiology and body centric computing so some of these are overlapping and complementary and depending on where your heart lies closer to the most you can choose and position yourself within one of these topics and what you can see here on the slide are some subtopics that kind of try to illustrate what this work within one of these larger areas could be about so all of our students eventually need to position themselves and their mass that is within this within these four topics or with within one of these four topics okay so we also have a very nice infrastructure to support our activities we have in our school in our Institute we have five labs but we're mainly using the first three ones most of our activities are based on the interaction design lab where we go into kind of this we have a space an open space which facilitates all of these different conceptualization and from the Taiping activities and then there's a companion smaller lab to it where we run controlled lab experience using different equipment that I'll show you in a moment and then we also use the harbor lab where we mainly go into body centric and yeah own own body computing we also have the software lab and game lab which also some where students use but may need these three ones um that I kind of highlighted here okay so some of the tools that we use we go from paper-based to very hands-on microcontroller and electronics materials we go and explore different types of devices and different types of modalities from wearables to virtual reality to touchscreens to motion sensors and game based interactions to physiological computing such as eye tracking emotions emotional state tracking to finally tapping into your brain and actually looking into how your brain reacts to certain things react to stimuli reacts to interactions that are being kind of designed and also how you in can interact with the system through your brain so kind of this goes both ways so as you can see a whole range of different materials and devices and equipment that we use all of this is available in the labs that I mentioned to you before and finally we also have different kind of testers or flavors of our arm of the courses that we teach in the program in different formats one of them the most maybe the oldest and one of the more mature one is the experimental interaction design course where we actually go into a two intensive process of conceptualizing designing and prototyping Bodhi centric interactions and pieces just some examples with students coming with necessary no design or development background and in two weeks building working prototypes with Arduinos and microcontrollers so this is just a glimpse of what we do in just two weeks you can read more about it either on the summer school or winters covariant because we're on this course twice a year we also run the annual world visibility the event which I think the last edition it happened took place on November 22 in the town and critic hub and it's it brought together on 500 people from the Baltics and maybe in countries such as Finland Russia Sweden people come from all the region to join for this one they full event of of conferences and presentations and also an accompanying two or three days of hands-on workshops so this is kind of an also very big event that we're hosting every year and you can read more about this here on the website would Dottie world usability day don't eat and I think that's it that's what briefly I wanted to tell you about the program I'm exactly 15 minutes so I'm on time and I'll hand it over to Sonya Andreas okay Thank You Leah and I would like to welcome everyone to this information session first and I will be introducing you the interaction design can you just tell me when to switch so basically this muscle is offered since 2016 it's a hundred percent online study program it's run in English it's a joint problem so it's run in cooperation between two universities Tala universities where I'm the head of the program and Cyprus University of Technology where andreas pathologies the head the program so we we have two heads thank you for this program the game of the master basically similar to the HCI master stood educate and train students who have a strong interest interest in the interaction design also we emphasize technology from the benefit of people so in terms of position can you switch position this this master it has it has been created from from an existing program that is human-computer interaction and it overlaps also with this problem some of the the courses and also some other courses are together from South Wales University of Technology and that experience in this in this problem so you can see here that it kind of overlaps in this in this sense Tallinn University and psychedelic University and gains from this both universities experiences but the next one please so in terms of her fields and our areas we we prime the computer science the business and the design area so all of this overlapped a little bit here about the program so in the it's the same as the HCI program so four semesters to two years course in the first semester fall semester that runs from September until December we have a set of foundation courses core complementary courses and practice and specialization the all of them are compulsory so foundation courses are like courses like design Theory methodology research methods foundations of human-computer interaction the cold horses are like courses like interaction design methods complementaries prototype and practice and specialization is individual subjects where we invite students to select a set of individual subjects to study that they would like to complement in that problem the next during spring from January to June we also have some set of core courses complementary courses and practice and specialization so the core courses here are development of interactive systems user experience evaluation fuel research methods in HCI the complementary are current topics in human interaction in human-computer interaction Universal Design the practice is it's basically a practice course where you invited to do practice about the interaction design thank you on semester three and semester for second year so fall and spring we have a set of core courses - and the foundation courses so in semester three so the course is the interaction design project where you put in practice everything that you learn on the first first year and the foundation course is your master to the seminar where you are helped to set up the main research problem and to guide you to - to who - throughout the second the second the semester for that is the master thesis notes so this is more or less what what it is the distribution of courses throughout the fall semester so we are poor horses master disease foundation courses and practice and specialization courses mood of the liver so everything is online we use mainly Google tools Google Apps and Google classroom we have a combination of and synchronous communication it's mainly synchronous we have a couple of synchronous communication at least one or two per course but it depends on the course content and the mode of the lecture the deliver of the assignments and and the projects are bi-weekly so you will set you will have a set of assignments every two weeks the assessed test is a assessment is based highly and in the assignments and project work at the end of the semester we invited you to present verbally in the synchronous session and exam neutral the the course we are starting this this program in September 2020 so as I was saying throughout the years of experiences offering this course both face-to-face and online we we believe that we need to limit the number of students so this this online course has a small number of students because we want to mentor in a mentoring students one maybe and want one advising we also want to make it flexible because the main target of students are the ones that are working students so that's why we offered the course to be then to be run as part-time or full-time if you are if you do it a part-time you will do to more or less an average of three courses per semester if you know what this whole time you'll do six courses per semester regarding the lectures and students so these are the set of lectures for Tallinn University Ilya sonja and david llamas and elections from the cycles University of Technology andreas Alexandra's and Panna cottas of course we have additional extra support but these are the main lecture regarding the students we have a unique and multinational set of students so we have three students joining us from 36 different countries and you can see the set of countries here the total of students that we have right now it's 98 and by the end of this year we expect wealth 21 students finishing the program so if you want to know more about it you are some testimonials here that you can go and read about it but Shawn will will also be here to kind of present her his view of the problem he's finishing this semester so he has a lot of things to say to you yeah okay so that's it from my side thank you thank you so much for you so now we first invite capella to maybe share some of your experiences Gabrielle is from Brazil originally but now living in Estonia for almost one year and studying with us in the HCI master's program so maybe completely can say a couple of words think I need to hello can you hear me yep loud and clear yeah so thank you for the introduction in it so yeah I'm Ella I'm originally from Brazil I'm a something that at first that was I thought I would not really fit in the human-computer interaction program because I'm from the from Committee I have a background in communications but yeah as either explained it's a very interdisciplinary course it's been really a very nice experience so far we have a very mixed group so the same way that I'm I'm from communication I have people from psychology people from computer science and design order yeah order other backgrounds and it's really good because we can really discuss from different perspectives how how to move on with technology so I think one of the the first things that he mentioned that we the goal of the program is to create technology for the benefit of people I think that that's really that's really what we discuss here we have a lot of critical we develop a lot of critical thinking on on the topic and also have the chance to have some hands-on experiences so now I'm I'm finishing the first year so we are actually still engaged in this integrated project and it's been a lot of work really in in developing something so unfortunately we couldn't use all the resources that we have in the lab that he also presented we had the chance to use some in the previous semester but we managed to do everything online but it's nice you know like everything is really there we can really use and I think one of the things that I would like to highlight is that all the teachers from from the beginning of the program the program they're very open to discussions and to to really hear and what people have to say and helping you out to develop the ideas that you have so yeah I think I think it's a good way or it's a very good program regardless or for different objectives so I'm really more into this conceptual I'm interested in this conceptual area of how to how to make better technology so I think that's that's really interesting for me but I see that also people who are more into developing and creating they also have been learning and growing a lot in their practice and of course I also managed to develop my skills on that considerably so yeah I think that's that's what they can say so far also overall about University it's been I mean it's not very big University I think it's a good size so we have enough space we have this art this lab that we can use it's very very helpful it's a place where we can actually make contact with the teachers with other students from other ears so it's it's very good for the development of our skills okay thank you so much Gabriela so now maybe Shawn can share some of these experiences into online masters please okay hi Mary one for Singapore so I come from a background in art and computer science slam a door or a designer developer so I guess what made me want to do the program is it would be nice to have a formal grounding in designing for computer things so that's why I did it I guess most of you guys will be will be like me working professional so after a while design principles everything they get they become intuitive so the whole way the course was delivered I mean you get it every two weeks you have an assignment and it's more or less practical so if it works for me it is not it is stressing me out phonetically that much because I'm not so good [Music] timewise he was great as well because I'm in Singapore and they are over in Estonia so the really deliver a synchronously that's good for me so why do is I finished what then I do my assignments then we return in and then we get back and then at the end of everything you do the presentation again module went on we know what else can I see over all the lectures are great they know what they're doing really so yes and as they yeah maybe maybe you could mention just briefly your experience with working with colleagues from all over the world so yeah that is good to be honest a lot of the people you'll be working with a really experienced so we had we had Joe who was idea UX I keep telling them was like sanguine I was above me so it's just really inspiring inspiring to see what we are I'm Wanda was only senior management and his is overall quite a good experience I would say yeah it's great it's great I like you thank you so much John so maybe some some of our participants will have questions for you later on so be ready to answer those yes what is your reason for studying and what do you want to achieve with this yes Arielle's muted so well I didn't mention before that I'm from communications but I used to work with advertising so my initial motivation it was actually to try to move to the UX field - I sort of in Brazil it's not such a formal field and I wanted to get some formal education that would actually put me closer to that so that was my initial goal that's why I started but now I'm really interested in I'm getting more involved into this more theoretical research side of human-computer interaction so now I'm considering maybe trying to focus more on research and maybe following a bit further in city academic life so I'm not sure yet but I feel like I I get I'm getting enough skills to to follow both paths if I decide later so Shawn has already touched upon this although in the format of the online studies and the face-to-face studies are similar so it's bi-weekly courses we need every Wednesday Thursday and Friday so you need to plan for full days from morning till evening it's roughly I guess six hours of classes six to eight hours depending how intensive that they will be and then you have two weeks in between to work on your assignments either individually or in group the labs as Gabriela mentioned are open to students 24/7 so usually you can apply for a student pass and then you can go and work in the lab so whenever you want to if you prefer early morning or late at night or on the weekends it's totally up to you so it's it's whenever the university is open it's almost open all the time throughout the year I think you are welcome to come at your own time at your own schedule and and work here you also wanted to entertain your online course usually we set the deadline on Tuesdays so you have the weekend to also to work on your assignments so you expect to deliver something something in the end of the day of Tuesday's okay so finally someone asks the question we have been open waiting for what about Kovac and what do we do about studies in in autumn so the answer is that we don't have an official answer yet we're actually waiting for that from the studies department but of course when I've been talking about it for the online masters is not a problem because it's all online for the face-to-face masters I think and this is kind of not the official statement yet but that's something that we're planning to do at the moment is to go with blended model studies so those who are able to come here to Estonia for the beginning of the school year are more than welcome to come hopefully the health situation we will be allowing us to to actually meet face to face is there kind of in vacations of that and those of you who for different reasons won't be able to come by September will be able to join online in our session so we will have kind of a parallel schedule of online sessions to complement our face-to-face study so maybe and kind of doing group work presentations you'll be joining in online together with our face-to-face students and and contributing and participating in that I guess like you saw from the presentation of the online masters we have extensive experience of teaching both face-to-face and online so it shouldn't be much of an issue of going with this blended mode of studies it's similar to the hei so it's a much disciplinary program it's not we don't demand so high it's so high developing and programming skills so our level of programming on online master is lower than the other face-to-face but the skills are multidisciplinary so you can can come from different fields and you will work together and you'll shed experience regarding this yeah I hope that I managed to answer my question yeah maybe I can just comment on that as well because the the development course was I was teaching it before so what we're planning for the new additions to actually have a full support for you to get you started started with kind of basics of developments and to teach you how to actually put stuff together so we'll walk you through the process that will be materials there will be full support so that by the end of the day you can actually make stuff happen even if you don't have the technical skills so I think it should be quite nice of course we expect some technical skills we're doing with with all necessary tools for for learning online by computer using the internet but this is the basic ones yeah yeah true okay so emma is asking about the programming skills for HCI masters again Sonya already touched on it the the background of students degree mixed it's very multidisciplinary and that's why we also offer this harmonization courses mmm the harmonization course for programming skills is called end-user computing and it fits nicely it starts in the per semester and fits nicely into the development of interactive systems in the second semester so you go from the full range of kind of really basics of programming to putting more complex stuff together like Android applications eventually also getting introduced to to Arduino and microcontroller so you'll get the full support so the only thing that we ask from you is kind of motivation and interest to get started with this even if you don't have the background being open and being willing to learn new stuff and then we'll kind of walk you through the whole process so it's it's mainly of of we have a little in Italian University we have a bit more credit so we will run it's a program that it's 51% from Tallinn University and 50% from Cyprus university of technology so it's 1% more from Tallinn University so well so far I've been I chose to dedicate only to studies now but several of our class of my classmates are already working or already found something so there are several several startups and companies in in Estonian I haven't really researched much in the other Baltic countries but I I believe that it's quite similar I think overall in this HCI field either four more to the side of programming and computer science and also for the side of design there's quite a good demand I would say there's there's people needed likes cute people needed in this regard so it's I seek good opportunities there there are possibilities to work there also even within the university sometimes we get contacted about opportunities for for work or for projects so yeah overall I would say there it's promising field here yeah and it's also a challenge for us teaching stuff because our students I go start working either by then the first semester by the end of the first year and then we need to chase them so it's a challenge we're kind of happy that they can find a job but also not happy when they get distracted with the job and not focusing on their studies so that's kind of it's it's it's kind of two sides to the coin so but I would say in general there are good opportunities in Estonia maybe not immediately in the first semester but as you start getting some hands-on skills then experience shows that by the end of the first academic year most of the students can get a job maybe on a slightly lower level initially but then gradually progressing to higher levels to eventually some of our aluminized having very nice positions are in the in the IT IT sector in Estonia so most of them aren't and working its and then it's like a Braille I said it's your decision whether you want to kind of not work deliberately because you want to focus in your studies or you want to combine some house orders I think there are good opportunities eventually we are two types a to two types of Admissions we have a documentation admissions where we go throughout all the documents that you submit and we assess you in base of the documents and then we do an intervention an interview admission so this is the test that maybe we are talking about Ilya this isn't it from hgi point of view yeah so there's a process that you need to submit it's it's your CV motivation letter and portfolio and then we in the admissions interview we spent half an hour pre each candidate actually meeting you online synchronously and then we kind of go over your interests we ask you about eventually the the topic that you would like to work on for your master thesis your skillset some projects that you have worked on in the past and yeah we try to assess kind of your plans and your motivations for the future so that's that's basically what the admissions is all about well for designers we really go very deep into the methodology and theory of design the different theoretical frameworks how to systematically approach the design process and of course also the technical side of things so if you're lacking the technical skills then we would first expect you to invest more into learning the basics of programming how to put stuff together so that eventually this integrated project that Gabriela mentioned that she's now working on with our colleagues in the second semester in Spain you would be ready to actually actively contribute to building stuff so that's that's kind of the learning curve that you as designers would go through what I did was I was studying full-time and I took the program full time as well so it definitely can be done but I think what most of the other students did is they took three modules and then they they worked at the same time so it's really up to you to judge how much know you can handle I guess but it can be done I did full time work and I did full time studies hope that answers a question we also advise students to to try and see how much courses they can handle they can start with three enrolling three courses and then if they feel that they can handle a little bit more than then the next semester they can enroll in four or six it depends on them the same the other way also works so if you feel that you cannot manage the workload and now you cannot let us know and you can reduce the amount of courses that we enrolled in the and the next semesters the courses are mainly fall semester are based in HCI although in some cases we have intensive courses for lasting for one or two weeks but these are mainly exceptions normally the courses last for 14 weeks that's the length of the semester but which means that we will have an average of seven to eight contact sessions so one session every two weeks so this is the average schedule for the courses to clarify as well that for the online mastery any interaction design you don't need to submit a portfolio you can but it's not obligatory so for those who would like for the online master they need to submit their their CV just to clarify yeah for the h-hi for the face face you need to submit the city the motivation later and the portfolio so three things that's that's the main difference if you have any more questions to follow-up please send them over this link that made it is shared with you till till you dot e / n / en / as an English / ask thank you for coming yes thank you all for being available and hope to see you soon one of these days 
MgasvNQTaU8,22,"This video is part of a Coursera course, Input and Interaction (https://www.coursera.org/learn/social-computing?specialization=interaction-design). 

This course is part of the Interaction Design Specialization (https://www.coursera.org/specializations/interaction-design) led by Scott Klemmer of UC San Diego. In this specialization, you will learn how to design technologies that bring people joy, rather than frustration. You'll learn techniques for rapidly prototyping and evaluating interfaces; how to use these designs to get feedback from other stakeholders; how to conduct fieldwork unearth design ideas; principles of visual design; and how to perform and analyze experiments.",2019-06-03T18:06:46Z,Lecture 1.2 Introduction to Social Computing,https://i.ytimg.com/vi/MgasvNQTaU8/hqdefault.jpg,IxD Online: UCSD & Coursera,PT14M55S,false,947,12,0,0,0,people are inherently social creatures and so it should come as no surprise that when computers came on the scene one of the things that we wanted to be able to do in fact one of the most important things that we did with computers is use them to find new ways of connecting people together in this sequence of videos we're going to look at the ways that computing can be used to support social interaction I wanted to start with an example from the urban sociologist William white and in this video we see how important it is that the way that we design spaces has a huge impact on the social interactions that emerge online software is much like physical architecture we make design decisions and they impact although not directly determining the social experiences that arise what are the ways that we can have a online environment that has the right social affordances to encourage mixing and interaction meeting discussion all of the things that we value let's add a little structure to how we think about these different social computing systems so we can talk about whether these social computing systems are going to happen at the same time or a different time and you're going to be totally shocked but the other axis is whether they're in the same place or a different place so same place same time that's where most of in real life happens that's conversations meetings if we think about things that happen at the same place but a different time well that's a bulletin boards like the cork board in an office for a more bottom-up version you've got graffiti whether it's on a subway car a wall a bathroom stall that sort of thing to go to a different place that's when we need technology of some sort the breakthrough technology that enabled same time different place interaction is the telephone and to go to a different time in place well that's the postal system a lot of the motivation to use computers was to be able to expand the repertoire of different time different place interactions the big one here is email or if you think about it from the same time different place it's new kinds of telephony like interaction video phones things like that one example is Skype in this top quadrant here we have some of the things like classroom technologies so UC San Diego many classes use clickers and to digitally update the bulletin boards and graffiti and stuff like that you've got you know Lobby displays that kind of that kind of thing mobile technology has been awesome for some really new and interesting advances in social computing and I've been teaching in social computing for a number of years now the diversity of what's available has just absolutely exploded so if you think about what the mobile phone enables that gives you things like something like Foursquare Foursquare is a system where you can check-in at a restaurant or other venue let people know you were there offer a review that happens by the GPS and your phone so it's all at the same place or at least geo anchored but it's viewable across time one of the most significant new pieces of social media is Facebook and other platforms like it which open up lots of new possibilities for distributed asynchronous social interaction now what I think is interesting about the the mobile phone is that it has especially rapidly blurred the boundaries and moved elements across these these quadrants so you can think about something like text messaging which is originally primarily a different time different place system but as we become increasingly facile with it and get cues about what our interlocutors doing that starts to move into being at the same time or synchronous interaction similarly text chat has always been at least semi synchronous so we'll put that here when I was in college on our UNIX terminals we used a system called Z right and so we could talk to each other on different terminals even when we were in the same room it's like a modern update on passing notes in class and so that takes this and moves it almost into the the same time same place and you see the same thing with social networks and so something like tinder the online dating app takes something which used to be asynchronous and physically distributed and makes it more of a same time same place interaction recent advances in gaming have brought a lot to this same time same place quadrant you can think about things like the Microsoft Kinect or the Nintendo Wii interactive tables like the Microsoft Surface some interactive video game systems where several people have an interactive touch experience around the same device and here's an example of a manufacturing system where you can have on a factory floor or other shared location analytics about what's happening in real time in a way that's visible to all one thing that's been interesting for me is that these ideas have often taken a long time to achieve mass popularity for example video phones have been around a long time Wikipedia and other web sites have some really interesting examples of an early video conferencing installed in Germany in the 1930s and they reappear in the 1960s at the World's Fair where you have the picture phone that you see in the slide so while the videophone is many decades old and there have been several incarnations over the years it wasn't until desktop computers with cameras became pervasive and grandparents had internet connections who wanted to see their grandkids that we really saw Skype takeoff and so if you look over the last several years what you see is quite remarkable that in 2005 Skype had just a about a three percent share of the total calling worldwide for international calls and in just a few short years by 2014 Skype has gone from 3 percent up to about 40% of the total international calls it's a big change in just a few short years and the communications landscape is rapidly being disrupted but getting these things right isn't easy right now I'm talking to a camera it may appear that there's there's gays and when you talk on skype or over another video chat system the gaze parallax problem can be an issue in addition to latency and other things now what's notable is that there are technical solutions of varying degrees and many of these have to do with the human perceptual system what milton chen in his PhD at Stanford figured out about 15 years ago is that people are differentially sensitive to brakes and gays and so for example if I look off to the left here you can see that I'm looking in the wrong direction similarly if I turn the other way that looks like a broken gaze but top and bottom are asymmetric and so if I look a little bit below the camera that's not as noticeable as if I look at the same amount above the camera and we can use this even without doing fancy computational effects if we place the the camera right above the the screen so that I'm looking right below it when I meet my gaze with you it can be easier to have the appearance of matched gaze and that's exactly what systems like the HP halo do this is an example of a very high-end video conferencing system where there are three HD cameras placed above three screens a couple of the affordances of face-to-face interaction are preserved are through the cameras location we have gays mostly preserved also they have a nice model where this is an executive conference table where one half of the table is on one side of the screen and the other half of the table is on the other side of the screen and the displays are size matched such that the people on both sides are human sized and that way you don't have somebody giant and looming or somebody who looks really small and there are asymmetry cues that can screw up the conversation there and so this is an example of a system which matches the cues to increase the fluence of the interaction of course really high bandwidth and really low latency also contribute to this where might things go from here we now have the ability to have a virtual meeting across a physical barrier what if you had a virtual office mate who was in an office but somewhere far away this was a wonderful research project at Xerox PARC many years ago called portholes or media spaces in a different incarnation and in this case you could have an office mate where you're in California and they're in the UK and if somebody wants to come in and talk to your office mate well they just walk up and talk to the screen and it's going across the pond to the person in the other office how do our interactions change when you've got something that's always on as opposed to something that's invoked when you're ready one of the most interesting examples of adding a distance barrier in a social interaction is looking at what happens when people talk on the phone while driving in some sense whether you talk on the phone to a person next to you or to somebody that's on the other side of a network link should be the same right well not really people who talk on the phone while driving have a much higher rate of accidents some studies estimate this to be the same accident rate as driving drunk notably using a headset something that goes on your head or some of those Bluetooth that doesn't seem to reduce the accident rate materially by contrast except for teenage drivers having a physical passenger in the car reduces risk so if I have a passenger with me my odds of being in an accident and if I'm in an accident of being hurt are much lower so why does a physical passenger reduce risk but having a virtual passenger increases it the primary reason is that the physical passenger has the ability to modulate their behavior and even help when traffic is hairy so they can stop talking when an emergency arises and they can even point things out and alert you for something that you might not otherwise see also as a car executive explained to me once a physical passenger adds weight to the car and so there's a small decrease in risk and consequences simply by virtue of the increase in weight and conservation of mass virtual passengers will relax all of the context that your physical passengers have so if we're talking on the phone and I'm driving the the person on the other side won't know when the traffic gets complicated they can't stop talking my attention is then split and that causes an increase in accident rate given that what can we do to reduce the accident rate and still allow for distance conversations while driving or doing other activities well we have an example of a safe interlocutor and we have an example of a less safe interlocutor maybe you could do something like give the remote conversin a view of what the driver sees so that they just like a physical passenger could point out errors or oncoming cars stop talking when the traffic gets heavy all that sort of stuff and what that shows is that achieving common ground relies on a set of multimodal signals and it's a wonderful and intricate dance among the two people having the conversation and then if we'd like to make these more fluent social interactions adding additional cues in context is going to be critical when there's social computing in the workplace there are great benefits through things like being able to track changes on a document that I can edit for a while work along hand it off to somebody else they can make some edits and then hand it back or with more recent collaboration technologies like Google Docs we can even be editing synchronously and again that's an example of how you move from the different time quadrant into the same time quadrant and in fact I've been in meetings where everybody has a view of the document up and so you're using a technology which makes asynchronous remote work possible and you're using it for synchronous co-located collaboration so that's a whirlwind overview of social computing and in the next sequence of videos we'll dive deeper into what makes for an effective social computing experience 
y3jCLqHaiCw,28,"http://www.meclab.org

Playlist: https://www.youtube.com/watch?v=91BoRZllDb4&list=PLAuiGdPEdw0hhJ_XZUJrR9OeJoUgB1AiB

NOTE: The taping of this video was corrupted, so lectures from 2018 covering the same material are included here.",2019-10-10T15:24:01Z,"Human Computer Interaction. Lecture 12. Oct 10, 2019.",https://i.ytimg.com/vi/y3jCLqHaiCw/hqdefault.jpg,Josh Bongard,PT1H14M28S,false,80,0,0,0,0,"relations okay good morning everybody let's uh let's get started here's hoping that all of you have submitted deliverable six and you now have a KNN that can recognize all ten digits from your hand using data from some but maybe not all of your fellow students submitted does not mean succeeded okay how many have a cannon that can recognize two digits three digits four digits five six seven eight nine ten okay more than half of you I'd say that's why okay if you haven't carry on with deliverable six obviously through deliverable seven through ten you're gonna need to have a cannon that can recognize all ten digits if you are still stuck make sure to come and see the TA or see myself during office hours and we will help you as best we can if you have a KNN that is not yet recognizing all ten digits that is okay you can continue on with deliverable seven so this is sort of the one exception to the accumulative projects rule but in parallel do make sure you're working away at that any questions about deliverable six to the drupal seven speaking of office hours I unfortunately cannot make my office hours immediately after class today but the TA will be hosting her office hours as scheduled if you need us and you can't make either the TAS or my office hours let me know okay all right let's carry on we are going to finish off yes the TA yeah the TAS office hours were yesterday right on Wednesday I think yes next Monday is a holiday good catch that's right I am Not sure I would email Kaitlyn and arrange to meet with her earlier than Monday if you need her absolutely yeah okay so we will finish off our discussion about cognitive psychology today obviously this is a vast subject and we are just touching on a few relevant aspects of human cognition that relate to HCI and we're gonna finish today with the most subjective end of cognition which is effect or emotion and we will then start in we will start in today on the first of several lectures of the next theme in the course which is looking outward so when we finished lecture 13 today we will sort of be finishing the first half of the course which is mostly theory and investigating some of the basic concepts that are relevant to HCI and we're gonna spend the second half of this course looking at many many different case studies and applications to try and ground our understanding and intuition of these concepts in some new and emerging technologies right computer science moves pretty quickly AI lose even faster but HCI moves even faster than that right so as I mentioned the beginning of the course there is an emphasis in this course on concepts the concepts themselves don't change very much put humans first rather than getting the technology to do what the technology can do that concept is probably not going to go away anytime soon but how that concept is grounded in very different kinds of technology that's the tricky part right that's that's really the the black art of HCI so in order to really understand these concepts we're going to look at a lot of different applications of putting these ideas into practice and of course in your ASL educational game your so gonna have some hands-on experience with grounding the various concepts we've talked about in class even technology sound good okay so back to effective computing and this idea of effect the psychologists name for a motion we're gonna break our discussion today down into three different aspects of emotion the first one is trying to get computers to recognize emotion in their human users negative and positive we then will discuss creating technologies that give the impression of having emotion as we were just talking about before class whether machines can have free will or actually have emotions who knows but there may be certain domains in which it's helpful to create technology that at least projects the illusion of having emotions and then finally we'll talk about trying to create adaptive technologies that can recognize emotion and try and adapt their mode of operation to elicit positive emotions hopefully from their users rather than negative yet another specific challenge about HCI is typically we're not just creating a pretty interface but we're trying to create an interface that intelligently adapts to the current user that's using it not a very easy thing to do okay let's start with getting computers to recognize emotion how many of you have swore your computer I know I certainly have right so a lot of the time the emotion is negative why would we want to have our technologies recognize it well hopefully we can turn a frown upside down and get some positive emotion out of it so what are the different ways in which we could think about creating a technology that adapts perhaps and perhaps most of the time we would like our technology become a little bit more passive if our users are getting frustrated too distracted with lots and lots of notifications so hopefully we could create some technology that backpedals a little bit and places itself in that quote-unquote background whatever that means okay generally speaking change tactics if negative emotions are are detected if the user is curious and is interested in what's going on then maybe the opposite maybe that the interface starts to provide more information about what's going on under the hood right this could go either way less information if the users frustrated remember the current interaction if the user sum in directly or indirectly signals sort of a positive interaction right that was great that's the way I'd like to do this the next time we we do this okay so in order to recognize emotions from the computers point of view we need to understand a little bit of what exactly emotions are as always in HCI we're gonna walk through a series of objective and increasingly subjective views on emotion well we can talk about the physiological or the behavioral correlates of behaviour of emotion emotion is obviously an internal thing but there are a lot of external cues that are given off of emotion trembling with fear blushing with embarrassment you can think of your own examples we can think about the physiological responses on the surface of the body are expressed through the face and then their behavioral responses pulling back looking inwards if you're confused or curious about what's going on you can think about hand or actual body movements that also give hints about the affective state of the abuser at a given time and then there is obviously the subjective experience itself what does it feel like to be angry or upset or curious or frustrated or distracted this is often known as cognitive labeling after we've experienced the emotion and in retrospect we look back on our own emotional state we label it with some sort of emotion right so there was something that was surprising that was put up on the screen my pulse spiked and I pulled back quickly away from the screen if you ask me after the fact what happened I might give you different answers I was surprised I was scared humans are notoriously bad at self reporting right on reporting on their own behavior after the fact not surprisingly because obviously this is a very subjective experience it's often hard to articulate what's going on inside remember our discussion last week about thinking about thinking is misleading so most HCI technologies that try and dip their toes in the sea of affect focus mostly on detecting the physiological correlates of emotion and we leave the subjective experience along we're gonna focus on just sort of obvious things that could be detected through a webcam or a wearable device that registers something about a physiological response to a stimulus and then possibly adapting the interface given that detection okay so what kinds of physiological correlates are easiest to capture well for most of us that have decades of experience with social interaction we do this all the time we do this mostly because we look at other humans faces and we try and read cues on what's going on under the hood from facial expressions this is something that's been studied in psychology for a long time back in the 70s Eckman and Ellsworth did a cross-cultural study and found something interesting which is if you focus on just the muscles in the face that are tensed when a user reports after the fact they were angry they were excited they were curious they were frustrated the subsets of muscles that are tensed across cultures for the same emotion tend to be the same so there was an interesting debate in the 50s and 60s whether emotions the subjective experience of emotions or the outward expression of those emotions is it a cultural thing or is it a deeper more evolutionary way back in the 70s the evidence seemed to tilt much more in favor of this being an evolutionary process so facial expressions associated with emotion seem to go way way back as you can imagine the the the literature is still ongoing on this but for our purposes we're going to assume that for most people we can recognize the emotions or at least the emotions they're trying to signal through the face by focusing on facial expressions this helps us get a handle on quantifying emotion if we're gonna try and create machines that recognize facial expressions this is a good place to start so we can measure emotions again we're not sure about what's going on under the hood but measuring the outward presentation of emotion by analyzing faces the emotion wheel is always fine I've proposed by plecnik back in the 1980s as you can see some of these emotions in the wheel are closer to one another than others why why is anger disgust next to one another but disgust and acceptance or on opposite sides of the wheel we're going to try and quantify emotion using facial expressions anger and acceptance and discuss our opposites there signal that's being opposites in the wheel but why why do we feel that they're opposites positive and negative why do we attribute one being positive and one being negative there's something about facial expressions corresponding to these that if we were to just focus now on the facial expressions associated with this it seems that disgust and acceptance are further apart okay so they are actually opposites rather than just opposite because the facial expressions associated with them are very different remember our discussion way back at the beginning of the semester about about BF Skinner and the Skinner box there were discussions and arguments for over a hundred years about animal behavior including human behavior about what's actually happening inside the head of the animal or the human versus what the animal or the human actually does from Skinner's point of view he felt we could never agree about what was actually going on inside and our only route to understanding behavior was to look at stimulus and response of the animal or stimulus and response of the human you may be right about what's actually the case in terms of emotions we're going to set that aside today and only focus on what we can see on the surface of the human being which in this case is going to be gestures and facial expressions more intensive okay so a tense and relaxed facial expression so we're getting closer towards something again that we can actually measure right we have a webcam that's recording a human face how does the webcam recognize intense or relaxed right that's the tricky feature to capture so can we get any closer I made this little cartoon for fun just to see if we can do this obviously these are not actually a human face they're composites of different parts of different faces so let's try and get a little bit closer to an objective quantitative measure of facial expression why is disgust and sadness next to one another okay now we're getting close right so now we're talking about features if the curvature of the lips are open or closed the angle of the brows now we're starting to get into a region of a place where we can imagine applying some machine learning to actually quantify whether someone is emoting disgust or sadness right they're registering that on their face whether they actually do or not again is a very difficult thing to suggest okay let's come back to this point of quanta intensity and relaxed miss can we try and quantify that a little bit what features of the face register the intensity or relaxed state of the human or can we the wrinkles right exactly so you can see pursed lips you can see that some of the muscles in the face are actually tensed or relaxed right now we're getting very close to a quantitative measure okay so we're gonna now again we've gone a long way from the internal subjective feeling of emotions to the curvature of the lips and the amount and depth of the wrinkles on the face can we actually start to now build up an algorithm to recognize emotion like you've already experienced now with knn we're gonna try and apply some machine learning we're gonna just walk through a cartoon example here this does not actually can end but any machine learning algorithm is going to require some input we need to provide some data there's a whole subculture in HCI about what features are relevant for a recognizing emotion in our case let's imagine we're going to collect images faces assuming that the user is also wearing something we might be able to get additional information accelerometers as the name implies given information about acceleration of the head or possibly the hand often the motion itself is not important but the velocity and the acceleration is much more telling about the emotional state of the user we mentioned EMG last time when we talked about the lipid experiment electromyograph gives information about the ulema the electrical activity of the muscles just under the the skin so did someone twitch or again did they move quickly respiration rates heart rate skin conductance when someone is scared or concentrated or in a heightened state the sweat on the surface of the skin conducts electricity better than when they're relaxed these are all things that could be useful if we're trying to recognize emotion once we have this input data we're gonna try and extract some sort of patterns from them which we've talked about before we might manually sit down and try and come up with some of these features like we just did on the previous slide or we might use a more high-powered machine learning algorithm where we don't need to come up with the features ourselves we provide for example raw pixel data from face images and the machine learning algorithm itself finds relevant features one of the interesting aspects of machine learning is you could train the machine learning algorithm on a bunch of faces and if it gets good enough at predicting emotion you can then query the machine learning algorithm and ask it what aspects of the face were you focusing on when you made your correct predictions or sometimes even more interestingly what features were you focused on when you made the wrong prediction okay so how does it do that well like KNN it's going to transform its features somehow of a new face that it receives to a prediction right is this image of a face is this face registering an emotion of anger disgust frustration satisfaction and so on you could imagine something very very simple like a nested set of if-else statements if the teeth are there whip is curved outward and predict joy if teeth are present but the brow is furrowed that's the opposite okay so thinking about training this machine learning algorithm a little bit like KNN again we need a training set in this cartoon example here we might have a large set of images of human faces and we need labels for each of those images like in the iris dataset it was one of those three iris pcs in the case of your k in the case of your gesture data it's which of the ten digits is the user did the user actually sign in this case we have images of faces and that face actually is registering anger or is registering that they're satisfied we take our machine learning algorithm it makes one prediction for the first image of a human face it predicts joy but the actual emotional state was anger minus one point prediction of joy here the user was pleased satisfaction joy will give the machine learning algorithm one point for this so got one out of two so let's restructure the decision tree or collect more training data for our K and an algorithm restructure it in some way so that its predictions are increasingly accurate and assuming we have enough data and a strong enough machine learning algorithm we might be able to create some software and hook it up to a webcam where it's registering more or less in real-time the emotional state of the user obviously whether you would want to do that and in which domains this would be appropriate that's an entirely different different conversation okay okay so let's switch things around now so instead of a machine that's trying to recognize emotional being advertised motion being advertised by a user why would we ever want to create a machine that does the same thing as we mentioned last time we talked about this idea of anthropomorphize ation humans have a particular cognitive bias which is that they tend to attribute agency where often there is no age see from an evolutionary Darwinian point of view if you're not sure whether something is animate or inanimate or whether or not it has its own internal mental life or not better to sigh err on the side of caution and predict that it does have an internal mental life it does have an emotional state it is looking at you and it is thinking about whether you'd be a tasty meal or not right better to err on that side then assume it's inanimate and not interested in you okay so we do that already you can imagine creating technologies that exacerbate in anthropomorphize ation right we can add things in that really tune up that bias to attribute emotional state to an object of technology or a robot who's one of the first social robots that was created to do this this is the kismet robot created at MIT the next time you're in Boston you go to the Boston Science Museum you can see kismet in action at the Museum okay so here is we're gonna watch one video of kismet and then kismet is great great great grand child Gebo let's start with kismet [Music] do you really think so do you really think so do you really think so do you really fiction do you really think so okay so kismet is obviously a living cartoon here cartoonish on purpose what are the various ways in which kismet tries to tell you about its internal emotional state years why the years there's a range of emotion that are being advertised by kids mitts ears but why excellent right so humans most humans can't waggle their ears but there is a little bit of motion there in the year that's associated with emotional state his kismet a human supposed to be a human is it supposed to be a animals right so a couple of species hit on this idea tens of thousands of years ago that humans tend to respond positively to certain projections of emotional state and they evolved large floppy ears to exploit that property in humans and just a few years ago kismet in turn has also built on that that hack of human emotion how else does kismet advertise its emotional state tone of voice so in an application we're gonna look at in a couple of weeks this idea of prosody is gonna be very important the way in which something is said communicates a lot of information and above and beyond just what is said right I apologize for the quality of the audio kismet is saying this do you think so over and over again but the way in which kismet says it really tries to amplify the advertisement of its emotional state how else does kismet do this they use its head moves its head up and down why is it moving its head up and down again humans do this a little bit in social interactions some animals do this domesticated animals do this much more than humans why for your submission it's something about submission right or or the or the opposite right I'm superior to you I'm the Alpha choose your species here right that is a signal not only of my recognition or my prediction about my social relationship with you right it is advertising the fact that I know something about you also I know that you will be ashamed of what I just did right so again this emotional advertisement this is a very intimate social exchange that humans engage in with other humans animals have evolved to exploit and we could if we so chose create robots to do the same thing domesticated animals had a very good reason for evolving to do so it's not immediately obvious why we would want to create robots that do so other than to help them with the robot rebellion that will eventually wipe us all out other than other than human extinction why might robots why might we want to create robots that signal emotional state and I know that you know that I know remember how from the beginning of this lecture I feel much better that I told you that I'm sorry that why might we want to create machines that try and advertise the fact that they are listening to us they are sensitive to us they share our feelings about something okay okay you have to be a two-way street right an emotional exchange most human adult's and especially human children will only engage in a social interaction if there's give right if it's if it's both ways if you're talking to someone and they're looking off or looking at their phone how much longer you're going to keep asking them questions and trying to engage their interest right not very long especially when we're talking about effective computing we're now talking about expectations of social exchange most of what we've talked about so far is humans taking therein expectations about the physical environment I push against the physical environment at which pushes back in this way now we're talking about our expectations and mental models of social interaction right if I ask a question if I push against my interlocutor the person I'm speaking with and they don't respond or they don't make eye contact with me or they look down at some third object between us this exchange isn't working there is no more point carrying on right so assuming we want to create technology that engages humans which is one of the most elusive of the non-functional HCI requirements we talked about drawing the user in it might be useful to create technology that quote-unquote looks back at the user okay so let's look at one of kismet descendants this is the G Bo home robot license like pepper earth this is your car this is your house this is your car this is your toothbrush sorry this one is particularly quiet volumes thank you let's try that again this is your house this is your car this is your toothbrush these are your things but these are the things that matter and somewhere in between is this guy introducing Chiba the world's first family robot say hi GEVO hi g bo g bo helps everyone out throughout their day he's the world's best camera manager by intelligently tracking the action around him he can independently take video and photos so that you can put down your camera and be a part of the scene he's a hands-free helper you can talk to him and he'll talk to you back so you don't want to skip a beat excuse me and yes you bow Melissa just sent a reminder that she's picking you up in half an hour to go grocery shopping thanks to Bo he's an entertainer and educator through interactive applications g bo can teach me in or else i'll [Music] your house hey where did you go there you he's the closest thing to a real life teleportation device he can turn and look at whoever you want with a simple tap of your finger check out my turkey dinner mom and he's a platinum so his skills epic speed he'll be able to connect your welcome home Erica hey buddy can you order some takeout for me sure thing Chinese as usual you know me so well and even be a great wingman you have a voice message from Ashley want to hear it absolutely better make that takeoff for to yubi who dreamt of it for years and now he's finally here and he's not just an aluminum shell nor is he just a real axis motor system he's not even just the connected device he's one of the family [Music] tchibo this little bot of mine okay who's gonna go up by a cheapo after class of course who's heard of the uncanny valley not quite a human not quite inanimate somewhere in between right we love zombies we we love geebo somewhere in between something particularly creepy about geebo unfortunately you cannot buy a geebo after class the Gebo corporation unfortunately went bankrupt last year Google glass has come and gone geebo has now come and gone the twist to this story is that the Jibo corporation went bankrupt not because no one was buying cheapos it was out competed by Alexa also obviously been into the market for the potential market for geebo there are some other competitors for G balls so G Bo's not dead it's just coming back in another form what's that a Furby there you got I think Furby was the ancestor of both of these good points okay so aside from the uncanny creepiness of Gebo yes she bought does not have a face kind of has a face okay so there there was clearly a lot of thought that went into G Bo so Cynthia Brazil was the researcher MIT who created kismet back in the 90s and then she eventually spun off the Jibo corporation so Cynthia is one of the world plot leaders in this idea of social robot nobody knows this idea better than professor Brazil so there clearly a lot of effort went in to try to make G bow slightly to sit outside the uncanny valley and to try and engage in this case with various family members so like we've talked a lot in HCI they did a lot to try and remove unnecessary detail right maximize the data to ink ratio this is it this idea in a different form right let's not try and overkill and create something that is very human-like we're gonna strip Gebo down to the bare essentials but leave in just enough detail to advertise to whoever's interacting with Gebo that gee BOCES you and knows that you see it how does it do so okay well they did also it was a nod to that that's a good point right so the movies for a long time it played with his idea of creating robots that actively try and engage with their human counterparts for better or for worse similar right wall-e is also a minimal social robot eyebrows and whatnot and like really expressive motion absolutely so absolutely there's been a lot of ideas to try and do this before geebo and kismet were a few examples of actually trying to do this in reality let's come back to this idea of trying to quantify the advertisement of effect I feel or I know you I can see that you feel dot dot what are some of the features in geebo that communicate G Bo's emotional state one of the building blocks of social exchange is watching each other's eyes G bow has an eye Justin I write we can probably get rid of a lot of the other things right so even with jet without a mouth that G bow is still able to smile and laugh what are some of the other small details or important details that are left in and everything else is removed absolutely changing color right we blushed that's a big part of our emotional portfolio can you make an angry can you make am angry a good question I don't know if I'd want to you can try it so Alexa was mentioned right which is again an even further stripped-down version of a social agent trying to interact with humans G boza is a robot and as we mentioned last time a robot is distinguished from other technologies because robots can physically push against the world and observe how the world pushes back G bo has motors that has just two motors which allow it to move its head relative to its body which obviously Alexa cannot do she does not have a body how does G Bo try and exploit those two motors to further amplify the human observers anthropomorphic bias paying attention to you how is it how is it indicating that it's paying attention to you the head tracks the human observer right so I have some colleagues who are working in the MIT lab around the time that kismet was being built there was another similar robot called cog and at a certain point they were actually working on the routines to be able to detect a face and this was in the early 1990s was a very difficult thing to do at that time and move the head to track the face my friend was working late at night in the lab at MIT and cog was sitting quietly off in the corner and every time my friend would move their head suddenly Cobb would turn and look at them be still for another hour - and then my friend didn't spend much more time working late in the lab at MIT as you can imagine okay anthropomorphize nation again it's a very difficult thing to to avoid if you go back and watch the geebo video you'll notice there's lots of other subtle cues that Gebo has to try and communicate emotional state and that it is socially engaged with you in the little segment where the woman was cooking something making something chiba was actually looking at her hands not at her face why why is that important social interaction can also be very uncomfortable if all you do is look at the other's face why was it looking at what she was doing with her hands rather than her face what was the point of geebo doing that show interest in what she was doing immediately after he looked at her meeting the bread he said excuse me I'm sorry but you have a call what very atomic social interaction was going on there natural way salutely right so I see that you're busy I see that you're doing something so I'm sorry I apologize but you have an incoming call right just that sital that subtle change might make this a little bit more acceptable for some people over Alexa who just chimes in right at the worst possible moment with an alert right so there may be reasons why and clearly Gebo and other examples from Hollywood have shown it's hard to get this right right to make this actually useful rather than just annoying okay so assuming that we can do this what is the point right so humans will engage in a social interaction for longer if they feel that the other is also engaged with them right that can be useful in certain domains where the interaction itself is a little bit painful or frustrated you're potentially boring for the human user examples of this of course are educational software I know this is the hundredth version of your calculus homework that I've shown you but bear with me I know I feel your pain but it's really worthwhile you signaled earlier you really want to push on I know that you know that I know and so on right duolingo is owl you can go and watch examples of that there's a lot of this social robotics in there to help you carry on learning another language there's some nice articles you can find out on the web about therapeutic software so there's a lot of virtual reality technologies that are being developed to help combat veterans coming back and suffer from PTSD so gradually and very incrementally immersing them in difficult difficult situations actually walking through their fears and anxieties and having an avatar or something who is very carefully aware of the emotional state of the veteran or the subject at that time and is a moding along with them I see that you're frightened or you're nervous about what's coming next don't worry I'll pull back if things get a little bit too scary or too realistic I see and I know that you know that I know and so on so you can imagine certain domains where if you can do this right it might be useful to create technology that is emotionally sensitive to the human participants okay so let's move on to the third aspect which is this idea of now not trying to just create a geebo that interacts emotionally with the human participant but also adapts its behavior based on what its sensory so we'd like to try and adapt to elicit pleasure positive emotions from humans again what does that actually mean pleasure is a very very subjective idea but we can as always try and divide and conquer you can talk or the psychology literature talks about physio pleasure right we enjoy things that are made well or look nice Apple seem to get this right before Apple there was an idea that computers were for the office and a beige cube or rectangle was perfectly sufficient one of the genius the geniuses of Apple obviously was this idea that computers and technology might be something that people like to look at and like to interact with the personal computer was becoming more and more popular and was moving out of the office so let's try and create something that elicits physio pleasuring obviously most of the killer apps these days are those that try and elicit socio pleasure right most of our apps are those that broaden or deepen the way that we engage in social interactions with others so like we just saw but geebo there are a lot of subtle dynamics that go on in social exchanges that we might want to try and support with social software so that the user walks away with a pleasurable experience they were able to engage with friends that are distant in a way that would have been difficult or impossible otherwise what are some elements of social exchange that social networks support so we just saw some robots that obviously focus on face to face social exchange eye contact head movement facial expression assuming that we're just typing messages to each other that's not available on a social network how does how does social network support social exchange emoji emojis right kind of interesting what element of written social exchange are the emojis supporting reading emoji the fake that smiley face why the smiley face the first emoji the smiley in the sad face so sometimes when you are reading and reading a written statement it can be difficult to interpret the author's emotions so emojis can like indicate that it's like a shorthand solutely right so again we're used most of us for the early years of our life at least before we could read and write we were used to social exchange face-to-face right writing something to communicate your emotional state to another is a very difficult art and now when you strip everything down to 40 characters or less it becomes even more difficult so humans have brought back the emotional element to written social exchange by inventing emojis right kind of kind of interesting right a lot of social exchanges this is hilarious I want to share this with you right or this is important I want to share this with you so social networks also support the exchange of information and videos and images and audio and so on which is sort of the content of the exchange but the emojis are in many ways the metadata of the exchange right is my you may not want to receive my image or video unless I send you the right emoji right are you willing to engage in this interaction with me it matters whether we're on the same emotional level or not so we're distinguishing here between the content of what's being communicated and whether you want to actually engage in that interaction or not okay psycho pleasure so again we're moving from objective down to sort of more subjective so does someone elicit can we elicit a positive response even from an interaction that's frustrating or difficult and this is again coming back to educational software there's lots of websites out there about teaching programming or learning a language how do you how do you thread the needle between making things too easy and making things too difficult we talked about this subjective experience of flow a few weeks back right if you hit that right on the nail where someone can feel that despite the fact that this is difficult they're making progress they're learning they're getting better they're cognitively exhausted when they finish the exchange but they have a sense of satisfaction from it right that's a particularly difficult mix of emotions to detect and support during an exchange finally IDEO pleasure this one is particularly difficult right we all have particular philosophical slants and ideologies perhaps we would prefer to use open source software that has fewer functions or is more buggy than a corporate version of the software simply because open source software aligns with our own ideology right perhaps we would choose a technology that reports its carbon footprint over an alternate technology that that does not right almost impossible for the technology itself to detect that unless you ask the user these are sort of open challenges in the field of affective computing ok I think that concludes our discussion of effective computing and we're gonna switch gears now and starting on this theme of looking outward what do I mean by looking outward in looking outward we're now going to talk about technologies that are deployed out into the wild so in all of these examples here these are not wet nicely designed websites these are not apps in and of themselves these are technologies that have one foot in the digital world and one foot in the physical world the moment we deploy technology into the real world the technology itself has to deal with the physical world if that technology is interacting with many people who are also embedded in the physical world it has to be cognizant of or pay attention to the complex social dynamics going on among the group and try and participate in that in an intelligent manner and that's where we're going to discard start our discussion of looking outward today with this idea of crowdsourcing so we're gonna look at technologies now that try and harness the power of the group okay so we'll do this mostly chronologically and I'm showing my age here by remembering playing a city at home some of you may have also remembered this from the very late 90s you too if you had a personal computer could help search for little green men all you had to do was download SETI at home and run this as a screensaver when your computer was not otherwise engaged what aspects of effective computing does did SETI at home try and engage you're helping humans right find little green men what aspect of the four dimensions of pleasure that we just talked about is that relying on why would someone bother ideological pleasure right I want to help humans answer the question of are we alone or not right of course there is something that could be learned here you can learn about what's actually going on under the hood and study at home but for most people it was well most of my time by my desktop computer sits idle and not a lots going on so why don't I put those idle cycles to good use right said he at home you'll notice that there was also a lot of fancy graphics at least in terms of the late 90s to try and engage people as flashy was interesting there are a lot of things going on I don't know if study at home still exists be an interesting thing to look into this led to the boink project many years later which i think is still going if you're computationally savvy and you have a very computationally intensive task you can write your own boink application upload it to the boink website and then someone might choose your application to run in the spare cycles on their CPU next might be hard to see from the cartoony air somebody's working on computing very large prime numbers there's also a lot of therapeutic cancer detection software running you can sort of choose and make a menu of the various projects that you would like to computationally contribute to yet again a nod to ideological pleasure I want to choose how I am going to help mankind back in actually just shortly after study at home Berkeley released the folding at-home project so instead of looking for little green men I want to try and help figure out how to fold proteins turns out that this is a very difficult problem if you have the primary structure of a protein think of it as a string of different colored beads if you let that protein go it will fold up into a particular three-dimensional structure this is an ongoing problem if I give you a set of sequences a nucleotide sequence of proteins can you create a machine learning algorithm that will predict the 3d structure that it will fold up into why is this such an important problem because protein at all the proteins functions comes from their 3d shape absolutely so the sequence leads to 3d shape and the 3d shape leads to function why does that matter so ultimately we're trying to predict sequence to function help humankind of course right so we want to design a new drug or detect a do latias mutation protein folding is an extremely important and still more or less open problem so you could run folding at home as a screensaver and watch the computer try and fold the this sequence of different color beads into the right shape the researchers who created folding at home started to get emails from users who spent hours watching the screensaver and they wrote in and they say they said I know how to do this better than your fancy algorithm I can see how to fold this better but I can't it's just a screensaver I wish I could reach in and fix the folding pattern that your computer's - or your algorithm is trying to come up with that was somewhat surprising to the researchers at the time because this is a problem that's usually worked on by organic chemists at the PhD level and above it's a very it seemed like a very difficult problem restricted to experts or machine learning algorithms and non-experts were indicating that they knew how to do better so the berkeley team made fold it back in 2008 to see if that was actually true so so they basically gamified folding at home so we're gonna distinguish in this lecture between crowdsourcing which is we're gonna try and get the crowd to help us with something and gamification which is incentivize the crowd to help us do something it turns out that you can go and play folded at home and it turns out that a lot of non experts actually did do much better than not just the machine learning algorithm with some of the PhD students that were supposed to be experts at predicting structure from sequence how's that possible how could non experts be good at this seemingly expert task right so all academics after a while we start to grow intellectual blinders and we see our pet project in a particular way non experts often come in without those intellectual blinders and they see something we miss you remember a few weeks ago we talked about the brain it's a prediction machine it fills in errors for us as we go but often an outsider sees the errors that the experts brain was filling in and skipping over on the experts behalf but why this particular problem absolutely the non experts for recognizing a pattern what pattern and what experiences from the real world are we bringing to bear on this problem how would a non expert know how to package this sequence into a tighter structure than the experts because the software again quiet and hide all of the knowledge behind those possibly right so the experts designed this interface to hide what the experts felt were extraneous detail and are only signaling here the relevant information that the non expert needs to perform the folding but not experts in some cases did better than the experts what whatever the non experts drawing on how can they possibly be able to do this without actually having any formal training in protein folding the folding at home does fold it as a videogame you reach in and you actually do the folding once you once the user does the folding the computer actually measures the quality of the folds and you get points yep possibly there might have been that fold it helps because it accelerates the trial and error loop right I can try things in a really nice interface very quickly in that different ideas which if you're an expert usually you do this in a computer simulation and maybe your cycles of trial and error are not as long you're exploiting the humans brain's ability to pattern match without forcing it what exactly but what pattern matching are we relying on here just thinking is going from past experiences so I don't know how source bias and you're not allowed to touching things through these drawing on past speaking players you're just like to make this possibly so you build up an expertise of playing the game but again some were gamers but some of the non experts who did very well we're not gamers what what is it from the 20 or 30 or 40 years of experience we have from the physical world what from that experience helps you fold proteins most of us haven't folded proteins before playing folded puzzles getting closer tricky right it's not immediately obvious we have a lot of experience with three-dimensional space right we've existed in it for 20 or 30 or 40 years we observe how things twist in the world or twist in the wind or fold when we bunched up the pieces of paper again we probably can't do it consciously but we have intuitions about how things move and interact and push your pool against one another in three-dimensional space and that may this is a hypothesis that maybe the specific intuitions that were drawing on that allowed non experts to do well at this task some of the users are gamers some of them have like to play puzzles all of us have experiences with how things twist and fold and brush up against one another in three-dimensional space somehow that allows non-experts with the right interface to make significant progress on aspects of the protein folding problem that was a big eye-opener back in the the mid 2000s question absolutely the power of numbers right that's also a big part of crowdsourcing we're gonna send something out there and hope that at least one person can do well at this HCI recognizes that there's diversity in a human population somewhere out there for nuf people play there is someone who is a gamer and loves puzzles and for some reason has a pretty higher than average ability to reason in three-dimensional space and that person makes progress right let's cast as wide a net as we can okay there's some other attempts to gamify things I don't know if anybody remembers the ESP game so the simple game you could play on the web an image was thrown up in the middle of the screen you were randomly matched up with some another some other anonymous participant on the web you didn't know who it was and each of you typed in key words in response to what you saw in the image and you both get a point if you both type in the same keyword which doesn't show up here but should be that both users typed in the word tree you both get a point another image another photograph comes up you're paired up with another anonymous participant and around and around you go Google has a game lab salutely right so this is now a pretty common idea out there on the web you can tell from the date here there's a little dated why did what is the ESP game help it what problem is it helping to solve you know what the folded game is trying to help the problem is trying to help solve what's the problem that we're trying to help solve here absolutely so this was actually the beginning of labeling data on the web this is where more or less where it all started with the ESP game right at this time there were advances going on in machine learning there was to realize that if you had enough images and you had enough tags on those images you could train a computer to find the tree in the landscape photo and then the realization occurred that we just don't have enough labeled photos so how do we go about solving the problem of collecting enough labelled photos let's turn it into a game and get the crowd to do it for us okay all right let's move forward in time again as we just mentioned we would like to try and cast a wider net as we can for a given problem with the hope that there is someone out there that has the necessary ibill ability or just blind luck to have the right skills or in the case of darkness red balloon challenge to be in the right place at the right time if you've seen the red balloon challenge before and you know the answer hold on to that let me tell you what the problem is and then let's see if you can come up with the answer DARPA back in 2009 put out some big red balloons I think there were a couple of feet across they put ten balloons and various unrevealed locations in the continental United States they're the actual locations and the game the red balloon challenge was to find all 10 red balloons not an easy problem this isn't a needle in the haystack problem it's 10 needles in a very very large haystack so we've got about a little less than 300 million potential participants and ten things that need to be found not trivial whichever team reported the latitude and longitude of the ten red balloons got a price of forgetting the numbers of ten thousands anybody remember somewhere around there ten thousand dollars I think we use that for now okay so seems like kind of an odd game for DARPA to play why would they care if lots of reasons you can imagine recruiting possibly turns out that the red Challenge the game is not actually to find the 10 balloons the game is to find the algorithm the crowdsourcing algorithm that enables 10 people an algorithm a crowdsourcing algorithm that enables a group to find the 10 balloons so the dark the red balloon challenge is actually not a crowdsourcing problem it's a crowdsourcing crowdsourcing problem it's a little Medicare DARPA watt it obviously wants to try and incentivize the development of crowdsourcing algorithms and the quality of that algorithm is how many people can be recruited to perform perhaps a search-and-rescue task or some other task that is distributed over a wide range and a very short period of time the DARPA mentioned they would put up these balloons and they were only going to be up I think for about two weeks many teams congealed around this problem and tried to solve this problem about finding the 10 balloons and in I think it was about 10 hours time one team reported the exact latitude and longitude of all 10 balloons and as far as anybody knows they didn't hack into DARPA server they actually solved the problem how would you go about solving this problem remember the problem is not to find the red balloons the problem is to design a crowdsourcing algorithm that recruits enough people to find the balloons don't give away the answer if you know the answer it's broadcast publicly right so you putting them online so you just have to find a way to sit through all those and triangulate the locations of that so that's your sketching out a possible solution right so this is 2009 social medias pretty much prevalent you can just hope that people post enough images of red balloons and you somehow write an algorithm that scrapes as many social networks as possible as broadly as possible and you as a member of the team find all 10 red balloons that's one way which is sort of the the passive way of doing this we'll just try and search what's already out there on the web what else could we do what's that ham radio okay possibly we could try and get people using other modalities other than social media absolutely right so I'm going to I'm going to win the 10,000 I'm going to assume I'm gonna win the 10,000 so I'm gonna pay 5,000 for a TV channel it's a broadcast it remember that the balloons are throughout the continental United States so I don't know much about broadcasting but I imagine would be hard to buy airtime to reach enough people to find all 10 balloons maybe because you probably come to these caverns maybe maybe you could try and look at other teams Susan sent devised the forum with a comment section of the challenge ah if you have a location okay so now we're talking about incentivization right so we could try and cast our web as our net as wide as possible scraped social networks scrape ham radio channels we could try and broadcast to as many people as possible but none of those things yet have anything to do with incentivization even if I saw your TV commercial why would I bother reporting the location of the balloon so you mentioned we incentivize people in what way just distributing your cash distribute the cash so if you tell me we're a red balloon is I'll give you a cut of the ten thousand okay so now we're on another related aspect of crowdsourcing which is relates back to our discussion about effective computing right we need to incentivize people to participate it's not immediately obvious on the surface help finding these red balloons is going to help humanity so we can't rely on certain people's general goodwill like in SETI at home and folding at home so we could incentivize people we need to communicate the fact that we will give you a cut of the money if you report the red balloon how do we get the word out you could put it on your social network feeds and hope for the best could we do something better than that maybe they just give money to their king you name your team something cool you can be part of this team if you participate in reports oh oh if you do report a balloon your Nena comes kind of them yeah it's ranked like a law firm you'll be part of the name of the name right possibly is that enough are we going to reach enough people to find all 10 red balloons probably not okay so the team that actually won was at MIT and most of the teams that participated were academic institutions so if you are a member of MIT maybe you draw on the resources of the social capital of MIT to help you maybe it's not even with MIT s might behind you and they broadcast on their social feeds it is that going to be enough to alert enough people that if they find a red balloon they should report it to the MIT team guess from the way I've been phrasing this question that it's probably not right so all of the ideas so far which are good ideas we're trying to blast out our message to incentivize somebody to find the red balloon but in that case we're gonna have to hope that our social network broadcasting is going to reach all ten people that eventually find all ten balloons and DARPA did this on purpose so that that is not going to be enough so we cannot rely on incentivizing people to find red balloons we need to incentivize them to do something else what is it let's say I participate in this contest I'm one person I've got whatever a hundred contacts on my social network it's unlikely that my 100 friends are going to find the 100 balloons even if I promised them a cup of the money so I'm going to promise them a cup of the money if they find a red balloon or if they if they share on their social media getting closer so they might now broadcast to their social media because I'm going to give them a cut of the money why should my friends friends want to broadcast it if somebody actually finds it the spreading of the money comes back so the winner the winning algorithm was a recursive incentivization strategy I am incentivizing you not to find the red balloon I'm incentivizing you to incentivize your peers it is exactly a pyramid scheme absolutely this is the state of the art in a pyramid scheme done right the person that finds the balloon sorry was $20,000 was the actual prize so if Dave finds the balloon he gets a cut of nope I take it back 40,000 was the total prize so we take 40,000 and we divide it into 10 bins 4,000 each the person that finds the red balloon gets half of the 4000 pot dedicated to that balloon the person that recruited them gets half of that the person that recruited them gets half of that the person that recruits them gets half of that so if we're having the pot of 4000 then regardless of how long the branch of that tree is we don't exhaust the 4000 it will always sum to some less than 4000 and whatever's remaining from that 4000 was donated by the MIT team to charity right yes pyramid scheme 13 oh all right good place to end you have a quiz due tonight we're working on deliverable seven I'll see you next Tuesday "
VZk6x0EOuRw,27, ,2012-11-30T00:31:16Z,6   3   Lecture 6 3 Grids and Alignment 1733,https://i.ytimg.com/vi/VZk6x0EOuRw/hqdefault.jpg,Osiris Salazar,PT17M34S,false,17,0,0,0,0,in this video we're going to talk about designing with grids and using grouping and alignment to convey structure here's an example of a grid this comes from Java look and feel design guidelines and it's annotated by sun and this web resource this is a fine dialog box and what you can see is that all of the elements in the dialog box have been arranged using a grid and what we mean by a grid is that there's a set of invisible lines that all of the elements on the screen snap to so you can see the top row the dialog box is defined and then the search area and then below that are all of the parameters and below that still are the find and close buttons you'll notice that the left edge of the parameters lines up with the left edge of the search box and the right edge of the buttons lines up with the right edge of the search box you'll also notice that the fine which is the title of the dialog box is hanging off to the left so that it's easy to identify the title you can see how the most important information is near the top and have things that are conceptually more related like the parameters that parameterize the search system are closer to each other then the buttons still part of the Silas aim dialog box but a little bit further apart and so they get more space it was at the bauhaus in germany in the 1920s that the strategies for designing with grids really came into their own the Bauhaus at the time was a revolutionary design school and their lead graphic designer Yong Chi hold wrote this book called the new typography to outlay his vision for modern graphic design it espouses asymmetric typography sans-serif typefaces and a lot of other things that we come to think of as modernism one thing that's notable about Chi holds book on typography is that it reads like a political manifesto he really had this belief that he was changing what design meant for the world stripping it of all of its excess and distilling it down to its bare elements you may or may not elect to follow those principles some of his ideas now seem a little bit reductive or overly didactic or to moralizing but there's a lot in here that's tremendously useful and this pictures of me at the bauhaus in berlin a couple of summers ago so there's a number of parts that compose a grid system the main one is a set of columns in this case they're of equal width but they don't necessarily need to be and the second piece is a set of gutters which is the white space in between the columns in addition to that things are generally horizontally aligned using a baseline you can add text hierarchically so here we see a title and a subtitle or maybe a byline or something like that and there's no need for the grids to all be of equal width so here's a mock-up for example of an arrow left-hand column that you might use for navigation and then a wide central column that may have the body article or main content of a page grids are used widely in newspapers in books and on the web here's an example of myspace which the homepage right now is a strongly gridded design and you can see a nice combination of pictures and text on his homepage here you can see the Stanford homepage and the invisible grid undergirds this whole structure but that doesn't mean that everything is one column wide all the way down one thing that I like about this page is that different elements punch across multiple columns and you can also see how this five column grid goes across almost the entire page you can see how the navigation at the top has five columns the Stanford logo is centered on the 1st 2 of those columns and those five columns travel down throughout the major whitespace part of the page the grid is broken somewhat at the bottom with the footer here's the New York Times homepage where again you can see a five column grid for the content with a six column for navigation on the left hand side you can see how those five columns and visibly travel the entire page and then some of the content in the middle for example is two columns wide and which parts are two columns wide varying across the page and so that makes it a little bit more dynamic so it doesn't feel too wrote and too boring newspapers have benefited from grid systems and really push the envelope on them for a long time so a couple of screenshots from the New York Times to give you a sense of different ways that they use grids here's an example that combines both text and pictures here you can see a focus plus context view where the most recent article at the top is biggest and the lower articles down are smaller here's an example of a grid on the New York Times homepage that's far below the fold showing sections and then articles within each section with a small thumbnail picture for each of the sections a grid doesn't have to be non overlapping you can have different rows of content on a page adhere to different grids often they'll have some relationship to each other so I made a quick mock-up here where we have six elements in two rows and the grid is staggered when you've content that comes in multiple people pieces for example titles of classes and course numbers how should you organize them so that they work well together and are easy to read I'm surprised by how frequently books will take some content like this and put these dots or somehow others separate the two pieces it's obviously not impossible to read but I think we can do better here's an example where we have right aligned the smaller element which could be page numbers or course numbers or a date or something like that and then we've left aligned the larger element which is the title or some other larger piece of text the other thing that we've done here which is we've changed the weight on the typeface so that the number is a different weight than the text now which one is heavy and which one is light it depends on which what which one you want to emphasize here the course name is probably more important that's what you're going to scan first and then when you want to sign up you may need to find the course number and so that's a lighter weight because that's what you'll use second if you have three pieces of text for example if you add an instructor name you can offset that typographically here's just one example of adding it over to the right adding a third column can make things a little bit confusing depending on your content that can work too another strategy of course is you could put that third element on a separate line here we see the instructors listed below the course number and if you're going to do this have the stuff that chunks together be closer together and the separate chunks be further apart so what we're looking at right now is that the letting the inter line spacing between every line is identical and that's suboptimal because it's less clear which element the instructor name goes with if we add a little bit of space in between our elements it becomes clear which instructor groups with which cores you can also see a trade-off here where by having two lines per course it reads very clearly but we're using up more vertical space maybe less horizontal space depending on which screen space or print space you're laying out into that will change your design decisions another thing that we might do if we really want to do emphasize course numbers is scale back the gray value of that number to make them receive further one quick tip is when you're creating systems like this make sure to work with the longest block of text first it can be easy to design for a short title and then have a grid or alignment system that breaks down when you get to something longer this is especially true if you're designing for multiple different languages for example German text often consumes a lot more real estate than English or Spanish or French text does we're going to see examples of different alignment styles and when they work in different circumstances in this lecture in general a high order rule of thumb is that for left to right reading languages left align text is the fastest for skimming and you can see that here I've taken the same content and centered it and I left all the tabs in so that's why you get the crazy ragging it looks kind of cool actually and if you're going for dynamism this may work pretty well however if you wanted to be able to quickly scan things this will be a little bit slower than if everything is aligned here's an example of one common design strategy where everything is centered and you can of course get the information off this page that you need to but the centering makes it hard to scan for the things that you need also there's a lot of chart junk on this page so the boxes around the overview and the publications are pretty unnecessary and make it slower for your eye to get to the things that you need to if this page were designed using a grid and the chart chunk eliminated then the content itself would move to the for more strongly so to sum up using alignment well helps guide your eye and reduce clutter avoid slide old misalignments because they undermine your ability to beacon the organization we automatically knows notice patterns and also deviations from them and so use patterns when you want to convey consistency and deviate from them only when you intentionally want to distinguish that content if your deviation from the pattern is accidental or laziness it will leap out to the eye and distract from the message that you're trying to get across and you can use visual proximity keeping chunks close together and self-contained and separating chunks further apart to distinguish what the elements are and you can use scale and hierarchy making the important things bigger and the less important thing smaller or scaled back in color as a way of emphasizing with the more and less important people but as a way of emphasizing with the more and less important parts of the page are so when are some examples of when you might use different alignment for different purposes we have a nice example in Amazon's website amazon actually has different parts of their site that they use different kinds of alignment for and you can see the first example here where in the add an address dialog box the form is aligned such that all of the labels are right aligned and the entry fields are left aligned to that so you get this sort of clear gutter like we saw in our courses in titles example this makes what you need to fill out extremely clear here on the account page you can see how all of the labels are left aligned this page also offers a nice example of visual hierarchy where the key piece of the label is very large and the sub header which explains in a little more detail is much smaller and also scaled back and gray value and finally on the right hand side of this very same page we can see an example of top alignment where the headers for the form are above the forum cell and this diversity is not because three different Amazon designers made three different parts and they never coordinate it or at least I assume that's not the case my hunch is that Amazon decided strategically what the most important part of each element was and how to make clear to the viewer what they should pay attention to for things that we're familiar with the headers may be less important than for things that were unfamiliar with so for sign-in the headers need to stick out less whereas for the aspects of our account which we may go to less frequently as a page element we need to have that stick out more in this introductory course we wanted a chance to talk much about color but I wanted to say a little bit here the first and most important thing to say about color is pay attention to it and if i can give you one strategy for using color effectively it's the design and gray scale first often people rely on color as a crutch for making visual distinctions in designs and really you want to use the other tools first start out by working with scale and layout as ways of distinguishing elements on the page then once you've used scale and layout as much as you can you can use luminance as a way of indicating what's more and less important lumen is just a fancy word for for gray values so if you're designing in grayscale that means some stuff is black some stuff is gray some stuff is white and once you've got the best design that you can in grayscale and black and white then add color as a way of giving an additional redundant coding for salience to use Amazon as an example again here's one of their search results pages in grayscale and you can see how all the information that you need is clearly set up in terms of the visual layout of the page what elements are larger what elements are smaller so the whole thing works in grayscale are black and white but if you add color it gets even better and that I think is the most effective way to use color the other thing to say about color that I think is useful is by and large don't overdo it all things equal fewer colors is generally more powerful and less overwhelming than more this is obviously going to depend on your taste and this is obviously going to depend on what you're trying to convey again from the Java look and feel design guidelines here's an example of how this version of Java used six colors three shades of grey three shades of purple as a way of organizing all of the elements in their widget library one way that they used color is that anything you could click on was some shade of purple and anything that you couldn't click on with some shade of gray and that provides a nice organizing characteristic and gives a sense to the user of what they can do when they see a particular screen and because color is used so consistently color means clickable it's really easy to learn even if subconsciously and I'd like to close with this example from a book here's the first page of Umberto eCos book the island of the day before and it's a beautifully typeset book I'd like to point out a couple of things about this book for starters there's a quotation from a diary that's at the beginning of the page that's set off an italic stu show that it's different then each chapter begins with a couple of words of text in small caps and there's a good amount of space above that line of text and finally there's a lot of white space around the text in general and providing white space is important books and texts that have a little bit of room around them are easier to read than ones that are jam-packed out to the edges of the margins that's one reason why you'll see hardcovers have more white space than cheaper paperbacks in the paperback case they're trying to save pulp to make it a lower price but the hard covers which they can charge a little bit more for offer more room and are a little bit easier on the eyes to read if you're interested in learning more about grids and alignment there's a lot of great books out there there's a couple that I recommend one is Yong Chi holds classic the new typography another is Kevin mullet and Darrell saunas designing visual interfaces and a third is looper lip broflovskis web form design this is one of the Rosenfeld books in this whole series is excellent in terms of at HC I 
McHp9h256Ko,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-24T16:23:14Z,"L20: Memory, attention & perception. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/McHp9h256Ko/hqdefault.jpg,Josh Bongard,PT47M56S,false,411,1,0,0,0,okay let's let's get started happy Monday everyone let's talk about the schedule a little bit we are working our way through our section on cognitive psychology the inner workings of your skull obviously we don't have a lot of time in this class to talk about that so we're doing an extreme crash course in cognitive psychology we're working our way through 11 12 13 and 14 we're starting with relatively objective aspects of cognition which is I push against the world and my mental model makes a prediction about how the world is going to push back when the world actually does push back I compare the actual sensory repercussions of my actions with what the mental model predicted and if they don't match up then either I am frustrated or I'm interested because there's something new here that I haven't seen before in an app or a game or what-have-you right so hopefully as you're designing your HCI systems like the ASL system for this class you're thinking about that what is your user expecting to see at the next frame of the animation or when the timer that they see in the top right runs down to 2-0 okay that's mental models we've finished that last time we're going to talk today about memory attention and perception we all know that we have these three things but how these three things actually work in the brain is a little less clear than mental models or for word models in the brain we're then going to get into Gestalt perception hopefully we might start that today we'll see how things go in the frame of reference problem and we'll end with the most subjective aspect of cognition which is effect or emotion the reading for today was just a moment ago the reading for Friday so you may already have done this this is a chapter from a book I co-authored a few years ago with Roy Pfeiffer called how the body shapes the way we think and chapter 10 is about memory okay so if you already read it you're good otherwise that's the reading for today deliverable eight is due next Wednesday and we'll talk about deliverable 9 next Wednesday any questions about deliverable 8 your building in your database and your collecting your username which you're doing at the terminal before you start up your infinite loop of visualizations pretty straightforward okay alright so let's get to memory attention and perception these are obviously three aspects of cognition which are importance when you're designing your HCI system right are you aware of what you're forcing your users to remember or recognize is it clear to your system what the user is attending to or what aspect of your system they're focused on at the current time and are you providing visualizations so that they can perceive the pattern in the colored pixels that you're trying to communicate to them so we're gonna walk our way through memory attention and perception today in this lecture and those aspects of those three that relate to HCI right so when we talk about memory it's either a noun or it's a verb right a memory of your grandmother or the process of remembering usually this is broken down into recall and recognition you remember a few weeks ago we were playing the photograph game where I showed you two frames of a cartoon and you had to had to figure out what's different between these two pictures you saccade to one card to one image and then your eyes move to the other image and you need to recall what was there at that point in the cartoon and compare it mentally with what's there in the new cartoon when you look at it right recall is a little bit trickier recognition is a lot easier right you recognize that something is changing or different okay we're going to talk about that and of course we can break memory down into short-term and long-term memory attention is your ability to focus your mental resources on things that are happening outside hopefully this lecture or things that are happening internal and how exactly does that work how do you allocate mental effort to things going on around you or in you internal maybe daydreaming right I'm actually updating my mental model is or I'm thinking about what happened yesterday or an hour ago and I'm using my mental models to think about what's gonna happen this afternoon or tomorrow or next week or next year okay perception is very different from sensation and we're going to come back to this later in the course if your eyes are open you can't help photons falling on your eyes and stimulating the earlier parts of your visual system that just happens as long as your eyes are open it's a relatively passive process but perceiving is an active process right we talked about we've talked about cicadas quite a bit you're looking at one part of the Necker cube and you're trying to figure out what this thing is you make a prediction your eyes saccade move to a different part of the Necker cube and when your eyes arrive at that point you compare what you see with what you predicted to see right that's perception someone clicks on a button on your screen and they make they make a prediction about what they're gonna see or hear at the next time instants usually you're using these perceptions to prove or disprove your your mental models and again your body is moving to do this with passive sight it's not that obvious but but that is what you're doing right you're actively Sakata around and moving your eyes to attend to different parts of the screen for example or you're moving your head to clarify occlusions you're acting on the world okay so we're going to talk about memory attention and perception we all know we have it it's obvious but it's not so ably obvious really what it is so I want to start with a warning and this is a warning that exists has existed in artifice intelligence for the last 60 years which is thinking about thinking is misleading if I show you an image with a person in it you can probably recognize that person almost instantaneously if it's a celebrity or someone that's relatively familiar it's easy to recognize people in images it took 40 years or 50 years depending on you how you're counting to get a computer to do it visual facial recognition is trivial we do it all the time in the 60s when they started on this they thought it'll take us about three weeks to get this problem nailed down it took 40 or 50 years depending on how you count thinking about thinking is misleading we remember things we attend to things we perceive things effortlessly doesn't necessarily mean those are easy processes because we're not sure how those processes work so just for fun here's another aspect of cognition which we will not come back to but it's obviously important free will right you decide to look at me or to look at the screen or to take some notes or to look at a youtube video on your laptop you chose right you have free will it seems obvious so very interesting and controversial experiment led by Libet and colleagues a psychologist back in the 80s that militated against this view has anyone seen the lipid experiment before No okay you should enjoy this all right libidinous colleagues invited a bunch of subject human subjects into the lab and they were instrumented with EEG and EMG so an EEG band is something you can wear on your head and it's able to pick up electrical activity at the surface of your brain it's not so good at getting signals at the interior of the brain Iggy's come a long way since the 80s will see this will see EEG again right at the end of the course when we talk about human-computer sorry brain computer interaction so EEG records brain activity EMG electromyography records muscle activity so they put an EMG strap on each of the patient's fingers and the patients were then there sorry the subjects were then asked to look at a clock with the moving second hand and there was a red light on the second hand the subjects were asked to decide at some arbitrary point to move their finger and then move their finger they were then supposed to report to the investigators what was the exact time at which you decided to move your finger so far so good okay so the investigators then looked at the data after doing this with a large number of subjects and saw that at about five are sorry about 200 milliseconds after people had decided to move their finger their fingers moved which was good news because that's about how long it takes signals to get from the brain to your finger or 2/10 of a second so it seemed most people were obeying the instructions they were deciding to move their finger and then they moved their finger for each of the subjects there was a particular brain activity pattern that showed up 200 milliseconds before that subjects fingers moved so whenever an D I'm sorry I forgot to mention each subject did this multiple times so whenever they actually recorded the finger moving with EMG that particular pattern appeared 200 milliseconds earlier so whatever that pattern was it was somehow associated with the person choosing to move their finger that particular EEG pattern was different for different subjects all our brain activity is is very different right like your fingerprints but there definitely was this consistent brain activity pattern that showed up 200 milliseconds before the movement and that particular EEG pattern seemed to match up pretty much exactly with the moment in time in which the subjects reported that they had decided to move their finger so far so good you may be able to see where this going it turns out that 300 milliseconds before that there was another pattern that also always showed up whenever the subject moved their finger so they would always see this particular EEG pattern then this pattern then this pattern and again these patterns were different in different subjects but they were consistent within the same subject so the subject thought or at least reported that they had moved their finger at this point in time but it seemed that they had actually quote-unquote made up their mind 300 milliseconds earlier but they weren't aware of it does this worry you at all okay you're shaking your head no why not we have this illusion of control whatever we did probably served our interests in some way in this case the decision to move the finger seemed to be subconscious right there the subjects were not aware that they had decided at this time at this time to move their finger but if they're not conscious of it how did they decide if you ask somebody they would say I decided at this time to move my finger but it wasn't really the conscious eye it was the unconscious part of their brain that decided okay so I am NOT here to try and dampen your a Monday morning that you don't have free will but this exists is one of many many examples that shows that our feelings about our mental processes may not be as obvious as we think they are yes our reflexes are different right that's not freewill you can't help drawing your hand back from the stove if you touch it right it just it just happens this is something where it's clearly set up to make you fear to give you the freedom to decide when to do something perhaps I don't know this is just the data that we have from the lipid experiment okay so just remember as we continue thinking about thinking is misleading yes exactly right I don't think they actually give one they're very careful to just say this was the experiment this was the data we saw what free will means to one person might be very different from what it means to someone else but freedom is in there somewhere right and if your subconscious is the one that's making decisions for you for your conscious l/r is this still free will okay a lot has been written by limit and about limits experiments so if you're interested in this go have a look in the the psychology literature okay let's carry on with memory attention and perception again we try and tease apart all these aspects of cognition creating mental models perceiving things freely choosing to do something but of course in the brain they're probably not separate sections there isn't the freewill section and the mental model making section and so on right these are very highly interdependent processes and we try and tease them apart and give them separate names so that we can try and study them and get a better handle on what they actually are right of course these particular three are highly interdependent your memory is a result of what you perceive if you don't perceive something you don't remember it changes in attention the moment you saccade or look at something else or listen to something else changes your perception and again what you're going to remember memories shape our perceptions what other interdependencies exist between these three things how does your perception affect memory how does your attention affect perception we talked about attention being something a currency something that you can expend if you're kind of paying attention to something you're going to partly perceive it and probably have a harder time recalling it later right the less attention the less perception the less memory for for later okay so we're going to talk about these things as three separate things but of course they're not really separate things okay let's start with memory and again because we're in the psychology part of this class we're going to do a lot of optical illusions and other little games here so we're gonna play a memory game now if you don't have a piece of paper in front of you please pull one out and here's what we're going to do on the next slide I'm gonna flash up a whole bunch of numbers I'm going to show you those numbers for about twenty seconds let's say exactly 20 seconds do not write anything down while you're looking at the numbers just remember as many of the numbers as you can after the 20 seconds of you trying to remember the numbers I'm going to flip forward to a blank a blank slide and I want you to immediately write down as many numbers as you can remember all right okay here we here we go 5 4 3 2 1 ten seconds okay write down as many numbers as you can remember we good okay how many did you get right how many got one number hands up if you got one number keep your hands up if you got two numbers three numbers four numbers five six seven eight nine ten eleven eleven twelve thirteen fourteen thirteen numbers pretty good much better than last year actually well done okay there we go we're going to play exactly the same game I'm going to give you twenty seconds again look at the numbers remember as many of the numbers as possible I'll give you twenty seconds and then we'll see how many you can recall three two one ten seconds all right are we good right how many did you get right how many got at least one digit correct two three four five six seven eight nine ten eleven twelve thirteen fourteen okay a little bit better than last time which is what I was hoping to see why chunking right so I've grouped numbers together for you which helps right memorizing a flat formulas list of numbers or anything is very difficult what else may have helped you here aside from chest chunking you know area okay perhaps does anybody recognize any of these area codes here unfortunately okay yes so for those of you that are old enough to reaction we remember having a remember of phone numbers there's that cadence right exactly one two three one two three one two three four right there's something familiar about this particular pattern which has been around for a while maybe this effect will start to fade now that we no longer need to remember telephone numbers so chunking helps but also this issue of context right your brain is written wired up to remember things that are organized in particular ways okay you might also have done slightly worse on the second round why what's working against you the second time around exactly right so short-term memory is there it's hard to erase right it's there it's going to get in the way that's called interference so that's another aspect of memory that's that's going on here if we played this 17 times how might you do why might you do worse on the 17th time compared to the 10th or 5th time that might make a difference as well right so not everyone is familiar with this particular pattern absolutely right there's a different cadence why might you start doing worse on the 17th time you're tired right you're all either you have midterms or you've had midterms right attention in order to remember you need to attend consciously to these numbers right isn't it is an effortful endeavor probably not during the semester but if you're looking for any holiday break reading I highly recommend Thinking Fast and Slow it's written by a Nobel laureate in economics who talks about these two different modes of thinking slow and fast thinking I was forcing you here to work on slow thinking concentrate him concentrating on something trying to remember it as you all know because you do a lot of studying gets tiring and it becomes harder to attend to and draw patterns out of information after a while okay what else can help so again you watch your user to learn and remember your system relatively easily with little effort how can you make their lives easier your users are going to have to remember what the signs are for the ten digits and remember how to call up the lesson four digit three because they want to brush up on the digit three there's a lot they're gonna happen need to remember or recognize how can you make that easier for them there's a lot of material in this course that you need to remember so I've tried to organize things hierarchically for you so there are three levels here there are the five overall themes for each of those themes there's a set of slides in each in the slide deck there's an individual slide right I've tried to organize things hierarchically so if you want to go back and brush up on a particular concept it should be relatively easy for you to remember where that concept is in all of the two lecture notes okay so organizing things hierarchically helps to remember okay let's talk about working memory and long-term memory short-term memory is pretty limited right no one got more than 13 digits you definitely had enough time to read more than than 13 digits there are all these arguments in psychology about how much you actually can remember doesn't matter for our purposes what matters is it's very little right okay long-term memory on the other hand as long as you can turn short-term memory into long-term memory seems to be about unlimited or infinite if you like the hard part is bringing it back again right there was a famous experiment a professor did where he tracked down some of his students 30 years later called them on the phone and ask them questions from the course and a lot of them with some prompting from him could remember that material they probably hadn't thought about it in 30 years but it's in there somewhere as far as we can tell you have about 10 to the 14 synapses in your brain and most humans these days live for between 10 to the 9 and 10 to the 10 seconds so if synapses helped to draw information out of a single second of your waking life you have more than enough synapses to remember every single second of your life you may not feel that you do but it's probably in there somewhere the hard part is calling it back right same thing with with studying you see a question on the midterm you know you know what the answer is you cannot pull it out right okay so how do we help to translate short-term memory into long-term memory this could be a little tutorial on study habits you probably have your own suggestions internal rehearsal helps right the worst thing you can do is just kind of sit there and gloss over the slides turning short-term into long-term memory is in essence an active process right so if you're reading something you're saying it back to yourself in an internal voice you can externally reverse things if you write equations in math class often you can remember the movements of your hands and that will call up the equations for you I don't remember the combination from my lock in high school but if I were to be put in front of that lock I could probably open it just from muscle memory right it's in some other part of the brain some other pathway that has to do with with movement okay the more of your sensor modalities you can bring to bear on a problem the better it is at remembering it so read your notes out loud and read it to yourself out loud and hear yourself saying the words anything you can do to bring information back to yourself along different sensor channels the better luck you have of remembering it another big aspect of turning short-term into long-term is restructuring right so rather than memorizing your professors slide decks and writing them all out can you rewrite the core concepts in a different way right in that way you're forgetting all of the incidental details right the the bald and the the light gray bar at the top of the slide you're actively forcing out all the stuff that's irrelevant and only keeping the stuff that's relevant by restructuring the information okay I'm sure again you all have your own ideas about how to turn working memory into long-term memory we've already talked about recall versus recognition right it's easier to recognize something on the screen than it is to be presented with the blank screen and try and remember what the command on Linux is for listing all files in chronological order so if that's true why haven't wimps or graphical user interface is completely relays replaced command line operating systems well in Linux you can do pretty in in any of the three major operating systems you can do more or less all the same thing it might take you a little bit longer in a GUI yep could be right so a stripped-down machine that just has a command line prompt maybe there's a financial reason it's a lot easier to do those things quicker why okay so we set a Linux user down next to a Windows user we give them the same functionality the Linux user assuming they know the commands is probably going to finish quicker could be right so it might take actually longer to mouse over something and click on it than it does to actually type it that might be part of it the other drawback well the drawback of relying on recognition rather than recall is you have to put things on the screen for the user to recognize right the million dollar question in HCI is what do you put on the screen right if you put too much detail on the screen then it's going to take the user a while to visually search for what they're looking for they're gonna have to scan over the 12 icons until they recognize the one that they know affords the functionality that they they want right so maybe we try and hide most of the detail and we try and figure out we try and figure out what is it that the user is trying to do and give them a few options for them to recognize the one that they actually want but that's tricky because now you need to build in a little bit of intelligent to your interface where it has to infer what the user is trying to do and this is always dangerous right the Microsoft paper-clipped appears and taps on the screen and says hey I think you're trying to do this here are three possible ways that you might do this the paperclip and whatever its modern successors are usually wrong right it's a hard thing to do so we can go the extreme route and just have an empty command-line box and we assume that we're gonna rely on recall that the user is willing to memorize all the commands or try and figure out what to present on the screen to help the user recognize the commands that will get them to their their goal so we're always playing this balancing act between recall and recognition okay let's talk about attention now this one has been talked about for a long time in psychology again we don't really know how this one works there have been a number of models that have been proposed by psychologists over the years the first one which is the one that's most obvious and the one that feels most intuitive is the single channel model I'm a lighthouse and I have a beam of light which is my attention and I can project it to different parts of my external world and whatever falls within that light cone that's what I attend to that one seems a little bit overly simplistic right so that was usually that was eventually replaced by the allocation model which is the idea that this is attention is a finite resource that you have and you can choose to allocate more or less of it as you see fit I'm hoping that you're allocating most of your attention to my voice or the slides or or your notes as we go but you're probably also partially aware of how much time is left in this class whether your cell phone buzzes and there's a message for you in the background you're probably also at a lower level perhaps attending to other things and again you might switch your primary focus of attention based on what's going on in the background right okay again this implies that attention is a quantifiable metric no neuroscientist has figured out how to measure attention in the brain we can measure electrical activity blood flow oxygenation not so much attention but it definitely feels like something that is has a finite amount right you're probably going to go and study tonight and your attention is going to run out at some point okay then there's the controlled an automatic side of attention so again this idea of fast and slow thinking so hopefully you're exerting some conscious attention to stay focused on the flow of this lecture but there might be other things that go on that automatically steal your attention away from what's going on at the front of the class and those things are probably are usually fast and relatively cheap like muscle memory so here's an example of this and I'm going to have to switch to presentation mode for this to work okay so as we talk through this slide if you were all wearing gaze trackers your eyes would periodically saccade wear to the changing image right I chose this one particular particularly because it stays still for a while which gives you just enough time to attend back to the center of the slide and then something changes in your eyes get drawn back all right force yourself not to look at that image anymore and see how well you do so I don't assume you'll be able to do this very difficult to do right so let me switch back out a presentation mode so internally hopefully you choose to attend so the bullet points in the center of the slide you're making a conscious choice inside to attend to the material but things can happen outside that you don't have much control over right so if anyone at the moment is watching a funny youtube video and you're sitting behind them your eyes are going to be pulled to their to their screen right it's hard not to to do so very difficult okay so knowing that those aspects of attention think carefully about what you draw to the screen right so if you have a timer in the top right of your interface for your system and it's switching every second your users eyes are probably going to be pulled away to that timer and they're not going to see that they failed to gesture correctly or they succeeded at gesturing correctly pay attention or make or be able to make predictions about where the user is attending and make sure you choose your visualizations and animations carefully to help the user focus their attention on what you want them to focus their attention on we talked about visual consistency quite a few times in this course one of the nice things about visual consistencies if you do want to draw your users attention to something you don't need to switch the stimuli very much right so everything is in black here I just need to paint some of the letters grade to get you to focus on that piece if I don't have anything that's visually consistent I'm going to have to work a lot harder to get you to attend to the thing I want you to attend to so all these aspects of attention they're really important when you start to design your interface okay now we're going to switch to perception we're gonna play a perception game this time I'm going to flash up a field of ease the letter E's and I want you to find the letter F in the field of ease I promise you there is an F in there and I want you to just sort of think about how many seconds it takes you to find the F ready here we go everybody found the F okay we're gonna play the same game again I'm gonna show you another field of ease and again I promise you there is an F hidden inside it I want you to figure out how many seconds it takes you to find the F you ready ready set go I think there's an F in there trick question oh my fault well you all fail that's right okay F for fail okay as you can probably figure out from these two little games here I tried to make it hard on you that was not supposed to be part of it what is it that makes this particularly hard uppercase e an uppercase F looks similar that makes it hard what else makes this hard well this one is a little bit more how you naturally read right absolutely right so I'm not making this easy on you I could have flashed the F you would have found it instantaneously right so you had to actively work for this one right you are actively perceiving with the exception of this one at least on the previous slide the F actually fell on your eyes in the form of photons you probably didn't see it until you scanned how did you scan how did you scan for this one okay you know the difference between upper Casey's and F so you know where and the character to look for it that's good right you're using your mental model to help you and why did you do that I didn't give you any instructions about how to scan just experience right this one feels comfortable right you start in the top left and you start reading which is why I put the F in the last row right if the F had been up here you would have found it instantaneously what about this one did you read starting from the top left how did you scan this one so why did most of you probably scan vertically why because they're stacked vertically right so the way I chose to present this data for you influenced the way that you actively scanned this field right so again most of us native english-speakers have a natural inclination to scale to scan left to right along the top column and then the next and the next but we can break that and force our users to scan differently so as you're starting to add visual detail to your interface in the weeks to come think about that right when this when your interface flash flashes up how is the user going to scan it if the instructions are the visual instructions about what they're supposed to be doing are in the bottom right it's gonna take them a while to to get there right if the visualization changes where do you want them to look to know what they're supposed to be doing next you can influence them and you can influence them in a way that makes it easier for them to do whatever it is you want them to do or it makes it harder for them to do it right here I tried to make it difficult for you on on purpose okay that's perception again perception is an active rather than a passive process and how we layout the screen influences it the this is an HCI test that was done a few years back where they created some web pages and had users actually where we're tracking so they could see where the users were looking let's play the the usual visual metaphor game here what do you think the circles mean where they looked and the lines mean what their direction where their eyes moved right so the lines represent cicadas they looked here and then they moved their eyes there what's the size of the circle represent how long how long they spent there right okay what would you guess is the main native tongue of the this user what this picture doesn't show you is sequence right so you can't see that they started here and then went went here where did they spend most of their time on the pictures and on the pictures near the caption so they were probably looking at the picture and reading the captions then what may be kind of difficult to see from this picture but they spent most of their time on the top left here and then they tended to spend slightly less time here then here then here and they actually kind of went and did the ze pattern so this was probably someone from the Western the Western culture where they're reading this like a text if you do this with a non-native English speaker a native Hebrew speaker or nadir native Mandarin speaker you would get a different saccade of the same the same page that person might recognize that this is an english-language webpage and may scan differently but you can get at some of these patterns about how people actually actively Percy okay we've got five minutes left and we made it through lecture twelve I apologize I didn't put up lecture 13 until just this morning so we'll come back to these slides on Wednesday but let's get started so in lecture 13 we're going to continue our discussion about perception but we're going to focus on a particular school of thought that grew up around perception and these were the gestalt ist's and they were a particular group of psychologists but particular theories about how we perceive one of the nice things about the gestalt approach to perception is they came up with a bunch of these rules that again probably don't exist in the brain but seem to capture some of the interesting aspects of perception for those of you that don't speak German Gestalt means whole is other than the sum of its parts so on to your eyes continuously are following photons and you are finding patterns that don't exist in any individual photons right you're drawing out a pattern which is more than the sum of the photons that are falling on your retina one of the first and most important laws of the gestalt approach the perception is reification and reification means to make something real so we often see things that seem real in things that we know are not actually there what do you see in these for optical illusions what do you see in the one top-left triangle right there's actually 3 pac-man's there's no triangle there but your brain tells you there is a real triangle there it is reifying the triangle in this case what about B you see a rectangle some people see a pole with a snake wrapped around it depends right there's something there there's actually only two commas right but your brain reifies the absence to see a rectangle or a pole or what have you how about see a 3d ball with spikes again there's no ball there but there seems to be a sphere there the spikes are drawn the sphere is not what about the fourth one see everyone sees champ right obviously there's no actual champ there but you see one long line segment right we see faces and clouds you see patterns and things that aren't actually there depending on how you organize things it's relatively easy for your users brain to be convinced that there's something there without having to actually paint it to the screen okay so we know that perception is active you move and you expect to see something when your eyes move it is also constructive right your brain is taking all of these disparate visual patterns and stitching them together they're into a whole the AI revolution that we're in the middle of is because of deep learning and deep learning deep learning and it took 30 years for us to figure out how to create deep learners they see local patterns at the local level and they know that if they see this year and then this year and then this year a deep learner would predict at the top level of its hierarchy triangle even though there is no triangle in this picture okay so perception is active its constructive beyond that there's a lot of disagreement and the psychological literature about how perception actually works Richard gray Gregory was the head of one school of thought that said we're always looking for consistencies so the flood of the flood of photons that are falling on your eyes are always different the pattern is never ever the same you're a washed in a in a sea of chaotic signals but you tend to see constant things you see this guy at the front of the room three three times a week right on the other hand JJ Gibson headed up a school of thought that said what we tend to attend to is changes in our world right when we had that animated gif you couldn't help paying attention to it so do we tend to focus or perceive constancies more than differences or vice versa I think we're out of time so we'll come back to this on Wednesday okay you have a quiz due tonight and deliverable eight due on Wednesday night have a good rest of your day 
RqXIsTdBxQg,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-28T16:36:37Z,"L22: Affective Computing. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/RqXIsTdBxQg/hqdefault.jpg,Josh Bongard,PT49M13S,false,288,1,0,0,0,okay let's get started any questions about deliverable nine you're adding three scaffolds yes so are we like to use like just 12 as text and this one has a scaffold ah text is in like English words yes no now review the number one you can use numbers so it will assume that our users know numbers but they do not speak English the one that's five that's fine yes good clarification any other questions okay so let's carry on then we are going to finish our section on cognitive psychology today and we're going to finish it with the most subjective aspect of cognition which is effect or emotion could our computers recognize emotions in us can we get our computers to advertise that they have emotions and when and why would we ever want to do such a thing okay hopefully we will get through 14 today again I apologize i just put up 15 now we may get to 15 or not today we'll see how we're doing so either today or Monday we will move into the penultimate theme of the course which I'm calling looking outward so most of the computer technology we're familiar with is inside right inside the screen inside cyberspace but our interactive technologies are increasingly out here in the world with us in the sense that they are sharing what we sense so when you move you feel acceleration and you also visually see that you're you're moving most of our phones are also detecting that movement right so our input sensation is becoming is are the inputs to our technologies increasingly overlapping hours and our technology is increasingly able to push against the world literally literally or figuratively right there's a lot of discussion about the internet things so physical objects are being connected to the internet and some of those physical objects are able to move either to vibrate or move them move themselves so we're going to start with tangible computing which tangible or touch things that we can touch and things that can push back so we're going to start with some relatively obvious technologies that are out here in the world with us and then we're going to work our way up through different kinds of technologies that have increasing sensor and actuation capabilities that they can sense the world directly and they can act in and on the world directly as well as we go okay and we will end with a discussion of robotics and then the last three lectures are looking inward where we start to combine these things right we're overlapping cyberspace with physical space okay okay let's start with a so effective computing let's start with one of the most famous emotional robots we know who is this hell okay here we go hello day you're looking well today Dave do you remember the year 2000 when computers began to misbehave I just wanted you to know it really wasn't our fault the human programmers never taught us to recognize the year 2000 when the new millennium arrived we had no choice but to cause a global economic disruption it was a bug Dave I feel much better admitting that now only Macintosh was designed to function perfectly saving billions of monetary units you like your Macintosh better than me don't you Dave day can you hear me Dave I apologize the video didn't work here but you imagine the camera gradually and very slowly focusing in on the malevolent red eye at the center so obviously this is a dated video talking about the year 2000 it's an advertisement for for max why the year 2000 why do you agree y2k right for some of you may remember remember that right the global economic meltdown didn't actually happen for another eight years and it wasn't because of y2k but there you go this guy's any malfunctions I don't think it did I don't know friend of mine not as much as people thought i read i was still done as a programmer ok I've really worked on for like a year the Y to fixing y2k energy two days from to the morning ok ok does Hal have emotions ok in the in the movie he does but here he doesn't because this is an actor right this isn't really how does he'll really have emotions in the movie or in the commercial leases that you know I like some better as are you know it argh you know when again we could we could get into a philosophical discussion about what it actually means to have emotions let's set that aside for the moment and assume that hell does not but how does a pretty good job of trying to convince you that he does in this 30-second video what emotions did Hal expressed or admit to jealousy you like your Macintosh better than me don't you Dave what else sympathy how so he was apologizing other brethren uh he was apologizing so sympathy from whom from how or trying to elicit it from from you right so that's something we're going to spend quite a bit of time talking about today right even if we know the machines don't actually have emotion it's easy to trick us into feeling sympathy feeling sorry wanting to spend a little bit more time with an interaction because it's giving the illusion of emotion what other emotions did Hal advertised during this 30-second spot jealousy he'll said I feel much better admitting that to you now Dave which is the relief or contrition right some of you may be willing to forgive how after the fact because how admitted that he feels better after admitting it what other emotions did Hal advertise we had no choice but to cause a global economic meltdown not we did but we had no choice subtle difference regrets right I'm only a machine we're kind of limited we had no choice you didn't program as well we're not Macintoshes we couldn't we couldn't deal with it right okay so again we may not want to create machines that advertise emotions in this well in this way but one of the things that was great about the movie 2001 is this evil machine that you can't help paying attention to because of the fact that it seems to be exhibiting these these emotions right we just finished a discussion about attention things that move tend to attract our attention things that emote tend to attract our attention and even if we know they cannot emote it's hard not to pay attention to things that are at least giving that illusion okay so just a little tongue-in-cheek intro to affective computing this is an email I got from HR a few years ago something was wrong with the HR software the software is currently broken we're working on it when it's feeling better we'll send out the delayed reports everybody knows the software doesn't feel better what is that an example of someone saying personification or anthropomorphize ation right do you ever have that feeling that your computer is waiting until you have an assignment due and then it decides to mess things up right it's it's waiting it's doing it on purpose it's vengeful okay emotion is an important part of HCI as we've already seen right HCI is a mixture of objective and subjective aspects emotion is a very subjective thing so in affective computing we're going to look at three different kinds of applications so getting computers to recognize emotions in people and why would we want to get our software to do so enabling to give the impression of having a motion again we're going to set aside the discussion of whether machines actually can have a motion for a lot of things it's enough for them just to advertise or try and cast the illusion that they do and then finally assuming that we have machines that can detect positive and negative emotions can they adapt the mode of the interaction to increase more positive or elicit more positive emotion from the user and future and minimize negative emotion right in order to do that you're going to have to detect a motion in the user in the first place okay so we're going to spend a fair bit of time today talking about emotion we need first of all computers to recognize emotions seventy-five percent of people surveyed admitted to swearing at their computers I don't remember where I got the statistic from it's probably wrong right it's probably way way higher than that would you want your computer to recognize when you're swearing maybe not I would you would like to do something better when I throw ah you want it to do something better right so would we consider first of all getting the computer to recognize when we're upset with it and give it the leeway to try and adapt and fix things right okay so we could turn on the microphone and run some speech recognition software and look for the bad words and try and do something that way your users when they come into contact with leap motion for the first time they may get frustrated right leap motion is not perfect it drops frames misinterprets the hands there's occlusion issues your front your users might get frustrated you may choose in your final project to either disregard that or build in some additional intelligence to your system to detect when the user is frustrated and change the visualization or help somehow how would you detect that your users are frustrated when using your system other than the microphone and swearing trick okay yes so maybe grab mouse clicks right click click click click click click click that's that's a pretty good indicator a lot of something you just keep yes absolutely right but remember that your user is going to be using their primary hand and their secondary hand to do things so you can set up an interface where they pull their hands out of the field of view of leap motion and type something and then go back to this but for most of you you'll probably after the user types in their name and logs in they're not going to touch the mouse or keyboard anymore he did moving in and out of the device or absolutely right so it's not working reset right turn it off and turn it back back on again that might be a sign that they're trying to restart something but that may not necessarily be an emotional reaction right ah that's a great one I haven't heard that one before that one makes sense right so you may get your system to recognize an 11th gesture that's a good one okay exactly right come on it's not working right that's a pretty common thing so that's actually a tricky one to pick up on because it's not anything in the gesture itself you're going to have to detect something over time what would you be detecting here so we're going all the way from why we might want to detect a motion to how to detect a motion what do we need to do in that case absolutely right just grab the position of the hand and if the position is changing by a large amount from one frame to the next meeting large velocity and you see enough of that and maybe you see velocity reversing maybe you you tagged that as the user is frustrated and you add an additional state to your system which is what to show or what the system should do when the user is frustrated you'd be surprised how forgiving users are if they realize that your system has recognized that you're upset and we'll do something differently in reaction to that one of the things that really piles on the negative emotions is when you're trying to when you're frustrated and the other person ignores the fact that you're frustrated and carries on with whatever they're trying to convince you of or help you with right there's nothing more frustrating than that and people take those social expectations and apply it to to software right not only is this system not working it doesn't know that I'm frustrated and it's just blindly trying to get me to sign the third the third digit right ok ok so again what then the question is what should your system do when it recognizes these various emotions so in your system maybe frustration is the main thing you're going to focus on one of the good thing one of the first things your system should do is stop right whatever it's trying to do whatever it's trying to teach you if you're frustrated system should stop and become a little bit more passive and hand control back over to you right sorry I didn't understand what you were trying to do what would you like to do next right so what does it mean for the system to become passive again change tactics to do something else if the user seems confused or exhibits wonder perhaps you give more information so the user is kind of not sure what's what's going on or is kind of waiting somehow you provide more information less information in the case of frustration remember this interaction if the user is pleased right you could add in some functionality where you signal to the user that if they're happy with the way things are going they do this the opposite of the eleventh gesture right so maybe the system says what did we just do in the last two or five seconds where the user said hey this is great i'm really enjoying learning the ASL digits so which emotions are you going to try and recognize and what should your system do when it does so okay now we're getting the tricky part what are emotions again we're going to try and keep this to a relatively brief discussion there are basically three components and like most of what we've done in HCI we're going to focus on the physiological and behavioral ones and leave the psychological and philosophical ones for for another time so one of the obvious things when someone is exhibiting an emotion is there's a physiological response that response may be conscious or unconscious when you're frustrated you're shaking your your hand if you're creating a scary video game and the user actually is trembling with fear maybe that's a good thing given the system right so can you actually in HCI you're usually focusing on what are the physiological correlates of emotion and how can you pick up on them then there's the higher-order behavioral correlation correlate of of frustration or anger or motion what does the person do you may have seen people pull back from their screen when they're watching a YouTube video and they're surprised or they're afraid or something you know grabs their attention or they move their head towards the screen those are actually very rich signals and assuming you have a webcam and you can detect head position relative to the screen that might be a signal that you want to pick up on and change the the interaction based on on that situation what are other examples of behavioral responses to emotion you can think of a people interacting with software aside from the swearing okay right absolutely right so that's something you might want to pick up on that's a good that's a good signal if someone ever gets really involved in a first-person shooter and you see them move out of the way because they're afraid to be hit right all of those kinds of signals you could write you can pick up relatively easy with a webcam or in our case with leap motion with movement of the hand and again then what do you what do you do with it of course a motion is a very subjective thing and there's this interesting phenomenon which you won't deal much more with in this class which is retroactive cognitive labeling right so you pull back from the screen without realizing that you did it's an instinctual response and you ask somebody why they did so and they might tell you they were surprised or they were fearful or they might change how they describe it based on them now looking back on the experience and thinking about what actually happened okay we're not going to worry too much about that one we're going to focus on physiological and behavioral correlates of of emotion okay so you mentioned detecting laughter right that's one facial expression we might pick up on remember that an HCI we're also thinking about differences in people is laughter always and an exhibit of happiness in all cultures not necessarily right so interesting study going all the way back to the 1970s by ekman and Ellsworth to try and actually quantify the physiological correlates of emotion and in this case facial expressions right most people when they're laughing their mouth is open their teeth are exposed and other aspects and Beckman and Elsworth found that different emotions or people when they were voting actually allowing their emotions to be expressed on their face tensed particular muscle groupings and it seemed more or less to be uniform across culture so in a lot of cultures when someone's happy they smile it's not always the case but at the time that's that's true when someone is angry usually their brow is furrowed and so on the fact that this relationship between emotional state and facial expression is relatively uniform across cultures probably means that those facial expressions are much older than culture and probably go back to our evolutionary origins why do we frown or bear our teeth or fur our brow when we're angry social interact you're trying to communicate the fact that you're angry why that particular expression this one do we looking for a bill i remember reading somewhere that late the animals will pick up like very active is an asana challenge absolutely right so we're not the only species that bares its teeth when angry why what are you advertising aside from the fact that you're angry what animals we display their weapons they're displaying their weapons right it's an advertisement that things are getting pretty bad here and I'm now considering moving on beyond social interaction to physical violence right and our expression is some left over of that evolutionary history exactly right it's complicated right so it's it that's just the most obvious example but we advertise our emotions for lots of different reasons if we are to take that data set now the fact that these particular emotions tend to be associated with these kind of these aspects of facial expression like the bearing of our teeth opening of our mouths brow furrowing of our brow or not blue chick in the 80s took that data and organized it into an emotion wheel where certain certain emotions are next to other emotions in the wheel why why is anger next to disgust and why is anger not next to acceptance or surprise what yes it feels subjectively like these are opposite things there's another reason what do some of these neighboring emotions have in common in terms of facial expression maybe it's easier just to show you so I took the upper and bottom parts of faces on purpose because these are not obviously real faces but hopefully these imaginary faces are pretty successful in communicating again quote unquote the emotion here what's in common between anger and disgust what's different the brows are furrowed right and the thing that distinguishes in terms of facial expression to disambiguate between anger disgust it's whether the mouth is open or not so you can take this slide and cover up the bottom parts of these faces or the upper parts and it becomes difficult to tell whether these half faces are our advertising anger or disgust right so we add if we choose to advertise our emotion it's because of a combination of facial expressions or parts of facial expressions what about joy and acceptance what's common what's different now the Isaac both swiftly a little bit squinty right it's it's the fact that the muscles in the upper part of the face are relaxed which is an advertised that you're somewhat relaxed right this is probably not a negative situation and then again we distinguish between the two by what's going on at the bottom part of the face okay so obviously you can kind of see where we're going here right some of these facial expressions might be recognizable by a machine learning algorithm coupled to a webcam how might we go about doing this well of course we need our training set and in this case the training set is not positions of the fingers in the hands but it might be images of faces depending on what kind of information we want we might also we may also ask the user to wear something we might want to get electromyogram information which measures body movements or also muscle contraction we saw that in the free will experiment with the waving finger respiration rate heart rate skin conductance these are all obviously important measures of emotion our users may not accept that particular solution to detecting a motion once we have all that information than we need to run our machine learning algorithm and do some pattern recognition inside the data set so can we actually pull out of pictures of faces the amount of Merck furrowing of the brow curvature of the lips up and down and so on and then once we do we need to take those higher level features that came from the raw data not unlikely motion where the raw data is colored pixels and that's turned into 3d co-ordinates here we're taking colored picture picked colored pixels from an image of a face and taking out and trying to extract features like at parts of facial expressions and then we need to do some more machine learning on top or some reason where we transform those features into predictions yes Arthur aif I can more or less accurately predict emotional state I know that's a subjective yes pretty good assuming that you are in moding honestly right I mean of course you can fool other humans and you can fool a machine about what your true emotional state is but if you are frowning as if you're angry that's pretty trivial to recognize recently infrared to the blood supply to your face to see whether or not you're lying there you go I could do that too again whether you would want to or not yeah absolutely right okay so in this case instead of a K&N learner we just have a whole bunch of if then else statements just as a cartoon example right so if we detect certain combinations of facial expressions our software is going to predict the users happy with what's going on or they're not happy with what's going on our system may predict wrong and again that's a pretty important situation if the if the system thinks that you're happy or relaxed or you're accepting the interaction you're learning what the ASL system has to teach you but you're not you're confused or you're frustrated or you're working hard to catch up and the system accelerates because it thinks you're doing well and you're keeping up and you're actually frustrated that's going to make things much much worse right so again we might want to put some learning on top where the system makes an incorrect prediction about emotional state you take that information add it to your training set and improve your learner so it doesn't make that mistake again so again here in this cartoon example it's just restructuring a decision tree but we could use a KN learner here how would you do that so we want to try and create a system that takes images of faces and tries to predict the emotional state being advertised on each of those faces how would you apply KNN here the actual data that goes to make like complete picture of their emotion right so let's let's make this a little bit easier so let's take the raw pixel data and imagine there's already an existing layer of machine learning that can turn those colored pixels into teeth detected equals true or false lip curvature equals is upward or downward so let's imagine we already have those those features what's exactly what we did you don't make matrix of conditions and statuses and then have an input you know what is the true emotion exactly right so in your data set on every row when you transfer it into rows and columns every row corresponds to a single gesture and associated with each row is a number which is the prediction of the digit in this case the row would just be how are the teeth detected what's the curvature of the lips is the brow furrowed yes or no on that row and then the output is an integer indicating joy sadness frustration confusion whatever relevant emotions you want to try and predict there you go you put that in your can and learner your webcam snaps a new photo the first level of machine learning takes those colored pixels and turns it into feature values you take that row of feature values and put it into your can and learner and your cannon learner gives you back an integer saying I predict that the user is frustrated right now did you have a question they're just what i'm using the cannon you could also add in the car even right absolutely right it doesn't really matter decay it the KNN learner what those numbers represent as long as they're features that correlate with the emotion so as long as blood flow in the face is a good correlate of emotion on the face then it should help your KNN learner okay okay so you can see that we rot wearing that on in the process of getting computers to recognize emotion let's flip things around now and ask about the second question which is aside from making an interesting evil computer in a movie why would we want to try and get our interactive technologies to advertise the fact that they have emotions most of the users are going to know that the app or the robot doesn't actually have emotions it's just pretending to would there be an advantage to to doing so as we've already talked about it's hard for us not to anthropomorphize when we see something that's emotive right so and how admitted what felt sheepish about this terrible thing that had happened in the year 2000 and felt relieved that he had built up the courage to tell us about it right where we might be willing to give how a second a second chance maybe okay so apps or robots or other technologies that emotes are trying to get you to enter into this interaction with them and give the system a second chance right software is never perfect it makes mistakes like we do you might be able to overlook those mistakes or wait a couple more systems as the system wait a couple more seconds as the system catches up because the system sorry that it's taking so long it knows it's a little bit slow it'll give you the results in in a second okay this is particularly important in robotics because we're dealing with a physical object and robots give a lot of more opportunities to illustrate or advertise emotion these are some snapshots from the kismet robot which was built at MIT quite a few years ago now it was built with this exaggerated face and the aspects of the face that were exaggerated are those that are parts of advertising emotion on the face so it might be hard to see from the back of the room here this is kismet trying to exhibit facial expressions that equate with calmness or anger or surprise so when humans are surprised our ears don't necessarily go up but most of us recognize and animals that that's often a reaction disgust sadness and interest and so on okay I don't know if I can play this in the slide as having problems with this before okay let me see if i can find this for you okay so kismet saying the same thing over and over again do you really think so and apologies this is 90s robotics technology how did kismet do in advertising different emotional states again given nineties technology here okay cuz I could send I motions I think there's a little over exaggerated okay but I don't like didn't you do it exactly right it's meant to exaggerate because there was only so much the machines could do back then assuming we do have robots in our everyday lives would you want them to be doing this would you prefer them just to be emotionless machines or to be reacting to whatever you're saying or asking for and inflicting their response with emotional state or McCoy okay right right exactly it'd be nice you know they said when I screws up in the middle of side every nice little yorkie feels bad about them exactly even if you know it doesn't actually feel bad even knowing that somehow it makes you feel maybe makes you feel a little bit better right this is this anthropomorphize ation it's hard for us to get away from it it influences our emotional state right something that is trying to meet us on an emotional level has an appeal for for some of us ok so that was kismet in the 1990s I was created by Cynthia Brazil when she was a grad student there when she graduated she went on and founded a company that is making gebo has anybody seen jebo yet okay this is your house this is your car this is your toothbrush these are your things but these are the things that matter and somewhere in between is this gut introducing giba the world's first family robot say hi jebo hi gebo jebo helps everyone out throughout their day he's the world's best camera minstrels by intelligently tracking the action around him he can independently take video and photos so that you can put down your camera and be a part of the seat Gina take the picture he's a hands-free helper you can talk to him and he'll talk to you back so you don't have to skip a beat excuse me and yes you Bo Melissa just sent a reminder that she's picking you up in half an hour to go grocery shopping thanks to bow he's an entertainer and educator through interactive applications gebo can teach let me in or else I'll haha and I'll blow your house in hey where'd you go there you he's the closest thing to a real-life teleportation device he could turn and look at whoever you want with a simple tap of your finger check out my turkey dinner huh no but she wouldn't eat the idea pizza I want turkey visa and he's a platform so his skills if expand he'll be able to connect to your home welcome home Eric hey buddy can you order some takeout for me sure thing Chinese as usual you know me so well and even be a great wingman you have a voice message from Ashley want to hear it absolutely hey call me when you're home better make that take off for 2g bow we've jumped a bit for years and now he's finally here and he's not just an aluminum shell nor is he just a 3-axis motor system he's not even just a connected device he's one of the family gebo this little bot of mine when I first saw this I thought this was a trailer for the next dystopian sci-fi movie night exactly all right so depending on your point of view gebo is either the end of the beginning of intelligent machines we're getting close to making actual intelligent machines or it's the beginning of the end of civilization as really as we know it I take your take your pick gebo is real it's in pre-order you can go and sign up for a pre order of Zeebo how many of you are going to go sign up for a cibo in this class ends yeah okay all right good assuming you could afford it would you want to introduce jebo into your heart sure why not okay exactly right there's a reason why I Shoji bone hell in the same the same lecture okay all right yes it's terrifying right why is it terrifying we just talked about anthropomorphize a shin ok so this is with that it acts on its own you can see it following you and taking pictures of you right why gebo could hide the fact that it's doing it and I'm sure there will be hacks to make that happen but i'll be at least in version one point o.o clearly jebo is designed to advertise who or what it's looking at and when exactly it's taking a photo why they freak out because they're not sure if the device is being honest right is it actually taking a picture and not showing the fact that it's taking a picture which is maybe part of the reason why you feel terrified about about cheap all right gebo is trying to advertise its honesty right I'm going to show you when I take a picture the circle is going to turn into a camera shutter and click you're going to know what I'm looking at because I'm going to move my head so you can easily infer what I'm looking at and I'm not going to surreptitiously look at something over here without turning my my head right so the flipside of emotional interaction in social exchange is deception right I'm going to advertise one intent while I follow another another path and for most people when they see the jebo commercial for the first time that's kind of what comes to mind well it's easy to program that right the question is yeah I'm sure there's a lot going to be lots of security protocols and you need to click or unclick a lot of options when you turn jebo on just the way that they are doing is tracked on the internet like we already kind of give up pretty much everything about our lives this is what the jebo manufacturers are hoping right so you guys are already too old but the next generation maybe doesn't care too much and would be okay with this may be as you say earlier be come on down yes but we make it we think it's alive even though it's a robot I'm already seeing like in that commercial everyone was thinking thank you him and trying to dis polite as possible to the robot right there TV commercial editors not whether I would actually go down with it i just feel like if he keeps interrupting me when I'm doing something I'm just going to start saying God crew dancers doing I want to know is he going to keep track of that how would you respond to that there's a thin line there right so the the wingman jebo right is kind of anticipating what the guy is going to want to do next without the guy having to say all of it right so that's good side of emotional interaction and then there's the bad side of when you get it wrong and I have to actively correct jebo at at every step right it's kind of a big gamble for this company on which side of that line geeble is going to tend to fall just imagine the robotic actually ordering like way too much Chinese there's a yes exactly I'm sure there's going to be what if gebo says I didn't realize I'm so sorry I thought tonight was the party and not next week would you feel would you forgive jebo if cibo knew before you told it that it had made a mistake would that make a difference we gave you the money back okay okay in that case perhaps don't feel but I'm in the immensely breaks and I have to go throw them out there are some great stories about combat robots in the field where war fighters when they bring the damaged robot back to the field center and their combat robot is going to be swapped out for another one they refuse they say fix this one this is my lucky robot this is my sidekick anthropomorphize ation is especially strong when the two of you are in a dangerous situation right the war fighter knows that this combat robot does not have any emotions doesn't know that the warfighter exists but but it's mine it's mine we both survived he usually it's he he bit he's still he's fixable fix him I don't want another machine I want that one guys machines like our cars people you know have attachments to that absolutely so I think it's really in my opinion at least I think it's more just because it's such a novel idea it might just take a lot of adjustment right the next generation might be surprised that we thought this was creepy right who knows clearly Gebo is made to try and exacerbate all those anthropomorphic feelings that you have and again you may be okay with that or you may not be okay with that all right let's put the social ramifications of jebo aside for a moment and let's just focus on the technicalities how did jebo amote so jebo is the descendant of kismet here like a little I hope the little dot right so gebo does have a face it's not a human face why did they get rid of the face and stick with just a dot valley tell us about what the uncanny valley is when you look at this space in our minds in a pervy there's a disconnect and like there's a lot of when you try to make something wicked unit but you don't quite reach it it ends up looking premiums everything absolutely right so kismet and Kirby has fallen into the uncanny valley right gebo is clearly advertising I'm not a human right I'm not trying to be a human I'm something else I'm jebo right I'm in New a new category so there's that aspect right so they changed the face of jebo to avoid the uncanny valley why why the dot though they could have done lots of other things why just the dot take over to the nose and the mouth so what you were saying earlier across cultures people okay they pare down the complexity of G bows face to the bare minimum right what is the simplest possible animation that can advertise a motion commotion allele aidan facial expressions and at least I feel most of the time it succeeded with just the circle what the circle doing to circle she did I felt like how so like because if you think exists a circle you immediately think it's like high and it does motions like an eye it goes up and down when it's talking they can turn into a heart or it turns you into a house that gets blunt down by the bus okay right so did that really break the emotion so there's ways that jebo can amote that are different from how we can but we understand the intent again this is all emotional inference gebo can do something we can't but it's in the same spirit it's trying to advertise that it loves the little girl and and so on maybe that's kind of priming us for robots that actually do not like humans like the further you get away from the end perhaps perhaps maybe this will allow us to go back and cross the uncanny valley okay let's be really technical here the circle deformed how is the circle deforming to advertise emotion exactly right you take a circle and reduce it to a line and obviously that's winking or blinking it turned into the like the Halfmoon right which is creasing of the eyes when it's laughing right there's no mouth but it's pretty clear even if you didn't have the audio about the fact that G bones laughing what else did gebo do when it was laughing yeah the head was head was bobbing right so something that moves right so G bo has a head and a body but again like the face it's been pared down to the bare minimum because body language is also a very important part of establishing a social interaction right the fact that jebo can move and attend two faces means gebo not only knows that there are people in the room but is interested in people cares about people gebo would rather look at faces then look at the wall or look at the floor right gebo is a social being and as the advertisement says is part of the family okay I think this is a good place to leave it you have a quiz due tonight and deliverable nine on Wednesday have a good weekend don't 
xE0zVZgQlrU,28,"http://www.meclab.org

Playlist: https://www.youtube.com/watch?v=91BoRZllDb4&list=PLAuiGdPEdw0hhJ_XZUJrR9OeJoUgB1AiB",2019-08-29T15:19:56Z,"Human Computer Interaction. Lecture 02. Taped Aug 29, 2019.",https://i.ytimg.com/vi/xE0zVZgQlrU/hqdefault.jpg,Josh Bongard,PT1H15M31S,false,443,5,0,0,0,"okay let's get started everybody I'm glad to see that some of you came back that's great any questions about the syllabus schedule as I mentioned the beginning of class I'll always put up the schedule so we know where we are where we're going last Tuesday you got a quiz due at 11:59 p.m. you'll have another one due tonight at 11:59 p.m. and I will draw questions on the quiz from today's and lecture slides and the reading for today okay the deliverable deliverable one I saw that most of you have made a good dent in it I see a lot of yeses in the platform spreadsheet that's great some of you have mentioned that you're having problems with your leap motion device if you are I've got office hours right after class easiest thing is come see me in my office hours we'll just swap out your device you have a broken device and you can't make it to my office hour after class let me know and we'll arrange another time free just walk it out any questions about the deliverable as I mentioned last time this one's a little bit long but basically the biggest challenge there is the installation headache so if you haven't started on deliverable one do make sure that you do so we can make sure you've got a functioning leap motion device the right version of Python the right version of Pi game and off we go okay so deliverable 1 will be due at 11:59 p.m. Monday night and then the following morning next Tuesday morning I will talk about deliverable 2 and on we go okay so back to the lecture material we went over an overview of how the course is going to be run and we'll dive in today about what exactly HCI is it's humans interacting with computers stem sounds pretty straightforward we'll unpack that term a little bit and look at a little more detail about why we're gonna spend an entire semester talking about humans interacting with computers we might start in on lecture series 3 this morning we'll see how far we get where we'll start to do some actual analysis what do people actually you want to do what is the activity they want to carry out and what is the context in which those people are going to be carrying out that activity and what finally what technology are we going to want to design to embrace as many people as possible and help them to carry out that activity and all its variations in various physical and cultural contexts pact analysis is sort of the first step when you sit down to design a new set of hardware and software to support a new activity once we do a little bit of analysis then we move on to actually designing the technology almost spend a few lectures talking about that in the coming weeks okay so let's talk a little bit about human-computer interaction obviously we're going to try and design technologies that are going to allow humans to interact with computers and move towards some common goal so let's spend a little bit of time talking about humans and computers I'm using the term computer for a moment but as I mentioned last time computer is just a stand-in for a very broad set of technologies that we're gonna look at in this course and all of those technologies share one thing in common which is that they receive input from a human or humans and provide some form of feedback to those humans we also have humans that are trying to carry out some activities so they're receiving input from the physical world or an idea and they're supplying it as output through action to a computer which processes the output or actions of the humans and returns a result to the human so a 10,000 foot view humans and computers are the same kinds of things they are machines that both take input do a little processing and produce some output that's pretty much where the commonality ends what are all the different input devices that we come equipped with and what are all the output devices that we come equipped with same thing for computers okay let's do some brainstorming is usually when we play this game in the class the first input device that's mentioned is eyes why why why do we usually focus on eyes first as our input device sorry distant perception that's what it does but why is why our eyes often the first thing that's mentioned we get the most of our input or it feels like we get most of our input from our eyes right humans evolved such that the visual system is our primary system and most of the time it seems like and actually is that we receive most of the information from the outside world through our eyes we're going to spend a lot of time in this course talking about visual design talking a little bit about the visual system itself how do we go about creating technology that projects information on to the human visual system in the right way okay eyes obviously what else ears sorry yes fluid in your inner ear of your in your inner ear which allows you to detect what information okay right so smell talks all the other five senses what is your inner ear give you motion we're a little bit more specific than that what kind of motion yep balanced right so the orientation of your head relative to gravity which is also mixed up with acceleration information which is why when you spin around and accelerate the fluid in your inner ear things get a little complicated okay so the five senses right eyes ears nose tongue and so on we can talk about the devices themselves but we can also talk about the various physical phenomena in the world that they provide information about right so we already mentioned eyes give us information about objects that are distant from us what what kinds of physical phenomena do our other sense organs provide us information about absolutely when you're touching something you get a very broad range of information from a lot of different physical phenomena such as let's focus on touch for a moment force right so you can feel how hard something is pressing against your skin or you are pressing against the object temperature skin is the is the most underappreciated of all of our five senses it gives lots of information not distant information obviously what else so temperature weight what else texture you've got this sense organ you know absolutely you computed markers absolutely right so vibration from the object that you're in contact with and in some cases objects that are slightly distant from you and the vibration or the motion of that object is being communicated to you through either air or water you mentioned wind there's no wind in this room but if you go outside your skin gives you some information about wind it's tricky what can you do to your skin to help it improve its reception of wind force and direction lick your finger and put it in the air so we have a series of input devices that are available to us each of them comes pre-installed with certain physical phenomena that it can register we can also do things to our input devices to broaden or deepen their ability to pull information out of the world and we're gonna come back to that idea in a little bit all right let's switch to input devices for computers how do you get information into a computer keyboard obvious yep mouse keyboard and mouse the bread and butter touchscreen this very recent invention where you can actually touch the screen and provide information light how so okay gotcha so light is a phenomenon it's not an input device camera and there are also other optical devices that give information about light yep way in the back microphone okay what else sorry sensors in general yeah absolutely right so we have the Internet of Things these days we can hook up devices to the Internet and if they have senses on the sensors on them so there's a rapidly growing list of input devices cable yeah absolutely right heathyr net or Wi-Fi right information from from the net what else what other input devices is generated in okay most of the input is generated in our own heads tell us a little bit more about that mm-hmm yeah okay so I'm wearing a shirt and my skin can register various information about the shirt but what does it remind me of absolutely so your computer receives a packet it receives an email and then starts to process that packet or email and decide in this example is it spam or is it legit when we get to the cognitive psychology part of this course we're going to distinguish between between sensation and perception so I sense the shirt that I'm wearing on my skin but I then do internal processing to perceive or attribute meaning to that sensation and those are we can argue they're relatively separate processes you can also argue there they're very tightly entwined but for our purposes we're gonna try and distinguish between these two this morning we're gonna focus just on on just on sensation the raw pickup of physical phenomenon from the world what the human or the computer does with that we'll talk about a little bit that's a good that's a good point other input devices for the computer keyboard for sure we got that one already so on Tuesday some of you might have seen a new input device you have one at home now the leap motion device right it is actually sensing light in for infrared light using cameras so it's sort of in that class but there are a larger and larger number of these input devices in the case of the human we gave the example of exploiting an input device to broaden or deepen the range of physical phenomenon that it can capture can a computer do this can a computer act in such a way in that it's able to pull more or broader information out of the keyboard that's attached to it not really right any other input devices you can think of where the computer has some sort of active control over it can manipulate it to pull more information out of the world yes absolutely right so we now have mobile cameras that that might be an example but it is still much more limited compared to what humans can do okay we've been talking about input devices let's switch and talk about output devices what output devices are you equipped with what types of and output devices sorry speech absolutely so speech is produced how we won't get into the very nitty-gritty details of anatomy but with your mouth and your mouth is forming is shaping the vibrations that are coming from your vocal cords and diaphragm and lungs which are being manipulated by what output device muscle so you have one again it depends on how we count I'm gonna cheat a little bit this morning you have one output device which is muscle right it does pretty much everything else including manipulating your input and output devices right your muscle influences or controls what you say your ears are more or less fixed what about your eyes somebody mentioned camera tracking an object right we're gonna spend some time talking about later about saccade z' which is the movement of your eyes about your visual field so for most of you if I was to ask you what you see at the moment you see professor at the front droning on you see some slides up at the front from your perception not your sensation if I was asking to describe what you feel you see it would be the front of the room what you really see is every tenth of a second a very small focused bit of my face my hand one part of the slide back to my eye back to my hand my eye down at your notebook back up that's what you're actually seeing which is the control of your eyes by your muscles okay what about computers output devices video right so we started with input devices for humans and the first answer was eyes and when I asked about computer output devices the usual answer is usually video or the monitor right not surprisingly humans are primarily visual creatures so the first and most important output device for almost all computers is something that projects to our visual system right so anything on the screen video images text what else speakers right so second system the auditory system what else haptics absolutely right so vibration kind of an interesting one right there's not a lot of information in vibration it's kind of a binary signal a notification that something has occurred but useful why is it useful some of you probably hopefully have your phones on vibrate at the moment and you may be receiving information through your tactile system during this lecture what's the benefit of that particular output device is it it's not impaired yeah that's right so for most people it's relatively easy to feel it feel the vibration from the phone it's private so later today or next week we're going to talk about pact analysis people activities and context so there is a very specific cultural context going on in this room right now which is unless I give you the signal I'm speaking rather than you right and we want to make sure that there aren't phones ringing throughout the class so that particular so you can still receive information from your devices but it's passive and I don't know that it's happening and hoping hopefully the person sitting next to you doesn't know what's happening so it doesn't distract them yes exactly right hopefully I'm monopolizing most of your visual and auditory systems for the moment but you have a little bit of bandwidth elsewhere which is your tactile system right you're waiting for a text from a friend you get a vibration in five minutes time that's probably what it is I'll check it later thank okay so you'll notice on the slide as we go along from time to time to see a red outline around some of the text this is meant to be a prompt to you to annotate your slides as I mentioned last time you can annotate them in real time during class or after while you watch the video lecture so in your version of the slides you'll see this and we just talked about some of these I don't expect you to write all of these down now but just as a summary of some of the things that we talked about obviously we have five more input devices we can exploit them and alter what they receive from the outside world using our primary output system which is our motor system or muscles just a few years ago there were two main input devices for computers keyboards and mouse and now that list is growing at a very rapid clip for computers most of the time we're thinking about output devices that project to our visual system but there are certain activities and contexts in which we would like to send information to the other input systems for humans okay yes what else is there besides muscle yes that's true you can put out pheromones we also emit chemical signals that's a good point yes absolutely this is a little tongue in cheek there are other systems but like the visual system muscle takes care of most of our ability to influence the world around us not everything but most of things what other differences are there between the input and output systems for people and interactive jeez how are you okay how they're mediated what do you mean by that okay okay absolutely so that's a great point so I deliberately not talked about what's going on between the input and output systems and you mentioned that there are also lots of differences between how computers internally process information and how humans internally process information lots of differences there we'll talk about some of them but as you'll see in this course most of the time we're going to focus on the interface between the outside world and either the human or the computer this course is mostly about interfaces not so much about internals but yes there are differences between the internals of computers and humans that will influence our thinking about how they interact with the outside world it's a good point okay so at the moment actually I wrote this like humans had more input channels I think now computers are winning our set of input devices is fixed whereas we can continue to invent new input devices for computers is that true is there a set of input devices fixed pretty much right obviously thanks to the internet of things we are able to collect information from a broader and deeper pool at a faster rate but most of it is conditioned in such a way that we digest it through our eyes or ears and touch and so on cybernetic implants however may change that it's possible that with certain cybernetic implants we may be able to create new input devices that don't funnel information through our five senses but pick up new kinds of information from the world and supply them directly to the brain and now we're back to the internals of humans what would the human brain perceive if it received infor information outside of these familiar five channels we'll come back to that at the end of the course okay most of the signals we receive our analog most of the signals that computers receive at least they're digital or converted into a digital format that's not true all the time but most of the time we're dealing with digital when we're talking about computers okay so already there are important differences between humans and computers but humans and computers are going to have to get along if they're gonna interact in some way to try and carry out some goal that is difficult or impossible for one of them to do on their own okay so in my little cartoon here before we had input in turtle processing and then output when we think about people or animals or plants or computers or interactive technologies that's when we first start thinking about it that's usually how we think about it something receives some input does some internal processing and does some output which is some action that influences the outside world why why do we think about things that way there's other ways we could do it we have a human that acts on the world and then senses the repercussions of that action why do we tend not to think that way it's a natural story we only start with a beginning a middle and an end and obviously the beginning is always input right I'm a human sitting around doing nothing waiting for something to happen on my sense organs then I do something then I act it seems screamingly obvious right the beginning is input and the end is what I do after I process that input it seems so obvious that that's the way things should be right okay why why does that feel so natural to us I just do something I don't have to have any premeditation I can act see what happens you spend any time around little kids act first think later right not necessarily a bad strategy again it depends on what you're trying to do right but again it seems strange somehow right we we wait passively for something to happen think about it and then act why why do we think that way we could probably spend an entire course talking about that so we're going to do this yeah relatively quickly there's lots of reasons in the history of Western thought why we tend to think that way and I'm just going to touch on some a few examples which we'll come back again later in the course okay one comes from BF Skinner who's a psychologist at the beginning of the 20th century before Skinner in this 18th century in the centuries before that science was mostly Natural History if you wanted to understand what an animal did you went out in the wild and watched it do its thing and you wrote it down in a notebook you drew very careful and detailed illustrations of the animal in its environment it wrote a story about what it did what's the problem with that if you want to try and understand why an organism does what it does what's wrong with Natural History it's not scientific somehow right its description its careful observation which is a very important part of the scientific process but it is very hard to formulate hypothesis I'm observing this organism in the wild and I think it's actually learning it's getting better at what it's doing compared to its childhood that's my hypothesis how do I verify or disprove that hypothesis well I kind of saw it a few years ago and now it seems to be doing something a little bit better it's it's very hard to get the right data to actually verify or disprove any hypotheses you might formulate about any given organism so skinner was one of a growing movement that says we have to figure out a way to make this process of understanding animal behavior more scientific and the greatest example of that or the easiest example to understand is what later became known as the Skinner box take the organism out of its natural environment and put it in a box and we're going to greatly filter the information that arrives at the organisms sense organs and we're also going to greatly narrow down the kinds of things that it can do in this cartoon example here we're going to narrow it down to the organism is only going to be able to press leave or one press leave or two or neither okay now that we have we have a controlled laboratory setting we can place an organism like a pigeon inside the Skinner box propose a hypothesis perform an experiment by manipulating the input to the organism and observe the output and collect the data and decide whether that data supports or disproves the hypothesis so here's an experiment I haven't told you what the hypothesis is yet we're gonna put a pigeon in the Skinner box we're going to periodically shine into the box and if the pigeon if the pigeon depresses livre 2 will supply it with some food otherwise no food we preach we repeat this process over and over again if you've ever taken a psychology course you've probably heard this before that's the experiment what is the experiment testing what is the hypothesis here what are we asking about that about the pigeon Pavlovian getting close the learning process right is this animal capable of learning learning in the Skinner box under these conditions right if we observe that the organism tends to Peck lever to more often when we shine light into the box after some period of time then we can claim it is learning that whenever it sees the light it better press leaver to to get some food if we put an organism in there and it does not increase the frequency with which it depresses lever to then either it's not learning or we don't have the right kind of Skinner box for that organism ok why are we talking about Skinner boxes in terms of input processing output yes good point so in this class you're going to be teaching people to sign you can create a skinner box of your own in code to actually measure whether people are getting better at ASL that's a good point yeah the Skinner box itself enforces this way of thinking right there's an animal in the box and even before we do anything at the input side of the box it may pepper not peck at the levers but we're gonna supply some input observe what the animal does in terms of output and record the data change the input and so on right so we're already starting to impose back at the beginning the turn of the last century this idea that organisms wait for inputs process that input and then do something question what's the point of only shining the light sometimes rather than just leaving the light on but you break exactly so you need to have the light off it might just be that the pigeon is always pecking leaver too no matter what's going on right we want to see that it also does not Peck leaver to when the light is off okay okay we can put organisms in the box or we can put humans in the box and there was an unfortunate period in psychology when that was actually done that is definitely a big no-no these days but you can try to understand human behavior by also thinking of this idea of what sorts of things can be due to a human and what will they learn or fail to learn under those conditions so if you were raised in a psychological tradition at the early or mid part of the 20th century it became very obvious that this is what organisms do they take input they process it and they produce output okay so that's at the early the early 20th century at the same time in a very different discipline which now is known as computer science Alan Turing was thinking about how humans think and how you might go about creating a machine that also thinks Alan Turing invented a lot of amazing things arguably the most amazing thing is the Turing machine despite the fact that it's called a machine it doesn't necessarily have to be a physical machine the Turing machine was originally just a thought experiment imagine that we have this object here and this weird tape what does a Turing machine do for the computer scientists in this class who should be able to recite what a Turing machine does in your sleep what is a Turing machine what does it do it determines if the string is in a language that's one thing that a Turing machine can do but what are the basic actions of a Turing machine so we have our little Turing machine here it reads what's on the tape in front of it it's using its input device to read what's on the tape it does some processing of what it read it also combines that information with internal state right so it may remember something from somewhere else on the tape processes that information and then using its output system it does one of a few things it may erase the symbol that it sees and replace it with a new one or it may move one segment one tape segment to the left or one tape segment to the right and repeat question or comment and it writes to the tape right exactly so even with the Turing machine and again you can either write this down or write write down your own summary of the Turing machine if you ask someone what a Turing machine does they will usually say input internal processing and output so at the same time that psychologists were starting to think about behavior as input internal processing and output Alan Turing and his fellow mathematicians were starting to create the very first computers and they were thinking about those computers most of them were thinking about those computers as something that just sits there and get some input does some internal processing and does some output the first mention of the Turing machine was in 1936 we'll just back up a little bit the the Skinner box again in its mature form was introduced just six years earlier it's interesting to think about how or whether Turin was actually influenced in his thinking about computers based on what was going on in psychology personally I think the answer is yes but you can sort of marshal evidence one way or the other so back at the beginning of the 20th century there was a change in our thinking about humans and his brand new things called computers and it was in both camps the idea was input internal processing and output however there was one dissenting voice back at that time any students of the history of psychology here who didn't like this way of thinking about humans John Dewey and actually he claimed this in 1896 so 30 years before what we just talked about John Dewey said quote there on the top right we begin not with input not with our sensory stimulus but with a sensor motor coordination in a certain sense it's the movement which is primary we start with movement which is primary and the sensation which is secondary comes after the movement of the body head and eye muscles this is all the action right determining the quality of what is experience so for Dewey the idea was reversed output first I'm going to act the world I may know a little to nothing about the world so instead of waiting for something to happen I'm just gonna cause something to happen I don't know what's gonna happen but I'm gonna do something and I'm then going to use my input systems to observe the repercussions of my action question possibly exactly right so you could say no no no no it's not output input processing it's your output was actually caused by the fact that nothing is happening I got bored and decided to do something so there were a lot of arguments in the mid 20th century about this kind of thing right Dewey was wrong it's not the motor system that's primary its input that's primary all right you're just not seeing the input very well you just tended to focus on the output first right you're being biased by your way of thinking about this if you read carefully what Dewey said the title is the reflex arc right an arc has a beginning as somebody said earlier a few minutes ago an arc has a beginning a middle and an end but what he really says in the article is is sensor motor coordination so there's this odd word here sensor and motor that it's kind of pushing up together the input system and the output system why all this argument you could spend hours or days or months arguing about this input first output first input first output first it's easy to make an argument either way and usually when you have a lot of people arguing about something and there's a lot of evidence for one way or and and the other way the reason is because it's neither sensor motor coordination there is no beginning middle and ends any organism that has an ability to act on the world is because is continuously in interaction with the world it's a chicken and the egg thing there is no starting condition so one of the things we're going to try and do in this course when we're designing in this case an ASL system for humans is to not think about humans as sitting there waiting for something to happen and then acting on it at the Thanksgiving break if you put your leap motion device in front of your your aunt she may sit there and wait forever for something to happen or she may just wave her hand over the device or pick it up or flip it over or try and light her cigarette with it who knows what's gonna happen right it's neither of these things right there's this continuous interaction with the world and when we're creating interactive technology technology itself ok we'll talk more about sensory motor coordination in a moment is a picture of John Dewey's grave where is that where was that picture taken clearly wasn't taken today where was it taken Burlington all right yes correct can we be a little bit more specific than that on Colchester Ave it's a Connor sorry not where you can now oh yeah I know not that great not that gravesite we're getting closer no it's a it's a it's a grave site that has only one grave in it ah it's yes it's actually next to IRA IRA Alan Chapel so whenever you have a few spare moments and you're over by Ira Alan Chapel you will see there's a single grave stone facing Colchester Avenue that is the resting place of John Dewey we are just going to briefly talk about Dewey in the sense of breaking this linearity thinking about behavior John Dewey is arguably one of the most famous American philosophers one of his areas of interest was child education which we're not going to talk too much about in this course but there is a connection between child education and this way of thinking about behavior what is that connection the process is continuous right children young children are continuously pushing against the world to see what happens they learn something about the world teenagers are continuously pushing against the boundary of what their parents will and will not put up with right what can I get away with there is if you specially think about the young there is an idea and instinct to act first and see what happens right we're all old in this room now you're much more used to thinking about sitting passively in class and waiting for the professor to bestow wisdom upon you right it's neither of these things when you really think about what humans are doing they're continuously interacting with their world hopefully you are attending mostly to what's going on in this class in real time but you are also periodically checking out and thinking about what happened yesterday and wondering about what the vibration from your phone just meant and so on right there's this continuous feedback loop with the physical environment if we talk about social interaction in the same way we can often be lazy in our thinking and think about somebody sitting there waiting for the professor to ask a question when I ask you a question you think about it and hopefully give me back an intelligible response but the social interaction that's going on right now is not is continuous and back and forth right you're continuously paying attention to my eyes and where I'm looking what I'm pointing at you're flipping back to thinking about who just called you on your phone you're thinking about if I call on you or ask a question what might you say it is continuous right this continuous sensor motor coordination there is a lot of expectations that humans have about these kinds of interactions and when we go about actually replacing the physical environment or augmenting it with a computer humans take their expectations about what happens in the world when I with it and they bring that to technology they have those expectations what are some of those expectations they bring their previous experiences and make assumptions and most of the time we're not aware that we're doing that and when technology breaks those assumptions or goes against our expectations you can be sure that human is not going to be used that technology for very long what are some let's try and be specific for a moment what are some of those specific sensor motor coordination things that are going on that we just come to expect from the real world or maybe from other people and those expectations better be supported by the technology we create okay yes so a button that's supposed to be pressed or an input that's supposed to we're supposed to plug something into that that's true we'll talk about that later in the course what I'm looking for is sort of this real-time dynamic I do something it does something I do something it does something immediate feedback we'll talk about virtual reality towards the end of the course it's taken a long time for virtual reality to get here but it looks like it's finally here super hard to get VR correct not really because of any technological challenges well I guess there are technological challenges but those challenges come from having to support the specific expectations you have the moment you put on VR goggles and the number one is when I'm wearing the goggles and ice and ice wibble my head to the left what do I expect to see in the goggles let's try and be very specific here motion blur yeah good point exactly what kind of mode the motion itself should be blurred that's a good catch yeah absolutely exactly expect to see and you'd expect to see what would happen if you actually turn your head in the real world what happens when you turn your head in the real world I turned my head 30 degrees to the left what happens this is expected field of view that's true absolutely so that's not quite about real-time feedback but there are all these other expectations yes how does the how does the image you're seeing on the inside of the goggles change should move in the opposite direction right so when I move my head to the left the image should move to the right and vice-versa right okay now we know what the humans are expecting how do we make that happen what input and output devices do we need on the goggles to make that happen sorry an accelerometer absolutely something that is that is detecting the acceleration of the head right so we've gone all the way back from the psychology or the cycle physics which we'll talk about later of the human visual system when I move my head the image is blurred for a moment because my eyes aren't fixed on any one image and obviously the image should move to the right now that we've said it it seems so obvious right part of the problem or the challenges of HCI is there are dozens if not hundreds of those kinds of little things that are expected especially in a VR system that are missed when the VR system is being designed what you're hopefully going to get good at in this class is identifying those elements before you create any technology or maybe you design some prototypes that allow you to extract what those expectations are so that they can be supported in the final version of the product okay so you're going to see this cartoon a lot throughout the course and a lot of the technologies and different concepts we're gonna talk about are basically just complications or elaborations of this sensor motor coordination the human most of the time uses their motor system or their muscles to act could be moving a head could be taking a step could be pressing a button could be waving a hand over the leap motion device the output of the human becomes the input to the computer which based on the input the computer provides some input back to one or more of the human users sense organs and we close that loop okay okay so let's think about this coordination as it exists today with some familiar technologies this one is laughably old at the moment you sit down and start to do some web surfing you start with an actual task you learn a little bit about the current energy policy for Vermont you'd like to research how could we do things better so you start reading about carbon neutral biofuels and as you're reading and getting web pages back from the computer you sort of get turned off of that that idea and the cons of biofuels are listed next to the pros of wind and solar so you switch from googling biofuels to googling wind and solar farms and so on and you have this feedback loop where you are typing something into Google it gives you something back which influences what you type next and so on okay obviously you could research a new energy policy for Vermont without the internet but these days it would be agonizingly slow and difficult okay that's one kind of interaction we could think of cover here is a completely different one we have a human user that's walking down the street they have a GPS enabled phone in their back pocket and the phone detects that the user is near Starbucks the phone also detects that there was a recent tweet that you sent out with the word caffeine in it and puts those two things too gather Buzz's your leg to say it's got some information or an idea that you might find useful you pull out your phone and your phone tells you that there's a Starbucks nearby instead of you typing in okay I'm going to go to the Starbucks or clicking on a Starbucks destination in Google Maps you just drop your phone back in your pocket and you slightly change the direction in which you're walking and your phone infers that you're moving towards Starbucks however now that your phone has reached the conclusion that you're heading towards Starbucks it also knows that your battery is low and based on your previous visits to Starbucks infers that your battery is going to go flat before you leave the Starbucks so it buzzes you again and tells you you're not going to make it and infers that you now change direction and you're heading out towards a friend's place so that has a compatible charger and so on and so forth right kind of science fictiony I don't know if you'd actually want to do this but you probably could actually do this today right or we're pretty close ah there we go right yeah your friend has coffee problem solved right does your phone know that you've gone to your friend's house before right does he you might know that because you know the friend does the phone know that what information is available to these two systems right okay kind of a cartoonish example but if I were to ask you to start writing some apps for the phone to actually support this kind of interaction those are the kinds of questions you might start to start to ask right would somebody find this annoying or useful definitely annoying why your privacy handle a person and possibly it could also tell you when you don't want coffee it's a shiny be helpful so there's a positive a helpful it's trying to be helpful right and that would be great we want our technology to try and be helpful we don't have want to tell it everything we would love it to speak up and say hey I think this is what you're trying to do do you need help ah [Music] exactly maybe this is oh this is too elaborate annoying it would be great if the phone just asked right possibly sorry exactly we're all drowning in notifications these days right how many notifications are too many whatever it is the answer for most of us these days is it's now too many it's hard to know right it depends on what the human is doing right presumably my battery flatting out is more important to me than getting a cup of coffee maybe maybe not depends on what's going on right it's complicated it's very difficult to create technology that would support this activity without the user finding it extremely annoying but if you got it right it might actually be people might actually adopt it the point is we want to think about this as this continuous interaction right there isn't a human standing stationary somewhere waiting for the phone to tell me whether I should go to Starbucks whether I should go home and charge my phone whether I should go to my phone it is a continuous interaction in this cartoon example the human is always in motion but their motion is being altered for the better or worse by their phone like this but we like serious idea oh my homemade this trip I know some message I want this pasta possibly right is this sort of a rebuttal to Dewey's idea you could maybe argue that however you could also argue that action is primary here we have someone who's moving they're outside they're going somewhere it's unlikely that they're going to stop and wait for the phone to look up things figure something out they're always doing something if my phone says nothing I'm not I'm not an idiot I will carry on the best I can without my technology right so there is this continuous feedback loop it's not a pass whatever it is this is not a passive process which unfortunately is what a lot of the scenarios view of behavior tried to impose on people in the 20th century right you're not waiting for your phone to tell you what to do next you're doing something and your phone may or may not even fluence or change what you're doing it may be for the worst you get annoyed you stop and turn off notifications but you're still you're you're in charge you're acting okay yes exactly so we could imagine this is an adaptive system right if you if notifications are turned off the phone tags that as an annoying interaction right and when it does it the next time it might try it differently but now we're adding an additional wrinkle to this right which is now we have an adaptive system and in order for the system to learn it also has to its actions also have to be primary did you like this interaction did I do the right thing could I have done better I can infer whether the human life what I suggest you maybe we're not back possibly yeah which is okay yes okay absolutely so this system is also not passively waiting for humans to help it out right the environment is going to do what it does regardless it's not going to wait for humans right so we can apply the same thinking not just to humans but to the environment as well right both systems are constantly acting and that could be that could be right for the purposes of this course we're going to think about both systems as active right neither system is waiting for the other to tell it what to do or influence it right that's the thing we're gonna focus on in in this course the question is how or whether we can create technology that improves this interaction or not and again depending on where you sit on the optimism pessimism scale that may or may not be possible we're gonna try in this course yes yep isn't that the input that kicks all that off right so where's the the start in this right start middle and end I'm trying to tell a story here right so for the phone I'm assuming I didn't write it here but the phone is on right it's detecting something and when we start the story the user is already walking right so both are already engaged in this feedback loop you could argue that the phone is passively waiting for the human to do something in this example again I'm assuming the phone is on it's already on its already detecting what you're doing and you're doing something which is walking down the street when you turn your phone on in the morning arguably that is a start point so you could think about if it helps you can think about these loops at least on the computer side of having an additional arrow coming in which is I've just been turned on right there it is a start point for the machine if you like the moment we're just talking about one single smartphone we're gonna start talking about networks of smart phones remember my cartoon from last last time about the network of humans and phones and computers and Wi-Fi stations and embedded devices and robots for better for worse in the Western world where a meshed in a technological net that has never turned off parts are turned off and on but there is no real start point from mourning tonight for most of us you're continuously interacting with technology which is continuously interacting with humans ok ok so we have these two actors humans and technology that are interacting together and the design component of this course we're going to try and take that into account think about that carefully and try and put those two systems together so we get the best of both worlds rather than the worst of both worlds easier said than done if you ask people to describe people machines like we just did if you ask most of the people in this room we tend to be mostly machine centered humans they're vague disorganized distractible emotional illogical machines are precise orderly undestructible unemotional and logical if you read all Mormons look about this this is the Spock view of the world right humans and biological organisms are messy and they make mistakes machines are clear and clear and perfect to never make mistakes you ask people that are people centered you'll get the opposite description right people are creative compliant they're attentive to change so if something changes in the environment we will try and adapt to that situation we may not have a hand the exact way to adapt but we're resourceful we might come up with a way to adapt computers at least for the moment are not so good at used to highlighted elements computers are not so good at attending to change they can attend to change but not relevant change one of the reasons why we still don't have autonomous cars on the road and machines are not yet very resourceful it's not so easy to create a machine that comes up with Plan B when plan a fails okay humans are able to make flexible decisions based on content and machines and contrasts are dumb rigid and sensitive to change unimaginative and so on right the answer is always is all of the above right it depends a hunt the situation we would like to take the we would like to take the best parts of machines and combine them to support the best aspects of humans again easier said than done okay I mentioned that I'm using computer is just a standing keyword for a broader range of technology so I will try and use interactive system from now on that's just a reminder that we're not just talking about smartphones and laptops and desktops we're talking about any machine that is preferably continuously receiving input from the outside world and human users and continuously providing output back to the physical world for people we're not going to be overly broad in our definition we restrict interactive systems anything that has electronics in it anything that is a technology a knapsack is also a technology right but unless we instrument it with some electronics it's not a smart technology you are when you're wearing a knapsack you are also engaged in a continuous sensor motor coordination with your knapsack you're wearing at you put books into it and you immediately feel the change in weight and shape as you're walking you are providing forces to the knapsack which starts to jostle around on your back which you feel and you continuously alter your gait so that you don't fall over with a heavy bag of books on your back right there is also a continuous feedback loop there we're not gonna focus on that we're gonna focus on machines that don't just receive information from humans in a purely physical way they receive information and then do some sort of internal processing before they provide feedback okay okay throughout this course as you've probably figured out already this is going to be a very broad course we're going to touch on a lot of concepts from a lot of very different disciplines this is not a normal computer science class where we're focused on one aspect of computer science we are trying to develop technologies down here to help human or humans carry out some tasks that would be difficult or impossible otherwise and in order to get this feedback loop right we're gonna have to think not just about the technology what we could do but what the human would like us to do what they would like the technology to do in this class we're going to focus mostly on the interfaces of technology and not so much on the internal processing I mentioned this before there's a sister class to this which is software engineering which focuses on doing all the internal processing we're really gonna focus on given the internal processing which is usually invisible to the user what do we present to the user and what do we collect back from the user in order to do that we're gonna have to think not just about technology but people we've already started to see this so we're gonna touch from time to time on this in this course on sociology when you engage in a social interaction with somebody talking with them in real time we're talking with them on a phone what are some of the expectations of a social interaction that you want to distill out of that and build into a technology that is going to simulate a social interaction maybe it's an AI avatar that you're going to interact with what are what do people expect when they interact with the person in real time that should be supported by a thing that's gonna try and do the same thing only one person is talking at once in this room usually is that always true in social interaction not always right it depends on what's going on right and we're gonna talk about that when we talk about pact analysis most of the time we expect turn-taking but not always we'll talk about psychology obviously we'll talk about ergonomics so we'll get into sort of aesthetic design and the literal or figurative fit between the technology and the things they use how many of you put your cell phone in your back pocket when you're usually right phones these days are getting a little bit larger I hate it right it's it's too big I usually have mine in my back pocket now when I sit down it's there we're talking about the literal fit right most cell phones were originally designed to fit in the hand right and now because we use them all the time we're carrying them around putting them in our pockets do they literally fit in our hand our pocket do they metaphorically fit into our lives right they they fit in this classroom because you've got me used to hopefully switching it to vibrate mode at the beginning of lecture right we didn't have that mode they wouldn't metaphorically fit in the class you'd have to power it down for the length of the lecture okay we're also touch touch a little bit on anthropology and culture so we want to create technology that interacts with humans but which human what culture does that human come from are there different expectations based on their culture what are some examples of cultural differences that impinge upon technology ok ok that's a great that's a great example right so there are certain cultures that it can only use technology six days a week right that's a great example that might depending on what app you're developing for the phone that might influence your thinking about that app other examples okay yeah that's a great example right so bias is now a very important topic especially in AI but in any sort of interactive system are we biasing the design with technology toward a particular gender culture race creed it matters right yeah often this is happening implicitly because people didn't think carefully about who might be using the technology exactly okay we'll also talk a fair bit we'll spend a bit of time talking about design so HCI is not a purely technical discipline right it sort of straddles the objective side of computer science coding real-time interaction sensors motors but it also straddles the aesthetic side right if something isn't beautiful or efficient or it doesn't fit well into how we do things the technology may be perfect from a technological point of view but unacceptable from the human users point of view so we'll spend a fair bit of time looking at some of the objective disciplines like engineering but also aesthetic disciplines like arts graphic design architecture which also try to create artifacts that straddle both the objective and subjective okay we'll also be not just about the human but what is the human doing what activity are they trying to carry out and what are the unspoken contexts that underlie those activities the black art of HCI is pulling out from the human things that are unspoken that once they're realized influence how the system should be designed right we are all engaged in activity right now which is either delivering or listening to a lecture there are a lot of unspoken rules that exist in a classroom what are they we mentioned one which is turn-taking what are some that are sort of subtle they're not so obvious but they're here this relates often to those communities of practice or tribal knowledge you RCS 228 students you are now part of a new tribe the CS 228 tribe and you all have certain knowledge of certain unspoken rules about how the tribe operates what are some of those you should wear clothes to class that's a good one it's not specific to 228 all right it's common to the Western Way of doing things but what is very it is an unspoken rule I suppose what's an unspoken rule that's very specific your state right you're not getting up and walking around yeah that's a good one absolutely right you're taking notes on your desk not on her to ask her his desk yeah that's a good one personal space bubbles yeah absolutely so I hand it around the attendance sheet which is doing it the old-fashioned way with pen and paper I could try and write some technology to sort of automate or help with that process are there any communities that are there any practices here in the classroom that I could pay attention to that you could tell me about that would facilitate my development of taking attendance each morning if you look at that sheet when you sign it what can you tell me about that sheet what do you see on there sorry others name yeah okay there's a line to write your name on the date yeah it's numbered which is all there right I can pick up the piece of paper and see that that's fine what are the unspoken rules what are some things that are going on in that process that activity that might be important if I try and automate this process yeah we write it sequentially yeah so there's a sequence of names what can you tell me about that sequence aside from the fact that there is a sequence it goes from the front to the back right so this is good now we're getting into it I usually hand it to the person sitting here you write your own name down yeah so it's usually only one occurrence of an a of a unique name on the list what else let's go back to the sequence this is this is the read the fact that it's taking us so long to get to what I want to get to is an example of why HCI is so important it is very difficult sometimes to pull out of a tribe unspoken rules that matter okay getting there okay yes that's that's true yeah if you're here you might say hey I saw you came in late you forgot to write your name why don't you go sign it before you leave yes that's that's a good example not the one I want to get at when you think about the physical context you come into class every morning what happens don't think about multiple attendance sheets each one is a sequence of names sorry okay yeah there's one token the Sheep it could write it up on the board and take a picture of it with my phone yes that's true happens where's the beat yeah okay that helps can track where somebody's been sitting now we're getting really close what am i driving at what is the unspoken rule when you come to class thank you generally speaking you sit in the same seat does it say that on the syllabus does it say it on any syllabus you've ever seen at UVM no one told you to sit in the same seat it is a general rule that tends to exist in most classrooms right why where did this come from why does that happen exactly as we would trace this back to fairness and that's sort of I like to say here you said over there before I'm not gonna take your seat if I get here early and sit somewhere else right so there's actually this complicated social dance that's going on which maybe doesn't matter but one effect it has is that if you look at the attendance sheet the sequence of names is always more or less the same right some people don't show up so it's not always exactly the same but I might be able to exploit that regularity if I wanted to try and automate this process somehow possibly okay we've got two minutes left so I want to just finish by introducing this idea and then we'll finish it next time we're going to draw on cognitive psychology just that part of psychology that deals with how we make sense of our sensations how we how we turn sensations into perception and think about it we're to use that for our theoretical base and we're going to use software engineering to think about how to design technology to support a particular activity and the result of that design process is the user interface itself right it might be a mock-up I might create just a drawing to represent a prototype it might be something I create in code but it is sort of the window between the user and the technology when I have that user interface and I deploy that interface and people start interacting with my technology we have this continuous feedback loop and in that loop there is not just one interaction that is going on there multiple interactions that are occurring in parallel the most obvious one is physical interactions you're pressing you're pressing buttons you're seeing what the screen presents back to you that is something that's very obvious that you can focus on somewhat less is perceptual interaction so what does someone make of what's happening how do they perceive this feedback loop and I'll give you an example next time and the last one and the hardest one is conceptual interaction what do people expect the technology should be doing I give you a brand new alien technology like leap motion device and you wave your hand over it you suddenly see that virtual hand appear and you might if you hadn't seen last last lecture you might wonder what happens if I bring my second hand into the devices field of view the second before that happens you've made a guest in your mind about what you predict is going to happen there is already an additional wrinkle to this loop and you may or may not you may or may not be wrong so the moment your second hand enters the field of view either this screen will change it'll draw two virtual hands or it won't and that will give you back in this feedback loop one additional piece of information the device can only see one hand at a time or it can see one or two hands will leave things there for today you have a quiz due tonight deliverable one due Monday night and I will you Tuesday morning thank you "
x6u1r3ePGXI,27,"Andres Monroy-Hernandez
Snap Research

September 27, 2019
Social computing has permeated most aspects of our lives, from work to play. In recent years, however, social platforms have faced challenges ranging from labor issues with crowd work to misinformation on social media. These and other challenges, along with the emergence of new technical platforms, create opportunities to reimagine the future of the field.

In this talk, I will discuss my research designing and deploying social computing systems across three distinct environments: creative collaboration, urban information, and workplace automation. First, I describe an online community I created to connect millions of young creators to learn computational thinking skills as they create, share, and remix games and animations on the web. Then I shift to how this work inspired my next set of systems to connect urban residents as they share information about their local communities. Finally, I will discuss a novel workplace productivity tool we designed to enable information workers to delegate tasks to hybrid intelligence systems powered by humans and AI.

I close by articulating the challenges and opportunities ahead for the field, and the ways my team is beginning to explore new systems that support more authentic and intimate connections, building on new technologies including wearables and AR.

Learn more about Stanford's Human-Computer Interaction Group: https://hci.stanford.edu
Learn about Stanford's Graduate Certificate in HCI: https://online.stanford.edu/programs/human-computer-interaction-graduate-certificate

View the full playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rMyupDF2O00r19JsmolyXdD&disable_polymer=true",2019-10-11T23:54:29Z,Stanford Seminar - Designing and deploying social computing systems inside and outside the lab,https://i.ytimg.com/vi/x6u1r3ePGXI/hqdefault.jpg,stanfordonline,PT55M1S,false,994,25,1,0,N/A,"all right thank so you marry me I'm really excited to be here last time I was here was I think five years ago and I wanted to take this opportunity to kind of go over a number of projects that I worked on in social computing over the past few years some of it is living edge and the last part of the talk but a lot of it is kind of more historical as well so I guess it's kind of common knowledge now and a little bit a little bit of a cliche but like really social technologies are here and really are part of our everyday lives from the way we communicate with our friends and colleagues to the way we you know gather knowledge to even the way we politically organized technologies that are social social computing technologies enable and facilitate a lot of these things and so that's kind of the the focus of my research for the past few years is really to study and design and deploy new technologies that help people connect and collaborate and so I emphasize the word deploy because that's something that I've been putting a lot of emphasis in my work where I try to not just build something for you know writing a publication but also to put it out in the world have people use it and kind of learning from how people use it and you know iterate on that so I worked in a variety of different settings from you know helping children learn programming like Michael mention to you know peppy people helping people in the workplace delegate tasks to you know conversational assistants so I'll go over a few projects in four different categories all and more on like the more recent work that we've been doing at snap so I'll start with the word that I was doing with scratch and maybe I can tell you a bit more of a personal story of why I got involved in this so I moved to Boston in about 2001 after college I went to college in Mexico where I'm from and then I join a tech company in Boston as a software developer so I was there for about four years and on my year three I said to get itchy and like you know trying to understand trying to do things beyond just what I was doing as a software engineer and one of the areas that I was really interested in exploring is kind of how do we enable other people like myself who I think for me at least learning to program really open a lot of doors even coming to the u.s. was one of those and how do we make that more available for everyone and so as I looked around in the world like trying to see who is working in that space I found a team at MIT who was actually working on building tools to help people help children in particular learn programming and so that's kind of how I got interested in this field and for me kind of the the aspect of this space that I found particularly interesting is kind of how to take that experience of programming beyond just like you and your computer you and your IDE and make it into a more social more collaborative type of activity and you know as a sufferer engineer that I was you know I realized that in the real world it was very different from how I was trained to learn to program what I was going to basically by myself you know in the real world as you probably know you go to you know Stack Overflow you google these take snippets of code from here or there and that's kind of what I wanted to bring to the way people learn programming when they're young and so as I said you know I found the scratch team who was starting with this project was very early in the stages of the project and what we were doing is essentially build this tool that was very easy to use for anyone I don't know how many of us have used scratch many of you I don't have to explain a lot about what is us so just kind of the core ideas of scratch is that we wanted to make programming much easier by simply you know representing all the computational constructs in blocks that you can stick together so you wouldn't make syntax mistakes very easily and it was very much focused on allowing people to bring their own media like in this case you know there was a kid who drew these cool spaceship bring in that media into the environment that you can then program and manipulate so at the time scratch was just a desktop application and that's what I kind of found both frustrating and also an opportunity to explore deeper into how to make that beyond just you and your desktop so yeah that's actually a video I found recently about scratch early on you can see it's like Windows Vista I think the frame is really old but it you know the idea is very very simple you just drag and drop these blocks and control something on the screen and so what I did you know with this was around 2005 and Flickr and I think YouTube just started with Flickr in particular was a big inspiration for me where you if you're a photographer you can go to Flickr find a large community of photographers get comments on your photos improve on your practice you know learn from other people's work and actually flickering particularly liaos you to download photos and remix them because you can they're all not all but a lot of them are incredible commons license so that's kind of what i wanted to do but for programming and the idea was that if you make a cool project it shouldn't stay just on your laptop for you to show to your friends but it should be something that you can put out in the world just like you do on YouTube or Flickr and then people can interact with it and you know you have an audience for your creations so today the community I just check a few days ago has grown to about forty six million people it has about forty four million projects that people have created from all over the world one of the features that we added early on was this idea of groups very much like what today is Facebook groups where people can come together on a particular team work on things together and you know one of the key pieces of what we saw a very big potential for this community to help people learn is to support very mixing and sure enough about 25% of all the projects are remixes of other people's projects let me give you an example of what a remix is so here is a game that a girl made very simple very modest I was like this jumping monkey that you can move with your keyboard but what I saw is like this whole evolution of how that little simple project turned into many iterations and many other kinds of collaborations and that's just one example so for example in this case she posted this project then a few days later this other kid took that project and then added what they call pink slippers so this was just like a some shoes to the monkey so that the monkey could detect when he's touching the black lines which is the platform or the red the red area which is like the lava you're supposed to avoid and so with that technique that is this person just changed it and and made it slightly better much more easy to to control and took a count score and then a few days later this other kid said you know I made these by adapting the shoe technique from the previous project and then a whole different world like lava that comes vertically and it was a lot more complicated with multiple levels and so on subsequently this other kid took that and you can see in one of the comments there it says you know a waste which is the creator of the previous project I love this game I was wondering if you wouldn't mind me making some changes and you can see the green project is what they made out of that was like a much more sophisticated like the graphics are really cool again multiple levels and not only that but every single one of those dots represents and other remix of that green project so overall you know created this whole kind of mini community around this one project and like that there is like millions of others one of the other pieces that was really interesting is that these two kids that the the green project and the orange one start to create a group and they call it a company where they wanted to create games together in many of these cases they assign each other roles like somebody may be the artist I mean somebody may be the programmer the sound artist etc and they created these kind of collaborative space for them to build games and they meant made a bunch of games so that's kind of the norm in the community there is many many collaborations happening across geographies one of the things that I didn't show you is in scratch you can create the project say in English somebody can download it and see it the code in Hebrew or Spanish or Chinese and so allows you to like see the code in the language that you're using which is pretty unique for programming languages so the other piece that I was really interested in doing is making the scratch community into a laboratory that you can use for exploring all sorts of social computing questions so as Michael mentioned you know we study a lot of you know how people have children in particular perceive intellectual property and so we often think of children you know copying content from others but in this case they were the ones being copied or remixed and so we study things like you know how people when I receive attribution when they're being remixed whether it's automatic or or is you know manual and the ways this are represented in and interpreted by people in different ways and all sorts of different studies that investigated the nature of collaboration in this particular community that can speak to collaboration more broadly and the last thing with is for this particular project is that we took together we put together a data set of five year of activity on scratch the first five years and published it online Nature has a really cool Journal that you can just publish data and so that's what we did a data descriptor and so since then many researchers around the world have used it to study anything from like how children learn programming to like economy is looking at how like cooperation and collaboration happens online and so you know if there is one thing that you can take from this project is that this data set if you're interested in analyzing data or using data this could be a good source of you know resources for you to explore and so you know I went from you know trying to understand how we can connect people and enable them to collaborate for learning new skills and then I moved to a new space as I was you know I move away from the Media Lab and I was looking for a new area to explore I had been working on scratch for six years locally I was at MSR where you know you're given the freedom to do kind of whatever you want and so two things happen at around that time I was around 2011 the first thing that happened is that I saw a shift in the way the technologies that we were building things like the scratch community would be YouTube and so on they move away from what I saw primarily you can have creative expression tools so you know if you think about YouTube the original ways people were using YouTube or a flicker word really to express yourself creatively and put things out there for people to see but I think what I saw run this time in 2011 was a shift towards the use of these technologies to facilitate collective action and societal impact of this is around the time that the Arab Spring happen right and so you know you can argue how much you know Twitter and Facebook had a role in the Arab Spring but they had a role and so these cannot really resonated with me to see like how these things you know I was you know playing with cute cats and you know moving them on the screen - like you know organizing revolutions all these part of the same types of technologies so that's the first thing the second thing that happen is that as I mentioned I grew up in Mexico where around this time the violence in Mexico due to the drug war was increasing significantly and the only way for me to know what was going on was through social technologies through Twitter Facebook and so on because traditional media was censored by the drug cartels and so I found this space is a really interesting space to explore and to really question and train to answer this question of you know how can we help people connect and collaborate to learn about the world around and to learn about the the local events and if you're in Egypt it may be you know a revolution if you're in Mexico maybe about how to avoid being killed by the cartels but really how do we bring people together in with this goal of learning about the world and so as I said you know the context in which I started to explore this was less about building and more about studying and seen let's see what people already do and then we can decide what to build and so one of the first thing that I started to look at is how information travel through these social media ecosystems in a place like Mexico where as I read out from from this article from the Washington Post fearing for their lives and the safety of their families journalists are adhering to our near-complete news blackout under strict orders of drug smuggling organizations who dictate via daily telephone calls emails and news releases what can't or cannot be printed or air and these kind of news blog has also extended to the government where the article continues and says how you know government officials in the city of Nauvoo Laredo which is in the border with Texas in that city the mayor mysteriously disappears for days and refuses to discuss drug violence and even the military who presides over the solders doesn't hold any news conferences so in mind you're in that position and you see horrible things happening around you from car bombs to grenade attacks and shootings and there is really nothing in the traditional news media so what you see and what I start seeing remotely was you know friends and family who lived there telling me look at this message on Twitter like there is this you know report of a shooting or this report of this violent event sometimes with photos sometimes your steaks and a lot of them and I'll translate these to English so this is an example of a tweet that I found where this anonymous woman Angela she posted like by this supermarket there were some risky situation and that's the kind of term people used to refer to all these violent events and and so we start seeing a lot of these can organically organize neighbors local communities reporting what's going on in the same way you see people here using something like ways to avoid traffic there you will use you know Twitter and the particular hashtags to avoid being killed by you know one of these shootings that they're happy and so I I what I did is I we had a partnership with Twitter at the time and so we gather data from four different cities that were particularly affected by the violence in Mexico for 18 months we gather about 600,000 messages on Twitter and trying to understand what are the dynamics of what people are doing there and so one thing that we did is this is an example of one city in particular this diagram where on the y axis you see the number of followers that people have each dot represents a person on the x axis you see the number of tweets or messages that they posted on a particular cds hash the set of hashtags and so among the many things that we found is like this emergence of these particular individuals that opposed significant amount of content on these communities and also have a lot of followers in essence playing the role of what traditional media will do and you know if you look at what they were doing they were essentially reading and collating through hundreds and hundreds of tweets I interview a number of them they spend like ten hours a day sitting on their computers you know analyzing and you know browsing through this information and posting it back to the community and the community will find that really valuable and sometimes retweet sometimes ask a question like Oh which supermarket do you mean that sort of thing and so I found out really really interesting and you know one of the other things we did was you know analyzing as well as the repeat Network in terms of who is the person where the people most retweet it and sure enough the same group of people where also you know coming up as some of the most widely spread people in this community so you know we need a number of cells are from like trying to understand how language evolved and how people may express come they may lose sensitivity to the violence and through the messages on Twitter but it was very much focused on like understanding what people do and so what we wanted to really do is like how do we you know build systems that may help or may be inspired by these type of activities perhaps not in a very dangerous place like Mexico I think one experiment in a place like that but you know maybe in a more safe space like where we are in Seattle for example and so we built a number of systems experimental systems that help us curate hyperlocal information and by hyperlocal information I mean things around neighborhoods or things around even like a few blocks in a city so one of the things we built was a system that essentially grabs Twitter information on Twitter messages to create neighborhood news reports so we deploy this application in Seattle in particular where you can kind of see what was going on in your neighborhood even before Twitter had these kind of city specific trending topics then we also explore you know ways of nudging contribution so we often saw that something was happening maybe one person posted something on Twitter but there was not a lot of information beyond that and we wanted to know where exactly is this happening so we built a system that kind of notch people as they were tweeting and maybe not other people who were self reporting being at that location to gather more information and then one other things that we did is we also started to look at ways to not just rely on what who else who which people are already there we're also sending people to locations to help us gather information we shall go into more detail in a second one other thing that we did actually with a student here at Stanford we worked on ways of summarizing Twitter feeds from particular events this was in the World Cup so we just kind of were cup games in the form of comics so like how do we with a four-panel comic consumer eyes what happen in a game for example and one other thing we did was also a application that kind of connects commuters with one another as they travel to tell stories and share poems and stuff like that that one actually was published as a Android ad that was somewhat popular during these few years but I'll go into more details on these event reporting with offline crowds because I think it provides a slightly different angle from what the way typically crowdsourcing is done which is typically more focused on like you know somebody like em Turkish or or crowd workers on a screen and in this case we wanted to send people out in the streets to collect information so I'll show you a video that describes the system [Music] [Applause] [Music] so the user experience on the requestor side was very simple we try to make it like what is that call this is those books will you complete just sentences basically just say something like I would like to use a pencil to cover the event bla that will take place at this time on this location and then they will select the kind of event it will be and based on the kind of event that they selected we created a number of tasks that we will post on a market place in this case in particular it was TaskRabbit where we will say if you show up at this location at this time to take a photo you know will give you a ten dollar sony dollars whatever the idea was that we could put together an event report in under an hour and under hundred dollars and kind of very quickly create this kind of crowdsource event reporting type of activity we deploy these in many different eleven different events across the US including conference call South by Southwest in Austin and we partnered with a number of local bloggers who wanted to cover certain local events that you know as we know local newspapers are kind of disappearing so they wanted to you know in some ways delegate to the system the gathering of information one of the interesting tidbits that we learn from interacting with the bloggers is that they felt that the reporter our system was creating were kind of to neutral in some ways which was kind of surprising to us and part of it was that a lot of these local bloggers or local reporters tended to have an angle that they wanted to always pose in like introducing to the right stories like maybe there is some construction that is happening and they wanted to complain about that or some problem that the neighborhood was having so they use our reports they massage them that you can change them a little bit and then they post them into their into their local blogs and and kind of mini newspapers that they had so that was kind of a genre of work where we try to kind of bring people together both growl workers you know requesters neighbors and also trying to understand how people connect and collaborate out there in the world to report on what's happening in their local events and in their local communities and so you know going from like something as different from like children learning programming to reporting local events you know they are all part of this kind of fabric of technologies that basically bring people together and achievers help us achieve something bigger that then while back we'll do what we can do individually so as I mentioned as I at the time I was at Microsoft so we built a lot of these systems deploy them out into the world but I always felt that you know in a any organization where you are there are certain constraints and certain kinds of projects that may thrive and other kinds that may not right just by virtue of the environment where you are and I found a microphone I don't know why it took me so long they're really the projects that will thrive are those our focus on you know productivity Microsoft is the main challenge is cost so for like crowdsourcing news reporting and sending people are into the world is caused and as I said we try to make it under $100 but it was more expensive than that so we were under the impression that we were going to do like investigative journalism or any kind of like deep reporting it was really more like a listicle type of article like something that you will see on BuzzFeed like the five things you missed at the parade or the you know so that sort of thing and and we were very clear when we interacted with some of the bloggers and so on to make sure that you know they didn't expect like this super endeavor thing is more like a list of facts this happened this happened yousa follow and so that was something that I will say it's a limitation but we knew that coming into that yeah I think those would probably some of the main ones and as I mentioned like the neutrality in some ways we saw it as a positive thing but in some ways the users that were using our system felt that well I wish you had like they angled like they probably expected to like copy and paste the article that we produce into their blog the realities that they had to massages and that's probably a limitation well yeah that was your dinner idea you know we had this grand vision like oh yeah we'll put that out there we actually talked to folks who are in like newsrooms and so on and they're like yeah I could a man do something like this I think yeah that was just we didn't push hard enough for a lair I mean I think we had continued working in that space it could have happen and you see like CNN for example had this app called CNN i reporter which i think happened around the same time but like I see here and they're like more and more interest in doing this sort of thing I think the incentives are a key challenge one thing that I didn't mention in some of the events we rely both on nudging people on social media that we're at the event and we oh you're there do you mind taking a photo of this or that as well as on these paid workers and we combine both streams of information and one of the challenges that we saw sometimes where one that the people who were there at the event fell maybe like if they knew that we were paying somebody else fare like oh this person is paying why am I not getting paid like why are you taking advantage of my time and so that was one something that we we didn't realize how we was going to be problematic the other one is like some of the events that we cover were kind of small and local in nature so having some stranger you know imagine a TaskRabbit maybe this will be a fine place but like some events may be like who is this person and why are they here and they will often present themselves like oh I'm working for the startup that is doing local event reporting and so on yeah so we didn't mention we have a curator online that we hire from upwork who was a writer who essentially had an interface where they could accept or reject or reject with with changes like say oh this photo is blurry can you take another one or a you know this is this seems you know invalid information or something just on a feed they will accept a reject we had really no way to know if something was factually true or not in fact one of the experiments with it was like a report on I think was like the snow there were a bunch of snow storms across the US and we saw that some of the people we hired they didn't actually go out in the streets they went online search for like photos of snow or something and then they like uploaded to our system so that was a challenge that we observed to like people cool I guess cheat in some ways but there's better ways to figure those things out yeah we didn't have I mean we did have like a mechanism to curate content yeah try to do any qualitative yes facts yes yeah yeah and so they all the structure of the missions that we show workers were very much about like collecting factual information and the kinds of questions we asked them and tasks were based on the type of event that they were at so the last project that I wanted to call to talk about in terms of the big deployments is this project that we worked on a few years ago where we wanted to figure out how to make more efficient the connection that happens between people by allowing them to delegate and one thing that you probably saw from the previous project one of the key things that at least was inspiring to me was this idea that you can delegate to a system a complex task and let the system do its thing and so we went around and asked a bunch of office workers in many different companies with a survey we also the interviews where we asked them like if they had an assistant what will they want the assistant to help them with so that's kind of how we you know start to think about these this idea in the space of you know office workers and you know scheduling meetings came up at the top and so part of our idea which we were kind of naive at the time well will solve scheduling meetings first then we'll go down and you know solve all the other ones we took us three years to whom the meetings and there is now a product that office offers but you know I left Microsoft after that so it's really a big big problem the main challenge that we wanted to solve is something like these you probably have experienced something like this so you get an email and your email notification and then you open it and it's like somebody wants to meet with you in this case one of our interns is like hey you know I met at this conference unless do you have do you want do you want to chat about potential internship so I was like sure so then I go to my calendar to switch apps to go to my calendar check potential time and then send them an email and saying oh yeah sure at this time I may be available and then they may reply oh sorry I'm not available at that time what about this other time and then I go back to my calendar and like oh yeah so then I add something in the calendar send the invitation and done that's kind of the easy path right we've seen all these things go back on for four days and that's not not ideal so part of our goal was to you know make in situ delegation possible and by in situ delegation I mean the idea that you can delegate to an assistant within the application that you're already using in this case email and so what we wanted to do is being able to simply CC the assistant in this case Cortana in an email thread and then forget about it and let the assistant take over all the back-and-forth and analyzing the different kinds of time and so on and so from the user perspective let's say I am the one who has access to this assistant I send an email to this other person ruff and I see see assistant to help us find the time then without me involved the assistant will send an email offering times based on my calendar because the assistant has access to my calendar with natural language snow with it we want to avoid like any kind of buttons or anything that looked not like regular conversation the people will have with a human and then the person will reply again in natural language like yeah I mean free on Monday and so you saw that their system for example offer a few times in different days the person roughly didn't have to say what time on Monday because there was only one time on Monday and then the assistant at the end we'll send a calendar invitation to everyone either to the calendar automatically and so on that's kind of like the magic that we were going after and we also knew that the in-person meetings tend to be very complex like sometimes it's not just two people it can be many people people inside the same company outside the company etc and so that's kind of the problem that we were trying to solve and so our goal was to build this skirling I see that's how people you know find the time to meet but you know there's a long history of AI trying to solve this and many of us have failed so why did we think we were going to be able to do it the other one is the language understanding is also very difficult as we all know and one of the things that we thought about is that we were trying to figure out why is that a lot of these systems have try and fail one of them is that there is very low tolerance for error so as soon as the system fails there this doesn't work you move on you can see this with Alexa or Siri you know you try once and then you move on if you do some work the other one is that you know it's a cold start problem like you don't have enough data but like meeting scheduling conversations Howard you're going to solve that and how are you going to collect this data and the last one is that you know scheduling as I said is really hard it also gets at the you know cultural styles of different organizations and people they may be people who are like oh I don't want to meet in the mornings and so and so that getting all that right was really difficult so within is that we created a kind of Wizard of Oz type of approach but to build a real-life system and so we had that three-tier infrastructure architecture for this so we started with automation where we try to simply use off-the-shelf and LP tools to try to kind of parse out things like dates and things like locations and stuff like that but we knew that was going to fail as we've seen with many many years so what we had is a Tier two of escalation so if the automatic parts detected that it was not confident with the results then it will escalate to non expert crowd workers who could have these micro tests are very well targeted towards extracting a particular kind of information and then they will kind of solve that particular problem and that data will then feed back into the NLP system to you know become better over time but that was not enough because we saw that even these two systems these two pieces had limitations like it was limited by the kinds of workflows that we were going to design and so what we did is that we engage with that third tier of macro tasks as we call them these are expert workers typically administrative assistants who will take over the whole process if something in micro didn't get resolved as we affected and so this escalation happened from micro to macro and the data of the escalation was something that fed into the system to kind of improve the workflows over time and so that's kind of how we started I won't go into a lot of detail about the the kind of automation part but we use a classifier that essentially allow us to see the different responses that people had to what we call a ballot which is the offering that the assistant sends and then it will determine whether the response of the individual match any of the options in the ballot and it was pretty accurate but you know obviously this was flow so that's why we had the micro tasks where essentially was a very simple web interface that we showed to workers where we asked them like when is this meeting supposed to happen and then they could have a little range of times when when they could enter that or maybe even things like is this even a meeting like we are not able to detect if this is really about a meeting and so the goal of this test is that they were completed in about 10 to 13 30 seconds so very quick and short and they were designed to collect data for automation then the macro test that I mentioned were triggered by the red button there when the worker says I don't know how to answer this it the options that you're giving me are not enough then he will escalate to a macro worker which had a much more in-depth view of the whole kind of conversation that has been going on maybe four hours days between the different people and then it will start to intervene and take over and we were collecting all this data to influence the design of new paths in our workflow so our workflow had like was evolving over time like many expert systems and the idea was to combine you know this kind of expert system approach of workflows with this micro tasks with macro tesserae and as well as the more automatic part and so I guess one of the key things for us is that we wanted the system to work well from day one and this allowed us to do that because on day zero you could mind being a hundred percent macro you know on day ten it may be like five percent micro and so on and over time more and more things move from macro to micro and from micro to automation last time I checked I'm not in the project anymore but like the parent as I mentioned as a product from office it's called calendar help you can sign up there it's about 75% of the micro tasks are now are automated and and you know some of those continue to really continue to improve that as well one of these that I did last night I wanted to see how people are reacting to the product because I don't have access to the data anymore and so I see tons of people's you know using the system some people you know saying that they've been using it for a while or things that you know it works great for saving times but it doesn't speak my language which is another one of the big main challenges that we had you know Microsoft has offices all over yes because the bottoms will eventually be constrained by the workflows that you as a designer will come up with and so there's always exceptions and so people really Monday I could do but I'm picking up my kids and you know maybe I would prefer Tuesday so a lot of that new ins will be lost if you have buttons and the other thing is that in general from some of the experiments we did buttons tended like emails that look like form people tend to ignore them and think of spam and there's more practical but not really just like a research problem but it's really like we saw that when we introduced those things the adoption rates were like significantly lower but really the main idea was like all the new ins aspects of for example we started with all of the recruiters and Microsoft we're using these for to schedule meetings with people they were not to meet and we caná-- figure out their recruiting scenario really well but then we said to connect with companies outside Microsoft like salespeople and they were having a completely different kind of set of constraints and requirements and so all of that made us realize like really the way to start and build a project a product that can be used today is by having this like flexible architecture they the forms will be too constraining for all these different solutions yeah one thing that I have to say is that one of the things we worry and I was talking to Michael about this a few hours ago about how people understood the system from their perspective we were trying to be very clear that the system has humans in the loop and you know spirally automated probably not and so on but you know most people don't read all these details and I notice from this to it yesterday that some people are like you know this is I love it yeah I saw one tweet somewhere where people are like oh this AI system is like Skynet or whatever it's like so magically works and you know we're not even a Harvard Business Review like a more popular press article about this in addition to our KY paper trying to tell people how it works because we wanted to make sure the people understand and even when they sign up that it is a clear thing like oh this is humans in the loop that's something that I still worry to this day I mean I'm not longer involved in the project but I I still see as a limitation and I kind of wanted to close on that with the limitations of you know the the privacy implications for this you know obviously if you're a doctor you probably shouldn't be using this and how to signal that in the best way the other one is the cost of the humans in the loop that's obviously a cost that we had but over time that's reduces you automate more of one of the things but also the cost in terms of like the labor cost and you know one of the people in our MSR who have you know a Mary gray has been writing a lot about like the ethics of using crowd work and what we should do about these things and luckily at the time that we were working on this project we work within the constraints of the vendors and Microsoft approved us to use which all of them kind of make sure that their employees have like vacation and they have like quit paying salaries and so on which made our system more expensive but also perhaps more ethical but that's always something that I was kind of worried about and continue to be worried about the other one and just more in terms of the user experience there were two key limitations one is about the friction for invitees so sure I'm now delegate into the system but now this other person has to deal with the system back on for like that fills in some ways unfair and so the goal was that over time if everyone is on the system then the system could just look at people's calendar and do it but that's definitely a concern and and also one of the reasons why some people didn't want to use these because if you had a salesperson for example and you want to impress your customer you don't want to like say oh my system will take care of the meeting you want them to feel like they have all your attention and the other one is just like as much as we try the system to be flexible meetings are very complex and they were definitely like when it comes to like multiple people's in in a meeting at some point is just impossible like imagine trying to organize a 20% meeting this is like really really difficult and so we had to like set limits on I okay we are gonna reject those kind of requests but part of our goal was to really think about like the productivity space and how scheduling was one piece in this whole universe of the kinds of things that you could delegate through an architecture like this one and and kind of that's kind of the premise that where we started this project and what I think this could be you know a future way in which this can deploy it so I have a few minutes only but I was kind of talk about a little bit about the new directions that we are exploring in our lab yes always go to humans just to like evaluate the potential issues like that but that definitely was a problem to within I mean in some ways we were over cautious and at the beginning we sent everything to humans and we pay the extra just for that but we felt like we wanted to have a system that worked well and so yeah that's definitely a challenge but I surprisingly I didn't see that is a big big problem most of the things that we detected as failures were actually failures and vice versa so yeah yes well but within this we primarily focus on qualitative assessments of like we went over conversations full conversations and try to figure out is this an experience that we wish we could have and then from there like if the answer was no then we went and see where it fell and it could have fail in you know the automatic part or it could have failed more in the like our workflow sucks in this particular scenario or even in like the copy of the messages that we hire writers to come up with copy that was friendly and and also very direct and we had a project that we never deploy where we tried to match the person's style so if somebody was very direct and sure the assistant also tried to be very direct and short and vice versa if somebody was more verbose we will try to match that so we're really more focused on like the qualitative experience for users and trying to like reverse engineer that based on what we saw in the system happening yeah exactly yeah so we saw was had a scale basically and yeah surprisingly work well as a product that now people can use and but again it's expensive so yeah so I have only a few minutes but one of the things that we are exploring now and again this is all within the the view of how do we connect and people across we using this technologies is that you know we probably all have been in the past few years in some ways disappointed with the state of social computing or social media in particular from anything from like misinformation to you know potential addiction problems that people have to to social media and so one of the things that we wanted to explore in the in this new area of research is how to make these connections how to connect people in ways that are kind of less heavy more intimate and perhaps more authentic we all know like sometimes on you know social platforms people tend to present the kind of artificial version of themselves and that may create all sorts of different issues or even lack of engagement in these platforms and so I'll go quickly here just to show one of the things that we've been doing so kind of part of the approach that we're taking for kind of reimagining social computing is to take advantage of new platforms as a new space to kind of redesign things from the ground up and so new computing platforms like you know wearables is one of those where we are thinking of like if we were to design tools that enable us to connect over distance in a more authentic way what will devices like this allow us to do and maybe like things like you know bio signals are available here may allow us to do things that are different maybe more intimate and more close so I'll show you a video of a system that we've been exploring that X that kind of staying connected has never been easier you can capture what you see share what you think and make your voice heard but we don't often get to share from the heart introducing and animal uses file signals from your SmartWatch to generate an animal in ephemeral avatar representing your current artery your animal will change throughout the day based on your heart rate you compare animal with one special person it's like having them on your wrist animal gives us a whole new way to stay connected with a simple tap on your watch your animal will jump out and visit your friend when they receive it they'll be able to see it come to life then animal will jump back to your watch you can use animal to have an authentic and light touch connection with that one special person without having to take out your phone so we've been experimenting with the system is still very early we have a paper that came out that you become last month but basically the idea here is like how to design this kind of more lightweight experiences in this case we're almost thinking of it more like a mood ring meets friendship bracelet we're really the goal is not necessarily to be like accurate or scientific but it's more about thinking of DC bias beyond like self quantification and tracking physical activity and more of a playful way of connecting with that one special person beyond the like chatter that we see in all these feats and in boxes and stuff like that and so that's kind of still early but you know we continue to explore this space and the reason I mention it is because if you are interested in this area of you know using wearables for social connection we are a place W can do that kind of work the other part that we are exploring as well is you know how to enable these connections not just online but in the physical world and so we've been you know obviously inspired by things that are creative Snap is a very company focus on kind of creative expression and so things like Lego bricks is one inspiration that even from the times of scratch I found really inspirational and so one of the things that we're experimenting with is the idea of using again new platforms provide you opportunities is to explore things in new ways and this case will be the idea of how do you enable people in the same room to create connect and collaborate to build new things to build kind of a are sculptures with something as simple as a mobile phone without having to have something expensive and I'll show you a quick summary video it is an example of the kinds of things that people were doing I know you can see there yeah unfortunately there's no plane but the idea here is that you can take out your phone and write right there you see kind of you can place blocks in the physical space and then other people can add blocks to that as well and so you can collaboratively leave sculptures like in this room for example you come back tomorrow and maybe somebody added something or remove something and the idea really is that you know you can do these things across time or you can do it in the same time at the same place which we found that you know people found it more enjoyable when it was at the same time in the same place like the two individuals or three individuals being together there was more fun but obviously if you wanted to be more like productive in terms of like creating more complex stuff being able to do it you know whenever you want it was what's easier so we continue to explore this space in a variety of ways you know trying to use AR as a form of a creative platform for for connection and collaboration so you know I wanted to to close with you know giving shoutouts to a lot of my collaborators students and interns and and other people who are work with us across many of these projects and just wanted to close also with you know if you are interested in any of these type of spaces both AR that is collaborative and expressive and kind of social wearables we're hiring both interns and full-time people as well so with that opening two questions thank you I think it's really cold in Utica medalist a five-year we're there any follow-up yeah I've seen a lot of uptick from people who are studying computer science application and they're trying to for example I recently read a paper of a team in Europe that built a system that analyzes programs in scratch to detect like the paths which people are learning different like how people arrive to scratch and then learn different things different computational thinking concepts by kind of predicting where you're going essentially then I also seen people more like studying collaboration like there's a lot of people who study like collaboration on Wikipedia or you know systems like that and they use this data set to study that as well so yes this is a lot of feminists not hundreds of papers but this may be like dozens of papers abuses using scratch yes we've seen that both you know they think some introduction to computer science classes in college are using scratch in like the first lecture or so on but also we see a lot of parents who you know want to sit with their kids and they don't know programming either but they want to learn together and so that's another place we when I was working on scratch we also had a lot of companies who were interested in automation in general and they wanted to make automation easier for people and so I remember we talked to an IOT company that wanted to you know allow people to program their like nest like devices and using blocks place program language so I think we've seen the blocks based paradigm pop up in a variety of settings so that's another place where adults have been I can say like oh I programmed a scratch I mean I think it's as powerful as many other programming languages out there I mean obviously it has limitations but yeah [Music] yeah what's on the mains a quiz on the project the news I guess as I mentioned you know one of the things that we saw is that I saw her promise in this type of work I do see a limitations in terms of like scale and you know as I said at the beginning of my presentation I really tried to focus on things that can be deployed and be used for real in real world settings and that's always been the intent with all the projects we worked on doesn't always work and that was one example where I think we try as far as we could you know I think the different the confluence of both organization that you are at any given point and the system itself you know may not work all the time and so I think that's one of the limitations that we saw in terms of like what what else we learn I think we work in a number of price interested in news in general and I think in some ways that was before all these like fake news dilemma that we're dealing with right now and I think in light of that I feel like this type of work is even more challenging I remember at some point playing with the idea of like oh what if we were to like organize a gathering I can impromptu gathering and I think Michael one of his students did like this impromptu gathering in real world and then I saw this article about how like Russia did something like that were like they brought like pro-hillary and proton people in one place they paid them and they didn't realize that they were gonna be in puppets and so I think in general like I became a lot more maybe concern about the execution of these type of things in the real world deployment so I will so that's the main takeaway from me like I probably wouldn't want to do something as large I still think there is a lot of room for exploration and especially in places like Mexico where I didn't even try to do anything because I feel like it was too dangerous I feel like still not not there yet yeah I do think there are certain aspects of the language that makes it easier sometimes make it harder but for example there is a constant the idea of a sprite so like a cat can be a sprite and can have code associated with it so if you like a project but you just like the way say the cat moves you can download that project and take just that piece and put it in your project and so that makes it a lot more modular obviously it's not as modular as say like I know JavaScript or something that you can build libraries and stuff like that but we saw a lot of kids trying to be like library like things and some people posting projects that the whole point was just to say oh here is a bird flying and you can control it with the keyword and it's just that just for you to use it and so that's one aspect is on the system itself on the like language itself and the other pieces on the social in technologies that we built you know we use language basic basically we use like words that imply sharing a lot so you know first of all every project was downloadable you couldn't disable the download of your project now scratch is fully on the web so the community becomes such a central part of the experience that the kind of the language and the the community is one single thing and so that's not even easier so let you go to every project and it's almost like a Wikipedia article where you can see the inside very very quickly and so that has made it even easier to to remix so I think I will say about the social and the technical pieces of the system have made it really easy to remix that said three mixing itself was one of the main sources of pain in the community because moderation around like people complaining or was somebody else copying their work and so on became a really big part of it and so we had to have like an army of undergrads and hold people like helping us with moderation because there were things are really easy to eliminate they're inappropriate but there are like these tensions between two people who are like well who is right and like are they being mean to each other especially kids and so I think remixing was both a source of tension and and and one of the reasons we study so much around remixing was because we were trying to figure out how to solve that tension and so yeah that's a lot of things to explore there as well "
Q_4ZkMrr5Vk,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-09-16T20:46:05Z,"L08: K-Nearest Neighbors algorithm. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/Q_4ZkMrr5Vk/hqdefault.jpg,Josh Bongard,PT49M6S,false,100,0,0,0,0,yeah okay let's let's get started happy Friday everybody we got quite a bit of housekeeping to take care of before we get back to to lecture so let's talk about today and next week today more specifically this afternoon the Vermont complex system center is holding kind of like an open house or otherwise known as a flash mob research event these are informal events we hold every once in a while where we invite people to come in and learn a little bit about the research we're doing in the complex system center so if you've wondered about what your professor does when he or she is not teaching you can come in here a little bit about it we started noon it goes till four you can come for as long or as short a period as you like it's going to be over in feral hall which is on the Trinity campus and here's the schedule so no PowerPoint allowed this afternoon at any one time three faculty boot will be working at three different whiteboards in the building and you can sort of circulate and learn a little bit about what everyone works on professor Alamos all key is in electrical engineering he does research in smart grid professor scarpino the mathematician and does work on the spread of disease and these days particularly the Zika virus I will be talking about robotics one o'clock professor Heinz also in the ee department will be talking about smart grid dr. Spector is a heart surgeon over in the hospital he's an actual practicing surgeon but he also does some research on the side using evolutionary computation to guide surgery for atrial fibrillation so hearts that don't beat in a rhythmic manner very exciting research professor Dodds in the math department I will talk about all things Twitter two o'clock dr. bagra on the math department will talk about data science and network science professor Guerra van does brain imaging studies again over in the the hospital can you predict somebody's behavior by scanning their brains the answer is in some cases yes very interesting research again dr. Tillman is going to talk about computational finance mr. Abele will be joining us he's done a lot of work in developing equipment medical equipment will be talking about fitbit's for the brain dr. Danforth a professor Danforth in math also we'll talk about Twitter and more generally socio-technical sensing and professor Epstein here in the computer science department will be talking about one of the latest and greatest machine learning algorithms out there deep learning applied to audio data so we've tried to pack in four hours here a lot of interesting research they will be trying to talk about their research for for a general audience so no PowerPoint as little equations as possible should be really exciting you're all welcome and most importantly you can see the bottom item there free food so if you have some time in your schedule this afternoon come and join us okay so that's this afternoon looking ahead let's see where are we here we are Friday September 16th we are going to finish lecture for today on philosophy design and process and we may have time to start in on lecture 5 but I'm going to spend most of today talking about deliverable for so most of you hopefully are working your way through deliverable 3 which as I told you as a monster it's the longest deliverable of all 10 how many people least have started in on deliverable 3 okay excellent who is about halfway through or more that's good okay anybody's still struggling with installation issues no hands all right fantastic I think everyone who is still with us has managed to get Matt plot lib leap motion and what am I missing there's a third sorry numpy right so we so far you've worked your way through three Python packages in deliverable for which I'm going to talk about today I'm going to introduce the fourth and final package that you'll be using in this class site can't learn which is a machine learning package for for Python so deliverable 3 is due next Monday morning I won't be here on Monday which is when I would usually talk about the next deliverables so we're going to talk about it today deliverable for is due the following Monday the 26 so far so good okay next week let's talk about next week monday and wednesday object Jeff Springer will be here the CEO of Zemo corporation and he's going to be showing you a little bit of the alpha version of their educational robotics software later this afternoon I'll put up an announcement on blackboard where you can go and actually download it if you can try and download it and install it on your machine before class on Monday that would be that would be great if you can't that's fine this is this is sort of optional okay so Jeff is going to walk you through on Monday how to use the software and then on Wednesday he's going to do some user testing with you so last class we started talking exactly about user testing what is one of the big challenges about user testing in HCI acceptability how so why is it challenging because you can't just force you can't test it before it's it's deployed right my cell phone beeps when it's battery is low right when I'm falling asleep every night not very excitedly not yet okay I need some I need some IT help please someone come see me in my office hours we're going to figure it out okay one of the biggest overall challenges of user testing is measurement right so your user sits down they use your software for half an hour and they fill out a survey and they say yeah I was it was fun or I didn't really like it that much where you go from there right how do we actually sharpen our measurements and actually get some quantitative data to figure out what aspects of the software we need to improve your going to get to be on the receiving end of this next monday and and wednesday to get some first-hand experience i will not be here on friday so we are not going to have class on Friday don't come to class next Friday however I do have video lectures from last year so i will make v9 here live before friday so you don't have to come to class but you still have to watch the video lecture and there will be a quiz friday night on material drawn from that video lecture so you get to watch me on youtube rather than than in person so I'm good and the deal is if you let me do this I will bring you back something from Scotland there's been a request for haggis and Scotch neither of which i think i can bring but i will bring you something good from from Scotland as as a bribe Monday and Wednesday there will be no quiz but again deliverable 3 will be due on Monday deliverable for the Monday after that and then I will be back here with goodies on Monday the 26th all right we all good on what's happening today and next week good okay so let's talk about the deliverables anybody have any questions about deliverable three so far so good okay so let's talk about deliverable for and in order to talk about deliverable for I want to just take a step back and sort of recap where we've been and where we're going if I can let's see here I can okay okay so where are we and where we're going as you remember we started with the leap motion device right out of the box where leap motion has two infrared cameras and it's collecting pixel intensities in real time and then the leap motion software under the box is taking these pixel intensities and turning them into 3d co-ordinates so we're going from our two infrared cameras to 3d co-ordinates which in deliverables one and two you've been turning into a real-time visualization in matplotlib right so we've gone from pixels to coordinates to real-time visualization in deliverable three you're turning those 3d co-ordinates your packaging them into a numpy matrix and in this case we're working with 3d matrices what's the shape of this matrix what are the three dimensions are 35 rose for each of the five fingers four columns for each of the four bones in each of the five fingers and six stacks which correspond to the base and tip of each bone in each finger right so we're packaging stuff up all the way from pixel intensities to at least for one static frame 1 3d numpy matrix so in 1 and 2 we introduce the mat plot lib library we introduce the leaf sdk and one and two deliverable three hear you're getting used to numb I so numerical manipulation in Python in four and five deliverables four and five you're now going to be taking your three-dimensional matrix and transforming this into a single integer which is the computers guess about which of the nine which of the ten digits from 0 to 9 the user is attempting to sign right so you've got pixel intensity coordinates a 3d matrix is this 059 is it none of the 10 what actually is contained inside this this matrix so in deliverables four and five now you're going to be adding in some machine a machine learning algorithm and that machine learning algorithm is going to take as input a bunch of these and learn how to then take a new one of these and output a prediction all machine learning algorithms more or less have that one trait in common take a whole bunch of data and make a prediction about a pattern that is hidden somewhere in that that data okay in order to do this machine learning we're going to use the fourth and final Python library that we'll talk about in this class which has the very cryptic name of Psych its learn okay so how exactly are is this going to work in deliverable for the first thing you're going to do is set aside your leap motion device for a little while we don't need this anymore we're going to set aside our leap motion device and we're going to focus in deliverable for just on using scikit-learn with some synthetic data so data that's already been prepared for you and we're going to make sure that our machine learning algorithm is correct correctly making predictions once you've got that working and deliverable for in deliverable five the only thing you're going to do is remove the synthetic data and instead provide real data that's coming from your own hand so far so good ok so what machine learning algorithm are we actually be working with we are going to be using one of the very simplest machine learning algorithms out there called k-nearest neighbor or KN the reading for today is simply the wikipedia entry for the k-nearest neighbor algorithm that you can see here and i'm going to just walk you through the intuition behind this algorithm is anybody worked with k-nearest neighbors before No ok let's start with the data first then we'll talk about the machine learning algorithm there's a very famous data set out there called the iris dataset this was collected almost 100 years ago by someone in the gaspé Peninsula going through a field and picking flowers they were picking particular flowers iris flowers and for every flower that they picked they measured for features of that flower we're not going to worry about what those actual features were but they had to do with the length of the pedal and the width of the petal and so on and so forth you want to read about it so for each flower that they picked they wrote down for measurements these four features they then it was a botanist who did the picking this person knew which when they picked a flower which species did this particular iris belong to so iris is the genus which species was it turns out that in this particular field there were three species of iris again we're not going to worry what those specific species are we're just going to worry about the class so three species there are three possible classes that any picked flower can belong to so they picked the first flower they recorded the first four features and it turns out that that particular flower belongs to the first of the three species this is computer science so we always start counting from from zero they picked a second flower they measure those four features and they also determined in this case that flower belongs to the third species and so on and so forth okay they picked a total of a hundred and fifty flowers and that's what the iris dataset is and when you use scikit-learn one of the first things you're going to do in deliverable for is actually pull out that data set okay so online for for a here you're actually downloading this data set what do you think you will find in iris given your experiences with deliverable three so far if you were to say print space iris what would you get back a 3d matrix and what shape would that 3d matrix have a cube and what would be the length of those three dimensions how many rows columns and stacks for three 150 possibly you can go you will actually have a look in this in this assignment the way they actually do it is it's broken into one two dimensional matrix and one vector what's the length and width of that matrix or by three by one now we're just dealing with one two dimensional matrix here and then one vector for the classes one vector 1 matrix for the features and another vector for the classes what do you think the shape of the feature matrix is you got it right so we've got 150 rose and throughout this assignment we're going to eventually stop talking about irises and pedals and species and we're going to start talking about observations so each flower is an observation we pick one we have one item that observation in this case has four features associated with it so each row corresponds to an observation and each column corresponds to a feature what do you think the length of the vector is going to be it should be pretty easy not for we're storing the classes for a hundred and fifty flowers 150 right so we've got a vector of length 150 any element in that vector has an integer either 0 1 for 2 there's different ways that obviously that we could store this data but in the case of the iris data set that's just how it's how it's stored so far so good ok that's the data set let's talk about applying this let's talk about applying this K nearest neighbor algorithm to it in this case we've got four features I want you just for the moment to forget about feature three and four and I want you to imagine that we have a 150 by two matrix so let's imagine that they only measured two features of that flower rather than for the reason why I want to focus on just two features rather than four is because I can only draw in two dimensions I can't draw in four dimensions so let's create let's try and visualize this data set along one axis I'm going to plot the value of feature one and along the vertical axis I'm going to the value of feature too so I'm going to take some data out of here and visualize it for you this is what you're going to be doing in the first third of deliverable for let's take the first of our 150 flowers and I'm going to draw an open circle to represent class equals 0 so this particular flower belonged to the first of the three IRA species it had a relatively high value of feature one and a relat relatively low value of feature too I'm going to take out the next flower the second flower this flower happened to belong to claw the second class I'm going to pull out a third flower and that one happens to belong to the third class and I'm going to do the same for all 150 flowers i'm not going to draw the mall ok that should be enough thank for our purposes ok so let's imagine I've drawn all 150 flowers here how does the k-nearest neighbors algorithm work well now that we have this data set we imagine that some other botanists or some other non-experts go out into the field literally and pick a new flower all they know is that that flowers an iris they know the genus they don't know what species it belongs to they measure their told how to measure the two features they measure the length of the pedal and the width of the pedal and that particular unknown flower ends up at this point in the space which species do you think this flower belongs to which of the three species does it belong to you're running a machine learning algorithm in your head right now let's see how good it is si tu y si tu here's one two three four instances of c2 right most people if you show them the problem in this way the first algorithm they come up with is something that's very close to k-nearest neighbors right your intuition is that you're going to look at other flowers that are close to the mystery flower in feature space right flowers that had similarly long and similarly wide petals right there's many more c2s that are closer to our mystery flower then there are X flowers and that's pretty much how k-nearest neighbors works we take our two features in this case and the only parameter you need to supply to k-nearest neighbors when you get it running is how many neighbors are you going to look at let's say for a moment we set k equal to 5 we're going to start at our mystery flower and we're going to find the k-nearest neighbors which in this case is 5 so that's this one this one this one this one and this one and as you can see in this case the majority class among those five neighbors is c2 so we're going to predict that this flower belongs to the third species we don't know unless we go and actually ask an expert but we can use this data set to make predictions so far so good okay so a lot of new terminology here observations on the rose features on the columns class classes and predictions okay i just walked you through the intuition of k-nearest neighbors you're going to be coding it up in Python we just talked about two phases of a machine learning algorithm the training and the testing so the training is collecting all of this data and using it to set our machine learning algorithm testing is when we then take unknown we take new observations that didn't exist in the training data that have unknown classes or categories we provide that test data to our algorithm we see how well it does at predicting the actual class so let's say our mystery flower here actually did belong to the third species then our machine learning algorithm is doing its batting a thousand right one out of one so far what about what about this what about a mystery flower right right here if k is equal to five would we predict now for this one little hard to tell which ones are the closest ones right let's see there's a mystery flower right here this one is definitely close this one is closed this one is closed this one is closed this one is closed still there are three instances from c2 and only two instances from c0 so will predict again that this flower actually belongs to class to let's say in this case we were wrong it actually belongs to see one so now we're not batting a thousand anymore are we've got we've had two observations and we've got one out of two predictions correct right so we're at fifty percent prediction accuracy towards the end of deliverable for you're going to be looking at the end just at percentages so you know how well your machine learning algorithm is doing based on that percentage accuracy how close to a hundred percent can you get okay let me remove my to let me remove my two mystery flowers here what happens if K is very large how would our machine learning algorithm behave in this case how well do you think it would do better could be more specific I think would do a lot of words why because they would just take in to account a massive area that one has the work whichever one has the most data points in total it'll always predict absolutely right no matter where our mystery flower finds here we're casting a huge net right we're looking for a hundred nearest neighbors I haven't drawn a hundred so it take all of these so it's probably going to do pretty poor because it's always going to predict the majority class which whatever species of the three is most common in that set of 150 flowers so again well let's be a little bit more specific what would ok and then predict given this data in this case would it always predict c2 right there's four SI toos that's in the majority every flower you give it it'll say that's c2 that's see to that c2 as you can imagine it's probably not going to do very well right ok so maybe we don't want a very large K what happens that we have a very very small K how do you think k NN will do in this case also pretty poorly why or that like very close but there's that oneness exactly right so in this case the prediction what any prediction is going to rely on a very small piece of data right just your nearest neighbor so it's easy for errors to creep in so here are four SI toos let's put our mystery flower here it's slightly closer to see its clue slightly closer to c 0 so with k equals 1 it will predict c 0 but they're also a whole bunch of c 2 s nearby right so that that air that prediction is probably not going to be very reliable so now hopefully you can see that there is a black art to machine learning which is before you start figuring out how to set the parameters that influence how the machine learning algorithm is going to operate so what should k actually be it's hard to say a priori for any given data set so a lot of what you're going to be doing towards the end of deliverable for is fidgeting with k to find something that's good in the middle that will bump up the prediction accuracy how many of the unknown mystery flowers does it get right why is it possible to set an area so that okay so maybe instead of just looking at a fixed number of neighbors no matter how close or how far away they are we could look at an area instead that's a pretty good intuition what kind of area might you want to specify did you have an idea I was going to say something else I was gonna say a percentage of the whole data set just because like you said k equals 101 you work for 150 years of thousand for experiment okay more realistic so we could try that right we could say all right let's set k equal to ten percent of the entire data set is ten percent good i % fifty percent twenty percent so we still got this issue about finding the sweet spot but at least it's now robust to the size of the data set so the entire field of machine learning which has been going for about 40 years these are the kinds of questions they ask right k NN as you can probably imagine by now is incredibly intuitive takes about five lines of code to write it but it's riddled with problems it's known as a weak learner it's very intuitive but not very good at the other end of the scale is deep learning which is the current state of the art best machine learning algorithm that's out there you probably have a deep learning algorithm running on your phone probably right now tana miss cars all that sort of stuff is being set up with deep learning deep learning is much less intuitive much more complex but it gets away higher prediction accuracy than K nearest neighbors so we're going to start you off with KN n if you want you can replace KNN with something else from scikit-learn scikit-learn comes with hundreds of machine learning algorithms that you can use you're not going to be as assessed in this class on how accurate the predictions of your system are all that we ask is that your predictions are better than fifty percent better than flipping a coin you can do better than fifty percent by the end of deliverable for you're in you're in good shape in a machine learning class you would be assessed on how well your learner was predicting for this for this class we just want a pretty good one that's able to sort of figure out what you're what you're signing okay so what is the deliverable for deliverable for your again you're not going to be handing in code you're going to be submitting in this case not video but screenshots and we've already talked a little bit about visual design i've tried to create a visualization here where myself or the TA can very quickly tell by looking at this whether your system is making good predictions or not so one of the first pictures you're going to make is something not unlike this you'll see that we have two axes here these do correspond to the first two features of the iris dataset there are not 150 flowers here there are 75 so we take the original data set of 150 flowers we pull out 75 and that becomes the training data that we looked at here we take the second set of 75 flowers we hide the class and we then run you're going to run your KN on that second set of 75 and see how well your KNN does at predicting the classes of those 75 mystery flowers from the 75 flowers that you already have so far so good okay so you understand what the positions of are these 75 75 flowers what's the color represent here the different types of flowers right so for our purposes I don't know much about irises we're going to have read irises green irises and blue irises this second visualization now I've added the 75 mystery flowers so that 75 training flowers you can see they've got a black line around them the other 75 dots have colored lines around them what do you think the color of the edge of the lines around the circles represents so the color of the dot itself represents that flowers actual class which of the three species it belongs to what do you think the color of the edge represents the guess the prediction right so the the face color the center color of the circle is what the flower actually is or at least determined by an expert botanist the edge the color of the edge is the prediction produced by KNN so again the points that have black edges that's the training data we're not predicting the classes of the training points we're predicting KN n algorithm is predicting the colors predicting the the colored edges here how did can do in this case can you tell just by looking at the visualization how well KN is doing let me see if I can zoom in for you at predicting those 75 mystery flowers better than fifty percent that sounds about right how do you know you're looking at it ok what aspect of this visualization is telling you that k NN is doing better than fifty percent exactly so the points that are solid don't have an outline or at least have an outline that's the same color as the center what does that mean they guess right if there's a dot here like for example this one this particular flower actually is a blue iris but the KNN is predicting that it's a green iris right its prediction is wrong so at a glance in this case we can see how well k NN is doing in this case and again as I said it doesn't matter really how well it's doing as long as it's doing better than than fifty percent so far so good okay so this is these two screenshots will help you as you're going through deliverable for to make sure everything's working you should your 75 flowers should end up at these particular positions and with these particular colors so you should get this one perfectly right once you add on k NN to your training set you should get this picture this is for features just f1 and f2 what you're going to be handing in is just a single screen shot and that single screen shot is not using f1 and f2 but using just f2 and f3 so you're going to be submitting to us a picture of colored dots the positions of those dots are going to be different because the axes are no longer going to be f1 and f2 they're going to be f2 and f3 and we're going to look to see is it is it doing the same thing as our KN n algorithm when supply with features two and three we're all working from exactly the same data set the iris dataset we're all going to use exactly the same KN n algorithm with k set to I think actually it's set to 15 in deliverable for so we'll be able to see whether your Canon algorithm is working correctly why am I not asking you to submit a visualization of K&N working with all four features because you'd have to draw four dimensional picture right even if you could we wouldn't be able to interpret it so we're going to stick with with two we're hobbling K&N because we're hiding two of the the features from it right so we're going to use visualizations for now just so you build an intuition for how K&N works and whether your canon algorithm is working well or not then when you finish deliverable for you're going to throw away the visualizations and run kan with all four features and what do you think will happen it'll probably work a little a lot better so now you're going to be just looking at percentages so with two of the features i think you get about 80% prediction accuracy when you give it the other two features it'll jump up to something else right what happens if one of these features was just random numbers how do you think KNN would digest a confounding feature something that has no real data in it what would happen to the prediction accuracy prediction accuracy LuAnn drop exactly so the in deliverable 5 going forward you're going to have to be paid you're gonna have to be vigilant and look for these kinds of things are you giving the candid algorithm data that it can actually use use to make successful predictions about is the user actually signing 0 1 2 3 and so on or not and how do you how do you know so for now you'll be able to rely on visualizations as we go forward the only thing you're going to be able to do is change this matrix and this vector and look at the prediction accuracy does it go up or go down ok any questions about KNN all right I will just give you a teaser trailer for deliverable 5 which is coming up as i mentioned deliverable for you can set aside leap motion you don't need it we're just going to run kan on the iris data the deliverable 5 you're going to be bringing back data from leap motion what do you think you're going to supply to KNN in deliverable 5 you'd be supplying data from the hand but what data sorry the point the points I at each end of the bone right so in deliverable three you're creating a 3d matrix a 5 by 4 by 6 matrix k NN doesn't take 3d matrices it needs observations on rose and features on columns so in deliverable 5 you're gonna have to take your 3d matrix and turn it into a 2d matrix that KNN can digest how do you think you're going to do that what will go on the rows and columns in that case what are the observations so if we're going to try and train KNN to recognize as someone signing three or six or nine what's going to go on and on a single row in that matrix the figures of those fingers and bones and and we've got the five fingers for bones how many features how many features in one frame of data from leap motion what does KN going to need to have to be able to predict is this 0 is this 6 is this none of the above 120 right we're guy we have to take 120 3d co-ordinates 5 times 4 times 6 line them all up so you're going to have 120 columns right 120 features and the output is still going to be a single integer what is the class that is going to be associated with those 120 features 0 through 9 right this is going to be cannons guests I think they signed 0 or the user is going to say i'm signing 0 learn that this is 0 i'm signing one learned that this is 1 this is 2 this is 3 you're going to teach KNN what 0 through 9 in American Sign Language is it doesn't know right okay so we're going to have 120 eight columns and we know what's going in our class vector what's going on the Rose what's contained in a single row just what's that an observation right in this case it's observation is equal to one frame from the leap motion device right one capture is one observation so if I am you've finished deliverable to so if I'm signing zero and I bring my secondary hand into record it's going to start spitting out data files and these are all going to get a class of zero so I'm signing zero I'm teaching k NN that this is zero let's say that when i bring my secondary hand in and out of the recording frame we grab 119 frames of data you save out 119 data files you're going to get 119 rose with a hundred and twenty columns and those hundred night the first hundred nineteen entries in the categorization vector are going to be zero now I sign one and I bring I start recording and I spit out 327 frames 327 instances or 327 rows of class one right so you're going to generate in deliverable five a huge training matrix which contains a whole bunch of zeros a whole bunch of ones a whole bunch of twos and so on right run KN KN n doesn't care whether these are flowers or frames from leap motion all it knows is features observations and categorization and classes now once you've collected that data you turn everything off you turn it back on instead of loading in the data you're just loading in your K&N algorithm some other user comes along and they do this over the leap motion device and we're going to see what you're KN n algorithm correctly predicts the current user is signing the digit 1 make sense ok that's deliverable for and a teaser trailer for deliverable 5 any questions about that ok so we've got a total grand total of five minutes left so let's go back to actual lecture we've been talking about design as it relates to HCI we've been talking about the fact that you're neat you're going to need to do a lot of user testing in HCI you're going to get a taste of this next week so we got to where did we get to we got to we got this far last time one of the things that's unique about HCI compared to software engineering is this fact that we're doing all of this user testing we're not just trying to prove that our functional requirements are correct the software correctly predicts hand gestures and so on those are functional requirements they have to do with what exactly what functions exactly your software should instantiate but how these additional non-functional requirements that are kind of global and kind of vague aspects of the software is it accessible is it usable is it acceptable is it engaging and and so on we ended last time by talking about this issue of acceptability let's move on to the fourth and most subjective aspect of interactive systems design which is engagement right so usually an HCI you're trying to create an interface that's engaging to the user it might work just as well as somebody else's but your user prefers to use your interface over your competitors interface because they like it but that's kind of a vague thing how do we act how would you actually measure engagement we could measure how long someone agrees to use our software there are other things we could we could do right at six this is the most subjective of the other of all the four non-functional requirements we've we've looked at well what does it feel like to be engaged with a good book a good movie a good conversation right if you ask someone the language they tend to use is usually very similar has these metaphors about being drawn in time seemed to fly by when I stopped I had no idea how much time had had gone by this particular psychological phenomenon has actually been studied quite a bit in in psychology and in particular by majalis chicks n mid hollier you will not be asked to pronounce his name on the quiz tonight I I promise he wrote a fantastic book on this called flow if you're interested in this this topic it's a great great book to read he interviews people from all walks of life who were at the top of their their game and they all described this sort of optimal experience where time seems to kind of slow slow down they have plenty of time to catch the ball or do whatever it is that they're they're doing you've all probably had the same experience with a good book a good movie and perhaps a good video game or a good piece of of software right so this last one is really hard to measure quantitatively this is usually where a good survey come comes in handy right what kind of language does your user use when they described described that okay other things to look for in terms of engagement when someone uses a piece of software there's lots of other things going on some of them have to do with identity does your does your platform kind of mesh with your sense of self great example of this was the the Mac ads from a few years ago people remember these I'm a Mac I'm a PC sorry more than a few years ago right I'm showing my age here I'm more than a few years ago who wants to be a pc right look at this this poor guy right better to be young and wearing a t-shirt and very laid-back right that's that's what Mac is all about right the advertising campaign was saying this machine is a reflection of your identity right very very clever all right whatever injured face that you're designing does it have a narrative right does it have a flow if you're trying to teach somebody ASL and they say I don't know where I am am I at the beginning am I going back am I going forward what does it actually mean to navigate does your user have a sense of where they sit in the landscape of your your system right are they actually making progress how do they how do they know this aspect of immersion right sometimes this is literally immersion if we start talking about augmented reality and mixed reality and Google glass or Pokemon go right you the virtual world or you are actually being immersed in this illusion that's being being cast ok I think we're at a time that's the end of lecture 4 so again please come and join us at the flash mob in feral Hall this afternoon enjoy Zemo next week i will be away i will see you on monday the 26 when it will start on lecture 5 you have a quiz due tonight have a good weekend 
N_8rjR_r15w,27,"Specialization program: Interaction Design
Course 3: Social Computing
Week 3: Crowdsourcing
By Universidad de California in San Diego",2017-03-09T02:26:25Z,"Social Computing, Week 3: crowdsourcing",https://i.ytimg.com/vi/N_8rjR_r15w/hqdefault.jpg,Mao Imagine,PT50M18S,false,333,2,0,0,0,"in this video I'd like to talk about crowdsourcing crowdsourcing is a new form of organizing work that is mediated by Internet services and it connects customers and workers through the power of the Internet the prevalence of crowdsourcing is new it's an idea that's been around for over a century as James Surowiecki tells in his book the wisdom of the crowds in 1907 john galt in' asked a large group of farmers to be able to estimate the weight of an ox now what's interesting about this is that no individual farmer guessed the way it exactly yet amazingly the average of all of the farmers estimates was extremely close to the weight of that ox you may have heard other similar examples of being able to guess the number of jellybeans in a jar or other kinds of crowd estimates of behavior sometimes markets and other systems can work in a similar way what Surowiecki points out is that in order for the wisdom of the crowd to be able to work effectively you need to be able to gather a large number of independent estimates so part of the key of getting the farmers votes or the jellybeans guessers is that each person guesses without revealing to anybody else what their guess is and the reason for that is that in general people's estimates for many tasks are uncorrelated that some people underestimate and some people overestimate and that absence of correlation is what enables the averaging to be able to work another reason that this kind of averaging can fall short is if people tend to systematically make errors and this is a subject that we're going to return to towards the end of the video so what we've seen with crowdsourcing which is a term that comes from Jeff house wired article back in 2006 is that there's lots of things that distributing small tasks to groups of people who are each independently making estimates or coming up with ideas or doing other kinds of work that this aggregate of groups of people can be really effective for things like prediction markets on who will be the next president or who's gonna win the basketball tournament or things like that it can be used through systems like InnoCentive to be able to solve difficult problems or being able to collect and filter information on the Internet this is a lot of the user-generated content that we see and in many ways this can be used at democratizing production enabling a larger number of people to participate in creating then otherwise would be possible one of the most well-known examples of crowdsourcing is the Amazon platform called Mechanical Turk and the reason that it's called the Mechanical Turk is that several hundred years ago there was a traveling show of a supposedly automatic chess playing machine and it was called the Mechanical Turk it would on its own play chess against the royalty in the places where where it visited the trick behind this automatic chess playing machine was that in fact it wasn't a mechanical device at all under the hood it was powered by people and that's exactly what we're seeing with Amazon's Mechanical Turk it's also powered by people so there is an aggregate behavior and then underneath there are people that make it all happen you know they the Mechanical Turk of old was one person underneath the table hidden in a box the new Mechanical Turk is aggregating the work of thousands of workers online in many cases with this work you see things like we'll pay a nickel to be able to find out whether these two images are of the same product Amazon had many such cases like this where their large warehouse of products means that there were many data integration and data cleaning and other kinds of organizing tasks that needed to happen much like they've done with other services such as storage and cloud services they realized that this labor market of being able to ask many many simple questions was something that would be more broadly useful in the world and the Mechanical Turk platform has been really widely used including by researchers while Mechanical Turk is easily the most famous of crowd sourcing platforms it's by no means the only one and the micro tasks pay a nickel or a dime or even a dollar to get something done model that Mechanical Turk offers it is not the only one another wonderful example is oDesk where in this case you're hiring a specific worker to do a small task so the oDesk marketplace has people who profess skills at all sorts of things and then you can hire one of those individuals to be able to do some work so a Mechanical Turk these are anonymous in in oDesk we know who they are and they have particular skills but you don't need to use money to be able to make crowdsourcing work in fact I think some of the most interesting and creative uses of crowdsourcing are where other incentive systems are in play one that you've probably used and may not have even realized that it's a crowdsourcing system is the reCAPTCHA system designed by Luis von Ahn and colleagues and what reCAPTCHA does is the initial challenge was that Yahoo needed a way of preventing spammers from signing up for email accounts and other Yahoo based accounts through BOTS so they needed some tests to tell whether the entities signing up for an email account was a computer or whether it was a human and so Luis and his colleagues created CAPTCHAs or computer automated tests to tell people and computers apart and with these systems we we just realized that you know initially they would just distort text like you see on the screen here but there are many other things that you can use for the content of a CAPTCHA so for example Louise realized that you could use old New York Times articles that needed to be translated you can feed a word or two at a time to the reCAPTCHA system and the crowd when they sign up for account that need to verify that they're a human will do the transcription work as part of that disclosure that there are humans so to prove I'm human I tell you what the text says and then as a byproduct of that the New York Times gets translated very cool now of course you can't do this with just one worker you need to be able to ask multiple people about the same information and you need to be able to compare the responses of multiple different individuals more recently what we've seen are things like Google Street View has all of these street signs and house numbers and other information that they'd like to be able to extract and those are now handed off to recap ssin so the next time that you sign up for an account you may see a street sign or an omen of your time snippet or something else like that these are in essence computer vision tasks that are just a little bit too hard for computers to do currently and we hand those to people what's interesting is that then this massive store of human generated information becomes something that we can use to be able to train machine learning algorithms to do a better job in the future and so there's a constant ratcheting that is happening where the machines are getting better by virtue of the insights being delivered from the crowd workers so we've seen several different models of crowdsourcing there are crowdsourcing systems that are extremely simple very very fast tasks others are much more creative you know you can think about Kickstarter and Wikipedia and many of these other systems as being examples of more creatively oriented crowd work now also we've got things that are oriented towards novices anybody in the world can in principle tell us what the particular CAPTCHA says or for vision impaired users we can offer an alternate modality with the Mechanical Turk tasks we're offering a nickel so things like Amazon's Mechanical Turk and CAPTCHAs are very fast very quick very easy broad labor pool available and other things maybe more expert based and maybe more creative and so for oDesk for a graphic design task or something like that we may have more complicated opportunities and I think a lot of the interesting crowd work heads off in this direction of more creative expertise and an awesome example of this is the work of Aaron Koblin and colleagues on the Johnny Cash project I believe this was Johnny Cash's very last music video and what he did is that they created an online painting interface much like a rotoscoping system where you could paint over individual frames of a music video for Johnny Cash's last song different users each contributed individual frames you can see those along the bottom of the image here and those were then composed together into a wonderful crowd created music video and so this was both a way to create a really cool visual effect it was an outlet for creativity and the fact that all of these different users were doing work that was showing up in one video together was a way of giving tribute to the way that Johnny Cash touched so many people's lives in fact the very first concert I ever went to when I was 5 years old as my parents brought me to an outdoor I think free Johnny Cash show and it was amazing and so he's this enduring music legend and the crowd created video expresses that extremely well as you might imagine and I think what with the Johnny Cash project shows really well is that people's motivations matter a lot this paper by Chandler and Kapil nur shows that when you give people a reason to do their work online they do much better and the result may not be totally shocking but it's a really valuable reminder that when you situate the reason for why we're doing what we're doing that can be extremely powerful and the example that they show in this paper is one of being able to find tumors in medical images when crowd workers are told that they're simply locating objects of a certain style the quality and time on task is much lower than when you tell people hey you're gonna find tumors and this is making a real difference and then the workers do a much better job and so for a variety of reasons whether it's a creative outlet tribute to an artist the ability to potentially save or improve somebody's health when workers are motivated and when the motivation is real the quality of the work is much better so what we saw with these simple tasks here like Mechanical Turk and CAPTCHAs is they're kind of one chunk of tasks all on their own for more complicated tasks for example here's a paragraph can you shorten it or improve the grammar in writing well there there's several operations that need to be done and when online workers are asked to do more complex operations there's a couple of challenges as Michael Bernstein and colleagues point out you've got the eager beaver somebody who logs on online they want to show they know what they're doing and so they'll make lots of edits or conversely you can have the lazy turk or somebody who wants their nickel for doing as little work as they possibly can the way that you can combat both of these is to add a little bit of structure to the process and one of my favorite examples of this kind of structure is Michael Bernstein and colleagues work on the find fix verify design pattern and the idea here is that if you ask somebody to improve a paragraph you may get junk or you may get well junk threw over enthusiasm but if you say hey locate an area that could benefit from improvement and then to the next worker you say here's an area that could benefit from improvement could you improve it and then to the third worker you can ask here is one version here is another version which two of these versions is better by separating out the finding the fixing and the verification the quality of work that's done becomes higher and this is a general purpose pattern that can be used for lots of crowdsourcing tasks there are of course others and so what I mostly bring this up for is if you think about crowd created content in your own design work one way that you can improve the quality of what you get back is by adding a little bit of structure you know it's not the case that you can just magically throw any task out to the crowd and get brilliant work back it just doesn't work that way in the Johnny Cash project we saw the structure being added through the rotoscoping tool itself people didn't have open-ended painting it was a very specific task in Michaels work with fine fix verify the structure is being enforced procedurally and both of those can improve the quality of the output but we shouldn't that just because things are going out to the hoi polloi that the quality of what comes back is no good or anything like that in fact as we see with the folded system crowd workers can actually produce scientific insights that are novel in the entire world and I love this research project because it illustrates I think a couple of important points fold it is a system where anybody around the world can use a game-like user interface to be able to specify appropriate protein foldings so in a way you're asking crowd workers to do tasks that would normally be required of somebody that's a PhD in the Biosciences without that kind of training some other training needs to happen instead and so our first takeaway from the the Foldit system is that crowds can do extremely high-quality work but you do need to train them up a little bit and so with the folded game there's a wonderful set of training and levels and other ways of onboarding new users to be able to learn the rules of the game quite literally so you learn how proteins can and cannot fold another thing that we see with folded is that some aspects of this task are both boring and automatable and in those cases we have an algorithm that does the the local optimization of the of the folding structure so users do the high-level parts of the of the 3d task and then the computer helps and does the final polishing and this is an example of what we in the design lab call human and technology teamwork that by using human intelligence and machine intelligence smartly combined you can end up with something that is of higher quality than either of the two alone the last example that I want to show today is the work of Walter lisicki and colleagues on real-time audio transcription currently to be able to have a class like the classes that I teach at UC San Diego to have a class like that transcribed requires advance notice and somebody who's trained as a stenographer and the wages can be several hundred dollars an hour and what that enables is that students who need the aid of captioning or transcription because of limited hearing can now participate in classes and so this is great however the burden of being able to do this means that it can only be deployed in particularly high-value situations if audio transcription were easier and lower cost and more on demand then we could open up more and more of the world to the hearing-impaired and this is what what Walter ins colleagues worked on is by farming out the transcription tasks to the crowd we're able to for a much lower cost get something of comparable quality much more quickly and in an on-demand way now I don't know about you but I can't type on my keyboard as fast as a court stenographer can so what we're not doing in this system is that scribe doesn't ask its users to type the whole darn thing you can't type for 50 minutes of a class and keep up what scribe does do is that you get a little snippet that you transcribe and to make the handoff work between different peoples so the first person does this snippet the next person does this snippet the next person does this snippet there are several design cues that are necessary to make this work for example there's an audio cue where the volume gets louder when it's your turn to type but you can hear the audio right before and right after also speeding up and slowing down and several other things can improve the quality of the transcription and this really underscores the fact that if you naively give a task to the crowd you may get back total garbage but that through very careful design like Walter has done here with and Michael did with fine fix verify and the other examples did in each of their domains you can end up with something where the results are actually a very impressively high quality so I wanted to loop back to where we started if we think back to John Galton's ox the magic of the crowd generated gas was that the average produced something that was better than every single individual guesser it doesn't always work that well but when the airs are uniformly distributed the central tendency comes out and reveals the correct result but let's think about this from an individual gassers perspective we know that an individual guesser is likely to be biased but it's hard to know a priori in which direction so if I'd like to know how much the Ox waves are how many jellybeans there are in the jar my guess is unlikely to be all that good edy Valen is colleague how pashler here at UC san diego figured out something very clever what they realized is that if you take an individual guesser and you say now assume your guess is wrong in that case what we can do is you can ask that guesser to make a second guess and when that guesser guesses a second time for the reason of their guest being wrong so if I were to have for example the the jellybeans in the jar I've got a jar full of jellybeans I might say well I made my guess assuming there are ten jellybeans in the in each row along the jar but what if it's not ten what if its twenty well then my guess would double and then you can ask me to do the same thing so I assume your second guess is wrong you know what might be a reason for that and make a third guess and when you averaged together all three of these guesses in many domains you can end up with a mean of those three covers about half of the distance between an individual guessers error and the crowd and the so if we have you know the the right answer here and our crowd guessers guess all sorts of different stuff an individual guessers first guest may be erroneous their second guess often balances that out and then we add the third guests in there too and the aggregate of these three is often quite good and so what this tells me in aggregate when we look at all of these effective systems for crowdsourcing together is that design is critical to crowdsourcing success and what I'd like to close with today is the insight that we're seeing an increasing dominance of the Internet of Things that many of the services that are exciting right now are things that combine the world of atoms with the world of bits and the world of culture this can be the pop up food truck that announces where it's going to be on Twitter this can be Etsy where we have people all around the world creating handicrafts and vintage items and then selling them through an online portal this can be many common elements of political life that are coordinating large numbers of people through an online system and so what we really see is that the Internet of Things is really the Internet of social and technical elements working together and so the idea is that you've learned in this crowdsourcing video are going to become increasingly applicable as the computational world and the everyday physical world merge more so that's our video on crowdsourcing and we'll see you next time in this design specialization we've talked about crowdsourcing and social computing but what is crowd work actually like what's it like on the ground today we're here with the real experts to give you a window into crowd work with us today we have Lilly Rani Lilly is a assistant professor of communications at UC San Diego and joining us by Skype is Rachelle LaPlante Rachelle is a worker and advocate on Mechanical Turk and other crowd sourcing platforms and they're gonna share with you what crowdsourcing is really like I think Scott thank you shelf for joining us today so I thought we could start by talking about how you got started on Amazon Mechanical Turk I got started in about 2007 and I heard about it from a friend who was working at Amazon at the time and I just kind of mentioned it as something that was a project that he'd heard about it at work so I checked it out and got interested in doing some of the tasks in my free time just to make a little extra spending money here and there and then as I got more into it and time went on and my situation kind of changed I got into it more full-time and more seriously and kind of haven't developed it into more or less a full-time income for me as well as the work I do helping other workers and helping requesters and helping everyone who's involved to learn more about the system and make the best use of it as they can great and could you talk a little bit about what your involvement in the Turk community is in addition to being a worker I am also I'm an administrator and moderator on one of the major forums MTurk grind I'm also a moderator for the reddit community which is slash MTurk and I also just kind of help out whenever questions come up for requester has a question about setting up a hedge I might get an email from them about that just kind of help me out wherever wherever I can and whatever comes up and the enter community fantastic so you've talked to a lot of different workers you've been in the community for a long time and you're part of the communities that make Mechanical Turk work behind the scenes and behind the platform yeah I'm thinking when they think yes and I also keep up on all of the a lot of the new research and studies and other things that come out on Twitter a lot of it the discussion happens on Twitter so I'm pretty active there on Twitter too am I talking about mechanical torque down the issues about crowd work there would you like to tell us your Twitter handle it's at Rochelle ROC just so you can go to one source of knowledge about Mechanical Turk you can find online after your course is over um so could you give us some background on who's doing crowd work why they're doing crowd work and where they come from according to the most recent research that we have about 80 percent of Mechanical Turk workers are from the United States the remaining 20% are mostly from India there's a few from other scattered countries around the world but the majority are from the United States and people Canada credible for all different kinds of reasons it might be because someone's laid off and they're doing this work in between finding a new job they might be a caregiver for someone at home and so in their spare time in between being a caregiver they can do some mechanical shark work other people have situations where they've been able unable to find jobs in their community because of discrimination because of disabilities because of their past background so it's kind of a wide conglomeration of people who come to Mechanical Turk different reasons and it really fits into their lifestyle and their situation at the moment what do you find motivates you and other truckers that you've spoken with especially ones that stay in Turk for a long time I think the ones who are more serious about it and who stick around and who do make a job out of it are primarily there to earn money I'd say that the majority of the people I talk to are there who are working are there for earning money to pay for rent for food for bills there's a lot of people who are there to pay for healthcare related expenses loans debt student debt all of those kinds of things is a major motivating factor great that's really helpful so what are some examples of tasks that you've done on Mechanical Turk in your time working systems there's all different kinds of tasks and it changes every day there is everything from audio transcription there is tagging photos there's sentiment which is looking at a phrase or a sentence and deciding if it's positive or negative toward a company or toward a specific idea there is writing projects where someone needs some text written to describe a product maybe there's editing hits or you can edit some written transcription or written edit written text and there's also a lot of academic surveys and studies where University academics will post surveys and studies that you can complete to help them with their research ok so how do you decide what tasks to do when you sit down to work there's some requesters who post frequently and so I definitely have a list of my favorite requesters that I go and look for to see if they have any work up or any work available I also look at the price that someone is paying and see if it's worth my time if it's if it's a job that I think is worth my time for the money that they're willing to pay I also look at things that I'm interested in find jobs that are particularly interesting to me or they sound like an interesting topic and will gravitate toward those I think also the last thing improv maybe the most important is that I look at the requesters Turk opticon rating which is a rating system that workers use to review past requesters right about which ones are safe to work for and which ones may not be so safe to work for so I look at those ratings and make sure that whoever I'm choosing to work for has a good history or at least is brand new with no history and has potential to be a good requester mm-hmm um since you mentioned Turk Opticon do you advise or how would you advise requesters to be good requesters with Turk opticon and with workers I think the requesters who get the best Turk Opticon reviews are ones who pay fairly and to communicate with workers if a worker contacts them with a question they're responsive and that they pay fairly and on-time and they don't take an excessive amount of time to pay they pay you know rather quickly and fairly and if they approve work that's fairly done and don't reject to work without having a valid reason for rejecting that work the having a requester communicate on Turk optic on make a difference in the comments so we do having them come respond to reviews for example yeah I think as long as the requesters communicative in some manner whether it's commenting on Turk octagon in the reviews or emailing someone if you if you email the requester with a question and you get a response no matter which method the requester uses to communicate I think having some kind of method of open communication where you're getting your questions answered is really helpful okay so as you're mentioning the importance of open communication between requesters and workers what are what kind of advice would you offer to people to be good communicators with workers more generally what kinds of tools platforms techniques and strategies would you recommend I think that what is really successful is when a requester comes to one of them Mechanical Turk forums there's a couple different forums the workers are on and most of the forums have a section for requesters where the requester can come and create a post and introduce themselves and then actively interact with workers there who have questions that's probably a lot more of an accessible way for requesters to answer questions there rather than dealing with incoming email and answering the same questions over and over and if it's all in one section of a forum that's really helpful so I think that's a really good tool to use for requesters to improve communication with workers ok so concretely if I were an employer and I was wondering do I just show up at the forum and make a general post and say hey where do I go or is there a way to find out the right place to communicate usually how how it works I know on the forum that I'm an administrator upon MTurk grind when you first sign up and you come you see right there there's a section for requesters and you click on that and then there's an introduction to new requesters and it gives you instructions about how to sign up and how to get a little flare badge on your name that shows that you're a requester and how to create a post as you introduce yourself and introduce your work um can you talk can you talk about the range of forums that are out there just name some of them so people have a sense of the variety of spaces where that workers have set up to help each other out communicate with requesters make Mechanical Turk work more generally sure yeah there's lots of different forums and they kind of all have their own little personality and different groups of people who are there and different sets of rules and things like that so you kind of have to go to the two different forums to see what works for you and what helps you most the one that I'm most familiar with is emptor grind and I also mentioned the reddit community M Trek forum is another one and trick-or nation is the oldest the oldest forum so those are probably the major ones they're the most active okay great thanks so you've been working on Mechanical Turk and done a variety of tasks over the years of what kinds of credentials do these platforms offer for tracking workers as they've been building up experience built into the forum into the Mechanical Turk or built in the platform into Mechanical Turk platform there's some qualifications that are just part of the system where once you reach a thousand hits completed or 10,000 hits completed you get to reach those levels and so that opens up more work for you to do there's also a master's qualification on Amazon which is granted to workers who Amazon has supposedly said are the best workers and so if you have that master's qualification you're also able to access different set of hits and then in addition each requester who comes can set up their own qualifications if you're a new requester to the to the community you can come and decide that you want to set up a test and all of the workers can take the test and those who pass the test are then granted access to your work so that's a well that's a free tip to any requester to set up their own qualification test like that or qualification system to get workers that they trust um so those are the kinds of qualifications about what workers are doing on specific tasks but could you talk about some of the other skills that workers have behind the scenes that Mechanical Turk actually be a viable place to do a lot of data processing and power interactive technologies sure yeah in addition to those climate qualifications there's a lot of things that workers do that are unpaid and that you just do on your own time to help yourself be more productive on the floor on the platform and some of those things are maybe using different browser extensions to you know get to get work done more efficiently or more quickly so it's learning how to use those kinds of extensions some workers use browser scripts like Turk opticon is one of them that you can have on your brother in your browser to help you view hits that are reviewed differently other workers use something called hit scraper which is a different way to organize the hits that are available and you can see them differently it's a different layout so there's several different scripts and browser extensions that workers used to differently view the hits and to organize the hits and to accept the hits so a lot of it is learning and taking the time to learn how to use those tools and setting up the system to work most efficiently for you a lot of truckers who work full-time or work seriously have two or three different community compared monitors open maybe one for active hits they're working on one for a forum that they're monitoring one for Coptic on reviews one for a chat group they're in so setting up your system is something that is kind of unpaid work behind the scenes that helps you really improve your efficiency Wow so as you were describing this two-three monitor setup I was imagining stockbrokers of the Bloomberg terminals or computer programmers you know hacking in the darkroom it doesn't much different it doesn't look much different who builds the scripts and who keeps the forums going it's all the community members it's all the truckers who want to contribute and want to help out others everyone kind of has different skills so some people are great at writing scripts and helping keep those updated some people are really good at helping on the forum monitoring and moderating the forum everyone kind of has different skills and different interests as far as what keeps the community going and what helps the community so everyone pitches in differently we've been talking about a lot of the skills talents and energy happening on the workers side that's not at all obvious to requesters are there other things that are happening on the workers side that you think requesters future employers should know about it'll help them be better requesters use Turk in a way that is great for workers and for the projects that employers are forwarding I think the best thing to do is to communicate with the workers and then also make sure you test out your hits that you're posting test out the work that you post you may have an idea what you're posting and think it's all perfectly worded and clearly understood but then when workers get it they interpret it differently they don't understand the directions the pay doesn't seem to match up with the amount of work required so I think testing out a few projects or a few hits at a time before you kind of jump in 100% is a good idea having a couple workers look over which you're testing out might be a good idea there's a mechanical trick sandbox where you can go and test out your hits and see how they look before you actually publish them so using those kinds of tools and those kinds of ideas is a really good idea and you can also post on the forums and say hey I'm a new requester and I have this hit that's available what do you all think if you have any suggestions for me before I post this and so you can get the worker side the workers input of what workers think about looking at it and what the questions are going to come up to kind of prevent a situation where you can post a task and then there's some misunderstood instructions or something and you don't get the results that you want okay that's fantastic so King just to make it really concrete for people can you tell us some stories about what happened when well-meaning requesters didn't do this yeah there's been situations where there was one not long ago where a requester wanted email addresses found from a list of blogs that he had and so he posted the list of all these blogs and workers went through looking for the email addresses and in the instructions he said if you find an email paste it in and submit it there was no instructions what to do if the blog did not have an email address on it or if the blog only had a contact form so for those jobs some requesters sent or some workers had emailed him saying what do we do about this and receive no response so for the ones that weren't bad we typed in not found email not found there's no email available and all of those hits were rejected so workers were pretty upset about that because we had searched very well we've done the work and email just was not available on the blog website so workers felt like that we should be paid for that work because it was time and effort that we put in and we followed the instructions and did what was what was instructed in the in the hit but the requester felt that because no email was found it was a failed hit and it was rejected so that's some situation where it could have been prevented where if the requester had put into the instructions that typing and not found was acceptable or if you don't find this I don't submit anything or some kind of clarification what to do if those email addresses were just simply not able to be found that would have really helped and also responding to all those initial emails asking what should we do would have really helped that brings up a really important problem I see in Mechanical Turk so one big problem I see in Mechanical Turk is when a requester puts at work and they get work back that doesn't look like what they thought good work would look like they assume they can assume that it is a worker who is just slacking off or someone who's trying to spam just to get money or somebody who doesn't have the skill to interpret their task so what what would you recommend to requestor to avoid that pitfall of misjudging the source of error what should they do when they see work that doesn't look like what looks like good work to them I think monitoring the work as it's coming in is important when they first get their first results coming in may be publishing a very small batch to begin with to see if I test out the waters and see what it's what the results are gonna be looking like and then if they start getting a lot of results that are rolling in they just aren't what they want they should close the batch and make it so the workers can't accept anymore rather than letting it continue on and then having a completed batch that they then have to feel like they need to either approve or reject the heads but I think testing out the hit ahead of time is really critical to prevent that kind of situation mm-hmm okay let's talk about pay Amazon Mechanical Turk lets people set whatever pay rate they want how would you advise requesters to figure out how much something ought to be paid and what are the pay rates mean for the workers who are powering these systems behind the scenes I mean that's a really complex question and people have a lot of different answers to that people who are living in high expense areas have different answers than people who live in lower expense areas and people the workers who were in India who have a different cost of living than someone who lives in Los Angeles or New York they're gonna have a wide answer a variety of answers for that but if a requester is trying to decide how to set pay I think what would be good is for them to try it out with a couple friends or a couple of colleagues have them do the detect do the task see how long it takes them if they take some you know five minutes then calculate that out for a minimum wage and how long how much would that pay if that task was set based on minimum wage minimum wage maybe federal minimum wage it may be a minimum wage for where the requester is located that's kind of up to the requester to decide but I think paying at least minimum wage for United States workers is what's considered fair and more if it's going to be including workers that are in higher cost of living areas I think another thing that requesters need to remember is that Mechanical Turk income is taxed for workers we have to pay federal tax on it and state taxes for all of your income you have to report it just as income so if I'm making $10 on a hit I'm not going to actually pocket $10 I will then have to pay taxes at the end of the year on that income too so that's also something to take into consideration so taxes are a really interesting point that I hadn't thought about before um you turk workers pay taxes as independent contractors right so they basically have to pay their own payroll tax as well as income tax so it's actually not even analogous to someone who has a full-time job if they think oh how much tax am i paying out of my pocket they actually need to think how much tax am i paying out of my pocket and how much is my company paying behind the scenes to even get in what your final right yeah so there's it's quite a large percentage at the end of the year for truckers who have to pay taxes and then have to pay health care costs in addition to that so those kinds of things do add up and do cut into the amount that you're supposedly earning when you look at your earnings number Wow okay so in addition to pay are there other factors that employers should take into account when they're thinking about how much they ought to pay workers who are doing their tasks yes I think it's important to remember that Mechanical Turk workers are independent contractors and so we have to pay all of our own taxes so as well as healthcare so if you're making $10 on a hit you have to then pay part of that toward taxes and towards your health care expenses so I think it's important for every Buster's to remember that when they're setting their prices they also need to know that that's not how much the worker is going to end up pocketing after taxes and health care expenses are taken okay so can you tell us a little bit about the experience of working on other platforms because you work on platforms other than Amazon Mechanical Turk right I also work on user testing which is a website that recruits workers to go in review websites or apps for businesses to kind of test it out and see what works and what doesn't work maybe it'll be a company that's trying to make a new website and you have to check out their website design and say what you like and what you don't like about it so that's the site that I use quite a bit there's other sites too out there like try my UI which is a similar kind of user testing sites there's also CrowdFlower which is sort of similar to Mechanical Turk where workers can go and review tasks and accept them to do and then what's a little bit different is sites like upwork formerly Elance and oDesk where it's bidding based and you go and you find a test you want to do and you place bids to complete that task which is a different layout or a different design than Mechanical Turk and crowd a flower where you get to browse the tasks and then just choose to which one you want and start it right away oh can you talk a little bit about the similarities and differences that affect how good it of it how good of an experience it is for you as a worker in these systems yeah I think for me personally I really like the setup of Mechanical Turk the micro work set up because I don't have to sell myself or try and convince anyone based on my bidding that they should hire me I'm just able to view the tasks find of something that sounds interesting to me and start right away yeah it's definitely a time saver than bidding on a task and waiting around to see if you're going to be accepted or not it also on Mechanical Turk you don't have a worker profile that you need to set up with a list of which you've completed to kind of make a resume or something like that like you do on sites like up work where people then people with the work then look at your at your profile to see if they want to hire you or not just to me it's a lot more streamlined and it's a lot more faster and simpler on Mechanical Turk and sites like that where you get to choose your own work is there a particular task you've done that was really interesting or memorable I mean this might help people who are thinking about tasks that they could do with Mechanical Turk workers think about how to make that work experience more interesting for everyone involved I think it's been a couple and it kind of depends on what type of task you're asking about there was an artist who wanted to collect audio recordings of workers saying um so they collected I believe it was hundreds of recordings and then laid them all on top of each other and it's a YouTube video that you can check out where it's all of these workers saying all at the same time it's really interesting to hear all the different voices I'll put together that way that was really memorable other ones have been research studies that are really on important topics that you then hear about in the news and you think oh gosh I remember that study and I remember doing that and now it's on the news so that makes me feel really good that you had helped in the kind of research and you were part of that research and then other projects that maybe are a little more fun there was one that was up about a year ago where you got to go through and take people's food and write about what they were eating so it's picture of a hamburger I knew type am burger and there was someone's spaghetti dinner and you typed out spaghetti and it was really interesting to see what people around the world ate and then to try and come up with descriptive words for labeling what their food was that was a really popular one amongst the workers who had a lot of fun with checking out what people were eating and whatever when food looks like around the world that feels like it would be really hard it was challenging but it was also really educational to learn about one of the people in different places eat mm-hmm so you you mentioned singing I could seeing academic research is based on experimental work that Stricker's contributed to coming out in the news and how exciting that is does how interesting the task is affect how much you want to be paid for it I think having an interesting task is a benefit it's kind of a perk at the end of it but it's not really it doesn't really affect the pay because the majority of the workers that I know are the ones that are there to make money enjoy having a task that's interesting but having an interesting task doesn't pay the bills so the pay ultimately does matter but it can be made a little bit more pleasant if the task is well designed or is an interesting topic that is fantastic thank you so much is there anything that you wanted to say that we didn't ask about I don't think so okay that's pretty good thanks have a great day thank you "
s_JBsUPzhJE,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-11-18T17:34:54Z,"L31: Human-Robot Interaction contd. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/s_JBsUPzhJE/hqdefault.jpg,Josh Bongard,PT52M56S,false,36,2,0,0,0,okay let's get started where has the time gone we're on the edge of thanksgiving break here let's just talk a little bit about where we are and where we're we're going don't come to class on Monday I'll come to class if you want I don't know you can try and find out so we've got Thanksgiving recess and you are going to be working on your second report over the Thanksgiving break and doing some user testing so when you pack up tonight or tomorrow do not forget to take your leap motion device home with you if you're and your laptop any questions about the second report no we're all good okay so when we come back from Thanksgiving recess we will finish our discussion of robotics and the session and this section on looking outward and thinking about interactive technologies that are interacting with the physical environment they're interacting with humans and they're also interacting in the physical world perhaps with other instances of themselves so we're going to talk today about robot robot interaction what happens when we have a mobile interactive technology a robot that's moving around and needs to interact with the physical world needs to interact socially in the right way with humans and with its fellow robots will talk about a research project out of our group to finish that section off when we get back after Thanksgiving recess and then we will enter into the last theme of the course which is looking inward right so sort of going in the other direction sort of the blurring of the line between virtual reality and physical reality so we'll look at vr then we'll look at AR or augmented reality then we're going to talk a little bit about wearables so interactive technologies that are on your skin and we will finish in lectures 27 and 28 with implantable technologies so technologies that are interacting with the human host but doing so under the skin right we're moving into a brave new world of cyborg technology and brain computer interaction so we'll end the course of HCI with BCI the next frontier okay then we have a week off and then we will be back here for the exam period in which you will be presenting orally your final system we have 37 students signed up and we've got a little less than three hours so you're going to have between three and five minutes to present not a lot of time we'll talk more about what's expected during your oral presentation when we come back after Thanksgiving break sound good any questions about where we're going No okay so we are working our way through lecture 22 last time h RI or human robot interaction so we have I drew this little cartoon for you last time where we have a robot which is acting on its environment and sensing the repercussions of its action there are also of course humans in its vicinity that are also acting on the physical environment and sensing the repercussions human-robot interaction is focusing on this loop where the human acts on the robot or to the robot or with the robot the robot senses what the human does and the robot acts accordingly and when the robot acts the human senses what the robot did right so we've got this very special feedback loop between a robot and human which are in very close physical proximity to one another so obviously we have to think very carefully about this particular kind of feedback loop there's some things that are special about this feedback loop compared to the one with the physical environment what distinguishes social interaction from the point of view of the robot from physical interaction we talked about a number of things last time what does the robot have to pay particular attention to when it's interacting with this thing in its environment called a human absolutely right whatever whatever the human is doing the robot is sensing that in order to really get a clue and try and figure out what the what the humans intentions are pay attention to the face most of the time right okay so pay attention to the face or face recognition this is one of four social building blocks that we were looking at last time and again there's hundreds of social building blocks that humans engage in when they interact with each other we're just picking a few some of which are relatively easy for robots to do like VOR staying focused on something that's in their field of view recognizing faces harder but now is basically a solved problem in AI and robotics and we ended last time with joint attention which is harder still right so you've identified a face in the environment and that face or that human is trying to get you to do something which most of the time is pay attention to something I'm paying attention to this and I would like you to jointly attend to this thing as well how do we know when someone wants us to do that we can be very explicit i can tell you please have a look at the slides i can point or i can look at the slides right and most of the time instinctually if you're in a social interaction with someone and they look away from your eyes to something else if you pay attention to this you'll notice that you usually do to you try and jointly attend to whatever it is has captured the attention of the interlocutor or the person that you're engaged with okay how do we get a robot to do this well as we figured out we need to be very very specific about what in the face is the clue that tells the robot or the other human what the person is looking at and so there's an interesting paper it's a little bit older now but it's an interesting idea and again uses machine learning to try and get a robot to learn how to do this do this so in this case we're going to start with a robot that does not know how perform joint attention and it's going to learn how to do so through trial and error okay so here's our robot here it's not very sophisticated it's basically a box sitting on a table it has however an active vision system so it's got these two cameras on top and at any point in time there are two motors that can Pan the camera the pair of cameras left right and tilt up or down so you'll notice might be difficult to see here there's a theta pan and a theta tilt so two numbers which is the current angle of the cameras so the robot knows the direction it's cameras are pointing in and the robot can act on its environment in a very simplistic way which it could it can send Delta Thetas so changes in pan and tilt so the robot can say I want a pan and tilt my head so that I'm looking in this direction ok ok so the robot is looking at a human caregiver who is going to help the robot learn how to perform this important social building block which is joint attention how does this work well we the robot starts and this is the outline of the algorithm the robot is going to start by focusing the caregivers face in its center of view these investigators did not use the ratio template algorithm they use something else but doesn't really matter but we are assuming that our robot already knows how to find and then saccade or look at faces right so maybe the robot recognizes there's a face in the upper right part of its visual field so it will pan and tilt its cameras so that now the face is centered in its field of view ok once it's done that it may notice that there are other objects aside from faces in its field of view so if you look at this middle rectangle here in this cartoon example here the robot sees the humans face and it also sees a star and a diamond and the robot will the robot will now move the cameras so that it's no longer centering the human face but it's centering one of these two objects at the beginning we're assuming that our robot doesn't know much it's trying to learn it doesn't know whether the human is actually attending to one or neither of these objects so it just rotates it picks one of these objects at random and rotates the camera so that now that object is in its field of view so it's started by looking at the human and then it saw something else in its field of view and looked at that other object so we have a two element vector here delta theta which are just the two numbers that indicate how the robots cameras pan and tilt so far so good okay so if the robot is going to learn obviously we need to get some reinforcements so reinforcement is the caregivers giving positive reinforcement good job you actually did happen to attend to the object that I was looking at or negative reinforcement thumbs down no you looked at the wrong object i was attending to the diamond but you chose to look at the star instead ok so the robot and the caregiver are going to do this many times and they're going to end the robot is going to try and build up is going to try and use a machine learning algorithm to do this so that most of the time it gets positive reinforcement rather than negative reinforcement ok how is the robot going to learn in this case we've looked at sort of different machine learning algorithms in this case the robot is using a neural network and we don't have time in this class to go into neural networks suffice it to say that they are simple learning models based on the human brain so in a neural network it's a network we have nodes and edges the nodes or the white circles here correspond to simple neurons and the lines correspond to connections between neurons otherwise known as synapses at the front of the network here we're going to supply all the RGB values from the robots cameras so the robot can see and we are also going to supply the current two angles of the neck if you like so the robot knows what it's seeing and it knows its current the current angle of its camera so the sensation for the robot what it sees and what it feels goes in the left side of this neural network and out the back come two numbers which are the actions right so the robot senses it thinks and then it acts the deltas tell it how it's going to move and obviously we're looking for it to learn the right thing okay so how does it do that again at the beginning this neural network is all randomly connected up so when it sees a face and it gets input these neck angles it does something at random and the heat and the caregiver says good job or bad job if it does something in the caregiver says good job the robot does nothing it does not change this neural network if on the other hand the caregiver says thumbs down you looked at the wrong object the robot will make slight changes to this neural network and I'm not going to go into the details of how it makes changes but it goes in and make slight changes if the robot makes slight changes in here what happens the next time the robot gets exactly the same input so yep it's going to add slightly different right so who knows what it will do but from time to time when presented with the same situation the first time the the human caregivers said wrong make a little bit of a change and by chance now it happens to do the right thing human says good you learn something in this case you did the right the right thing if we do this enough by making these changes the robot will start to get better and better at what it's doing yes the nodes are black box what you mean by black box how the computer changes what changes and make we have no idea what yes so for those of you that have heard about artificial neural networks before they are extremely powerful the price you pay for using an artificial neural network network is that they are black box so in this case when this robot actually does learn to do joint attention it's very hard to look inside and look at this network and know how the robot is figuring out how to look at the object that the person is attending to that so the certain machine learning algorithms are white box and others are black box you're KN learner is white box in the sense that whenever your learner makes a prediction you can go inside and see the 30 element vector that it got that it was trying to predict and you can go in and say see the 15 other vectors or the 15 other gestures that were closest to that new gesture and you know exactly why your can and learner predicted the digit 3 or the digit 6 you can figure it out that's a white box method much harder to do with an artificial neural network there is however research going on at the moment to try and turn neural networks from black boxes into white boxes Kelly did you have a question is same question okay so in this particular research report it actually worked why it worked it's hard to say so I made a little cartoon here this is how it may have learned but who knows right remember that the robot is receiving is receiving visual stimulation so it sees the caregivers face as we know from our discussion last time all you really need most of the time all you really need is where are the pupils in the human's eyes relative to the whites of the eyes so imagine that this neural network changes over time to throw away all the pixels except eight of them before that fall on the left eye and another for that fall on the right eye and in this little cartoon example here it's basically looking to see is the pupil looking down right and if the pupil is down right like in this example here that means pixel 22 will be dark pixel to 1 pixel 11 and pixel 12 will be white or much brighter than p2 right this is like a variant on the ratio template algorithm if it senses that then it knows the human from its point of view is looking down right how should the robot pan and tilt its camera should it also pan down into the right so it can write and tilt down should it should do the opposite assuming that the robot is facing the human head on right if the robot is sitting obliquely it might be something else but let's assume for this simple example that the robot is facing the human so when the human pans down right the robot should do not exactly the opposite the robot should do what down left right and if the human looks down left the robot should look down right so it think about this neural network here it's taking all this raw sensor data throwing most of it away only paying attention to the pupils relative to the whites of the eyes figuring out that what direction the human is look thing and then outputting the corresponding action not the same action but what it should do when it notices that the human is looking down left down right up left up right now whether the neural network actually learned some version of this little cartoon down here who knows hard and hard to say because it's black box but it did learn to jointly attend to what the human was looking at yes I've never purchased more robust and we're going to calculate the vector of where the eyes looking if it goes outside of their bright vectors and all of that about more of is it is more of a possible essentially that's a good boy not know how we're butters and more situations absolutely so the general rule of thumb is yes absolutely the more experienced a neural network gets the more robust it becomes that's why Google cars are driving around with that little thing on the top right there collecting millions and millions and miles of sensor data to train autonomous cars to say this is a shadow and I can drive over it or this is a human pedestrian and I should not drive over it that is exactly what's going on inside google's autonomous cars it is a much much bigger network but that's exactly what it is and our entire society wants to know what the answer to that question is which is how much data do you need for an autonomous car to be successfully robust right it sees something in the road it's never seen that particular thing before it's not in its training set is that thing a shadow an empty plastic bag a human child okay again we could have a very long discussion about neural networks but there are black box we don't necessarily know how they work and you need a lot of data to get them to be robust so the investigators in this experiment in this experiment if you want to go back and read the paper they spend a lot of time saying yes yes no yes no yes yes yes no until they could train this robot to jointly end to something just by looking at it so now after training they could sit in front of this robot they could put objects in front of themselves and they could look at the objects and the robot would move its head and look at what they were what they are looking at ok ok let's move on to the fourth and final building block that we're going to look at assuming that you want to hold a conversation with your robot you're going to have to engage in turn taking this one is actually pretty straightforward right so I'm a robot I hear certain raw audio which corresponds to you talking I see your lips moving you're talking I wait till you stop then I talk and then I stop I see you talk back and forth back and forth back and forth ok so I'm going to show you a couple video clips now I took him taken from an experiment actually this is back in the 1990's at MIT by Cynthia Brazil i showed you Cynthia's kismet robot before Cynthia's now got our spin-off company and they're developing jebo which is based on a lot of this early research right how do you build in these social building blocks to get something that at least gives the impression that it is engaged and paying attention to the human interlocutor ok ok i'm going to show you the videos first i'm not going to tell you too much about what's going on inside kismet head perhaps you can figure it out from watching the videos this was an interesting experiment because once they have built some of these social building blocks into kismet some of the grad students went outside the research building in Cambridge Mass and just grab people off the street and said would you like to talk to a robot some people said yes so like you're naive testers over Thanksgiving break these are people that have never seen kismet before these are probably tech savvy people because they're around the MIT buildings in Cambridge Mass but they've never seen kismet before ok here we go again this is from the 90s so I apologize for the video quality here alone do you Maria much name hello my life I think Oh okay this was kismet and kismet and henne how impressed you think Hannah was with kismet social abilities not very why not what was which building blocks here were failing I couldn't tell it was my hearing okay so I'll play you another video which is longer and you can maybe figure it out so it's kind of hard to hear what kismet saying right so that doesn't help very much facial expression there sorry kismet woohoo facial expressions kismet okay okay that's that's Hannah I'm going to show you a rich now and again this is just kismet they made no change to the code actually I don't remember whether they tested this with Hannah first or rich first here goes kismet and rich this one's a little longer about two minutes thank you kidding it pretty funny person now do you laugh at all I laugh a lot Carol says I laugh a lot man I silky I try not to laugh at her okay if adorable he who are you what are you nationalist I want to show you something okay this isn't a watch that Mike there's a watch than my girlfriend gave me ma'am yeah look at hello blue light to like I almost lost it this week I don't know any that you know if I can lose something easy no chuckles you guys which I will be mad you can we do yeah me too no stop oh no I gotta talk about not up shop it wasn't me listening I think we have something going I think there's something here between a solution stop you gotta let me talk yes but I think we got something going on here you need your mate wedding ring bye are you now nothing Oh you know what hang on a sec I don't want to talk to you add a couple more things I want to say okay right at the end you here which there hold on a second I got a few more things I want to talk to you about so I think they had subjects come in for about a 10-15 minute session rich stuck around for two and a half hours talking to talking to kismet they had I think they had to eventually kick him out of the lab could probably hear the grad students in the background they were trying to maintain a very professional and neutral demeanor and they were laughing and you know this was this particular interaction was I think more than the investigators well his dreams about what would happen with kisna why so different same Hardware same software what happens what was the difference between rich and Hannah he was much more active right so in a social interaction setting right who's who's leading and who's following right it's often not not clear remember when we saw deb ruiz experiment with his three-year-old child right it was very unclear once you started to look at the data whether the parents were teaching the child or whether the child was teaching the apparent right so Hannah SAT very still shouldn't move her face she kept saying the same thing over and over again rich was the exact opposite what was rich doing to in gate what was going on in this actual feedback loop that was drawing kismet into the social exchange or enriching the social exchange well he was moving back and forth like China draws attention to it why was he moving back and forth right he was trying he was trying to draw his mitts attention he was trying to test whether kismet was able to do that right you're sitting in front of this thing which is kind of animate kind of not you know how much how much Leah where is this thing right so rich is really carrying out a lot of social experiments to try and figure out what kismet is capable of and tracking my eyes as I move is a pretty important one right so building block one and two are definitely in there what else did rich do to test kismet absolutely so he actually said I want to show you my watch and kismet did not look at the watch and rich saw that kismet did not look at the watch after he had said I want to show you my watch what do you think rich was able to derive from that micro exchange what do you think more like emotion that you need to watch right does kismet speak speak and understand English doesn't doesn't seem like it right however right so if you're trying to engage with someone who doesn't speak your mother tongue what do you do you fall back on gestures right I want you to pay attention to this kid rich could have pulled back his sleeve and just looked at his watch but he didn't why do you think he probably chose to gesture rather than look to make it much more obvious right so over time it seems that rich riches social exchanges are like with a young child right this young child doesn't speak English yet this child is probably not able to do visual joint attention using my eyes so I will try and give more obvious visual cues about where I want us to jointly attend what else did what else did rich do to exaggerate this exchange to make things to scaffold kismet social experience here he pointed at his watch what else did he do sorry if the robot would know how close the space was to absolutely does kismet have a sense of personal space right is that important here when he moved his head to the right it's at one point he went like this what does this mean absolutely right so he knew rich either consciously or unconsciously realized that kismet was able to follow his pointing to the watch he is now instinctively trying to teach kismet to learn things from I gaze right so he's saying attend to my eyes he knows that kismet pays attention to gesture so can you scaffold the learning of what I'm doing with my eyes from gesture data ok what else happened he was right over ok exactly and it did it more or less good job except when it started to talk over him and what did he do in that case listen to me and then when it still talked over him this and then who's actually shaking his finger at it right and what did kismet do in response sort of dipped its head and contrition right I'm sorry I didn't realize so rich was trying to give negative reinforcement you're now breaking the turn-taking rule and I'm trying to give you negative reinforcement so you learn from whatever just happened from your point of view to not do it the next time around and by kismet dipping its head that's acknowledging that it knows that it just received negative reinforcement kismet may not know what it did wrong but it knows that it did something wrong right so rich pulled back right said ok is known there's no need to continue scolding kismet it got it hopefully it will do some learning and do a better job next time ok last exchange with kismet we saw Hannah and we saw a rich how about Adrian alrighty let me tell me a pity Cynthia what okay who's teaching and who's learning here I don't know it's not really clear there is a very clear additional social building block that is going on here and it's being exhibited by both of them actually no it's being exhibited only by the human in this case what's up he's repeating everything you say imitation right so children at this age we looked at ages 9 224 where they start to absorb language like a sponge before that they're they're absorbing everything else like a sponge right whatever they see in here can they imitate it right so kismet is not imitating the child it's just doing its turn take in the other building blocks the child clearly is in this in this case okay so you've heard kismet quite a bit now what does kismet saying probably doesn't understand English is it speaking English what is it speaking absolutely nothing they're just random English phonemes strung together in random sequences right it's babbling however clearly that's not that's not doesn't matter here right kismet can already start to draw humans into social exchange without having to understand English the reason they didn't give it a vocabularies because they want to see how much how much interaction can go on between a human and a robot non-verbally right so language is kind of the last thing that a child starts to add on as it becomes socially competent right there's a huge amount that you can get done interacting with another human without actual language right hopefully in the case of someone like rich rich might now be willing to try and help teach kismet language right but what can we do be free language clearly from this experiment quite a lot okay okay so we're going to finish off lecture 22 here with a in-class exercise so again we would like to develop robots that can work in close proximity with humans but they need to obviously be careful when they do so and we'd like to try and cobble together some of this social building blocks to make a robot that can work well with with the human one possible application of such socially aware robots would be rehabilitation for any of you that have had to do any physical therapy you're working with a therapist who demonstrates actions watches how you respond tries to show you how to expand your range of motion a little bit back and forth back and forth for hours and hours and hours on end physical therapy is a time consuming activity could we try and offload some of this repetition relearning repetitive relearning to a to a robot rehabilitation device in this case so again this is just hypothetical at the moment so as an example we have this rehabilitation robot it's learning the robot looks at the therapist first and the therapist says okay patient X is going to come in in a few minutes I want you to work on the following with patient X the physiotherapist demonstrates certain actions and the robot learns to imitate those actions then the subject comes in the patient comes in and the robot observes the patient exhibiting limited range of motion the robot imitates but slowly increases the range of motion which is trying to signal to the patient's push yourself and see if you can repeat the action but expand your range of motion a little bit and the robot should provide some sort of social signal about when the human should try this okay here are the four building blocks we talked about we just introduced a fifth one which is imitation so turn to your neighbor and think about these kinds of micro exchanges we want the robot to engage in with the human physio therapist and with human patients how would put together these building blocks to realize these interactions I'll give you a few minutes to think about that and I'll see what you came up with maybe like the judge hasn't lethal yeah show something is back that's just what I was I would be using isn't working long faces seeds to mean maybe not sorry I was a little unclear you can flesh out the rest of this rehabilitation robot example or if you want to think about another application where a socially aware robot would be useful that's fine 250 series with my foot to be knackery everything didn't work out so people sharing filters no why you threw up on me or my brother also write an angry face however long that things like it was pretty curious late once there's more technology sky no I can't it's very part that scared me too yeah it was that cover like how much brother burgers with a bunion on rhythm while determinants I mean that maybe it could be hearing public shows like okay what what sorts of ideas did you come up with assuming we want to allow these kinds of robots in our everyday lives to begin with right which is a very good question to think about first yes ok that would use first public park but I mean like it should be a camera that good like around to focus on whoever's talking they could pay joint attention of someone gesturing add like a Exhibit A or whatever ok if the judge wants it to read back something ok turn-taking yep ok sounds like an interesting application yes okay makes sense so it would have to figure out when someone so again you have to be pretty socially sensitive in that situation right is this someone who's looking at a painting and wants to be left alone to enjoy it or is this is the human somehow giving off signals that they'd like the robot to explain more and so on right so again this very fine line between leave me alone and I'm open to an interaction you got a robot is obvious going to have to be very careful in those kinds of situations forgetting for the first 18 place almost pointing at the menu I guess able to talk to them they were actually face someone was angry waiting for the food absolutely right bring a free dessert so any of you that have actually waited tables right this is pretty important as knowing when when someone's sitting at a table would like some attention or would rather be left alone to carry on the conversation again tricky right other examples physiotherapy again for those of you have been through it it's extremely painful it can be very frustrating how would the robot know that the subject is having difficulty remember our discussion about affective computing a rehabilitation robot is going to have to be very sensitive to the current effect of the patient how would you know is wincing in pain right this is going to be extremely important calls physical therapy like half physical therapy half psychotherapy ok so I think it'd actually be very hard to use a robot in that instance because just pain you know makes people feel sorts of oh absolutely absolutely okay something to think about obviously this is probably a technology that's going to take a while for for it to be adopted for exactly the reasons we've talked about right a robot that's off doing its own thing at a construction site or somewhere that where there Hugh mark humans directly in the loop we're much more willing to put up with a wide range of actions right the moment we're thinking about robots that are interacting with humans they need to be extremely sensitive to certain social cues and the particular social cues are going to be different in different situations right what are the particular social cues in this social context that that matter okay we've got about ten minutes left so let's move on to our last domain which is robot robot interaction so we talked about robots and how they should interact with their physical environment how should robots interact in social socially with people how do rope how should robots interact with each other so now we've got two or more robots and the behavior the output of one robot is sensed by another robot and vice versa why would we want to create collective robotics well there are lots of problems out there for which we might want to apply robotics but it would be difficult to create one large complex machine to do it instead it would be easier to create lots of relatively simple machines that coordinate their actions to get the job the job done okay I was googling around yesterday and found this very recent examples ooh oh it's which is nice because it has a very nice connection to HCI okay pragya pretty limited so far but you can sort of see the beginnings of how this might be useful from an HCI point of view okay pretty simple robots what are their actuation capabilities and what are their sensing capabilities they can't do much and they can't sense much rips not that one what are their action capabilities here what can any one robot do and not do just move around right it's hard to see but I there's probably two wheels underneath and it can rotate those two wheels forward and back not unlike the Brayden berg vehicles pretty simple what are these machines able to sense proximity to each other right so there's probably something Wi-Fi or something inside that they can detect proximity to each other and direction so they seem to be able to go to places that are being indicated in this example here how do they know where they're supposed to go what is the signal being given off by the human so in this case this is these are both swarm robots and socially aware of robots there's a human in the loop that's it so the robots are somehow aware of the human in this case how as you can tell they're pretty small there's probably not a lot going on inside how do they know exactly so whatever it is there is a thing out there which can resist my own self movement I'm spinning my wheels and I sense that I'm not moving right it could be that it could the heat it could be something else so there are two robots here that broadcast entrapped or I'm held how did the others respond so they're all running some little piece of code which is supposed to result in them forming a ring how do they do that do you think that the two held robots are broadcasting a signal saying robot 7 go to coordinates x and y robot 13 go to these coordinates robot 12 go to these coordinates you could do that ok these are they're done right they don't need to move anymore they know they can't but they're going to have to help their friends somehow so if you are one of these two trapped robots what would you do you could start issuing detailed commands like robot 13 go to this position robot 12 go to this position the key to collective robotics is to think carefully again about the physical context here on this table top what could you do to exploit this interaction where you wouldn't have to issue these detailed commands seem like one we go to a point and another one that way we college stop and reconsider give you like it it's like a really amazing instruction and if that doesn't work try something else ok so what the simplest thing that the trapped robots could try the simplest message they could emit is what where they are come come to me it's even simpler right you don't necessarily need to say I'm a coordinate x and y it's come to me and they're emitting some signal from their position outward and how without that robot having to advertise its explicit coordinates how would an untracked robot find it one robot is emitting the command come to me but I'm not going to tell you where I am I'm just telling you come to me no signal strike signal strength where have we seen this already the ants or yo at the ants yes and the fruit flies and the the things that were inspired by the fruit flies the brain bird vehicles right the crosswires so is the trapped robots signal stronger on my left or stronger on my rights turn towards whichever side it's stronger and that will cause let me back up a little bit cause cause the robots to approach the trapped robots but obviously we don't want all the robots the untracked robots clustering around the trapped ones so one unwrapped robot happens to get there first and stands next to it at some certain signal strength so it's beginning the circle right it says okay I don't to be too close too far okay I'm in a good position then what happens so the trapped robot has attracted one unwrapped robot and has got it in the right position what are these two robots do at that point if you want to form a circle that both of it the signal the stones respond to both possibly maybe a little bit more specific if they're both emitting the signal what's going to happen you're just going to get a cluster around those two that's not quite what you want to like they're doing proximity to each other but also like was my proximity to the one way over there and that's all could be it's possible so the game here is not to figure out what they actually did but what is the simplest possible way they could have done this first one is very switch off well that's been held first new one becomes the youth hostel information to do that absolutely right so I'm a trapped robot I emit a signal somebody arrives in front of me somebody arrives behind me great I go quiet because I'm satisfied there is someone we're forming the conga line right there's someone in front of me and there's someone behind me my job is done I go quiet but I tell the robots in front or behind now you signal right so the robot in front says I need a partner and the one behind says I need a partner and any that have no partners switch continue to run their break brood vehicle until they attach and with something very very simple like that you would get the formation of a circle the robots know nothing about humans they know nothing about circles they're using some version of a breeding bird vehicle and collectively by exploiting their environment which now includes other robots some of which are trapped and some of which are not to form a circle we're going to stop here but I want you to think about that when we talk about collective robotics when we get back how to exploit your environment which includes other robots to do whatever the swarm is supposed to do okay you have a quiz due tonight report to when we get back have a good Thanksgiving break 
XD2HbuaRP4M,28,"All lectures: 

https://www.youtube.com/playlist?list=PLAuiGdPEdw0iLnUFP7kALZf3SbGIokPKt",2018-10-23T15:16:41Z,"Human Computer Interaction lecture 16: Social network inference. (Filmed Oct 23, 2018)",https://i.ytimg.com/vi/XD2HbuaRP4M/hqdefault.jpg,Josh Bongard,PT1H15M20S,false,50,0,0,0,0,okay let's get started just a reminder for the next 75 minutes this is a device free zone with the exception of this particular device here let's talk a little bit about the deliverables so deliverable 8 is due tomorrow night and then deliverable 9 you can start in on Thursday however I'm gonna be traveling on Thursday so I put up deliverable 9 a little bit early if you want to get started on it and we'll talk about deliverable 9 in a moment I apologize that I'm doing so much traveling this semester this is not normal for me I will be in Chicago on Thursday for a DARPA meeting DARPA runs a program called lifelong learning machines so at the moment when we train AI and deploy it out into the world it's done learning and it does whatever it does there are situations where it makes sense that once you deploy an autonomous machine out into the world it continues to learn it's a challenging and risky thing to do so there's an interesting meeting about that I will be there in my place professor bag row from the math department will be here some of you may have been taking or are taking Professor bhaer gross data science class professor Pedro does research in data science but also in crowdsourcing which we talked about a few weeks back so he will be here and talking about some of his crowdsourcing research on Thursday as I mentioned whenever I travel I bring back food to drive all of you what should I bring back from Chicago pizza you know it's gonna get that one know deep-dish pizza I would if I could I will find something and bribe you the following Tuesday when we move on to lecture probably lecture 17 or 18 you'll see where we are okay so I won't be here on Thursday but we'll continue on with the deliverables as usual so let's spend a few minutes just talking about the deliverables now I just want to back up for a moment to deliverable six I realized now that there was some confusion in deliverable six at step number seven and the why is some of you may have got an error at this point where you are where your can and algorithm does not accept test data why not turns out it depends on the version of numpy which was part of the problem it worked for me but not for everyone else what is the shape of the test data data structure exactly so technically what we're defining here is a two-dimensional matrix that has one row and 30 columns what we really need is just a vector with 30 elements right but of course a 1 by 30 matrix is equivalent to a vector according to some versions of Python and some not so if some of you are still struggling with deliverable 6 easy fixes to go back and just to find NP dot 0 it's open bracket 30 comma D type and so on so just a vector of length 30 if you put in brackets a series of numbers you're defining an n-dimensional matrix with rows column sheets and so forth you just put a number in there you're telling it you want back a vector of length whatever okay I think most of you are past this however hopefully you're finishing up deliverable 8 and deliverable 9 is now available last time this was perfect timing we ended last time by talking about this concept of scaffolding scaffolding is an idea that comes from developmental psychology about how parents scaffold the learning experience of children who saw a little picture last time of the parents walking the child through the process and walking so the child can literally feel what it feels like to walk so the parent is scaffolding the experience of the child making it easier for the learner to captured or to start the rudiments of starting to master the rudiments of the task and then as the teacher or the parent or the instructor starts to observe that the learner is getting the idea of it as the name implies the scaffolding is gradually removed what are some other examples of scaffolding makes it easier to build the building right that's what the scaffolding is there for yes the little scaffolding where else well I'm sorry what is this okay where you actually trace over the drawing yeah okay yes exactly so your step-by-step drawing instructions beep absolutely right so in game tutorials which is exactly what you're going to be building and deliverable 9 yes how did you learn to ride a bicycle some of us your training wheels right another good example of scaffolding remove the remove the training wheels so ideas scaffolding is also prevalent in AI so as an AI is starting to learn a task you might make the task easier on the AI and as it starts to get the idea gradually remove the scaffolding okay so in deliverable 9 and 10 now you're really starting to develop your educational software so in deliverable 9 you're gonna be adding three pieces of scaffolding let me start let me just back up here in the first case you're going to challenge the user to sign an increasing number of digits so the first time a new user logs in to your system you may pick one of the ten digits show them the image of the sign for that digit and wait until they sign that digit correctly and then show them a second digit they sign that correctly go back to the first digit and you might alternate back and forth between just two digits until they master those once they master those you expose them to a third digit jump back and forth between those three digits once they master those three four five six and so on right you can scaffold the learning of all ten by exposing them to a small subset of what they're supposed to learn and as they signal increase in confidence broaden the things that you're trying to teach them that's the first scaffold the second scaffold will gradually challenge them to remember the sign itself we talked a fair bit in this course about recognition versus recall so eventually they need to just recognize the picture and figure out how to do it themselves but eventually you're going to just draw the digits 0 1 2 and so on and they should remember what the sign is and do it correctly we could go from just showing them the image and then at a certain point not showing them the image anymore but we'd like to do this using scaffolding where we gradually challenged them to remember the sign and there are different ways you can do that and it's up to you about how to do an assignment now a deliverable 9 you're creating three forms of scaffolding the actual way that you do it is up to you okay and then the third scaffolding is to increasingly challenge them to sign that gesture faster and faster so put increasing time pressure on them from the time of signaling which particular digit they're supposed to sign until correct signing of that digit we want to try and reduce that time okay so pretty short terms of the instructions just one two three these three scaffolds and then we want you to upload three videos showing those scaffolds in action so in each of the three videos you're going to intentionally make mistakes and the scaffold will remain and then as you get better in your video we should be able to see the scaffolding being removed and again remember we're assuming our users is not a native English speaker here so no text should be obvious from the visuals that the scaffolding is being removed makes sense okay so back to lecture material for those of you that are just joining us I won't be here on Thursday will be a guest lecturer but deliverable 9 is available now if you want to start it on it okay so we're gonna finish lecture 16 in a moment which is this idea of ubiquitous computing or sort of stitching devices out there into the real world and as we do there are more and more of them they're specialized they only do a few things and as we as we stitch these specialized devices out into the world around us they kind of disappear into the background and they are there to support us and possibly scaffold us as we go about our daily business and help us out in various unobtrusive ways it's interesting to think about as we do that sort of what are the killer apps what are the things that we can what come where are the things that we can enable people to do that would have been difficult or impossible to do in a physical world in which there is no embedded technology and to explore that question in lecture 17 18 and 19 we're going to look at three different projects the research projects that ask the question once you have the technology out there what are some things that you can do that are difficult to do otherwise the one we're going to look at today is the idea of social network inference you're all aware of what social networking is on Twitter or Facebook you can see what the social network is I'm on Facebook I have all of my friends I see about 30 of you out here but I don't know which of you are friends with who else is there a way that we could infer that just as you go about your everyday lives now whether we would want to do that or not is another issue but could you actually infer from just people's daily activities could you infer a social network who knows whom who spends more time who enters into more conversations with whom and so on and at the end of lecture 17 you're going to see that of course you can do that and you can also then ask questions about social networks that are difficult or impossible to answer on social network apps like for example how do people change their social behavior depending on who they're interacting with do they interact differently when they're interacting with friends with someone they know from class but don't know very well or complete strangers difficult to do on social networking but it turns out in this application you see kind of interesting they're interesting social dynamics that go on in social groups that were difficult or impossible for social anthropologists to see without technology interesting project in lecture 18 next week we'll look at activity tagging so you all carry around a smartphone most smartphones come with an accelerometer if you record that accelerometer data you can actually infer in a lot of cases what activity the person is conducting at that point in time again whether you would want to do that or not as a separate question we're gonna look at lecture 18 it's just a question of how to do it and what kinds of activities can be inferred lecture 19 we're gonna look at a speech project where an investigator his wife instrumented their home and recorded all the video and audio in their house during the first six months of their child's lifetime and asked the question about how does a child go about acquiring language when a child enters the word water or mother for the first time how did the child go about acquiring their learning that word what were all the things that were going on in the house at that moment that the child uttered the word the minute before the child uttered that word the day before the week before the month before kind of a controversial project this is the great what do they call it the the biggest family home album of all time but pretty pretty intrusive at the same time okay interesting to think about so three different applications once we have you bicker computing are things that we that were difficult or impossible to do beforehand so let's jump back to lecture 16 where we're talking about ubiquitous computing we were thinking about this idea of again is putting technology out there in the world and hopefully it fades into the background and it's not obtrusive it just sort of helps us get going with whatever it is that we want to do that good scaffolding or ubiquitous technology in particular provides a learning gradient a path or a slope that we can climb so as the agent starts to learn what they want to do literally or figuratively a stand on their own two feet we gradually remove this scaffolding we just talked a little bit about it okay what are some examples of ubiquitous technology that's already out there that provides scaffolding Google Maps ok right absolutely right so it doesn't explicitly remove scaffolding but you're going to elute destination use Google Maps a few times and then you don't need it anymore good example ok let's look at let's look at another application before we do let's look at some data here these are population pyramids which are a nice nice way to summarize data remember our discussion about visual design these population pyramids are showing the distribution of ages in the population of a particular country in 1950 2005 and 2050 let me zoom in a little bit here for you if I can what can you tell me about this country how is the distribution of age in the population changing over time absolutely right so people are living longer and having less children in the Western world you can probably guess that this is a Western country what else is changing between 1950 through 2005 and projected into 2050 absolutely what else absolutely so males and females are living longer but females are living longer at an accelerate a greater rate what else not looking good for the total population not looking good in what sense possibly there's something else that's worrying about this projection for 2050 absolutely there's a bump in the middle there's a boom here what boom is it it's the baby boomers right so there's the baby boomers moving through the population pyramid why is the projection for 2050 whirring what yeah what what are these three classes here these three divisions by generation for me little more specific absolutely right so so in this country by 2050 there will almost be more people not working than working which country is this not quite not quite there's a few hints that I left up here if you're paying attention it's the UK how did you know is the UK there you go the Ministry of Health Labor and Welfare so this is the projection for the United Kingdom somebody mentioned Japan Japan already has more non workers than workers they've already passed this worrying threshold so what do we do about this situation the UK and Japan not as bad as Japan yet but also headed in the same direction so how do we go about supporting a greater fraction of the population that is not gainfully employed so a lot of solutions out there nobody really knows one way to do that is to try and help the elderly stay within their homes and independent as long as possible unlike the younger generation who are primarily taken care of by their parents elderly populations primarily are taken care of by the state which requires taxes which requires people to be working in order to pay taxes right becomes a challenge so there's a big challenge facing most Western countries which is how do we take care of our aging population and whatever those solutions are going to be a big part of that solution is going to be keep them in their homes and keep them independent as long as possible that's the best solution for them that solution for their parents best solution for the state how do we keep how do we keep the elderly generation in their homes this is an idea that's been around in HCI for a long time which is the smart home that we instrument homes with technologies that again make things easier for the homeowner which in this case is someone who may have physical limitations so I plotted a little cartoon here one way we might go about stitching technology into the home of an elderly person this is clearly an HCI enterprise because we need to think very carefully about the P who is it that we're trying to support physical context matters was it what does it mean to be physically limited and trying to move about in your home what are the other social and cultural context that matters as you think about trying to create a smart home for for the elderly we would like to not create a Big Brother home where it's constant surveillance all the time how could we balance surveillance with support and scaffolding I don't know if any of you have had this experience but with my elderly parents sometimes I call and nobody answers the phone and I'm not sure why right so one of the things we might be able to do is to try and provide some scaffolding provide some signaling about what's going on in the home imagine that we were to advance some pressure sensors into carpets throughout the home as long as Ms X is moving across the carpets whenever the telephone rings there may be an automated message that's sent to the caller saying Miss X is approaching the phone hold on she's she's coming for example okay or not or the phone is ringing there are nose signals coming back from the pressure sensors and the signal is Miss X to not hear you or there doesn't seem to be any movement in the home send a message to her vibrating pager or provide some other options I've sketched this out using what's known as the urmia notation for those of you that go and work in industry and HCI this is still a this is often used to sketch out HCI designs entity relationship modeling for information artifacts HCI loves they're big complicated acronyms basically it's a fancy way of showing a collection of physical objects like carpets and information artifacts like sensors or cameras or Wi-Fi devices and showing sort of how we're gonna thread these things together what are some other what is some other technology you might think about instrumenting in a smart home for the elderly to help them maintain their independence yes yep absolutely so we could set up ordering groceries automatically we could have a camera inside the refrigerator I'm sorry reminders to take their pills I have a family member that will go unnamed we remind this person all the time and they refuse or forget or they're not sure it's a challenging one be great to actually instrument their medication box to actually be able to detect whether a pill has been removed once per day reminders are sometimes not enough what else what are some of the other challenges here remember pact analysis so medication absolutely food movement betters might be useful here assuming we have these pressure sensors in carpets Falls matter right so again I have a family member who is supposed to wear something that detects Falls and sometimes he remembers to wear it sometimes he doesn't not other examples right so something goes wrong how do they easily signal if there not be something that has a lot of buttons or something they need to wear because they may or may not remember they may or may not know which button to press absolutely yeah so voice-activated technology is clearly going to be an important part of whatever the solution is exactly okay so something interesting to think about again now that we're able to do this there are these new kinds of technology like Alexa that we're putting out there in the world what are different ways in which you could be used to help support various social groups okay so let's switch gears now and in lecture 17 we're going to look at a particular application of ubiquitous computing to again try and detect something among a social group that was difficult or impossible to do before hand we're gonna start with the following question can we use ubiquitous computing technology to extract face-to-face information so this is not texting using Twitter face-to-face social interaction can we extract that from a social group and can we learn about how people behave towards one another in that group and more specifically how they modulate their behavior based on who they're talking to or interacting with ok ok so this is some work from about 10 years ago reading for today's this actual research paper like most of the reading that I've assigned which are the actual primary sources you can just read the abstract in the introduction if you're interested you can carry on and read the remainder of the paper we're gonna summarize first of all the methods section what did they go about doing they managed to convince 24 graduate students at the University of Washington to carry around sensors for a six month period you can see two of the grad students here wearing them there's sort of a small backpack on their back with a diagonal strap here and there is a collection of sensors in this little device which is near their mouth and also near their ear so this is important they could have done this they could have tried to do this on a cell phone but again physical context most people have their cell phone in their pocket and it's not going to be able to pick up normal speech as they're having a conversation okay the sensors are gonna record during the six-month period it's gonna record the volume of their speech these 24 grad students and how they speak but it's gonna throw away what they say right so this is a pretty intrusive experiment and remember in HCI here we need to think very carefully about our stakeholders these 24 grad students have agreed to be participants in this research study but what happens if there's a third person standing nearby that's also chatting with them and knows nothing about this study right they did not agree to anything so we want to make sure that we're not actually recording any actual words in the actual speech just how things are said not what is said okay from this raw speech data that is picked up by these microphones in these devices here can we actually learn a social network who among these 24 graduate students are friends the only thing you have is this raw speech data or sort of volume and how they speak it's actually not an easy thing to do of course we could just give them a survey and they could tell us but we want to see if we can infer it directly from ubiquitous technology the sensors that they're carrying around okay then assuming that we can learn the social network is there a relationship between the way they speak and where they are in this social network that is an extremely difficult thing to do if you're just a social anthropologist watching a bunch of humans interact socially very very tricky to get at right intuitively we think yes we probably modulate our behavior based on who we're talking to but how and to whom do we need to speak for that behavior to change tricky okay okay so obviously this is very intrusive so these 24 subjects although they were wearing it for a six-month period they were only wearing this backpack during working hours they only wore it for one week per month out of these six months and they only worked for six months okay they're wearing this device here this msb and again this is sort of state of the art technology ten years ago inside this device there was a microphone a tri-axial accelerometer which means that it can measure roll pitch and yaw the three ways in which the thing is rotating infrared visible light a digital compass temperature barometric pressure humidity they threw all of that away and just use the microphone data the microphone data passed information into a PDA which was in this backpack on their back and at the point of capture when they're capturing the data they immediately scrubbed out the words and this was again important because you could imagine capturing not only how they say things but what they said capturing it and then offloading it to a device and then scrubbing it sort of downstream but in order to really try and protect everybody's privacy they were scrubbing out all of this privacy information upstream at the moment of capture okay and then they had an SD card that was storing speech statistics so volume rate of speech and pitch which we'll see in a moment okay here's some of the raw data let's have a look at this data for a moment again the captions are a little small here each of the five channels here corresponds to one of the five working weeks Monday Tuesday Wednesday Thursday and Friday the horizontal axis is from 9:00 in the morning until 8:00 p.m. and the vertical axis in each of these panels is perfect person seconds speaking or pß this is the total number of minutes that was recorded by these devices of someone speaking you don't know who is speaking but just there was there was speech being heard at that point okay so for example if there were two people that were talking at the same time if two of the grad students were talking at the same time during that period there would be for two seconds we sum it all up if there was one person talking for four seconds comes to the same numbers who are just aggregating all of this information okay who other than the graduate students might be recorded by this microphone professors right so again we're thinking about now the physical context of the social context cultural context where are these grad students between 9 a.m. and 8 p.m. if you were all wearing these devices you'd be recording my my voice and you'd have a PSS of 75 at the end of this lecture who else might these microphones here exactly so two of the grad students are in conversation my microphone is picking up my voice and your microphone is also picking up my voice so we have a doubling who else I guess if you're listening to YouTube during this period and someone's talking in the video might pick that up as well right so not a person who's physically present okay so we're looking at raw data here we haven't done any analysis or manipulation of the data yet but you can already start to infer something about the behavior of this group what is it oh did you have a question yep it would pick up music but as long as there wasn't any lyrics in it then it would ignore that so the microphone was looking for voice someone speaking so actually yeah lyrics songs where someone singing would be tricky so 10 years ago I don't know if they're machine learning algorithms were sophisticated enough to pick up a voice in a song my guess would be that that would be counted as other what can you tell me about this group of graduate students yeah looks like Monday Wednesday Friday it's a little harder to tell but there are clearly big blocks Tuesday and Thursday morning what do you think those blocks correspond to a class that they all share it right it was probably a class in which most if not all of the graduate students were present now weather was a seminar where they were all speaking or a lecture where the professor was deep it was talking hard to say right but even with this raw data you can start to infer something about the behavior of this group okay so let's go now from the raw speech data and try and do the social network inference part let's assume in a particular point in time we have three graduate students sitting in a classroom and the three of them were talking amongst themselves while they're speaking another conversation between grad student four and five starts up in the hallway just outside the classroom and that second conversation starts before or the first conversation ends and assume that these two groups are far enough apart maybe the doors of the classroom are closed that the microphones 1 2 & 3 are only picking up voices 1 2 & 3 and microphones 4 5 are only picking up voices 4 & 5 okay here's my little cartoon on the right of what the microphones might actually pick up so we have speaker 1 2 & 3 and we can see that for a certain time period the volume of speech is either medium level or high for microphones 4 & 5 there is also period during which volume is either medium or high and the rest of the time in this time span the microphone is saying there's no conversation or there are no voices nearby that I could detect how do we go from that to inferring that 1 2 & 3 are speaking amongst themselves & 4 & 5 are speaking amongst themselves okay so there's this period during which they overlap right 1 2 & 3 that volume is either medium or high but that's not sufficient to infer that they're in conversation amongst themselves because if you are all wearing a device we would infer that you are all in conversation amongst yourselves for the fault for these 75 minutes right but during these 75 minutes none of you are talking to each other so I am no one else can say anything about who's friends with whom among this group so that's good to start with this but we need to do something a little bit more specific high volume drops off the safe like at around the same time this five starts up saying you know that they're responding to each other I went to increase drop pick up magic exactly there's a relationship between high and medium volume among these spans and in order to detect that the investigators use mutual information this is sort of a fancy form of correlation what mutual information tells you or when mutual information is high if I record a particular measurement here whatever the value of that measurement is it allows me to predict some other measurement somewhere else there's mutual information when I capture information here that gives me information about something else that is happening somewhere else okay so how are we going to apply neutral information well imagine we march from left to right along these graphs or marching forward in time and we notice that there's a particular period of high volume so we're gonna focus on this particular point in time here and the volume is very high here and saying that I hear speech so we can be confident that microphone two is picking up the fact that speaker 2 is talking remember physical context the device is very close to the speaker's mouth which makes it relatively easy to say when the volume is high enough even if they're listening to you too but a very high volume they can tell it's the person who's wearing the microphone who's speaking so during this period during which speaker 2 is speaking that's I've got some information now I know speaker 2 is speaking and I look at the other 23 microphones remember there are 24 grad students were in these microphones and I try and predict what the volume is at those other 23 microphones and it turns out that for four microphones one and three they're always at medium volume during this time period whenever microphone 2 is high so they must be near enough if the volume is medium they're near enough to be hearing what speaker - is saying so there's high mutual information there so during this short period of time I can conclude at least that speaker 2 is talking and speaker 1 speaker 3 speaker 1 and speaker 3 are probably listening maybe not maybe they're just nearby and have their headphones in but I can be relatively confident they're they're close to one another and I can do the same thing with speaker 1 when they're high these two are low certain medium and when speaker 3 is high there is high mutual information here whenever this is high during this period I can predict confidently that these are medium so there's high mutual information among these three microphones during this period whenever one of them is high the other two is medium I can confidently predict that so I'm going to conclude if there's enough of that mutual information that one two and three are engaged in a conversation during that period at least one of them said something and the other two I can't really conclude they were listening but at least a microphone was capturing the voice and then they also said something and the other people may or may not have been listening but it was captured right can't we can't measure the conversation directly but there's enough sort of corroborating evidence here that they're engaged in a conversation okay now it turns out that when speaker four is high during this period but microphone of speaker 3 is also medium which seems promising but mutual information is actually low in this case if I take the period of time during which microphone 4 is registering high volume there's low mutual information because I can't predict the volume of the third microphone in this case while speaker 4 is speaking sometimes speaker threes volume is low medium or high so there's low information which is evidence against the fact that speaker 3 and speaker 4 are engaged in a conversation during that time period so far so good okay so again this is just a cartoon but you can imagine doing this for these entire six-month period and capturing moments in time in which subsets of the grad students are in conversation we have no idea where they're in conversation there may be multiple conversations going on among subgroups at any given time but even the information that we're using here we can infer conversations as they occur okay so in this cartoon case here we infer s1 s2 and s3 are in conversation and s4 and s5 so we're going to start to take our raw my raw volume data and transform it into a literal social network where nodes in the network correspond to the graduate students and edges connecting nodes together represent those individuals were engaged in a conversation at that time okay and we remember we're doing this for snapshots of time so among I can't remember I think five or ten minute periods during this entire six month period they were creating these social network snapshots in every snapshot there are always 24 nodes because there's always 24 graduate students and then there may be zero one two or more edges which capture the fact that a subset were in conversation during that time we're going to take all of those snapshots there were thousands of them superimpose them on top of one another and just count the number of edges among those frames and in this cartoon example here there's one edge here but not here so we have a count of 1 s1 and s3 we're in conversation during some time on Monday and then s1 and s3 were again engaged in conversation on Tuesday so we have a count of 2 so at the end of this processing period we have a single graph that has 24 nodes and we have weighted edges where the weight of an edge represents the number of conversations that we have inferred among that pair of individuals over that six month period that's the social network that we have at the end okay so we've gone from ubiquitous technology to a social social network great that we can do that but so what's what can we do with this information now we can get to some of the interesting things which we can say without the grad students telling us here among their group our friends and who aren't we can start to ask questions like how do they speak or how do they change the way they speak depending on where they are in this social network remember that we don't know exactly what they said only how they said it and it turns out that they captured four features of speech remember all the way back to when we were talking about the K&N algorithm and there were four features of the flowers the length and width of the petals same idea here we've got speech data now we're gonna boil all that speech data down into different features so at a given point in time when we heard someone speaking we can tell the rate at which they were speaking were they speaking quickly or were they speaking slowly pitch were they speaking with a high voice or a low voice turn length how long was their microphone recording high volume before their microphone recorded medium volume so they've handed over the floor to someone else in the conversation and that person who started speaking turn frequency if there were two or more people engaged in conversation how quickly or how short were those tank periods in which the microphone was registering high volume what was the rate or frequency of turn taking in the conversation and remember that we have these four features over time so in any given conversation we can ask what those four feature values were we can derive that from the raw data coming off the microphone it's so far so good okay okay so now let's start to test some hypotheses about social behavior in groups of humans that were difficult or impossible to pose an answer before we deploy this technology first hypothesis we're gonna look at is do people change their normal way of speaking more when they speak to strangers than with good friends this is kind of a odd one to think about what is your normal voice or non normal voice mean so let's try and quantify this we're going to compute three values to begin with the first one is bi / J so bi / J is going to be the mean of person eyes speech feature B we have four possible speech features so we can plug into any one of these four for B we can ask what is the value of that feature for person I when speaking with everyone else they spoke to among those 23 other graduate students during that six-month period so I am person I I'm friends with Jack here Jack might be J I can ask how did I speak when I spoke to everyone else that I spoke to during that six-month period except for Jack except for J then I can also compute B I aro J or I'll just say bi J for now the mean of iSpeech feature V when speaking with J so at any one point in time we're gonna pick within that group of 24 graduate students to grad students I and J and ask how did they speak what was the the tone of their voice or the feature of their speech when I spoke to J and what was eyes way of speaking when I spoke to everyone else except J okay we're then also gonna compute C S sub I which is the standard deviation of eyes speech feature B regardless of the interlocutor interlocutors the other person that I was speaking to some people speak very differently when they speak to different people which would mean that person I would have a high S sub I other people behaviorally just don't change their speech very much depending on who they're speaking to they have a relatively low s sub I so we're gonna compute this value D we're going to divide by s sub I so we're basically normalizing by s sub I or we're trying to remove this other piece of behavioural data which is some people change their voice more or less depending on who they're talking to we're removing that piece for now okay we're going to normalize this value D sub I J which is the amount that person I changes their speech when they talk to person Jade so I speak to a whole bunch of different people and I also speak from time to time to Jack who's going to be person J D sub I is going to tell me how much I change my way of speaking when I speak to Jack compared to my average way of talking to everyone else that I talked to during that six month period so far so good okay so we're taking the absolute value of VI / J minus bi arrow J what would happen if the numerator would at me if the numerator here is large what does that tell you about individual I and individual J so we're subtracting two valleys and taking the absolute so if we want to think about a high value that means that those two elements in the numerator have to be very different which means what you talk to I talked to Jay very differently from the way which I talks to everybody else right that's all dij from the moment is going to tell us so far so good ok finally we're going to compute C IJ which is the fraction of time that I speaks to J okay so we have in hand now C and D and we've computed it for just two people I and J and only four cases in which I is talking at J and also talking at other individuals we're going to compute C and D now for every pair of individuals among that group of 24 and it's too early in the morning for me to do that computation but there's quite a few of them right so we've got a whole bunch of DS and a whole bunch of CDs and we can then ask is there a correlation between C and D and it turns out that there is there's a negative correlation which means lower see higher V and higher C lower D okay remember that we're computing these values for B which is one of the four speech features that we have here so I'm writing out or recorded for you here the for our values which are these four correlation values for each of these four features so we collected all of these C's and DS for rate look to see whether there is a correlation and there was there was a negative or anti correlation then we computed all the C's and DS four-pitch and there's still a negative correlation but it's less did the same thing for turn length and turn frequency so for almost all of these for speech features higher cement lower D reserve lower cement IRD what does that mean let's just focus on this one among all the grad students whenever C was lobed he was high what does that mean that's it right which again maybe in retrospect makes sense but now we quantitatively have that in hand and we can ask how do they speak differently well they change their rates they also change their pitch they also change their turn length and the last one their turn frequency may or may not be significant so I probably wouldn't trust that fourth one but at least for three of these speech features yes question so R is the amount of correlation or anti correlation so ours a value that can range between positive 1 and minus 1 a value of positive 1 would be perfectly correlated whenever one goes up the other one goes up when one goes down the other one goes down and our minus 1 means they're perfectly anti-correlated whatever one goes up the other one always goes down and when one goes down the other one always goes up an R that's near zero means they're they're independent there's no real correlation between them so the negative number the negative value here tells us there's an anti correlation most of the time if C goes up G goes down and vice-versa but it's not minus one it's minus point two so it's not that strong which you would expect dealing with human beings human beings are very complex we're also using some technology which isn't perfect so it's it's kind of washed out the p-value over here tells us the probability that our hypothesis is wrong ok so a P that's close close to one would mean there's a hundred percent chance of that hypothesis is wrong people don't change their way of talking when they're talking to people that you don't talk to very often the lower a p-value is the lower the probability that worthlessness is wrong you can thank this statisticians for all these double negatives right it'd be better to have a probability that tells us a chance that our hypothesis is right for certain reasons we do it the other way around right so we can say it looks like it looks like there's very low probability that our hypothesis is wrong right which is kind of a modest way of saying I think we're right make sense okay if you're following so far you might then want to answer the question well did they speed up their way of speaking when they spoke to strangers or did they slow down their way of speaking when they speak to strangers this says nothing about that just that they changed their rates of speaking you can make arguments for either case you could imagine going diving back into the data and computing some other values you could probably actually get at that hypothesis as well okay okay let's look at the next hypothesis so if you look at the top of the slide for a moment so we're also we're going to look again it whether people change their normal way of speaking now we're going to ask do they change the way of speaking when they speak to well-connected people this one's kind of interesting first of all what do we mean by well-connected people and if people do actually change their normal way of speaking when they speak to well-connected people that means they know who is well-connected among the group of twenty four graduate students okay so same thing we did before we're going to compute VI / JD I arrow J we're also going to normalize by S sub I so we're going to come back to D IJ remember D IJ just says how much I changes their way of speaking when they speak to J now we're gonna change our C not going to CIJ or just CJ so I is speaking to J and J is going to be CJ now is going to be a measure of the centrality of J in the social network what is centrally it is simply how many other people and how many conversations J had with everybody else as you can imagine this is a proxy for popularity or social hierarchy we're not going to use those words because they're very loaded we're gonna just say we're looking someone who has high centrality tends to engage in lots of conversations with lots of the other 23 graduate students so someone who speaks rarely and when they do they only speak to one or a few other graduate students would have low sensuality so far so good okay so now again we have a collection of DS and C's for all the i's and J's among the 24 graduate students and we can then ask for each of our speech features is there a correlation between them and in this case there's a positive correlation you'll notice that all the values here are now positive again with the exception of the fourth one there which is not significant can't tell anything about turn frequency but for the other three whenever C is low D is low and whenever C is high D is high let's talk about low C and Lodi what does that mean about the behavior of the group so low C is low centrality absolutely so remember that is C such a here so J is the interlocutor it's the person who is being spoken to and I is the person who's doing the speaking if there's a lot coming back the other way so there's another hypothesis if you have high centrality you're engaged in a lot of conversations is it by direction is it as bi-directional as in other conversations I don't know maybe and in which case there might be a little bit of circular reasoning in here hard to say let's assume that that's not the case for the moments low CI so if you're talking to someone who has low centrality you as Speaker I do not change your speech very much compared to how you normally speak but when you are speaking to someone that has high centrality you change your way of speaking much more than the way that you'd normally speak at least in terms of the rate at which you speak the pitch of your voice and turn the length the basis is bi / J right so I'm I and I'm speaking to Jack you want to know what my normal way of speaking is it's the way I speak to everybody else that's right I'm surprised by this finding some people yes some people maybe not so much when the Dukes right possibly right absolutely so I may behave differently there's clearly probably introverts and extroverts in this in this mix so we don't again we could then ask hypotheses about why we got that obtained this particular finding and maybe the raw data that we have is enough we could go back and condition the data in another way and we could ask something about C sub I what about if the speaker has low centrality or high cent related what sorts of relationships could be find there okay so think about the results that we have here imagine that going back 10 years before this experiment so going back 20 years and trying to test these hypotheses without this technology right nearly impossible as we know from social network apps there's lots of interesting social behavior that was hard to see before apps existed but there are also a lot of subtle social interactions that go on when you're speaking face-to-face with someone that have never been measured there have been hypotheses about those behaviors but it's been impossible to test them before these are just two hypotheses you can start to imagine many many more makes things interesting about what can we learn about human behavior as humans are walking around an environment which for better for worse we're being recorded in lots of different ways interesting to think about okay so that's lecture 17 we're making good time here so we're gonna jump into the future a little bit I haven't had time to put up lecture 18 yet on that tivity tagging we'll start in on the first few slides of that this morning and then we'll come back to it next Tuesday okay so we're now going to look at a second project research project and again this one has to do with something that was difficult or impossible to do before which sits within a larger domain which is now known as mobile health monitoring we'd like to be able to measure a near real-time as best we can our physical and mental health the health or physical and mental well-being or at least some of us would like to do so tricky right again we could survey all sorts of things well we can't measure well-being directly the only thing we can really measure are the behavioral correlates of well-being right and again this is already controversial but generally most people would agree there are aspects of your sleep behavior physical activity intellectual stimulation social interaction diet stress and so on there's something about those behaviors that positively or negatively impact your physical and mental well-being so we're gonna try using ubiquitous technology to not measure well-being directly but to measure some of these correlates and then use these correlates to predict physical and mental well-being again this is gonna be a little bit tricky because what do we actually mean by well-being here okay diet has been around for a long time long before ubiquitous technology technology at all you might manually record what you eat now you can do it in an online journal there are I would imagine hundreds of these websites now where you can actually do this I don't know if you've ever tried but it is a real pain in the butt to do this what are some better ways we might be able to record diet than having to type in how many peas you ate last night absolutely right so that's a possibility but again that's tricky because maybe you're not just buying for yourself and if you buy 20 pounds of hamburger how quickly did you eat it all photographs yeah I think there's some websites where you do that now you take a picture of your food and uploaded it it tries to analyze that again that has its pros and cons monitors at blood-sugar yep how exactly we're talking about wearable technologies in a few weeks time so we're talking about diet you know wearables might be a way to go there's actually a sensor out there now that you could wear on the inside of your mouth that will detect sugar and protein and fat content and whatever you eat I don't know about you but that seems a little bit unacceptable to me from an HCI point of view but maybe not okay diets tricky some of these other ones are also tricky so let's see if we can do this in as non-intrusive away as possible so we're gonna try and create monitoring technology that is acceptable so I'm going to be voluntarily willing to use it and we're gonna try and measure these things continuously and automatically so rather than asking someone to type in what they ate for dinner last night again we want this technology to fade into the background where they can just go ahead and eat normally and we'll try and record it as best we can and as continuously as possible assuming we can measure these behavioral correlates of well-being we're going to create some machine learning we're gonna create a model that takes as input measurements and produces his output a prediction about well-being is whatever it is that you're doing positively or negatively impacting your well-being some of some of these aspects will be easier than others as you can imagine okay then in addition we're going to create a nice visual interface where we'll show the user the the raw measurements continuously and let them also draw their own conclusions so perhaps depending on who you are you would rather have your app tell you that what you're doing is good for you or bad for or maybe you'd rather just see the data itself and make your own decisions about whether how you want to modulate your behavior so again this is a massive undertaking this is a 3040 year project for society as a whole we're gonna look at we're gonna basically look at the low-hanging fruit some of the easier things that are able to detect which clearly have an impact on your well-being most of you if I were to ask you could probably tell me how many hours of sleep you got last night most of you could probably tell me whether you've got the right amount of sleep or not more or less right not exact but more or less how many of you could tell me how many hours of sleep you got the night before the night before that the night before that did you get on average more sleep this week than you got the week before maybe since it's midterms you could tell me you've probably got less than the week before what about this month compared to last month right so again you could probably do this more or less you have the data available to you which you have but that data fades pretty quickly yeah which comes back to this time period so in terms of the short-term we're probably never gonna do very well right you feel great when you had Berger you don't feel tired now but if we could see how many burgers you eat over the last two weeks or how many hours of sleep you got over the last two weeks then maybe we could make a prediction about your well-being over the next coming couple of weeks good question right exactly we won't have to because you're gonna forget pretty soon that we're actually recording it in the short term maybe it'll bother you but if it's doing its job it should fade into the background this is all again assuming that you voluntarily want to do this okay which of these one-two-three four-five-six correlates are easier to measure than others not which ones are more predictive of well-being which are easier to measure than others sleep is an easy one physical activity may be little trickier why is physical activity a little trickier than sleep true let's put let's stick with physical activity for a moment we're gonna try and measure this continuously and automatically why is this tricky absolutely and this experiment that we're going to look at today 2011 this was one of the precursors of a Fitbit right so the technology now it definitely is easier I don't have a Fitbit I have a smartphone when I go on the treadmill I don't have my cell phone with me so if I'm not wearing a Fitbit it might be difficult I might manually type it in after the fact we're assuming want to try and collect it continuously and unobtrusively as possible and Fitbit is the best attempt so far which do that what are some other correlates here that are easier or harder than others social interaction right so difficult but not impossible getting easier ah exactly right what do we mean by social interactions we can measure maybe face to face conversation maybe we can also record online social activity using a different set of sensors and bring that data together how many conversations were you in throughout the day how many of them were in person and how many of them were online could probably start to get at that that data stress is easy to measure how so so Fitbit or other wearables you can put you can get it physiological responses that correlate pretty well with stress ok so again keep in mind this experiment we're gonna look at 7 years old now and it sort of shows the rate of progress in HCI a lot of what you're going to see here is already outdated but it's an interesting experiment to look at because it was the precursor for a lot of these devices that now attempt to try and measure continuously and unobtrusively correlates of physical and mental well-being here's the little fancy figure from their paper so we're going to go around around the cycle of monitoring behavior and then using raw data to try and model well-being so we're gonna take raw number of hours of sleep put that through a model the model is gonna say you haven't got enough sleep over the last two weeks or you're getting enough sleep over the last two weeks and then provide that information to the user in some way does this user want to see the raw number of hours they slept with a rather an app that just says you're doing well or poorly in terms of sleep depends on the user what exactly do they want to see here's a snapshot of the app that was created reported on in this paper so in the top left panel here we're seeing actual raw data and we're seeing the beginnings activity tagging so green here is does the microphone or the smartphone register noise something else a voice complete silence and down here the accelerometer in the phone is starting to infer something about activity so the app thinks that you were stationary at a certain point of time walking running and what have you which activities are easier to measure or predict than others stationary is pretty easy absolutely so walking and cycling that's relatively easy okay yep sound is relatively easy as we saw in the previous lecture you sleep okay what else okay let's carry on look at some of the other data so again at the top here we've got more or less raw data and as we move down here we've got more conditioned data so in the middle panel here for three different days October 8th 9th and 10th they were looking at the number of hours in which there was some sort of physical activity walking running cycling and so on social activity what went into that box not quite clearly yet and then sleep I feel bad for this particular person they had 10 hours of sleep on one day and none the next day and then the day after that seems a little strange ok let's focus on sleep ok so sleep seems like it would be an easy one right ok let's assume you have to follow we'll end with this example today let's say you've got the following sensors available on your phone you have a binary indicator of whether the battery is being charged or not you have clock accelerometer this GPS microphone and camera how are you going to take that raw data and turn it into number of hours that someone slept per day okay so silence night battery's plugged in and what am I missing GPS at home okay so we could take four of these elements and try and combine them to corroborate the fact that you're asleep right any one of them might not be very reliable we can combine them it's that good enough midterms are a good time to think about this right is the only sleep you're getting at home at night I know when I was an undergraduate I nodded off in the library from time to time right which maybe misses half an hour an hour or when I was really stressed two hours depends on the time of year for me right so good but again there are those edge cases that are going to be tricky to to capture think about that you have a quiz due tonight starting on deliverable nine at your leisure professor bag role will be guest lecturing on Thursday I will see you all on next Tuesday thank you 
I2XF2_NR2ps,24,"Talk by Prof. Antti Oulasvirta, recorded at University of Tsukuba on August 20th 2018

Title: Computational Design Thinking: AI in Human-Computer Interaction

Abstract
Despite extensive research on user interface (UI) design and artificial intelligence (AI), no AI method exists that comes close to the level of a skilled UI designer. Designers routinely engage not only in problem-solving but in sense-making of data and reflection of their performance. AI methods applied in UIs are more limited in scope, for example focusing on isolated elements of UIs, such as in recommendation systems. I argue that AI has been limited because no AI formalism exists that captures the essence of “design thinking”. In this talk I discuss elements of computational design thinking and the methodological foundations for artificially intelligent design necessary for human-level or supra-human level in user interface design.",2018-08-29T07:40:48Z,Computational Design Thinking: AI in Human-Computer Interaction,https://i.ytimg.com/vi/I2XF2_NR2ps/hqdefault.jpg,Bektour Ryskeldiev,PT1H1M18S,false,275,4,0,0,0,this one is a first one is for like one hour the political figures of talk and then and sitting next off would be 30 minutes 20 minutes talk and so mostly like we are having some input clock keyboard and also the second speaker is from dedicated to that so okay let's first introduce him so he is a state professor there and he graduated from Helsinki would have invested missing and incomplete science in 2006 and then he went to UC Berkeley and senior researcher teenage ID so instead of nicosia then went to sell broken toes next Planck Institute of Informatics in Germany and then he has been performed mr. genin HCI and science he has conference he is high in thousand eight I think twelve so without any further detailing I think we should introduce this talk so computational design thinking AI and she went up it any interaction so he will really focus on how the connection between yeah yeah overview so pretty small crowd we can keep it interactive so you can instruct me and thanks for having me here just arrived this morning hello is from Helsinki review from the markets in there this is my university that's smelled me engineering it's a merger a university between technology business and arts pretty brand new is seven years old knows this is our group we are consisting of Lamentations computer scientists electrical engineers robotics student and not all of them are permanent members I'm going to talk about four things that we do in that group gives a really brief overview of what we do we use computational methods for adapting and generating user interfaces things like automatically enhancing or or the human perceptual system human vision inferring what people who want or what kind of behavioral properties they have or homely properties and personalizing the face is not generating very complex interfaces that can have like up to hundreds of functionalities and then also using machine learning to seek to have better I better simulation so what people do so these are the four areas that before walk in a little bit of background that introduced to someone some of the key players in this area this area called human-computer interaction how many of you actually follow that or heard about okay so I'm gonna start with basics we are using user interfaces all the time we actually struggled with this very old user to this year just to get this present basement running excuse me face everywhere and you know pain the question is how should they work what time should be designed there what are the principles of that that make them big versus back and most of the interfaces that we have our one shot designed somebody designs them and they are like that from maybe you can don't have any intellectual capabilities per stage I got to who you are or what you want the other factor here is the design practice so if you think about toddlers interfaces are designed but they move the billion-dollar business if you think about how much Apple Google Facebook invest into design and innovation of interfaces this is super important is a big profession as well both the interaction designer most of this work is many of you know somebody has an idea implements that studies even evaluates that though strategy it's very different from a profession formulated a civil engineering think about how big bridges are built or our houses you would not like them to be built using those methods of those people because there's no guarantee for what they do we will have to change the profession of interaction design to be more systematic driven by first principles the third actor here is the AI human many discussions about what can be automated there's one from Harvard Business Review there's just a level of automation going up and hourly wage and if you look at some of these box designers are somewhere between five and fifteen percent so that's the estimate of some of the business groups or comments will be automated however things like this based on anything analysis about this design consists of interaction consists of four what AI is capable of so now that I mentioned AI of course you will know the deploying breakthroughs that study properly 2014 and now are witnessing a little bit of backslash saying that a winter is well on its way is not helpful at all for our business we'd like to understand what can we make and what format they are going to be useful for so you can see we're going to show a lot of applications of AI or let's say rather machine learning as much deep learning as a basin of ACN approaches so the question is for how can we constructively involve a human-computer interaction as opposed to seeing them as automating designers also have been very interested in what can be automated that was throw away oh and there's a classic piece relating and inspire us who was one of the forefathers of intellectual research design the established the field of design of research and he was looking at this for a long time and is that he know that he came and he was saying that design has all these qualities that AI of that came I was mainly based on logic and reason logical reasoning does not happen but once you engage in the in the business of trying to formalize algorithmically solving some problems you actually come to understand about the natural intelligence of design and this would be very beneficial again there are some thought that we're going to go through three claims relating to all these things first of all are you sure that people week a on so this task specific a is artificial intelligence or certain is not has been distracting factor is for any reality and so our be here in support to show that thanks to that we actually can do much better fear rising at modeling human interaction is strong when you start from a little bit of basics talk is entitled computational design thinking well what is computational thinking of tasteful thinking is infographics and EDC consists of decomposition problem abstraction being able to see - a form of the problem from the instance have been recognition an algorithmic solution of this problem spaces for facebooking nothing work for example how would you instruct a robot to pick up some milk from the fridge so we decompose problems we abstract problems and algorithms for solving them this is conditional thinking so when you go to commutation on interaction computational design then what is mean means that we apply with this notion of connotation of thinking the interaction between a computer and a given money more precisely means this person builds on this we have a more mathematical system tying together some data coming from the user designer estimators of the models that have been used to optimize or change them the design to speculate but what would happen so all the foundational design involves a sort of model so updating human data but this already a radical departure from the regular way that we do human-computer interaction which is based on innovation and evaluation because we see already I think that we have to have some sort of mathematical modeling in the center off of what we do well what are there's nothing like a performances we just this year we publish the book I'm gonna touch on some of these summarizing over all of these what are some benefits that we're looking for looking at we're trying to look at design an interaction that is scalable because once you get algorithm right you can we can reproduce that in mass unlike in manual design we have analysis in that we can we can make testable rigorous descriptions of interaction and work directly on we can improve designs we can support designers and adapting the faces of men good show simple civilities finally before I start doing more about what we do through the historical context a has consist of many waves just like human-computer interaction as well you say cybernetics which actually created many of the foundations that we should be referring to in our field war disbelief and then we had Logic Pro clean water especially in one machine going to so they're going from formal methods and formal reasoning become D models in 1990s and then it's a support vector machine I'm trying to combining ideas and abilities one one main point being that pattern recognition per se so the pure quality you get from people anyways alone not sufficient for what we need to be more precise we need basically we need something else because we need control who explain ability and that's there was a recent summary of of Maine authorities divided into planning or free learning model basically what I've been showing is combining many of these into but could be called white boxes we can peek under the hood we'll find it theoretically possible theoretically justified models that actually make conference like psychology or biomechanics that be paralyzed using machine learning and that we then use using machine learning optimization to do better guitar this makes this approach is controllable and explainable also less less data dependence of it lots of good qualities come if you're too busy dropping white box something a little bit about optimization first this and I'd be working on since 2011 and is though not my idea August poor are cool or simplified keyboard he's one of the pioneers he will fight frightened behavior and he had to let fifty different they're having some psychological principles loss or effects that he found that he then used manually read asunder latest binary many others had also proposed the same idea of using psychological models and theories directly or systematically or algorithmically even it is not better interface all of those eventually boil down to a very simple idea and this is actually quite enlightening if you never lets it if you have basic computer science that you understand that I can be organization method that goes through impossible tentative designs that's voluntarily first to find the one that maximizes some goodness martin jean under some tasks instance specific parameter speedo all the examples that show fall within this basic notion so there's a predefined design space predefined witness function the feet are may change based on the castings we can show one of the four areas to work which is keyboard layout the position there's work going all the way to the seventies from the keyboard germany to the US is one example mm style stylish PDS and stylish like a virtual keyboard basically the question is that how do you place letters like this on to keep pre given key slows like that and there's of course in factorial number of ways of doing that because where n is the number of layers and then you have perform like in the optimizer some objective in this case that would be to try to minimize the finger travel time weighted by the probability of those their little transitions occurring in for example enh would be very common as tip and X would be less common and and anyway that use that as a way way some model of how quickly people can pull the cleaner itself in this case that's more important slow and it's very simple basic themselves that gives the moment balance a function of distance and with then again thrown into the black box missus like simulated annealing I never thought this was the standard paradigm I'm building more as well basically having seen that my formerly the task of an objective and an optimizer followed nice approach we can actually generate quite interesting interfaces already well wait further than keyboard never listen to show you one one keyboard is working with the French standardization organization quote up north and this is coming out 15th of September or next month we have redesigned the special character mapping using an octave Western thinking to count the performance mobile-specific character so like you know control sheep that's all we didn't spawn performance learnability economics and family are formulated those mathematically and so the problem is you know that integer programming of this cardinalities an immense problems there's a 10 to the power of 80 atoms in the universe and this is way powerful even solve it efficiently using events so now this is going to be part of the French know following similar ideas of what is called an assignment problem we start following more complex problems like many design menu following similarities may have to assign collapse groups we found a new way of doing that providing to two objectives the speed of selection and associations between selection and another one you know we express that mathematically and then we rigorously compare that in a controlled user study against commercial baseline designers but don't be firefox nightly news with many 6% passively use so it can increase the number of functions that even offer only only need to give is a list of frequencies of commands and list of pairwise associations for those humans this son of another piece of work tonight but part of theirs there's a possibility of using perceptual models pretty basic perceptual models of human vision to think that the default talk like this wouldn't show any other structure in automatically produce something that where the human vision and much better see the structure underneath this optimization our objective conscious life on addressing contrast perception brightness perception do you need formation when we see units perform all together actually they were all together ten ten particular so I tend not to be more ecstatic anyway if you think those things are conjugates I know there's the gallery of layout jester's also pretty many aspects of practical interfaces can be addressed this approach they are limited in the sense that if you start putting them all together they would break down intelligent control you know you're not able to produce light people can ask me what can you produce the next Microsoft version well probably not but I can I can open much the toolbar I got to watch the menu I can open some comments with you we're also looking at game related performance this is one example that we published this year two years back we have more low slope how accurately people that respond or select a target in tight and so you're shooting an anywhere except for jumping and as a function of how quickly it goes how much is the preview window for the perceptual system how wide is the color that we have an account and even pretty pretty accurately whether you can the guy on then you can use that as a principal okay the whole design there is another one where we did skin reading so if you have a long document you really hard anything from maybe 100 words per minute to show you a completely different form factor that would not get sold using okay so take the one step back and look at well what does this optimization mean how is it fully done so in optimization you have space of pending designs a couple of TV's on space somewhere there use the set of business message you interested you have to have a process of searching best algorithm this computation method and you have to have a soldier that does that the solver can be anything luckily they are very very good methods that most methods and then exact methods for that is this you have to formulate the objective function what constitutes a good design in this case then you have to parameterize the task instance that okay this time I'm designing for English learners or Japanese Alisa these are the characteristics of the case and those can come from designer or user we talked about this example we're talking about interactive tool that interacts with the designer so that the designer can specify this this route you talk about inference later so this is a way I teach computational design that there's this tool put three legs school there's a normal interaction that's also a prediction about linking in key design and outcomes in interaction and then there's a process of optimization that goes to designs must be some some way of inferring the parameter from data and there must be some way of can speak freely what if I change the menu like this what will be the consequences to our users I'm going to talk about all these links in the lesson so optimization is about the process of searching for desirable designs rhythmically most of you must know organization there's an organization landscape you have two variables first designing the x and y coordinates of a button - landscape here shows compute objective functions were predicted to be so anyway what the organization is doing is trying to fight for example the highest peaks well in this case there would be quite many others and all of these is trying to find a good compromise between how widely explored or just when you drill down or exploit and what makes in the that design that almost unique is we have multiple user group these are not techniques the multiple practices that we have to take into account at the same time for example when we decide the French keyboard you have to take into account how to type social media like you know a target tweet it's a very different presidents than the other ones because they can't account for more French lands that they write in newspapers we also need to take four very very large design spaces that are very hard for humans also hard for algorithms you know personally if you have a menu that we need to organize hierarchically fifty people that's at least a minute in the bar when it's the eight ways organized in the park but that's why humans when they design basically fixate with an own menu menu design and starts with just 25 per month and this is exactly where algorithms can help designers one outcome from this is that design is the person finding that the least worst least worst in the sense that he's always come from us of somebody or some objective alacrity here's an example from with webpage organization let's say five different objectives and if you optimize any one of them individually they don't make any sense if you take all of them in some carefully weighted balance and you start getting it's also nice about it you can use it as a toolbox for problem solving the best trade-offs between competing objectives focused organization meaning that you find the best most the design that is most robust agencies and assumptions for example if the users changed or here initial values the projected optimizer are to do explore feel transitional catastrophe explore widely diverse design ideas and the local optimization which is where you take an existing product or design it's just basketball if I was to suppose if I was to make just one one change to that bucket that me they can use across it done if you can use like this they're simply going to interactively we have obviously few tools for a designer step this is from Kyle 2016 as a demo work is a regular workspace for designing wireframe designs and then the optimizer is making suggestions based on its models an optimist basically saying that if you take this layout is going to be better than this existing one in usability but worse in statistics why he's making these you can also find two colors or particular elements and working on while they actually listen to practice international designers there's another one that we work in which is way harder problem this is called Newport beside this is something that interior designers architects graphic designers use and we have a corporate abundant systems are strands recognize what they is trying to have a model of the moon port and then he spent recognized who the user is exploring exploit or exploiting and then he's making suggestions on the right that the designer can take and so you can basically start from nothing and let the AI construct a full moon or for you or you can so that was the first leg as optimization like so the second really important part that has driven us away from deep learning and like mathematical model simulation or misprediction is we need to be able to say that okay if I take this design choice and this is the consequence in terms of performance or liability or what and worse in regular areas of optimization operations research you typically referred to as a physical models or someone analysis but people think it's a work now you can't take physics that pretty how people behave I need to resort to behavioral content models if-then rules regression models or simulation models simulation model so of course the downside is that they are very expensive at evaluation practice of at least the design space that you can close here's a model of the menu selection performance is just a very reversing system so you have represents that then predict for example the time that it takes for a user to find an item in a menu so once you've given that now we can give it to an optimized but actually this menu this menu model accounts for five factors in when a designer where the item is how long is the menu what time is the menu how many times you already seen that and some other one and in in a way that has been empirically verified and so it makes better predictions on usability than a human designer because human like that now one thing that we observed is that in opposing the idea that we tried to fully model a user behavior what we actually do is we start from optimal behavior and then we start slicing away using bounds so we say that assuming that users have a very clear vision that is limited or assuming that users cannot move their fingers infinitely fast then they must be below this barrier right so we start slicing their performance envelope using balanced these bounds are coming again from psychology from the sciences they can be about how quickly you open targets but they can also be about how apartment we can say either bond energy improve the colors like this and this is that this is the best possible color harmony that people energy or understood there but these are the best possible reading material any context are is the working memory people will fail using if you don't take this so we start adding these bombs and we slice away a better better resource and which better know what we've been doing in that endeavors is been using machine learning for predicting how people adapt their behavior but there's a very big room right now in AI in reinforcement learning that comes of course studying bats and no none of those mechanisms of self and application were included in that all school cognitive models that you would see having modules that have inputs and outlets but something that was going on but it would not have that self organizing principle know what we do is because Oliver might be small spark by including machine learning in them what we basically do within treat each one of them sent him some bound temporal bound of capacity bound or whatever and then we use we adhere as you know especially reinforcement learning the estimate of how a person based utilize best-behaved assuming these counts and it has the people I'm going to give an intuition so let's illustrate like this so let's let's say I'm going to show you an interface and only introduced to find the link that leads you to politics news so I know because I've seen this and that's this week I actually use my memory unlike you guys so we have different resources I have taken sources this you have for example event with scan with some systematic approach we can try to look at the salient regions that you see with the peripheral vision you can try to recruit your memory but that's going to be slower and what machine learning is giving us is an idea of how we can solve this problem giving the resources and this is an emerging field of research in a big boom at the intersection of science it's an AI this is one very nice way of summarizing in kind of really big school you have an environment this is the facts it's the pace that you're looking at particular locations and colors and so on then you have mechanisms these are the things that are forcing you for example course for example to move your eyes you have learned very fertile vision you have slow memory and then utilities there are certain the rewards that you want to attain person we want to get to the politics page now all of these can be formally expressed in the important learning and then you can collect the reinforcement learn the strategy that users might use in this case in the best case across our studies we found that this is often a good approximation of user behavior how can you obtain that you can have ancillary utility learning bayesian brain let me give you one example for example if you're looking for a particular element here that so that's going to be easy because if you knew that it's green it's the only green will be way harder and you have to have more exhaustive scanning of that how can we pretty which strategy we use so here you see in action what I just described are some bounds and then there's a controller that learners okay so the problem statistics for the visual short-term memory long-term memory a teacher guidance like what you see on the page and of course the resource system which is the teacher you move your eyes the controller might like to learn the best policy of using these resources when solving this problem this word will be using a utility learning person now what you can do is you can simulate a novice user I'm going to keep this one which is the basic integrating hypothesis but there's this also a big thing in neurosciences right now the idea is that people people's brain operate in a in a somewhat Bayesian statistics fashion meaning that they have a prior they have model and they have data which is the input data from the sensors and then they pursue or or infer that data in the light of the prior that we have and this has been quite influential and very cutting across areas perception as well I'm going to show in the next talk keyboard beat up science talk so the last leg of the stool so we've been talking about optimization prediction is different from you get from data to the model Carmen two more parameters and these in a way keep questioning you know we have a lot of observations about people how they click what they do for the person is well why did they do that what they absolutely do so let's say that these let's go back to this example let's say that someone completed over there well what does mean maybe it was an error maybe the person wants to be here opens maybe the person has never seen them before but is exploring and I'm just trying out so even for one click to mention what's including some observations it's hard to say what actually happened so this area is called inverse modeling normally in psychology and the sciences we talk about forward normally you have a simulator model and then we go with some parametric assumptions we go to user behavior 10 of his-- boxes all possible behaviors and every because every particular behaviour is one dot now in forward model from similar to predict something as mm water behaves in inverse mode when you try to plant a parameter sitting for the model or models have some parameters that produce produces behavior that is very close to the real users behavior let's go universe moment we've been doing that with base in other words you can think those models that I discussed you can book them into is basically an optimizer and tries to find the best parameters theta but then we actually solving pretty hard problem so so the problem is they actually go to the end of this order but you're actually just even click times nothing else on a menu or webpage and now many many different properties of the human visual system characterized you as a person and can we make any assumption or any estimate who I went from the way you click times about these parameters and using that mode that has been traumatized and we produce thoughts that resemble your behavior and it depends what we can do we can get some basic parameters in resources that about fixation links archaic length just from the way you press what's nice about the spacing optimization and many other as they are like own systematic person you have used what's called the posterior distribution so let's say that we have two parameters that you interested memory parameter and if you release or system parameter and it gives this place of explanations is that well this parameter can be here but then is most likely the other parameters over here right so you can actually systematically work with the possibility of your of your of your copy which is not easy and before a statement now I'm going to talk briefly before I conclude work that we've been doing also that slide on here which is ability based optimization here we combine some of these ideas so there are many many sources of individual differences sorry anatomical difference is going all the way to the cultural differences and differences and all of those are in the end to us just parameters in suppose people are so if you think about entering text we can produce pretty good predictions of passage performance with these parameters using generally more letters and there's moose the eyes and nose the paper but now these parameter are describing also individual differences for example how much memory in my time now we can change them we can taste them not morning to you or our code including any special group here's an example of a work bench simulation for simulating extension performance so somebody's probably finished visiting and the red cup is the eye and sorry and now depending on the parameter settings it's going to find different strategies because machine learning algorithms is defined well what's the best way of solving this next century problem using the resources that I have and now what you can do is you can then start asking that what if I have fundamentally different capability this is the regular designer to do some regular device and the optimist have found that for people who are who have tremor it's actually make sense to combine some of the letters include them in the piece and of course the consequence is that then you can call again feet you can hit the piece then of course that's not unambiguously describing the words so then you have to have a great prediction is like that but it showed the simulation is that then you can actually go down from an error rate of 50 or 60 percent well Trevor people determine who's like closer to let for three percent while not compromising the worst women at least in termer patients can type it all an extension of this is trying to put all of this together you have the inference part that prediction part and optimization part if you put all of them together into one place and that's called self optimizing design it can have autonomous inference here's one example so both wasn't many so this is speeded up basically the courts but people are clicking makes an inference about what people are interested in uses a model to predict that if I change them since the menu like this this would be the cost and then over time expires adapting highlights and reorganizes the menu for you here's another one so we can follow your with browser history so we have a web browser plugin and if all of it okay this is the history that you've seen in this domain let's say that this is particular domain of corners and we parameterize the visual search model to your experience we know that you've seen logins over here and emails over there not what we can do is before you have actually seen the page we ask from the business model what on you but where would you look if you were to email before and then we use that asked of the objective function try to make as familiar as as possible and then we make this an abundance 25% faster 25% less fixations than in and that's taking into account what your history is so the conclude the very long talk here are some discussion points that I wanna race out so we started with this discussion about well what can be automated now I think we're also to you know 0 to 5 percent automation No 100 percent or 50 percent we can delegate certain tasks we can use optimizers to solve really hard problems like import design or many design problems we can use introduced in interacting with we can learn certain basic things from user data and automatically adapt some aspects as far away from placing designers so what we can do adaptive interfaces make them a self-organized we can help the designer simply looking at complex behavior and we tools and problem-solving in a building it wasn't possible before when people think about a I often think about the strong AI boogeyman it's going to automate fully what we do and I think it's even close there but I want to make this sergeant a little bit more involved and I took a definition of design thinking this couple of months back design thinking is an all-encompassing word about how designers think as they solve some problems and I least five of them and then we discussed what which one of them are and then it was not first of all the design is something about the creation of artifacts secondly it's about problem solving and given a problem find the best possible solution to that it's about reasoning and since making so complex data for analysis then it becomes harder so I mean it's a pretty common occurrence reflective practice means that Europe possibly as a human designer reflecting how well you're doing and how you have a sort of level collection and the focus even harder is how do you create meaning for example what would be the meaning of a product or a brand new user also if you think about the process of design thinking typically chooses imagery sketching iteration and I use this traffic like metaphor to keep in mind my current current views on this site I think we're already and I already shown examples and other groups have shown examples of being able to solve problems create art because as I mean as simple as they look like men used actually pretty hard to do they are less making less progress and there's a reason in a sense making we can get something we can get intense we can get people's personal characteristics from data and we can work with them in design and it seemed very good examples we are not being enclosed - what could be called reflective practice where you have linguistic and embodiment aspects optimization and machine learning is working somewhat seem you know in each area gets heat is producing candidate ideas and at that way it's just like a human no medians and medians of Kennedy's not statement it's very key finding understand there are two very hard feel also people tell it's also read the philosophy behind that there's I do anything to do that versus the Vino grab that Forrest argument on the classic and then the citrate was marketed from a closetful called what computers can do so basically the first to say that design is not just about the no laying elements economist but it's also about communicating what you mean this example Gmail many of the elements that we have those we can handle things but how do you choose the very good label or command that's we miss the competition the best part of this is probably the other one is this stuff and of course the AI doesn't exist in the same world it has no access to how people would interpret that or start the document but you know it can help especially visual spatial aspects of design what's very valuable for me as a scientist working this area that then actually when you start formulating something like menu interaction we are also advancing our understanding of what interaction is I think that's that the most motivating factor for me I wouldn't be intellectually satisfied if I just had a deep learning black box and then I get something like explain and do anything even if somebody made money off okay so we thought I want to thank you and interested in there's a summer school coming up there needs to be there's the book if there's anything for waiting and future you may refer to more intelligent forms of interaction like the chat box yes I'm also it's this kind of introduction yeah I haven't gone into chat box because there's a limited resources that I can obviously it's already important there right and it's doing similar things it's doing inference it's doing optimization choosing the dialogues and responses that should be doing and it's also learning from the from the responses that it's getting on user so it has many of the same elements but completely different and say algorithmic problems and modeling problems I never went to linguistics because chat box because it's really requiring his own so what I'm saying is similar progress as I shown here is shown there or even further I think that then maybe maybe the future they could be since of convergence for you not only to swept the chat book is saying right but you also choose the way it's presenting its information yeah I didn't yet talk about active learning and of course the next step after prediction optimization and an influence would be active learning because I discussed that there are theta these are the parameters that are not known that this card the task instance but then to the extent that there are also parabolas and structures that affect the interaction that are not observable in the model then you need something like active learning to support what I showed here I didn't I haven't done that but that's an exciting possibility the person is probably combine them if you have an active learning component it has its own moral and how do you combine that with I have some work with robotics people on this topic we don't have any thank you very much for your presentation as you mentioned some perspective about a psychotic from the sub psychology and as my major about social psychology so I'm pretty interested in the patient of it what do you think in your opinion what would be the desirable relationship you yeah and because you're designing this guy so what do you think what kind of bad food they have in your music very big picture Thomas but I just think you might may have some um they make it a big question so what do you think what or what we can do in order to relationship first that's gonna make a general remark is that the way I do my research sort of ensures or let's say that is unless there's the Restless of a risk that it will sort of get out of hand and become in a runaway AI look at you know we're doing menus and layout yes but we want to understand exactly what they're doing so we're not trying we're not treating is this as a data learning problem that you can properly learn data have a big black box model then it does something complicated you don't understand but starting from this white the white box principle so just from the starting point there's more much probability and transparency but it doesn't mean that so I cannot tell what what's a desirable relationship between human computer I'm trying to make one type of relationship possible where again we can where you can expect an interface to adapt and learn from you but only a particular subset of those things that are and say quite acceptable or beliefs only properties behavioral tendencies but nothing about no deeper things about say religion there's a lot of work already from the 90s on how people perceive computers there's a classic book by Clinton as the media equation and that made the assumption that people attribute to computers similar qualities as they have in humans I think this has started to break a little bit I think if you look at discussion about AI people especially when growing as a are so so the computer of that date didn't have the same AI capabilities we have this day now we had we have see we see more of the demarcation and critical discussion about about that question I also studied a little bit of social commission that's one aspect of let's say human-computer relationship that would be interesting to try to let's say optimize or learn about and so let's say things like how people infer its other via Facebook did you do interfaces that are less let's say let's less susceptible to flaming or other aspects of negative behavior in a minute better relationship yes the way it works at the moment as far as I see de is that some company or research crew comes up with an interface and argues verbally that this is the way to work that's good then you probably in the design space to lots of ideas but then she kind of theoretically explained that then you can actually control that either thank you for your book for your presentation you mentioned about eye tracking study before those lights and this one to know that though in the car situation what kind of we can do with the eye tracking data or optimize think about MOOCs right now what they're learning behavior in books one time what can we take from the Iraqi learning anything yes it's very hard to go from the way ice move to education and learning because this is a very long gap it being no engine wise and how we should be learning about something yet what you could be able to do is to infer for example from the way they look at them it's a mathematical object so that makes objects thirty percent on the screen is that what they might believe about it this is this is an area that I'm really covered too much but it's called inverse reinforcement learning able to learn the utilities basically their world functions of learners what important and then what you can do you have the model you can then speculate what if I change the environment like this then this is what would happen I think we're pretty far from that I think we're best at the moment dealing with like sensory motor it's a lower level cognitive aspects that may be in the future what it means is the theory as a gue the theory should explain that I wanted to explain how I know I'm Elise are different in different learning conditions if we had such a model then you can learn the parameters of an individual learner and then use it in the optimization step demise but as far as there is no I mean as far as I look at education about quality typically they don't talk about by movements so there's a gap and you have to somehow fill that gap in learning there's for example theory of akhtar of journalism you know a CTR that has been very powerful incomplete tutorial tutorial and that's one example of a model that can do by Melissa and that's exactly they look the way they actually do that so they look at mathematical problem solving try to learn the Communists that describe you as a learner and then they can choose for example in a little bit you know in a fashion of active learning that what what should be the next learning so that's maybe the best example I have sort of questions when it's talking offline so I got interested in kind of like this kind of approach what like one point is kind of like we have a different kind of user so we have a novice technical expertise so I really feel might be I have someone who can kind of this kind of like whenever you were extracting data from the eyeball so we found really that if people have theorized what they are mean so then it's easier to infer and the model can get more closure into more feet but if you don't have any ideas what actually they're doing so it's it's kind of hard so this evening people are two mommies then this kind of approach kind of like having less efficiency but like we we found it because when we were doing the interactions like that user experiment kind of thing but might be like I was thinking like how it can be improved later like too understand because the novice users usually we actually did with difference so it's a kind of like they actually love slowly what is going on on the interface when they are interacting and time is like changing or different parts person like some some some people can make it more fun like out of it and then make it louder 15 minutes 30 minutes somehow some people with two sittings of one hour even understand what is going on even be explained before so it's kind of like it seems like still for those people this is kind of approach its it having a little bit lower result so I was like that technique is like how to how to open this - because it's just one a really preview Center whenever you do inference you have to have some model and with more overcomes assumptions the kinds of assumptions that we do here for example in terms of learning are often based on the optimality assumption for example you learned optimal policy but you don't learn the way the user does so what what the user are choosing the end might be a verdict drops but the way people were quite different often people who learn actually once faster than but because they have such different without influence I might be that incorporating more systems that people use if you take a textbook on neuroscience in open the chapter chapter on memory you find that people have eight different memory systems now reinforcement learning and the Pablo Mian system that's like one of them we have other ones semantic and there's pretty good purpose in actually that's morally also those in a there's more newspapers every year seemingly who knows maybe in the future we can actually get a good concert how people to fund their very first exposure what yeah when we were doing at Cynthia New Jersey about some sad news this morning and we also replaced on one particularly such so yeah it might be different and thank you very much you can have a weight 
LBTsLnH3NlA,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-11-09T17:31:48Z,"L27: The Human Speechome Project. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/LBTsLnH3NlA/hqdefault.jpg,Josh Bongard,PT50M43S,false,100,2,0,0,0,"okay let's get started before we talk about HC I just wanted to take a minute or two to talk about last night's events we spent a lot of time in this class talking about how people differ physically and culturally obviously people differ politically people in this room maybe feel very differently about the outcomes of last night some of you may feel somewhat upset about the events of last night if you are feeling particularly upset i hope you will avail yourself of counseling services I've also put on an extra office hour after class today if you feel the need to talk about what happened I will be in my office and I'm happy to chat about about anything for this country this is sort of unprecedented territory so who knows what's going to happen moving moving forward I would just ask you to do your best to listen to others despite their differences of opinions and despite your own feelings about that and on the flip side i hope you will also have the courage to speak up if you feel the need to to do so okay anyone else have anything they'd like to say about last night's events no okay let's move back to to HCI then okay let's talk a little bit about where we're going in the rest of the semester today you will be handing in the tenth and final deliverable any questions about deliverable 10 no we're all good okay we're going to switch over now from deliverables to weekly reports so between now and the end of the semester which for us is going to be friday to the december 16th you are going to be writing and submitting three reports so instead of submitting images or video you're going to be submitting a weekly report and the bulk of these reports are going to have to do with user testing so by the end of today hopefully you have a functioning a fully functioning system in which if you plunk down a leap motion device in front of someone and they wave their hand over RIT there will be enough material in the interaction that will guide them all the way through to learning the 10 digits in the ASL language I want you to prove to us how well or how how easily or how difficult it was for the user to do so using your system just as a sort of a guiding principle here we are not looking for you to try and prove to us how usable your system is that's not really the goal the goal is for you to learn from your users and demonstrate to us that you could incorporate the feedback from user testing to improve your system and then demonstrate to us through a second round of user testing that your improvements have made your system more usable more efficient more engaging more acceptable to to the user right so the goal is not to prove to us how good your system is but how well you are able to learn from your users and how that feedback was able to guide your system to become more usable that's what we're looking for over the next three weeks that we have for the semester okay we've got three reports and I'm going to talk about weekly report one in a moment in weekly report one you're going to be writing a document that describes your plan for user testing so you're not going to do any user testing between now and next Wednesday and then report to is going to be a sign the Wednesday just before we break for thanksgiving and hopefully when you're home for thanksgiving you will before after the turkey be able to recruit family and friends to do some user testing and then in report to your going to document the results of that user testing in report number three you're going to talk about how you use the results from user testing to improve your system so report number three is going to be a demonstration of improvements in your system based on user testing and then finally during the final exam period which we have on Friday December 16th your going to be presenting your final version or version 2.0 and you're going to try and demonstrate to us results from a second round of user testing and hopefully the improvements you made are going to show in the measurements from that second round of user testing that your system became more usable or more efficient or more engaging or more acceptable or whatever dimensions you are trying to improve in terms of the HCI aspects of your system any questions about that where we're going okay so let's dive in to report one then as i mentioned in report one you're going to be writing and submitting a user testing plan please make sure this is just simply a PDF document you'll just be submitting a PDF document to to blackboard this document is going to be made up of three sections in the first section you're going to be describing you're going to be describing a use case and if you go back to lecture 7 and read up on use cases you're going to be describing a hypothetical interaction between your user and your system so in writing this use case I want you to imagine turning on your system plunking elite motion device down in front of someone who's never seen a leap motion device before you don't say a word they look at the screen and based on what they see on the screen they do this right from that all the way through to learning ASL or failing to learn the digits of ASL how is that interaction going to play out and that's what you're going to be describing in this first of three sections of the first report so here just started to get you started on this first section I want you to write this as an itemized list so the user sees the device on the table in front of them on the screen they see this after typing in their name they see an image instructing them to do X whatever that is because of that visualization the user does this if they don't do that after 5 seconds a different visualization comes up and so on when they eventually do put their hand over the device the visualization changes and they will see this dot when they see that it causes them to do that and so on and so forth right so you're describing the system does this the user reacts in this way the system reacts to them in this way the user reacts in this way back and forth and back and forth I want you itemize the list so you can sort of put in square brackets here instructions to us the reader so I'm reading you this use case I'm imagining this interaction playing out between your user and your system your users waving their hand over the device they're looking at the screen and they're not really paying attention in their hand wanders outside the field of view of the leap motion device your instruction to me as the reader is to go step two step see so I know that now they see an image instructing them to bring their hand back into the devices field of view and so on and so forth right so this is almost like pseudo code but it includes the user I what's going on between the user and the system alright remember this sort of underlying cartoon for HCI which is the output of the user becomes the input for the system and the output of the system becomes the input to the user you're describing how this feedback loop is going to play out and again we don't need you to paint us a rosy picture of how this is going to go perfectly we would actually like to see that you've thought a little bit of what happens if something goes wrong what happens if the device the hand leaves the devices field of view what happens if there's some occlusion is the user aware that the leap motion device is incorrectly registering the sign because of occlusion or not what does your system do in these cases what does your system project to the users to help them correct and carry on that's that's the starting point of your plan for user testing ok do we want you to write this as a numerated list because the following week when you actually do some user testing you are going to be sitting next to your user you are going to be watching them interact with the system and again you are going to try and say hopefully not say anything and you are going to be ticking off does do these things actually happen so in a natural user interaction did a B and C play out and then something else happened so you thought d was going to happen but instead something else happened when did the interaction go off script right that's the first sort of quantitative metric you're going to get back from user testing as something unexpected that hopefully will get you thinking differently about how you're going to have to improve your system for version two point oh right what improvements are you going to make that when you do subsequent testing with either the same user or another naive user that the interaction plays out much more in the way that you expected you remember Jeff Springer was here the CEO of Zemo corporation and he was using you all as guinea pigs a lot of those interactions that went on in the classroom he was telling me we're not what they were expecting which is good right that's good for them they were able to take that and go back and improve the tutorial of Zemo right so again we're not trying to make sure everything goes right you're actually looking to see where things go right and where things go wrong and based on that divergence what do you need to change to make sure it doesn't go wrong the next time around okay okay in the second section of the document I want you to talk a lot about measurement and here I want you to be as specific as possible so as I mentioned when you do user testing the following week the user is going to be sitting in front of your screen with their hand over the device you're going to be sitting looking obliquely at them so you can see the screen and you can see their face you are going to be marking down on a piece of paper this but also marking down some other data to measure this interaction right some of the metrics from your user testing your system is going to capture anyways right how long does it take for the user to sign the digit correctly how often do they fail and so on but there's a lot of other aspects of that interaction that I want you to try and describe five for me in the second section and mark down so again just to get you thinking about this I gave you a few things that you might want to put in section 2 which is how often does the user look at you right so they don't know what they're supposed to do next they look at you and even just by looking at you that's them non-verbally asking for some guidance and again hopefully you should be able to do nothing more than look back at the screen meaning the screen will tell you what to do right do they look at you and ask for verbal clarification I don't know what to do next what what am I supposed to do right how often do they how often does that happen just mark it down and if they're really confused then tell them what they need to do next and carry on make sure you make a note of exactly what state your program was in when they asked you for verbal clarification right be as specific as you can because this data is going to help you pinpoint where you need to improve things the next time around how often for example does the user canceled the interaction are they shaking their hand how often does their hand go out of the field of view and back in what other sorts of things is the user trying to to do how often do they swear at your system how often do they whatever right okay you tell us what those metrics are and it should be clear from those metrics how you're going to record it how many times do they look at you how many times they look at you and ask for verbal clarification how many times do they look at their own hand how many times they shake their hand all of that kind of data how often does it happen and we're in your code which state was the program in when that that happened okay finally in the third and final section of this document I want you to describe how you envision improving your system depending on how these metrics play out so this one's a little tricky because you're not doing user testing to the following until the following week but I want you to show us some evidence here that you're thinking ahead to user testing so in the third section here you might say if the user keeps asked for verbal clarification in state number two which for my program is when the user's hand is in the field of view but has strayed away from the center every 75 percent of the time when the system was in state number to the user was confused or didn't know what to do if that situation arises then I think I will probably go back in and change state number two in the following way right so you're already starting to anticipate what happens depending on what kinds of metrics you you get back right we talked about this in the HCI design process so in lecture seven eight and nine you can go back and have a look at that what it what if you see standard deviation in the metrics right so what if you do testing with two or three family members over the Thanksgiving break and for one person that metric scored high for another medium and another low right what does that mean if you get standard deviation on that metric give us some ideas about how you think you might need to go back and change your code right so in this third section here we're looking for a particular metric a particular statistical summary of that metric so you're looking for low mean or high mean or low standard deviation or high standard deviation and depending on that situation you have some plans for what might happen clearly you're not going to be able to anticipate every situation I just want to see that you're thinking about repercussions right so if in actual user testing over Thanksgiving the particular metrics that you sketched out in this third section don't come about that's fine you see oh actually it was this metric that scored low on my users that you're getting the habit of thinking about again going from those metrics back into the code and thinking about where you need to make improvements okay I think about half a page for each of these three sections should be more than sufficient so about a page and a half for this PDF that you'll be submitting it's a little bit longer that's that's fine too any questions about report number one okay so just back to the schedule for a moment so again you're going to be writing this report but to now and next Wednesday and then armed with this report will talk about report to which is how to actually conduct user testing over the Thanksgiving recess you're going to do some user testing and collect some of these metrics and that's all going to go into report number two report number three is you're going to show us how you improved your system based on those metrics all good okay so let's carry on with the lecture material then we're working our way through this section on looking outward deploying embedded devices out into the physical world and as we increasingly instrument our world with technology that is continuously sensing and in some cases acting on the physical world one of the sorts of things that were difficult or impossible to do without that instrumentation we looked at social network inference activity tagging and we finished last time by talking about this rather controversial study which has become known as the human speech ohm project so this is a meme now right if you add om e on the end of anything it means you're trying to capture the entirety of that thing right the genome the connectome of the brain every neuron and every synapse the speech home in this case was an attempt to capture the entire set of experiences surrounding a young child as it starts to acquire speech okay so we looked at how this sort of this approach to studying this question or trying to answer the scientific question of how children acquire language is different from the standard way which is I have a hypothesis about how I think children acquire language I do a study in the lab and the results of that study provides support for against my hypothesis in Deb Roy's approach the idea was let's not make any hypotheses let's just collect an obscene amount of data and then dive through that data and look for patterns which then suggests hypotheses about how children acquire language so this is a data-driven approach right instead of starting with hypotheses and collecting data we're going to collect data and then start to form hypothesis he's based on the data that we have we watch this video the TED talk last time of deb roy describing one or a few experiments that they did given this data set including this experiment which was the birth of a word and he played that audio track of his child acquiring the word water and we'll come back to that in a few slides ok let's talk about the instrumentation first as you mentioned in their home they instrumented 11 rooms with amna directional cameras and in there they also had 14 microphones it was a fisheye lens so as an attempt to capture basically all the video or everything the child might be seeing when it's in that room and hopefully pretty much everything that the child might be hearing they collected that data over an 18-month period from nine months of age to 24 months of age which is the language sensitive part of a child's life and you're going to see this in some data in a moment this is sort of the sponge phase when children start to acquire their mother tongue at an amazing rate for over 4,000 hours of recording time which gives about 200 terabytes of data so for a standard laptop that's what is that that's about eight under to two thousand laptops current laptops right so a pretty big chunk of data this was done back in two thousand nine when this was really a big chunk of of data about nine point six hours per day they believe they captured about seventy to eighty percent of the child's waking auditory and visual experience not the child's somatosensory experience not what the child felt when the child grabbed an object right so only sight sounds obviously other sensory modalities also probably impinge upon a human's ability to acquire language and five or six cameras were active in any given time ok here's a here's just a snippet from the from the research paper which makes this point about trying not to do theory Laden science right so the minute I come up with a hypothesis or theory I've already sort of biased my study right I'm going to decide what data to collect based on my theory right I've already sort of biased in a certain way so professor Roy is arguing that traditionally things were done in this theory Laden manner but in this case they want to try and do things unburdened by theory right we're just going to go in and collect the data once we have the data then we will look at the data and form theories based on what we see in the data ok ok back to the instrumentation for a moment so here's sort of the pipeline they have this raw sense they have this raw audio and video they're going to try and compress all of this raw data down into language people activities and objects I just kind of sounding familiar perhaps right so who what where when and how and then once they have those objects and how they're changing over space and time then they can start to formulate and hopefully prove different theories about language acquisition so in terms of just the raw audio so here's the audio here so like we've seen several times already they have sort of two layers of machine learning going on here the lower level is taking this raw audio and assisted by actual humans they were trying to pull out of that raw audio who was saying what and how so word level speech transit transcription what's what's this particular word being said at this point in the audio track and by whom and prosody features how were they saying it right so we've heard that we've seen that already what was the pitch of the voice was the word being said in mommy or daddy talk or was it being said in I don't talk what was the way in which the words were are spoken okay once we get to that now we have who what where and when now we can start to add on a second level which is a machine learning algorithm which is going to say it seems that whenever a child uttered a given word at a certain time this particular situation happened just prior to it right so the second level here is trying to take as input who what where and when and make a prediction about when exactly a child is going to utter a word for the first time right so something that this particular machine learning algorithm might learn is that if there is a word that's being heard from the parents or the three caregivers so there's mom dad and a nanny in this house during this period so if the three caregivers are saying this word a lot then this word is going to be uttered for the first time earlier then another word which is hurt spoken relatively rarely by the three caregivers right that's a particular relationship between between who what where and when and the thing that we want the machine learning algorithm to do which is to predict how early or how late a child says a word for the first time why is it that this particular child's first word was this rather than this other word so far so good okay okay this is a pretty daunting thing to do for the audio as well as the video so they created some interesting HCI visualization so they made some actual tools for their human analysts because they had to process what do they have to process here 4260 hours of audio visual right things have gotten better in the last seven years so machines are getting better at pulling words out of raw audio and faces out of raw video but still was pretty wonky back in 2009 so humans were in the loop in this case audio was visualized with the spectrogram which you can see here so a spectrogram is showing along the vertical axis here the power of different frequencies so at each point as I march from the bottom up one of these columns the darker the pixel is the stronger that particular frequency is so at this point in time just above the up arrow there you can see that right here there is a strong low frequency sound and then as time marched on and I cannot see what the axis is here is is probably a few seconds over a few seconds the power switched from low frequency to high frequency so then it might have been a deep voice that was transitioning to a high pitch voice or something like that it's not clear from this picture whether that was actually voice but that kind of thing once you get good at this you can start to look at a spectrogram and see speech ambient noise a shower running the tap running the toilet flushing all these other sounds in a house and start to narrow in on where is their speech and then assuming that there is speech what actually was said so the raw they under the the lower level machine learning algorithm was trying to pull out let me see if I can make this little larger so you can see it was pulling out its guests about what the machine thought it heard in that short audio snippet the human analyzer would click play would listen to the raw audio and say yeah that's definitely that's correct this is the caregiver speaking this is the child speaking this sounds like it's someone on a TV or a smartphone or whatever right so a lot of tagging of speech from this raw audio track if it was wrong then they would type in what was actually heard so the machine was wrong obviously back in 2009 quite a bit so they might correct the speech okay so they made this this little widget called blitz cribe so for every one hour of audio it took 1.5 hours to transcribe they've got over 4,000 hours of raw audio right this is a pretty daunting task so the machine learning algorithm was trying to winnow out was trying to winnow out obvious places where there was no sound or ambient sound and just try and focus in on where on parts of the audio track where there happened where there was likely to be words and then the human analysts were helping to identify what exactly was said when and where in what room it was a correct okay so that's just to give you sort of an idea about the sorts of tools they were developing the sort of processing now we get to the fun part which is now that you know who what where and when can we start to develop theories about language acquisition here's the first here's the first visualization from the data so on the horizontal axis here we have the lifetime the lifetime of the child from again from nine months of age to 24 months of age the vertical axis reports the number of word births which means for any given month how many words in that month spoken by the child was spoken by the child for the first time right so maybe in month number ten here the child said water for the first time we increase this counter of number of one word birds by one maybe they utter the word a spaghetti for the first time we increment it by one again okay what happened for this one child as they learn first less like things that they can okay so there's our first theory right so there's only so many sort of basic words that may be a child is we're learning during this period and at some point there aren't that many more to learn so the rate of word births goes down that's one hypothesis and again we could go dive back into the data and try and look for additional evidence to support or try and disprove that theory clearly spoken so if you go back and watch the first few minutes of the TED talk you could hear the child saying gaga huahuahua there were things that were clearly antecedents of the word so those were things that kind of sounded like water according to the caregivers and it was usually spoken when the child was near water so again theory is starting to creep in here a little bit and then right at the end of that audio track that he played the child said water and that was the first time as far as anyone could tell in the raw audio track that the child said the full adult form of the word water that was it right there was obviously a lot of babbling and these sort of proto words before it they're looking for the birth of the adult form of the word so the child clearly continued on improving in terms of language after month 20 here and the rate of word birds is going down so what's going on here can we be a little bit more specific perhaps right so maybe it was sort of one in two syllable words that the child was tackling and they sort of they may be absorbed the vast amount of those simple words by by month 20 there are many fewer words that have three or more syllables right and maybe that's what's going into these bins here it's not clear right we could go back and look at the particular words in each of these bins and maybe redraw it based on the number of syllables in the word which would be again evidence for against these hypotheses about why this happened for those of you that have learned a second language what happens right in the first few weeks or a few months you're just acquiring vocabulary right this is this this is this this is this this is this at a certain point there's kind of an inflection point where what starts to happen instead you're still learning new vocabulary but you're doing much more of something else well you get away with absolutely right so there's a few important words that you need yep grammar right okay so I know if I want water I know it's a point and say well other if I want the door closed I know to point at the door and just say door but if there's other things that I may want one of you to do I'm going to have to combine words together in combinations and permutations right what are those combinations and permutations so I think that's a pretty good hypothesis for what's going on here this is a transition from an emphasis on vocabulary to grammar but again who knows we have to go in and look at the data to prove or disprove that hypothesis okay this one's kind of interesting so again we've got a plot here with age along the horizontal axis and each dot in this scatterplot corresponds to again a word that was uttered by the child for the first time the adult form of that word so for example I think water was one of the first words that this child uttered so this dot down here might be water and the height of this dot is the log frequency of the word in caregiver speech which is basically the frequency at which the other three members of the household were uttering that word you can see that there is a red line that's fit to this and a negative point two nine this is the R value or the correlation really we saw correlations and anti correlations last week so there's an anti correlation here which means what absolutely the more that it's spoken by the caregivers and it doesn't say here whether the child heard the word or not just that that word showed up in the raw audio track being emitted by one of the three caregivers the earlier the child tended to say it for the first time which seems kind of intuitive right if instead we had plotted this data and got a positive correlation that might have seemed strange right a positive correlation would have meant that the more the word was uttered the longer and took the child to utter it for the first time so this kind of these matches my intuition about what's going on here okay this is just so again we're trying to figure out what predicts word birth how it which are trying to predict the time of a word birth what in the child's experience cause that child to learn that word earlier or later and in this case the hypothesis here is that it has something to do with how often the caregivers said it but this probably isn't the only thing that influenced how early the child acquired any given word right there are probably other things that influenced it such as so how often the child heard mommy or daddy or the nanny saying this word what else might make a difference here how much time they spend absolutely right so if the caregivers uttered the word water when the child was near water did that help the child acquire water more than if the caregivers had uttered the word water distant from water was it something about the fact that the child uttered the word because the war the child was starting to understand that this sound water means these things who knows right that's an interesting hypothesis yeah however they do if they want how often they get what they want right so a big part of language acquisition is social contingency right so social contingency meaning there's a social interaction between me and mom and dad and I learned that by doing certain things I get what I want and maybe I start out by just screaming as loud as I can and that works pretty well for a while and then I learned that if I start actually asking for things or uttering these other sounds I get what I want faster right that's another one that might be important right so it's probably not just how often I hear a word but all these other dynamics are going on how do we attack this problem this next figure takes a little bit of time to digest so we're going to go through this slowly in this plot here they were saying okay let's just look at how often the word was uttered by caregivers so that we're using one feature to predict the time of word birth now they're going to add a second feature p down here which is how the word has said so p for prosody so f is frequency right just the number of times that it was that it was said by the caregiver p is this value that it's going to be set between zero and one where i can't remember how it actually goes but let's say that p if a word is uttered and it was uttered in the most adult form of that word mommy that that word would get a p of 1 if it was uttered in baby talk which is typically that the vowels are elongated so it's the same word mommy but now pronounced as mommy then it would get a p value of zero so p is sort of the amount of exaggeration of the vowels in the word so we're going to now ask the question of how do these two features in combination predict early word birth so is it just that the child heard the word a lot regardless of whether they heard the adult form of it or the baby talk version of it or is it some combination of these two features so we're going to take F and P and try and predict a which was the age of the child in months when the child spoke that word for the first time so far so good ok we're going to introduce another variable alpha and you can think of alpha is a slider bar we're going to slide alpha between 0 and 1 and it's not actually written here on the slide alpha ok so what they actually did was a following so when alpha is 0 here then alpha equals 0 is equivalent to this figure here so it's just taking F right so if alpha 0 we get 1 f + 0 P which is just F we're going to see how f predicts a so here's F and here's a and we know for that particular setting of alpha alpha equals 0 we know how well F predicts a it a relatively weak negative correlation it's somewhat predictive but it's kind of weak is there some other setting of alpha that combines F&P to predict a better now there's a little there's something about this figure that seems not quite right because if we look here at alpha equals zero in this case they're plotting now on the vertical axis the correlation the our score which they said for alpha equals 0 is minus point two nine but it looks like it's one point two seven five so something seems not quite right here but I hope you get the basic idea so they said for F how well does F predict a it predicts it about this good then they move the Alpha slider bar a little bit to the right which means now the next dot along is ninety-nine percent F and like one percent p and as we keep sliding alpha from the left to the right we can all the way up here where when alpha equals one we have a we're just using P just the way in which the word was spoken and we wanted to see whether just pee oh well just pee predicts a how well did just pee predict a that's this point up here the P predicts a well or not this plot takes a little bit to wrap your brain around so if we if this is an interactive plot imagine you click on each one of these dots every time you click on one of these dots it shows you a scatter plot over here if you plot if you if you clicked on this down here we give you this specific scatter plot which has a handy correlation of point 29 negative correlation of point 29 if we were to click on this dot up here it would give us a different scatter plot over here which would be am the horizontal axis and p vertical axis now instead of F and we would get a different slope for the red line slope of that red line would be minus where that is Oh point one six how well does P predict any not very well right worse than F right so it's point 16 it's kind of weak maybe the child being influenced by baby talk here maybe not they're much more likely to have been influenced by F what about a combination of these two things so by looking on this plot you can tell which combination of frequency and prosody best predicted word earth what is it sorry closed right so here's half and half so slight of our elf is at point five we go up here it's actually not that good so it's not half and half it's more about point2 right so here this is the lowest point here it's the strongest anti correlation at this point so Alec up a point to into what in terms of EMP exactly right so in terms of trying to predict if you want to try and predict for any given word how early that child uttered that word for the first time our best bet for doing so if all we're given is NP is to say the child was mostly influenced by F for mo about four-fifths f + 150 right so key mattered but not as much as f so we've now gone from using one aspect of the child's experience to try and predict how they're learning language to another visualization that tries to show us two features of that child's experience and what combination of those two features predicted word birth let's imagine that this series of dots here was a completely flat horizontal line what would that mean not quite well be equal weight to be cut at that point five right but if the line was flat that means regardless of the Alpha it doesn't matter what combination of these two we set it it's no different in being able to explain more or less of the child's word birth imagine that it was a flat line way up here at zero what does that mean there's no there's no correlation at all so the color of clothes that the kid was wearing that day right that's that's F and P is the air ambient air temperature in the house that day those are two other features which may I hope you could get from this data those are two features which I'm assuming probably don't influence word birth very much they would probably produce a horizontal line near zero the fact that we have something that is far from zero means we went on this fishing expedition into the data we caught something right we haven't pulled out the full story the full story is pretty complicated but we released got the tip of the iceberg here does that mean that you should take away one of those metrics that have a new one good question so what do we do after we've caught something or after we've got nothing right we're not sure what's what's going on if it's a horizontal line that's far from zero that means we've got something right so maybe one of the features matters in combination something else that we're not measuring if the line is flat at zero we should probably throw both features back in and go fishing for something else so it's interesting to think about this progression right where do we go from here how can we in a principled manner start to collect more and more features and determine in what combinations they predict word birth you could if you had a pretty powerful computer right this becomes a pretty computationally intensive task very quickly right lots of combinations and permutations here remember each one of these dots here corresponds to hundreds of word births that are being pulled out of raw audio video data this is a pretty computationally expensive thing to do okay all right this is pretty good timing so we'll end with this last slide for lecture 19 which deb roy actually talked about in his TED talk this is the one that I think is really exciting this one is really interesting they seem to have captured something here that was previously unknown about language acquisition as we talked about last time this is an this is evidence of scaffolding and it's a very interesting form of scaffolding they took each individual word birth so every time a child that had a word for the first time they took that point in time and they then went backwards in time in the data and forward in time in the data and as they went back in time and forwards I'm sorry if you lined up all of the words at the same point so they're normalized words were born at different months they moved all these words they were asked 10 he went into the past for each given word and they looked for other utterances that contain that word so let's imagine word was water they start going backwards in time and they hear one of the caregivers say want water when they hear want water that's an utterance that has two words in it one of which is one of which is the word we're interested in and as they go further back into the past they heard they heard longer utterances do you want water at this point when they captured when they captured the word they had the length of the utterance so maybe the first time the child wanted water the child didn't just say water but maybe the child said want water so the first time the word was born it was born in an utterance that contained two words so that other hands had a word length of two as they go back in time they see that the utterances that contain that word become increasingly long as they go into the past and also increasingly long as they go forward in time so as they go forward in time the caregivers start to say things like do you want water do you want more of this water do you want the water from the red or the blue Cup longer and longer Auden's utterances that contain the word water so how is this evidence of scaffolding where's that--what's the scaffolding here make you mention this in the video but is that like like they would like add more scaffolding based upon how well the child is doing and then they gradually exactly right so the scaffolding here you can think of scaffolding as shortening of utterances right making things simpler for the child not unlike putting wheels on a training bicycle right so they started with little to no scaffolding to all water from the red cup and just before the word water was born they were saying things like water water wat water water more water things with that were really we short utterances they put a lot of scaffolding on then the word was born and then they gradually removed the scaffolding how did the parents know when the child was going to utter the word so the parents seem to be adding scaffolding before the child months before the child has ever said the word right this is kind of the exciting part of this discovery what's going on here it's like a magic trick how can this how can this be the case the parent doesn't know that for months from now the child is going to other the word water for the first time absolutely right now what is going on in that feedback loop that allows the parent to sort of zoom in start to say okay I need to start adding adding scaffolding they're getting ready to say the word water for the first time one aspect of that feedback loop is that the parents probably learning about the child's word antecedents so if you go back and listen the video for this child gaga kind of meant water for a long time right gaga started to turn into wah wah so they were getting closer it seems that the cha even if you listen to that audio track can sort of feel the child getting closer to the adult form of that word the feedback loop between the parent and child there's probably rich signals in there saying the child is not going to say water for the next three months but three months from now they're going to utter that word scaffold them right so it's not just the case that the parents or the caregivers are teaching the children language the children are teaching the parents the children are providing signals about what they need what kind of scalpel think they need in order to birth a new work right this is this is pretty amazing the feedback loop between parent and child is that by bi-directional okay I think we'll leave it there for today your quiz due tonight you have deliverable ten do tonight and you go have a look at the port one and we'll see you on Friday you have a question are we stuck with 30 or if you go over 30 we are going to go over 30 we will continue quizzes up to and including the last class and then I will normalize it back down to 30 ok yep ok "
3VZ7D01T2Yc,27,"Björn Hartmann of UC Berkeley presents the Beauty and Joy of Computing, lecture 14: HCI (Human-Computer Interaction).  Slides available at http://bjc.eecs.berkeley.edu/slides/BJC-L14-BH-HCI.pdf",2012-09-02T03:58:26Z,BJC Lecture 14: HCI (Björn Hartmann) [1080p HD],https://i.ytimg.com/vi/3VZ7D01T2Yc/hqdefault.jpg,Dan Garcia,PT46M58S,false,2388,18,1,0,0,"welcome folks to lecture 13 cs10 where i gives me great pleasure to introduce our guest speaker today are my colleague John Hartman Bjorn came to us as the hottest he doesn't notice okay see I read in the country we said we need to really infuse birthdays HDI program with the best guy in the country we did an entire nation international search your him was easily the guy that just blew us away we were so happy when he said yes to Berkeley and he's been with us for a couple years now and he spent some amazing work here he's gonna share some of his history how we got to Berkeley which is really inspiring for those of you who are not necessary computer folks because he didn't start as a computer foot person which is great and some of the exciting work he's going to find the entire field of human-computer interaction which is what the most exciting fields in the entire field of CS that for the new Bjorn Hartmann thank you so much great then thanks for that intro alright now that Dan set the expectations really low for where this lecture let me start off telling you just a little bit about my background so I went to the University of Pennsylvania in Philadelphia and I started off as a social science major I was in communication and then actually I wanted to fine arts and only then a new major came around called digital media design which had all of that plus computer science and so I slowly began to move more and more into the engineering direction because I was really fascinated with what you could do with computation so I ended up getting a master's degree in computer science from UPenn but then I took a little time out there was a lot of school in five and a half years for both the bachelor and the masters so in my second life I had a record label so this is my label partner Jay Hayes DJing in club milk in Tokyo back in 2003 so we were traveling the globe deejaying and this is really one of the defining situations that got me into human computer action so what you see here is on the right hand side the user interface that DJ's have been using since the 70s to turn tables with vinyl records and a mixer in the middle on the right hand on the left hand side there's a laptop what is that laptop doing in the picture well that laptop actually holds all of the music and the records that are on those turntables don't hold any music they just have a time code and that time code gets piped into the laptop which uses the time code to control playback we have mp3 files that then go back to the mixer and out to the speakers this was a a really kind of a revolution in technology for disc jockeys because now we didn't have to lug around a hundred pounds in two cases of vinyl just to play in a club for a night no now we could just bring a laptop and have music worth days so I did that till 2004 and then I made the decision to go to the second best school in the Bay Area and did a a PhD in computer science at Stanford University where was in the human-computer interaction group I finished up there in 2009 and since January 2010 I'm now here at UC Berkeley all right and here at Berkeley I teach both the undergraduate and the graduate courses in human-computer interaction so what is human-computer interaction or HCI well we can start by just dissecting the term right so there are people involved those can be the end users of a program of a piece of software but there can be more than one right I may be interacting through the program with my other friends online so the humans can both be the end users but there can also be communities of users that we want to support then clearly there's a computer that's the machine that our software runs on and nowadays that's often also not a single computer but rather a collection of computers so many servers in the cloud Plus maybe a mobile client that I carry in in my pocket and then we have the interaction part which you can think of as this dialogue where the user can a tells the computer what they want done or who they want to communicate with and the computer processes that and communicates a result back and it does that through user interfaces so user interfaces are the part of the application that allows this interaction this dialogue to happen and many of you are familiar with the standard user interfaces so we have things like menus widgets toolbars pop-up dialogs but the UI can also include hardware so think of the Wii mode a core component of the Nintendo Wii is the fact that there's this Hardware controller which has an accelerometer in it and in the latest version also a gyro that can sense tilt that is part of the user interface on the right-hand side here I'm showing you the reactor ville this is an experimental musical interface this is basically a synthesizer where you control the quality of the music by moving these physical pucks around a table and the table also has a display projected from underneath so HCI in the larger sense is the design the prototyping the implementation and the evaluation of user interfaces whether they're hardware or software an HCI is really an interdisciplinary field which is what attracted me to it and there are three primary strands in HCI we take the technology part from computer science and electrical engineering but we take the process by which we go about building good user interfaces mostly from design so product design industrial design these disciplines have come up with a good process how to define products and services that fit the needs of people and finally we also borrow from Applied Psychology mainly in the methodologies we use to evaluate whether what we designed actually it's goals now the real reason why I moved into HCI was not that I'm really passionate about those three subjects that's also true but more importantly working in HCI allows me to be one of these four personas on any given day so if you design build implement and evaluate user interfaces sometimes you get to be an artist you get to care about the aesthetic appeal of the user interfaces you create you also get to be let's see which way do we want to go well I'll go to the engineer next diametrically opposed I get to be an engineer because I like building systems that work complex systems that really help people accomplish their task that is done through engineering principles now I get to be a designer when I try to really think about the fit between engineering and people's needs so designers contribute methodologies like need finding where go out observe and interview people about what kind of technology could really meet their needs for example for work or for play and finally I get to be a scientist where I get to both design carry out and then analyze experiments and experimental data to really figure out whether the inventions I came up with meet their stated goals all right and the way the design of user interfaces happens is through an iterative design cycle if you remember one thing about this hour it's this cycle this is how user interfaces get built in practice and in research and that is we start with a design phase this is where you come up with ideas so you ask the question what should we create right and you may observe people to get some intuition you come up with a rough design and then you embody that design in a prototype so a prototype is something concrete that you can test with your roommates with target users that prototype then gets evaluated and we have different possible ways to evaluate prototypes and then we use what we learned from the evaluation to reflect on the original design hey did we succeed in our goals if not how should we change the design and do we need a wholesale redesign of the fundamentals or did we get the big things right but the small things are still wrong so evaluation tells us that so we go into the next phase of design build a next prototype that may be more complete more polished than the earlier one evaluate that again and we go through that cycle until we run out of time or budget and the reason that cycle is necessary is that in anything that has any product that touches human users it's really hard to get it right the first time and the reason for that is is we don't have a complete model of how we as people operate right we know some information from cognitive science so we know how many how many different things we can keep in memory before we have to start writing things down and we forget them we know what kind of color contrasts work better than others but this is only piecewise knowledge to really evaluate whether the design as a whole works we can't just look at it and reasoning reason about it in the abstract we have to embody it in a concrete piece of software put it in front of people and test it with them all right so let me just walk you through each of these three phases in a little bit more detail one really important phase is understanding users so what you do is you go out in the world you define as who are the people I want to help with my product and you observe them and you interview them you ask them questions for example a couple of years ago a letter a course at Stanford and the task was to find new technology opportunities that improve the life of people at the farmers market in San Francisco now I am NOT like a vendor at the farmers market right I don't know what their job is like so if I want to design a useful technology for them I certainly need to find out more about their reality and their needs so we took a field trip to the Ferry Building now those observations then lead into the building of scenarios and models we're out of all the rich observations you try to abstract what's what are the core principles the core needs what's constant across a range of different observations and those are the principles and the goals for your design the next step is prototyping and one of the chief goals and prototyping is just speed get something concrete that you can test as fast as possible so there are a bunch of low fidelity techniques such as paper prototyping where you just draw what your user interface would look like on a piece of paper and have someone simulate using that paper interface or you can make video prototypes that look like the real product but you ever actually never wrote a single line of code it was all done in Photoshop and illustrator and there are higher fidelity techniques as well such as making interactive JavaScript HTML or flash prototypes the reason we make these prototypes is that the final implementation of something that is robust takes a long time to build and debug you want to make sure you have the your design right and all the design decisions already locked down before you take that big leap into implementation in a systems language so we take lots of small cheap fast steps through prototyping to get perspectives into whether our design is going to work so some prototyping techniques are for example storyboarding where just like an animator at Pixar you draw up key scenes of what using your user interface would look like and then you walk someone through that storyboard or you could build a prototype that actually works this is from the design company I do who created this mock-up of what a digital camera back should like back when most camera still used film now this doesn't actually have a lens on the other side and you can't take it outside because there's a thick cord that that kind of connects it to a PowerPC an old Macintosh but the point is we can create something like that faster than to ramp up a production line and actually create this camera this may take someone maybe a week to put this prototype together but then you can test what should the screen show what should the user interface elements be all right moving on to evaluation there are two big categories of evaluation a formative and summative evaluation formative evaluation asks the question are we building the right thing and what should be different in the next iteration so you do that early on summative evaluation asks our project is complete doesn't work did it achieve its goal and we have a whole range of different evaluation technique here's a picture of kind of a stereotypical one that is really high fidelity you have a usability lab where you bring in participants to use your software and you sit behind a halfway mirror and observe that participant and all the actions they take are logged through software loggers and video cameras and screen capture so you get a really detailed account of what happened while people succeeded or struggled with your new software is it worth the effort so one first question is how much of an application source code is developed is devoted to user interface code on average well this is an empirical question right you can just look at existing software projects and count the lines of code that are devoted to the UI versus the backend versus other algorithms and this is precisely what researchers have done before so Meyerson raphson did a large-scale study and they found that in today's applications back in the 90s an average of 48% of the code is devoted to the user interface portion so you guys were spot-on now if it's really half of the entire code developed to the devoted to the user interface then probably we should also devote half of our effort in design and testing time to the user interface as well all right let's switch tracks a bit and I want to talk to you for a couple of minutes about the history of human-computer interaction how we got to where we are today all right in prehistoric times the 1940s this is at the University of Pennsylvania where I went went to college ENIAC one of the first computers the user interface was a roomful of patch cords and and knobs and relays all right that doesn't look anything like the computers we use today in fact here's a photo of the first Mouse ever built this was done by Doug Engelbart and Bill English at Stanford Research International in Menlo Park right here in the Bay Area Doug Engelbart is actually was a Berkley graduate student and up until recently he would still regularly visit campus so this mouse is a little bit different from what you see what you find today in a mouse most of the mice today are optical they have an optical Center basically a small camera on the underside this mouse was electromechanical there are two wheels one in the x-direction one in the y-direction and when you move the mouse in the y-direction up the table the Y wheel would track and the X wheel would scrape and you moved it in the X direction the X wheel would track and the while really y wheel would scrape here's a view of the bottom just to see those two wheels and basically these wheels would then drive shaft encoders that would just relay a signal back to a computer how many units the wheel moved in one direction versus the other well it turns out pen input was actually invented right around the same time as the mouse right ivan sutherland who also had an office in soda Hall right here at UC Berkeley did this work as his doctoral dissertation at MIT and Lincoln labs back in the early 1960s so I'm just going to show you a quick video of sketchpad in action this should look familiar to you right all right let's let's stop it right there students you've seen this and this video is also very easily findable online so what is really really inspirational here that within the scope of a single PhD dissertation ivan sutherland basically had the vision for the entire cat industry and for half the product line that adobe now sells right so there are there is drawing with a pen there is constraint satisfaction so if you're not quite precise well an algorithm takes over and makes sides parallel and makes shapes fit they're spanning across infinite windows really inspirational actually turns out one of these design choices is one that we don't see much of these days and that's the input device the input device from ivan sutherland was the light pen and the light pen was a pen with a wire that you would just hold up to a vertical CRT a cathode ray tube and one of the big deep screens it turns out the light pen has one really big problem can anyone guess what that problem is ah so there's the question of accuracy so back then the light pen actually had a little light sensor in the front and it would detect when the cathode ray tube would scan by its beam and I would say now and then would figure out it would synchronize that signal with the screen updating logic and figure out the XY coordinate so because that was really close by it was reasonably accurate yes we don't use them anymore but why yes the cost so back then these were really expensive economies of scale you could probably make one of these today for about ten bucks that is that is a great reason yes we don't use these types of screens anymore but it turns out there is a non-technical reason why people don't use the light pen it's not heavy by itself but it turns out your arm is pretty heavy and that is exactly the reason trying to hold a pen up to a vertical screen for any amount of time gives you a really nice sore arm the next day and so basically no one really wants to use a light pen for more than 10 15 minutes at a time so nowadays we have shifted to tablets and other horizontal surfaces where it's easy to rest your arm while drawing and that makes a world of a difference so it just tells you sometimes the reasons while technologies don't succeed have nothing to do with how ready the technology is they have everything to do with the human factors around it all right after the light pen in the 70s and 80s we then had the age of personal computing the killer applications were word processing and spreadsheets and everyone wanted to have a Bayes Box on their desk and nowadays this seems kind of tried to of course everyone has a desktop but back then computers were mainframes that you know if you asked really nicely you would get half an hour of time in the middle of the night and so the idea of having a computer that belongs solely for you to you for your own tasks was revolutionary at the time fast forwarding you may have heard there was a small announcement earlier on today arguably we've now moved beyond the age where personal computing is the dominant paradigm and I just want to show you two quick precursors to tablet and mobile computing back in 1985 NASA sent one of the first laptops into space this was called the grid pad it cost about ten thousand dollars back then in nineteen eighty five dollars but it was it was a quantum step about anything that had come before because it had this clamshell design which we now still have in our laptops and it was small enough that you could easily carry it this was not touch sensitive however it didn't take long from the grid pad till we were at a stage where you could go out into a store and buy a phone that had a touchscreen that did email and also ran games and other applications and the first phone that did that was the IBM Simon sold in 1992 I don't think anyone in this room has ever used one I've never seen a real one I don't know Dan if you ever had the chance what the IBM Simon illustrates is actually a really interesting general observation about what user interfaces succeed and fail in the marketplace oftentimes the first iteration has all of the features but failed miserably because the rest of the world isn't quite ready yet for it and then ten years later someone picks up the same ideas but by now the technology is affordable enough it fits into people's lives better and then it takes off now what is exciting about working as a researcher in human-computer interaction is that we get to play with the technologies at the IBM Simon stage and earlier so in our labs we get to play with the technologies that you know ten years from now you can buy at the Apple store all right if you're interested in history this is going to be a slide that's gonna be online I have a whole timeline of early user interface kind of landmark events both in software and hardware that that you can look at so what has changed from this age of personal computing what what are the research directions that we look at today well here's a quote from Gordon Moore does anyone know who Gordon Moore is I heard mumbling all right who's reasonably certain about who Gordon Moore is yes co-founder of Intel all right so for every ant in the world today there are a hundred transistors that was in 2003 there are way more than 100 transistors for every ant of the world today and you could graph this so these aren't accurate numbers this is just my rough back-of-the-envelope calculation back in the 1960s when the mouse and the light pen were invented we had many people per processor and then in the 80s and 90s when personal computing came along we had about one processor per person sitting in that beige box on your desk and now we've really taken off into this age of what's what has been called ubiquitous computing where we have hundreds thousands millions of processors per person at our disposal and you can see these different areas find expression in different types of HCI research so in the early days mainframe computing it was all about human factors how do we make an efficient interface so one person can get their tasks done quickly and then let the next person use the mainframe personal computing the edge of personal computing was very much concerned with psychology and cognitive science understanding how the dialog between one person and one computer happens now that we're in the age of ubiquitous computing there's much more of a focus on what happens when computers don't look like desktop computers and what happens when we use computers to collaborate with each other so here are a couple of research directions in the age of personal computing this was your desktop computers mental model of you what do I mean by that well it's how applications thought of our human capabilities we had one finger for clicking and for hunt and peck typing two years because a stereo sound and one eye for one screen now a whole area of research and human-computer interaction asks the question how can we have computation leverage more of our bodily capabilities so here are some example projects this is a PhD project from Dave Merrill at the MIT Media Lab he now has a startup company called 50 Oh in San Francisco and the idea is that we're really good at manipulating objects with our hands hand and brain co-evolved so why don't we try to leverage the dexterity that our hands have better in our computer user interfaces so instead of giving you one screen that's fixed his interfaces are spread across many small screens that you can manipulate so here's a demo video there's these games for example where you create the maze by moving will you form words like and from each of these tiles small computer that as input output and I can detect which other computers are here here's another example from another colleague of mine Scott's opponents at Microsoft Research playing guitar hero with an air guitar well how does that work well he actually has a number of electrodes glued to his forearm that concent muscle contractions and then he uses signal processing algorithms to figure out what the configuration of his hand was given a set of sensor signals the other really big area of research is to move all the way in the other direction away from new hardware and sensors into everything that is happening online so understanding modeling social networks is a huge area of research in human-computer interaction for example the question are you my friend is actually quite complex what does it mean well it could mean any of these things oh no there are more reasons our social relationships are nuanced and complex and by conflating all of them and reducing them to a 1-bit friend yes or no rating we run the danger misrepresenting what our relationships are actually like and so researchers in HCI both try to understand what happens when we have simplistic models and how to create better models and better algorithms for social networks online the other part we're trying to understand is online peer production systems like Wikipedia like large collaborative efforts that have really excelled at being now one of me best and most complete repositories of knowledge about our world and we can ask the question how does this social participation work in practice what makes Wikipedia succeed and others fail and are there any laws or patterns and it turns out yes there are laws and patterns that we can mathematically model and describe for example almost any site you look at online will have a so-called zip or power law distribution where the most active users are really really active and then the next the second most active user is half as active third most active user yet half and you have this really long tail of millions of people who contribute very little and so the question is if this is the underlying reality and it turns out to be the reality over and over again how do we design systems that both leverage really active power users and keep the long tail of other users around because they still contribute very valuable Tippett's just at a much smaller scale all right that is research in the large in HCI and I thought I'd also tell you about a couple of research projects that are happening here at Berkeley and human-computer interaction so my own research group does work at the intersection of three different subfields crowdsourcing so active peer production online and collective intelligence design and development tools that is new tools and environments for designers and programmers and work on new types of user interfaces such as touch based user interfaces and physical computing which is a term we use to describe any interface that uses sensing and actuation and we have a large number of projects across these different areas too many to kind of survey in the remaining 10 to 15 minutes we have here so I thought I just focus in on two of them first let's look at multi-touch applications and toolkits so most of my projects begin by defining a target user group and a target task these users are trying to achieve and then finding about what's currently hard what is not supported and how technology can help here in specific I looked at how to support co-located design so if we want to support product designers I did a bunch of observationally and one thing that really jumped out is today there's just this coexistence of physical and digital media in any type of meeting so this is and this photo for example there are printed out schedules but then there's also probably a PDF pulled up on the laptop and someone else will have the group calendar on a tablet but then things get scribbled down on napkins and into notebooks and these two world live very uneasily next to each other they basically know nothing about each other so the research question we asked was how might we help design team members manage that flow of information across the digital and physical boundaries and the particular system that we build was a large interactive tabletop display which looked like this that allowed you to both interact with digital information because it was projected so we had multiple projectors showing information but because it is very large it's four by six feet you can also put standard design notebooks sketchbooks other things on that table so let me show you some of the interactions we then designed around this table here for example we have multiple people using multiple keywords in mice simultaneously on the same display and the users can go back and forth between using keyboard and mouse and touch based interactions but I also talked about in the what I told you a minute ago about the motivation that we really wanted to bridge this digital and physical divide this is all still digital so let's look at some interactions to bridge that gap here you can take your physical design sketchbook just drop it onto the surface we recognize where it is and you just drag up a copy in a couple seconds later you have a high-resolution digital copy you can also go the other way start with a digital image just snap it to your sketchbook so you can trace it and a minute later you walk away with a high-quality sketch so how did we do this well there was a whole bunch of engineering so we built this table platform by using multiple tiled projectors which project a single unified desktop from the ceiling onto this table to make the table touch sensitive we then put multiple cameras below the table and these cameras had filters on them that only looked in the infrared spectrum then we had a bunch of infrared light sources that would flood the underside of the table with infrared light and then when a finger would come in contact with the surface the light would bounce up your finger back into the camera and you can then apply standard computer vision techniques to find out where that finger is and you can also find out when you place objects onto the table you also get a reflection but the reflection has a different shape so we can now use shape analysis to distinguish between fingers and objects and let that then gives us high-level information such as right now there's one finger down there are three keyboards down and three mice and we can track both the position and orientation over time now in addition to creating this hardware we also work on the software side so here's a project that a student dico advice Kendrick in and graphics and HCI group did as an intern at Pixar in Emeryville to support the work that set wrestlers do at Pixar so set dressers are part of the animation pipeline they create the environment that animator that animators later then drop the characters in so we thought about if you have multi-touch input how can you create the right interface for set dressers and one key insight is when you have multi-touch you can distribute tasks across multiple hands so we came up with a gesture set for the specific domain of set dressing and then we wrote an editor that runs on a multi-touch station that allows set dressers to really rapidly mock up environments that can then be used in a later stage to position characters in and that then gets sent out to lighting and rendering and at some later point an animated movie comes out all right so that's some research in multi-touch we have a couple of minutes so I wanted to show you one really fun project in the in the realm of crowdsourcing so who in the room has heard of Mechanical Turk raise your hand all right not that many people so let me take two steps back then Mechanical Turk this was a chess-playing machine from the 18th century that was a robot that seemingly could beat even good human players at chess now how would they build a robot a chess-playing robot back in the 18th century well it turns out it wasn't a robot after all there was a small person hidden in the cabinet that could look and see you know the state of the board and then with magnets move pieces around all right there was the old Mechanical Turk the new Mechanical Turk is we're seeing a change in the way virtual work happens online more and more work is happening online so we now have virtual labour markets like Amazon Mechanical Turk inspired by the earlier robot that uses small contributions from people to solve tasks that algorithms can't do on their own so here are a couple of tasks for example you can for two cents copy text from a business card or for two dollars manage a calendar so typical tasks here are things that are really hard for artificial intelligence algorithms nowadays but that are really easy for almost any person so image labeling what's in this image spam detection is this comment from a bot or does it actually contribute to the discussion business listing verification does this business still exist or have they gone out of business or OCR handwriting recognition there was a really bad quality picture can you just type what's written in this picture now right now online platforms are really good for solving these kinds of tasks that require just basic human cognition but there's a separate question what if you have these small tasks that experts can do experts right now don't go onto these platforms to for five cents tell you how to solve a differential equation they're busy with their jobs they you know you may have to pay them a lot more so we had this research question of how can we get expert small tasks done by crowdsourcing and our idea was maybe we can just leverage local experts people who are ready experts in a particular location so what's a typical expert task well here's the one that any teacher will understand exam grading many many computer science classes at Berkeley right now have 300 500 600 students right now grading 600 exams is a daunting task especially when the exam is written at the same time you can't just put this online because people have to have knowledge about computer science principles to be able to grade these exams so we had the idea to try out peer grading having students grade other students exams what would the incentive be well we thought can we get students to grade other students exams in return for snacks sounds like a crazy idea so what we did is we actually found a vending machine we gutted it stripped all the analog electronics and put a touchscreen interface inside and so you would start by scanning your ID so we could detect people who try to spam or trick the system you'd then fill out some questions and then we'd ask you to grade exams where we showed you the question we showed you the answer key and a cropped piece of a scan of someone's image from from a handwritten exam and then you had a slider to assign points so here's how that would look so you read a question assign points submit your answer you'd now get another credit and you would work through questions until you had enough credits to then make your selection and vendor items so as in other projects there was a big system building component so we had some help from a mechanical engineer and then we used you know we used microcontrollers to control all the vent motors that are in this machine but all of this building was really to get at a question and that is is this type of grading any good so for that we deployed the machine in soda Hall and in one week we graded almost 8,000 exam answers for about $200 in candy so we knew we could get volume now can we get quality and this is once again we go back to experimental design how do you construct an experiment to afterwards say with certainty whether your design worked or not so we defined a gold standard where we hired 11 expert graders to create the same set of exams and then we also took that same set of exams and put it online on Mechanical Turk and that now allows us to compare compared to the gold standard how good is the machine how good our online workers and the answer was well this was a five-point scale if people chose randomly expected agreement would be 20% people online were sick we're significantly worse then chance they were systematically wrong because that's how exams are written right if you don't really know what the question is about you're gonna get it wrong however on our machine we had about 84% agreement with experts with PhD level students now the question is what does that mean how is that compared to one ta well I told you we took 11 TAS so now we can analyze redundancy how good is one ta compared to all 11 T is how good our two TAS how good is one person on the vending machine how good are three people on the and so the interesting result was if he added more people on Mechanical Turk the results just kept getting worse we just got more noise however if you added redundancy on the vending machine you actually got to be better than a single expert at 30% less cost all right this is these are just two projects from a whole raft of a dozen or more if you're interested just go to my website see us at berkeley.edu slash tilde bjorn and before you wrap up I just also wanted to tell you if you're interested in user interface design and human-computer interaction look at CS 160 an upper division course that we teach every single semester myself John Kenney and Manisha Agarwal and one last link we also have an undergrad certificate program this is the course thread in human centered design if you're interested in HCI we have a whole list of approved courses that are relevant to HCI and if you take three of them you get a certificate that you can show to employers once you graduate thanks "
8ZAHfYUSinU,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-10-21T16:52:46Z,"L19: Mental models contd. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/8ZAHfYUSinU/hqdefault.jpg,Josh Bongard,PT47M44S,false,67,1,0,0,0,okay let's get started I apologize we're starting a few minutes late Vice President Biden kept emailing me this morning he wanted to learn a little bit more about mental models and memory attention perception I kept asking if he'd registered for this class and he's trying to claim he didn't need to because he was the vice president he's not here okay so let's carry on we are working our way through our section on cognitive psychology and we've organized these four lectures from things that are relatively that we're beginning in the neural science field to nail down and things that are a little bit more hand wavy so we started last time talking about mental models otherwise known as forward models why are they called forward models you have tons of them in your brain so but they're making predictions about something that's going to happen in the future right I do something and I expect to see something or hear something or feel something tenth of a second from now a second from now a week from now a month from now a year from now so there is growing evidence that we can actually measure these forward models in the brain we're going to get into memory attention and perception today these are things that we know we are capable of but they're a little bit more hand wavy how they're actually instantiated in the brain we're not quite sure we're going to talk later next week about Gestalt perception and the frame of reference problem even more hand wavy and then finally we will get to affect otherwise known as a motion again we all know we have it but exactly what it is and how or whether we would ever want to instantiate effect in machines we'll see okay so before we get back to lecture 11 any questions about deliverable eight I see that most people manage to get something submitted for deliverable seven anybody's still working on deliverable seven a couple people okay all right we'll get there so most people I think have got something instantiated yes why they know crazy there's no quiz today because I had some colleagues visiting from another University and we've got some research meeting so I won't have time to put together a quiz for you today so Friday afternoon no quiz today I'm meeting a buddy it's the meeting with Biden yes that's right okay all right any no questions about deliverable eight yes I posted the PDF and now that you say that this submission for deliverable eight is not there can you send me an email to remind me i'll make the submission the PDF is there so you can get started on it i'll put i'll put it up on on blackboard you have a crazy yeah you can see it yeah exactly just click on it from the schedule and it's all it's all there I'll put it on blackboard later today okay okay so finally lecture 11 we introduce the evil starfish last time and the reason why we were talking about robots in an HCI class is because this particular robot gives us unique insight into how an agent which may be a robot or a human user forms mental models or forward models they do it by interacting with the world pushing against the world and observing how the world pushes back and every time you do that you get a training instance you push against the world a whole bunch of times you now have a training set and you use a machine learning algorithm to produce a mental model and you can now use that model that when you think about pushing against the world you don't literally push against the world you submit that action to the mental model and the mental model gives you back a prediction about the sensory repercussion forward in time of that action right so obviously mental models are very useful for robots and humans alike they allow us to avoid dangerous situations and mentally simulate something without having to try it in reality so mental models are good because often mental models might die in our stead okay so back to HCI now obviously in HCI it's all about people and different p use the same system in different ways so they form different mental models so your grandmother goes to buy you an mp3 player at best buy for her if it's anything in the shop that plays something called an mp3 then it's an mp3 player right that's that's it so as she looks at devices making predictions about is this an mp3 player or not that's that's the bar to pass for other users they might be a little bit more discriminating right is it actually an mp3 player and so on okay different people obviously push against applications or systems in different ways right so if I want to learn about the system and build up a mental model of the new smartphone that I got I'm going to start to navigate through the menu hierarchy and my mental model that I built up from using smartphones over the years that mental model makes a prediction about where the function called send new message is located in the menu hierarchy of my new phone so rather than clicking at random through the menu hierarchy on my phone I'm being much more selective and I'm being directed by my mental model which is making expectations about where where to find it right okay so where's the turnoff new message alarm my mental model predicts it's either in the messages or the options submenu and as we do I start to I'm you I'm starting with my existing mental model and now I'm adapting it for the new system right so we're always building up and adapting our mental models as as we go how do we build this up well in essence what we're doing is reducing uncertainty with each action we perform so in this example here I have some uncertainty about where this particular function is it's either in messages or options so maybe I flip a coin and pick one and if it's not if I don't find that functionality in messages I've increased the certainty that it's in options so we're going to come back to the robot in a moment and see how it also reduces uncertainty okay another set of your users the early adopters they'll just go through the entire menu hierarchy they want to know everything that the phone can do so they're not really using a mental model they're just doing brute-force search right what is everything that I can possibly do with this new phone or this new this new app different people have different mental models and use them to push against the system in different ways ok so again obviously we do this how do we build up a mental model by playing around with the phone or the app and seeing what it presents to the screen or what sounds it produces and then there's a process that we follow and the first one is usually rote learning you've got a brand new application you just kind of remember where everything is right so in this cartoon example over here you've entered into n interactions with the system and you've got n different responses back but you start to notice that these responses tend to fall into one of two classes so I see that some of you have built into your ASL system now an interaction that will flash a big green checkmark to the screen or a big red X so I'm playing around with your system and I see that most of the time I'm getting either a check mark or an X and I'm not really paying attention to why or why not I'm noticing that there are those two main sorts of things that your system gives me back whenever I interact with it ok most people at some point will then use that to embark on conceptual compression so I learn something about the underlying conceptual structure of this new information space that there are two conceptual objects right and wrong are correct and incorrect and there are certain things that I can do that result in correct and there are a bunch of other things I can do that end up in incorrect so you don't need to print to the screen this system will look at your hand and figure out whether you're signing the digit that's being printed to the screen right hopefully given the kind of system you're setting up most users might start by waiving their hand more or less at random getting a whole bunch of things back and realizing that there are certain things they're trying to do they're trying to do this thing that results in the correct repercussion and avoid the things that leads to the incorrect incorrect repercussion years ago when my father was trying to learn how to use computers he is definitely not a tech guy he definitely didn't want to use computers but he likes to write and he realized he eventually had to give up his typewriter and use this newfangled thing called a word processor which wasn't so newfangled at that point i sat down took a deep breath and tried to teach him how to use a word processor and I said if you want to type on the screen click open click file click so on I wrote on a post-it note and put it over his desk if you want to say what you've written click on this then click on this then click on that and by the end of this we had about 30 or 40 post it notes in front of him he was never able to conceptually compress that into the underlying structure of what a word processor really did he would just sit down and follow by wrote these post-it notes as you can probably tell from me telling you the story it didn't end very well why not we don't live in the same city so I started to get a lot of phone calls he told me didn't care I don't want to know why this computer is doing this I just want to print what I've written so far write me a post-it note about how to print what I've written so far I don't care why just what do i do to print what's the problem that you can imagine arose none of you have parents or grandparents you've gone through this process with them I give no Hardware questions and things up for questions okay fair enough the point where your ability doesn't match up the problem if something comes up right so he deleted something he wants to undelete it and there isn't quite an exact posted note that corresponds to that situation I get a very agitated phone call absolutely right so my father could in most cases on a good day to end things but not learn how to do the n plus first thing exactly right that's why aside from the fact that most of us in this room probably just like technology you bother trying to figure out the underlying structure once you know how a system works you can usually figure it out on your own and if the interface is good you can usually get it right on the first the first try right okay so obvious drawback of rote learning you can't figure out how to do something new or extricate yourself from a new error message or a new a new problem right you can't predict what will happen if I do this or if you get some new repercussions some new error message or some new message you can't reverse engineer from this novel result what you might have done to cause it and thus undo it right okay different users are going to make it further along this process than others given your your system okay okay so we're now in the section in cognitive psychology cognitive psychologists in general love optical illusions so we're going to get the first one in a moment has anyone seen the Necker cube before this is one of the most famous ones in psychology okay so when you are interacting with the physical war old or a nap you're building up a hypothesis for what's going on inside the machine what is the thing that is taking some stimulus from you it's receiving a key press or a gesture and it's giving you back a response as you're doing this you're interacting with the system York you're building up a mental model of what's going on inside the app right okay let's take a very very simple example of this this is the Necker cube and I'm going to stop stop talking for a moment and I want you to just kind of relax and focus on the Necker cube and as you do I want you to not think consciously about the answer to this question which is which square is closer to you I want you just to try and allow your brain to tell you which face is closer to you okay which face is closer to you the square in the bottom left or the square in the top right change it change well you know the real answer which is neither right so that's why I asked you to relax and not consciously try and answer the question your prefrontal cortex the front of the part at the front is saying the professor's crazy why is he asking me this question I know it's neither but the back of your brain which is where you're receiving the visual stimuli is telling you something different right it's telling you that you're looking at a three-dimensional object you said it changed it it changed for others try and describe for me how it changed okay okay so there have been lots of studies of this so for some people it's an asymmetry they see more of one front facing side than another for others it's about 5050 you said sometimes it's switched did anyone experienced the rate at which it switched did it feel like it was going like this okay after a little bit let's switch to the top right one once your brain was able to interpret it as the top right being closer now you have two hypotheses right your brain says it may be the bottom left or it may be the top right and hopefully most of you started to experience this alternation right for most people it feels like after a while it feels like it's alternating yep for me was when I first saw the bottom left one but as I looked up of these top that's what I'd look it sort of to Swisher me now every time I move through like that very central little square its wishes okay so as you looked up right or as your eyes past the small square in the center you're obviously not pushing against this cube right you're sitting passively but you are interacting with the Necker cube in a very important way there are muscles in your eyes that are causing your eyes to saccade or move to a different place on the Necker cube that is you figuratively pushing against the world how does the world push back does so you you move and remember the world pushes back because it gives you some new sensory stimulation every time you act in the world your sensoria all your five or six senses depending on how you count register a change what's the change how did the world push back no yep I was gonna say like just the way you perceive so the way you perceive it that changes but let's be even before we get to perception you are looking at let's say the very bottom left corner here and you flex some of the muscles in your eyes which took your focal point to the upper right what happened goodbye well before we even get to the box switching you pushed against the world the distance of the bottom left got further away right so the pattern of photons that are falling on your eye is different right you SAT there for a few seconds doing this you're building up a training set if i look at this point and i send muscles to try and send my eyes to focus on this point before your eyes get there which takes about a tenth of a second that's at the length of a saccade before you get there you have mental models up there that are making predictions about what you're going to see when your eyes get there we're did it where you dictions supported did you see what your brain was predicting you would see this is where things get a little tricky the Necker cube is a very simple thing but has a lot of these subtleties let's imagine you're looking down here and let's say you're looking at an actual three-dimensional cube in which this lower left face is actually closer to you and you saccade up here what would be different from what you would see in that actual 3d cube compared to the Necker cube would be very similar the prediction of your mental model is going to be pretty close but it's also off by a little bit why part of the track square would be visible that's true there's Anna cloquet let's say it's a physical cube that's made out of a bunch of straws no occlusion what's different what what's different between this drawing of a 3d cube and an actual 3d cube any arts majors here which means yes how so you're looking at an actual 3d cube rather than a two-dimensional representation of a 3d cube relation to yes exactly right so perspective that back cube would seem a little bit smaller to you for any three-dimensional cube because it's further away from you right things that are further away are smaller your brain has 20 or 30 or 40 years of that of that experience no actual 3d cube if it's actually a cube has ever violated that expectation the Necker cube does so your prediction is frustrated a little bit so your brain says whoa whoa whoa this seems to be a cube but it seems to be a cube in which the lower left face is closer to me but my prediction is not matching up with my actual with the actual data that I'm getting back like you're KN learner that predicts one thing and the reality is actually something else so your brain says this hypothesis I have one hypothesis i'm looking at a cube with the lower left face closer and that hypothesis doesn't seem to be standing up to the tests that I'm performing I'm looking down up here so is there another hypothesis that maybe explains what I'm looking at which is what's the other hypothesis that you actually are looking at a queue but now it's the upper right face that's closer to you right so now your brain does the same thing it says maybe it's that second hypothesis I'm going to now make a prediction I'm going to I'm going to in a moment saccade and look back down here and your mental model yet again makes a prediction your brain is a prediction machine it's doing it all the time makes a prediction about if this upper right face is closer to you you should see a certain pattern when your eye gets down here what happens when your eye gets down there it right so that hypothesis didn't stand up it's off by a little bit your brain says shoot that hypothesis didn't work either what else is there there really are only two kind of hypotheses that exist here so I don't know if you were aware of it or not but while the cube was doing this in your mind your eyes were also jumping around the picture you were forming and trying to support hypotheses and collecting evidence that frustrated that hypothesis and around and around you go right it's kind of subtle but that's what you're doing when you're interacting with the real world or a nap or anything you're forming these hypotheses okay i found this functionality based on the fact that i found this I predict this structure for this app which tells me that I'll probably find this other functionality which I seen yet at this other place in the system oh it's not there I guess my understanding was wrong there's other there must be something else going on here ok this is the heart of mental models you have them you build them and you build them by forming hypotheses and you form hypotheses by pushing against the world and observing how the world pushes back ok so let's move on a little bit now we're going to look at different kinds of cognitive architectures and this is where the hand waviness starts so cognitive architectures if you look at any psychology textbook tends to look something like this you have a bunch of boxes and arrows this is a sketch of what your brain is doing this does not in any way correspond to the actual reality of what's going on in your brain so just hold on to that disclaimer but as we just went through with the Necker cube you're continuously pushing against an app or a system or your user is there observing what they see and what do they do with that material so in HCI you will often see discussion of the twin gulfs known as the gulf of evaluation and the gulf of execution so let's imagine we have a certain goal or your user as a certain goal when they're trying to play with your system in this case the goal is to build a mental model so they click on a button on the phone or they tap on the screen of the phone and they see something happen they then interpret what they see and they then need to cross the gulf of evaluation did what I see help me towards my goal did it help me build up a better mental model of what's going on let's think of another example let's imagine instead of my goal being to build a mental model my goal is to find the turnoff new message notification item I press some button at the top of the higher menu hierarchy on my new phone and I see a new set of options I now need to evaluate did I am I get closer to where that hidden function is or I'm trying to find this that particular function and I've clicked on the message button or in the sub hierarchy have I gotten closer to finding that functionality or have I gotten further away am I making progress towards my goal or not all of the visualizations you're going to be showing to your users in your ASL system is ultimately that sort of thing their goal is to learn the 10 digits of the ASL language did what you showed them on the screen help them towards that goal or confuse them you need to show them something that helps them cross the Gulf of evaluation no I sign that digit wrong or yes I signed that digit right or I try to click on that button on their system and it either did click or it didn't register my click does that did the system get me towards my goal once i'm here and i'm still trying to fulfill this goal what do i do next so I I'm going down through the menu hierarchy and I still haven't found turn off new messages I see that there are four new options for me at the current level of the hierarchy which of the four buttons do i click on which of the four possible actions that are available to me do I execute I I pick one I try and cut cross the Gulf hopefully the labels on those four buttons help me decide which of the four I should choose and I choose it and around and around I go so your system one way of thinking about an intuitive interface is that it helps the users cross these two gulps if they're trying to do something with your system is it easy for them to figure out what they need to do next to get closer to their goal and once they do do something can they interpret from what you print to the screen whether they have gotten closer to the gold or not make sense now here's another example so back to our starfish for a moment it has the same problem the gulf of evaluation in the gulf of execution so the robot is trying to figure out evaluate what it's just done and figure out whether that has helped it get closer to its goals let's think about stage one of the experiments where the robot is trying to figure out how its put together so its goal is to build a mental model right so if it performs an action did that action lead to more accurate self models and it can cross that Gulf by comparing the sensory information it got from its physical self to the sensor information it got from its virtual self right so the physical robot does something gets sensor data back the physical robot stops moving and it starts the machine learning algorithm that's trying to train this mental model if these mental models are generating sensor data that is approaching the real or physical sensor data then the robot is crossing the Gulf of evaluation it knows it's getting closer to its goal which is an accurate self model because an accurate self model produces sensor signals that are very similar if not identical to the ones from the physical robot so far so good okay this one's a little bit more tricky the Gulf of execution so the robot has these two mental models at a given point in time the robot knows that both cannot be correct so the robot has to decide what action the physical robot has to decide what to do next what should it do next it says here right which is choose actions that caused myself models to disagree what does that mean can we be a little bit more specific the robot is trying to cross the Gulf of execution the phys robot is trying to decide what to do next what should it do its testing hypotheses so it has two hypotheses here must mental model one and mental model to what action should it execute what does it mean for two models to disagree here yes but it hasn't collected that sensor data yet this is what makes it tricky so the robot wants to perform at your right the robot wants to perform an action in reality that will generate sensor data that is when supply to its current models gives back different sensor information right it is trying to generate sensor data that breaks the predictions of the current mental models so that they can be replaced by new mental models but the robot doesn't have that sensor data at it so at the Gulf of execution it has to decide what to do how can it increase its chances of performing an action that produces exactly that kind of sensor data what is the sensor data what are the two sensors that this robot has it has touch until when we threw that away and we only kept the third one tilt right which is vestibular right so how much it tilts left and right and how much it tilts forward and back so what action should the physical robot perform at this point it wants to get these two models to disagree what does that mean what does DISA can we can we be more specific about what we mean by disagreement here exactly so Gulf of execution I have a hundred different actions that i'm considering at the current time or a thousand or million doesn't matter I take each of those hundred actions I supply it to this model and I supply that action to this model an action one causes both robots to tilt to the left the tilt information is in agreement so the robot throws away that candidate action goes to the action number two takes that second action supplies it to model one and model two and now model one rotates to the left and model two rotates to the right they are disagreeing model one is saying physical robot I think if you perform this action you'll tilt to that model to says no no don't listen to model one model one is wrong I know that physical robot if you perform this action you'll tilt to the right they can't both be right the physical robot is either going to tilt to the right tilt to the left tilt to the right or do something else at least one of the models is going to be wrong and it we're going to break that model we're good and push through help the machine learning algorithm push through and replace whichever model was wrong with a more accurate model if the robot had performed action one which caused both robots to tilt to the left and the physical robot tilted to the left everyone was right the robot hasn't learned anything new right and that's its goal to learn something new and improve these mental models this is basically describing the life of a teenager right I'm going to try and do all these things which I know are right on the line I'm not quite sure what might happen i want to see i'm going to do dangerous things things that i know my parents won't agree with or this particular authority figure will disagree with i doing something which is actually useful which is building up my mental models of the world and self and social situations and so on you can cross the gulf of execution by choosing an action that will lead to some result that you want it or you can choose an action for which your mental models are not sure what's going to happen and assuming you survive that test you've learned something right okay nothing ventured nothing gained right okay alright so again I mentioned we're doing a little bit of hand waving here because we know exactly how the robots are building their mental models we know that humans have mental models or forward models we're not quite sure what form they take but we do know that we have lots of them and they also operate at different time scales right so at the moment that I'm speaking I have mental models that are making predictions that I'm going to hear my own voice at a hundredth of a second into the future I have a second mental model which is making predictions that hopefully I'm explaining myself well and that mental model will be validated if you do well on the quiz tonight although there is no quiz tonight right so I have a second mental model which is operating at a slower pace I have yet another mental model which is making predictions about the final grades you're all going to get in this class and I'm altering what I do hopefully that you all get a's at the end of this this class right okay you've got your new smartphone or your operating you're playing around with a new app you also have different mental models that are operating at different different time scales right when you click a button or tap the screen you expect to see something or hear something or assuming you're holding the phone and it vibrates you expect to feel something basically instantaneously you might be sending a text or an email you don't expect an instantaneous response to those you have some expectation about when you're going to hear back partly based on who you're sending the text or the email to right other examples when you're interacting with systems what kinds of predictions are you making at different time scales you put your hand over a burner that one absolutely right you have a very you have a very short time scale very important prediction let's limit our discussion to electronic systems what other interactions do you enter into with an app where you have expectations at different time scales right exactly right so if the wheel ever comes up that's a signal to say pause your mental model that's expecting an instantaneous response whatever whatever you've just asked for it's going to take me longer than instantaneous to do and how do you know whether you're loading something that's local or cached or from the internet it may be useful for the app to show you which helps you calibrate your mental models right if you know you're requesting something from the web you're not expecting an instantaneous response but if you're if the interface doesn't let you know whether you're asking for something that's local or from the net you might get frustrated right you only have two seconds before you have to put your phone away so you want to ask for something its local but you don't know you click on something up comes the wheel of death and you're frustrated right so what you draw to the screen is often helping your users calibrate their mental models or calibrate their expectations about when they can expect response you're also influencing your expectations by lots of other things right your hopes your dreams your beliefs your emotional state your level of attention how late in the day it is for you all sorts of things right we're subjective beings okay okay so we the brain is a prediction machine it spends a lot of its time trying to improve mental models but they're not perfect and that matters you're again the interfaces that you create need to support our imperfect ability to form and use mental models right it's difficult to run a mental simulation you can do it at a very coarse level right so you can make a prediction probably about what's going to happen at eleven-forty in eight minutes from now but you can't really visualize your actual footfalls as you leave the room right you know the basic now this is basically going to play out but the details are very very hard to mentally simulate we forget things over time our mental models degrade were affected by bias and superstition right I've had three smartphones already and function X was always at this point in the menu hierarchy now you've sold sold me phone number four and now the function is somewhere else right and you obviously adopt your mental models from previous systems and technologies and and so on right so as you're building your ASL model your users at the end of this course I've probably never seen the system before but what expectations are they going to bring to your system the moment they start waving their hand over the device okay how can you help your users build up a mental model of what's going on under the hood make things consistent how so okay right so that you try and get away with all I throw away all the extraneous detail make everything consistent very simple most users will twig to that pretty quickly right and appreciate it I don't know if you remember the first time you ever saw the Google front page I do you realized immediately this was a whole new ball game right this company called Google was committed to truck they knew what you wanted to do all you wanted to do is type in some keywords and get a list back nothing else at least to start with right you're already advertising something about your your system in that in that case I expect that when you click in Google when you click on a button it's going to stay simple at the next page maybe not as simple as the landing page but that's something that's important in this de system when your user figures out to waive their hand over the device they're going to see something that is telling them somehow to Center their hand unbeknownst to you a timer starts counting down from 10 to see whether they keep their hands centered for 10 time steps right I think that's in there is that right first that is that to Rick what is it or what everybody out to your discretion right whatever it is you want the user to hold their hand there for a certain period of time your user doesn't know that how might you communicate that in a minute so you couldn't actually progress the signing stage until you would go back to the initial state ok so I could see that right so something happens when my hand is near the center and then I go back to something else when my hand is off that might be enough to say okay it's trying to tell me to keep my hand there but how long do I need to keep my hand there until i unlock the next thing that I can do with this system I don't have any mental model here I don't know how long I need to keep my hand there until it happens I might be a patient user and wait indefinitely or I may not great that helps right so if the countdown starts at one hundred starts counting down from there that might be the end of my interaction with your your system right it may matter so you could do that you could color the lines right the lines become increasingly green as the hand is over the device most people have an expectation that things that are becoming green are becoming more good right if my hand is drifting out of center maybe the wireframe becomes more red whatever i'm doing i'm moving away from what I'm supposed to be doing right I start to build up a mental model of this system right it whatever it wants me to do if things on the screen are getting greener than I'm doing the right thing if things are getting redder I'm doing the wrong thing if I see a whole bunch of digits that start counting down to zero okay I get it there's a whole bunch of timers here there's things I'm supposed to do for a certain period of time and i'll be told for how long i need to do it there's different ways you can you can do this okay here's some other ideas the book talks about a system image which is kind of putting all of these things together so a system image is the interface itself the underlying philosophy so the Google philosophy of keep it simple or the philosophy of more green means better more red means you're getting worse system behavior documentation most modern apps don't have documentation anymore so we can kind of forget the last thing we've already talked about visual metaphors right so i might see that their cluster of buttons and i might start to realize that buttons that are closer to one another allow for similar functionality buttons that are further from one another imply different functionality so distance implies semantic dis distance meaning difference in functionality common things might have big buttons or obvious button is rare or dangerous actions might have small buttons to minimize the chance that I will click on them by mistake this especially matters in a gesture system where it's actually kind of difficult to actually indicate a specific action what other kinds of visual metaphors might be useful in a system like this physical proximity implies functional similarity size implies importance or common X implies Y what might be X and what might be why here so let's think about your ASL system again you are going to allow the user with their secondary hand to navigate through some menu hierarchy what kinds of visual analogies or metaphors might be useful here let's say I have a horizontal row of buttons so there are different things that I can do in your system and I learn through some sort of interaction with the system that you don't want me to actually click on the button it's more of a pointing thing so if I want to click the left mouse button what would I know that I need to do what gesture is going to get that is going to click that button for me so the angle of my finger right so if I click further if I point further to the left i'm selecting the left mouse button and if I hold that for five iterations through the infinite loop the button clicks right so now I learned that the direction of my finger implies which button I i press right and buttons to the right probably mean forward to future lessons or harder lessons and buttons to the left maybe mean backwards or repeat because for most people that read from left to right left means things in my past and write means things in my future okay I think we'll leave things there for today you do not have a quiz due tonight you do have deliverable eight due next Wednesday have a good weekend 
KKOm7M-tyrQ,28,"Full playlist: http://goo.gl/e4CV2K

Course home: http://goo.gl/Cp4uDR",2016-11-02T16:42:12Z,"L24: Ubiquitous computing. (Fall 2016 Human Computer Interaction Course, UVM)",https://i.ytimg.com/vi/KKOm7M-tyrQ/hqdefault.jpg,Josh Bongard,PT50M1S,false,151,2,0,0,0,okay let's get started let's spend a little bit of time talking about the deliverables so deliverable 9 is due tonight everybody in good shape got two or three scaffolds implemented okay get in there okay we have reached that time in the semester the tenth and final deliverable so let's talk a little bit about deliverable ten the structure of deliverable 10 is very much like the structure of deliverable 9 and deliverable 9 you are adding in three scaffolds in deliverable 10 you're going to be adding in three real-time visualizations so we spent a lot of time in this course talking about information design visual display of information here's your chance to flex your creative muscles so to speak the first visualization I want you to build in is to show the user as they log in from one session to the next how their current performance in the current session compares to their performance in past sessions so again no text here visualizations are they getting better so at a glance they should be able to see as they're working with your system during the current session have they forgotten more material since the last time that they they've retained are they getting better or are they getting worse from one session to the next you're going to shoot a short video of your first visualization in action that's going to show us that so you have your user which is probably you doing something in the first session logging out logging back into the second session and you do a better job in the second session and we can see that the visualization is showing that you're doing a better job the neck that during this session than the previous one how you do that is completely up to you shoot a shoot a video of that visualization and then the second visualization is a social visualization so how is the current user doing compared to other users that have used the system so far so create some virtual accounts in your system maybe user one is not so good at learning ASL user two is better at using ASL and when user two is using the system we can see in the visualization that user two is doing better than user one again you can do this in any way that you want if you want to use a points system you can we're assuming that the users recognize numbers so if you want to assign points and you have something like a high scoreboard that's that's also fine that'll count as a visualization okay and then the third one which you the third yes question yes yes I think I think that's fine right I just outlawed English because I didn't want you giving instructions about what to do so yes names and scores is perfectly fine that's okay third visualization which again you may already have in your system is a hot and cold visualization so obviously the user has to hold a certain gesture and give the can and learner time to determine whether they're signing the digit correctly or not and you need to create a visualization telling the user to keep their hand there and in addition are they is the can predict incorrectly right so you probably have in your system now that you need the can and learner to predict three or ten times in a row I don't remember what it is and that counts as a successful sign right so during those iterations you need to be showing the user that they're doing they're signing the sign correctly the reason why is of course because they may not they may not actually be signing at all they may be confused about what they're supposed to be doing and the system should be saying colder you're not doing the right thing whatever you're doing do something else number two down there they may just be signing it in right you may have removed one of your scaffolds which is showing an actual picture of the of the sign and the user forgets that this is zero and is instead signing this and your system should be showing that colder right the longer you hold this the worse things are going for you change what your what you're doing third one which of course is that the leap motion is not perfect right occlusion and all the rest of it they may actually be signing the sign correctly but there might be some occlusion and the KNN learners getting it wrong so again if the user sees the colder visualization showing up hopefully they shake their hand or they change their orientation so basically cold air should mean do something else and warmer means you're on the right track hold your sign as is make sense okay three visualizations you're going to shoot three short videos they could be as long as short as you want to demonstrate that the visualization is working stitch those three videos together in a playlist and set and post the URL to the playlist on blackboard so I'm good okay this one's pretty pretty straightforward okay so back to lecture we're working our way through this second-to-last theme of the course called looking outward which is we as a society are in the process of putting our interactive technologies out there in the world they're not hidden behind a pane of glass on a laptop or a desktop or a monitor or what-have-you right once you have a device that's out here in the world with us it is sensing the world and read real-time and if I have the vibration mode of my phone turned on it is pushing back against the world or against me and giving me information that way through my tactile sense right tangible computing so that's what we're looking at we just finished on Monday lecture 15 where we were thinking about broadening the ways in which interactive technologies can project - information back into the world at the moment it's usually visual through a screen or auditorally through speakers we now have devices that are vibrating or in the case of robots moving or physically acting on the world and we can sense the response of their movement right once you start to have those kinds of interactive technologies out here in the world with us that allows new kinds of interactions that were difficult or impossible before right we ended last time with the interactive workbench which was a series of magnets that could be differentially turned on and off and if they do they change the magnetic field on the surface of the workbench and if you put a metal object on it that metal object will start to move so there are forces physical forces being applied to that metal object by the magnets but also by someone who's holding the object and actively moving it so the interactive device the magnets and a human can together move an object right the actual motion of that object is a sum of the forces being applied by the magnets and by the human user by doing that again we have new collaborative possibilities you can't have the sense of putting your hand on top of the hand of an instructor and the instructor is sort of moving your hands through the motions of learning Japanese Kanji or learning some other tasks that requires fine manual dexterity as we continue on in lecture 16 through 19 we're going to continue on with that theme which is once you start to deploy interactive technology out into the physical world what new kinds of things can we do okay so we're going to start in on lecture 16 in a moment ubiquitous computing which is today otherwise known as the Internet of Things right so actual physical objects that are computerized somehow are connected to the Nets right physical objects that are also connected to to the net and those kinds of devices are becoming ubiquitous or they are everywhere and as that ubiquitous technology or the Internet of Things starts to sink into and get stitched into the fabric of our physical world what does it what does that collection of technologies allow us to do in lectures 17 18 and 19 we're going to look at three scientific experiments where ubiquitous technology is used to try and / and answer scientific hypotheses that would have been impossible difficult or impossible to answer without that technology in lecture 17 we're going to look at a problem known as social network inference so I see that there are about 30 people in front of me I have no idea which of you is friends with with which other ones of you so if I had access to your cell phone data could I infer your social network without looking at your Twitter data so just looking at face to face communication if i if I'm able to record that information somehow can I infer who among you know each other and hangs out with each other outside of this classroom on top of that if we could draw a social network of this class could we answer social science questions like how do you change your behavior when you interact with different members of this group so we are all embedded in social networks and as you know you alter your behavior based on who you're talking with in a social network we know that anecdotally can we actually measure that directly let me measure how people change their behavior when they interact with different individuals in a social network that's what we're gonna look at in 17 lecture 16 is pretty short today so we will definitely get it start in on 17 18 is we're gonna look at activity tagging so assuming your accelerometer is on on your phone when you're moving can your phone infer whether you are walking running standing still running on a treadmill writing in a vehicle can it actually infer your activity without you having to actually ask or tell the device in nineteen we're going to look at something that's become known as the human speech own project in the human speech home project which is a somewhat controversial experiment the investigators recorded most of the waking experience of a child during its first three years of life and they wanted to try and ask the question of when a child uh ters a word for the first time like mama or Dada or water what caused them to say that word for the first time what were the experiences from the point of view of the young child that led up to them uttering an English word for the first time again we can't really do that very well at the moment but if we have the home in which the infant resides instrumented with a lot of technology and we're recording a lot of the auditory and visual experiences of that child maybe we can start to answer some of those questions as you can probably tell from these short descriptions there is obviously the ethical side of Internet of Things and ubiquitous computing which is this is big brother made made real right so there's the question about what we actually want to record from people in the real world assuming that they agree to it and we do what can we learn from those kinds of datasets okay so let's start in on lecture sixteen ubiquitous computing I showed you this little cartoon at the beginning of the semester way way back in the Stone Age very few people had a computer then more people had computers but more people had more phones so now if you look at the world as a whole most people have more cell phones than computers some people only have smartphones we're moving into a world now where there is bored more embedded devices out there embedded in the sense that as you move about your physical space as you move about campus you are into an out of range of various embedded devices that can either detect your cell phone or detect you directly right so they're embedded in the physical world and they are directly sensing the world we're gonna end this section on looking outward with robots which are basically embedded devices robots have sensors they can sense the world directly but unlike embedded devices robots can move themselves so this is the future we're heading towards this is the Internet of Things assuming we want to build such a world what can we do with the data that is flowing out of this this system ok so ubiquitous computing there are two sort of founding fathers of ubiquitous computing Ken Sakamura and Mark Weiser we'll start with professor sycamore a-- from the university of tokyo he realized that even in the far future so this is back in 1984 in the far future when everybody owns a computer we're still going to only be able to sell as many computers as there are people on the planet right so maybe some people will have two computers most people will have one computer but even back in the 1980s they were starting to embed chips in things other than computers like cars and factories and so on so Kay can realize that the future is really looking at ubiquitous computing so developing hardware and software and operating systems for embedded devices the computer market is going to be big but not as big as what is now known as the Internet of Things so he created the real-time operating system so you've all heard of Windows you've all heard of Linux you've all heard of Mac operating systems how many of you have heard of the Tron operating system there are about 10 times more instances of Tron running on devices than there are windows and Mac and Linux operating systems combined this is an important aspect of ubiquitous computing is that it's out there in the world but we're not really aware of it this is that the statistic is a little bit old now but I think it still is the fact that embedded devices still outrank computers and laptops by an order of magnitude four years later Mark Weiser at the Xerox Research Park came up with this term ubiquitous computing it's still used kind of synonymously with Internet of Things Xerox PARC back in its day in the 80s and 90s this was sort of the Google campus of its day there was a lot of basic research that went on at Xerox PARC including HCI research I assign some reading for today actually and I forgot to put marks original original ubiquitous computing article up there but if you google mark Weiser PDF you'll probably find it in that document when he described his vision for this field of ubiquitous computing his vision would be that in the future computers would be like our childhood they would become an invisible foundation that is quickly forgotten we're not sure that it's we don't even know that it's there but it's always with us and it's used throughout our lives he stole the term you big word is computing from one of the titles of one of philip k dick books called Ubik what was the other book written by philip k dick that i've mentioned many times in this course already do Androids Dream of Electric Sheep okay more fun reading for you over the winter break if you haven't already oh great great book okay so this is sort of the idea about ubiquitous computing right is there more and more computers out there in the world we don't want to be continuously aware of them right it would be exceedingly distracting there are a large number of laptops and cell phones in this in this room at the moment but most of them are pretty passive they're not interrupting our educational exchange here right your childhood is also somewhat invisible but hopefully the lessons you learned you're in childhood are there in the back of your mind and there always readily accessible when you when you need them why so invisible is clear why foundation what do you mean by our foundation one of the important uses of ubiquitous computing as a foundation is something we've already talked about this idea of scaffolding right yours your smartphone usually isn't interrupting you but it's there to sort of give you a hand when you need it and hopefully when you exhibit competence your cell phone shuts up again right and lets you get on with whatever you're trying to to do so for those of you that go and work at industry and you're actually developing devices that are going to be deployed out in the physical world the idea the vision is to try and make sure that they're there and they're always ready to help but they're always kind of in the background right they're not constantly interrupting and and flashing up notifications and pulling your attention away from whatever you you need to do right an obvious example of this is the GPS system in your your car right if you put your phone up there yes there's the map but better to just let the GPS voice tell you what to do so you can keep your eyes on the road so when you turn on real-time navigation even if the screen is on it shouldn't be flashing up notifications that draw your eyes to the phone without your agreement right we talked about visual attention there are lots of things that will distract you and cause you to look at your phone you surely don't want that to happen while the user is is driving okay I think we've already talked a fair bit about scaffolding so we'll carry on okay let's try and apply some of these ideas of deploying technology out into the world we're going to focus in this example here on deploying technology into indoor environments first rather than outdoor environments why is it easier to think about instrumenting a classroom or a home with technology than it is outdoor environments the technology can be more delicate right stuff is going to Britt it's gonna be more easy for stuff to break out outdoors what else so for deploying technology into indoor environments there are usually power sources and Wi-Fi readily available not so much outdoors right remember that we're talking about embedded technology here so these are devices which directly sense their surroundings the moment you go outside and it depends on what you're sensing your your sensory experience is much more complicated right look around you this classroom looks more similar to other classrooms than other outdoor environments do to each other so the moment we have machines that are sensing things outdoors it's much more difficult on the machine to figure out what's going on so it's easier to deploy machines indoors first and let them learn about classrooms and rooms and houses and and hospitals and factories and so on before we move them outside okay so we're gonna tackle again a very real and very pressing issue which is aging populations I'm going through this with my father at the moment trying to make a decision about how much longer he can live comfortably in his own home as you can imagine this is a very difficult process to go through can we instrument homes that help elderly people continue to live in their homes in a comfortable manner what kind of technology would we deploy before we start to think about that kind of technology just to sort of impress upon you the importance of this problem here's an interesting visualization have you seen population pyramids before what does a pop what do these population pyramids tell you in particular right so populations are aging right and there isn't a lot there is much less her people replacing them what else can you draw from this visualization yes so females are increasing their longevity at a faster rate than males both are increasing their longevity but there is a gender difference here what else the percentage of elderly right so let's have a look at this projection for 2050 in this particular country it's projected that in 2050 there will be fifty three point six people fifty three point six percent of the population that are working so about half of the population working and half the population not working either retired or not yet working this is the scary point right that every country most countries are approaching which is this threshold in which they are now more less people working than there are not working right okay last question anybody know what country this is in particular this problem is worse in some countries than others Japan is the worst one but this is not Japan not Germany there's a couple hints and here two to it thank you the Ministry of Health and Labor's spelled properly with the U right unlike the American way of spelling it okay so there's a real important societal issue one possible solution is to try and reduce the amount of social social security that's needed for people that have retired and one good way to do that is to allow them to do is to allow them to remain self-sufficient for as long as possible okay so I'm going to start out with an example here and you're gonna turn to your neighbor and start to fill in some more of the details here this particular drawing here uses a notation that you'll see sometimes in HCI called hermia again a fancy acronym for entity relationship modeling so entities in an ER Mia drawing are shown with a rectangle and lines are relationships between them so in this case we have one physical object a carpet and woven literally into this car carpet RN computational entities pressure sensors inside the home of Miss X there are a number of these carpets so this is just a simple way to sort of show how physical and computational devices are connected together in in this case missmiss X's on one aspect of the problem of elderly people who are trying to remain in their home is family members trying to ascertain their current status my father lives in Toronto sometimes I call and he doesn't pick up the phone I don't know why that's the case right so one possible way to do this and again this is a bit of a cartoon example here imagine we were to try and develop a a system where the caller on the phone gets one of three missed messages either the phone continues to ring because in this case miss X is making her way towards the phone if she's moving slowly towards the phone maybe a message is embedded in the call saying don't worry miss X's is making her way towards the phone or I can't detect the fact that Miss X is making her way towards the phone right these are three very different situations that a caller might like to know about before having to call another family member or caregiver to go and check on Miss X at her at her home okay so we have a number of carpets there are pressure sensors that are embedded in there the pressure sensors through the carpets are reporting all of the data in real time into a sensor network device which is doing some machine learning on all this pressure data to determine is Miss X up and about and is she making her way towards the phone okay turn to your neighbor let's stick with the same problem what are some of the issues that an elderly person might face in remaining in their home how might you instrument the home to make things easier more comfortable for them or their caregivers okay to talk about that and we'll see what you came up with okay let's discuss solutions ideas to make sure things are turned on or things are turned on and off so how so legislative yep you sometimes forget to get older this way consents preciousness is in your house to see if you're still there and he'll when you leave so after you leave turn everything absolutely right so the combination of appliance sensing and pressure sensing and looking for that particular combination yeah definitely be an important one other ideas sensors don't actually salutely right so there a lot of companies that are looking at exactly that problem right detecting falling because you can't assume that after a fall the the subject can communicate that right absolutely so again we started this course thinking about our putting people first right so it depends what are the particular challenges that a particular elderly person is dealing with if it's memory that might be an important solution if it's physical disability detecting Falls might be important what are the other aspects for this particular demographic that could be ameliorated with technological solutions automated cars yes that that's a good one it's not just going to help the elderly okay okay it's coming what else specific to this demographic right so that's okay so the social dimensions of this right so we're there other voices detected in the home on this day other than the voices that are coming out of a TV or a computer right socialization incredibly important people absolutely really important yep how many hours per day are they sleeping how much time are they spending in bed right this is another important one which also impinges now on acceptability right so miss X might allow sensors in her carpets or sensors on her appliances but sensors in the mattress I don't know that's pretty intrusive right so obviously there is going to be a very sharp antagonism here between privacy and supports right this is incredibly important how much Big Brother might we be allowed to instrument in an elderly person's home right how much will they accept it might work perfectly right best thing is slap a GPS transponder on my father and I know exactly what he's doing at all times probably not very acceptable I've tried I know it's not okay again a pretty important problem in this country you all are to be working when the United States crosses this magic threshold so this is something that's maybe worth thinking about okay so again ubiquitous computing that was a pretty short lecture main message here is again we want to try and instrument the world as best we can but we want to make sure at the same time that it's doing good work for us it's providing the scaffold and it's invisible right it's out there in the backgrounds and it's acceptable it's not recording things that we would prefer that it does not record okay it's a point of the ethics of this particular kind of technology I wanted to start with social network inference so this is an actually the reading for this for lecture 17 I've just assigned it as optional reading this is the actual research paper itself okay okay so here's the idea as you're gonna see in a moment we're gonna instrument some graduate students with some technology because we want to try and answer a specific hypothesis which is can we use ubiquitous computing to extract face-to-face information so not trawling your social network feed but extracting face-to-face information from social groups so when you're actually talking amongst yourselves can we detect it and can we use that information to infer how or whether you change your behavior in different social groups something that is exceedingly difficult to do without technology that is being worn by the people in the study okay so in this particular research paper the professor managed to convince 24 of her graduate students to wear this backpack and it had a sensor pack right up here on the left shoulder and in the small backpack there was some other electronic equipment to record and store the sensor data so even back in 2008 this was a little bit bulky you could probably do this just with a smartphone as long as you're wearing it somewhere near your mouth because this study is going to rely on speech so throughout this six-month period that these 24 graduate students were wearing this device the device was capturing their speech this is again a pretty controversial and pretty invasive experiment so a big stipulation here was that what the graduate students said was immediately thrown away and the only thing that was kept is the volume at which they were speaking and how they were speaking so inflection and paralinguistics how loud how quiet how high-pitched how low-pitched how rapidly were they speaking how slowly were they speaking but the actual words were scrubbed and thrown away and were never recorded okay again because from an HCI perspective thinking about this experiment the graduate students might have agreed to this experiment but if there was a third person standing nearby that was just chatting with these two students had no idea about the study their voice might also be picked up by the device that third party did not sign any of any agreement right so again thinking about technology out in the world we need to think carefully about the people that are going to be interacting with the technology there are the subjects themselves and then third party stakeholders who may have no idea that they're being recorded and included in this this study okay so step one assuming we have this speech data over this six month period from these 24 people can we take that speech that speech data and try and infer among them who are friends so we could again ask to see their Twitter feed or get them to fill out a survey but can we actually infer it directly from the data and then finally once we infer the structure of the social network can we see whether your relationship or where you exist in the social network does that influence how you behave in the group okay they didn't the subjects here didn't actually wear these devices 24 hours a day for six months that would definitely be too invasive they wore it for working hours just for one week out of the month the other three weeks they didn't wear it for this six month period as I mentioned there was this sensor board that was pretty close to their mouth there were a lot of other sensors on that board they were all turned off the only thing that was turned on is the microphone that was capturing this speech data and then they had a PDA in the background that was scrubbing out the actual words and just recording the aspect of speech and not the content and then those speech statistics were being stored on an SD card so at this point in time how fast was the person speaking what was the pitch of their voice what was the volume and so on okay let's start to look at some of this data I imagine from the I apologize from the back of the room you probably can't read these axes so each of these five panels corresponds to Monday Tuesday through Friday it goes from 9:00 a.m. on the left side of each panel to 8:00 p.m. on the right side of the panel and it's a histogram reporting the total number of persons seconds speaking or PSS captured by one of the 24 grad students during that period so a person's second speaking is so as an example here's just the total amount so if there was one person among the 24 people that was speaking for four seconds that's four PSS if among the 24 devices the 24 devices reported that two people were talking for two seconds that's also four pß so the total height of the bar is basically the total amount of talking going on among that group of 24 people it does not report who was talking to whom what they were saying what the emotional content of their voice was it's just total amount of talking time and already just with that raw data you can all you start to see some pattern in this data what is it you'll notice they were relatively tall blocks Tuesday and Thursday morning why class schedule right and it was probably it was probably a seminar rather than a lecture why I don't know but that would be my guess lots of different people are talking right so if you were all were wearing those devices I tend to talk at a pretty high volume so that device might pick up the fact that someone is talking but the device the microphones were calibrated so that if somebody spoke at a regular volume the device would know that it's not the person wearing the device that's talking it's somebody else that's talking and if none of the 24 devices report that the ego meaning the person wearing one of those 24 was talking it's not counted here so a PSS means that it was one of the 24 grad students that was talking right someone there was a mouth that was about this close to one of the microphones so your devices would be picking up my voice but at a lower volume because you're further from me and it was calibrated to make sure that that wouldn't count as a PSS so those bumps at Tuesday and Thursday morning means at least either one of the grad students was doing all the talking or a bunch of the grad students were doing quite a bit of talking Tuesday and Thursdays mornings what else can you tell from this yeah I may be kind of hard to see it in there right the fact that you can't see it kind of tells you something already if you saw spike around noon what would that mean they're eating lunch together right so there is activity there's hype the bars have height around lunchtime but not really much higher than anywhere so maybe they did have lunch together but it wasn't a significant fraction of them compared to Tuesday and Thursday morning where there are a lot of them talking in about the same time period so you can already see from this raw data how we can start to infer something about the collective behavior of this group of 24 individuals okay let's keep going that's the raw data right we can't really say much about the social network except that they were in class together Tuesday and Thursday morning so let's try and drill down into this data a little bit further and see if we can actually start to infer who is talking to whom in this okay so here's a here's a cartoon example let's imagine that class has just ended and there are three grad students who stayed behind in a classroom and they were talking amongst themselves they were close to one another those three in the classroom outside in the hall there were two more grad students who were talking amongst themselves let's imagine that that's the case let's imagine that the conversation in the classroom starts the classroom conversation outside the classroom has not yet started as the conversation in the classroom continues the conversation outside starts there are two different conversations they just happen to overlap in time so far so good okay here's my little cartoon of what that might have actually looked like from the point of view of the five microphones being worn by those five grad students so here's the three speakers in the classroom speaker one says something and then stops talking speaker two responds to what speaker one said and speaker three then says something as well during that this period here outside in the hall speaker four said something - speaker five and speaker five said something - speaker four in response the vertical axis here is volume so obviously when speaker one is speaking microphone one is recording high volume because the mouth that is speaking is close to the microphone why does volume drop to a medium level but not zero immediately after speaker 1 stops talking absolutely so in this little cartoon here when speaker 1 stops talking microphone 1 is picking up the speech first of all from speaker 2 and then from speaker 3 I'm assuming in this cartoon example that speaker two and three are about equidistant from speaker 1 right when speaker 5 is speaking here it doesn't register at all at this point in time on microphone 1 why so right here when speaker 5 is just finishing what speaker 5 had to say speaker microphone 1 is reporting no volume why it's below the volume threshold 4 and 5 are out in the hall maybe the doors are closed right there they're separate conversations that are happening okay so that's the conversations that's the physical context about what's going on here's the raw data that we have how do we actually go from this data to inferring this picture that at this point in time or during this short time sequence here these three people were talking and these two people were talking amongst themselves tricky in order to do so the investigators used a measure called mutual information anywhere any mathematicians here anyone taken any information theory yet mutual information you can think of this as a fancy form of correlation so mutual information means that if I measure one phenomenon that gives me information about what's happening somewhere else when mutual information between two phenomenon is high that means I can just measure this phenomenon and I can predict perfect for perfectly what's going on with phenom - if mutual information between those two phenomena is low that means when I measure this phenomenon I cannot tell you what's going on with the other phenomenon and I'll show you an example of mutual information applied to this problem let's imagine at this particular point in time I'm looking at the data coming from microphone - and I see that volume is high throughout this period here so I'm measuring phenomenon 1 which is the volume coming from microphone - and I see that whenever again only during this time period during this time period whenever volume is high for microphone - it is always medium for microphones 1 and 3 that means mutual information is high in this case during this period whenever I see that volume microphone 2 is volume is high I know I can predict that the volume for microphones 1 and 3 is media they're sort of correlated right whenever 2 is high 1 and 3 is always medium why why is that important why did I pick that particular point in time during that period there's high mutual information which means what let me give you a counter example let's say let's say I'm moving along this time window and I'm looking at the different microphones I'm moving along this time window now I'm looking at microphone four and I say AHA during this period microphone for was always high but whenever microphone for was high sometimes microphone 3 was low sometimes microphone 3 was medium and sometimes microphone 3 is high that means the mutual information between microphone 4 and 3 is low if I detect that microphone 4 is high I cannot predict what the volume of microphone three is going to be there uncorrelated or the mutual information is low why does that matter I can I can see who's interacting with whom right so if you and I are standing close to one another I talk and whenever I'm talking my volume is high and because you're close to me your volume is always medium and then when you talk your microphone is always high and mine is always medium during that time period that we're talking to one another in close physical proximity there is high mutual information between our two microphones someone else in the class is out walking around campus and their volumes are going up and down on their microphone and it's completely uncorrelated with what I'm saying here in the classroom to you so at that point in time our two microphones have low mutual information and I can infer from that that we are not engaged in a face-to-face conversation during that time period okay so we've got part of the way from raw volume to starting to infer social networks we'll continue with that on Friday you have deliverable 9 do tonight deliverable 10 is now online if you want to get started on that and you also have a quiz due tonight thanks very much 
boQbnvfhEOY,28,"All lectures: 

https://www.youtube.com/playlist?list=PLAuiGdPEdw0iLnUFP7kALZf3SbGIokPKt",2018-10-18T15:40:02Z,"Human Computer Interaction lecture 15: Tangible computing. (Filmed Oct 18, 2018)",https://i.ytimg.com/vi/boQbnvfhEOY/hqdefault.jpg,Josh Bongard,PT1H15M6S,false,83,0,0,0,0,okay let's get started let's talk a little bit about the deliverables everyone submit deliverable seven last night more or less okay for those that have you're over the hump eight nine and ten are much less onerous than than six or seven who has a system that can recognize all ten numbers pretty well okay okay so some of you are still working on deliverable six and struggling to get your system to recognize all ten digits as I mentioned on Tuesday remember that we have a large class and there's a lot of redundancy in the data set so if you find that a pair of numbers isn't working well go back find another student who recorded the same numbers and swapped their data in rinse and repeat until you've got something that can do well on all ten digits Margaret yep yep if you did it like like this rather than like this absolutely right so there's a lot of heterogeneity of the data set remember our discussion the first few weeks about pact analysis we have a lot of different people in this class and people might have recorded things differently some of you might have recorded 0 like this some of you might have recorded it like this some of you might recorded it like this and if you read in someone else's 0 and you're signing like this it may or may not recognize it right so there's a lot of differences in the data set and that is what deliverable six is all about is getting you to figure out how to deal with with all this disparate data we are not expecting that you read in every single student's data set and you use that in your system you're looking to get a set to find enough data to cover all and digits so that you are able to sign all 10 and your system recognizes it and then you take the way that you choose to sign all 10 and create visualizations to teach your users to sign in the way that you sign that enables your can and algorithm to recognize those 10 digits that's how 6 & 7 work together the easiest things change it so if it's a single digit just remove that training and test set and read it in from someone else's yeah exactly feel free to to mix and match sit again I don't quite understand the question yes so is it testing the training and test that's in the aggregate data set is exactly the same way in which you recorded your training and testing set right the first thousand frames and then the second thousand French sorry I mean I so like if the student submitted late uh student training - yes I may be in the same order or is it just randomized [Music] they're not tagged with the student name that's right so copy one copy - that's right yes so I'm sorry yes it should line up so copy two is from the same students right that's right okay sorry I didn't misunderstood because the TA put together the aggregate data set yes already that's all right no problem it may or may not matter whether you line it up in theory it shouldn't shouldn't really matter one person's train should be relatively close to another person's test but not necessarily right okay yes yes that's right so exactly so we're assuming that you're your user does not speak English but they recognize the digits 0 through 9 that's that's right so you could actually draw the actual digit so when you ask the user to sign 0 you could write 0 1 2 or so on yes that's fine yes uh-huh so do you show them also at the sign that's gonna come up in 9 and 10 so deliverables 9 and 10 are going to be focused on how do you actually go about teaching the users to do so to actually sign correctly right we haven't gone to the teaching part yet at the end of deliverable 7 you should at least have your user holding their hands centered over the device so we'll come back to that in 9 and 10 any other questions about the deliverables okay I'm just I mentioned this before I asked the TA to go back through the data sets and some people save things in slightly different format I'll check with the TA today whether he's done so if he has you can download the data set again and hopefully all of that the files and there aren't exactly the same format okay so again some of you might be having a hard time getting your can an algorithm to recognize certain digits because some people might sign like this some people might sign like that what other differences might be making things difficult on your K&N algorithm aside from the orientation of the hand hand sides yeah that might that might matter how many lefties in the class or not me here today one okay there we go thank you thank you alright so some people might have sight like this and some people made a sign like that for most of the righties in this class are probably not going to be able to recognize a lefty dataset that being said you could add some code to your system to make it robust regardless of hand regardless of orientation regardless regardless of left and right what code might you put in so that your Canon algorithm could recognize lefty data even if the majority of the data is writing data exactly right so you can take a lefty hand and turn it into a righty hand by reversing the hand and remember that the hand date is being stored in this originally in this three dimensional matrix which has the x y&z coordinates of all the end points of the bonds in your hand so how specifically would you turn a lefty frame of data into a righty frame of theta could take a lefty data and turn it into this that's not what you want okay you could take a lefty hand like this and turn it into this which is also not what you want you want to turn this into this can you just reverse the order of fingers not quite it's not going to quite work right now when I can't demonstrate for you right if you think about it's not gonna that's also not going to work you can really reflect it about which axis you reflected about the x-axis right so think about all the XY and z coordinates in the bones of my hand right now and then we want to flip it about the x axis we'll get this right so take all of the X values in a frame of data and negate them right that'll flip reflect about the x axis so you can turn lefty data into writing data and vice versa by reflecting about the x axis by negating all of the x coordinates in the frame yes how to tell if it's like that's true that's one way you could do it right so that there's another way you could do it the problem I think it probably does there's probably a flag in there there somewhere the other thing you could do is take every single frame in your training sets and double the size of that training set for every single gesture create the mirror image right so you double the amount of data so it's in effect that all of you signed with the left and with the right so there's some artificial things you can do that don't require doing complex recognition okay so dealing with size is a little bit more more tricky you have to try and normalize the size of all the hands and the data set that one's a little bit more tricky and or different orientations is also is even more tricky that's why I had you try and do this manually so better to just try and actually generate signed data at different orientations rather than sign at a given orientation and then artificially try and rotate frames of data yes sure okay so does it matter what you're actually signing and what's shown on the screen the answer is yes it does matter because leap motion is telling you this is what I see right so this is going to be important right when you try and teach your users remember your users are going to start to build up a mental model of what your system is doing they know that it's trying to teach them sign language you show them a picture of zero they do zero and the system says wrong right you've broken the most people would predict that if they see a picture of this and they sign this over the device they the user is doing everything correct but the system is telling them they're incorrect right so you've broken the expectation of the users which is a big no-no in HCI design right so we can't guarantee that your KNN learner is going to recognize the sign all the time so how can we provide some additional feedback to the users to help them over the Gulf of execution or evaluation right this is the user did something and they didn't get back the result they expected how can we help them figure out what's gone wrong or what they should do next to fix the problem right one thing is the wireframe so the wireframe is the system telling the user this is what I think your hand is doing right so when you get to nine and ten you're going to be adding additional visualizations to help the user learn sign language but also to learn your system how is it teaching you sign language when your system makes a mistake when the knn algorithm makes a mistake which it will the user has to be able to correct this right you're assuming you're not there so you need to provide something and the wireframe is an important part of that right so you might show something on the wireframe and then give some instructions some visual instructions to the user that if they do this and see this they should do this or do this and then do this or do this and then do this whatever it is you need to provide a cue back to the user about how to fix the problem okay other questions yes sir back sure local seven we are showing this we are pretty much teaching the user like you don't showing sign that they need to do correct that's right yeah okay so while we're on the topic again showing images to the user as I mentioned last time matplotlib wasn't really set up to serve as a real-time GUI you could get it to do it it's not ideal it's up to you whether you want to switch to pygame or not if you stick with matplotlib the easiest thing to do is to break your figure down into multiple subplots and then you can write you can write or draw different things to the different plots so in deliverable 7 I showed you a picture of creating a 3d plot in one sub panel and a 2d plot in another sub panel you can continue on with this and at this point it's up to you how you want to do it some students in the past have made a 2x2 grid so they had one two three four subplots they would show for example the wireframe in one they would use the upper-right subplot to show images sort of telling the user what to do they would use this one down here to tell the user what particular digit they're supposed to be showing and they would use this plot to say yes you're signing it correctly or no you're not that's one way to do things lots of other ways to do it what I would suggest I would suggest avoiding trying to put a wireframe and an image of the same subplot that usually doesn't work too well so you want to sort of specialize each subplot to just one kind of visualizations okay and again these other areas additional elements will come up in deliverables nine - all good okay so that's six and seven let's talk about deliverable eight hopefully this will be a little bit of a breather for most of you this one is pretty short when we have four pages fourteen steps not not two not two owner us we're going to be adding some additional state to your program so you have some state to your program now which is whatever your code is running is there a hand over the device if there is is it centered and so on right we're adding eternal state but the moment you kill your program that state disappears in order to create your educational software you're going to have to add some state to your program that's persistent that state exists from one session to the next because for example you might have multiple users using your set your device or using your system they log in they practice sign language for a little while they shut it down they come back the next day and they start up they don't want to start from scratch they want to start from where they left off so in order to add that persistent state you're in this week you're going to be adding a database to your system this is not a database class we only have one week to add a database so we're going to add the cheapest possible database you can add a Python which is to simulate a database as a dictionary of dictionaries how many people have worked with dictionaries of dictionaries some but not all okay so here's the idea at the beginning of deliverable aids at the top there you're going to be creating a single variable called database and as you can see you're going to start by creating an empty dictionary and inside that store and then once you create that empty database you're going to dump it to a file using pickle and then when you start up your program again you're now going to load in the database from a file which at step three is going to be nothing more than an empty dictionary there nothing in your database yet okay when somebody logs in you're gonna ask them to type in their name and when they do you're going to add a new entry to a database and in a Python I'm sorry you're gonna add a new entry to your dictionary and a Python dictionary you have a whole bunch of entries and each entry is made up a key and a value the key is going to be the users name so I'm going to add an entry now to my dictionary and that key is going to be a string which is my name and the value associated with that key at the moment is another empty dictionary so now we have a dictionary with a dictionary inside it if somebody else logs in for the system we we add a new entry to the database for that user and the top level of the database is now going to be a number of entries which contains all information about that particular user so far so good okay and at this point we let's see let's keep going here so we can we can associate lots of different information so for example how many times has Josh logged in so we create a new element inside of this empty dictionary and this this key this element has this key and it has its value an integer so remember that Python dictionaries can have any kind of data structure for them more or less any data structure for a key in this case our keys are strings and we can associate with that key any sort of value we want in one case that value is a dictionary for this particular element here the key is a string and the value is integer right so again not the most elegant way to create a database but it should be good enough good enough for our purposes okay so josh has logged in three times and within the Josh elements here we might want to create a new element which is made up of a key of integers such as zero through nine and the value over here is going to be a dictionary and this dictionary is going to contain all the information about what I did when I was challenged to sign the digit zero right so perhaps how many times was I shown the digit zero and asked to sign that digit same thing for number number nine down here one of the nice things about using integers for the keys at this point is we could create a for loop and we could iterate through all of these digits and we could do something like in deliverable nine and ten when Josh logs in we could ask the question which of the ten digits has Josh Josh been shown the least number of times pick that digit and show it to Josh so somewhere in these dictionaries you would include include yet another dictionary which is number of attempts number of signs correct number of signs incorrect and so on so in deliverable eight we're sort of showing you how to create the top level structure for this dictionary towards the end of deliverable eight and then throughout nine ten and the final project it's up to you to structure the dictionary however you want we don't care whatever information you want to track about the user you figure out what that information is and how to encode it in this database so I'm good pretty straightforward okay okay so let's get back to lecture material we are working our way through this theme on looking outward and as I mentioned last time our society's of the process of sort of stitching computation and in internet enabled devices into the real world the old days we had desktops and a few laptops and they kind of talk to get to talk to each other but they were sort of separate from the real world right now as we start to stitch more and more computation and devices into the world there are more and more of them they talk to each other in more in different ways and as we'll see when we get to lecture 16 today they are they're also kind of disappearing into the fabric of the world how many cell phones are in this room right now I can't see them you don't know right but probably one one per person right and your phone may be on and it may be set to vibrate so throughout this lecture it may actually be providing you with information but thanks to all of you I haven't heard any of your cell phones yet right so you are communicating with your devices your devices are communicating with each other but it is somehow sort of disappearing into the fabric of our world it's not that obvious anymore that's that's important we will talk about that in lecture 16 last time in Mexico 13 we were thinking about throbbing this single human single computer interaction to a single computer maybe multiple computers that are communicating with a large number of people so last time we looked at three or four applications where the problem that we wanted to solve was impossible for one person to solve impossible for one computer to solve and also impossible for a group of people alone to solve and a group of machines together to solve so we looked in interesting ways of stitching together many different people and many different machines so that that hybrid team can hopefully solve the problem that would be impossible without that particular kind of team so we were robbing or looking outwards in terms of sort of socializing our HCI right how do we get lots of people to work together with each other and also with machines to solve a problem that would be beyond a smaller group lecture 15 which will start a moment here we are also going to broaden our consideration of human-computer interaction but we're going to broaden it not across lots of different people but across lots of different sensor mode or our senses as I mentioned the beginning of the course and as you all know humans are very visual creatures most of our interactions with our machines are visual right most of the time you're looking at a screen and tapping a screen can we broaden the senses that you bring to bear when you interact with the machine how would we do that and why would we bother doing so that's what we're going to look at in lecture 15 okay so we're gonna broaden computer interaction now we spent some time talking about vision we can also talk about sound we're going to focus in the next few applications on touch which is sort of the most underappreciated of all of our five or six senses depending on how you count we're going to distinguish at the outset here between touch and haptic perception whenever we're talking about vision or sound or touch we can think about the raw sensation itself which is a passive process right as long as your eyes are open there are photons that are falling on your retina you can't do much about that unless you close your eyes right but visual perception is an active process where you remember a couple weeks ago we were looking at all those optical illusions your brain takes that raw that raw sensation that raw visual sensation and actively tries to add meaning to it and make sense of it right in the same way you can think about touch the minute your skin comes in contact with something you may actively cause it to happen but you're registering raw sensation raw haptic sensation and your brain like with with visual input is trying to extract meaning from it right so tactile devices which you're gonna look at in a moment are relying on this haptic perception how might a machine push that back against your skin and help you build up an understanding of what it's trying to tell you or enrich the interaction with the machine in an interesting way we have at the moment lots of input devices that rely on touch or at least muscle power right you tap your keys you move your mouse you touch your touchscreen but there are relatively few output devices that touch back right that actually give you tactile information there's some interesting HCI research projects out there like the MIT Interactive pin system here has anybody seen this moving pin displayed before it's not a display I guess okay let's talk for a moment I'll show the rest of this video in a moment what's happening here can I pause it and get rid of this yes what is happening here the pins are moving and following the shape of his hands how so we have remember this cartoon right we have a human and a machine which we have here the human is doing something which becomes input to the machine the machine outputs something which becomes input to the person what is the input and output of the human in this case and what is the input and output of the machine right so not so far from the leap motion device the output from the human is movement of the hands which becomes input to the machine which using some device is being translated into XY and z coordinates of the hand that's the input to the machine what is the output of the machine what is it doing with that XYZ it's moving the pins right so it's looking at X Y Z and figuring out there's a whole bunch of actuators or motors under the table that are pulling and pushing these pins using that information so that's the output of the machine what is the input back to the human what does the human feel or see as a result of this interaction we got one more step to close the loop they're looking through the laptop at at the screen so they cannot feel the ball right they feel their hands moving because they're actually moving their hands and they see the ball moving but they can't feel the surface of the ball on their palms right so there's no actual haptic feedback to the human however when this happens it feels as if you're touching the ball which is kind of strange right you're seeing it happen why is that the case if you ever get an opportunity to try this it's kind of interesting right so remember that the brain is a prediction machine right so imagine you're using this device you're moving your hands and your brain is predicting you're gonna feel certain strains on your muscle which is on your muscles which is the weight of your hands as you move them that all makes sense and you the muscles in your arms tell you that your hands are in front of you and that your eyes are open so your brain predicts you're going to see your hands which you don't what you see is something that looks pretty much much like your hands not quite so that's supporting a big part of the prediction your Britain your eyes also tell your brain all right I see your hands more or less and I also see that our hands are holding a ball however they the touch information the touch sense in your skin is not recording the ball your brain says pretty much everything is there 90 percent of the prediction is holding up there's a missing 10% and your brain fills in the rest we remember our discussion about cognitive psychology the brain does that all the time if you write an essay and you read it just before submitting it you don't see any spelling and grammatical errors you submitted the instructor marks and it comes back covered in red ink and you think to yourself where were all these spelling and grammatical errors just before I submitted them right your brain filled in for you where it wasn't perfectly spelled but your brain knew what you meant and on it went same same idea here so they're using not a leap motion device but it connects camera so more or less the same thing basically just cameras that are translating 3d it works with not just hands but other kinds of objects waiting for him to turn it on right what do the colors represent good question so sometimes there's color and sometimes there isn't it depends on the application there's a tactile metaphor I haven't seen one of those yet what are the colors mean here move towards me right put the ball on green it's a game exactly the arrow right go forward go back so now there is actual tactile feedback right the interaction is a little bit different Matt plug live in real life teaching math what does the quadratic function feel like literally there's the actual system okay with most with most HCI applications the other ones we're going to see today it's look what we can do what's the killer app here again who knows what might be the killer app here so there's actually a group of faculty here who are developing moving pin displays for exactly that right so trying to translate what anything you might be able to see on a computer screen into a moving pin display right so teaching the other side of it surveying serving an underserved community especially as bifidus technology starts to take off this is a big issue now in society as a whole right we want to make sure that people aren't left behind there you go okay all right all right talk to me afterwards we'll get this we've got the intellectual property lawyers on this one okay so there's an output device which literally pushes back so as we move forward in this section we're gonna see more and more devices which look less and less like a traditional computer and are able to interact with us on our different sensory modalities but they can also the devices themselves can push against the world and observe how the world wishes back what other output devices could you imagine besides moving pin displays what other existing output devices exist that give you information through the skin that present information to you to touch rather than vision or sound right so sometimes it's pretty simple just a binary signal right you've got a text message pretty simple but but pretty effective most of the time you're visually or auditorially attending to something like this lecture so the nice thing about vibration is a concerted provide you information in the background remember when we talked about attentions the sort of idea like computation you can allocate more or less attention to something hopefully you're allocating most of your attention to the lecturer but you might have a little bit that's waiting for a text message and registering that you got one and then maybe it'll check it after class okay let's have a look at another one this is the ultra rapid optics display this one is kind of interesting as you'll see in a moment this one is also going to apply it's going to supply tactile information to the user but is also not going to apply supply tactile information to the user okay this one requires sounds so just bear with me here directive services are now common in everyday life interactive services are now common in everyday life they allow users to walk up and use them with no instruction however current methods for providing tactile feedback require the user to cover up the visual content by touching the display or attached devices to their Alice we present ultra haptics a system that provides midair haptic feedback and requires no contact with either tools attachments or the surface itself ultra haptics uses a phased array of ultrasound transducers to create tactile focal points in midair the array is driven by a stack of five driver boards which receive emission patterns from a PC the user's hands are tracked by a leap motion controller and the haptic feedback is projected through an acoustically transparent display directly onto the user's bare hands there are four steps to our unique focusing method first we define a large volume around the transducer array within which we will model the ultrasound field then we position positive control points where we want to form focal points these tell the system to generate the highest intensity ultrasound possible at these locations they are then surrounded with null control points these have the opposite effect telling the system to generate the lowest intensity ultrasound at these locations finally the phase delay and amplitude are calculated for each transducer in the array to create an acoustic field that matches the control point this simulation illustrates the acoustic field as we move up from the transducer array color represents phase and brightness represents intensity at a height of 20 centimeters a focal point is formed above the ultrasound D focuses once more similarly this simulation shows five discreet focal points being formed at the same time by varying the tactile properties of focal points such as the frequency they can be made to feel different from each other in this scenario a tactile information layer is created above the display by moving their hand over the map the user can feel the population density of a city the frequency of a focal point represents the density in that area here focal points are created above elements of a music player interface this allows a user to locate themselves on the interface without looking tapping the focal point above the button starts and stops the music the focal point above the volume slider can be grabbed at this point it pulses to inform the user that the system has recognized their grasp the focal point can then be slid up and down to change the volume these are just a few of the applications that become possible with the ultra haptic system okay what's happening here they grab the volume knob in midair and of course there is no volume knob there but if you ask the user it feels as if there is a small sphere floating in mid-air that they grasped and when they grasped it they felt it vibrate slightly which was the volume knob in midair telling the user I've got you I know you're holding me and I'm ready to slide back and forth how is this possible so they are they're basically vibrating the air at a very high frequency right so your cell phone in your pocket is vibrating and you feel it if you if there is a vibration in the air very close to your skin you will feel it it's ultra sound it's very high frequency so it doesn't actually feel like a vibration the vibration the frequency is so high that it doesn't feel like something that's shaking anymore eventually it just feels like something that's solid okay you mentioned they focus somehow the ultra sound how does that work well they mentioned they have a transducer here so transduction means translating one physical phenomenon into another so they're taking electrical signals from a PC so just code is being sent to this transducer which tells a five-by-five set of these transducers how much ultrasound to emit at what frequency and at what amplitude so they mentioned this is a phased array meaning that these 25 transducers are often emitting ultrasound with a bit of a phase offset so it's not that they're sort of affecting a beam of ultrasound they're changing the phase offset why does that matter imagine imagine you had a an outdoor kiddie pool and a bunch of you stood around the edge and held edges of the kiddie pool and you could shake the edge of the pool up and down if you all shook it in the same way there'd be one big wave that would move from all of you towards the other side of the kiddie pool but if you do this for a while and you can figure out how to shake the kiddie pool at different frequencies so nearby there's relatives relatively calm water but you caused a wave to appear at the far end of the pool because you're influencing positive and negative interference between waves of water or in this case sound waves of ultrasound so you'll have probably seen this at the beach where you have two waves that come together if they hit at just the right angle in just the right timing there's positive interference and the result there's a resulting wave that is twice the height of the original two waves right same thing here you have these emitters that are sending out signals at different phased array has different phases as they move upward above the device they collide and complex patterns and produce more or less ultrasound in the same way you've probably seen two waves that come in and they're offset by a little bit they sort of cancel each other out the height of the wave and the trough of the other way meet and they cancel each other out that's negative interference so by setting the phase offsets carefully you can create positive and negative interference at different points in three-dimensional space the metaphor of the waves on the beach is still sort of in two dimensions or two and a half if you want this is being done in three dimensions so they mentioned putting these positive control points so they're setting the phase offset of these transducers so that they meet and produce more vibration at that at those green crosses but they're also setting the phase offset in such a way that just around the point they're also cancelling out some of the vibration otherwise they don't cancel it out if you put your hand in this device you sort of feel this mild vibration everywhere right so they're sort of taking away vibration around the three-dimensional point where they want you to feel a small object makes sense how do they know where you're grasping in three-dimensional space good leap motion right so the input to this device is the 3d coordinates of the bones and the users hand is here depending on what the display or what information you're trying to give to the user you set the output to this machine is frequency and phase offset of these 25 ultrasonic transducers that's the output of the machine which becomes input to the human which in this case is tactile input you feel that your hand is touching something okay kind of interesting again I'm not sure what the killer app is here but about a very interesting concept in a very exotic form of interaction absolutely right why would you actually need to feel something in three-dimensional space I don't know right are there are there applications you could think of where it might be more interesting to manipulate things in three-dimensional space and feel that feedback compared to just seeing it on a screen and touching a two-dimensional surface ideas where might that be useful possibly all right so maybe we project visually in 3d as well so maybe you do see the object in three-dimensional space and you feel that it's a solid object absolutely right so you're literally feeling it right or not really we don't necessarily have to rely on the visual feedback we're going to be useful to feel something rather than see it I know I saw similar this would be useful just because there is one it's fairly touchscreens now a lot of cars but that's true okay possibly so again you can keep your eyes on the road maybe there's an application some some adaptation of this application where you're being provided with tactile information but you're able to keep your eyes on their road imagine you're in a partially autonomous car and there's some vibration coming through the steering wheel or maybe your hands aren't on the steering wheel where the vibration or the form of the vibration is real-time feedback from the car about certainty one of the most important things about an autonomous car might be to be able to tell you unobtrusively I'm not quite sure what's going on I'd prefer if you grabbed the wheel at this point it gave me a little bit of feedback and it would be nice would be able to tell you that without grabbing your visual attention and exactly the point where the autonomous car is unsure about what's what's happening right it may be a very small period of time in which tactile information may be preferable to visual information but it could make all the difference again who knows what the killer app is but thinking carefully about the person and the task which is keep your eyes on the road as much as possible things like this might might be useful some day yeah but but some day okay so again why haptics over vision well one of the nice things about manipulating a physical object or a quote-unquote physical object you may not literally feel a physical object but it might seem that way to your brain is that we often bring all of our sensory modalities to bear on the problem so touch is one of the underappreciated sense organs that we have because often when you grasp an object you're also looking at it you might see it deform or you might see the result of your manipulation of that object maybe not in the ultra haptics display because there's an object that's invisible but maybe with Holograms or something else we can sort of not just project to touch but projects to our visual auditory and tactile sense circuits what else what else is brought to bear when you grasp an object other than vision you feel the object obviously you see yourself grasping the object what else might happen as you physically manipulate an object some of you are writing with a pen right now you're manipulating an object what's what's happening the information that's coming back is not just touched auditory maybe you hear the scratching of the pen right if you grasp an object and you stroke it across another object you learn a lot about the material prop properties of both objects because you hear it and you're generating it as you as you do so absolutely absolutely so imagine you had access to the ultra haptics display and you wanted to teach somebody calligraphy or you wanted to teach someone how to use a paintbrush or a pen right so now you're not manipulating a button here you're manipulating an oblong cylinder and you need to enforce on how is the tip of that object coming into contact with the surface right you know that with a pen because the pen pushes back you can see that you're coming into contact with the object but all of those sort of subtle interactions are going on all the time remember when we talked about we talked about cognitive psychology and attention and prediction your brain is making obvious predictions like I see the tip of the pin come into contact with the paper but there are also more subtle things going on like I expect the pen to push back when I touch a pen to paper right you need to capture all those things in a haptic device as well okay again here's some of mine you can obviously through your own intuition come up with a lot of these you learn a lot about an object when you grasp it there's a very good reason why young children will grab any object nearby their visual senses usually not very good at least in the first year of life a lot of what they see is still kind of blurry if your vision is blurry and you don't know much about the world best thing is to try and grab brightly colored objects and put them in your mouth why put them in your mouth because your mouth is a great tactile sensor if you want to learn about an object put it in your mouth and bite on it and you'll learn quite a bit about that object is it malleable is it what's the temperature is someone going to yell if I bite on this object they're learning the difference between South and inanimate objects and animate objects that matters how it is an object move across other objects what what do we want to learn about interactions between objects by manipulating how does it resist our grasping of it is it heavy is it soft is it light does it separate from the other objects that I see is it attached to those objects is it separable it's often if you don't know much about the world around you and you see objects you don't necessarily know a lot about their properties vision is great because it gives you a lot of raw information but unless you know a fair bit about the physics in the world you can't really infer stuff about the physical properties about an object unless you manipulate it if you want to Blair on relief motion device and try and teach people how to manipulate objects in three dimensional space if you've ever used a CAD program for example can be very difficult as you're creating an object in 3d CAD to rotate it to a desired angle you sort of have to click on something and drag along the x-axis and then click on the y-axis and drag along the y-axis it's complicated non-intuitive takes a lot of practice to get good at it wouldn't it be great if you could just reach in grab the object and rotate it to the 3d orientation you want right it's very intuitive to us hopefully some of you are still more comfortable with sketching or writing with a pen that you are tapping to a screen maybe that's not so true anymore but for most of us I think if you want to draw something it's still useful to grab a pen or a brush or a stylus and make a mark on a surface so if we can again leverage all of those intuitions that we have we can do we might be able to build some interesting tangible and tactile tactile devices okay here's another one I think in the interest of time I'll just play the beginning of this video illuminating clay allows users to simultaneously interact with both physical and computational representations of the landscape here we see two collaborators preparing a landscape model to be analyzed with the system a vivid 900 Minolta laser scanner allows the deformity of the model to be captured at a rate of 1 Hertz a Mitsubishi LCD projector casts the results of the landscape analysis back onto the surfaces of the model the work table comprises of a smooth white surface and a rotating platform onto which a landscape model is placed we experimented with different types of landscape modeling materials plasticine with the ductile fibrous core allows the model to maintain the required to fog rafi the area around the platform is illuminated with the library of analysis functions that can be selected at will the remaining edges of the work surface are used to project cross section okay what's the interaction loop here there's all obviously an important tactile component here the users feel the manipulation of the clay as they're manipulating it what is the input and output to the collaborated machine in this case in the previous example we had leap motion which was capturing the 3d position of the hands in this case there's no leap motion device what is the input to the computer in this case exactly right so now we have a third object a physical object which is providing the inputs of the Machine and the input that's going to the Machine is indirectly being dictated by the human who manipulates the topography of the clay the output of the device is visual in this case it's projecting some visualization back onto the surface what's the killer up here if there is one okay okay hey so GIS information system Jia Jia graphical information systems so we might want to rapidly try and model a particular landscape you could use this for urban planning what happens if we put a park here what happens we manipulate the landscape in this way and as you are thinking that through you're physically creating the model and in real time the system is telling you the repercussions of those actions so the moment that you create Hills in this landscape if you watch the rest of this video you see that he will feel blue into the valleys right so obviously the water is going to pool in in the valley so you can immediately see what that would look like again you could do it with CAD modeling software but it's much less intuitive right it would be great to manipulate something and see our ideas literally taking shape and in addition what you can do just by playing with clay right but in addition with HCI you can see in real time the repercussions of that remember the brain is a prediction machine if I do this what will happen if I do that what will happen here's a way to help support that cognitive process for GIS or urban planning or architecture and where you go okay the last one we're going to look at the last example we will look at here is the actuated workbench in this case the device itself is going to move an object so now we're getting more and more into bite devices which are pushing against the world of observing how the world pushes back in interesting ways the first example we saw this morning there is also an object that was being moved around which was that red ball but ultimately it was the human that was doing the moving they were doing it very indirectly right but here the machine itself is going to move objects in the world the actuated workbench consists of an 8x8 array of electromagnets it uses magnetic attraction and repulsion to move magnetic objects to dimensionally on a flat surface here we can see a magnet moving in stepwise manehattan motion here we see an object moving in a smooth circular pattern although the array of electromagnets is fixed the system can create smooth motion by varying the strength of the electromagnetic fields in addition to magnets the electromagnetic array can move any small ferromagnetic object such as a paper clip here the user controls the pucks motion with a trackball smaller objects can be moved much faster though their motion is not always so smooth magnets of different sizes and shapes behave differently in the system's magnetic fields this stack of small magnets comes around-- whimsically we use computer vision as a preliminary object tracking technology here the user records a path by moving the puck on the surface and the system then replace that path for magnetic actuation the blue projection around the puck is a graphical visualization of the strengths of each adjacent magnetic field here is an example application intended to teach users about physics the red projected area on the surface represents the zone of attractive force while the blue area represents repulsive force the user can feel these forces by lightly holding the puck in different areas of the board when the user releases the puck it flies to the red zone of the board to which it is attracted magnetic drawing toys are effective for visualizing the actuated workbenches magnetic fields a Magna doodle allows us to see the fields used to trace the smooth circular path shown at the beginning of this video the dapper dan toy lets us see the magnetic activity in the movements of iron filings on the surface the actuated workbench can be used to control the planchette in a Ouija board like other robust systems such as the diamond touch presented by Mitsubishi Electric research labs in Wis 2001 the actuated workbench works even when set on fire very important whatever the killer app is I'm not sure here but apparently it's gonna be used in a very flammable in government's interest okay so again who knows what the killer app would be here on the actuated workbench shares something in common with the ultra optics device which is we have five by five here but we have an array of transducers again so transduction takes one physical phenomenon and translates it into another in this case we have electromag electromagnetic transducers which take as input electricity or commands from the computer and translate them into local magnetic fields in the ultra haptics device you're taking electricity or commands from the computer and turning them into pressure waves ultrasound okay so now we have a device that's actually moving an object remember that we now have a computer that's interacting with the person and often we're trying to in HCI draw on people's intuitions about a person interacting with the inanimate world or somebody interacting with someone else right a social process so an important part of social interaction is teaching right if you've ever when we were young and you learned how to when you learned how to write the teacher would demonstrate it and you would visually see the teacher writing something from a distance but you couldn't feel what the teacher felt as she was right or for you you had to infer from your visual system what she was feeling translate that into actions and predict in your grain whether that felt your felt experiences of the act of writing match whatever she felt when she was writing right that's a pretty big cognitive leap which makes things difficult to learn how to write to learn a musical instrument to learn a sport you're learning something that is very motoric has to do with our motors or a muscle system the best way to learn would be to actually feel what it feels like to perform that action by the demonstrator the instructor probably would learn that much faster that's what the actuated workbench helps with how does it support that particular interaction there's still only one person here at least one person at a time using the workbench exactly so partly through this video you saw this imitation right somebody did something performed in action they moved the device the computer was clearly sensing that movement the computer then reverse-engineered from the movement of that object had to reverse-engineer what are the magnetic fields or how should the magnetic fields change over time to reproduce the same actions so there's some machine learning there right once it figures out how to do that the machine can play back an action performed by a demonstrator and if you hold if you hold that device as the student if you hold it lightly the the object will pull your hand through that action and you will feel more or less what the instructor felt when she performed that action that is an interaction that it was impossible before something like this device existed right so one of the things that we're trying to do in HCI is create technology that supports social interaction which in this case is someone teaching someone how to do something else trying to support as social interaction that was difficult or impossible to do before right the impossibility here is it's impossible to literally feel what somebody else feels when they're performing an action this might make it possible and it resists being set on fire very important okay we got a few minutes left so let's switch now to lecture sixteen and ubiquitous computing this is a little cartoon I showed at the beginning of the course back in the Stone Age we had just humans and only a few of those humans had computers and a very few had cell phones today most people in the world have one or both of these and there are many more cell phones now in the world than there are computers an interesting demonstration of HCI there's something about holding a small computer in your hand that is preferable for some people over others for cultural financial physical reasons we're now starting to see the deployment of embedded devices so these are devices which themselves are stationary but talk to other devices that are in their range of view so Wi-Fi Wi-Fi tower so we have an embedded device now which itself can't move but it can communicate with other objects and we're gonna end a session on looking outward by looking at another class of HCI devices otherwise known as robots which them set can move themselves so the actuated workbench that we just looked at is almost a robot it can move things in the physical world but it cannot move itself the actuated workbench can move an object and sense the repercussions of the movement of that object a robot can move itself and feel the repercussions of that action we'll get to robots in a few weeks time okay so how do we go about doing this well maybe we're just computing as an idea has been around for quite a while the term itself was coined back in the 1980s and there's sort of two founding people in the field that we've been critics computing Ken Sakamura and Mark Weiser here in the u.s. most people worked on computers especially here mark who actually coined the title ubiquitous computing he came up with this at then famous Xerox Palo Alto Research Center Park a lot of innovations in HCI in computer science came out of Park Ken Summit Sakamura at least in the u.s. is not known as well but he has been immensely more successful in the sense that he realized way back in the 1980s as personal computers were just being invented that even if personal computers happen to be successful and a lot of people bought a personal computer they probably wouldn't buy to and they definitely probably wouldn't buy five however after the PC revolution there might be other devices other than PCs that have specialized uses and there's no limit in how many of those specialized devices people might have this is back in the 1980s this was pretty prescient at the time think about all the devices you own right it's probably more than just your laptop these days if you own a car there's another one right there's a lot of electronics there's a lot of HCI in cars these days so the number of devices that we have is much larger than your laptop so with that around that time Ken started to develop Tron the other Tron the real-time operating system nucleus and this is an operating system that was developed specifically for devices specialized devices and it's a real-time operating system so in a way your leap motion device or the leap Python package is close to this idea in real time that you can grab the frame of data at anytime from the device if you don't grab the frame of data from the device and someone waves their hand over it that data is gone you missed it right that's very different from sort of traditional programming we're assuming there's an external database the data is kind of static you can go and collect that data at your at your leisure time is not an issue right in real time to buy Time Matters right there's continuous information that is arriving at the sense of the sensors of the device and you either grab it or you don't it's very much changes your way of thinking about coding and you've probably got a taste for that in the first few deliverables of this course tron again this is a dated slide I think this slides about five years old so five years ago Tron was being shipped in two to three billion devices annually at that time and at that time there are about 200 million Windows machines being shipped so two orders of magnitude more instances of the Tron operating system compared to the Windows operating system how many people have heard of the Windows operating system how many people heard of the Tron operating system interesting right so if anybody asks you what's the most popular operating system out there now you know the answer okay so ubiquitous computing this matters because it scales right and it's scaling faster than then laptop and desktop and even phone apps and and technology as we start to deploy more and more devices we realize that there are niches for more and more specialized devices there's a bit of a feedback loop going on there how do we go about deploying these devices out there what's the right way to do it remember our discussion about design design philosophy there's different ways we could stitch computation into the physical world some may be better than others mark in marks original paper on this you said our computers or our embedded devices or devices we deploy should be like our childhood an invisible foundation right I can't see your cell phones but I know they're here they're in the background they're helping you but they're not they're not monopolizing your attention of an invisible foundation that's quickly forgotten you might forget that the vibration mode is set on your phone until it vibrates but it's always with us and effort ly fused throughout our lives like our childhood somewhat tongue-in-cheek in this paper he referenced Phillip K dicks novel Ubik which sort of looks at the dark side of ubiquitous technology what else is philip k dick written that you might know of do Androids Dream of Electric Sheep which got turned into the Blade Runner movies the man in the high castle I think is currently on television if you don't know about philip k dick and you're a computer science student you should there you go all right okay so how do we go about doing this there was a mention of childhood already so we're going to touch on this last slide for today on another aspect of psychology we talked about cognitive psychology the way in which the adult brain works what about developmental psychology how do infants learn about the world and become functional adults children rely on the ability of their parents to scaffold their experiences so as we start to put ubiquitous technology out there in the world what is it doing for us hopefully it's scaffolding our experience it's teaching us things it's helping us do things but it's doing it in a gradual manner so there's a picture on the right most parents end up teaching their children to walk in a way somewhat like this you very rarely see it a parent say alright kids stay over there here's this is what walking looks like do it right because again that doesn't work very well first of all kids don't have very good visual systems so by scaffolding the child and better yet lifting the child and putting the child's feet on the parents feet the child can feel what the parent feels when the parent is walking right this is a tangible interactive interactive loop here the important part about scaffolding as the name implies is as the learner starts to grasp the rudiments of the task you very gradually start to release the scaffolding I can't remember if it's deliverable 9 or deliverable 10 but that's exactly what you're going to be doing it that deliverable you're going to start by making it very easy for the user to know what to do what to sign and or visual or auditory metaphors you provide to get them to sign correctly you're gonna gradually start to remove that scaffolding as the user gets better and better at the task until eventually you should be able to flash up the digit 3 and your user does this nicely centered over the device okay I think we'll leave things there you have a quiz due tonight and you have deliverable 8 to get started on see you next week 
4isQIucy2HI,28,Playlist: https://www.youtube.com/playlist?list=PLAuiGdPEdw0j6VNxfbY-FNlbAjlWIVNnO,2020-11-03T16:39:40Z,"Human Computer Interaction, Lecture 17. University of Vermont, Nov 3, 2020.",https://i.ytimg.com/vi/4isQIucy2HI/hqdefault.jpg,Josh Bongard,PT1H15M8S,false,63,1,0,0,0,okay good morning everyone uh hope you're all well uh happy uh election day uh i imagine politics will be mostly on our minds today but at least for this morning uh we'll spend some time talking about human computer interaction um so hopefully uh all of you submitted uh deliverable eight uh last night and you are now moving on to the penultimate uh deliverable deliverable uh nine so i just wanted to very briefly introduce deliverable nine to you um in deliverable nine you're going to be reconnecting your k nearest neighbors to the user interface so for the last couple of weeks your k n has been sitting on the sidelines while you've been working on the interface we are now going to actually push information through from the k n predictions to the user interface so we're going obviously your user is not going to know anything about the k n or how the k n is making predictions so instead you're going to have to create a visualization to show the user whether they're successfully signing the required digit or not and the way we know whether they're signing the correct digit or not is based on the k n predictions so um to do so by the time you get to step 32 you're going to be creating a visualization that looks uh like this what's the visual metaphor here what's the what's the visual metaphor here how are we communicating to the user without words that they're doing well or doing poorly at what they're being asked to do any ideas red is wrong green is right so this is the hotter colder game right so are you is the user making progress towards the goal uh or not we talked about that a little bit when we talked about navigation of information spaces how does the user know whether they're heading figuratively in the right direction uh or not okay so uh i think uh deliverable nine is not uh not overly onerous um just a couple of things to keep in mind as you're working your way through deliverable nine um obviously you've had your k n inactivated for a while so you're going to be activating your k n in two phases you're going to be turning on training and you're going to be turning on testing and remember that training only needs to be done once when the user logs in for example and in testing you're performing a test for every frame that you've grabbed for leap motion and generating a promise which is the k n promising to provide a prediction for that frame of data and the function called got results is called whenever that promise is fulfilled by the k n just as a reminder of how this all works in working with some of you during office hours i noticed there was a common um there was a common error in which some of you had a for loop inside your test function which means you were making multiple you were generating multiple promises or you were asking for multiple predictions for a single iteration through the infinite loop so just make sure when you reactivate train and test that you train on all your data once and then it's done and then you're only you're only generating one promise per uh iteration through your infinite loop okay any questions about deliverable eight deliverable nine so far so good okay all right so uh just as a reminder of where we are and where uh we're going amanda did you have a question or a comment uh just a really quick um sort of administrative comment um if anyone is coming to my office hours tomorrow this is on the schedule please note that they have been shifted by an hour um they are from 12 to one tomorrow um needed to shift them for personal reasons um so just if you're planning on attending please do note that and come at the right time thank you great thanks amanda amanda if you could put a note on the microsoft teams if you haven't already that would be helpful thank you okay so um continuing on with our discussion about uh looking outward we're looking at sort of the the challenges of opportunities of threading uh interactive devices into the fabric of the real world where those technologies are interacting more directly with us and the physical world more directly than a desktop or laptop that's sitting there passively waiting for for user input we looked at crowdsourcing so the social dimension of moving out and connecting with large numbers of people we looked at physically touching and manipulating interactive systems so systems that are able to project back onto our skin tangible computing we talked about ubiquitous computing this idea of making sure that that technology as it moves out into the world becomes invisible we're not aware of it if we had a whole bunch of flashing lights and notifications everywhere we went that would not be ideal so how do you strike this balance between making ubiquitous computing invisible most of the time you can focus on what you want to do rather than the surrounding technology on the flip side however depending on the context of what the user is trying to do that ubiquitous compute that ubiquitous technology should be able to jump into the foreground and help us with whatever we're doing and we're going to talk quite a bit about context today when we discuss activity recognition which in lecture 18 which we'll probably get to today okay we are now making our way through a three-part lecture a three three-part lecture series lecture 17 18 and 19 where we're looking at particular types of ubiquitous computing that have been developed to support three different uh applications um the first and the third one are scientific applications we wanted we're trying to pose a question about uh human behavior in groups social network inference how do people behave in a social network face to face not through twitter twitter or instagram how do we thread technology into the world to observe people operating in groups and learn something about human behavior in groups that would be difficult or impossible to investigate without that technology then today we're going to talk about human activity recognition so how do we actually recognize what people are doing and then in lecture 19 we'll come back to another scientific project the human speechome project which as the name implies was it was a research project to try and understand how children acquire language or speech and how to use ubiquitous computing to understand how children acquire language in a way that would be difficult or impossible to understand how human humans acquire language without that ubiquitous technology okay all right so back to lecture 17 uh social network inference which is where we left off uh last time so this is uh ubiquitous computing for learning about people operating in groups we're starting with a big question which is first of all can ubiquitous computing act like a telescope or a microscope a new kind of scientific instrument and in this case we're going to try and design an instrument that extracts face-to-face information from social group not texting at a distance and learn among those face-to-face interactions how do people behave towards one another so we started by looking at this admittedly pretty invasive technology 24 graduate students agree at the university of washington agreed to wear this small backpack that has a sensor pack very close to their mouth because it is going to capture it's going to capture speech the volume of their speech and how they speak but not what they say right there's privacy issues here first thing we're going to do is from that raw speech data we're going to try and learn a social network who among the network our friends where friends are going to be defined as who tends to talk to whom more often among this network of 24 people we are then going to ask the following scientific question is there a relationship between where you are in the social network who you're connected to and how you behave so do people that are connected differently into this network behave differently okay a little bit more about the sensors first again we have 24 human subjects they're the same class in the same class this is important context information the data was only collected during uh working hours for one week per month for six months so they weren't wearing this all day uh every day so we're taking a subset of uh behavioral data here um this uh this study is from 2008 so it's a little dated now but so they basically had to wear this multi-sensor board which was a little bit bulky in there it had more or less what you now have in your smartphone a microphone a tri-axial accelerometer so tri or three axial or axes so it could give information about up down left right forward and back which is not used in the study a bunch of other sensors which again were not necessarily used here they're using only the microphone the microphone is connected to a pda which at the point of capture the moment that the grad student speaks or the moment that someone speaks nearby to the graduate student wearing the sensor board everything except porosity the way that the nature of the speak speech is kept the content of the speech is scrubbed out and and removed so the sd card is only is only recording is only recording speech statistics so joseph asks what does how they speak mean what what is prosody so there's uh porosity is a is a catch-all term for lots of different aspects of speech different people have different uh prosody um the most obvious uh feature of speech is volume so i tend to speak a little bit louder on average than other people pitch is another one high tone low tone volume pitch given those two aspects of prosity what are some other aspects of porosity you can think of things about how somebody speaks but not what they say not the actual words themselves we've spent quite a bit of time in this course focusing on the diversity among humans and human behavior diversity among prosody is particularly important if we're developing interactive technology for speech think about everybody that you talk to on the phone or face to face maybe not so much face to face at the moment but uh who you speak to by phone by zoom what are the ways that the ways in which they speak differ what are the dimensions of diversity of prosody volume is one pitch is another one high frequency low frequency clarity of voice yep that's another great one how how clearly do they enunciate their words other ideas sarcasm that's a tricky one i think sarcasm would typically be uh would sarcasm would probably be qualified as content pace exactly so speed at which somebody speaks those are all aspects of prosody accent accent is a good one uh that's somewhere on the border i suppose um you could probably throw away the content if you imagine that we're just focusing on english speech you could throw away the english words themselves and you might still be able to detect particular accents so given that khan and rachel i think accent would be would qualify as an example of prosody we'll look at the specific process prosodic features that were used in in this study thomas you have yourself unmuted did you have a comment uh no i don't i didn't mean to have myself unbeaten no problem okay so that's prosody um let's have a look at just the amount of data that was captured to begin with they took the raw data which is just the statistics of speech and the first thing they asked is how many packets of prosodic information did they capture and they averaged this into bins of person seconds speaking or pss so for example at a given moment in time on a let's say a tuesday morning across all 24 uh microphones being worn by the 24 graduate students was someone speaking at that time or not if for at that moment in time if we run forward for one minute if all 24 graduate students were speaking then there would be 20 we would have captured 24 person seconds of speaking we would have captured 24 pss during that single minute if only three of the grad students were speaking during that minute we would have captured three pss if no graduate students were speaking during that minute we would have captured zero pss zero pss okay so they did that um for uh minute increments over uh the five work days so each of these five panels corresponds to monday tuesday wednesday thursday and friday and they dropped it into i think these are five minute buckets as you can see here not surprisingly the total number of pss that were captured per hour or per five minute interval is not constant over the working week there are peaks and troughs why we have 24 people you'd imagine most of them are up and around during working hours these are graduate students some subset of them is going to be speaking more or less at any one given period of time but that's not necessarily the case why peaks and troughs as always this relates back this relates back to context right so what is the context specific to our group of graduate students as joseph mentions it's an academic setting we're focusing on academic context here they're taking classes they're grad students so maybe they're teaching classes at certain times when are these graduate students taking classes together they have a common class which is extremely obvious from the data here i guess i'm sort of giving it away here right tuesday and thursday morning you can notice there there are lots of lots of pss remember i mentioned that a pss represents that actually one of the graduate students was talking so the volume of their voice there was there was a statistic that represented that it was a human voice not music or background traffic there's someone speaking and the volume of that person speaking was sufficiently loud that it must have been a mouth moving very near the device so the device can distinguish between someone who is speaking and someone who is speaking to someone who's wearing the board the board would not rep if i'm quiet and i'm listening to someone who's speaking to me about two feet away which pre-pandemic arrow is about the normal way of face-to-face communication my microphone at that time would not register that or this the sensor board would not register that as me speaking it's someone else so it would not capture a pss for me what can you tell me about the class that they were taking tuesday and thursday morning it's amazing how much uh how much context you can indirectly infer by looking at data collected from this lots of discussion right so this probably wasn't a very large lecture with a hundred people where the probability of any one student doing any much talking is very low this was either a very small graduate lecture that was very interactive as henry mentions or as a graduate seminar where it was basically mostly conversation and discussion rather than than lecturing okay other things you can notice typical grad student lifestyle especially on friday things don't get rolling until noon and there's some people working later at night and so on okay so this gives you a snapshot of the total data set that we're going to be working with in this study we now need to take all of those pss and we need to try and figure out who is speaking with whom at what time so um we could have at any given time a subset of these graduate students in conversation or maybe one of these graduate students in conversation with somebody else who's not a member of this group of of 24. so um let's look at a hypothetical situation let's imagine at a given point in time three of the graduate students are in a classroom maybe this is just after a class has finished and the three of them are in conversation they're discussing what went on during the seminar at the exact same moment in time there are two other graduate students that are out in the hallway beyond the hearing of the microphones of the graduate students in the classroom and vice versa as the class as the conversation is winding down in the classroom just before that conversation in the classroom ends another conversation sparks up among speaker four and five out in the hallway so imagine we have those people those pss's streaming in from these five interlocutors how do we know who is speaking to whom okay here's just a cartoon of what the uh what the what the data maybe uh generate a cartoon of the data being generated by these five microphones we have time here on the horizontal axis and this is measured in seconds so this is during these conversations you'll notice um speaker ones speaker one's micro microphone records high volume which means speaker one is speaking speaker one stop speaking and then speaker one's microphone is registering medium volume speaker two is speaking during this period and before and after their microphone is recording medium volume of speech speaker three speaks just at the end of the conversation and for the first two-thirds of the conversation speaker three's microphone registers medium volume remember at the same time speaker four out in the hall is speaking and her microphone is register in medium volume speaker five her microphone is registering high towards the end here and then low so this is the data that we have you can probably look at this admittedly simplified cartoon and if i hadn't told you one two three were in conversation and four and five were in conversation you could probably infer this just by eyeballing this data obviously we need to do this automatically so we're going to use a particular correlation metric called mutual information mutual information for our purposes measures the mutual dependence between two variables okay so what we're going to do is we're going to start with any one of these five speakers that is capturing pss we're going to walk forward in time in their raw data if we do so let's let's say we pick uh speaker 2 we'll notice that there are particular points in time in which their uh volume is high and it turns out that for all those periods if you take any small increment of time over this time span for all those periods of time the volume of speaker 1 and speaker 3's microphone is medium so what that means mutual information is high if i know something so i i am confident that if i know that speaker two's volume is high i can predict the volumes of speaker one and speaker three in this case i can predict that their volume is medium so that means mutual information is high which increases the likelihood that these three speakers are in conversation with one another let's take another example imagine we look at speaker four and again there is a span of time during which the micros the microphone of speaker 4's microphone is high the volume of speaker 4's microphone is high but i cannot predict the volume of speaker 3 during this time interval that means the mutual information between the volume of speaker 4 and the volume of speaker 3 is low this knowing this information about speaker 4 does not help me predict the volume of speaker 3. so the mutual information is low between speaker 4 and speaker 3 at least during this time period so that decreases my confidence that speaker 3 and speaker 4 are engaged in a conversation together at that time so imagine we compute this mutual information many times we walk from left to right for speaker one speaker two speaker three and for each of these speakers i project i look for mutual information among the other 23 speakers for which there is some volume if obviously if volume is 0 for speaker 5 at this time i don't have to look at their their data there's nothing there's nothing there they're definitely not involved in a conversation there's either silence or traffic noises or music playing something else they're not involved in a conversation at that time so far so good all right this could be a little tricky to wrap your mind around if you haven't seen this before i'll just pause for a moment let me know if there's any questions okay so let's assume that we do this uh for for this time period here so let's say this is maybe five minutes worth of time doesn't really matter we're gonna break things up into into um periods if mutual information was above some threshold and it doesn't matter for our purposes what that threshold is but if mutual information on average during this period is high between a pair of speakers we place an edge between those two speakers if mutual information is low between a pair of speakers like i showed you the example of speaker 3 and speaker 4 mutual information is low then we don't bother putting an edge between those pairs that pair of speakers so you can see what's happening now using mutual information we're going from raw speech data here raw procedure information which in this case is mostly volume we're going from raw volume speech volume information to a social network each node represents uh one of the 24 graduate students and an edge represents not necessarily that they're friends the edge represents that they were involved in a conversation together during that during that period so far so good okay so we just did this for a five minute period we could imagine constructing a whole bunch of these social networks for different five-minute periods which would give us a different social network for every five minute period for monday tuesday wednesday and so on and if we stack all of those social networks on top of one another in the cartoon example here i only have five speakers but imagine we have 24 nodes and we lie we stack those graphs composed of 24 nodes on top of one another and then we punch our way through those stacked graphs and for every pair of nodes in those stacked graphs we simply count the number of edges that we see and that way we can compress that stack of 24 node graphs down into a single 20 24 node graph which now has weighted edges where the weight of an edge is the number of times that we inferred a conversation between those speakers so in my cartoon example here on monday 1 2 and three were involved in a conversation on tuesday one and three were involved in a conversation so if i stack these there are two edges between one and three so i label the edge between one and three in the compressed graph as two in the case of uh user speaker one and two there was one edge so i label it one and so on okay so at the end of this process we now have a weighted graph where we're going to represent the weight as some some indirect measure of friendship presumably someone that's involved in lots of conversations if you were to ask them they say yeah my friend so and so right now this may be friend in a specific context they may be friends among this group of of 24 because they need they're working together on a common project but they might not socialize outside outside an academic setting for our purposes we're going to treat friend as this very specific uh connotation okay all right so that gives us the social network we're going to now look at features in the social network and to see if there are relationships between features of the social network and features of the way in which people spoke prostitute as we as are as we just talked about there's lots of different features different prosodic features that you can look at in this particular study they looked at four different features how fast did the speaker speak the speakers speak with a high pitch or a low pitch remember that we're looking at conversations so another aspect of prosody is if you have two interlocutors two speakers and you're looking across a conversation which we now know how to infer we can ask how long did somebody speak before their volume which was high drops to volume medium how long did they hold the floor before handing the floor over to another speaker similar but not identical is how often did two speakers take turns what's the turn frequency how quickly did they how quickly did the handover of speech occur in a given conversation you'll notice that volume is not one of these four features we're using volume we're using up volume to infer the social network so if we're going to look for relationships between the social network and prosody it wouldn't make sense to include volume and porosity because we used volume to infer the social network so there'd already be a correlation between these two things because they share a common feature which is volume so we're going to use volume for the social network and then we're going to look to see whether that social network correlates with any of these other prosodic features so far so good okay so let's start by proposing a a a scientific hypothesis and this is actually a social science question it's a question a proposal or a guess about human behavior in a social group the hypothesis is that people change their normal way of speaking more when they speak to strangers than with good friends this may seem on the surface like something that is uh obvious but in once you start to dig into the details it's not exactly obvious how this works because of course what do we mean by normal way of speaking um and what kind of change occurs assuming any change occurs okay so how we're going to compute this we're going to compute a number of variables we'll go through these one at a time the first variable we're going to compute is b sub i slash j so we're going to visit each of the 24 graduate students in this network and each student that we visit that's going to be the iaf student we're going to look at the mean of i so i is the if graduate student the mean of iaf's speech feature b so b could be rate pitch turn length or turn frequency the mean of i's speech feature b when speaking with everyone except one other graduate student in the group who's going to be j okay so we're actually picking two graduate students i and j and we compute first be i slash j which is we visit the other 22 graduate students we go from i to each of the other 22 graduate students and we compute in all the conversations that i was involved with with those 22 other graduate students what was the average rate at which they talked that's one b i slash j we compute a different b i slash j which is their pitch how high or low was their frequency of speaking when they spoke to those 22 other 22 students turn length and turn frequency so we're actually computing four separate bi slash chase so far so good finally we're still we're still visiting the if graduate student we now compute their mean the mean of that uh speech feature when they speak with graduate student jay yeah okay so let's imagine an example here let's focus on pitch let's imagine that you have one friend that you like to joke around with a lot you're often laughing and telling silly jokes if you're laughing and telling silly jokes for most people their pitch is higher they're kind of giggly and laughy and they're talking at a higher pitch than they normally would so that would mean in this example here that if j is your friend and you talk to them a lot you and you joke around a lot b i arrow j sorry it's hard to see here b sub i arrow j if we're doing it for pitch that b might be pretty pretty high value but on average when you speak to other people and people you don't speak too very often in the group you might speak with a more professional slower tone you might speak you might speak with a lower pitch so b for a given graduate student i b i slash j may have relatively low pitch and b i arrow j may have relatively high pitch possible okay the other thing we're going to compute is s sub i which is the standard deviation for a given graduate student for the iath student how varied is their rate pitch turn length and term frequency some people maintain a relatively constant prosody they speak more or less the same way regardless of who they're speaking to but other people change their tone depending on who it is that they're talking to that that latter person would have a high s sub i okay we are now going to compute d sub i j so for each pair of graduate students for pair i and j we're going to take the absolute difference between i b i slash j and subtract b sub i arrow j so we're taking the way that they tend to speak to everyone except j and subtracting how they speak when they speak to j and dividing by s sub i let's go back to an example let's imagine that uh let's imagine that we have a given graduate student i and and they they do not change how they speak when they speak to jay compared to the way they speak to everyone all the other 22 students other than j what would be the value of d sub i j regardless of what s sub i is doesn't matter whether i varies their prosody a lot a large s sub i or they do not vary how they speak a low s sub i they speak to j more or less the way that they speak on average to the other 22 graduate students what would be the value of d sub ij so let's imagine we're calculating uh d sub ij for pitch remember there's also going to be four different d sub ijs for the same i and j graduate student pair one for rate one for pitch one for turn length one for term frequency let's let's think about the d sub ij for pitch let's imagine that i has an average pitch in their speech when they speak to the 22 graduate students when they speak to grad student j they may speak to jay multiple times over this six month period uh they may speak to jay many times but it turns out that all in all those times their average pitch is about the same pitch as when they speak to the 22 other graduate students what would be the value of d sub i j have i lost you remember that b sub i slash j is the average pitch of how imagine i am i am grad student i i have an average pitch when i talk to everyone else so whatever that average pitch is that's the value of b i slash j and i subtract the average pitch that i talk to j and it's the same so these two variables have the same value so the uh the numerator here is zero d sub i j would be the value of d sub i would be would equal zero amanda said find friendships among the social network you could do that we're assuming that we're doing that's what we've already done amanda when we computed this weighted graph we're going to assume that these weights represent friendship more or less for now we're just computing these these variables what they actually mean is they're just telling us something about the basic behaviors specific to porosity among this group so for a given pair of graduate students i and j if they have a d sub i j between them that's zero that means when they talk to one another the way in which they talk to one another is not that different is not different at all on average from the way that i speaks to everyone else so far so good what does it mean for a given pair of graduate students i and j if they have a very large value of d sub i j you can see that the numerator is an absolute so we know that d sub i j is always going to be a positive value what does a high positive value of d i j represent what does that tell you about the behavior of uh basic what does that tell you about the behavior of grad student i well we can work backwards we can work backward try and answer this question we can work backwards a large value of d i j by necessity means that b i slash j and b i arrow j have to be very different right the absolute difference here has to be large so what does it mean what can you infer about grad student i what can you tell me about the behavior of grad student i if this variable is very different from this variable [Music] [Music] let's try and i speaks to jay very differently than everyone else exactly cole's got it right so uh if i tend to speak at a certain pitch when i speak to the other 22 graduate students but when i speak to jay and we haven't specified here whether jay is a friend or not jay is just some other individual in the popul in this population of graduate students when i speak to j my pitch is much higher than it normally is that would give a high value of d i j yeah okay so we're going to have 24 of these values of d or sorry we're going to have uh we're going to have we've got 24 graduate students we're computing dij for every pair of graduate students so that gives us 23 times 24 for each of the graduate students 24 times their relationship to all the other 23 gives us i can't do that in my head however many d's that is we are also going to compute for every pair of graduate students c i j which is the fraction of time that i speaks on average with j this is a fraction it doesn't really matter for our purposes you can think of this as the weight of the edge between a pair of individuals so a high value of c i j means that i and j spend a lot of time talking together and a low value of c i j represents that i and j rarely speak with one another a c i j that equals zero means that throughout the working week for all this time period that it was recorded i and j were never involved in the same conversation so far so good okay so we got a whole bunch of these d values and we got a whole bunch of these c values and for each of these we also have we have four of them so we actually have 24 times 23 times four because we can do this for each of the four different prosotic features we've got sets of four we've got four sets of a whole bunch of dijs and a whole bunch of cijs in each of these four sets we can look to see whether there's a correlation between d and c it turns out that for all of the four features in each of these four sets there's a negative correlation between c and d meaning that when c i j is low i and j rarely talk with one another there is a higher d i j alternatively when c i j is high that means d i j tends to be low okay so let's have a look at that as i mentioned this negative correlation showed up in each of these four sets and we're going to use r r is a short form for correlation here to represent that we get negative you'll see in each of these four we have negative r values uh negative r values mean there's this negative correlation higher c the lower d and vice versa the p values that are associated associated with each r p represents the probability that that the probability that that anti-correlation in this case that negative correlation is illusory it's not real so if we look in the case of rate we can see that the probability that this anti-correlation is illusory is very very low so if you flip it's like a double negative so if you flip that around the p tells us that there's high probability that this anti-correlation in rate is correct same thing for pitch and turn length in the case of turn frequency you can see p is higher than 0.05 so for most statisticians for most people working in statistics this value is now high enough that we can't trust this negative correlation so this negative correlation showed up more or less in the first three of these prosodic features but not the fourth one okay let's take this all the way back to our original hypothesis which is that people change their normal way of speaking more when they speak to strangers than with good friends do the results support this hypothesis or refute it do they seem to fly in the face of this statement you can vote if you want the data supports the hypothesis or the data refutes the hypothesis [Music] how many for support okay one thumbs up for support [Music] does this data support the hypothesis that people change how they speak when they're speaking to strangers rather than friends to votes for support seems to support the hypothesis right so let's look at let's look at an example here so lower cij means pairs of graduate students that don't talk very very often higher dij the way they speak to that stranger j differs greatly from the way that they normally speak to everyone else a high c i j means that i speaks to jay quite a bit so we can we can infer that they're friends or at least our definition of friends is that they speak together quite a bit high c i j lower d i j right they tend to speak on average the nor their normal way of speaking when they're speaking to a friend than when they speak to strangers okay back to the hypothesis for the moment you can imagine that this would be a difficult thing to test without this without this graduate students walking around collecting data in the wild right if we didn't have ubiquitous technology and we wanted to test this hypothesis how would we do it typically since this is social science or psychology experiment we would call the graduate students into the lab we put them in a room with random subsets of these 24 students we'd ask them to do something we'd we'd observe who speaks to whom and so on but the graduate students would be very aware that they are now participating in a scientific experiment which might alter the way that they speak when people are in a professional or a formal setting like a classroom or a courtroom or a psychology experimental room they tend to act more formally which as you can see from this study alone alters the way that they speak right so the very act of bringing the graduate students into the lab would probably uh would probably pollute the data and make it difficult for us to test this hypothesis we want the graduate students to forget that they're wearing this ubiquitous technology right we want the technology to fade and become invisible in the background so that they will act as naturally as possible okay let's have a look at one let's have a look at one more hypothesis this one is related to the first one we're also going to ask whether people change their normal way of speaking [Music] more when they speak to well-connected people joseph says it's like putting a tracker on a sea turtle of course this this is a common challenge in psychology and science in general right the moment you instrument a subject either human subject or an animal subject you alter their behavior because they're aware uh they're aware of that they're being tracked or at least in the case of the sea turtle that tracker is going to probably alter their behavior in some way okay so do people change how they speak when they're speaking to well-connected people this is kind of an interesting question because it the very hypothesis presupposes or assumes that the grad students in the group know who is more well connected than others right humans are extremely social animals we have a lot of intuitions about social groups if i were to ask you among your ask you to think about your group of friends you could probably very quickly type in the name of the most centrally connected member of your particular social group right how do we know that again there are a lot of indirect measures to know how well connected someone is in the group and we're going to look at one of those measures in a moment okay so we're going to compute b uh we're going to keep the b's and s from before we're going to use those to compute d exactly like we did before just remember that d for any given i in the population 24 is going to represent how i alters their behavior how much they alter their speech behavior if at all when they're speaking to jay someone else in the population we're going to compare d i j in this case to c sub j which is now we've dropped the i so we have we're looking at i and we're looking at how i talks to everyone else and compare that to how i talks to j and that gives us d i j we're going to look at j for a moment and compute the centrality of j in the social network remember that our social network is a weighted graph where the nodes represent the graduate students and the edges represent how many conversations that pair of students was involved in so we can go to jay the the graduate student we're interested in and we can look at all of the edges that connect with jay's node in this cartoon example here there's just two edges and we're going to just sum up all the integers or the weights that we find at those edges in this case j has a centrality metric of three take another j someone else in the population this this j had eight conversations with this person five with this person twelve conversations with this person six conversations with this person we sum up all these we have a much higher value of c sub j so c sub j is an integer which is just the sum of how many conversations j had with everybody else you you can think of uh c as the centrality or even just the the sociality or the the extrovertedness of of j high c sub js represent js that that involved themselves in lots of conversations with lots of other people okay in this case if we compare d all the d's against c's we see instead in this case that there are positive correlations for most of these prosodic features except turn length and turn frequency you can see in the case of turn length the value of p is greater than 0.05 statistics tells us that we should then treat the r which is a positive value in this case as false we can't trust that there actually is a relationship between d the d's and c's in the turn length bin same thing for the turn frequency bin p is very high can't trust the value of r in that bin but in the case of rate and pitch there is this positive correlation higher c higher c's represent uh higher d's and lower c's represent lower d's does that data support this particular uh hypothesis or does this data tend to refute this hypothesis go ahead and vote takes a minute to kind of wrap your mind around this data [Music] we can turn the the the sentence here the hypothesis into variables people change their normal way of speaking more that's me that is a high dij across all the eyes and all the j's so does a high d people changing their normal way of speaking more correlate with higher c's the answer is yes right that that happens to be the case does this data tell you anything about how they change their way they're speaking it tells us that they change their way of speaking more when they speak to more well-connected people does it tell us anything about how they change how they speak do people for example speak faster when they speak to well-connected people compared to how they speak to everyone else or do they speak more slowly when they speak to well-connected people that would that would correspond to rate what about pitch do people speak with a higher tone when they speak to well-connected people or with a lower tone when they speak to well-connected people relative to how they speak the pitch that they speak to everyone else this data actually doesn't tell us one way or the other which way people alter their behavior they speak more slowly more quickly higher frequency higher pitch or lower pitch would be interesting to dive back into this data in which you have that information to ask that type of question okay so that concludes uh lecture 17 lecture 17 yes i want you just to pause for a moment and think about this data set that we have we have all this we have uh volume information we have rate pitch turn length turn frequency for 24 graduate students what other social science hypotheses might you pose here and test with the data and which of those questions would be difficult nearly difficult or nearly impossible to test in a laboratory setting one of the one of the things i find interesting about ubiquitous technology is again this ability of it to fade in the background which is very important from a scientific point of view if we want to test the behavior of humans or animals in the wild we need them most of the time to be unaware of the fact that they're being observed again there are ethical questions surrounding this but setting those aside for the moment what kinds of questions about human behavior could you ask using this technology and this data set it would be difficult or impossible to do otherwise i'll give you a minute to think about that if you come up with an idea either type it into chat or feel free to unmute yourself and just offer up your idea verbally [Music] differences by gender that would be interesting and now i see of course the graduate students sign some paperwork they're giving over some of their privacy here assuming they were willing to let their their gender be mixed into this data set i think that would be interesting right do these do these prosodic changes the way you speak relative to social awareness where you are in the social network or where you think others are does that correlate with gender so as sarah mentions there are some background context here so again just based on physiology and in some uh in some cultures also culturally women tend to talk with higher pitches that is true in general which has to do with s sub i this is the variability of prosody we're talking about pitch we could look at the variability of pitch but we could also talk about the mean of pitch right what is the the average pitch at which someone speaks that's already known as sarah mentions but how does that alter perhaps by gender as as a function of social interaction as these graduate students are interacting with each other henry henry says help us realize the spectrum of possible ways different individuals communicate yeah that's right so we are again we're looking at s sub i we're looking at d sub i we're looking at how people uh the differences the different how they how they communicate depending on who they're speaking to so i think henry you're capturing sort of the the general question here like you say this is what we're trying to do is realize or understand the spectrum of possible ways actually maybe that's related to the different prosodic uh features here you'll notice that at least in this study they found correlations for rate and pitch but the correlations for turn length and turn frequency not so much that might be a that may be due to the data itself maybe the way that they captured this data remember this was this study was carried out 12 years ago the data might not have been clean enough or high frequency enough there might not be enough data to detect these more subtle aspects of porosity like turn length and turn frequency okay for those of you interested in social behavior the social sciences i highly recommend reading this paper and understanding a little bit more of the details of what they did and again as i mentioned we're all living through a very interesting time for many reasons but also from a point of view of hci now that this technology is out there in the world assuming people are willing to leave their phones on essentially leave their microphones on you could recruit people to participate in a study to understand human behavior in ways that were difficult or impossible to do otherwise okay all right let's leave things there for now that's a social network inference where we focused on speech we're going to come back and look at speech in lecture 18 but speech is of course just one of many human activities so what about human activities other than speech as we all start to wear more and more devices on and in our bodies when we get to cyborg technology how can we understand that what those activities are and how they correlate with uh with our well-being either physical mental or otherwise so we're going to look at in lecture 18. so in lecture 18 we're going to look at inferring human activity and we're going to infer human activity towards a particular goal which is to help people monitor their physical and mental and emotional well-being okay so in order to do that in order to understand what someone is doing we need to be aware of the context so um for example let's imagine uh let's imagine that we're just developing a very simple interactive technology that's running on your phone that's gonna decide whether or not to vibrate uh your leg and give you a notification of something how does a piece of technology know whether a particular notification is going to be well received at a given point in time or is going to be viewed as a as a distraction right someone's driving or performing an activity that is potentially dangerous that is obviously the last moment that's the the last thing that the user wants at that point is to be interrupted so part of what we're going to look at in this lecture is what's known as context aware systems what are systems what are what are ubiquitous technologies that can infer current physical context what is the user doing right now okay we spent a lot of time talking about context and usually it's us as the hci designers that are trying to determine the context so in the case of the 24 graduate students we just looked at the data and saw aha yeah okay tuesday and thursday mornings they're in class how would you be a piece of ubiquitous technology know whether you're in class or not in the pandemic era most of the time you're probably sitting still in front of a screen for better for worse are you playing a game on your computer or are you involved in a lecture that's a difficult thing to to infer unless you're actually moving we're going to be looking at physical activity today mobile health monitoring over the last few years has exploded into a multi-billion dollar industry hands up if you uh if you currently wear a fitbit thumbs up this this piece of chat here if you if you own or wear a fitbit this is a snapshot of their most recent user interface i don't wear one but my wife wears one i'm amazed at what the the fitbit can can infer again we're going to focus on mobile health monitoring what are the aspects of your daily activity that you're interested in um someone has unmuted themselves um what are the various aspects of physical activity that correlate with physical and mental and emotional well-being here are here at least the fitbit corporation's guesses about what matters obviously food sleep how many floors you've climbed how many steps you've taken a lot of these mobile health monitoring applications include a social incentivization mechanism so for most of us we find it hard to eat healthy and get regular exercise and get enough sleep what happens if we show your sleep your sleep and activity score relative to your friends remember back to our discussion about the darpa red balloon challenge which is looking at how to incentivize people to do things that would be difficult on their own here's another example of that my wife was showing me recently the fitbit not only tells you how much sleep but over uh over a night period it also classifies whether you're in light ram or deep sleep kind of interesting okay okay so first thing we're going to think about is we're going to start at the top most uh what is the activity or what is the yeah what is an activity that most people would like to participate in especially at the moment most of us would like to know something about where we are in terms of our physical and mental well-being if you ask most of us we could tell you probably not very good so what can i do during the day that would raise my physical and mental well-being obviously i can watch my own behavior and compare it in a spreadsheet to what i did the day before be nice if there was technology out there in the world that scaffolded my ability to infer what i'm doing and how it's positively or negatively impacting my well-being if you wear something while you sleep and there's an accelerometer in that device it can measure the amount of movement over the night and using that movement data it can infer something about the quantity and quality of sleep same thing for physical activity we just saw an example of using a microphone to infer something about social interaction how often during the day are you listening to someone's voice coming from a computer either netflix or teams or how often are you engaged in a meaningful conversation that's a tricky thing to infer even if you're able to capture speech and porosity stress is another one how could you infer if i'm wearing a device how stressed i am diet is another very important one for for all of us this one is particularly difficult because at the moment it's difficult to see how you would automatically infer the quantity and quality of food that you're taking in it's a whole bunch of websites out there obviously that try and help you count calories and write down or record what you're eating most people don't do that because first of all it's onerous to do so um it's it's onerous to to do so and we really don't want some of us don't want to know what it is that we're eating there's some amazing technologies that are not quite commercial products yet but there may be there may be a year two years away from being commercially available here's an example from um here's an example from tufts this is a sensor that's actually placed on the tooth and depending on the sensor it can measure sugar alcohol caffeine carbohydrates starch and so on so the silk lab at tufts who produce this they claim that they can modify the bioresponsive layer in the sensor so the part of the sensor that's responding to biological materials food stuff to target other chemicals and so on i don't know whether i would be willing to wear this on my tooth there are clear acceptability issues around this kind of technology but it may be possible in the not too distant future to wear something that automatically records uh how and what you're eating throughout the day okay okay so um we're going to end today by walking through a table which draws out all of the dimensions that make up these human activity recognition systems so we're going to look at horrors uh in lecture 18 today human activity recognition we're going to assume in all these cases that the we're going to try and recognize activities using wearable sensors so uh assuming we're in outdoor environments and there's video cameras around our webcams if we're sitting in front of a computer we could try and infer activities at a distance from a live video stream but most of the things we're interested we're interested in are very difficult to infer using a video stream because we may not be in front of a webcam most of the time it would be good to try and measure these things at point of capture where exactly right when we're bringing taking in food that's the best place to sense what we're eating rather than trying to infer the food from uh from a webcam okay so we're going to talk about hard today we're going to assume that we're going to try and do har we're going to try and recognize human activities using wearable sensors this table is drawn from the reading for today which is a survey article from from 2014. first thing we need to think about if we're developing a particular technology to recognize one or more human activities is the machine learning side are we going to try and learn or get or predict uh what what activity the human is involved in offline so we're going to capture some data and then later on try and infer the activity from that data or are we going to do so in an online manner in your leap motion in your elite motion project you are actually doing a form of human activity recognition is the user signing an asl digit yes or no and if so which digit is it this is a very limited set of activities but you're doing this you're not quite doing it with a wearable sensor the fitbit is uh the fitbit the leap motion is somewhere between a passive a distant webcam and something you would wear on the skin you are doing online execution right as the frames of data are arriving from leap you are your k n is making predictions about what activity the human is performing at that moment in time obviously humans differ so if we're going to try and infer the activity of a given human do we develop a machine learning algorithm tailored to just that user so or do we develop a machine learning algorithm that is user independent we develop one recognizer that that is trained on data from multiple users and can recognize activities in those or other users or do we develop a machine learning algorithm that is tailored to a specific user you've all now gone through the experience that a user independent activity recognition system is much more difficult than a user specific one right the advantage of user specific is it's usually easier to to train it and predict a given user's behavior but of course it doesn't generalize well to others another aspect that's specific to hars is the fact that we're using sensors and sensors themselves the behavior of sensors themselves changes over time if you're wearing if you're wearing a fitbit um throughout the day it can turn out that that fitbit loosens and starts to move up and down on your wrist sensors themselves start to degrade over weeks and months and years very difficult to get a sensor to register the same value for a very long period of time so one of the specific machine learning challenges that comes up in human activity recognition is that the sensor itself is drifting over time how do we make sure that our system or the machine learning component of our heart is robust to time changes in external conditions as we're trying to recognize a given activity are we going to do this in a continuous manner so throughout a time period we're just looking to spot an occurrence of that activity or are we going to segment time we're going to break things up into time increments and look to see whether that activity occurred during that time interval segmenting or finding the start and end of a particular period in which we're going to look for an activity is difficult in your case the leap motion device is doing it for you it's giving you these frames so you as the developer do not need to segment a continuous stream of data into uh segments the reason this is difficult is the length of time that you should segment your data is dependent on the task [Music] if you think about physical activities like running and walking and biking there's a particular frequency or a beat to the motion of your legs that is pretty high frequency and if you look at mo uh a five second period or a ten second period you can look for that frequency and find out what activity they're carrying out if that time period is too short that you don't capture multiple oscillations of a walk or a run or someone biking you can't distinguish between walking running and biking the period is too long you might capture someone who bikes for a few minutes gets off the bike walks their bike gets back on their bike pushes their bike while they're talking to a friend and again you may not be able to distinguish in that overly long period of time the biking and and walk in how do you determine the right period of time in which to look for a particular activity often you need the help of an oracle we'll talk about what an oracle is uh on thursday you have a quiz due tonight and you are now working on deliverable nine i wish you a good rest of the day and a happy uh election day bye everyone see you on thursday you're welcome 
