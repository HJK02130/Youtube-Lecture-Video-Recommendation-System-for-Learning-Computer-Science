id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
tkZjed2L4F8,27,"Paolo Bestagini
Dipartimento di Elettronica, Informazione e Bioingegneria
Politecnico di Milano (Italy)

Image and video source attribution in multimedia forensics 

Information about which type of device captured an image or video can be used to help determine or verify the origin of the imagery under analysis. This can form an important piece of evidence in some scenarios, such as analyzing images and videos involved in child exploitation investigations. While metadata may contain information about an image’s source camera, metadata is both easy to falsify and frequently missing from an image. As a result, signal processing researchers have developed information forensic algorithms that can exploit traces intrinsically present at pixel level.
In this talk, we present an overview of methods developed through years to attribute an image or video to the originating device. We will see how promising approaches can be found in either model-based or data-driven fields. We will finally analyze the future challenge in device attribution.


Paolo Bestagini received the B.Sc. and M.Sc. in Telecommunications Engineering at the Politecnico di Milano in 2008 and 2010, respectively. Since 2011, he joined the Image and Sound Processing Lab (ISPL) at the Politecnico di Milano. He received his Ph.D. in Information Technology in 2014. Since 2016 he is Assistant Professor at the Image and Sound Processing Lab.
His research interests focus on multimedia forensics and acoustic signal processing for microphone arrays. He is an elected member of the IEEE Information Forensics and Security Technical Committee, and a co-organizer of the IEEE Signal Processing Cup 2018.",2019-08-09T19:07:31Z,Seminários: Image and video source attribution in multimedia forensics,https://i.ytimg.com/vi/tkZjed2L4F8/hqdefault.jpg,Unicamp IC,PT1H26M15S,false,114,1,0,0,0,"today we have professor photo mr. genie he's a professor anybody technical in Milano he's a very good friend of mine with matsing 2012 think so yeah this is the second time he comes to Brazil and perhaps some of you were here last year when he gave us a talk about these research topics and I'm glad to have him again dizzier and we are already discussing and I too have again next year and so forth of course I'll go there too so thank you very much for being here today although it's not talked to us about some of the imagined sound research that they have been carrying out in we have some of these works are being collaboration and some others he has been doing with partners in the US and Europe so thank you very much thank you thank you for giving me this opportunity I probably saw Jennifer detector with Amira know as Professor Arthur so I was telling you before going deeper into the technical parts of the presentation I'd like to spend a couple of words about the research group are working with it's for those of you who might not know the place in Italy so this is the main campus it's a leading University in Italy since 1863 the peculiarity of Politecnico di Milano compared with Allen University is that we only teach in three areas engineering design and architecture so there are no other area investigated there this is a picture of the historical building of the main campus back in 1930 it might be not so impressive to you right now but if you think about that for Italy back then already having campus like this was something amazing actually in Milano despite the name is not just based in Milan actually it's based in a multiple city in the northern part of Italy the most of which are in Romania which is the region to which Miran began belong but not only Nadia and that's me on this map you can see we have part of the campus displaced in Como and lack in Cremona in Manitoba but also kitchens which is just outside of the region to give you an idea of some numbers how many students we have how many faculties we have in the barrios areas I have the slides here so for architecture if you think just about that we have almost 300 in terms of faculties and almost 7,000 students if you think about design it's 94 faculties and 3500 students whereas for engineering we are almost a thousand faculties and more than 27,000 students in Milan what about the spread designers in engineering according to Politecnico di Milano origin well on this map you can see that the 21% of the architects in Italy that were seen in so they actually are graduated from Politecnico di Milano the 16% of engineering of engineers in Italy come from Politecnico di Milano and the 42% of designers of course there are many departments so let me spend a couple of words about the department I'm working with the department of the electronica formación of the engineer here actually there has been quite a war whether we wanted to translate this in English and also sometimes someone translate into English sometimes it is left in Italian I think that here is probably very easy to understand the meaning was in Italian but the Department of Electronics information about engineering the history of the department is pretty long as well so it was born in a in 86 and the name was different actually we'll just Department of Electrical Engineering and through here's the name of the department changes and since 2013 we are they the department to the electronica for mastering engineer which is the name that we actually have nowadays in terms of people distribution within the department we have around 200 research assistant 14 CNR which are other researchers collaborated with Politecnico 60-something technical administration staff 59 sister professor 72 who professor almost 300 PhD students at the department and more than a hundred associate professor just there in terms of revenues actually these are increase increasing year by year so here we're just reporting there's a quick screenshot which are the revenues for the department does the debt department from 2014 to 2017 and you can see that actually the amount of funding that we can get from different sources is getting higher and higher which to me means that we are hopefully working well we have published every year and we try to research share as there actually are multiple the researchers that are being investigated politécnica that day so from system control to bioengineering electrical engineering staff electrical branch computer science and finally telecommunication engineering which is actually the branch are coming from which are just to give you the last numbers the last figures about the department how many labs we have in which are which one they are so we have nine laboratories just for pioneering people 17 managed by computer science researchers five by the electrical engineering people age for electronics in ten laboratories culture with telecommunication research one of these laboratories is actually different lab in which I'm working so is the image and sound processing lab is PL we are we are five faculties in between from all associated an assistant professor right now we have four postdocs working with us each PhD students and other accounting research assistant visiting people you might probably know Pedro was visiting us right now from University of Campinas and these numbers actually rise a lot so this is the suspension of today but if we count visiting person actually this changes in time research areas in which were involved actually there are many from all the processing to image processing to video processing we can split them mainly into four areas so in terms of image and video processing we have some ongoing project with DARPA we had some we had some ongoing project funded by the European Commission we have some ongoing research of spaced and processing for audio we also denies the physical data so we have people working specifically on Sydney conversion and analysis of the physical imaging and there is also students working on music information retrieval stuff here a quick overview of the main topics we are dealing with so in terms of music information retrieval for example we are working with similarity between music tracks we are working with music recommendation systems in terms of space time of the processing we have ongoing research on acoustic source localization and rendering of acoustic scenes we are working also on the detection of us are those events from microphone recordings and there are also people working on no leader or nonlinear audio processing specifically dealing with sound synthesis and built on analog so they're trying to recreate algorithms to generate sounds according to maybe some old series that were used in some old instrument I was telling you we work into physical images so we have season he mentioned processing research going on with an Italian company working on the oil we have some research and investment vacation concerning computer vision and digital image processing for example for food product assessment we are working on some typical 3d processing and right now we have two projects going on one for automatic docking of boats into harbors using cameras and 3d view of the scene and also the analysis of doctors working with patients to see whether actually they are keeping the correct position or they might be in some dangerous position for their working condition and last but not least multimedia forensics which is the branch I'm mainly involved here we were not tampering detection forbidden images we are developing deep learning methods for forensic researches and i alighted here the works that we are doing on device attribution it was actually today my talk is gonna be mostly about device attribution so thank you for the patience for this and let's now get to the technical part of my talk what I wanna talk to you today about his image and video source attribution in not media forensics so my idea is to give you a quick overview which kind of algorithms have been developed in the literature and which kind of argument not working on nowadays for this topic why well I think all of you are convinced that images are part of our daily life we take lots of images and picture and photographs with our smartphone we have cheap cameras we can share those images online we have social networks that we use like a lot we can even share pictures with apps and use apps and our devices and smartphones to modify these pictures before sharing and that's great for sure but not everyone is using pictures just for this so there are risks associated to the use of photographs and images and pictures photographs can be used meaning with malicious intent by many people just a few example here you can use picture to force opinion formation by spreading v news because you know a picture is worth a thousand words so when you associate a picture with a post of yours on a social network or a newspaper you are enforcing somehow some ideas but if you edit it those picture or if you use wrong picture for the Broncos you might really misbehave and create very very bad situations so for sure nowadays there is a need of linking the pictures that are shared to the acquisition devices that were used to shot this picture why why do we need to link a picture to the device what what is this giving to us in terms of information that we can exploit to fight this misbehavior well let me give you some examples one is the following let's of course there is a anonymous unknown fake Facebook user that is posting some offensive or illegal material you don't know who is it who is but you know there are lots of other Facebook users of course and they also post pictures what I'm going to show you that you can do is okay you can download pictures from all the other users that are maybe suspicious from these pictures you can build a sort of fingerprint of the camera used by the users and then you can link the pictures link the picture published on the fake account to some other users so you might understand would a user wolves like the one to your left sorry like the one in the middle it might be fake it might be original you don't know but if you can split the image into several blocks and you can leave link each block of these image the specific device like it's done here you link the two blocks highlighted in green to a specific smartphone and you understand that the other blocks come from another smartphone well you can understand that the picture was modified by splicing together two different images so just linking part of pictures to devices might have seen descent and you can do the same for videos let's suppose you huge video long very long video you might analyze it and understand that okay the first 400 frames have been acquired with one device then you have some other frames that have been acquired with the second device and then the last amount of frames were acquired with a third device so just by analyzing the video and being able to link the video frames to specific devices you can understand that the videos of computation that the video was spliced at in time it's not an original sequence just by a single camera may be organizing some closed king-tv recordings and you might understand that that recording was tampered with or being able to link images to devices might also be helpful in terms of fallujah netic research being developed here at UNICAMP let's suppose you are in this situation you have a scene and you have three different pictures of this scene like the two on the left and the one on the right you in principle might confuse them as all edited version of the same pictures but if you're able to understand that the two photographs rounded in orange come from one specific device and the one ranked that in green and blue comes from a different device you can understand that actually those are not all images coming from the same root image the first two image are actually near do because they are edited version from the same picture the other image comes from another device so if you're building Scylla genetic trees of images you might use this technique to separate images of the device and separate different trees if you're working with feed alternatively forest as Professor Anderson is doing in these many years of research on this topic so if you are convinced that's being able to link an image to a device might be helpful let's now formally forward the problem and let's see how we can do that so let's start from the problem source attribution we have a picture we want to understand where does this picture come from from which device this is everything and nothing if you think about that I mean it's a very general problem formulation it doesn't a lot but it tells us also us nothing because it's too broad and actually why because link in a picture to device might be interpreted in many different ways because there are many devices we have scanners they have come corners we have what kind of cameras we have smartphones then we have different models of the same smart I mean we have different devices of the same model so the rather than treating source attribution problem as a whole something that is typically done in the literature is to split the source attribution problem into different levels so we have the first level is the support class attribution you have a picture you want to just understand which is the class of devices used to part a picture which means you might want to understand whether the picture comes from a camera or from a scanner this might help you in stocking your investigation the level two is the model or brand of tribution you'll have a picture you know you're working with the cameras no no because you already split scanners and cameras beforehand and now you won't understand ok which brand of camera or which model of camera was used to acquire this picture but then you can go even further and you can reach another three of device attribution which is the real specific device attribution you have a picture and you want to understand okay given that I have several devices of the same brand and model which one of these was used to shot at pictures and these of course is it looks and it is much more difficult but it's something that in the end you want to do so let's go through these levels and let's see what you can do you have a picture you just won't understand which is the class of the device that was used there are some researchers dealing with scanner versus pictures so you have a picture one understand does it come from a scanner for my camera here I'm listing a few references actually you can find more of course one of the very well and very good medicine works very well for this kind of problem actually follows the pattern recording reported in this slide you have a picture you the noise the image you subtract the mist the Benoist version you obtained is noise residual you extract some features that are related to how color channels of the imagers are correlated in between them and then you feed these to a classifier and you can understand it whether this feature have been extracted from a feature that come from a scanner or from a cable there are other approaches as well and another well investigated problem into this level one of classic tribution is even a picture can we understand whether it comes from our camera it's a natural picture or it has been computer-generated and let me test you on this if you have this ten images here can you just by visual inspection understand which are real photographs and we should have been computer-generated I think also due to the projector it's even harder but even for me and even for you in a screen this is a very hard test easy victory is the challenging GG CG States for computer-generated the real stage for the camera pictures and naturally the first two to the car and the coffee cup were computer-generated well you might have guessed the coffee cup of the car is really really well done the cork open the bottle opener the third picture was real whereas it might seem computer-generated I mean the coffee cup the coffee cap and you see that also for the other feature is not so easy for us to understand which are really which are not but luckily we have algorithms that have pausing taking this problem how do they work but there are many many again so here listing something and I'm just giving you we preview of how they can work so for example we have some working extraction feature in a wavelet domain from the images and then fitting this feature to classifiers of course and as you might have guessed there are methods of their working with convolutional neural network learning directly from the images whether they are real or not but there are also some interesting metals that are specifically focusing on some peculiar cases for example if you're just talking about picture of faces of persons you can try building a 3d model of the face then try projecting the picture on this 3d model and look at the motion the statistics of the movement of the person of the face if it's a video or if you have images you can try to see whether the picture really can fit the 3d model or not and this might help you integrating real photographs from computer-generated ones if you notice the designer of this visor was using for his last topic and talking to you I used the classical approaches titled why did I do that because these approaches have been developed through years some of these approaches are still part of research but why do you think that today is important to deal with this sort of research because today we have a new class of computer-generated images that are not simply generated by using computer vision and 3d modeling we know that nowadays with convolutional neural network and generative adversarial networks we can really obtain pictures that looks like natural but they're not this is a very classical example of a network developed by Nvidia where they show that they can generate pictures of faces but none of this is real no one in this picture that looks that photograph no one of this person actually they have been just generated by machines and you can understand that this might become a serious track in the future because if you can generate faces if you can manipulate faces you can only put it feature using artificial intelligence with such a high quality you can undermine the credibility of news that are spreading because you are modifying the pictures or you can cyberbully people using these sort of pictures and modifying pictures of your colleagues and so on and so forth so nowadays I think it's really important to be able given a picture or given a video to understand whether it is real or it has been computer-generated with one of these techniques exactly this is something that he is being faced in terms of research in the literature basically this year some interesting methods that have been developed for different leaders well one specifically focus on eye blinking one researcher noticed that on the previous generation of deep fakes videos the faces were not blinking the eyes were as of course any natural person any real person sometimes needs to close and open the eyes why they would be completely dry so they developed a method to detect the eyes and then to spot whether the eyes blink in time or not and if you have a video of a person that never closes as well that video is fake this method was very nice but I think after few weeks if not less than that from the other part of the research community those working on generating this face they were able to generate videos that now open and close the eyes so this method was rendered completely useless for the new generation of deep face videos but fortunately there are other techniques there are some other methods that exploit an SDN so long memory network to study the evolution in time of the fringe no we don't understand whether it's real or it has been generated there are made of extracting some features and classifying these deep features and we also started working to stop it and what are we doing at Politecnico NB as well we tried a different approach as if you model the videos well the different creators might change their algorithm we asked ourselves why don't we try modeling the real videos and detect the fake videos the computer-generated ones has no real so we start actually from a very old idea that was already exploited for other forensic stuff that was the use of Bedford slow actually if you have a picture a natural picture so you're not just taking random pictures of random pattern but if you go outside take a picture of a nice landscape picture with people pictures of natural scenes you can statistically observe that the pixels should follow some statistical distribution Benford's law is one of these particular statistics that should be followed by by the pieces of our image so we try training a classifier understand whether our picture follow this but for now because computer-generated picture might not follow as well did be slow they are not no one is telling the generator of the face precisely form of the statistical law whereas natural teacher mind so we tried implementing this and we try this method on different basis of natural images I will use the you see it raised president there are three well-known forensic data set of pictures and we used can generated images obtained with different implementation of different networks here are some examples of the gun generated images we used so some of you might be familiar with the cycle gun turning apples into oranges like in this picture bottom left and oranges interactable we have the face is generated by the program in media we have picture generated turning paintings into real images and vice versa we have pictures of zebras are actually or system are being turned into zebras by artificial intelligence method we try disambiguating given a picture is it natural or is it or has it been generated by a neighbor results were actually in this picture I show you for each class for each dataset so for each class of pictures that we had the accuracy that we had in detecting those pictures are natural or lamb generated so the first tree column here showed that for the three datasets of natural images we basically always get with accuracy that higher than 95% that those picture are natural whereas all the other pictures are pictures belonging to gamma generated and what can we observe actually for many of the datasets like the faces the Apple to orangey is the orange turned into a post zebras turning into horse horses turned into zebras somewhere to winter and so on and so forth actually the method was pretty pretty promising we can detect that the picture was computer-generated with a very high probability but we had some problem with a couple of datasets to me this is nice let me show you why and let me show you to do that let me show you which are those data set we are having trouble with the first one is the data set composed by picture that are painting from the artist Monet turned into photographs when we analyze this picture photographs obtained by on a painting this was that was that roster yes here our argument doesn't work very well but to me is nice because that method was specifically implemented to turn something into photographs so probably the method also learned the statistical behavior of the pictures that we are exploiting another dataset on which we are not working that well is the photo enhancement dataset those are pictures that are actually real photograph that are being answered by genetical personal Network algorithm so actually these where pictures at the beginning this has been an answered but probably the generator didn't change the statistic that much so we can't understand that they were real just by observing at the statistic which means that there is still a lot of work to do in this sense let's not wait to be level two of device attribution which is brand of mode now we have a picture if we now we know that the picture comes from a photograph for from a camera sorry let's say and we are interested in understanding the model of course this is very challenging it's even more challenging than before if you think about that because if I just show you this two groups of three pictures here and you understand the different pictures of the same object of the same scene come from different cameras but just even a picture from the same good model that was used by visual inspection of course it's not that easy you can do that algorithmically which is the key assumption behind that well the key assumption is that every camera model from every different brand as a very specific appellation pipeline so of course there are some building blocks are always in there require picture with the camera and these are represented by this pipe and humidity in this line you have a scene a nice growth lens you probably have an anti-aliasing filter the light rays pass through the corner filter array hit the sensor and then you have a demo sizing algorithm to recover the color of the image you probably have several post-processing step you have a compression there to save the imaging to whatever compression format whatever point he or selecting on your device and then you finally have the Beatrice but all of these blocks can be different for different brands and models of cameras so the idea is we can exploit we can study traces left by these small building blocks in the pipeline to understand which is the camera that was specifically used for acquiring a picture in this sense we have methods in the literature dealing with exploiting traces left by the lens itself we know that lenses are not perfect so they introduced some rather distortion if you have a straight line if I take pictures of this world where there are lots of straight lines and my camera doesn't have a perfect lens the straight lines might be distorted while I'm taking the photographs so on one hand you might end up with distortions on your image you saw straight lines and one more straight on the other hand you might use your camera might use some correction algorithm that tries to make the lines straight again either way or the lens or the - that is used is leaving some crossing trace or your image that you can exploit here animators will actually try to estimate how curved are the lines that should be straight on a picture and based on that they can understand which kind of lens so it's kind of device you are using these are nice work but what's the issue of one of such algorithms well that very high quality camera on DSLR cameras you can change the lens so they can detect the lens but you change the length of your device they can't protect you anymore just because you change the way so of course there are always some drawbacks in these sort of techniques other techniques that are very robust exploit traces left by the color filter array which is in your initial pipeline now what's the color filter array for those of you who are not familiar with this to understand this we have understand how images are acquired by a single sensor camera how does the sensor work you can imagine that the center of your camera is a matrix composed by multiple cells each cell records how much light it the cell and the more the light the brighter will be the pixel represented by that cell so you have a ray light hitting with a very strong power one cell of the sensor the corresponding pixel on your image will be white you have one cell of your sensor that is not hit by light at all the corresponding pixel on your image will be black if you have a grayscale image that's easy if you have an average amount of light it's neither white nor black it's gonna be of some great level the PISA correspond to the cell but how can you record information about the color well the trick is to use the so-called color filter array CFA you superimpose you superimpose back to the picture here to your sensor a very thin layer of color filters so now each cell of your sensor only acquired information about win or about red or among blue color which means that you end up with a picture like the one on the bottom where for each pixel you only know the amount of red or green or blue that belongs to does B so but how can you get the full color image at full resolution at the end well you need to interpolate all of the red information that you have for all pixel you need to interpolate all of the green information that you have for your pixel and you need to interpolate also all the blue information that you have pixel by pixel then you can merge all the three colors under together and obtain a full color image but why is this interesting from a forensic point of view because different cameras might use different color filter arrays so here I am reporting possible cover filter arrays that might be used by different vendors so if you have a picture you can understand which color filter array was used to acquire death picture you can understand which kind of camera was used another thing is different brands and different models of cameras has different field works of course and all the figures you have implemented the different algorithms for interpolating the red blue and green color information which means that if you can understand which interpolation algorithm was used you still can get her some information about the news device they use Brenda Leigh is model and there are some Nathan doing this in the literature what's the issue of this method well that not all cameras work to the core a filter array there exist cameras that exploit multiple sensor light in the picture that I have here is this an acquisition scheme of a camera it has three sensor you have a prism in front of your lens the pre separate the red blue and green colors of the light red light is diverted to a specific sensor that on the far right information that green light goes for another sensor and the blue light goes for another sensor here you don't have the filter array you don't need to interpolate anything this method would not work so very robust methods but they do not always work and another issue that they have they are strongly hindered by JPEG compression because this interpolation traces are really really subtle you start compressing your images like you load your images on Facebook you share your image through what's up that is compressive name is more and more and you lose the possibility of protecting these interpolation traces there are other techniques as well that are exploiting statistical traces left that basically capture some statistics of the overall acquisition pipeline and not just one of the specific building blocks one that is very very famous and that was very very well is to use the so-called rich features to capture pixel statistics and to link those to a camera model or brand what you do you have a picture your plants and the noise snapping noise level from the picture you can quantize even more intricate to this noise level and then you start counting how many occurrences or series of pixels you have like how many times have you observed that you have two neighboring pixel big one and two how many times I have the sequence of pixel 1 3 how many times I had the sequence of pixels 0 tree you count these for your image this is a feature vector this histogram counting how many occurrences that you have you feed this feature vector classifier and actually this so called the rich feature were really really well what's the issue is that in order to be informative enough this rich feature vector you need to compute it on quite large resolution images if the image resolution is too small you don't have enough statistics to build this this vector so it typically doesn't work well enough about of course those were all kind of model based techniques but we are in a narrower data driven approaches are taking the lead and of course also for this topic we can find in the literature's many data driven approaches so we don't model any more specific steps in their position chaining what we do is with feeds a learning algorithm with lots of labeled images and we try to directly understand to make a network typically a convolutional neural network understand how it can disambiguate pictures coming from different camera operator what's the problem of this method is that they typically work very very well also in small patches or the rich feature were not working well enough and nowadays the state-of-the-art accuracy is actually achieved using data-driven method that model base may be for this specific problem what's the problem here which are the issues if you have a big enough in terms of resolution picture then the rich feature might work slightly better or on par but you will need to have harvest make sure if you add the small low resolution picture or just a few patches doesn't leave it approaches work much better what do you call high resolution how many my 5k 5k or 2k but - okay so at least two megapixels before tour which nowadays is not impossible of course but if you're interested in analyzing just a small region of the image like to detect splicing that might be a problem because maybe a very small object inside cute 2 K by 2 K is not the typical size for social networks right normally they are smaller also when you send images by email you can typically choose with low medium or high resolution so sometimes you compress them you considering these dr. even approaches what did we do at Politecnico so let me show you quickly one of the methods that we implemented we worked on this problem and proposing a neural network that actually allow us even a picture to understand which camera model was used but also to understand which parts of the image are used to achieve this results in to understand the camera model what we called the reliability mask why because as I'm going to show you our net report work so that's my patch but not all of the patches are informative enough so how this pipeline works well we have a feature extractor which is a convolution neural network actually we use the fairly simple Lynnette inspired architecture with convolutional layers yellow and some exploding nothing more than that in a couple of fully connected layers in the end we feed this network with the small patches of 64 by 64 pixels and for each patch our goal is to understand the camera model we were using by training the network with 18 possible camera models so the output layer work is a tan 18 element in the collective network so this CNN takes us into the 64 by 64 calmer image and output 18 element vector we feed this feature vector to another fully connected layer or if you think about it you might think that we are using a transfer learning starting from the pre-training feature extractor network and we are adding some awfully competitive layers at the end of that and we ask this other network after the transfer learning trick to just output two element feature vector telling us whether that patch was meaningful or not in terms of Cable model attribution why because in doing that for each path we have an information whether this patch is important or not for our final decision when we get masks like this we have a picture we have a reliability mask where each pixel rise from 0 to 1 so we know whether that pixel was important or not to detect the camera model with these architecture why because actually we were interested in understanding whether we could segment images somehow and whether we could use the entire image for getting to the results or not and apparently there are part of the images that are really really hard to be classified part of images that are not that hard to be classified then we merge all the information for the reliability maps and the feature extractor into another block that performs just classification and this block provides us with remedial information about the camera model so basically we are filtering only the blocks for which the reliability must tell us ok you can analyze this block and trust this block and then we use information about this block to detect the camera model when the results were pretty impressive here are confusion matrix is obtained with 18 different camera models on the left if you are not using the reliability mask and on the right if you use the right that's the use of this reliability mask was improving the results a lot because the matrix is much more variable of course this was a work that we proposed to three years ago now you can find other works performing better if you have a look at the literature the works that are performing advanced nowadays are basically fusion of different pre-training networks that are finite unit directly for this problem so for example here is actually a method proposed by our former colleagues of you unsaid warfare ADA who's working on this topic right now and what proposes okay you can use like Inception Network and exception based method to extract to pre-process the pictures then you can merge the results of Duty Network and train another and the idea fusion of fusing multiple networks to the technique in RAM model was also the idea used by the winner of the 2018 I Triple E signal processing confidential fully organized challenge in 2018 and the goal was even image understand the brand the model they use the shuttle picture and actually the winning team used one of this method completely doesn't even using fusion of different of different person that was working very very well last but not the least concerning class level two of the mice attribution there are betas that allows you to understand which camera model was used just by analyzing metadata traces why because actually in images but also in videos you have a lot not henry information is not information that come with them different brands and different models put different kinds of metadata some metadata some header information our standards so everyone should put those some metadata fields are not standard some brands include those make a better feel so by doing a statistical analysis of the metadata and they have information that you can extract from images and videos you can understand the brand or the host used what's the problem here the problem is that metadata of course can be easily modified so you might not want to trust the metadata to cope with your pictures because someone might have tampered with them very very easy but this is not always true actually there is a work that is from this year from some colleague at University of Florence that rather than simply using the metadata they were using video container information in order to understand which brand of camera was used to shoot a video what's the video container actually when you record a video you have to save it in the video file so you need to choose a format and different cameras this different format but even if you think an mp4 which is a very common format the way the bit information is structured within the file changes a lot from brand to brand from camera monitor camera model you can think as file video file is a container that is built by different elementary items and if you statistically analyze if you try learning the way these items are sorted which items are present which are not which has the table which are the values in this a tone which it over returns are closer and which are more far within the stream of your video you will actually notice that different brands organize the video and within in different ways within them before containers and this is much harder to be modified because if you want to modify this you just cannot simply open the video and edit some text inside of this you should like really coat the video and we encode it following a very strict coding architecture but typically there are no easily usable and user-friendly tools to do so so if you really want to change the you know you can do it but it's typically a hard job let's see then the last level of device attribution and let's understand now given a picture we know that comes from a camera now maybe we know the branded model but we have multiple cameras of the simple and often how can we understand which specific camera is there the idea here is that actually you can treat cameras and hisses guards so from police inspired movies and probably you know that if you have a palette that was shot by a gun you can try linking disbalance began by analyzing the footprints that are left on the ballot by the gun itself because the ballot turns inside the part of the games before being shot in some specific traces are left there well the same kind of operations are applies when you shot a picture with your camera there is one specific fingerprint that is very very useful to a tribute picture the specific device and is this so called PRN you the photo responds non-uniformity what is this this is a sort of background noise that you can find on every picture that is shot with a camera and that is very very specific on the sensor not just a brand of the model but every different sensor has a different noise pattern that gets impressed is a fingerprint on your image why is that because different sensor are physically produced by machines but physical production of the sensor is something that is not perfect so the machines always introduce some minor imperfections in each sensor even if they come from the very same building block the very same wafer of sensors and each pixel of the sensor then responds slightly different to light and this caused this noise this noise is not something that you can see by observing and by looking at a picture with your eyes because it's a very very subtle noise that you're having there but you can extract it with something I think a great move so how does it work this is I think one of the questions that I have my presentation and I also put color so it's even easier to be digested let's assume and actually this is quite true that the blue eye is the image that you have in your hands after taking a photograph so the image that you save it on disk after taking a photograph this can be thought has been as being composed by different elements i zero the red one being the ideal noise free image that you would capture if there were no noise in the world which of course doesn't exist then there is this term K which is a multi flat multiplicative noise term the green term that is the perineal we're interested in that's the term that is introduced by your sensor and then you might have some other entity independent additive noise the yellow term so according to this model when you have a picture when you shoot a picture the blue one you of course have you permission about the scene that you're capturing the red picture but you also are capturing the PRI new information and some other noise if we want to understand which camera was used we need to extract with PR any information so that we have the fingerprint of a camera we are analyzing how can we do that typically through the noising steps so we have an image hi the first thing we do is we obtain the noise that is superimpose to the D base and hopefully this noise contains also DPR a new information which is the green term together maybe sorry with the yellow term of other noises that might be there of course here in just an example of a picture and two different kind of noises that can be extracted from what do I mean well you have several the noise and I agree they're out there and the leaders are depending on the one you use you obtain a different version of the noise from the even some might be good for you that might be not that good so this is one of the degree of freedom that we have what we were comparing you then let's suppose you have your camera you have multiple images you have multiple noise how can you have the finger we're talking you extract the fingerprint from the camera you average you perform a weighted average of these multiple noise that you get for multiple images that weighted average gives you a noise posture like this that is the signature the fingerprint of your device of course I was telling you depending on didn't izing algorithm that you use you might get different results we talked to chief legalizing argument that I'm using the literature right now for this topic well we have no local mean base of the noising or CNN based the Kaiser I think everyone knows here how it works you know that there are many performing the noising there is a so-called DN c NN but they're also harder that might be used for this topic so never network you feed them with a noisy image and they simply extract the noise may be in a resonant fashion and you can use those but emitted that was used in the past but it's still used because it's very very effective is to use non-local mean the noisy how does no can does not come into nosing work well let's suppose that you have a natural picture typically you have many many patterns that repeat within that picture the idea is if the noise on the virus because p some of your image is space independent you can try to obtain a noise free version of your image by detecting all of patches with the same pattern from your image realign them and then average those so my can be example here you might have several blocks i like to be red that are very very close you can extract those blocks then you can average all of those blocks together and you obtain a sort of noise free version that can replace those blocks this is more as in the of course this is not exactly how it works pros and cons actually this is not welcome in based the noises were very very well preparing you even though they were not originally specifically designed for that CNN the noise our security aren't working as well for very near stuff but they're keeping it much much faster because to really have a good - the noise the image using non welcome in the noising well it might take you some time because you really need to compare lots of block within the image and this is really really time consuming unit like structure which you need to train them with like the different kinds of possible creations and few tricks that's true I mean they can be also traded for different noise models because you might train them for additive noise or a multiplicative noise the Avenue itself is a multiplicative noise by definition but editing noise the noise or RGB videos with the literature extracted which is kind of strange but it works really well actually it's not completely clear to me why this really works that well and I was also talking with other people working with the logic field and they had missed it out yet because mathematically they're quite different but in practice you can like detect the presence of the the multiplicative noise by extracting the additive noise which is exactly and it's also true that the image bother where something from is not the most complete image model then we have these are sort of simplified image models so actually there are other terms in there we heard we might be interested in extracting or deleting so there are nonsense of questions you have extracted the fingerprint Perrineau of a suspect camera you have a picture and you want to try contribute that picture to the camera that specific device what you do is a correlation test you have your image you extract the noise from the image and you simply correlate the noise and the fingerprint extracted from the camera if the correlation is higher than a given threshold you can conclude that the picture belongs to the device otherwise not and there are many many papers comparing different thematic techniques comparing different devices of the same model older models or more modern smartphones and up today this pipeline is still the best working pipeline for this sort of problem so you can find now a couple of sitting words of succeeding works trying to train networks to understand typically semies network understand whether a picture and P are in you are relating well or whether two pictures come from the same device but they're still not working as good as the very classical model based partly issues of these it's time consuming it works very well but it is not consuming because the denoising costs a lot the correlation test is not big images might cost a lot as well memory requirements are an issue because let's suppose you are police investigator and you have many maybe maybe you have hundreds of suspect cameras you need a huge database to store all of these fingerprints but these fingerprints despite having the rest of an image are not images not just images you want maybe float 32 32 bits resolution per pixel and it's not easy to losslessly compress them so you if you really want to store all of this information without losing information by coding them well you need a lot of you had the problem of alignment because if you consider sizing if you have degree the image is like this - pretty much the same digitally but you destroy them yes exactly it wouldn't pass the correlation test that's why different sleeping different resizing and also different shifts it's not just a simple cross correlation and it's all and you have the coding problem as well the more you encode the more you aggressively code your images with JPEG or I mean lossy algorithm the more you are deleting these noise traces which is especially a problem for videos because videos are typically compressed with very aggressive coding schemes and so you lose a lot of disparate information in videos there are alternatives there are faster tests rather than doing the correlation test or some tests based on random projecting clearing you in another space there are made to compress very new in order to overcome the memory requirements but at some cost so typically you compress the very new you use less memory but you're losing some percentage points in terms of accuracy and there are also metals that rather than music clearing you as a signal they try distracting some other more features from the perineal and then using those for classification but still are not getting result are as good as the very plain and old-fashioned pipeline just two words another kind of attribution I was with the rest of the talk I was mainly talking about hammers but it's not only this so just to name a few not going to the details there are methods concerned with printer attribution they have been developed also here at given account we developed some as a collaborations and there are also baited trying to classify smartphones from other sensors this is something were working on a couple years ago rather than using just the cameras we also we're acquiring the recordings of signals how it would buy the gyroscope and the SLR meters in those subs are scripts an accelerometer had some differences in terms of noise pattern and feature that could be extracted that could be used to understand which kind of smartphone you were using it's a very advanced being as the last topic of the talk let me spend a couple of words on our minimization because up to now we have discussed how can I understand whether I can understand which is the device just acquire a picture but sometimes this can be very dangerous let's consider situations in which privacy's are concerned like war photographers or countries that are not free at all and are trying to hide things from the news but there might be people trying to shoot pictures in those country and sharing them and of course it would be great danger for those people to get identified because they're sharing pictures to share the true so in some situations we need to anonymize picture so the image that they just show you might fail because we need to prioritize privacy and other musicians of situation which from one end might be what if I'm the bad guy and I want to be anonymous but it can also be we don't want good people to be detected by regimes it's always dangerous because when you working forensics you always have the unti forensics point of view which might be good or bad depending on the people using it you need to have both hacks we need both it's a balance yes sometimes you have to sometimes there used to be a magazine previous five versus five but negative white spy fighting each other if you think it's Star Wars it's like beating and I think it's really easy to understand what we thought is okay we know that there are these noise patterns that are characterizing devices what if we destroyed the picture so that he destroyed the noise patterns and then we reconstructed the pictures from the picture itself so given a picture like this what we did was okay let's carve a lot of holes into the pictures and then reconstruct what was inside the rolls by painting and let's do this several times with different patterns of holes basically every time that we have picture with hole and we reconstruct them the portion that we cut out of these pictures we get some new pictures that were not there before that had been interpolated by the others that hopefully looks like the original pictures but for sure we have scrambled the statistics and also the noise so we can start from a small pen sees that we're comes right from the image and then add the other patches that we were constructed from another experiment and then and back the other patches and then the last pages again we've basically reconstruct the overall picture starting from the original one carving goals and feeling deals and then mixing everything together of course we have problems with these because the picture that we have is very blurry so actually if this might looks nice the actual picture that we get is the very blurry one on the top but these can be mitigated if we use some edge compensation in monitoring the details right now for the sake of time but the results were pretty impressive actually we could really destroy any Perrineau traces by simply applying this recursive in addicted me a nice thing about this method is that we didn't even know we didn't even need to know the fingerprint of the camera that we wanted to remove we simply had a single image and we were destroying and reconstructing that eaglet to conclude as it's probably getting a bit late what we've seen so far and actually there are methods that can help you understanding which class of devices has been used to generate pictures and nowadays this is especially this is particularly important for me if you're concerned with the gun generated pictures now there are methods to understand branded models that works very well and is for this specific problem doesn't even mean to actually are working very very well in the literature and there are ways of to understand the specific device that was used to shot a picture in this situation that even techniques are not the state of the art yet and also it might be important so there are mental to anonymize the pictures as well of course not everything is sold in this huge family of methods there are many many problems to be solved yet one is the use of videos of electronic image stabilization is Professor Anderson was mentioning before if you work with these noise patterns as soon as you rotate shift resize the picture you are destroying these noise patterns if you think about a video captured by a handle device like any smartphone that you have those videos are typically automatically processed to remove the handshaking motion given by your hand so every frame of the videos is compensated which means is slightly rotated shifted resized in order to match some of the previous frames and remove your hand shaking this completely kind of completely destroys the experiment race so there is there are some nice words in the literature trying to recover these noise phrases from motion compensated readers but for sure there's still much work to do these multi-camera devices I was assuming up to now that we were working with a single sensor a single device but it's not the case anymore today we are the smartphone's with two three four I don't even know how many cameras that smartphone has and these are almost real smartphones on the market these are not just crazy computer-generated images when you have a picture that comes out from one of these markers here which sensor are you investigating how the multiple sensor combine the setting a single image well this is something would need to be started from the forensic point of view high dynamic range images now you can acquire HDR picture with your smartphone but also with much better cameras of course how does this work well let's suppose you're taking the picture that's Church so you have a very bright areas and very dark interests what you can do in order to have a very nice looking picture is you can shot different pictures at the same time one after the other of course but from same position almost the same time and they try combining the different pictures shot with different exposures into a single picture that works that the troops very very well but again you are mixing different pictures with a single one so what happens in terms of this noise pattern that you might want to study well it's not clear those are getting shift to destroy the resize so this is something that still need to be studied and super-resolution as well these are pictures that are example coming from Google painted one of the latest pixel devices they are implementing this super resolution camera so basically rather than simply taking a single picture of a scene they take multiple pictures and they combine these multiple pictures together in order to provide you with a better looking picture with less noise and with a higher resolution that the one of their sensor which is amazing from the point of view of computational photography but from the proper forensic perspective this breaks a lot of the things that we've seen so far with this let me thank all of the people that contributed with our material to the talk and all the companies from which I stole some of the pictures this work that I showed you today and let me also thank you for the attention after now and of course I'm here if you have any questions thanks a lot dynamic focus the new cameras that you take and then you select the focus after you have the picture so that one it's gonna be also very very good because I mean some of this technology were already available some years ago but there weren't we're not so widespread whereas right now with computational photography you can do a lot of smart fill in the world cheap devices so they're becoming more and more widespread and this way look at the bright side you have new problems to think about so new new investigations yes lots of new papers new PhD alright so guys we are open for questions and Q free for like real images that have parts yes yes yes some of these can be used there are papers that are like real use case pictures downloaded from website that could be analyzed with these methods both related to measures to a laboratory so with level two you can split an enmity to different batches and try to attribute each patch of the image prosper city camera and so you can detect whether a portion of the image comes from one device and a person from another one if you move to level three so if you think about clearly new you can run the correlation test locally on sliding windows so you can detect that maybe most of the image belongs to that specific device but part of the image is not belonging to this device at all so you can really draw the boundary of objects being carted there are some nice examples around they works pretty pretty well given that the assumptions are straight and steady for example for the PR any technique you need to have a candidate fingerprint on a camera to make it work of course there are some assumptions but if they hold the work or you don't need to have something but you just need to say that they come from different devices yes yes yes more folks that record do you have a bunch of questions I know Marcus more features for instance take a picture from this room from iPhone X and then the finder whose iPhone is at the end of the hour it says oh it's someone who actually doesn't study at any conference so that combines these kind of permission I mean the way that before the style of posting and social media I think this is very interesting because actually to the best of my knowledge up to now the research community still split into the current branches those blindly looking at these algorithms and other people maybe the same people but in parallel looking at the distribution of information but putting the two things together for sure would improve a lot like the results from other point of view basically if I understood your question you mean could we put some reasoning behind yes this hybrid and my answer is yes I think we should but there are to the best of my knowledge there are still not specific works using this camera traces and reasoning out there i think that anderson might be one of the first trying these with these new projects and these very ambitious projects because there's a really really a lack of these sort of techniques but this is probably the way of doing these especially if you wanna fight fake news new spreading that are incorrect because you need to not just blindly use these are great to say ok this picture has been manipulated or not but he won't understand why by who so reasoning might be the answer just one thing I would not try to answer why just of now because why is way more complicated we would need to bring like psychologists ISM but incorporating the semantics seems to be the trend from for the next years more questions you guys here no questions so poll one thing that I was thinking is he presented level 1 level 2 level 3 right it seems that all of the parts that you mentioned at the end we would need to call them level pruple right he's a very chief good problem so the the level that we're gonna need to attribute oh they're gonna be led of Europe yes to be turbo forensics to detect those kind of things yeah and the problem is that computational photography staff are getting developed faster and faster so it's hard to get the haze with those they have the industries their favor and the forensic research doesn't have the industry in their favor not yet not yet that's why the truth might be hidden even though the goal is not what I turn around your image is about you have any other questions that you can answer when you talk about printers or scanners they seem to have much simpler prepare especially these printers let's talk about printers you know text painters very commonly have done they identify document well it was printed I do you also apply this kind of technique so they can be much simpler because the kind of image III wouldn't say much simpler because one of the issue is that printers evolve in time much faster that you change the toner you're changing lots of information the printer itself integrates in time so if you have a lot of data but of that printer but it's not so coherent with the new date of the same printer for sugar traces left by the printers in terms of mechanical traces that's of the paper how the ink or toner gets imprinted on the paper but I wouldn't call then easy another thing is that one of the ways of dealing with printers with these sorts of techniques is to focus on some letters so I bet an example here this is the Python of one technique focusing on the printed letter e so it depends then how many e you have on the document and you might need to specialize a measure for different letters I mean it's not the only solution of course but it might be very very tricky that reminds me you probably never saw type right there you know and some some things that are not present in images digital images that complicate things for printers is that first of all in printers you have like the connection of digital and analog which for images cameras you don't so you have something that's on a paper that's analog and then you need to transform it in introduce though and then you need to analyze a trace left in the analog world and not not introduce the words so this is one of the complications the other is the other is room I don't know if I told you there was a paper that we wrote and the reviewer asked it okay I'm convinced that your technique is good for white papers now I want to see what happens with yellow red and green so we came back to the lab and okay let's collect some colored papers now let's print them and let's see what happens and it seems that the medical is okay for for these different colors but when you change it gray matter like from 75 to 90 the algorithm was broken because it was the texture of the paper was so in the printer you have like the analog playing important role that introduced though you don't have a problem so discover plays a role in the scanner so the place that you put the the printed document in this scanner on the on the blade also differs no because look you have the analog it's spot on different problem but the interesting thing is you have the analog document that you want to look for delteros but then you put on the scanner right which is going to capture the acquisition so for that case you need to know which is the pair a new pattern on the specific camera was aa demise but there are other methods actually I skip that slide but another technique that we developed and the specific specific tyranny and that network was trying to remove specific work that it needs to do that actually if you do that it's not that easy because you need to balance very well the power of the pure a new that you're injecting so there are some tests that you can do an image to understand whether the paramilitary extracting is the natural carry new or it's an injected Perrineau removing the Perrineau of - then you cannot learn anything they always some traces that you can also yeah I mean that was the rationale behind the method that we started working on if we rub it and modeling began which by modeling what is not again what is a natural process this is probably easier an easier way of fighting games because in the end can still apply some mathematical operations of I will fear data in the end the picture that you are that you get will suffer from some interpolation artifacts the pictures of the picture will be somehow coordinated in between them because they come from filters that have been learned of course it's not easy to spot again but I think it's not that easy to for again to completely generate something that absolutely look as a natural picture and the other thing is from my point of view gas needs a lot of data to learn so even if we develop a forensic algorithm they might learn how to fight that specific method but maybe for some algorithm is not that easy to generate quickly such a huge amount of data for them to be learned and also if we insert things that cannot be back propagated in the algorithm with develop maybe for gain is not that easy to understand how to fight those techniques so I think there are few few weapons that we can use against guns it's something that we are gonna find out I hope in the near future but let's see anybody else nope Paulo thank you very much and I hope to Sun you and this money here other times sure thank you [Applause] [Music] "
coxu1Y_y9cA,28,"Elements of psychoacoustics

A lesson of the course of ""Sound analysis, synthesis and processing"" by professor Augusto Sarti",2017-06-06T09:42:37Z,"Sound analysis, synthesis and processing (A. Sarti)",https://i.ytimg.com/vi/coxu1Y_y9cA/hqdefault.jpg,PoliMi,PT1H51M51S,false,5278,69,2,0,0,"good afternoon everybody there were the idea for today is to talk about one of those topics that are really the seem to be completely aside from the main course of the content of the course but on the other hand is actually quite central in many ways when we talk about perception in audio when we talk about how we hear things and how we perceive the things that we hear we actually are set in laying down the ground to understand how to design a system after these constraints that are part of our perception and our understanding of the things that we listen to there are quite a few things that are important to understand about the hearing system and to be honest to talk about this topic with a certain degree of completeness it would take us probably an entire course and we are just trying to do so in a couple of hours so this is going to be a very compressed compressed crash course for all of you we will have chances to discuss some of the topics that I'm going to cover with a little more in-depth analysis of some of the topics later on and especially when we come to the point in which we need to understand more about it because we are designing a system on the other hand before then I really would like you to understand our how our hearing system works and how we go about interpreting the things that we hear in based on our understanding of the physiology of the hearing system so elements of psycho Cousteau kind of a reduced title for a topic that should be much wider but we will begin with something that is not very psychic oosting but is really more physical I'll give you a very cursory explanation on how are the human human ear works during acoustic perception so here's what it looks like the the ear is divided into three parts let's say we have an acoustic part merely acoustic part the outside of the year which included includes the pinna includes the ear canal and then we go into the mechanical part of the system which is made of a bunch of little bones and I'll tell you what they how they work together and finally we're going to the more electrical part of the system we go from acoustic to mechanical to electrical design of our hearing system the reason why I want to go in depth that much is because the the critical part of our discussion is really inside the the inner ear so the outer ear is made of the earlobe earlobe is kind of reductive because the ear lobe in English will be just the bottom part of your Pina so let's say the entire Pina okay the patty you know Rico Larry in Italian and the hearing duct which is the ear canal the the hole that goes all the way to the to the eardrum at the very very end so this is a very this is about it has a very acoustic role so basically has two functions and well quite a few more than than two to be honest but in practice what it does is to perform some filtering of the let's say or the sound that he perceives and at the same time it performs also a little bit of focusing so because of the shape of our penis we are able to focus our attention more in the forward direction but that's not the only thing there is a lot more about it and in fact there is a lot of frenzy about how we actually perceive 3d audio just with two signals have you ever wondered about it how can you localize sources around you despite the fact that you only have two signals that you're analyzing well it's really related to the acoustic response of your body your body has an acoustic response it reacts to the acoustic sound field that it is that it is immersed into and which means that that if we have a source in a certain direction it will be filtered in a certain way but if the same source is moved into a different location it will be filtered in a different fashion so the variation of the filtering properties that arises from the interaction between our body and the acoustic sound field causes different colors in the sound which allow us to localize sources in space to be honest it's not the only perceptual cue that we exploit in order to localize sources around us there is also the so called ILD and I TDD intra lateral time delay of arrival of the acoustic fronds so if something is on the right it first reaches the right ear and then the last year and vice versa or and also the inter lateral level difference so ILD introductory level different which means that if something is on this side because of the shadowing perform but I are on around head it will be louder on this side than it is on the left-hand side so all these cues put together including also motion cues the fact that we move around the object that we do a little bit of interaction give us an idea of what the acoustic scene is like what shape he has and where objects are so all these things will become particularly important in the second part of the course for the moment I just want to focus on the structure of our hearing system so let's focus on that and just be content with the notion that you're the outer ear so the penis and and the ear canals will help you focus and will help you filter things out now there is one fact that you need to know we are what we are animals we are animals and we have evolved as animal in order to you know specialize ourselves into the perception of specific targets animals especially predators learn how to become very sensitive to their preys and so they learn how to catch them basically if you think about the bats they will develop mechanisms that are based on the emission of ultrasound and and the analysis of the returned the bounced off acoustic field that comes back from the target also sounds don't propagate that far so it's going to happen only in in the near field just to be honest but they be able to catch a fly with that kind of frequency well we don't we didn't evolve with the press Pacific purpose of catching flies though it's a very funny thing to do when you're at home just try it you'll see that you will have a hard time doing that and there are reasons also for that perceptual reasons for that we'll talk about that another time however the our perception is actually has evolved in the past few thousand years well a little more than a few thousand years in order to maximize the throughput in interpersonal communication meaning understand each other speech being able to have a conversation in a crowded room like in this case being able to discern what a person is saying despite the noises around despite the reverberation in order to do this there are quite a few mechanisms that we employ and we'll discuss them all as we go along with the course but what the minimum thing that we can do is to try to make sure that our here is system hearing system is maximally sensitive to the frequencies that are produced by our speech and if you think about think about the distance between our ears think about the size of our lobes of our peanuts you will immediately recognize that the frequencies that are comparable to the dimensions are actually the vocal vocal frequencies so when it comes to detecting a speaker and getting them the most out of the conversation you will find that the speech will be I mean that our hearing system is maximally tuned in order to maximize this throughput in the conversation there are many other things that happen but for example the filtering per se is actually a bandpass filter so it takes away many of the low frequencies many of the high frequencies because most of the information in speech is actually between in a range of about five or six thousand Hertz starting from about a hundred earth so you don't need big ears fortunately but you just need the size that is adequate for the kind of frequencies in fact we'll if you think about you know yes that we talked about wavelengths wavelength is important to assess what the size of the hearing system microphones and so on is expected to be like well in our case the lowest when I speak like this my voice doesn't go below 100 Hertz a woman will speak about what one octave above my my frequency so let's say the women speak with a frequency that starts from 200 Hertz up and well so it really changes but let's say we can be safely safe with the range that goes from 100 Hertz let's say to five six or seven thousand Hertz when you really buy you're a smoker you will have higher frequencies but I assume that none of you is any more I'm hoping so the the shape of the the pin has served this particular purpose getting enough stereo image so the distance must be adequate for that having a little bit of focusing so the shape of the penis is meant to help you with that and then one other important thing the size of your ear canal is actually very small it's small enough that space basically doesn't matter anymore because it's always smaller than the wavelength of any frequencies that you might be interested in this is why I insist on the fact that you actually perceive sources in their location in space despite the fact that you truly have one dimensional signals so you have audio signals you start with a pair of audio signals so no space time signals not acoustic signals these are audio signals because the ear canal is too small for space to matter is this clear for you then eventually we're going to have to go back to discussing the so called HRT F the head related transfer function and the body related transfer function because our wrong response to acoustic field will be crucial to understanding how we perceive 3d audio and how we can reproduce 3 or 3d audio for example using headphones so that is actually all that we can say in this course about the outer hearing system the the outer ear the middle ear is quite important as well though we will just fly over it once again the middle ear is made of an ear drum so we have a membrane at the end that closes up the ear canal this membrane transmit the vibration as you can see from the image up there those yellow bones there are actually a mechanical transducer are you familiar with the notion of a transformer in electrical systems right you you know that a transformer is a nearly transparent electrical object that transforms voltages and currents in such a way that the product remains constant so you will have constant power so you don't have loss of energy in that operation but you start for example from high low voltage and low current and you go to low voltage a high current for example just to give you an idea so it's a it's a nearly transparent object that does a conversion without bit without being lossy in this operation well that system is a system of levers that is there is expected to do pretty much the same thing the reason being the fact that you have a large membrane at the end of the ear canal is the eardrum and a small membrane corresponding to the other side you can see there is a so-called whether there is the last bone or the yellow and the attaches to the cochlea which is still like organ inside your inner ear which has another little window it's similar to the idea of the eardrum but it's called the oval window which receives the motion on the other side and transfers it into the liquid that is inside the cochlea so because you have in air membrane which is the eardrum on one side and you have a smaller window that interfaces with a liquid on the other side you need to perform some sort of a conversion so you have to go from low pressure high surface so low force on eye surface which means low pressure to something that requires high pressure on a small surface right so the same amount of energy that needs to be distributed over a much smaller surface so you need basically a transformer and the levers that are depicted there the three ear bones they tend to perform this amplification of the pressure and consequent reduction of the motion in order to pass the energy into the liquid the perilymph that is included inside the cochlea is this quite clear obviously this operation is not completely lousy we we don't have you know ball bearings or anything like that inside so you have to be content with that with a certain efficiency or transmission of this energy and which is usually more than enough to be so sensitive as to you know detect rustling leaves at a certain distance or or a pin drop on the table on in a different room where you are so there there is quite a certain degree of sensitivity that we can attribute to this system despite the fact that these are very lossy systems after all let me stress this aspect ear bones says here really vibration and reduce motion why did I talk about transformers because the same thing pressure it corresponds to a voltage and motion corresponds to a current force velocity have the same rolls of voltage and current respectively you will see that there is an equivalence between mechanical system fluid dynamical system electrical systems which will be stressing a lot in the second part of this course so we'll come back to this equivalencies because we will be using also equivalent circuits in order to model basically physical systems such as musical instruments or the circuits that you use for making sound effects on your guitar and we come here to the third part of the hearing system which is given by the so called energy the inner ear is basically the cochlea the cochlea has everything inside so one of the most incredible transducers that you can find in nature and it's very interesting because it's basically a cone whose length is about 13 centimeters so 130 millimeters of length imagine taking this cone and start rolling it up okay let's that's how you build a cochlea you can do that in your own garage I'm kidding so the cochlea is this pipe which you know it's a it's really a conical pipe which is cutting to inside by a membrane which is called the basilar membrane the basilar membrane is there in order to collect the electrical signals on the bus the other thing that I that I mentioned before I like to insist on is the fact that the liquid that is inside the cochlea is called perilymph so the speed of propagation is not the same as in air in air you remember in water the speed of propagation is about four times the speed of propagation you have in air almost four times so this is barely is very similar to water but it's got other things inside and we we're not talking about that but speed of propagation is different you need a certain energy in order to start propagation in there it kind of dims down more slowly than in air so the absorption effect is a little more limited that's kind of cool inside the parallel therefore we need to have something that catches these vibration these way pressures the in and in order to do so you have basically 30,000 hair cells that they're also called neural receptors those 30,000 30,000 hair cells actually behave like little filters actually resonating filters to be more precise imagine each one of those hair cells to be pretty much like a like a mass a little mess with a spring underneath all right if you build a system like bow there is also a little bit of a friction but if you have a system that is built mechanically like a mass and a spring it behaves like a second-order oscillator so you give it a little shock you will see that it will start oscillating obviously in this case they moved this way okay so they are fairly rigid fairly but they tend to vibrate of their own accord so if there is a stimulus stimulus coming like a wave front that reaches them and or if there is a like an acoustic stimulation they will start oscillating depending on the frequency content of the other stimulus imagine being in a stationary situation where you have a periodic signal or like a stationary signal that reaches all the all of these cells all of these hair cells well depending on which frequencies are present in the stimulus you will have that only some of these hair cells will start vibrating those that are reasonably well tuned with the present with the presence of frequencies in the stimulus itself correct so if I have a sinusoidal signal that I'm using in order to excite the vibration of these hair cells so I'm listening to like a tuning fork just to give you an idea only those cells that are at the same frequency vation frequency that are pretty much of the same frequency of oscillation of the tuning fork at 440 Hertz they will start vibrating because we're not so infinitely precise and having the frequencies each one of those cells will actually be sensitive to a number of frequencies around their center of ocean oscillation so that I will have a certain type of cells that vibrate exactly a 440 but others vibrate a little less there will be less sensitive to frequencies at 440 which means that you will have sort of a bandwidth of sensitivity for each one of those hair cells so as we can imagine we have basically a filter Bank your receiving system is basically a filter bank made of a bunch 30,000 to be precise filters that are tuned around different frequencies and these different frequencies are regionally distributed over the basilar membrane actually they will they will have a growing frequency of natural oscillations that will go from the lowest frequency around 20 Hertz let's say to all the way to 20 K maybe 22 except those are 22 will die very soon your frequency sensitivity will of course diminish as you grow older to the point in which around 40 or 45 years old you you can only perceive around 16 kilos and maybe 14 when you're 60 to give you an idea but still it's a pretty good range if you think about it there are quite a few things that we need to say about there the frequency distribution over the surface of the basilar membrane remember I said before that you have quite a lot of room for place in 30,000 of these filters and the room is about 130 millimeters one thing that you discover right away by simply detecting one of these you can do that with any of your friends is to measure the resonance frequency of them and you will discover that every three point five millimeters the natural oscillation frequency will double so it will go to let's say from 440 if you're you know catching the the middle a of your of your piano as a fundamental if you move three and a half millimeters three point five millimeters you will get twice the frequency 800 and Heretic and an 80 you move another three and a half millimeters you go at 16 and you double again the same frequency so you go at 17 hundred and sixty okay and so on this is kind of interesting if you think about it because it's telling you that for every fixed interval I am growing by in sort of an exponential fashion and I have a exponential growth of frequency and this is why you have the piano organized that way think about the the keyboard of the piano it's organized you know in with the same kind of exponential progression if I move this is the position for one octave if I move on the keyboard every time of one octave I'm actually moving on the with the fundamental of regular intervals on the basilar membrane I'm moving every time three-and-a-half millimeters because one octave corresponds to doubling of the frequency so get used to the term octaves because octaves are very much used in in sound processing and they come they all come from the fact that we have a an exponential sorry they yes an exponential progression in the like a geometric progression in the in the frequencies by moving every time of the same interval on the basilar membrane so we have to use octaves we have to use exponential growth in that case is that clear so far I think I said pretty much the important things about this but I haven't talked about the consequences yet the fact that we have a filter Bank with heavily overlapped filters it's going to be it's going to you know bring us to interesting conclusions in our perception on one hand we are we have a sensory system that is imperfect because we don't have highly tuned filters on the other hand we have a sensory system that is very robust so if anything happens to your to your hearing system may be locally in frequencies there is always a lot of redundant so you will still be able to function in society have a conversation listen to music and so on now we can talk about how this is going to happen is actually quite important for example are you familiar with the well with the reasons why you might have the hearing problems here in defects well there are many reasons why you might have problems in hearing definitely one of the reasons that that should worry you the least is being exposed to blasts because it doesn't happen that often first of all but it's not that the reason why you have damaged hearing system you might have a damaged hearing system due to viruses due to illnesses of some sort that is also not so frequent the reason that is frequent the the most frequent reason why you might have hearing problems is actually a constant exposure to loud signal not necessarily super loud signals but loud signals so in order to understand this mechanism let me tell you a couple of things first of all how because we haven't talked about it yet how do you convert mechanical oscillation on each one of those neural receptors that are present on the basilar membrane into electrical impulses because that all the brain is able to to read and interpret the brain needs to receive electrical impulses well it's very simple to be honest if you take one of those hair cells think of it as you know like a little rigid bar imagine that rigid bar being shaped like a reverse T to give you an idea it's not the way it is if they actually shaped in a slightly different fashion there they're there they're building in a you don't even recognize it as as an oscillating if you look at it have a microscopic image I'll show it to you but for the moment just for the sake for peace of mind think of those as your rulers those are you use for draw straight lines and imagine making it vibrate okay and believe the the ruler one of the ends there is a T okay now this T does this you know as you as it vibrates this way it will move this way underneath the T because this is a rigid portion there are some neural receptors or neuro emitters in this case the neural emitters when they are squished when when you step on them they will react by basically firing and an electrical impulse as simple as that so we are just we have a bunch of angry electrical limiters that when something steps on their toes they start firing there are their electrical impulses that's how it works so if you have a very strong vibration you will step on their toes quite often right and you will step on their toes quite heavily which means that the density of electrical impulses that are fired by this system will become higher larger and larger so you have a high density of electrical impulses the brain will not count them but it will look at the density how many impulses do I receive every time unit well if it's large that means I have a strong you know excitation at that frequency if it's low I don't have much of a strong excitation but now how do you protect yourself from this kind of situation well we do have protections in every other you know sensing system for example our our eyes are equipped with our equipped with with an iris right you think about the amount of light that there is outside of the door and when you go outside is orders of magnitude stronger than the lighter you have inside that's the reason why when you walk in front of a window even with the light on and outside there is Sun it looks black because there are orders of magnitudes of difference in in the intensity of the light from outside to inside in order to equalize the sensation there is an iris that closes and opens depending on the simulation of the light we have a similar mechanism in the with our hair cells the hair cells are possess sort of an IRS which is based on it's like a faucet that is based on the blood duration when you have when the intensity of the firing becomes too much the brain in order to reduce the intensity and kind of avoid sensory overload reduces the the blood aeration of the basilar membrane and it goes into zones so if you reduce the blood duration the basically you render your your head neural emitters a little more dormant so they will produce fewer fewer electrical stimulation electrical impulses that's an interesting way of doing things but it's also dangerous and there is not much you can do about it I mean the brain shuts down the blood and it shuts it down completely when the type of array of acoustic stimulation goes beyond a certain threshold so if you are if you are operating a jackhammer and you're breaking the the the asphalt in front of your of your house well you you better wear some protection because that's going to shut down all of your blood Eurasian system well you don't do that there often but people that work with that every day will find that their brain has shut down for a long time there the blood duration on there on the basilar membrane now how long it takes to rip restate to restart the blood circulation well it I can give you an order of magnitude if you shut down the blood for let's say an hour because you've been exposed constantly to an hour of sound it takes you about three to four hours to go back to normality so your hearing system will be diminished for a while problem is if you keep doing it if you keep doing it as soon as you restart the noise it will shut it back down and eventually what's going to happen it's like plants if you don't water your plants they will die and it will happen the same way to your hair cells and it's pretty interesting to see what happens you will start seeing your health cells floating in the para length so will detach literally from the vascular member and believe me there is no surgery that can place them back in the where they were it's like losing your hair so let me give you some numbers so you have an idea in Italy about 7% of the younger younger people have hearing problems it's a horrible number think about 7% you know what caused it but simply keeping music too loud I mean just especially when you have you know in ear earbuds or headphones it's a problem because when you go outside there is traffic noise you pump up the volume the volume in order to hear the voice speaking or even more to hear the music undisturbed and that is a primary cause of hair loss much more that we can think and to give you an idea of this what this turns very quickly into just think about what happens in the u.s. in the u.s. younger people have hearing loss to a much wider percentage but I'm talking about 14 to 15 percent that's really dramatic because these people will reach my age and will not be able to enjoy music anymore and believe me it's not a good thing but in some cases not even have a conversation and remember when you if you think you can solve problems by simply putting on a hearing aid believe me you don't solve the problem because you are trying to reconstruct information that is not there let me show you an image this is a microscope electron microscope image of a cochlea so I looked like I lied for about an hour now because you don't see any of those hair cells that look like little rulers planted in in a bed of rock no it doesn't look like that as you can see actually as a matter of fact the the cochlea is contains a double layer of oscillators basically and there is also to obtain to stratify also the dynamic range of the system so you need to have a certain geometry in order to maximize the dynamic range of the receptors but I won't get into the details of this I just wanted to show you what it looks like in real life I have a slider that forgot to remove and it's a once again one of those slides that doesn't serve any purpose whatsoever so just ignore it and I wanted to come to discussing some of the consequences of the structure of the hearing system so the first thing that I wanted to tell you that derives from the fact that you have receptors that are not infinitely selective but they have their own bandwidth is the so-called first order effects of the the first order effects of superimposed frequencies okay imagine having two sounds then we'll discuss what it means in terms of physiology of the hearing system you do now have two sounds that two pure sounds two seniors are those sounds that are very close frequency to each other so f1 and f2 so we assume that the distance between f1 and f2 is very very small example around five ten fifteen Hertz no more than that now if you try that you will perceive a single sound at a frequency F equal to F 1 plus F 2 over 2 so it's the average between the two which is modulated at a frequency which is f1 minus f2 over 2 now why is that well there is a simple explanation but it starts from an assumption the assumption is the two frequencies are now hitting the same hair cell and the hair cell because of the two frequencies are very close to each other and the selectivity of the oscillation a selectivity of the filter is not small enough not high enough the the same hair cell will perceive both oscillation so you will perceive the sum of the two sinusoids if we had an infinitely selective system the first frequency will be perceived by one hair cell and the second frequency will be perceived by another hair cell but this doesn't happen because the two frequency are so close that the same hair cell will perceive the sum of the two so just by saying this you can see immediately the result the consequence of this you have let's say this is the first frequency sine of Omega 1t the the other frequencies is sine of Omega 2t and because each air cell that will oscillate will perceive the sum of the two more or less you can easily use any other trigonometric you know rules in order to turn it into a product of two sinusoidal signal for example you can write it as 2 sine of the half some multiplied by the cosine of the half difference correct that is what I was mentioning before the first term is the note that I here which is that the average of the two frequencies but there is a term that oscillates very slowly we said before let's say a hundred Hertz for the first frequency and a hundred and ten Hertz for the second so the oscillation will be at a hundred and five which is the average of the two but the modulation will be at ten divided by two Hertz so at five Hertz so I will hear something that goes up and down in in volume at half that that that speed half the difference of the two frequencies as a matter of fact I will actually hear it not a half but it can hurt because I'm sensitive to the power of that signal so it will go up and down at twice the frequency which is 10 Hertz I have an example here which I hope I'll be able to play because I didn't test it before the beginning of the class let me see if it works there is a door open so I don't know if it works but let me try here's what what I'm going to play we have 100 and 110 Hertz so it's exactly what I mentioned a minute ago this is the type of when you when you show the combination of the two cosinusoidal of that combination so it's this particular formula showed up there as you see the sinusoid is at five words but the magnitude because of the reversal will be a ten Hertz okay that make sense all right so let's play it let's see if it works hopefully it will do something [Music] it's a little distorted but it should give you an idea we started from two sounds at a hundred and one hundred and ten Hertz and then the hundred and ten starts going down in frequency until it approaches a hundred Hertz so he can hear that he starts from 105 and in fact it's Friday detuned and then it goes down it approaches a hundred Hertz but it stops beating you lose the first order beat musical beats let me try again to see if it if it gives you a better impression [Music] [Music] okay so that's it's interesting to notice that that thing that appears to be so dynamical is not dynamical at all the signal is a constant signal we just send two constants enjoyed and we perceived something that you know dances around a great deal we perceive the signal as dancing around let me be careful about this words that how we perceive it it's not the way it is as a matter of fact these sort of things are also even exploited by for musical effects for example jean-claude ICI is a very famous musician who actually devoted a great deal of his of his activity in trying to create a catalogue of sounds and examples that made people that worked in this area go crazy but it was also funny as a this was the example number and they're all numbered so you can do ok let's let's work at the example number 276 let's see what we can do and you discover a lot of interesting things that he did so in particular he came up with some ideas on how to be creative about musical beats this is an interesting example that I stole from Perry cook from Princeton University that precook is a famous researcher in Princeton University's famous also for the virtual the virtual poly effects so you know for the effects are mr. Foley is the first guy who did audio effects in the movie theater without having any sound so it was using coconuts like in to make the gallopping horses sound he was using you know salad leaves and many other things and the stones all kinds of stuff to create the background noise and you had a tool box that was kind of fun that you could use in order to do this digitally it the only thing is we did a bunch of very interesting things but anyway he went and dug this example out and this is kind of interesting because it has basically a combination of seven sounds each one of which has seven harmonics inside okay so the choice of seven is not incidental to be honest seven is is a number of it is a number that will explain some of the things later first of all it's a prime number and when you do the combination you will see that it creates all kinds of varieties of musical beats anyway you're familiar with the idea that a complex sound at a complex stationary sound will have multiple harmonics inside right this is part of the representation of periodic sounds using the Fourier series so if you have a sound that had seven harmonics it would be richer than the sound of a simple sinusoid because it will also have multiples of the fundamental now let's take the first one the first sound is a hundred Hertz so we'll include we'll also include the first harmonic at two hundred then the third at 300 400 500 600 and 700 so you have seven spectral lines superimposed to form the basic Lego block for building this combination then if you take a second one if you take a for example the same kind of sound with a slightly different fundamental so the lowest of these frequencies and you place it at 100 point one Hertz the harmonics would be at 100 point to 100 point no 200 point to 300 point 3 400 point 4 and so on so if you do if you look at the the the the if you look at them pairwise you will see that the the beat in musical beats between the first fundamentals would be 0.1 art and the distance between the two but the distance between the two first harmonics would be point two and the distance between the third harmonics would be point three and so on I imagine doing this with another one such sound 100.3 a hundred point two in which case you will have the harmonic set at 200 point for 300 point six 400 point eight and so on so now we're playing around with seven on such sounds with different fundamentals 100 100 point one and the point two hundred by three four five and six so each one of the harmonics will be a completely different frequencies because they will be proportional to the fundamental so they will form all forms of musical beat patterns at the end what do you expect to happen when you put them all together but you will have a very varied variation of musical beats that will change the color of the sound constantly because the weight of each one of their harmonics will change over time at a different speed a different rhythm compared to the others and do choosing fundo as we say in Latin music wise people said in lighting let's remember that the rhythm of this variation will be after a we have seven of them right so the you will you will not have a regular rhythm you will have a natural rhythm so the balkan people will be very happy because the tradition of the afteri rhythms comes from there well I just discovered just thinking about you know take five day Brubeck right there was a five for rhythm you also wrote a blue Rondo alla turca seven eight why do you think he was using those reasons I just found out that he was doing the military surfing service in all the areas of the Balkans and so that's why you have that canister mmm and he's not the only one you think about heavy shy : who there's a lot of interesting Pali rhythms that have multiple unusual rhythms he also bases is a multicultural Calder cauldron that puts together many calls in particular rhythms from the Balkan areas as well so that's kind of interesting to see anyway let's play this so you can have an idea of what I'm talking about you will first hear the basic Lego block the hundred earth sound then you will have all the combination of all of them together then you will only listen to how the seventh harmonic if you remove everything but the seventh harmonic of all of them you will notice they will have exactly the after a rhythm will be more audible how you have the after E rhythm at the end let me try this basic sound now all together [Music] count it will be seven okay they're after right one two three four five six seven so if you want to have unusual rhythms in the changing of the color of the sound work with multiples of the the root so if you want a natural rhythm work with distances that amount to seven okay that's an idea so kind of interesting to see what you can do with this sort of things I mentioned before there are people they actually do it on purpose of course there is a lot of stuff you can do in a in a electro acoustic music but even in you know more popular music you do that you can exploit these sort of things in jazz i mentioned before i wish i Cohen there is a piece that I can remember he was playing them he played with the bass and singing just bases it was a it been so long does anybody know that song it been so long well you're a bass player right so you must've played that right we must it's a must you you you want to learn it's like a stairway to heaven for a guitarist he goes to the store they kick you out because you play stairway to heaven same thing for you the stairway to heaven will be it's been so long so it's been so long there is a moment in which you he plays simultaneously two notes so they will form musical beats and surprisingly enough the distance ative the to note is exactly the rhythm of the song so that's done on purpose the musical beats don't stop at proximity they also work at Priscilla T between harmonics or proximity between multiples of the other notes for example if you take two sounds as f1 and f2 + f2 is slightly off - f1 for example a hundred Hertz and 200 and fibers to give an idea so it's slightly off twice the frequency of the first one you no longer have the problem of that the same hair cell will perceive the will perceive the music booth at the same two frequencies simultaneously you will have two separate regions of the basilar membrane that will be excited by the two tones independently so there is no longer mixing in air thus clear right the reason why this doesn't happen is because the bandwidth of each one of the hair cells is more or less one minor third am I saying something that makes sense to many of you but of course you should know so I don't even ask you do you know what I mean with a minor third or a major third that's pretty much the bandwidth of each one of your hair cells just have an idea and it changes as you move in frequency it will change so if you have high frequencies one a minor third or a major third depending on your sensitivity will be quite wide in terms of frequencies but the low frequency will be quite compact okay so because we're talking about two frequencies are approximately one of table octave apart this is not going to happen that means that the two frequencies will excite to separate clusters of hair cells so you can't have the same kind of musical beat that you have before on the other hand the brain is actually quite amazing the the brain will receive these stimulus in the form of electrical train pulses pulse trains sorry - pulse trains and despite the fact that they come in let's say in pulse modulated form the brain will still be able to track the phase variations of the two signals and how the phase variations will interfere with each other will form the the wave shape let me give you an example but first of all we have a we will have a tone beat at a frequency that is now f2 minus 2 f1 but instead of being a magnitude beat it will be sort of a phase beat so there will be something that changes it's hard to detect for how but at a rhythm that is basically at twice the free f2 minus 2 f1 okay so if it's 205 and 100 it will be a 5 Hertz you have to be careful because like I said it's no longer amplitude modulation like in the case of the first-order but there are cyclic changes in the wave shape let me show you what I mean just by showing you how the two things mix together as you see the magnitude is nearly constant but the phase changes you have a change in the in the wave shape can the brain you know track such changes even if the it's working at the brain level now can you change can you track it but the answer is yes now I doubt honestly that I will be able to to render it well to you just using a pair of speakers in a reverberating room I try anyway but don't have and not having high expectations on that but definitely if you put on a good pair of headphones you will be able to hear the cyclic change and at the end of the class if you have a earbuds that you want to try just let me know play this for you with little hope [Laughter] so close you can hear it but from there probably now right I try again if it doesn't work we'll just move on can you hear it a little bit I can but I'm in the back of them very close to it anyway so that's the idea so it's sort of a demonstration of the fact that we do have a very sensitive machinery installed in our head and so we can actually track such fine-grained details in this in the signal people used to think that we're not sensitive to face especially at high frequency we're not sensitive to face I heard it so many times especially when I was your age people believed that all that matter was the magnitude spectrum the answer is no it's not true it's absolutely wrong we are very sensitive to face you can do a lot of things to change the shape of signal just by operating on the face you can even reconstruct the speech just from the phase of the signals there are lots of things you can do so it was fundamentally wrong if you look up the dictionary and read some of the descriptions that are of some words like Tambor dunno the color of the other sound there is no definition that the people can agree about in describing Tambor so be weary of what you read especially when it comes to reading definitions on dictionaries and I will say something that my wife will hate me for saying this because she's very strong believer in you know don't use Wikipedia as a matter of fact today things have changed but the only place where I could find a reasonable definition of Tambor was actually Wikipedia all the other dictionary definitions are really wrong so sometimes can be useful also to read on Wikipedia some things will come back to the definition of tumblr for the moment I just wanted to focus on this idea there is one thing that I wanted to mention which is concentrat the notion of consonants consonants consonants well first of all let me tell you a couple of things I'll do this on the on the blackboard if I can find a piece of chalk yes I have a very small piece of chalk so I'll write half of the what I want to say with the chalk and the rest with my fingernails so be prepared okay so here's the basilar membrane the basilar member we indicate the geographic position on the basilar member using you know X expressed in in centimeter in millimeters just to give you an idea if I produce and let me use another axis here okay the other axis is f frequency in this diagram I want to write what happens when I take one specific hair cell for example the one that is maximally tuned around 1,000 Hertz just to give an idea and how does that cell respond to like an impulse for example well what I want to find out is what the frequency response of the hair cell is therefore this is 1,000 Hertz and I have the maximum response there and then this could be the frequency response of the hair so okay now how does this translate onto this diagram in order for your brain to interpret it to interpret what's going on if you know that each hair cell will have a frequency response ability it's kind of bell shape okay think of it in these terms now let me do the opposite I'm here I want to know what the fire ink density is so the the brain receives roughly just the training a bunch of impulses electrical impulses from each one of those cells because we have 30,000 of them they will be very densely packed in this interval correct so if I now produce a tone at a hundred and a thousand Hertz so I'm generating on the basilar membrane a tone at 1 kilo Hertz what will happen to the density of those to the firing density well it will take a similar shape though the meaning will be kind of different so the hair cell that is placed here it will be in the location X such that the resonant at the maximal resonance will be X 1 kilo Hertz so this is the position of that particular hair cell that resonates at a natural frequency of 1 kilo Hertz on the other hand there will be a bunch of others that will react to 1000 Hertz but a little less so you will have a this will be the maximum then you will have a few more around it that will react to a thousand Hertz so that firing distribution actually looks very much like the frequency bandwidth of each one of those of those hair cells except here everything is written in frequency like Hertz and here it's written in millimeters but what I'm saying is the width from here to here corresponds to the location the distance of between the all the hair cells that correspond to this bandwidth the reason one-to-one correspondence between the two remember they're not the same diagram so don't get confused this is frequency response of one hair cell and it will change as you change the hair cell so if I if I if I consider another hair cell will be slightly different for example at 998 Hertz the next one right to give an idea so this tells you that our filter Bank is not infinitely selectiveness fart when it comes to sounds the ideal system would have a bunch of Dirac impulses right spikes give an idea now how does this translate into understanding when there are multiple tones what does the brain do well when there is a single tone the brain that's something that is very simple it looks for a peak and it tracks the peak right because the situation is similar to this it's pretty easy to find out at what frequency the sound the pure tone is because you just have to look for the maximum of that shape but when there are two tones what happens the same diagram with two tones will look like this superposition of two that are like this right so the result could be like this if they're far enough you might have something that goes like this if they're farther again you will have something that goes like this even deeper than that can you see that now how hard does your brain have to work in order to tell the two frequencies apart it really depends on the depth of the valley between the two mountains between the two peaks if the two peaks are too close you won't be able to distinguish them but if the two peaks are far enough you will have a valley in between it your brain will be able to tell we'll say okay there are two sounds here and I can clearly tell because the shape is the bimodal shape of two tones that excite a system that has limited resolution this ideal resolution we'll come back to that later because now we're going to have to talk about you know frequency time processing so short space for your transform short time for your transform so analysis of the spectrum as it evolves over time all this sort of things so resolution will be your friend for or at least another 10 days so be be happy with the idea you are going to have to walk hand-in-hand with that for a while but once you grasp it it's going to be so much easier later on but look at this this is not possible to discern this is harder to discern this is easier to discern the level of hardness or difficulty is also kind of indicative of your level of discomfort when you perceive sounds that are together it's not the only reason it's one of the there is a strong correlation between your level of discomfort when you have tones that are next to each other related to how hard it is for your brain to tell the tones apart in a very crude fashion let me tell you that the consonants on how is very sensitive to this idea when you have two tones two simultaneous tones and you want to tell them apart well the the discomfort that you have and trying to do so will be called a dissonance okay we call it dissonance and the complimentary concert concept it will be consonants of course so if you have two tones at the same frequency of course that's a very definition of consonants they're the same tone so there is no match you can tell if you separate them about a fourth okay you have maximum dissonance pay attention to the two tones out now today is no longer true but you know when you bang on the piano and do bang on two tones that are right next to each other usually you say okay I'm doing something dissonant here you don't say so but usually the matter will kick them back to the bedroom but that kind of tone is usually quite dissonant it's considered dissonant and if you go that oh by the way I didn't say about a fourth of what a fourth is more or less out tone a fourth of the critical bandwidth what is the critical bandwidth well it's exactly this frequency response of each one of your cilia and I've already spoiled the surprise because I said before then it corresponds to a third minor more or less to a minor third so if your brain is see the limit of the critical bend would correspond perceptually to reseed a flat even idea more or less so you have a perceptual idea of what it means so but a fourth of critical band is a fourth of a minor which correspond to a half tone right so if two tones divided by 4 is 1 half tone and there it is in fact I just played it and you correspond it to a house tone half critical band is a distance of one tone more or less more or less it changes from been due to bandwidth but it's more or less one tone and people consider it 40% consonant what does it mean that means that someone and usually these are American universities in the 70s and 80s that did a lot of perceptual experiments were extremely useful for designing algorithms for compression and encoding well they asked people there they paid ten dollars for each person to go and undergo the test and the test consisted in pressing the button seen how dissonant it was after being exposed to a bunch of tests tones that simple enough you might think that it's kind of a stupid way of doing things but believe me when you have a sample size on in the order of several hundred people you can already establish something useful so let me play the half critical band a little off but it's about one tone 3/4 of a critical band it's a third minor but not high enough and one critical band is a hundred percent consonants which is in this case more or less one minor third okay so you go from hundred-percent consonants when they are when they correspond to each other let me see if I hear you go to maximum distance dissonance here which is half a tone all the way back to perfect consonants which corresponds to the minor third because it you know we're used to that it's also related to our own background musical background we've all been exposed to tonal harmony so we all know where a minor third is we all hear it every day and sad songs and so we perceive it as a normal thing as a consonant thing what about now now this is consonant so we are considering two tones at the same time now here we want to consider a single tone single tone for asthma it means that understanding what pitch is pitch is the subjective impression of frequency which knows are my banging are right on the piano so subjective impression of frequency well it really is less simple that it might seem to understand what it is because we could say that if we have a complex sound that is made of a bunch of spectral lines the fundamental frequency tells you what the pitch is but it's not necessarily true and actually you are exposed to this kind of situation because you speak on the phone all the time and usually on the phone the fundamental is missing not because the person is missing a fundamental because the tone the phone cuts it off and not because the phone is not able to produce it but because people don't like it you might be surprised about this but a few years ago there was a study in which they kind of interviewed a bunch of people asking whether they agreed about the possibility of reinstating the the fundamental as part of the you know it's important to have a nice speech now the night nice quality speech and they did some perceptual tests and people didn't like it people preferred to have their voice altered by the phone this actually dates back the over one century ago that for the early phones were based on an electric carbon electric microphone Carbon grant granule microphones that could not produce anything below 250 or 300 Hertz so anyway the fundamental disappeared so your voice was very nasal very high-pitched not not I it was still the same pitch same period but didn't have any low frequencies in it and in one century we learned how to perform the fun directory reconstruction on the fundamental believe it or not I have an example that I play later what you can tell it's the same pitch but it just devoid of some of the frequencies so we are able to detect the pitch even in the absence of a fundamental so yeah people chose voted not to have it basically and the only case in which you hear full bandwidth is when you do teleconferencing when you do no Skype or whatever it is that you use then you hear the fundamental but to people it's like the choice of Noah reinstating the fundamental was like the choice of doing teleconferencing without the video you know you're in you're at home with your penis slipper maybe you're a woman and you're wearing big curls and you're brushing teeth or you are in the bathroom while you're answering the phone people don't want to know you're doing it it's your problem you just don't want it to be you want to have your privacy while you do that and so in a way you are not giving up the level of privacy that is afforded by not having the fundamental restate so people chose that so that's a typical case where you don't have the fundamental actually not having the fundamental trained you to learn how to reconstruct it and now we're all you know able to do that and so we can use fundamental detection in order to do pitch pitch tracking you have to do something else and we'll talk about that when we talk about pitch tracking okay it's a psychic oosting variable so it really depends on on your psyche acoustic subjective sensitivity for example if you have perfect pitch okay if you are a cure saluto in driving it's a perfect pitch you might be more sensitive it might be a disadvantage to you as a musician sometimes but you are in a situation of the damage and the sense that you will be able to identify more notes you will be able to identify more changes the pitch of a tone allows you to identify the note on a mold musical scale but also that depends also on the culture for example middle Easterns are far better than we are in detecting pitch because they use scales with many more tones with multiple intermediate tones so they're trained to that we're not we have we're used to detecting the same tone that there are on the keyboard on the piano keyboard 88 tones maybe slightly more because now you can extend that virtually in other ways so let's say that you're sensitive to more or less 120 you're trained to listen to 120 tones your ability will tell you in a minute what it means in terms are the Hertz so I have some examples here they also tell you how complexity of the tone and be deceiving or can be decisive in assessing your own sensitivity to pitch however I just want before I play these examples I wanted to mention the fact that one reasonably widespread approach into measuring your sensitivity to pitch is the so called assessment of the sort of the just noticeable difference which is the smallest variation of frequency that we can perceive the experiment is very simple you sit down with headphones and I play one tone followed by the same tone and you say okay it's the same I play one tone and slightly different one you don't detect any difference you don't press the button and P and I increase it or do it random every once in a while you will perceive the difference and press the button right at the end you you collect all of your statistics with a bunch of people from different culture from different races from different genders from different all kinds of things and you do correlations and you measurements and correlation so at the end you will discover that some very obvious things and the lowest perceivable frequency is actually quite low it's 20 to 30 Hertz the highest perceivable frequency is obviously between 15 and 20 we all know that we also know that the ear is maximally sensitive to the frequency to the pitch in a very specific range which is one two point kilohertz didn't I tell you that I tell you before I told you before that we are machines that are maximally tuned for interpersonal communication that's pretty cool and in fact one 2.4 point killers one two four killers is exactly the range of the speech but the J&D is actually what is normally used in order to assess the smallest variation of frequency that we can perceive and they'll give you a number in a minute but I'm only after putting my hands forward and telling you about some disclaimers this is only true in very simplified conditions because our perception of pitch is extremely varied depending on the content and in fact let me play this example this is a pure tone hopefully will not distort my hope was not well placed this is a complex tone but not that comprise just a triangular wave shape so it has a bunch of harmonics above it I don't know what this distorts so much considering the brand then we have this example it's pretty cool actually I like this example so I'm going to play twice for your own enjoyment this is an example that has a 1 kilo Hertz burst that lasts about 40 milliseconds initially it's a train of bursts and it will shorten and shorten progressively as time goes by until it reaches the duration of 2 milliseconds and then I will ask your question so be prepared I won't grade you on that ok what did you notice you notice at the beginning you could tell what pitch there was and you noticed at the end that you couldn't tell anymore what pitch was and it wasn't the fault of the speaker's believe me at least that and that wasn't the speaker's fault it's actually due to the fact that your ear has an integration time ok do you meet you know what I mean when I say an integration time remember what the impulse response is ok look at this shape this is the frequency response what does it look like if you look at it does when we remind you of a Gaussian so because your fresh memory when it comes to math what is the inverse Fourier transform of a Gaussian you know if you want to find out what the impulse response is I knew you didn't remember so you better go back in and find out the Gaussian function is an auto function of the Fourier transform therefore if you take a Gauss here and take the Fourier transform you get a Gaussian the only thing is that the signal would have a spread and the Fourier transfer will have the inverse of the same spread so if I make it spreader if I spread it out and make it wider you will get narrower in frequency just to give you an idea okay so remember that gaussians are quite important so if you look at the frequency response you have a bell-shaped function if you look at the impulse response it will still be about function more or less more or less so you will have a duration that duration is sort of an integration time now the duration in this case is in the order between 60 50 60 milliseconds if anything doesn't last long enough it will be very small compared to 60 milliseconds and you will Canada you the year will not have enough time to be able to detect frequency content will not have time to be able to discern frequency content actually for that kind of duration of two milliseconds all the possible tsylia will be excited because it will receive a Kim pulses it will look like an impulse when you have an impulse it contains all possible frequencies inside and so it will excite all possible hair cells of your massive basilar membrane that's why you perceive it as a white noise or an impulse noise did you notice that now that I told you all these nice things let me go back and play in the game so you will notice many more things that even notice before hopefully okay let me go back it's this one okay now it should be pretty clear if events are not long enough we won't detect them properly so we need to have we have a loss because of our hearings is that we have a loss of frequency resolution but also a loss of temporal resolution so our ability to detect events over time is altered obviously we are particularly sensitive to pitch when it comes to modulated signals there is an interesting example which is the cello does anybody play cello here okay well enjoy the sound because this is one of those cases where the pitch is technically hard to detect but we perceive it so naturally because we're used to that kind of tone so a sound that is so complex and we yet for us it's very natural to detect the pitch complex because there are modulations of all sorts you are creating the broto with the motion of your hand you're also creating harmonics that are excited you are creating interference with the harmonic sympathy vibration of the other strings and you're creating overtones so it's of extremely complex sound there is the intervention of the whole body of the instrument the changes in others the frequency content of the sound it's also not a harmonic sound is slightly an harmonic so the more the frequencies the multiple the the partials that are supposed to be a multiples of the fundamental are not because the rigid rigidity of the system the alters the pitch of the harmonics and there are situations where the situation gets even worse like in harmonic tones of a gamelan gamelan tone we are still at the point in which we detect the pitch as we would with any other mallets for example so if take a vibraphone if you take a xylophone or marimba whatever you want you will be able to detect the pitch despite the fact that the partials are definitely far from the multiples of the fundamental so understanding how our perception of pitch four works for this sort of tones is extremely difficult yet we can still do a lot in a very rough and engineering sort of way oh by the way I mentioned before the that I would show you the comparison between the missing fundamental and with the fundamental I have two examples here this is the sound with fundamental and this is the sound without okay it's the same pitch but it's just a weaker in many ways but we're missing the fundamental in the spectrum and yet we perceive it anyway so it's not related to the presence of the absence of the fundamental so I just wanted to prove the point by doing that I said before that despite the complexity of the notion of pitch we can still say something about the our sensitivity and in a very rough and some cursory sort of way so I mentioned before the term J&D which stands for just noticeable difference and I also told you how to measure it you can do it like I said we are we all have different J in these especially if you're a musician if you're not a musician if you're trained that your ear is used to listening to music if you come from the Middle East or if you come from from Italy if you come from the US or if you're a male or a female oh yes if you're male or female there are differences women are particularly sensitive to changes in frequency at higher frequency that we are because well it's nature again nurturing children even if today we are both doing the same thing male and female in the past centuries it wasn't that way and so our our hearing system has become accustomed to differences and has adapted to the situation yet if you do statistics and averages statistics on anything you want to get some measurement that makes sense and is safe for all possible situation there is a very very simple rule for medium intensity senior Zoids and I insist on the word senior Zoids the just noticeable difference is about five per thousand of the fundamental frequency of the tone well in this case of the frequency of the sigmoid because we don't have other tones other than the fundamental so which means that if you have a tone of 1000 Hertz we we can detect changes in the order of five Hertz if you have it at two thousand it'll be ten Hertz if you have it of six hundred earth it will be three Hertz plus or minus 3 Hertz but if you go below six earth 6,600 thirds it will flatten there so if you take five and it will still be three Hertz 403 Hertz so all the way to six hundred earth is proportional half a percent below 600 Earth flattens are three Hertz so that's the magic number you you might think that this is not very useful right now but you will find that you need it for designing systems that perform frequency analysis you want to know you want to design the length of the filter what design the length of the window you want design the length of the FFT not of the window in relation to the window in order to have an error of localization of Peaks that is below audibility in that case you have to know the J and D so I'm telling you a bunch of things but believe me remember that because otherwise you won't be able to design systems later on have you ever tuned the piano has anybody ever tuned the piano I have and I didn't succeed that was very very bad at doing that so I had to call someone else to retune it after I did it the first time so there is a any inherent difficulty in doing this sort of things so I have to warn you don't try it yourself unless you know what what I'm talking about there is a strong dependency between pitch and intensity that's one of the reasons why it's so hard to tune a piano if you tune a guitar is not much of a big deal student guitar you have an instrument that doesn't have a super high dynamic range so you can trust what you hear but if you are as an opera singer it's really hard to stay in tune if you are because you have a very powerful voice if you are a piano player you need to have tuning done in a certain way because there is a very strong dependency of the tuning of the frequency on on the of the pitch on the intensity on the objective intensity of the sound this is a very interesting diagram here that I'd like to show because it's very indicate about what's going on this diagram tells you that well first of all you have to objective measurements here sound pressure level which is an objective measurement and frequency change so this is expressed in Hertz Y free water how much do we need to change the frequency in order to perceive the same tone so the this these are curves that describe ISO pitch curves they describe how to alter the frequency of a own is such a way to be able to perceive the same pitch so normally diagrams the describe perception our diagrams they have two objective measurements on the axes on the abscissa and the ordinate but the curves are curves that describes something that doesn't change in perception so I have another diagram that I'll show in a few minutes this is one of those okay so you will notice okay these are these number represent what you hear for example the 150 says the it tells you what the frequency is when you perceived it in at a standard condition if you now alter the sound pressure level let's let's do this we take a tone at 150 Hertz in control condition we start increasing the sound pressure level and this then you have to reduce the frequency of this amount expressed in percent in order to obtain the same tone to maintain the perception of the same tone so as you notice if I am an opera singer and I hear my own voice singing which I hear very loudly because of the diffraction of my the borders of my mouth I buy singing louder I'm going to have to do some adjustment in order to hear my own voice in in tune I'm gonna have to lower my voice sing lower sing flat the problem is if you're sitting down there and I'm singing here you the tone that you hear down there is much lower than the tone that I hear so you will hear my voice being sharp so I hear it right and you hear it sharp if I'm an soprano singer which I'm not and I'm singing at a higher pitch and now I'm starting and I to my own voice because I'm singing louder so I'm saying is slightly Sharper in order to avoid the problem you down there will hear me slightly flatter because you hear a different level of intensity from the one that I hear so my adjustment would be wrong you see the problem so what I'm saying is that's why when you go to an opera if you are in an opera choir and so you sing along with the singers the you will notice a bunch of things that Dominic doesn't make sense to you so the the usually the opera director the director of the opera will tell the the bass singer to sing flatter and the then issued and the and the soprano singer to be to sing sharper right in order to compensate for the lack of judgment and the opera singers will have to adjust they will have to learn to sing out of tune in order for other people to perceive the correct the correct pitch believe it or not this is just one example another example is the piano you have a piano that is tuned in an environment that is highly reverberant and very small will be much louder than an auditorium well you're going to have to tune the piano using frequencies that are recorded for that particular piano in those conditions of course now you can always say a real piano tuner will now use an instrument yes if you're willing to pay for eight hours of work that's okay but if you're willing to pay 80 euros for two in your piano well you're going to have to be content with the piano tuner using a machine right in which case you will use a machine that has already recorded the particular model of piano that you have at that moment which could be I don't know a Yamaha c3 just to give  so picks up the Yamaha c3 and the environment that it's in in which case you will have a certain recording of frequencies after having done the perfect pitch pitch tuning or the on the instrument otherwise the same piano brought to a different base doesn't happen because you have to retune it anyway but if you the same piano into a different place it will sound outta tune not just because of temperature variation but because it was tuned in a different environment so actually this is this is also something that is a problem for the orchestration if you do orchestration you also have to keep and you're doing really refined stuff I'm not talking about simple big-band or other simple things where you can do perform a certain degree or self-adjustment but if you take the orchestrators of the early twentieth century ii there is a lot of thinking that you need to do because you have to group large numbers of musician doing the same thing and you have to be careful if you have big ranges of intensity you have to make sure that the perception will not sound out of tune I mentioned the last time I mentioned the bolero Ravel which goes from a quadruple piano to a quadruple Forte and that kind of variation is really dramatic in terms of pitch adjustment and most musicians do that automatically either if you if you play reeds if you play double reed you can do a certain degree of mouthing mouthing judgment also if you play you know trumpets or anything like that but if you play beyond a certain level you have to do some retuning on the fly and musicians can do that but you can't really test it so you just go by memory you take the you know you shift the the curve of your trumpet in order to make sure that you will not completely be outta tune sometimes you can even do that and in the bolero at the end there are some have intentional distances especially in the climax if you if you know the piece I'm talking about there are some trumpet dissonance is then placed on purpose in order to create the to force the listener to adjust to a new pitch so it's an intentional operation that sometimes you do in order to favor the auto adjustment of your pitch perception if you read the original manuscript you will find our notes that tell you that so pretty interesting you can learn a lot about psychoacoustics by looking at the original manuscripts you if you can find a copy of of the the orchestrators it's a Orchestrator is the quintessential engineer if you I'm having fun a little taking lessons on orchestration and it's really interesting because the the orchestrator thinks in these terms thinks in terms of perception things in terms of adjustment balance and it built the the entire foundation of the musical piece in a very architectural fashion you have melodic design and then you have to build harmonic orchestration of rhythmic foundations in order to hold out that design and it's really really cool to see happen going on and I'm going to take another few more minutes I apologize if I kind of go slightly overboard with with this I absolutely want to talk about acoustic intensity acoustic intensity is something you measure in a objective fashion but the perception of the acoustic intensity which is defined as loudness doesn't work that way these are very famous diagrams that are have been used constantly in the past decades there there are the so-called flattering Munson ISO phonic curves as before you have two measurements that are objective right so in the horizontal axis you have frequency in the vertical axis or in the ordinate you have DBS so you have loudness not allowed sorry the sound pressure level in DB s-- so both measurements are objective measurements and of course the curves describes something that is constant in perception exactly as we did before but this time we're talking about loudness so these are ISO loudness ISO phonic curves so how much sound pressure level do I need to change in order for my perception of the intensity to remain constant so if I you so first of all what what the phone it is a ISO ISO phonic curve so that means that the measurement of loudness or perceived intensity is expressed in phones which is the amount if you take a sinusoid at 1000 hertz standard value the measurement in DBS will be where you start from and that's expressed in phones at that frequency so you start from here we are here for example a 1000 Hertz here you have 10 so the crossing with the 10 is happening exactly at 1000 Hertz so this is 10 phones and then you move away from there by changing frequency and in by measuring the adjustment in the end allow in the sound pressure level that you need to do in order to obtain a constant loudness tone you do this for all possible intensity and you obtain this kind of diagram that means that the lower the curve the higher the sensitivity and incidentally you will notice that frequencies that go from say 100 Hertz all the way to six thousand seven thousand Earth's he's exactly where you have the maximum sensitivity which by the way corresponds exactly to the speech so once again our system both for listening but also for producing utterances will be designed in order to obtain maximum throughput in interpersonal communication so another thing that you might want to consider as interesting is if I record something in a specific condition so I'm in the studio and I am recording jazz quintet with a saxophone player you have a certain intensity of sound and so let's say you're working on one of those average curves let's say you're working at an average pressure of 80 DB so one killer so we're working at 80 phones of intensity but then you listen to what you recorded in the car and you want to hear the traffic sound outside so now the the level that you listening to is about 50 now because you're listening to the same content at a difference at a different condition that means that you acts as if you're as if the sound were going through a filter because let me see so you have to take the curve at 80 phones and then you take the curve at 50 phones phones and now you're listening to this what you recorded this way so you have to take it you have to take the difference between the two curves right and that means that that it's as if your piece went through a filter like this which is the difference between the two curves and incidentally it's as if you were band passing the signal it's been passing so you're lacking low frequencies this is why because of that operation you need to compensate you need to compensate through some filtering and there is a button if you remember it's called loudness which is in the stereo system in your car and so on long ago it was just a button that would you know pump up the batter the bases and they're thick and a little bit of the high frequencies and that's it but that's not what loudness is loudness is something a little more sophisticated now in the modern cars there is a measurement on the noise in the car and so you can do loudness adjustment so it really works like a physiological compensation filter the changes is frequency response depending on the level of noise in the car this is why it's called loudness because it comes from those curves these are the intensity thresholds in their extremes the top one is called the threshold of pain and the bottom one is called the threshold the hearing threshold and they actually migrate over time so they change they become you get less sensitive as you grow older you will get less sensitive so if you listen to your headphones too loud and so on so let me play a couple of examples and I beg you to be a little more patient with me because I have one last thing to tell you and I'll be very quick with that so this is an example that it's not going to work perfectly because I don't know at what level it was recorded but equal loudness example means that you are playing with directly with the compensation and it so we're playing a bunch of tones left like that this is already covered this is a bit too much there are two examples because you don't know why what Level II was recorded okay it was just approximate but I can guarantee you that these stones if they were played at equal magnitude you won't you wouldn't hear the low one of the high ones you will only hear the ones in vocal frequencies all the others would disappear so there is a strong compensation in order for you to be able to hear the low tones and the high tones okay last thing I wanted to tell you is this because this is important it's a so-called frequency masking remember I mentioned before that if you are listening to two simultaneous tones you have a hard time telling them apart when they are very close to each other remember why there is still the image here when the two tones are very close to each other a you have a hard time having this discrimination of the results this is the fire intensity as it changes with the location of the basilar membrane now in particularly if one of the two tones is very you know low in magnitude it won't it will have a very hard time surfaces from the mounting created by the first tone so in a way the presence of a strong tone will make it harder for a weaker tone to emerge and this is called a masking effect and it's due to the fact that you have a critical bandwidth that your receptors have a large bandwidth not a very highly frequency selective one now this also ends up affecting your threshold of audibility so if I already have a tone present at 1 kilo Hertz let's say this is now a linear scale that's why it doesn't look symmetrical that it's not a Gaussian the way you would expect it to be but so in a linear scale if I place a tone of 1 kilo Hertz fairly large very loud so let's say 60 DB is above audibility threshold so it's expressing in SPL the SPL is 60 DB well and I want to play a second tone in order for the second tone to emerge from that situation I will need it to be louder than it should be in order to be heard alone because the presence of the first stone will match the second one so it's as if the the masking tone lifted up the threshold of audibility the hearing threshold for the second tone okay so that's called masking effect I think I have an example I can play about this but don't worry I have it afterwards Oh before I play the example let me tell you that this masking works also over time so if I have a tone that is masking and I turn it off and immediately after I turn on the second one it's as if the tone was still there so it takes a little bit of time for your brain to adjust of the two the absence of the tone there is some sort of a hysteresis behavior on the part of the brain and if this might surprise you it's even anticipatory so if I turn on a weak tone and immediately after I turn on the the masking tone I remove even the weaker tone that started first so it's anticipatory in effect but it's the anticipatory effect is much shorter than the effect after words so if you want to picture the situation in a more in a higher dimensional space you can think of it like that your tone the masking tone is like a blade in the frequency time domain so it's a sort of a yeah blade lifted lifted up which lifts up a rug a carpet okay so you will have the masking tone that operates in frequency but also over time with even a slight effect on the in anticipating returns to close I like to play this last example this shows what happens with masking and then we'll close the lecture today you will hear higher frequency tone the masks lower frequency tone and then a lower frequency tone that masks the higher frequency tone the frequencies are mapped on the slide let me play it for you could you hear the second case was a slightly more difficult because of the noise in the room I played one last time maybe slightly lower in volume disappeared disappeared again so you know why it's important to know that this happens because we're right now we're working with the Fraunhofer Institute in Germany we have a contract with them so we have a close relationship with them they design mp3 mp3 is in fact an application of perceptual coding and it's an application that heavily relies on this effect so it's not just to know words floating in mid-air this is actually what makes it possible to design perceptual encoding systems that allow you to pack more content in the same space and I'm talking about an order of magnitude of compression I'm not arguing and I'm not talking about the quality of the result I'm just telling you that if you tried in any other way other than perceptual encoding something that exploits the weaknesses in your perception you would get worse results I can guarantee you that so all the theory of encoding decoding perceptual encoding and decoding which are the most effective solutions today for packing more information the same amount of space are actually based on exploiting such principles nonetheless all the information I gave you today which is very compact inform will be very useful for designing system so I'll keep you know taking them out again and pull it putting it on the table for you to remember J and D masking effects perceptional pitch and so all these things J all these things would be extremely important for designing your system "
