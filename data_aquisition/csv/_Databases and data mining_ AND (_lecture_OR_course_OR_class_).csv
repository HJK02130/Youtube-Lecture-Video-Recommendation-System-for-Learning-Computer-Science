id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
CHAPMjbK_QU,28,"Summer School in cognitive Science: Web Science and the Mind
Institut des sciences cognitives, UQAM, Montréal, Canada
http://www.summer14.isc.uqam.ca/
http://www.isc.uqam.ca/

PETKO VALTCHEV UQÀM

        Mining Patterns from Linked Data

    OVERVIEW: The Web of Data (WoD) can be seen as global database made of multiple datasets. These datasets are published separately — by using new or reusing existing schemas on the Web — yet get interlinked through either direct references between data items or indirect ones, i.e., identity links between items representing the same entity. The technology underlying the WoD, called Linked Data (LD) allows for the construction of a global data graph in which data items are vertices related by edges of different nature. Entities, aka resources, as well as their links, aka properties, are globally identified through URLs. Beside this inherent graph structure, parts of the WoD can behave as a traditional, i.e., relational, database. 
        After substantial efforts on the standards for publishing and querying of LD on the Web, and lately the interlinking and cleansing of sets of LD, the next big issue is properly extracting new knowledge from the WoD. Data Mining (DM) discipline is about finding chunks of useful knowledge hidden in the data. DM methods are roughly divided into predictive ones, where past experience is analyzed in order to guess what the outcome of an unfolding situation, and descriptive ones whose aim is to provide insights into the regularities in the data without a specific goal. Mining LD is both useful and challenging for many reasons, not the least among them being the rich and complex graph structure induced by a large variety of link types, the availability of domain knowledge expressed as schemas, and even fully-blown ontologies, the heterogeneity in the modelling goals behind individual datasets, etc.
        In this talk we discuss the implications of LD for a specific branch of descriptive DM, called pattern mining. We present two different mining methods for that are complementary in many respects. The first one targets usage regularities: It analyses the consumption of resources from the WoD by the users of a specific semantic application and summarizes it as behavioural patterns. The second one mines purely descriptive patterns from a dataset of multiple resource types, which are expressed in a WoD-compliant language and therefore supports ontology design.",2014-07-16T12:33:00Z,Mining linked data - Petko Valtchev,https://i.ytimg.com/vi/CHAPMjbK_QU/hqdefault.jpg,Guillaume Institut des sciences cognitives,PT49M22S,false,230,2,0,0,0,and I will speak a blog link data and linked data because it's a little hot topic of respect the community on the liquid the web of data and linked open data which also called web of data has spent a lot of time working on standards about publishing about creating publishing cleansing wearing and so on even interlinking of link data set and now it's time to pass it to the next step which is mining of this amount of data that is ever-increasing so my point today is to show you what the challenges is are sorry and to show what the possible solution could be actually what I will be presenting today are two different studies so on data that is it actually predates the link data the web of data and i'll show how this can be applied to what's now the web of data so the first one is comparable to click stream analysis and it was designed for semantic application it has as input a sequence of clicks which may be interpreted in the link data context requests your eyes of link data resources and it outputs patterns that combine schema elements polling data that means classes and properties well by the way i had to announce that i am particularly grateful to phobia who introduced the lotions from the link data framework I'll nevertheless make a very small Rico of the basic very basic ones so if you didn't assist to Fabian talk don't panic I'll explain what I mean by this but right now I have to nevertheless use these notions to make myself understood then a second approach is a little bit different because I look at a link data normal as a graph but as some kind of structured data that fits in a tabular format you can think of relational database it's easy because most of the data on the web of data actually comes from relational databases on its relatively regular structure to a point so it can be imagined to be put back in this tabular format note that I want to have a relational database but I apply something called former concept analysis and mining from concept patterns to this kind of data so in this format graphs will be simplified into triples I hope you remember what triples are and then i'll use i'll show how to use some constructs to simplify to put links from the data into more propositional that means that object attribute value type where that values are not resources beta literals and will provide them a method for extracting conceptual pattern so concepts made of resources and their descriptions okay be between there so outline of the talk first starting with some crash course on pattern mining because I was speaking about finding patterns in the data and the first topic will be frequent patterns then I'll speak about concept analysis and produce formal concept analysis as an approach for pattern mining then this promised recall of what web data is and then the two approaches I mentioned just previously so what's data mining mining is the activity of finding knowledge shanks sorry if I'm massacring here a well-known definition that I just need to before only the essence so finally knowledge chunks in the garage volume of data and this chanc scam phone can far come in various flavors they can be concept they can be friends they can be associations and so on most importantly these chunks for being variable need to be previously alone means new non trivial and potentially interesting actually useful sorry the demanding is established discipline with the large community thriving with many venues and large large literature so I won't be doing here and traditional hold the main I only focus with what I'm interested in this is pattern mining but i need to say where pattern binding stance so you can have this duality in data mining tasks which are roughly divided into predictive ones and descriptive ones I'm only interested here in descriptive and as I said on pattern mining but if you want an example of what prediction is this is typically classification so in the decision tree induction for example clustering is another kind of descriptive technique so to summarize descriptive techniques will look for trends and regularities in the data without a particular question in mind without particular concrete variable to target whereas predictive methods will answer specific question and predict the values of a variable so pattern mining so patterns can be characterized as structural regularities in data records one particular case of application of pattern mining is recommendation so I guess everybody has already been on IMDb IMDb or in Amazon both of this sides will recommendation and at least in the past it used to be simple that means based on frequent pattern mining results from click on Peyton Manning one would find first the patterns in user consumption and then you'll use these patterns to recommend new previously let's say non consump products or Frizzle envisioned movies so patterns mainly may be of different structural complexity simple patterns just sets as I show in the next slide but there can be more complexity in there in a structure like having sequences in even more complex like trees a sickly graphs or just general graphs so what I'm interested in is the most simple for the time being these are item set patterns item sets in the in the framework of item set mining buttons are subsets of larger set of items items stand here for products typically for products that are bowed by a client and the patterns themselves are also sets on both sets of item sets a further refer to them as item set exactly so item sets test patterns when their mind are filtered because we are not interested in all potential patterns but some all existing patterns but only by those who are potentially interesting so the interestings in interesting necessary is measured by a function which tries to predict their potential usefulness and typically we will use the frequency of occurring of the patterns as interesting this measure so it's called also supported patterns so most frequently will mean will be equal if you will hit two here two most interesting I have an exam for here which is lot from moon is not movies and not neither products to buy on Amazon but rather on the touristic side and travel side this is made of users who are actually named here because I use the same data set later on but in typical out outfit of pattern mining actually users are anonymous so here my users have visited attractions turistic attractions you can think of a tripadvisor or any other site where this visits can be tracked part of the user profile so the task here we need to mine for frequency for frequency computation we need a specific level that will be the cut level 44 for interesting patterns if the frequency is sufficient that is above a given threshold then we'll declare the patterns to be interesting in my example if I take a very live fifty percent value yeah half I can see that only three patterns all of them singleton will score above the threshold fifty percent if I lower it to 40 percent then I have more I have eight patterns okay they are there for example we have two people who have visited British Museum and Stonehenge yes I can use this a British Museum install henge de skoura and add so now to go for a sorry step further patterns can be split or a different kind of patterns can be see Asians in which item sets actually a combined in a implication like manner where a previous item set and a consequence I can set a combined and I i put here some of the most interesting associations meaning that if somebody has visited nagra the per the same person has also visited in hundred percent of the cases the Louvre museum in Paris however the reverse is not true that means only two-thirds of the people who visit Luger I also went to Naga Fox so how do one how does one mine frequent itemsets the most arguably the most popular approaches what's called generate and test which means that potentially interesting patterns i examined stepwise can iterate iteratively so the the search starts with individual patterns much like what we've seen previously and then the size of examined patterns is increased on 3 2 3 4 and so on and it ends whenever we don't see any frequent battle anymore well it each iteration after the first the result for the previous iteration is used to generate candidates then to test these candidates against the data set of what's called transactions forget say that and frequent patterns got those occurring sufficiently frequently kept whereas the infrequent are discarded the typical algorithm for that is the a priori algorithm which this year celebrates yes and I show here with this diagram sorry no no animation here how greater works which starts by the most general patterns which are the singletons and goes down up to no more interesting potential interesting patterns are found so in the first step we see here this very schematic example of five items each of the items excellent and since the item C as declared as infrequent it skipped and everything that has an infrequent subset is no more considered so this is a kind of monotony here which means a set item set having a frequent subset can law has no chances of being frequent which is quite logical yeah so all these patterns that have no particular signs around them actually we won't be tested at all so how does the this generalizes sorts of patterns here I provide a table which summarizes the different elements of pattern mining problem depending on on the structure of the pattern of the data sort data data records I actually use the two extremes the most simple one and the most complex one although as we shall see linked data is still different here so whenever we have to define a new tasks of pattern mining we need to define a data language a pattern language and matching that says whether a pattern represents a data record a transaction a generalization generality relation between patterns that structures the set of patterns much in the way you have seen previously with all subsets of patterns universe interest exposure and what's called refinement operator what are the final operators here these are actually the individual steps moon that a logarithm will use to move in the pattern space pattern space being all the possible patterns that means the pattern language combined with the generality relation so what we've seen previously was a hierarchy which is actually called lattice because it satisfies a structure account of strong structural constraints and this lattice represents the pattern space the algorithm will then move down using strategy that this level wise and going from level 1 level 2 level ok so if I have the answers of any of these questions what are the pattern language what is the data language what is the pattern language matching and so on I can define a problem and if I have all the necessary primitives like especially refinement operators or own patterns I can also design an effective ad so for graphs Street things will get more complex first the languages the respective languages are made of graphs and you know that graphs are hard to to mine because of the following problem matching between a pattern and graph patterning and a graph data record is akin to sub graph isomorphism which is a very complex complex problem it's knowingly the link the same goes for generality actually there is no surprise here because both both matching in generality have the same usually have the same nature now interesting sting is so interesting strength interestingness measure similar to the one used item sets but then there can be other additional criteria for discarding potential patterns now going to the final operators for item sets what we saw actually implicit in the strategy of the Apriori algorithm is that every time we move from one level in my lattice to the next one to one existing pattern you added one new item item for grabs it gets go similarly but here we don't have items not in the classical sense so the operators are either adding a vertex or adding a new edge so as I said to define a new method these primitives need to be combined so first in the implementation for all the print primitives definitions for the languages and we need a concrete effective traverso a strategy for the induced by pattern space that's what i'll be showing in my next put on my next the next part of this talk before that I'll briefly introduce formal concept analysis which is the techniques I will be applying to my structural patterns so for our concept analysis is mathematically grounded theory or as a framework for how about how concept will arise in a context so these are not general concept but contextual content concepts which are extracted from data so this is rooted in lattice jewelry this isn't second time I I speak about lattices but I won't provide a formal definition because it takes too much time so I assume you've heard already about lattices in particular boolean lattices so this so formal concept analysis also some influences from philosophy AI database jewelry and so on but they basically its largest lottery it has been why I'm speaking about this it has been successfully applied as a framework for pattern mining not necessarily the plane patterns but some particular flavors of patterns that I'm not discussing here today there is a large literature about that so before explaining how the data will be analyzed let me make some gymnastics with ray coding my data that this was the original transaction format of the data easily be put into a bipartite graph okay and you may see here that I also put some mnemonics for my a long name of real attraction so that I can spear spice space by doing this you excuse me for that compaq encoding possess you will see a real lattice structure made by formal concept analysis you'll appreciate the compactness so my target data format is a what's called cross table or in this particular area formal context formal context has have rows and columns Rosa interpreted as object and Collins as attributes of these objects and excess of course mean that a row an object on the road has the attribute on the column so with that in mind we can extract for more concepts which are by sets means that they are composed of one set of rows called extent and another set of collins CO intent so for our concept can be think you can be thought of as being a by clicks maximal by clicks of the bipartite graph that I just show another way to see them as a Mexico maximal long continuous rectangle in the data presented as a formal context for example I have the concept to hear this you see here all the context concept sorry from the formal context which are organized by generality you can recognize some of the patterns I previously show although these names are quality analysis different to see but let's say in the context in the concept to the intent is made of luver and Niagara Falls is just mentioned before it was a support for the percent ND and indeed we have the extent which is made of n who really visited lure and niagara falls and then who did the same thing the intent is not made of all the visits but just by the sheer visits okay and it says a kind of pattern as well so why concepts are more interesting than plain patterns because we can interpret them for recommendation purposes surface we can interpret them as user profiles with the shared visits and the users being roped together and foundation one can think of using non-shared visits for example one can see that and has also visited the sorry bed to visited the last Lascaux cave one can think of suggesting to an what actually she missed from the benz visit because they share a common profile yeah so this is a way of saying that although it has some limitations that I will be discussing later on so now let me move to the pull of web of data basics so we have graph structure in which resources which are drawn here in red okay are connected by a properties which are the edges properties I said property see links of properties properties can connect also resources two literals which are drawn in dark blue rectangles here I have to pee people who have visited Montreal sorry one of them has with Montreal the other one has visited Beijing okay Montreal is connected to Canada or presentation of Canada and we have indications of the population we have the names Europe the of the two tourists who all of this are literals so usually both resources entities and links properties identified by your eyes here you have the qualified to your eyes meaning that I don't see yet the exactly right but one that is replaced by a prefix to make it more compact in human friendly we have also indication of the type of this to resources which is person and it comes from a specific vocabulary for different resource types which has been mentioned by her from here last Friday which is fourth dbpedia is another one from vocabulary very popular on the web of data and we use it to define the types actually to import definitions of entities that are wildly recognized so that the cities and the countries so we with such a data which is a graph with multiple sorts of links also schemas in schemas define schemas define the way properties are connected two types of classes so each property has a domain type which is a resource final class and arrange that which can be also resource type Oh a simple value type here the name of a person is strength but the value of the visited property is a resource type from wikipedia called city so before I go further I just must remind you that besides connection between properties and resource types or simple that evaluate data types they also generality links between either classes or properties object types or link types expressed as a DFS across off or a DFS sir property of which actually come from the standard vocabulary of the air DFS language f DF s is the language to describe naked data schema I now this is a challenge why Nick data challenge I spoke about graphs and indeed their large literature on mining graphs yet the graphs that are known and well well mastered in the data mining literature of I would say lesser structural complexity in particular because they usually use only one type of links so it makes for a single dimension of the structure whereas in link data will mix different kinds of links and makes for no 2 dimensional there are still I would say there are some recent works on mining types of data that start approaching the structural complexity of link data but we are not there yet in particular one important element is availability of a schema so they're usually rely to metro in data mining area one would rely too much on a schema that is actually constraining the graph although the schema that we have in link data and not constrained they just used for inference but they don't provide really constraints that means since we have an open world semantics at any point new links can be added to between two resources and then they don't need to belong to the scheme Alice just everybody can say anything on the way out there so another problem that may come here is that the web of data is in constant evolution so it may put the emphasis on the design of online methods which are more complex to to achieve and of course I need to say that the data is here i'm not speaking particular about that but james sorry i was correct about that link data is considered as one of the possible formats of big data sees its graph and it's a very rich structure and most of all of course they are now a huge amount of data published on on the web so it's big there ok so i'm done with all the introductions and i can go straight to what I had to propose today for you so first my point will be to convince you that it's in to observe to put the links into the pattern language so I use the same kind of example which is tourism with users say connected to a resources of e tourism type but this in this particular outfit I want insist too much on users I will insist on them in the second part the second approach I I will discuss today the first one emphasis is only on e tourism objects and I have a large variety of them so instead of just having attractions I add to the landscape destinations accommodations and I can go further on but I think it's enough so there is a real hierarchy here i have the class destination which is split in two subclasses country and urban area which itself has subclass travel sorry city all this come from a particular vocabulary control which is completely OT are a V which is completely invented here then accommodations can be hotels it can be something else also but I I skip hear the details and hotels can also have a sub class luxury column then attractions can be museums or monuments okay you name it different properties I used destinations have accommodations and they can also attractions then in some how predictable manner a sub property of has accommodation as a has hotter by the way you can often see this kind of naming conventions on the web of data where the property's just added adds has to the name of the range class so that's hotel because the range is hotter okay and has museum is another property witches as a property of a subtraction okay is everything clear okay because now it starts getting more interactive and I have other questions for you so since I'm interested in a recommendation but I'll put everything in a more dynamic context I propose with the following exercise let's say again we are with our previous introduced users and then Cora and so on and I have registered only three user sessions with semantics it's a semantic portal where people will actually click on web pages corresponding to resources and try to compose something like it in early for their next visit visits I mean global that is going something to travel sorry okay and let's assume that we have first a transcript of a session where and whatever the technology behind to guess what she did first click on Germany oh sorry this German s why as disappeared vanish here I don't know why so then she clicked on a page representing resident or requested something about the resident Berlin then a hotel adlon I can tell you that this photo is in Berlin for disambiguation purposes and finally or let's say 20 this goes in my example goes she clicked on the Pergamon Museum for those of you who don't know this is a famous Museum in Berman then bending similan things not exactly the same for but for united kingdom he clicked on London then on sofitel Covent Garden Hotel British Museum then somebody came to Montreal well and clicked of course first on Canada Toronto Montreal probably hesitated which one of the city then rich cotton in fine arts museum of Montreal so let's think now we have a no phoning session where our dan has clicked on friends then Paris Ritz and stopped and we want to help him go further so my question is what do you think the right suggestion the right recommendation will be here I mean it's obvious but still yep Bluebird yeah everyone I mean if you understood that pro I probably can stop here with this with this example not only the example by the whole approach because you know you correctly guess that just because you saw a pattern in the and user sessions okay and I'm sorry about that this was a nice surprise but my animation didn't work so let's try to make it a surprise nevertheless okay so we think that you guessed correctly so that funny a first visits a visit Simon clicks on that page of representing a country then goes to a relatively big city finds the looks on a hotel before thinking about something more it's a cultural and finds the nice museum to see when you there for the first time well the question time actually spoiled was ok do you think that in such a schema are there there is enough information in the question of course is a trap because if I only have this it's not enough I it's not this information enough for computer to use the pattern as a recommendation based because I could actually suggest as well British Museum because because British Museum is just a museum okay but now if I have a way to constrain my classes theatre actually shown which you were the one who abstracted from my user sessions if I can put some constraints saying okay first the city must be in the in the country that has been visited then the hotel must be here and also the museum must also be located in the city okay everybody agrees with that so my point now is I mind such patterns how I go from my initial sessions and it's even worse than that we shall see that in a second to this nice patterns so I'd say the challenging my knee problem first there are some definitions to make and then of course we need algorithms that efficiently my left kinds of patterns okay so I must disappoint you I don't speak about algorithms at all today I just because I decided not to put any formula on my slides for those who know no me this is surprising because i usually put lots of formulas it helps me now it's a bit more challenging but I hope out there to the challenge so first define the data the data is not going in the in the right format which is actually this one that can help help the abstractions an abstraction of interesting patterns it actually originally looks like that please I just request for some URLs that was taught in the web server logs and registered user sessions way without semantic portal then this can easily be translated as requests for particular resources but still we need our constraints okay because these are the links here are the support for mining the previously mentioned oops sorry okay let's look here that this is my pattern language I think that one step look so to have this nice constraints here that represent the properties from the schema will need to take off this links imported from the link data so just yeah again what step non-oil I'll go to the right place to say that so if I imagine this sounds without surprise my pattern language the elements of my pattern language the patterns then I have to warn you that you don't need to think of them and also of the previous format as being the resources the resources themselves these are requests for sources rather than the resources because if I start constructing something like that and if I want to exchange that for somebody with somebody else even why not publish on the web then I'm in a big trouble okay because I'm saying things that are not necessarily always true okay and especially that I have to have sometimes repeating mentions of the same class or even of the same resource so these are not the resources themselves these are kind of place holder so if you want is a surrogate ok ok so I'm going speedier because i have a time problem I was very low to start patterns must be matched to data records in order to estimate their interestingness their frequent I've been going here with the frequency the most basic one so matching here is very interesting problem i'll be back to that in my conclusion part if i go to them but one of the principles know that i have a nice alba little not yet doing that very efficiently but at least I have the mathematical formulation excuse me there are strong max hero at least some interesting mats but I hide them so you don't see them so the way I put it is if I want to know where the pattern is a summary of a particular data record I have to find a mapping from any element in the pattern to an element distinct element of the sequence of the data sequence that respects the order so the images of to class elements from here should be the same order in the sequence and moreover there need to be compatibility of types that means for example module needs to be a city it cannot be a lock circuit its associated to this class a lot ok thus everybody understands what happens there are also some other constraints of the property properties need to be mapped to links that also either links of the same property or of a sub property of them for example has attraction has been mapped here who has museum but has museum is just that some property of he has attraction okay so put in this way it's easy to say but mathematically it needs some definition anyway just needed a flame of that the same goes with the generality I have almost the same constraints to respect all here I'm up classes class elements to the other class elements which are comparable so a class mention here needs to be a superclass were identical class to the one that it's button by the way you understood that this is the more general and this is the more specific pattern here so goes this way now the more interesting question is about the structure of the pattern space how do we move from one pattern to the patterns that immediately below remember that in the large lattice of all item sets it was easy we had to add one new item but there is as I said previously no item here so what are we doing well we have some operations okay some operations that can be used nevertheless and without any surprise because Jim starting to press me these are there are four actually final operators which are akin to what I mentioned about graphs between adding 1 inch adding 1 virtus here i have the adding part so adding one new vertex in what kind of a graph here actually sequence that's completed to a graph so adding anywhere one new but you see root class not any class can be added at any point but root class then I can and also one root property but more interesting using the hierarchies of sub properties and subclasses i can replace an existing class by an immediate subclass or existing property here by in immediate sub property so with these four links sorry for operators i have the structure of the pattern space so I'm speeding up obviously have to skip the set the second part of my am I approach part at the top just to say that even defining here what a level in the patterns pains is is tricky because patterns of similar complex structural complexity actually can be a very different levels of generality this one is probably one of the most general and this one which is isomorphic here as much more specific okay so i take two minutes to summarize what i had to say well you have 220 spicy at 240 actually not know much but to move back to formal concept emerged that i introduced and explain how it can be extended to much to perform analysis of linked data okay it's not that I I'll put graphs directly but I know code grass and this is the motivation that's not important grant graphs into a way that is into a format that is compatible with form of concept analysis and we have a different schema hear that again users again attractions and visits of these attractions I'm back to my duality only users and attractions with part of the graph represented here with additional properties that help define actually find similarities between user profiles than just matching exactly matching visits subsets of visits this is the way it counted and I have the property visits which deserves coding of its own plus plus all other properties of the respective types being converted in formal context so with that in mind actually end up and skip here all the explanation how this structured into a case is because the the two contexts are easy to say and I finish my metal here to encode links this time we don't use graph structure but specific constructs that I'm actually bought from the scriptural logic and the scripture logic as you may know uses this way to actually bring first order information to proposition a lot ok we have propositional items that are composed in a way you can understand probably that that if I say that done is a user and he visited lasko i can say that knowing that lasko and Nas initial a french attractions that done has visited at least one french attraction here actually since these are all the non visits all of his visits French attractions okay the trick here is I don't have the French attractions class but well well guess I can find it I can find it and then use not that I can name necessarily French attractions immediately under the machine can recognize it but the machine can discover concepts and use them instead of this nice named classes to produce automatic automatically new items or formal attributes as you can see here and then if I add the new attributes to the existing ones I go to a larger lattice when some new groupings and new concepts will arise on top of what already has been produced with the initial codex okay I'm done I just include that links as you might see here are very important they provide context to resources and taking that into account mind mind them mind them in a mindful fashion it's important so I show it two ways of doing that and I am particularly sensible to sensitive solid to the context effect and this is how this context effect played out and in my two approaches so I think I stop here otherwise i will get killed by James 
oX-zzKBiCrQ,27,This presentation provide information about SQL queries to predict probability of developing medical conditions such as diabetes.,2019-08-18T17:24:57Z,Data Mining and Predictive Queries in Healthcare Database for SQL,https://i.ytimg.com/vi/oX-zzKBiCrQ/hqdefault.jpg,parvaneh vaziri,PT6M21S,false,252,7,0,0,0,they don't mind inquiries in SQL for predicting medical outcomes this presentation provides information about SQL queries to predict probability of developing medical conditions such as diabetes this presentation is prepared by para poner Vasily MD as part of health 709 health informatics program at George Mason University diabetes is a disease in which the body's ability to produce or respond to the hormone insulin is impaired resulting in abnormal metabolism of carbohydrate and elevated level of glucose in the blood and urine diabetes affects an estimated 29 point 1 million people in the United States and is the seventh leading cause of deaths diagnosed diabetes increases all cause mortality rate 1.8 times compared to person without diagnosed diabetes diabetes increases the risk of heart failure by 1.8 times diabetes also is the leading cause of kidney failure lower limb amputation and adult onset blindness in addition to these human costs the estimated total financial cost of diabetes in the United States in in 2012 was 2.45 billion dollars which includes the cost of medical care disability and premature death in 2015 30 point 3 million Americans or 9 point 4 percent of population had diabetes approximately 1.25 million American children and adult had type 1 diabetes did 30 point 3 million adults with diabetes 23 point 1 million were diagnosed and 7.2 million were non diagnosed a study showed that diabetes case finding algorithm using data exclusively exclusively extracted from a comprehensive electronic health record can accurately identify patients with diabetes at the earliest possible date within the healthcare system the data used in this presentation is simulated data from major health healthcare institution electronic health record in the United States for the purpose of this presentation I have used agency for health care quality and research clinical classification software CCS code to define diabetes and bacterial infection at a risk factor for diabetes these CCS diagnosis caused corresponds to the format of icd-9 codes in the data a comprehensive set of diagnoses were used to predict the presence of diabetes or the risk for diabetes this is a total number of diagnosis for which I had at least 29 observations the age greater than 40 was added to these predictors by using their command this is an example of obvious predictors these are medical conditions that always occur with diabetes or have their world diabetes in their name but were not used as a definition of diabetes examples obtained rare diseases that are predictive these are diagnoses that occur more than 29 times but less than other diagnoses these are examples of 10 most predictive diagnoses that are not obvious for data mining purposes naive Bayes theorem was used to aggregate data from any individual diagnosis for overall risk for diabetes these are identification of case with five diagnoses these are queries for calculating corresponding likelihood ratio and calculating odds of diabetes for cross-validation purposes 80% of the data was set aside to be used for training and the remaining 20% will be used for validation of the models predictors that occur after diabetes were ignored a receiver operating curve used for the cross-validation accuracy of the predictions this curve is a plot of a true positive rate against the false positive rate for the different possible out points of the diagnostic test it shows the trade-off between sensitivity and specificity this presentation conclude that data mining can be effective approach to harness the information from massive data in healthcare calculating likelihood ratios determining specificity and sensitivity and the receiver operating curve our queries to predict the health outcomes the statistical analysis used in this presentation provides insight about outcomes and prognosis of diseases for the healthcare providers thank you for listening 
WmJdXlOlK6E,28,"Most people consider a database is merely a data repository that supports data storage and retrieval.  Actually, a database contains rich, inter-related, multi-typed data and information, forming one or a set of gigantic, interconnected, heterogeneous information networks. Much knowledge can be derived from such information networks if we systematically develop an effective and scalable database-oriented information network analysis technology.    In this talk, we introduce database-oriented information network analysis methods and demonstrate how information networks can be used to improve data quality and consistency, facilitate data integration, and generate interesting knowledge. Moreover, we present interesting case studies on real datasets, including DBLP and Flickr, and show how interesting and organized knowledge can be generated from database-oriented information networks",2016-08-17T21:21:31Z,Mining Knowledge from Databases: An Information Network Analysis Approach,https://i.ytimg.com/vi/WmJdXlOlK6E/hqdefault.jpg,Microsoft Research,PT1H17M53S,false,136,3,0,0,0,"each year Microsoft Research hosts hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available good morning it's my pleasure to introduce professor Java Han from the University of Illinois urbana-champaign and I want to thank dong who is really the host who has acted but he very kindly we can ask me to sort of introduce professor Han so I'm doing that so professor yamaha almost said all of you probably know he's been really prolific in the area you know data mining data warehousing with here strike over 400 publications in major you know major conferences and journals he was awarded the highest honor of the sem CKD d which is the special interest group or knowledge and data mining he got the k DD innovations award which is the highest recognition in this sub field in addition he is a founding editor of the ACM transactions and knowledge discovery again that he really made happen and it has really flourished since then he has a book data mining concepts and techniques that has been used as a textbook worldwide he has been in numerous program committees has been the PC chair for k DD STM the and also the ice EDM conference all major data mining conferences and as you can see his work is over regarded that all you know in the sponsors list you can almost you know see everyone that could have possibly sponsor that shut off speaks to his being a very prolific contributor in a technical so welcome and thank you okay yes thanks sex is to search it for the nice introduction it is my pleasure to with microsoft research I think that's for quite a few years I mean not really come to Microsoft Research being busy work out many things so today I'm going to discuss more mining knowledge from database especially here we have a very strong database group I was thinking about no linking data mining waste databases using a new master card information effort analysis approach the work actually has been done together with several of my PhD students actually xiao xin is now Microsoft Research member right sitting here so and the two other students are in a newer one sincerely under the ph.d program okay so for about database and the information networks people originally is thinking these are two very different things databases people doing transaction management kun doing query processing storage and all these things and information networks people are more thinking about like social networks Twitter's or you know Facebook actually if you really think about these two things there are some inherent links which is very interesting so the first thing is about this traditional view of database most people are thinking database is a data repository and David system usually doing like transaction management data retrieval quarrying updates you know indexing all these different games but on your hand if we have another view for example from a data miners Field you say database accurate organized information networks okay why because not only data themselves they have structure records but they also linked to different things we know for example you have joins you link to the other things even you do not really do joins they inherently the link together okay so to the extent we are thinking the database actually information-rich interrelated data relation and they form really really huge okay information network another interesting seeing different from other information network is it is interconnected it is multiple typed to some extent you can say this is where structured information network comparing to summer net other networks they are not so structured okay then i'm going to show you if i say this one is DB internet its database internet we actually can derive surprisingly rich knowledge from the database information networks and you are going to see it so that means if we want to uncover or discover knowledge from the database we better view this one as a network then we do link exploration link analysis then you probably see there's something amazing we can get up okay so instead of it is showing you something very abstract I just show you very concrete debates probably everybody understand its DB RP ok unfortunately this DVR p is only one huge davis instead of it you get a complicated entity-relationship data modeling many many different relations together that one current hair do not have a very concrete example to show you but I just show you this one DB RP data if we know this why is not too small you get a millions of records you're going to almost half a mini Arthur's and you get a thousands or tens of thousands of conferences okay with this we linked together where things may happen okay you're proven say can I derive some interesting knowledge from the state ways actually for example you may say what are the popular or growing research fields or safir's in computer science or you want to say who are the leading researchers on database or just on X queries or something or you say whether the different stuff years whether the author collaborate evolve and all these things or you even want to know for example we found there shouting did how many way walls just the same name okay and which paper belongs to which way was or something or you just say even search brand did not get a PhD who are his who was his supervisor okay so or you want to know who are very similar to Chris those valises those are the very simple a very typical question the problem is whether you just take the VRP whether we can mine it how we can answer those strange for AIDS or something so this actually work i'm going to show you all these questions actually can be answered readily and also can be answered efficiently in this network okay so you will see something pretty interesting so the first thing i probably will just give you a link what is information network you probably heard a lot of social networks like Twitter's or you know Facebook all these things but on your hand if we look at the information network we can say this is someone more general than social network why because social network just link people with people okay but information effort to link objects with other objects okay but objects could be people as well so that's why the social network is just part of the big information networks okay for example you think about DVR p not only you linked with your co-authors but actually you link with the conferences with different keywords you know like different sub fears so it is richer than just the people linking with people okay then actually people already study lots of networks but we try to categorize the network into two different categories one week or homogeneous network another week or heterogeneous network how much in your network means when you link things you link the same type okay just thinking about you got facebook you say i linked with my friends and friends link who is friends those are homogeneous one even you think about a page rank okay people say webpage link to the other web page indoors the other web pages these are page link ways pages this is syre homogeneous okay but what a heterogeneous one you were thinking about multiple types this is exactly like a database using oh this entity linghu the other entities okay that forms heterogeneous multi type the network just give you some for example like a medical network you have patients doctors disease contacts treatments are these things remember both doctors and patients are human okay but on your hand they play very different roles patient link who is patient and patiently linquist duckling go dr. a very different ok so the same scene as a bibliographic network like a DVR pee you get-a publications authors venues terms or these are linked together ok so those are heterogeneous information networks ok if you look at a web scare everybody knows this is very important get a web is big network or protein get a protein networks by all networks co-author get networks here i do not even sure networks you see social network sites because there are so many ok but you know you got it like a facebook or something they claim they have 100 million you know even this number now I was touring 300 minute ok but anyway it is a very big thing but if we categorize networking two kinds for example co-author network in BB RP you will say this is more like homogeneous one but you get a conference author link has network this one is heterogeneous way even just two types okay now if we want to mind this network okay the first interesting thing is we were thinking about a clustering ranking of heterogeneous information network so you may say why we want a clustering and ranking actually this clustering ranking regularly practice in any network you just think about this okay the first thing is before even like a Google came okay most people you meet other people say gimme you I are okay why give me you are air because otherwise you may not even be able to find this researcher okay you have to go through you know like a Stanford University go to computer science department no you would know those gopher or something you know you go down you know it's a painful okay that's the reason people start your links i use my webpage link to a lot of database researchers because i don't want to keep crawling but with google why Google is interesting its ranking okay if you say you'll get a name say you know Jeff Armand okay then you if you say oh 10,000 pages found you have no time you know if they are not ranked you probably abandoned this but if they are ranked you find a home homepage right on the top then that's nice so that's why ranking become critical for any network the clustering is also very important because that you you cannot find clusters you you get the network like a millions of knows how could you search how could you comprehend but the problem most people are thinking is ranking and clustering or two things okay but actually ranking clustering really should work together thinking about this I say rank all the people in academia okay you would say oh it's a computer science or physics okay even get comparisons you see is database or Dave mining or III okay so you know you have to make sense instead of comparing chicken with ducks you just comparing within the pier some sales so that's why you need clustering and also you need ranking I actually will show you for example you mix database and architecture together you try to rank them you'll get garbage okay just these two okay so that's the reason we say we need to do both but people usually think they can first look clustering into ranking or first two ranking they do clustering actually we we're going to show you if you took clattering ranking these two process together integrate them together you get much better clustering and much better ranking and much more efficient processing as well okay so you will see that's something pretty interesting the paper actually published last year ed BTE Joe sent her paper actually we were thinking is very novel in the sense you're really getting two things together you get better on both sides and a more efficient on both sides ok so I firstly just minger this database data mining which day hardware a computer architecture these two communities together merge them in DB RP see what I will get you this left side is what I got what I got here actually is I look at it as to my surprise okay I even the first top-five and cannot find Sigma of your DB okay you as you can see the Sigma V R DVR in the total twenty conferences no the top-ranked oh why I'm here d be Sigma I CD was number eight to ten or something that's pretty dismal and if you love the researchers even even more you know disappointing because I barely recognize any way except I had no chase and Kong not because he's a database person just because he was a serious error a department head and also graduate from UIC that's why I know him I'll hide i won't melt actually if you really partition them okay you do cluster the you two ranking you can see this is the ranking in one of our parameters or system you get a beer DB sigma like this okay our Terry yvr DB on top then Sigma okay but anyway these are the researchers are we in one of our running you know getting this okay yes oh this for a ranking later I'll show you this is just to say we do clustering for each cluster we get top ranked list sector you can rank learn pretty long user basically you can enter something like a clustering computer science or classroom and ranking comparisons someone like that okay so if you think about it this whole process we just take this rank class the DB ARP network okay but we take a shorter or simplified version it's only two things this part is conference ok show you is database data money and AI conferences okay and here is just a bunch of researchers the link in different ways okay what you can see is for this kind of a study you don't even want to do natural language processing okay these are just links this is like a Google game you just get links you link them together then you analyze it what do you do you can do ranking using some kind of ranking rules i'm going to show you what are the ranking rules but you use ranking you get different things okay then based on this you in them in the middle of the loop okay the first ranking may not be even good okay you go to clustering then you go back to ranking clustering turn this one beam after a few rounds you're fine it's amazing good clustering and ranking on boats okay so you may say why I have to do that you you say if I were you I will first do clustering okay because I know Davis is a cluster you know a is and a cluster I why bother to do ranking okay there's why intuition you probably will be commenced okay suppose you got one you know I got one huge student trying to do Davis okay maybe some it to some kind of workshop get a paper on TV RP and here we have swayed chattery got it hands up papers and the very influential okay when I rank this to wouldn't I get this class ring does this to people were wait exactly same when I clustering database cluster different is not okay why I guess so its charter with so many years so many publications influential with the first year PhD student got one paper there in a workshop they are different so if we want to do meaningful get good clustering you have a think not everyone is equal okay so if we think about this what about it give a little more weight as a highly-ranked guys so you get better cluster or get worst class likely even semantically you'll get a better cluster so that's what I'm motivation we want to do that ok so this rank last philosophy starting not treating every note ii corps ok that's a changing of the game but it's a nice change of the game okay so now you can see why we want to rank in classroom together the first thing is we say they can be mutually enhance mutually improved okay ranking if you can do cluster the ranking become more accurate but if you can do you know both things you know very interesting thing is we said not everybody body not every Bob objected be treated equally in clustering actually that's the different game of clustering but why the other people don't think about this way they treat every object just the same as the hard clustering game is doing this but it should not okay so with a network you actually will treat something you think they are different they are very close for example you think of weird EVN Sigma they are different if you do natural language processing you will see there you know edit distance X is 3 far away but on the other hand we know they are so close so that's the reason you think they are different maybe they are same you think they are same they are different ok so the game should be changed now we look at this algorithm the algorithm we work out if you think about this it's pretty simple initialization we don't have knowledge we can randomly parties you anything okay but with random punishing anything into K clusters suppose you say i want holcomb percents partying in 220 clusters you can do that randomly okay then what you do for ranking is you can kick in some kind of ranking rules i'm going to show you what I ranking rules immediately yes yeah these are when you cluster them the notes you first don't even consider the graphs just to say knows are separated okay of course once he separated the network could be separately but you don't care that much about it whether it's mean cuts or max cards you don't care okay because this is very rough just a graph you just take notes you just put them in randomly into different clusters that's it ok it's very to me yes the object function later you'll get this so the ranking actually will do you know like you take some ranking rules later i'm going to show you everybody if you want different thing you can get different ranking rules ok then you can generate new measure space based on this you say oh this one agree in this cluster could be maximized or somehow you get it get some function then you can adjust your cluster ok once you're just a cluster you'll repeat step 1 ranked again you generally measure space again adjust again ok turn this into loop if you know k-means you know II am this is a similar game except you put the ranking in the middle ok now in the real network of course all the network most people on the play is a matrix ok you just thinking you get matrix of conferences and authors and conference and authors ok so if you take a conference as X authors why this is author publishing the conference compliment conference takes the authors and this is also linked with authors by initiate just co-authors and here conferencing conference initially is empty ok even Sigma and VOD be and not linked ok so then after few rounds you will see who and who will be linked together closer so now we get into this ranking ok and we test the two ranking strategies one week are simple ranking another her Authority drink okay what is simple ranking simple ranking is very easy you just take those links okay you get more links to get more ranks okay just thinking you publish more papers okay you rank higher okay or you the conference took more papers the conference could run high but that may not be quite right thinking about this if some guy got a paper or rejected they say all set our own workshop we publish 20 papers in this wet works you have a conference that's it okay maybe it will be indexed by DB RV but remember there's a one golden rule okay the golden rule we can do for this Authority checking is you have to restrict you see whether the author could be ranked it's not only you publish papers you publishing highly ranked conferences like Sigma V relieve me those conferences you probably in a junkie some your serapha workshop it doesn't hot okay but how could you know which conference is highly ranked we don't want anybody to train it okay what we do is with a highly ranked conferences attracted many papers from many highly rec we their double Manning okay not only taking more papers you'll be here you have to take more papers for many highly ranked Arthur's if you instead of bogus conference I bet it you know people here we're not even joy okay so that's a reason the highly ranked authors actually the highly ranked authors many were dictated this highly ranked conferences okay and also we actually see the rank of author is enhanced if you cooler with some highly ranked hawsers okay that one were distinguished for example you get into like a stanford university with a university or Community College the reason is you can publish paper but you probably should stand for you co-author with some big-name guys pretty get more credits than you publish one paper in it by a community college with no neighbors okay so if we know this is a rule we can encode this rule in our matrix form okay actually this rule warming code in this way rule 2 and rules three okay rule 3 because the people disagree each other on collaboration how much weight he can get so we put alpha as a parameter you can sliding the bar and say I like the collaboration or hate collaboration whatever you can slide it you get different weights okay so then we play the game just to go back to this 20 conference minger with database and the hardware now you see with this rank class we put in the middle you will see one line the green line is the hardware one and the red line is database and a mining one you press see the space after surfing intuitions they are we're separated on the ranking you can see the highly ranked guys here will be very low ranked here the same thing highly ranked here will be better very low rank here okay so they are very savory community of course I take this to community now take databases data mining as well because it's a little harder okay to to top our teaching them then we get into our step two is generating new measure space using mixed model okay this is a little statistic game basically is you just use it using those you know conditional probability you get this model you think they are generating model then your finery can get some kind of maximizing the log-likelihood but I'm not going to get into very detail you know just a very typical traditional statistic game you can get it okay then you actually can see after two or three iterations this too you know cluster not only ranking a separated the cluster we're separated this is database called a mining cluster this hardware computer architecture class they are far apart so now you'll get into after this you to cluster adjusting ok adjusting basically remember we do this clustering is centuries like EML go this is not a k-means so you got is a soft clustering succour soft is you can say oh you belong today base 0.83 percent or something like that but it would be on to the others or maybe 0.05 or 0.05 is you know different thing it's not like one percent a 100-percent be okay then you see this this is initially initially you'll probably see because it's random okay so somehow the two things actually mixed up even when we do initial we're actually getting like you add up a little so the ranking if you got a natural again more papers you get it or higher ranking so it's not a completed random here but you look at a cluster they really mean go together okay but then you after you do ye Thracian two iterations reiteration they become stable you see they have we're separate okay so it's very small number iteration you actually can get it done complexity okay people say network user is very complicated you have to do use cloud computing you know like you know very powerful things you do to do this in many cases like in Google definitely needed but here we play a different game why even we get this DB RP network the game is different from sim rank you pretty her sim Rehnquist I'm by Jennifer Williams group in stanford university they they rank and cluster networks mostly you know what they do is they calculate every two nose in Derek calculator similarity sink every two notes okay how many nodes you have you have n nodes every tool you get N squared pairs and they are not only compute directly computers their computing your first neighbor then your neighbors neighbor your neighbors neighbor ok that's the complexity you have the bear you get a quadratic number of this pairs and you should pair your computer links and links and links okay you can hurry optimize it better because thinking of quadratic plus those links the complexity is there for us why we got a linear algorithm okay the reason is every time we evaluated number you your distance to the center of the cluster how many centers if you say I want to get many clusters you have 20 centers 20 is a constant you get million people many papers you get only 20 centers okay so that's why this K you said k10 take a is 20 k is nothing okay that's a reason you go to the linear the number of edges remember everybody published limit number of papers okay don't publish in every conference every thing so probably a maximum is 400 or 500 okay so this this even you say on average the E versus n is 500 you still have a constant that's why it's constant number of notes okay so this is a linear algorithm is very fishy okay then if you do case study this one we just run this set K equals 15 okay and because we don't have 15 columns it's too too narrow so we only show 5 this is database networking AI theory I are for example you look at Siri remember this this guy knows nothing about feared but they put a soda starfox probably those Siri people know these are the top three things okay and a database actually you may argue why we are DBA icd at least a little higher than Sigma remember nowadays we r DB and the ICD a cashier many many co you know common authors with Sigma V their ranking they do take a lot of repeated authors on the other hand they take more papers Sigma lunatics 70 to 100 and VOD be take a war honey more plots papers to get more papers as you get more reputation if they have the same authors okay so that's a reason actually it's hard to judge remember we don't have citation count if you have citation count in DB RP it would be a different game okay but anyway then you say you can only do two by type network then we got another paper actually this way in the last year kdd Tucson I we say we're not confined to buy type because I by type is too simple for example within database you want to say who are the top researchers are XML or on transaction and on query processing okay so if you want to know that they publish in the same conference you just cannot distinguish them so we need what we need terms okay you said oh this is a transaction management know this is a query optimization okay you got different terms only you introduce term this value and an author will really make sense right you can further partition them okay so that's the reason we say we need a more type the network you have a research paper linking with this this is from if you are familiar with data warehouse this is more like a star network okay that's why we call this one is starnet schema you look at we could start networks schema it's not star schema because otherwise you you you come here with the OLAP people okay but we got this remember there are many many operations actually follow this star network okay then what we are going to do clustering and a ranking is we break these sing we think this thing is a type to think this you get all these small one week or this small type once you break them apart you're not breaking authors as author conference at conference you actually even you break them into tiny nose they still keeps these stars structures okay the bigger star getting too small stars you get star from the hierarchies okay so and every d star you may have ranking you may have crossed for example you can get a hardware cluster database clusters theory cluster okay you can get this so what we call the star network schema is you get a center node which is more like in data warehouse you have the star as a center okay then you take all the attributes or different other nodes you put them aside you form a star network okay and with the star network we can study for example I can give you example this is DVR p you have this this different things okay this is delicious delicious he also can get this for example you can see in this is tacking event the tagging event you may have users and you have websites you have tax ok so you if you can get this network this is IMDb it's about it you know the movie database so you get a movie as a center and here you have director here you have an actor or actress and here you have tighter and pot so you also came from started full star network almost can be anywhere okay then I just show you i won't give you the detail because we have lots of things to do so I just show you this why essentially the general principle is very similar to rank class but you working a little more complicated network okay but you probably can't see the running we took this country took this for Fears database data mining machine learning information in favor because this for Fears nowadays is our intermingle together we want to do clustering and do we want to do ranking i will show you one minor small cluster whatever get okay here you can see suppose we look at a cluster 1 parties kdd you see how different intuition they go okay you look at the kdd the zero iteration you get database data mining machine learning and some this bc is others you look at this okay at the very beginning is almost evenly distributed because it randomly partitioned okay but after wire after certain trajan okay like 10 CT reaching and also the the stable iteration you got once you get state board you see this is 0.92 kdd belongs to date money okay but of course you may also TDD maybe run illiterate or the IR there's worth machine learning little forced a base okay but it obviously you get the idea what about Max stonebraker okay my stone care act is ninety percent let me see 95% billions of David's he's very dedicated David person you barely go to like a data mining or machine learning so that's why 000 person yes there isn't there sweetie okay but as a column that classifies the paper has a data-mining you know the date away is each paper remember in DB RP record you have a conference you have this paper ID this paper ID where one way where you have the information about authors another information of information is about title okay that's the only information you have you don't have abstract don't have links okay but then you have conference information like a cig mod or vldb on the concert data mining concertina basis is it one of the clusters in your guitar ok yeah actually this one that's a good question actually this question is we first do clustering once you do clustering you actually get the database clustered a man in cluster how do you get it later your see we even you use robotic and get it you don't need a human to intervene okay of course we did intervene we put a labor so this is david's the state money okay but later you'll see how I claim the robot can do it okay get the ladies yeah you don't get Labor's but you use just top two or three keywords you market you get it okay do you your see it's very interesting you you you will see this state which we can information retrieval you see the top information in your cluster the top word wines information when is retrieval okay then of course you grab this top like XML you get a wines xml wines ex Corey after there are other quarries or something optimizations other keywords but you grab two is the XML ex Corey you actually get a class or name ok so the same thing you can see this ok yeah this one I actually show you how things with the machines running what you are see okay when we do the all the for Fears doing clustering what do you see finally after a few rounds you get a stable you can stay where these are numbers but you can either see what the numbers for example chip or a I cheaper re i belong to the third column and cvpr beyond sir column ECML third column I CMS or column each Chrysler column what the column is about yeah you know this is a yeah at the first column ed BTW i see de I the first column let me see Potts Sigma V 0 DB what's what's the first column tables so that's very you just look at the top one but there's one outlier you first see I wouldn't mark anyone read cikm cikm ERISA is a mingle with quite a few things you look at this we don't know this wife is ir ok this one's database this is david's this is dave mining so they have more IRS more somewhat data mining then they have a little database tiny little machine learning or AI or something so you know this is a little bit nervous or mixed conference we do not know is even with fear it is but anyway actually I RT reclaim this the second I our conference or something like that that's probably they had to have some some arguments because the highest wise I are let's just look at davids cluster because we have many database peeper here you can see what you see while ice claim it you can get a labor automatically the top keywords database database is system data queries system quarries management object relational those are the races right as all the game database is playing and these are the conferences and these are the rank I cannot see rank authors but they are very dedicated database and very productive okay so these are there like a switch a duramax no break room I carry you know a lot of names I think almost every porous here David luma is here so everybody is here right so that's a Davis okay and we actually can go deep her to do sub cluster you can see this is a sub cluster of XML ex Corey who are the constant ones search a bit poor Victor vianney no Jairam Mike Harry no you probably also know this name they play a lot of games on x correo x databases xml not only that you actually hang can do the same game on other database like flick okay flickr you you type a raleigh what you'll get okay we don't know but it raleigh when the people got it actually there are several clusters you can see there are three clusters this cluster likely will be raleigh north carolina is character right and this rally or raleigh bikes is famous back everybody knows okay and this rally i really do not know that i know this one is I actress or some singer or somebody I'm not a fan of the singers but at least there's some thing related to other rallies okay they automatically they can do remember there is no human being no pattern recognition involved okay we do zero image processing but we actually can get something pretty nice the bait on tax of course so you probably can see this is a pretty general framework we actually did it on you know I am DB and a delicious and Flickr and many things using the same game okay so it's a very interesting okay so we work out a system actually the system car I next cube while we call I next is information and now so everything's I ok I next year now I next act is the information next and network enhance the text cube okay so we have a cube we had tax we having with net or get everything put together okay so what do you see here like we did a VOD be damned are you actually if you have computer here you can just type in on like it yesterday in being I just ask them to type it being you know like I next cube they actually can play with this right right on spot and these are different things actually I remember last December hector garcia-molina actually came to UIC he asked me what's new I said oh we got a system pretty interesting I told him he said all you cannot for me I was a department head of Stanford I do all the evaluation all the fears he'd have graphics he look at the names and he times his theory he looked names he said oh this system is pretty trust or as to why he said oh the Stanford professor are on the top hahaha so that then we probably say this one has no training okay now we look at what about I add training inside I a lot of people wander training it won't say I want twist it I don't like your you're at once so that means you what you say i do have prior knowledge i want to put in okay i want to do training so the input is not only this network but i give you a bunch of class labors it can you do better can't do worse or can you really do it okay of course it would be different game the previous game was was a clustering game now it's a classification game okay so what we can see is this okay now the game becomes a little interesting for example if you want to say what emails likely could be more terrorists ok suppose you want to get this so you may put some suspicious users or words the emails as a training set you put it down there then with this wine you do the network analysis you may be able to analyze which email likely worry more some terrorist activities the same thing like we actually work with the US Army Research Lab they give us like a 16 million dollars of course it's big Center you more quite a few universities and we do information network analysis you will be able to to classify which things these are related military camps ok now the general philosophy is this kind of picture ok you got you you mark some class say this one state money suppose this one is your knowledge then you can propagate from the conference down to papers and you can prove that down to authors and these propagation may help you actually to classify databases okay so the interesting thing is the class labor you got sometimes you think is funny you give a labor to the conference at it get very high performance on the authors and on the keywords because they are linked okay links things can propagate ok let's week or knowledge propagation so we work a new paper or news new software called ginette mine ok G is a graph a crease is a graph regulations under nice so we use thus the network mining ok you can sync it this way and the general philosophy is you use a graph-based regularization network the regularization base is some kind of smooth you can think do some kind of boosting or something you do something smoothing you get a better classification accuracy ok so the general idea is you use consistence see some assumption you do this this maximization some kind of propagation based on the class labour you can you can propagate down to the other parts ok i probably will not give you this mass formula to the explanation you just know this is some kind of mass formula you actually can get the normalization terms then you get this like a some kind of estimation on the maximum likelihood then you find her you can do you know this game ok the students who work on that Hector she got very good training on that originally when she did an undergrad study so what we still do is we take these four areas ok database data mining AI information retrieval so we gotta you know these tents islands or papers and authors and we actually get these three things we do different kind of training ok now we actually compared with a few quite right or algorithm need to classification they do graph classification network classification and these are pretty influential papers published like in nips in in ICM error in all these conferences we do the comparison ok i will now give you the very detail but you see this result we the major difference is we use heterogeneous multi type the network they just trade everything just like a normal i equal play they just are doing this now you can see the difference these are three other algorithms this arch in admin a cpt is Arthur conference paper and tighter okay these four things for my network but they don't do that we put the same network as input but they don't do the classification or type these four types and we do it okay now you see the the accuracy this is how many percentage of author and hummin percentage of paper you mark them you see these are some paper in David's this is the paper in machinery okay then what you can see is you get 0.1% training data and you want to do all the others as testing okay then you can see we can get eighty-two percent okay they got it is only you look at others they won't get it that much inspector you think about this is twenty-six percent total for Fears this is almost random random guess right you get twenty-five percent is the real random gasps okay so you get 0.5 percent you can get get pretty hot ok this is accuracy young authors remember even author themselves it's ambiguous a very much fear like for me some people call me a database person some people call me at a mining first it so it's a little ambiguous okay so and for papers you get this for conferences you get a very high accuracy you can see this because a conference usually the fear is decided ok we'll paper sometimes is all time biggest these are the ranking reader that one is a clustering this ranking results you can see database top-ranked keywords are these yeah this is data money this is a I this is the information Airy table so you can see you don't need to get a labor you actually we're know them okay and these are top ranked the database researchers the remaining researchers AI researchers information okay this ranking to some extent is pretty a lot of people pretty recognize this okay and these are the other conferences okay so it's very good result remember there's no real human being but we did to have a little training we took some 0 one percent you know paper or are or you know like authors okay so but you can see with training Acura dart get even better than without training you can see that you just look at the names and the conference is super C is better okay so with training is good then with this classification clustering what I want to say is also you can dig out a pretty hidden knowledge in the network we call road discovery okay what are the road discovery okay suppose you give a network like this okay then if you really can do suppose this is the army communication network okay so of course the army will give us the state all right so but on your hand after you to this row discovery you may form a little clean and neat network hierarchy call a different levels okay so then you may be even suppose you can even recognize who are the commander's like the center one these are captains these are the really foot soldiers right so at the end so whether you can see the network so probably is more useful than this one right okay so can we really do this of course you say if we can do this automatically just give me a networking and do that you will be great right if you have intelligence you can see how they are the enemy army instructor okay but with we do we cannot do army we don't even have data but we do have DVR p what we do is we say we extract semantic information from the links we can of course there are lots of tricky things you can see but there are lots of attractive things you find I can get then I'll show you we got a paper in this year cked conference the tree while he was my actually is my first year student but he's very very capable ask him to do this and he did a marvelous work he just say we with TB RP network you know nothing about it which university this guy from you may be able to identify who are whose adviser and my advisee ok that's pretty tricky you think about this but there are something you no is adviser advisee if adviser advisee never published paper together there's no information we can I get it okay you have the public paper together somehow okay so but how can we get this okay we're actually doing the major major chick is you don't give too much knowledge it will give some common sense knowledge as constraints you can get it why do not give too much knowledge because the process for exemplary in Stanford it's very different from like a University of Munich or some University in England or in another know singapore someplace so you give it a rule it may work in stanford university or in the US it doesn't work in singapore okay so then you get lots of errors so the rule we finally put it this into network is very common sense rules one rule says the advisor actually it has a longer publication history okay then the advice see at the time of supervision that probably is true another thing we give is the advice see becomes adviser will not become advisee again okay so that means once you become professor will not become student again okay somebody's is to okay maybe seven seven dollars but anyway we we use this common-sense rule ike this you're very common every country will take it then we just do this basic is constraint propagation you just come find this confine that you find I can get it okay so you can see we've construct a network of the authors and papers and year you do simplification fabricated things like this okay the part we have is author paper paper year starting time ending time ranking score using this of course the highly ranked alliances this number one professor could be candidate for the or likely will be the advisor then their number two advisor actually we do sometimes we cannot make a decision will give you two or three based on the ranking school so with this ranking score we do this we call a time constraint probability factor graph remember the time is critical if you don't have a year we cannot do it you do have a year okay in DB RP so we take the starting ending time doing this this propagation so you probably see the result actually is pretty respective because we checked there's a message geology project and aig rh project they do have adviser advisee relationship we do not use that for training but we use this as a checking okay then what you can see I give you a few examples probably is no one guide and I put in actually the Joseph Heller Stan who had juice even Harrison's advisor actually we got to we originally started what's wrong because Joseph Harrison got peach in Wisconsin the shoe to advisor is Mike Stonebreaker and Jeff Norton we said max number cannot be but actually we checked in detail max number was his master advisor another interesting thing is even in their PhD committee finally found max Stonebreaker steer serve as a coat advisor on the committee so that's the reason we actually right right so even he got fishy in Wisconsin and this was my own student he got to advisor China and Jerry hon okay i know because Chang was her master sees his visor so to the accent is true we took a search brand he did not even got PhD but his advice or that time at least we do not know whether he finally Craig he's not we were not certain the visor is Rajiv Gidwani actually is true okay so that you've got something pretty interesting the accuracy is pretty good and running time is reasonable okay so that's about a row discovery another interesting thing is with information network analysis you can do data cleaning and data validation okay i will show you that this actually was done by Xiao Qing is here okay why interesting data cleaning work is object distinction you know there's an object con solid conciliation is centuries you get different names like you know like a bill clinton William Clinton fancy oh that's the same person but object distinction is you got away one okay there are 14 way once actually there's Carrie same last name same first name in DB RP and the object distinction is partying them into 14 slots automatically say which paper belongs to who ok so that's a really hard task actually when charging first posed to me actually said I said it's almost impossible because I'm not sure because three-way walls you know co-author a different time I they are probably the same conference okay so actually there's another one shouting fun is seventy two sons and three albums carries same name as forgotten ok so those are the very hard things to distinguish but he finally did it ok if you look at those things interesting thing is he came back to me a few days later I should mark this one is a little darker but he told me you know it's not just the collaborations with the same author occasionally but they are degrees of high collaborations you for example just a way while this UNC we wanna she actually collaborated with Joanie on digg months because she got PG UCLA okay but the other way one like a he got PhD under Hong Jin Lou collaborating with Sanjay newest shaming because later he joined shavings group and there's a another way one if you see you at the at SUNY Buffalo because my student GMP actually published papers that's his student but the problem is this GMP actually working with this way while also or humans this way while it was going straight up for a different way it was and you know at the same time that's a reason you know you know make sense is much much mario but the problem the interesting thing is what we heart the first thing is we use a link paste similarity use a random walk the random walk has a good thing is this okay you were automatically judge which link is more important you think about this okay it's a conference link is more important or co-author link is more important okay if you use a random walk you're automatically co-author probably carry more weight than co-conference because you'll publish it one conference Sigma may take you know 100 papers okay you're cold conference same conference doesn't mean that much but you got only three or four author one paper so they have tiger dick but another thing is who is going to train it you got like we got a half a meaning authors you your they're not even to find a good training set in a way one who is going to train where you are okay but the interesting thing is we said there are clean data in DB RP we use self training what is self training then we seem DVR p okay you will be able to find some kind of clean day just show you for example Johannes Kirk okay you're not 14 your highness Kirk's in DB Ravi actually is only one okay so why because a girl is not a very potter some people are even have a hard time even pronounce it right okay and your highness of course in germany maybe it's okay but maybe here overall in DB RP is not that popular not like John but anyway we took this as Halle gathers you it do a little you know statistical thing you'll get those are the clean data you take those clean data runs through this you actually will get a parameters you get a training see how sharp the knife should be to cut it okay and with this you you know the parameter you actually can training using svm or something you can get something pretty good i'm going to another interesting thing is about a clustering remember to get a cluster you can use single link complete link original and all these and finery searching found it using average link and a refine it okay then you get a really good knife you can cut it okay i will show you just a few example super can see this example there's a jew Heller Stan you probably know him but they are quite a few to Harrisons agree on there but they actually not a confusing each other because you have different min or initial ok so we play the trick we put a Joe Harris to Jill Heller stands actually there's three we all took their mirroring this you're out we merge them together see whether our system can do it okay then to our surprise actually the system any two headers and they say I'm whining % sure the precision active twenty percent is right but we do have some record not everybody we can do the we dare to say it right ok so the relay car is not new percent but it's still decent job if you look away while I perceive us you're not so good ok which is true because they are 40 of them but we check we want it one by one we found actually it's not that bad you think about this 14 we once we got the 13 right only this one was merged with eating up by this way one okay but this way while to suffer five papers to the UNC Chapel Hill a while but this Ungh everyone she only suffered ish even sever six to the Buffalo a while okay actually these 11 papers was all muddied it with champagne my previous student because he collaborated with all the three at the same time same conference right so that's a that's otherwise the accuracy should be higher okay yeah you just get out to that he said some people are asking what about to we want so authors said papery though I cannot do that sure then we look at another one is data validation okay data validation actually was also done by Shawsheen that's why she actually got cked 2008 dissertation war he got several great piece of BC's work that data mining people really love it okay so this was about a truce validation is you get a network you actually can consolidation came back from a one summer intern is not in microsoft research but he came back to me he said some company people asked him to say on the web somebody is telling truth some webpages is sly okay you get a conflict information can you write a program say which was right hey that's almost seems impossible to do it right who but he came back he said he wanted to think about this to the research original so sorry this is very hard you know how could you get it but then he work out a very interesting Messer I better show you the master instead of saying this he actually really got a real data set containing such thing which actually is a non icing is structured ok this structure is much better than the natural language things is very very you know funny here you look at this this is a book called rapid contextual design ok you perv do now know the book I didn't never read a book okay well when shocking bring this book a look check all these popular websites ok to probably everybody's surprise they all give different answers ok of course a book only can have one right set of ulcers of course you may say Barnes number is not doing too bad because the a1 book is the right answer barnes noble only missed this middle name for the middle of the second author ok it's not too bad but there some did a really Bismil ok it's really bad but but the problem is if you cannot trust the barn over even who you trust how could you get right answer ok so shocking work out a very narrow Messer information network analysis ok take this while it's a game ok you can think these are the real book these are the websites who setting books like a you know barnes noble you know powers book these are where they stated the authors ok so you can see some actually have more tense connection some have more sparse connection then you previous eight with us just watch this network can i judge which one is more trustable looks almost impossible to think about it actually if you really think about it there are some rules you can get ok of course one rule is the assumption this something can be extended this one said there's only one shoe facts for property for the book is true you know it's only one true set of authors ok but for some events like a political events you know Democrats and the Republicans a debate you even do not know which ones do ok but something actually like Obama you know you say Obama is a present with shoe but if you look at a time you know time axis you check the older web the Obama is a senator of e9 okay there was a you know Democrats candidate or president-elect or something you get quite a few different titles which was true they are truly to a different time slots okay so we just assume there's one we do not say there's older where but there's a newer web okay we don't deal with this now even something is a little much surer than the others you look at Jennifer Widom and Jada with them you know the sort of have degree of to choose but probably generally there may be even more accurate but these two actually becomes a critical one okay this one says the force statement user is more diverse okay it's a little hard to converge which is true when you heading mistakes you make different mistakes okay for example you just assume that the police you know catch three robbers robber bank ok the police Stefan not say we have a panel discussion okay what they do is they isolate those thieves ok the question one by one why they isolate them just because they know when they want to Terrell I likely they tear some different versions they wouldn't coverage okay when is having shoes likely there could be converge okay that's a golden rule for us another one of course is the trust ability if the website keep telling you the truth you say all this website hyper trust more otherwise if we keep telling lies I probably ignore you okay so with this you can build a choose or confidence propagation game this game is somewhat like hits algorithm or you think history habit you know haves and our 30 pages you can turn around propagate this one we can do it okay for example you can think they're hops their authority okay at the very beginning you may say the websites and facts I don't know which one is more trustful they are equal okay then even they are equal okay once you build this network they become unequal again why you look at this this is owed to is a book there are three guys said this should be this author there's one guy said you should be author f4 okay based on the Rue if you do not know anything you press a this one likely will be true this one likely will be the force okay and if this was true maybe this guide heading this FY maybe a little chore then you know w2 because W already made mistakes so you do this you start propagating okay of course you with the massive you get tiny network in our work with massive network you like the Google can play the game because its massive okay so that's all finally you can make it work okay of course this game the difference is they are you know the computation is not additive its propagation probability you know those kind of computation on confidence comes in you use different formulas accretion really did this one you see this is the truth finder competing with barnes noble why pick up bars number bunch number google rank number while hey you if you can be number one you can beat many people okay so and if you look at the shower chinga this these many books from 8.com he randomly pick one hundred books and judge whether it's good or not in order to see almost every inch okay the choose finder coded by xiao xin if he can do better no worse comparing to barnes noble okay so but actually it's not not because you're Shane is Magic charging takes many websites away including Barnes number itself refinery can beat bars number okay so actually he even did this is try to see which bookstore is much festival and the google rank Barnes number number one and the powerbook number three but actually there choose trustworthiness wise these free three no-name bookstores actually ranks pretty high hahahaha but anyway they probably only publish option i really do not know but they published last but they do a better job right but a power spoke icicle issue to blame their data input ER they really go to pretty abysmal you know ranking this one is about data validation using network and another thing is using network you can do search you can't do similarity search okay i'll just show you the very last one remember people doing similarity search you say how similar we r DV vs sigmod if you just ask me profit or they know it but you ask d BRP it's hard to say because they are added distance it's very long okay very big for example if i ask you you know who is most similar to Crystal Palace's that's also a tough tough question and we did it in two very high quality there chase the trick is we use information network but the question is much harder to answer in the sense even for human being you ask who is most similar to crystal follows us you press I don't know it depends on which standard a similar is just because they have tight connection by co-authoring are they attend same conference or publishing same conferences are they exactly what kind of same topics okay so actually these different thing by network you can use Pass Ski schema okay what is past schema look at this this is paper this is the author this term this conference if you want to say who is most similar to Christmas villosus you can go to paper back to the author what do you got most similar one aries co-authors but if you go from author paper to conference the most similar ones is they published in a similar set of conferences but if you go through author paper to turn what you got is they work on exact same topic even they may not even publish in the same conference okay using same terms so you will proceed we took Christmas fellows as a quarry okay this APA means author paper author who you got it you see spirits Parvati me to Jamieson you know pan you know most of these are his students including yuri less Kovac actually I think flick corn is also his student what's its gilded you can check back i am not want any % sure but anyway you this similarity it could be co-authors because molarity is lower but anyway these are author paper author but if you go to author paper conference paper author you go through a conference you can see the most similar wines me the reason is i publish both in davis and dave mining colony it is the same way right so Andrea kashi agora also right but if you look at exactly terms actually GM pay or srini jeffrey actually ming ming and qing actually higher than me because I even probably the same conference at work on different things I published like association rules you say another I will forget that he will do different things that's a reason okay you get different things so that's very telling if you single it based on many many papers many many titles this network big network in terror something very smart we even tried on flickr okay you can see we take this flower this lotus flower is the quarry number one the most similar is to accept of course then we want to find in most similar pictures using tags okay but if you use tag image user use tag imageshack using this way you that's why we say ITI image tag image you just look at this you say this is ok but not that good because this bird is not lotus flower okay but if you use image image tag image group going back this group tag actually is quite telling now go through group tag you see those are our lotus flower even this flower is not blossom yet these are steer loaders okay so you actually can see it's very telling so that's the nice thing you you see this similarity search using network actually can't get Sigma V DV is much similar than say Sigma Lewis kdd sigmod with stripper act okay that even Sigma was sick graph they all carry sick but it's still pretty far apart okay so we actually can give you a conference e which conference most similar okay i'll give you add a spa we don't give you a sigma because Sigma would automatically you cannot hair we give you that so I say who is more Sigma is similar to that spa you use the PageRank personalized pagerank you got a device i see de Vere DB Sigma Texas reason is that's why it's working database they put at a higher wise highly ranked line will put it high actually that's why is not that highly wrecked so they are not similar to Sigma you would not agree small similar to Sigma so but if you use our cpa pc based on our authors remember sigma house or may not always want to send paper so that's why so what do you got is that that's why is most similar to dexa why ap web cikm you know that's definite alley right you think about this you get levels of conference clearly rather you just take highly ranked line give it to him ok so this similarity at you're using massive network is very very telling so the similar thing we use we competing with sim rank the interesting thing is sim rank going way way way far ahead to fetch neighbors what do you fetch after many iterations we'd run many iterations you got Sigma ball singer is foundation chains they have HPT see of course they have cidr but they have something actually pretty strange but here we got you can see Sigma almost similar to this conferences or joiners so this one sim rank even they are expensive they tend to grab a little remote neighbors and he there we follow this path you can actually tear quite choose okay so that's why this similarity function foot foot for our new measure at least very good okay now I finally finish the whole thing of course we got lots of students did a lot of things there others inside and out there but D Singh all related to information networks but I think it's very exciting in a sense the information network is specially hook with state ways we call DB internet actually can give the database a new life because remember we'll just take DVR p or a flicker or something as an example TVR he is structured okay with this structure network you do far better than the web okay remember people or Davis people were bullied by the web people say oh we can do more things actually database people can do many many more things because database hold the rear assets of data and it is structured okay with the structure the database you will do far better than many many people working in a ir ir okay that's why i say the database linking team up with information network you'll get a new knowledge base and it's very powerful knowledge base ok so that's i finish my talk these are the recent papers including xiaochun's several papers these are all this year's paper and of course with now we publish mostly in KD d sigma we are give a tutorial and we also publish in ww conference but there's a 10 pasta okay okay so thank you very much the reason we got the spies that's why this year's in Japan really want to go there haha yes in there the truth discovery work that we describe do you consider the fact that some websites are poking information from each other yeah very very good point charges original one consider a little but the copycat effects is is very tricky thing I do have a student working on this copycat problem okey last year lunatone he is in AT&T research now she she published two papers in vldb actually two paper both side shot scenes paper actually did a follow-up work it did a really good I enjoy reading that two papers we still work on it I think it's a very good problem on this choose one the copycat effect definitely it's very interesting one and it's also very tricky one yeah it's very good point okay yeah thank you much okay thank you "
iZMrDYpwQJ0,22,,2017-08-12T01:43:10Z,Lecture 5   Data Mining,https://i.ytimg.com/vi/iZMrDYpwQJ0/hqdefault.jpg,Jordan Kern,PT55M3S,false,1414,22,0,0,0,all right welcome to the lecture five is the first lecture that is going to be video recorded and posted on YouTube so we'll see how it goes I guess I highly recommend having listened to myself speak to watch this on youtube in the higher-speed viewing option I think you can do that get set it up you know one and a quarter maybe one and a half speed so if you get if you get bored you can you can skip ahead a little bit this post lecture is going to be on on data mining so data mining is actually kind of a misnomer I think the visual that it conjures up is of you know the process of looking for an extracting data from some sort of database or obscure website or you know government lab or something like that but really what we mean by data mining is extracting useful interesting patterns from existing databases sometimes very large databases you know the process of extracting the data to begin with and then cleaning it up printing it in a useful form or form that facilitates easy exploration is part of what we would call this sort of larger discovery process but data mining itself is really taking a big database in looking for interesting stuff but then you can you know tell a story with or answer a question especially when it comes to academic research research or you know coming up with some sort of deliverable for a customer and so all of this would be I guess it goes without saying computationally based so a you know quick look on on the internet will show you that there is they're sort of competing standardizations for for the data mining process and one that is most prevalent is this kdd knowledge discovery in databases process and you'll see here that data mining is one part of this five-step process so before we get to the point where we're pulling or using computers or algorithms or scripts or programs or whatever you want to call it to look for interesting patterns we have to do a bunch of stuff to the data and some of that can be particularly mind-numbing but I don't think we're going to do too much of that in this class I don't think we're going to do any real database management but we will go through the this five step process in class will in a do selection and and pre-processing especially so data mining here is for the other steps as I mentioned or selection so that is as it sounds like looking for and finding the data that's relevant to the question you're asking pre-processing I guess another word for that would be sort of data cleansing sometimes there's missing data sometimes there's data in there that doesn't make any sense and sometimes the data is in a format that doesn't work with your you know whatever language or software you're using so that all has to be rectified before you end up doing something with it transformation is usually a mathematical approach where we are taking data and converting it to a form that makes it more relevant to some sort of statistical analysis that we're going to perform on it data mining itself there are a number of different approaches you can use depending on what sector you're involved in and the whole point there is again to fish out some sort of meaningful pattern in the data that you have and then the fifth and final step is interpretation and evaluation and you could even add on here visualization at the end we're going to have a separate lecture on visualization so again data mining itself is some sort of automatic or semi-automatic so computationally based analysis of large quantities of data so big data and either the idea here is that you would be coming up with ways to discover previously unknown patterns so you imagine having a really big database and you know what it's from you know what the source of the data is and you might have a guess about what might what patterns might be there and that's especially true with the energy sector I think the more conceptually you learn about the energy sector you're aware of what patterns could be underlying in the data and so some in some cases the data mining is just about sort of confirming the patterns are they're the ones that you already know or quantifying the strength of certain types of relationships but in some cases you can you can sort of surprise yourself and you know data mining depend depending on what sector you're involved with there's just a million and one different techniques we're going to go through a couple really common simple ones cluster analysis that has to do with grouping things together grouping usually objects together so think about a database of households you know for for an entire city every households energy consumption or something like that you may be in a position where you want to group different types of energy consumers together based on some sort of common trait which you can do mathematically anomaly detection in the object based so like households or people or consumers or it can be a point in time if we have a time series of commodity prices or electricity demand or some sort of weather variable we may you know be able to recognize at one point it really should you know looks much much different and you know there are different types of ways we can you know classify a point in time as an anomaly based on its value but we have to also be wary of how we do that because sometimes points that look really strange and that we might statistically characterize as an anomaly could be real and have some sort of underlying interesting explanation classification we're probably not going to do too much of in this class that has to do with coming up with algorithms that characterize things as you know junk mail or something that should go to your inbox based on you know text you know in an email you know applied to the energy sector it might look a little bit more like cluster analysis where we're we're going through you know a big database of people's homes and classifying them as you know energy efficient or not energy efficient there's also image classification increasingly aerial imagery is being used to identify objects and you know estimate their size and volume and then from that estimate what their energy consumption can be another use of classification which we'll get into sort of towards the end of the class is identifying you know higher resolution signals in people's home energy data so low disaggregation so taking you know a signal of electricity consumption from somebody's home without knowing what individual units or devices are being used and based just on the shape of that signal being able to disaggregate and tell with some certainty you know what part of that overall signal is their dishwasher versus their dryer etc and then regression I'm guessing most people are familiar with this there's a number of different types of regression the most common that we all see is linear regression or ordinary least-squares regression and this is just fitting a pattern to some observed data with the understanding that the pattern has some sort of goodness of fit but also some error associated with it so let's walk through this this five-stage process so the first the first step is its selection I'm going to try to use a late pen here to make it look a little bit like Khan Academy so this is what we would I guess you know if you ask somebody on the street this is sort of what they would think data mining means where we're really pulling data from some sort of database and you know there there's two important parts to this and the first is obvious and we want to pull the right data or at least the data that's relevant to the question we're asking we don't have to know exactly what we need but we have to be pretty close and so that's you know part of why this class is going to focus just as much on the conceptual knowledge of energy industry as much as we cover different analytical approaches so if we have a truly really big data set there may be cases you know and that if we're trying to visualize some part of this data in its entirety or we are running up against the limits of whatever computational power we're dealing with or we just want to save time or we're doing a preliminary analysis and we don't want to you know deal with every single data point we have when we would want to take a smaller subset of the data to study and this is called sampling so take for example this plot on the y-axis some some you know metric or variable and x-axis we have another metric variable and they're sort of related in a nonlinear way right so the larger value of x in general the larger the value of y at the extreme and then at the other extreme the smaller the value of x the bigger the value of wine in the middle you know more moderate values of X give you the minimum sort of value of y but there's a lot of spread and variability here so this is a thousand data points in each point we can describe in terms of an ordered pair so an x value and a y-value while depending on what this data actually represents we may be okay just randomly selecting members of this data set with replacement in other words picking picking one sort of out of the Hat and then going now you know and then sort of putting it back and then go putting your hand back in and picking another one out of the Hat and doing that you know less than a thousand times maybe a hundred times I actually don't know how big this sample size is you can see that if we do this randomly if we're we don't we're just putting our hand in the head and picking something at random then the sample that we select should show the same general characteristics as the much larger sample here and you can sort of see that visually that you have the same general relationship between x and y here so one of the examples that I've talked about already is this idea of household energy data and you know to my knowledge that's really and one of the main instances where you would be dealing with databases that that are so large that that you might have to do some sort of sampling and you know the other could be you know the other example I mentioned if you're looking at if you're using aerial imagery to you know estimate electricity demand based on the presence of structures right building this maybe estimate how tall they are their volume and then estimate their heating and cooling you know demands that could be another one I'm sure there are others you know but energy analytics in general I think it's big data but it's probably not big enough that you're going to ever need to do a small smaller sample I don't think you are going to run up against the limits of of computation if you're just doing sort of an analysis now for for modeling electric power systems which we'll talk about towards the end of the sister system or semester then you do have to sort of make some concessions for computational power and then just time so the second step after we do data selection is pre-processing and this is the least fun honestly and if you're involved in research or if you're in any sort of analyst job this is sort of what lies ahead of you inevitably data sets are imperfect and you can think about you know there being a million reasons why a lot of its instrument error for measuring something then we have something that's doing the measuring right whether it's a thermometer or a stream gauge or you know weather balloon or you know a plug on a monitor on you know a device these can all go wrong and so for measuring things over time and evitable we're going to have some weird points and we want to get rid of that stuff and sometimes it could be missing sometimes it could be just nonsensical and so you know what a special circle of hell is resolved for is reserved for the people whose job it is to go through and do that by hand right so normally we want to come up with more automated ways to search through a database and make some sort of decision of Alice's data correct if it's not you know interpolate some other value or if there's missing data do the same you know fill that data in make a best guess about what should be there now how exactly we deal with that depends on whether we're talking about a database of objects so for example a list of homes and their energy consumption in a month or something or a list of coal plants or power plants or for Deering dealing with a time series in other words you know a record of hourly electricity demand over a year or coal prices over a certain period of time it may also depend on what other type of data is is available the first option which is one I wouldn't necessarily suggest but depends on what situation you're in is just to ignore it you delete the data point now this could work if you're talking about a database of objects right so if you're looking at if you have a database of you know a hundred thousand homes and you you're looking at their energy consumption data and one looks just bizarre you could you could delete it now that might not work if you're talking about a time series you know if you have a time series of annual natural gas prices and one years missing then you know depending on what you're doing with that time series a gap in that in that record would be problematic and for example if you were using that as some sort of best part of some sort of simulation model you would need a value there otherwise you'd have a gap in time and your program might fail so if we're missing data there's a couple other options we can do besides just stick our head in the sand we can we can try to replace the data with the most common value and this is this is these are all options for replacing missing data when we're talking about object data so not a time series not a record with something through time but more sort of a catalog of a bunch of different things so the example here at the bottom is different customers a B C and D and I guess how much of each thing they buy so 25 units of cereal for customer a thirty one point two five units of milk whatever that means and a lot of spoons so you can see each customer has a value for cereal milk and spoons except for customer C and we don't know how many spoons they bought but we do know how much cereal and milk they bought so one option here we could replace with the most common value so imagine if this database were were much larger and we found that the most likely number of spoons that any customer bought was forty five one option would be to just fill in 45 for customer C we could also replace with the mean value so the average value of spoons bar which is a little different than the most common value and do that we could also perform a silly type of regression here so we could say that we could assume that the number of spoons someone buys is related or could see if it's related to the amount of cereal milk they buy which could be true it could also be you know not true and we'd have to find that out but we could fit a regression relationship which is shown under the table there and in that case it would be finding the value of a B and C a little a little B in Big C which would be the constant that comes up with the value that's a the best predictor of the number of spoons everybody buys for customer a B and D alright and then whatever that model is that best fit model we would then apply to customer C and make a guess about how many spoons they bought so the other option is imputation so this would involve saying well is a little more sophisticated than coming up with the most common or mean value if we knew something about customer C and in particular if we knew that that customer C really wasn't much like customer B your customer D but it was most like customer a in some respect we might be justified in saying okay well we'll just assign that the number of spoons the customer a bought two customers C so that's for object data time series data it's a little different and it depends on how much data is missing sometimes if it's just old it's a lot of data that's missing that affects the choices you make in terms of what you're going to do and I run into cases where if there just isn't temperature data or what you know data about water availability back far enough in time then that means you just your that's when your model stops right you can't you can't model something that that doesn't have any existing data to train the model so it's you know if there is just a little bit of data missing then there's some options for fulfilling those blanks in so to speak and the first again would be imputation and you could identify in this case we have days 1 2 3 4 and we're looking at the maximum temperature and each day and we also know peak electricity demand during each day and we also need know the off-peak electricity demand for each day so for example if we if we decided that we looked at each of these days and and what we really want to know is what their electricity demand is like because we know what the temperature is for all four days but the electricity demand data for day 4 is missing one option would be to say all right well let's look at what the what information we do know which is temperature and it's most like day three I guess we could just simply substitute day three values for peak and off-peak electricity demand so you just drop we would assume the day fours peak demand is 1400 and day force off peak demand is 84 there are 840 a better approach in this case especially when you're talking about electricity demand is regression although even that is a mistake and we'll people will talk about why there are very specific ways you would model electricity demand you know temperature is an important one but it you have to go a little bit beyond that so for regression again here we're assuming that there is a direct relationship between temperature and peak and off-peak electricity demand and so if we wanted to find peak demand for day four we would build a relationship between peak demand and temperature for all the other days for which we do have information and we would fit we would find the values of little a and Big C here in the bottom in the equation underneath the table that result in estimates of peak electricity demand that are closest or have the least error when when we're describing days one through three and then we would take that model and apply it to day four and make a guess about how much peak electricity of man there was given the fact that the maximum temperature was 90 we can also with time series data do linear interpolation and so in some cases this might mean that you know if we are looking at a situation where we know that the information for day one we don't know the information for day two and there are electricity demand we don't know the day three electricity demand but we do know day for you know we I guess we could make an assumption that if we know that demand over the next couple we could assume the demand over the next couple days after day one increases to eventually reach 1300 go it goes from a thousand to 1300 in terms of peak electricity demand we could assume it may be a bad assumption that that increase is linear right so that each day the first day it increases by 150 and then on day three or I'm sorry so the the first day would go by a hundred from a thousand to 1100 on day three I would go from 1100 to 1200 and then from three to four or jump from 1200 to 300 now when we're talking about electricity man that's a pretty silly assumption there's no reason why things would have to increase like that look could be it could be true but it's a better idea I think in this case to do a regression based on on temperature but even that would be some very simplified approach okay so that we went through selection and we went through the data cleaning and now we're ready for transformation and so you know that the need for transformation of the data is going to depend on what particular statistical analysis you want to perform on it and really the most common one that that I use is trying it's a lot of statistical tests that we user are based on this assumption that the data is normally district distributed or its fits to some sort of Gaussian distribution the normal would be a Gaussian with zero mean and a variance of one so if we want to get data in that form and we're dealing with some sort of variable that is not normally distributed there are options for us to convert the data that doesn't fit this bell curve and Matt you know perform some sort of mathematical function on it and then it turns it into data that is is normally distributed then we can do whatever statistical test we want to do on it and then transform it back afterwards so one example of that would be if you want to estimate a confidence interval so you have some estimate of the mean of a population so population or just the you know something you're trying to describe statistically you have some sort of data sample and then you realize that that sample is just as we talked about at the very beginning only a part of a subset of this much larger data set that you have if you want to make a guess about what the the population mean is plus and have some some confidence within a 95% probability that that that that mean is going to be within this sort of plus or minus integral you can calculate a confidence interval now for well normally when we talk about this calculating a confidence interval for a normal distribution means you're just adding and subtracting two times the the standard error or the standard error units to get the confidence interval plus or minus the mean but doing this requires means that you're you re working on an underlying Gaussian or normal distribution so it wouldn't work if your data looked like this if it had if it was skewed to the right or had a really fat tail that's what we would call this and many processes especially environmental processes or economic processes that depend on environmental processes like electricity demand or some types of commodity prices are not normally distributed they are log normally distributed or there's some other distribution out there that's a better fit for what they look like and so in this case on the y axis here we have a relative frequency or probability of occurrence and then on the x axis some sort of value so this is a probability distribution based fitted to a histogram of the you know the sample of some certainty is this does not look normal it's very asymmetrical so if we want to come up with a confidence interval for that data that has a really fat tail we have to convert it to a normal normal or Gaussian distribution at least temporarily and so for data that is log normally distributed that you have a reasonable confidences is log normally distributed for every point in your data set you can take its natural logarithm and when you do that and then read plot the histogram the data ends up looking like a Gaussian or board in this case Gaussian distribution and once we do that then we can compute the confidence interval and then transform those values back using this mathematical relationship shown at the bottom every value that we transform we can then take the exponent of of that in order to transform it back to X there are original data that is log normally distributed so there's a number of ways you can ascertain whether the data that you have is normally distributed and one of those is using a quantile quantile plot which just graphs observed versus theoretical normal quantile and what you what you really want is to see this data distributed perfectly along this diagonal line so in this plot here each little circle is a different comparison of observed versus theoretical normal quantile and you want this to match up perfectly and be this straight or is a diagonal line in this case we can see that most of the data is normally distributed but at really low values that relationship starts to break down we have some values that are below the dying on the line and we can see that at very high values that the relationship starts to break down a little bit for but for most of the the distribution it looks pretty normal here and there is a function in MATLAB that allows you to develop these QQ plots for data that you're dealing with and that's the QQ plot function shown in italics at the bottom so in other cases you know for your if we already have data that's either exists actually as a normal or Gaussian distribution or we've already converted it in some cases we may want to process it a little bit more and in order to whiten it and all this means is converting data to a standard normal distribution so a normal distribution with mean equal to zero and variance equal to one and if we have an existing data set that is log normally distributed for example we could we could take the following steps first we would do what we already suggested which is take the natural log of each data point in order to convert it from that distribution that looks you know sort of like this with a really long tail in order to get it to the bell curve right that's my bell curve so that would be step one we take the natural log of each data point so then it's normally to start Gaussian you know it's normally distributed but it's not the standard normal so we might not have data that is mean centered at zero with variance of 1 to whiten it in order to get it to a standard normal distribution we subtract the mean of our of our data and we divide by the standard deviation so that this is what we do right here if we have a data set Y which is our transformed data right so we took this log normally distributed data and and normalized it then we can subtract the mean of Y divided by the standard deviation and then we get this distribution that is a bell shape and standard normal distribution so not only does it look like a bell state but it is centered at zero and this one let's say that one was centered at 8 or something like that so that means that the this value right here would be 8 right as this one was a up here the mean was 8 and we'd have to subtract 8 here divided by the standard deviation we get a nice standard normal distribution down here so again if we do that if we convert take this two-step process with going from log normally distributed data we're transforming it using the natural logarithm and then whitening it ultimately we get something that looks like this now think how you would try to convert this back so the original log normally distributed data from here and there are some reasons why you might want to do that you know usually if we if we get data to this point it's because we want to do something to it like we could you know increase its variance or you know transform it in some other way but eventually we want to convert it back so we just sort of retrace our steps we would multiply times the standard deviation that we divided by add back the mean and then take the exponent okay so now we're at the data mining step and I briefly outlined the the four things we'll sort of talk about in in this lecture and the first is anomaly detection so and oh it could be an outlier or statistical outlier and that doesn't mean it's not real it just means that we have sort of permission from the students that's from the snack gods to sort of remove it from consideration or it could be something that just doesn't look right based on deviation from normal values or there's some sort of step change it looks really strange but the bottom line here is you're identifying unusual data records that that might be interesting I mean you might want to remove them if they're errors or they could be you could be looking for anomalies in order to study them more closely again clustering is the task of discovering groups and structures and the data that are in some way similar to each other so we're sort of taking a big database and we're saying with figure out the you know the the best way mathematically to take this you know very very large database and put it into eight different groups where within each group everything is kind of more similar than they are with members of other groups classification is the task of generalizing some sort of known structure you know like classifying a program that might classify email as spam or legitimate for your inbox and generalizing that to apply to new data right so classification algorithm is usually are trained they you know parameterize themselves they act on you train them based on some sort of existing data set you make sure they do well sorting the day you do have and then you give it a test right you do give it new data and you try and then try to make sure that it does just as well classifying this new data and then regression again attempts to find some sort of function that models the data with the least error so for anomaly detection there's a couple different pretty just really simple standards to use the first is min max you know that or the other is using some sort of a parameter of a statistical distribution like you could say we want to identify as and anomaly anything beyond two or three standard deviations away from the mean or something like that you know the basic idea here is to create some sort of filter right if you have a lot of data you could create a filter that goes through the data and says you know it from our conceptual understanding of the underlying system we know for a fact that objects can't have a value greater than some max or lower than some min and we can design a simple program to search for any objects with values outside that range so besides a really simple minimum or maximum filter again I mentioned you you could come up with a filter that if you if you think your data is normally distributed and it doesn't work otherwise but if you have data that's normally distributed you could say all right any any data that's outside of the 99th percentile right so if it's far away far enough below or above the mean that I have you know less than or equal to a 1% chance of experiencing that data then we'll flag it right we'll remove it or flag it for further study so again clustering the alcohol ID appears to you know discover group naturally sort of occurring groups in the data that are internally similar all members of the group would be more similar with each other than with members of other groups and it's a little hard to pin down an exact definition on what a cluster would be but that's it's part of why there are so many algorithms for doing this but the upshot here is that you're really just grouping data into different objects but there you know there are a number of different cluster models that are out there and won't cover just maybe one or two but I think it should be understood that there's just a million different ways you can you can design a question model or you know design ways to group data into different different clusters so one of the cool things about clustering algorithms it's probably easiest to explain using two dimensions or even one dimension but let's go a little a little higher but you know imagine if you had an XY axis and you had a bunch of data so here's your y-axis use your x-axis so think back to that original sort of cluster right so not cluster but scatter plot right see had data all over here and we can describe each of them in terms of an ordered pair right so an x-value and y-value and when we're doing a cluster analysis we want to cluster these in space right but our x value and our y value might be something that we really care about not just a random x and y it could be the cost of some sort of generator and it's reliability or lack of reliability or these could be customer groups and x-axis could be the size of the house and y-axis could be energy consumption or something like that or income or anything like that right so we can make clusters based on two dimensions here in terms of an X and a Y but we don't have to stop there the process is not limited to just looking at two dimensions we can go to three or four or five and really come up with a pretty robust way of grouping objects into into groups so one of the one of the most probably the easiest to wrap your head around of clustering algorithms is the k-means clustering and this is just what I showed you so we have an X and a y dimension x axis y axis and we have a bunch of different data these dots that are distributed in this XY space right they can describe each individual dot as an ordered pair an x value and a Y value and depending on those values they end up somewhere in this big square and we sort of imagined that at first all these dots are black all right or red or green but they're all the same color and what we want to come up with is an algorithm that looks at all this data and identifies you know we can input we want four groups or three groups or two groups or ten groups in this case it's four so we want an algorithm that will take an input from the user that we want to you know cluster this data into four groups and it will identify the four groups that that minimize this argument at the at the bottom this equation and what that's what this is basically doing is saying each one of these objects here is an X this is an each X here is an ordered pair right each one of those this X means every set of all these ordered pairs and what we want to do is minimize over each one of these groups over Group one group two no matter how we configure them group three in group four we want to minimize the sum of the distance between each individual point so like this little blue dot and the middle and then this little blue dot and the middle and we do that for each one so we would also do that with a green cluster up here each each dot in the middle right so work what we're really doing is figuring out how we can draw circles around these dots or at least classify them or group them into colors that we're minimizing this the sum of all the distances between the centroid or the mean the center of the group and all the points that are distributed around it and you can imagine that that's sort of a lot of work to do by hand but for a computer it's just this algorithm it's sort of a search algorithm it tried lots of different combinations until it finds the way to color all these dots so that it minimizes that so there are a couple other clustering techniques besides k-means and there's an example of k-means that I guess I I would highly recommend going through this is the geometric or k-means method with example from it directly directly applied to MATLAB and then there's another here here are kacal clustering which you could take a look at as well okay so then there's classification you know I don't think we're really going to do much classification until we get to the non-intrusive load monitoring so that's the energy demand disaggregation towards the end of the semester and even that's not I don't I mean it's it's a type of classification but you know most classification that you really hear about from a machine learning perspective is text-based so going through you know different you know emails or other sub types of texts and and classifying on this in some way you know for for our purposes you could apply classification algorithms to numerical data energy related data you know you could open in examples would be looking at peer you know records of precipitation and rain flow and describing them you know as a drought or not a drought or electricity demand you have some sort of way of classifying it as a peak or not peak period power plant emissions dirty or clean home electricity consumption efficient or not I mean the idea is to take a database that already appropriately classifies objects according to some criteria which you set and then to train it to similarly classify a bunch of new objects that haven't been analyzed yet and there's a there's an example of different types of classification that you can look at again the link is at the bottom I would I would suggest doing that so the last data mining approach will cover and very briefly here today is regression that we're going to spend a lot more time on regression later on in the semester but the whole idea here is you're trying to model the relationship between a response variable so an output and some other predictor or group of predictor input variables and it's a you know it's similar to classification and that it's a it's a you know it's a training algorithm you're taking some sort of existing data set that you have you're building a model that does well to predict that existing data and then really you want to use it to predict something else you want to apply it new forms of data to make predictions or estimates about the output variable and so these models can have a number of different forms and you know usually we think about linear regression as being just a straight line but that doesn't always have to be have to be the case on my order it doesn't not necessarily have to be linear here we can have exponents here or we can have different variables a B and C to interact with it alone on there so here Y is the output variable it's what we're trying to the process we're trying to model or maybe and predict and what we're assuming here is that given some sort of value of a B and C we can we can come up with coefficients that would go in front of a B and C or multipliers that no matter what value of a B and C we have correctly or well while minimizing error predicts what the value of y would be and you can see there's all sorts of different forms you could use and that you know the forms you use here would be maybe based off of your understanding of the physical system you're trying to model or maybe you know selected with help of F of a fitting algorithm so we're going to go into much more depth of our regression again so I would just look again through the the math works workflow example and just you know just go through it to make sure you sort of know what what's happening so that's the end of the first lecture these are the references I used throughout unfortunately they are not it's not specified throughout where I'm pulling things from but these are all resources that you could take a look at it you're curious you know look I'm looking at the clock I spoke for about an hour that's about 20 minutes at least longer than I wanted to talk so that's I'm sorry it was so long but it's informative for me I'll I'll try to keep it a little shorter in the future all right I'm going to put this on YouTube Thanks 
2iEkl_k9azA,27,"Advanced Digital Signal Processing-Wavelets and multirate by Prof.v.M.Gadre,Department of Electrical Engineering,IIT Bombay. For more details on NPTEL visit http://nptel.iitm.ac.in",2015-09-09T09:27:18Z,"Mod-01 Lec-38 The Applications *Data Mining, *Face Recognition",https://i.ytimg.com/vi/2iEkl_k9azA/hqdefault.jpg,nptelhrd,PT55M54S,false,1094,6,0,0,1,a warm welcome to the 38th lecture on the subject of wavelets and multi rate digital signal processing as promised in the previous lecture we shall use this session to discuss two applications of wavelets and time-frequency methods in great depths in fact the two students based on whose application assignments this lecture is constructed are going to discuss what they only introduced very briefly in the previous lecture they had kind of given a trailer to their presentations in the previous session in which they had just explained the essence of the application that they had done in this lecture they should be explaining the details of the application and also pointing to some of the results that they have obtained I must mention that these are two students who have actually used these two lectures to learn the subject use the lectures over the semester to learn the subject and have undertaken to explore two applications of wavelets all over the semester out of a match of about fifteen to eighteen students the assignments that are going to be presented today have been found to be some of the best in the class and therefore in a sense it is also an appreciation of the excellent work that these two groups have done that they have been invited to record their presentations today in a broader sense this is also to encourage whenever people use these lectures outside that students should be involved in exploring applications and we hope that the hard work and the very intelligent efforts put in by these two storing these to grow actually of students would inspire many other students who listen to this lecture to explore several other applications of wavelets many as they are anyway with that little introduction let me put before you the two applications that are going to be presented in depth today you have had a trailer of them but I would like to put them in the broader perspective of the subject so in the lectures today we are first going to look at the application on data mining Kunal Kunal Shah is going to present the application on data mining on behalf of his group of two students namely he himself cannot shine or poach Audrey now data mining is a generalization of representation so in data mining canal is going to show us how one can use the properties of wavelets in efficient representation to advantage in retrieval and other such applications from a database the second application which is going to be discussed today is face recognition now you know there are also going to be subtle differences between detection and recognition and so on and these shows I don't want to take away the thunder as I said from the person making the presentation but face recognition is an important and increasingly important application in many security systems and other image processing or vision systems so both of these applications are of great importance in the modern world and we shall now without much ado invite these two speakers young student speakers to present the work that they've done over the semester and to put before you both the concept and the result of what they've done I shall first invite Kunal Shah to make the presentation based on the work done by his group of two students ganancia and orko Chaudhary thank you hello friends today I would like to talk on the various applications of wavelets in data mining at data mining as sir said data mining is used for efficient representation of data so today I would like to speak on that so the problem statement is given a time series data the time series data could be huge our aim is to improve the efficiency of multi-level surprise and trend queries from that time series data now first of all what is the meaning of surprise and trend queries now when we have a long time series data generally we don't encounter point queries for example if they record the temperature of a particular city for a year say we never asked what was the temperature at this date of this month we always asked the trend how was the change in the temperature during the month such queries are called as trend queries one more type of query which we encounter is surprise query was there any sudden change in the temperature in a particular city of this month so such type of queries we have to handle and wavelets efficiently handle such type of curries that we have to look now what is the meaning of multilevel yes such queries are generally encountered at various level of abstractions it could be a month it could be a year it could be a decade so if someone asks what is the change in the temperature during a particular month we should be able to answer that easily and if he asks what is a particular temperature during a decade sudden change or average in a decade that also we should answer so decade here month shows various level of abstractions so we have to improve the efficiency of such type of queries so first of all how such huge data is represented so suppose X tilde is my whole data so I represent that data in the form of a matrix say of size m cross n suppose I store the stock prices of a particular company or say many companies so let n be the total number of stock prices that I store for a particular company say if I store for one year then n will be 365 let M be the number of companies of which I am storing the data so each row of this matrix represents the stock prices of that particular company this is how my whole data is represented now what I need to do I need to do three things first of all I need to efficiently store this data how can I efficiently store this huge amount of data say for example if I have data of say one decade so n will be equivalent to one decade and M will be say hundred companies or 200 companies so efficient storage is very important secondly how can I retrieve the data efficiently and thirdly if I want to add or modify certain things in the data how can I easily modify it these are the three things if these three things could be done efficiently then we can using wavelets then it will be a very good tool to the represent this data so first of all let us look at one of the methods which are already used in data mining namely singular value decomposition method what is done in this method is I have a data say X tilde I represent it in matrix form you which is a column orthogonal matrix of size M cross R this is a diagonal matrix of size R cross R and this is again a row orthogonal matrix of size R cross n where R is the rank of matrix X tilde so I am representing this data in terms of three matrices now instead of storing the whole data I am storing these three matrices so if R is small I am already saving under storage capacity of my data which I will need to store M cross n matrix now I am storing just three matrices m cross R cross R and R cross F of which one of them is a diagonal matrix now I have u diagonal matrix and V suppose I want to extract the data for particular company say a particular row of this matrix X tilde let that be represented by X so to extract a particular row that means to extract the data for particular company the complexity required is of the order of size of V because multiplying these two since this is a diagonal matrix multiplying these two I will be able to extract a row of a particular matrix so to extract a particular row the complexity required is of the size of B but the size of these R cross N and n is huge in our application say a data of a decade so still the complexity is huge so the complexity of reconstruction in singular value decomposition is huge now one can argue here let us reverse the order instead of storing it as M cross n why not store n cross M because M is not that huge but the problem here is now if you want to extract a particular company's data you need to extract the whole column rather than a room the complexity of extraction of a row is of the order of size of me now of the column so again if you want the extract column you your size is equivalent to the complexity of me so you are not improving your efficiency by reversing the order further one more disadvantage in this method is if I want to modify the data I need to recompute all the three matrices again which is not the case with wavelets as we will see I would just like to warn the audience here that this does not imply that singular value decomposition method is not a good method in fact it is used extensively but in this case since of n is huge and since we require frequent updation wavelet has a upper hand over this method now what is wavelet how are we going to store the data using wavelets X is one row of that whole matrix X tilde I am decomposing it into two subspaces approximate subspace and a detail subspace the approximate subspace is also decomposed into two one more a second level of decomposition approximate subspace and a detail subspace this is called as a TSA pre this is friend surprised abstraction tree this stores all the trend data and this stores all the surprised data so wavelets naturally store decomposed the data into to form strained and surprised you don't need to extract something because wavelets naturally can decompose any data into trend and surprise form this is the biggest catch here in this type of application now how does this splitting and if I want to merge this merging take take place the split operation is very easy if I have a data ax I I just passed through a low pass filter and a high pass filter down sampled by two and I get the next level of subspace so this is a split operation this is generally done in the Bay in the course I did need not emphasize on this similarly the merge operation is just the reverse of this up sample by to pass through the same filters and then add so you go one level higher so this is the merge operation the split and merge operations are easy but the properties that this tree hold are very important first of all we can get pufta tree construction so now what I am doing here is I am not storing this rather than this and storing this approximation subspace and detailed subspace so I am storing this data rather than storing the signal and now I am saying I can perfectly reconstruct this original signal from this data and this is very important perfect reconstruction is very important if it is not so then if in case I have a point query say what is the temperature during this day how will I get using that approximation and detail subspace so perfect the reconstruction is very important which is the case with wavelets this we have already studied again the power complimentary property is also very important because on decomposition we are still preserving the power and this is also done the third is very important as we move down the tree the size of each node these are called as nodes is decreasing so if the size of this signal is say n this will be n by 2 n by 2 and so on this will be n by 4 n by 4 n by 8 and by it so size is continuously decreasing so that is that is very important that we will see how secondly these nodes are called as leaf nodes and we will see there that instead of storing all the nodes it is appropriate to store only the leaf nodes now why this since the size is decreasing why this point is very important because here we are assuming that the split and merge operations do not incur any cost they incur negligible cost whatever cost is incurred is to extract the data or cost is due to the disk i/o operation so the amount of data you that you are extracting is actually incurring cost so cost is directly proportional to the size of the data and bingo is decreasing if I want a eight level decomposition what I need is just the approximation and subsidy tails of space of that eight level and the size of that is very small as compared to n so the cost incurred is very less just require a small post-processing and you can reach X for example if I want a third level decomposition I just need to extract a data equal to n by 8 and n by 8 I can perfectly reconstruct here I can reach her and by that I can reach here X so I just have a cost equal to 1/8 the cost which I require by extracting the whole signal now as I pointed out the leaf nodes are sufficient to give us the trend as well as the surprise details how suppose I require a trend at the third level so what I do is I just extract this data and without using this I don't perform perfectly construction I reach this data by up sampling and passing through a low-pass filter again without using this I up sample and pass through a low-pass filter and reach at this level and in this way I reach at this level so using a data equal to size n by 8 I am reaching at this level without perfect reconstruction and I am getting the trend at the x3 level of decomposition if I want the surprised data I go from here go to X to go to x1 and X just here I need to pass through a high-pass filter instead of a low-pass filter this is the synthesis branch merging operation now if I wanted this level then first I need to extract these two both and I had to need to perform perfect reconstruction at this level and then I can go to approximation so this is our way with small amount of post-processing of split and merge operation I can find out the trend n surprised ways now the optimal TSA tree is that tree which toes only the leaf nodes now when I store only the leaf nodes it is very easy to see that the total size of the leaf nodes is equal to the size of additional signal so I'm not increasing my size by storing only the leaf nodes I'm not storing the whole tree I'm still getting all the information by using only the leaf nodes so my optimal TSA trees that tree which incurs a minimum cost and a minimum storage and by storing the leaf nodes I am incurring the minimum cost as well as the minimum storage further if I introduce one more node which is not a leaf node I will improve my performance but that is also not difficult to prove that that performance increase is just marginal so it is no point to store any other node except for the leaf node once we have seen that even the retrieval is cost efficient and even storage is cost efficient can we improve more on the storage by reducing further on the leaf nodes if I remove some of the leaf nodes can I still get up on most accurate results yes wavelets are very good at compression you compress your leaf nodes and still you can efficiently store that data and improve your accuracy I would not say improve your accuracy but you are not compromising more on the accuracy so one of the methods is no dropping and in no dropping we are exploiting a very important property of wavelets which is the orthogonality property what does that say suppose I drop one of the leaf nodes say DX 3 and I reconstruct the whole signal using the other leaf nodes and let that reconstructed signal be X cap now if I find the norm norm square in the error that X is the original signal and X cap is the reconstructed signal by removing one of the nodes then because of the orthogonality property norm square is equal to the norm of the node which we have removed so the error is actually the error in the coefficients which we have already removed so s is the number of nodes which we have removed this is true only because of the orthogonality property of wavelets which is not the case always so if this is true then we can use a greedy algorithm to determine which other nodes significant in that data so what I calculate for each node each leaf node I calculate the norm square of that node and divide by the size of the node and if I calculate the maximum of on and this is the most significant node which I find so I store that node then again I store the second best node again I store the third best node and I reach a particular stage and if I find that I have already completed my disk space then I remove the remaining nodes so this method is called as no dropping but here there is a slight disadvantage in this method is that if the node which I have removed contains some outliers or some important information then that is lost which is not the case with coefficient dropping which which we shall see in coefficient drop ik I store all the leaf nodes as a sequence and each coefficient whichever the significant coefficients I store the whole significant coefficients rather than the nodes and since I am storing coefficients I need to store the index of the coefficient also and still this belongs to which node since now for every coefficient I am using double the memory storing the coefficient as restoring the index I have to just check how many coefficients I have used of a particular node if I am using a memory greater than that node size then it is better to store the whole node rather than storing the coefficients if I am not storing any of the coefficients of that node I will remove the whole node and if this size is less than the size of that node then I will store thus these coefficients and this is called as a coefficient dropping method or hybrid coefficient dropping method now let us see the results what we have got here so I will just switch over to the slides to see the results here these are the group numbers kunafa myself and arc or coach oddly these these are the details of the trend analysis this is the first graph is the original data it is taken from stock market and it is a data of SBA State Bank of India of the two years so it has around 700 coefficients these are the trends or the decomposition at various levels the second graph is a 2-day decomposition the third graph the four-day fourth graph the eighth day and so on and as you see as you are going down the averaging is increased so these are the trend analysis if I see the last level and if you ask me what is the average over a particular eight level data then this is the eight level data average which you are seeing it would be more appropriate to show to interpret it on surprise analysis if you see on the surprise analysis here the first graph shows the there is a small outlier in the original data which is more prominent in the second average in the second a surprise as you can see but as you go on increasing a decomposition level you can interpret the data this way the surface which was looking prominent in today is not looking as prominent as you increase the number of days it is averaging out in fact you might see that by inspection from the original data you could very well observe that surprise but as you go down the level that surface is actually averaged out so if you look at the last level there are many surprises which you can see in the previous data which by inspection you cannot encountered only by decomposition you can encounter that here here also there was a surprise component here also there was a surprise component now this is the no dropping method the above signal is the original signal and this is the recovered signal after no dropping and we can see that it is almost the same but the disadvantage here is that we have missed that outlier which is seen that impulse sort of which is seen in we have missed in the note dropping and we have removed 300 coefficients in this out of the 700 coefficients and this is the result so we we have removed almost sixty forty percent of the coefficients retained only sixty percent but in coefficient dropping method as you can see that outlier is retained even though we have removed 300 coefficients in the trend analysis we have used dobisch wavelet and we see that as the as we move above the family the only thing that is changing is the averaging is more as you increase the filter length so as you increase the improve I improve go on increasing from dock to 2.4 to debate the averaging is increased this was all about application of wavelets in data mining the reference paper is the following is as seen on this slide I have just read this paper and interpreted on as a student but I can interpret from this paper so my observations my interpretations from this paper I have just shown in this application of assignment and this is the reference which I have taken for stock prices of SBI from Yahoo stock thank you I hope you enjoyed the application assignment so that was a very interesting presentation from Kunal Shah on behalf of Kunal shine or pooja during the group which worked on the application of wavelets specifically they have looked at the application of the lavash family in representing databases efficiently you will notice that the idea of course representation and incremental information was given a different meaning in this context so the idea of course representation gave what is called the approximation information which he called X and the so called incremental information gave you the surprises the novel information at every scale now one must also take note of the difference between the node dropping approach and the coefficient dropping approach in the know dropping approach one notice that with the same level of compression some important information was lost a sudden spike was not so clearly when coefficient dropping approaches were used then one could see that that was retained rather well so in addition to the transform that is used how one uses the data obtained from the transform is equally important in wavelength based representation that is what canal's presentation has amply demonstrated now taking further the presentations of students on what they've done for their applications we have the second presentation which was very briefly introduced in the previous lecture namely that on face recognition without taking away from his presentation I shall now invite Rona cronic Shah to make his presentation here based on the application of wavelets in face recognition so row not for you now as Professor gonna mention at the second application that we are going to look into today is on face recognition through wave packet analysis so Kunal mentioned an application in data mining and it was in one dimension so the data that he has was in one dimension when you move from data mining to face recognition the data that we have is in two dimension as I mentioned in the last lecture there are two keywords here one is face recognition and another is wave packet analysis before going into details I would like to briefly summarize what these keywords mean outside with your packet analysis why we need wave packet analysis for the task of face recognition as I mentioned in the last lecture we need some kind of D correlation in spatial as well as in frequency domain for the task of classification as we know the task of classification can be better come pleased if we can do some kind of D correlation in spatial as well as in fact dumain so in that way billet analysis naturally fits into the scheme of things but then why we go for where packet analysis again as I mentioned in the last lecture in wave packet analysis we decompose not only approximation subspaces but we also decomposed detailed subspaces as we have seen in lectures now when we do the task of classification we need a richer representation of the underlying signal that is we should not miss anything out from the signal from that the length signal and then the length signal here is face that we have seen so face images will be order signals so the task of face recognition can be can be accomplished by wavelets as well as your packet analysis but as I mentioned we use view packet analysis just for the richer representation and wave packet analysis also provides the correlation in spatial as well as in frequency domain so that is better suited now before going into nitty gritties and details I would like to first provide references so the work that I'm going to present here is based on the work done by Garcia zeikos and Zetas it was a research paper in European conference on computer vision and the work nicely by utilizing wavelet based framework of face recognition now the work that I'm going to present has been motivated by their work but this is an extension as well as a normal description of the work done by them so here we utilize a different database namely the Yale database and the work provided by Garcia's eCos and Zetas was based on different database like ferreted farad database as well as faces database so here we don't only provide extension to different database but here the understanding is purely from a point of view of a student how a student can look into this application and how this particular framework based on wavelength it fits into the whole framework of face recognition like what are the approaches that have been there in the literature and how that threatens how this approach fits into a particular framework there are some other reference is like the second reference here is based on eigenfaces and this is quite a popular approach also there are some other references like the approach based on PC a principal component analysis this framework has also been popular the reference that I mentioned the first reference compares this PCA based approach and eigenfaces based approach to wavelet based approach the fifth approach here that is mentioned like by Zhou chilapa Phillips and Rosenfeld it provides a nice framework or a nice literature survey on the overall work that is done in the task of face recognition so this is for references now we shall move into like why face recognition is required as a mentioned in the last class face recognition can be required for biometric authentication but as I mentioned the face like the signal that is obtained from face namely the image a 2d image it can easily be moved like someone can grow mustache or seven can grow beard in a given period of time and that can easily be moved or someone can come up with sunglasses even there are some other signals which are available like retina or let us say for that case fingerprints they cannot easily be moved and they can easily be utilized for biometric authentication so the task of face recognition for biometric authentication framework is limited however we need face recognition for the task of surveillance in surveillance one needs some area or region to be manned and some generally in this kind of region only a limited number of persons are allowed so the axis is allowed also one might be interested in what the subject is doing inside that particular region also we have some learnt prototypes like which kind of activities are allowed someone is telling inside that particular region or someone is doing some kind of abnormal activity inside that region we want to detect that as well and generally the cue for this kind of framework starts with face recognition so under the task of surveillance we may do activity tracking as well as recognition and if we can do this faithfully then we can also provide abnormalities not only in this task but also in tasks such as automatic character recognition in let's say movie clips if there are some broken movie clips on let's say some parts of the clips are available on let's say for example YouTube then we can also perform automatic active recognition which actors are present in which scenes and we can also do effective archival based on actors so in YouTube when a large database of videos and images are available we can perform this archival tasks faithfully so we can classify some of the SOPs like soaps and serials based on what are the characters that are present inside those scenes so this is for the task of face regulation why we required face recognition and we see the task of face regression is crucial so accuracy should also be commissioned for that case now the approach that I'm going to mention falls under the tar falls under the approach of feature based approaches so there are two approaches that are basically used for the task of face recognition one is geometric approach another one is feature based approach in geometric approach one goes on detecting the basic features of Lex a nose and eyes cheeks Chin's etcetera and generates features based on this this extracted features of nose eyes and the cheeks and chains but the problem here is these features are difficult difficult to extract because what we have in effect is just a 2d image so why not to look at face image as just a 2d image and we can represent him in some other features rather than extracting eyes nose and other features etc so the other feature that I'm the other approach that I'm going to talk about is a feature based approach and the approach that I'm going to mention falls under the category of which are based approach the block diagram if you look at of the approach that I want to mention looks similar to this this is for prototype learning so given an image or video one may detect faces so this is a face detection algorithm but this is not part of this discussion because as I mentioned in the last lecture this two things can be decoupled easily because it face detection one looks for the features which are very similar across all the faces so that we can faithfully detect what are the faces that are available in the image on itself for example video whereas in case of face we'll look at features which are distinctive across phases from phases so this can be decoupled and hence we are not going to talk about phase detection here what we are going to talk about his face recognition so after extracting a region of interest in which phases are available we can perform sub-band decomposition and here we are going to perform sub band decomposition by using wave packet analysis after performing sub band decomposition we go for feature extraction now this feature extraction is a crucial job because we cannot involve each and everything to be inside a feature vector that I'm going to discuss next we can also perform future normalization because we don't want one feature vector for each image rather we want feature vector per class so if we have let's say 6 to 8 images per one subject then we want a compact feature vector for each subject and not for each image so feature normalization can also be performed this will basically give us learnt prototypes and this learnt prototypes can be stored in memory and can be accessed in future when a query image comes to us this is the block diagram of machine so if we have learnt prototypes with us in which feature vectors of different classes are stored then we can perform matching so given an image like this a query image we can first do phase detection and then we can basically apply the same algorithm will end up getting a feature vector and we have nonprofit eyes that we can access from memory and we can utilize some distance metric or some similarity to come up with different matches so based on application we can give output as one image or multiple images now there can be two kind of applications for face recognition one may be based on content-based image retrieval when one image is given as input in content-based image retrieval framework we need all the images that are there in the database to be extracted which match to the query image and in another application we can perform a simple traditional classification in which an image is given and we just want to know to which class it belongs to in that case only one match is required now how do we perform decomposition like in case of boomlet analysis like this application like so this slide provides how we can perform decomposition of a 2d image this differs slightly from a 1d signal because now the image is to be here so this is a 2d signal so we need to perform filtering in rows as well as in columns and down sampling will also be performed in rows as well as in columns so first we perform filtering across rows and we do down sampling across rows after getting images here by performing filtering as well as down sampling across rows we can move to columns we can perform high pass filtering and low pass filtering and down sampling across columns and we can get four sub images so this ll is the approximation sub stress here because it is passed through by two low-pass filters and other filters like other semi mages required to be passed from high pass filters so they contain high frequencies in effect but this differs from you packet analysis in wavelet analysis we move from first level to second level of decomposition by only decomposing approximation sub species whereas in case of wave packet B composition we decompose details sub species as well so if this is the first level of decomposition this is the approximation subspace and these are detailed subspaces then we do not only decompose approximation subspace but we also decompose details of species so we'll end up getting sixteen subspaces here in which one is the approximation subspace and others are other fifteen are detailed subspaces so this approximation subspace will contain low frequencies as desired and this details expresses Wilkens will only contain high frequencies in this order so this last detail subspace will contain the most high frequencies so we get some kind of decomposition in spatial as well as in frequency domain and this also provides D correlation so now we can generate feature vectors but before going into that we need to look into which kind of filters we have utilized for this purpose now the important advantage of using this vo packet application is we can utilize any filters here when we go for face recognition we do not want synthesis or a perfect reconstruction to be we only want our analysis filters to be good so that they can provide D correlation in spatial as well as in frequency domain so here without worrying about orthogonality property or any other property we can generate those filters which provide very good feature vectors for our classification task so in practice one can go for imperil empirically searching filters which are best suited for this application of face recognition and here are the features which were found to be empirically suited to the task of face recognition and these are the impulse responses of low-pass filter and high-pass filters that are used for the task of face recognition this filters are discrete in nature but for the ease of visualization they are connected like the two integrated samples are connected by straight lines so here we can see the impulse responses of low-pass and high-pass filters one might also worry about how they look in frequency domain and this slide basically shows their magnitude responses so this corresponds to crudely a low-pass filter and this corresponds to a high-pass filter so the filters that we have used did not they are not so much different from what we have seen in the course they are basically low-pass filters and high-pass filters but this magnitude responses were found to be better suited to face recognition tasks so when we talk about first level of B composition this high-pass filter and low-pass filters were found to be better but we are decomposing this to the second level because after second level generally the images that are found from for for face recognition they are quite small in nature so as we go on decomposing these images they become smaller and smaller so it becomes really tedious after second level of decomposition to handle such small images and also the localization that we obtain in time domain becomes really crude so they are not of much importance they do not yield much information so we go up to a second level of decomposition when we look at this view packet analysis in second level of decomposition we obtain four filters and I'm going to present the magnitude responses of these four filters so when we decompose an approximation subspace we obtain magnitude responses of this low-pass filter this band pass filter here we can see that this is cochlear low-pass filter again but this cutoff is quite stringent down and here this is a band pass filter but this mostly emphasizes the low frequencies which are available because we are decomposing approximation subspace also when we decompose a detail subspace as is required in wave packet analysis we end up getting these two filters again one is a high-pass filter and one is a bandpass filter but this band pass filter emphasizes higher frequencies rather low frequencies as we saw in the previous slide now this is this filters arc okay when we talk about one-dimensional signal but here we are talking about two-dimensional signal but the important characteristics of this filters are they're separable in nature so as you apply in one dimension you can have plane second dimension and we end up getting sixteen filters rather than four filters as in one dimension case and but the ease of visualization in one dimension helps us to talk about one dimension and I will not talk about today I would have mentioned impulse responses or magnitude responses because they are just natural extensions as well as they're difficult to visualize in nature rather than 1d signals so given a lena image let's say for example we can end up getting this kind of subspaces well this is the approximation subspace we can easily visualize this because this contains low frequencies and the information here is more compared to other details of species these are seven other detail subspaces important thing to note here is this detail subspaces and approximation subspaces are quite small in nature but just for ease of visualization they're being zoomed into also like this detailed subspaces are bipolar in nature like they contain negative as well as positive values so just for visualization they have been taken as absolute the values are absolute here the the subspaces are also detailed subspaces and we can visualize here so those were eight and these are again eight other detail subspaces so these are all sixteen and it are subspaces that we are going to obtain here we can see that some of the features like eyes are very clear in the high frequency subspaces and other sub species contain the overall shape of the face image now after getting this subspaces 16 subspaces namely we can go on to do feature vector extraction now the trouble with feature extraction see here the important thing is to extract those features which are really want to face Amina's one might say that all the features all the pixel values that are there in 16 sub images are important to us and go on to concatenate all the values that are available to us but the problem doing this is this feature vector will be quite long and if we want to do a classification task this becomes increasingly difficult because if you have a long feature vector we better have long number of images large number of images then and then we can go on to do a better classification so rather than going into this we can provide a compact representation of the faces by providing one way of doing this is to go for moments so first order moment or second order moment we can go for mean and variance is basically so we have 16 sub images we can go for mean and variance in sub in each sub image so we can extract a 32 dimensional feature vector but here if we look at a detail sub spaces then they have zero mean so we do not need to capture that so we can basically go on to represent 17 dimensional feature vector for each face image but if we can see at approximation and detail sub spaces they're different in nature an approximation sub space we have more information so it caters for handling approximation subspace differently for rather than detail sub spaces so if we look at this approximation sub space what we can do is we can bound it by two boxes in one box we can basically go for a high dimensional features like what is the exact shape of the face or what are other fringe information that we have in that bounding box and we can go on to cater for mean and variance for that bounding box we can take another bounding box in which other information related to shape a like face of eyes and nose are related so we can extract mean and variance in that bounding box so we can extract four features from approximation sub space and we can extract variance is in each detail sub image and we can expect 15 dimensional feature vector from here so four dimensional feature vector from approximation sub space and 15 dimensional feature vector from a predator sub spaces we can extract 19 which a letter for each image now given a task of classification we can go on to extract nineteen dimensional feature vector for each phase image and in effect we can do some feature normalization so that we can end up getting one feature vector for each class and after that we can compare the feature vectors that we have in database with the feature vector of the query image and we can do some kind of we can utilize some kind of distance metric but the important thing here to note that if we have utilized mean as well as variance so we have information related to probability density functions of the underlying feature vectors now if we utilize the Euclidean distance as a feature vector as a distance metric then the travel is Euclidean distance assumes all the dimensions to be independent of each of them so it doesn't cut off for the probability density functions that we have rather than we use bhattacharya distance that takes care of probability density functions that we have it basically goes on for mean as well as well so we have two different entities here and one for mean as well as one for variance so if we have the same probability density function mean will be similar as well as variances will be similar so both these terms will vanish and we'll get a zero distance the exact match also but Achara distance scatters for all the requirements needed for it to be a proper proper distance metric so but atera distance can be utilized but there is nothing magical about bhattacharya distance any other distance metric which caters for PDFs underlying PDS under feature for feature vectors can be utilized for this purpose now I will quickly move towards the results that we are that I have obtained by utilizing the yield determines yield database is basically available for free download for non-commercial use from this link and these are the typical faces that are available in L database for one subject gel determines basically contains hundred and sixty-five images for 15 classes and 11 images per class this is these are just the typical images that are available in the L database I'll just provide a typical archival result that I have obtained from the CBR interface for face recognition tasks so given an input like this a query image we just surprised expression over the face of the subject we could retrieve images similar to the subject like the subject is similar in all the retrieved images but expressions are different like one is winking for example the other is with happy look on the face it's a trap so this is the CBR interface but in order to quantitatively evaluate the algorithm we need to provide quantitative results and this can be provided by traditional called classification tasks so these are the experimental results three experiments were performed based on different number of images utilized per class for learning in first experiment four images per class were utilized for learning in second experiment six images were utilized and the third experiment eight images for utilized per class so in total in first experiment we had 40 images for learning and second experiment we had 60 images for learning and third experiment we had 80 minutes per learning all the 80 images were utilized as query images and the fifth column shows the accuracy that we have obtained in first experiment 66 images mesh to the native class in second experiment 64 images and third experiment 64 images again mesh to the native class so the accuracy is around eighty to eighty two point five percent to eighty percent within the accuracy of one point twenty five percent now before going before very too much on results I would like to remark on some of the things like this experimental results can be enhanced like this 80 to 82 percent accuracy can be enhanced to 90 to 95 percent of accuracy if one goes from machine learning techniques like for example support vector machines or one can probably utilize more number of moments like kurtosis or third order moments etcetera and this accuracy can be enhanced with this with this remark I would like to conclude this application thank you very much that was a very beautiful presentation on the application of wavelet specifically wave packets in face recognition I would like to emphasize a few of the points that were made in this presentation one was the distinction between face detect and face recognition in fact in a sense wavelets for that matter wave packets assaulted both to the detection and the recognition problem in detection one looks for commonality what constitutes a general phase in recognition one looks for specificity what is the incremental information in that face so the separation into common and incremental information is again critical in the context of both face detection and face recognition we looked at the beautiful decompositions which roll up short using wave packet analysis we notice that different kinds of features came out in different sub bands now in fact one can study that in greater depths what has been presented here is an indicative study what difference a band show you know Roanoke also presented the frequency responses of the filter that the second iteration in wave packet analysis and you notice again a confirmation of the theoretical discussion that we had when we carried out wave packet analysis in one of the previous lectures we saw that when we decomposed the high frequency band that is when we followed a high-pass filter and a Down sampler by the anguses filter Bank again there was a band inversion so high followed by low actually gives you the higher frequencies and high followed by high gives you the lower frequency this should be noted as you notice as expected there were two bandpass filters aspiring to become bandpass filters between PI by 4 and PI by 2 on the normalized Omega axis and PI by 2 and by my fall on the normalized Omega axis and of course there was a low-pass filter with cutoff by by four and there was a high pass filter with cutoff three PI by four these were all aspiring to become these kinds of filters now we've seen two very interesting applications of wavelets and filter banks today there are several others I should just mention a couple before we conclude the lecture today one of the other important areas of application of wavelets and time-frequency methods is biomedical signals biomedical signals could involve evoked potentials there could be magnetic resonance imaging signals MRI signals they could be signals obtained from cat scanning computerized axial tomography scanning and many other imaging modalities that are useful in medical image processing today that's one area in which wavelets and time-frequency methods filter banks have been heavily employed we hope to have another presentation of an application of this kind in a subsequent lecture wavelets have been used also or proposed for use in digital communication people have talked about building modulation and demodulation systems on wavelets because they are nice time-frequency atoms another area for the mathematician in which wavelets have found great applications is the use of wavelets and solving differential equations at this point I shall not say more safe to mention that these are just some of the myriad areas in which wavelets are used we shall hopefully be able to see some of these applications in greater depth in subsequent lectures with that then we come at the end of this lecture and we shall proceed to discuss something different to the next one thank you you 
hqjDGpaEx5Y,27,,2021-03-13T17:27:38Z,1.2 Need and evolution of data mining,https://i.ytimg.com/vi/hqjDGpaEx5Y/hqdefault.jpg,OU Education,PT3M55S,false,400,2,0,0,0,hello guys we are back with our next lecture in this lecture let us go through the concept of why we need data mining or what is the need of data mining okay yes so similarly we will also go through the evaluation of data mining so basically how data is stored previously and nowadays how it is being stored so those things will be discussing in this lecture yes okay so let us start so basically what is the need of data mining so what we are getting by using data mining guys so from a huge data we are getting some important things right okay so if i give you 1 tb or 2 tb of data can you extract the useful things guys yes you can extract but it will take some time right yes so indirectly the process that you will be doing is nothing but data mining guys so as the growth of data is really high to extract the important information or important patterns we need the concept of data mining so in that way you can write for this question guys okay so as we know today the growth of data is moving from terabytes to petabytes yes so whenever the data is a huge data they'll be calling it as big data guys okay hence storing that data and getting some useful information from that data is the process of data mining okay yes so basically even that processing that that all things will take some time right yes so instead of saving all those unwanted things so we will be filtering that and we will be doing the data mining concept to filter the data okay so now let us go through the evolution of data mining so we will be discussing from 1960s to 2000 guys i'll be just giving you the names guys because there is nothing much to learn in this evolution okay yes so in initial 1960s they used to do the database management system guys they use they need they used to store the data in the database management system so how we are even doing nowadays in the websites so basically whenever you are storing in terms of websites the data the back end everything will be stored in the terms of databases but those are relational databases initially it was only normal databases okay like tables inside tables they used to store the data that's it simple so data collection data creation ims and network based dbms okay so in 1970 so they started using the relational database guys which we are using till nowadays nowadays also we are using those things okay so relational databases so relational databases the difference is nothing but here the tables can maintain some relation so previously there is no relation each table is separate yes so here there is some relation between them okay so in 1980s they used rdbms along with the advanced data models like object oriented all those things came into play guys okay in the 1990s they started the concept of data mining so they started using data mining to extract the useful information so they started storing the data in data warehouses okay similarly multimedia databases and web based okay and nowadays currently in 2000 so we are using stream based management so basically even your youtube live streams or any kind of websites live streams everything will be running parallel right so it will be sent to many users so all these things are managed number days guys okay so data mining is one of the most important things so even in web technologies we are using multiple storage mechanisms to use the things okay right so i hope everyone got some basic idea about the need and evolution of data mining so in the next lecture we will be discussing about one of the most important topic of the whole subject guys if you ask me there is a high chance that we might get and i like you on this directory right so there is a chance i'm saying okay yes so in the next lecture i'll be discussing about a kdd that is nothing but a knowledge discovery from data so in this we will be discussing how the data in the raw format that is nothing but in the form of big big data format to the knowledge so how it is a transfer and that will be discussing in the next lecture guys okay so let us meet in the next lecture thank you thanks for watching you 
NEEEUOmYRd8,27,"Data mining is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. #Data #mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. #Data_mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD.

Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative rapid qualitative data analysis and interactive data visualization.

⭐️ Download Orange: https://orangedatamining.com/download/

⭐️ Table of Contents ⭐️
⌨️    0:00:05   Welcome to Orange Data mining
⌨️    0:02:08   Data Workflows
⌨️    0:04:31   Orange Widgets and channels
⌨️    0:07:32   Loading your data
⌨️    0:11:34   Hierarchical clustering
⌨️    0:15:25   Making prediction
⌨️    0:18:58   Model evaluation and scoring
⌨️    0:22:36   Add ons
⌨️    0:25:17   Principal component analysis
⌨️    0:28:08   Feature scoring and ranking
⌨️    0:31:36   K-means
⌨️    0:35:49   K-means clustering
⌨️    0:45:10   Image analytics - Clustering
⌨️    0:48:16   Text preprocessing
⌨️    0:52:12   Text clustering
⌨️    0:55:50   Text classification
⌨️    0:59:45   Importing text documents
⌨️    1:02:52   Multivariate projection - freeviz
⌨️    1:06:24   Text processing - documents embeddings
⌨️    1:10:07   Text mining - document to network
⌨️    1:13:15   Sentiment analysis
⌨️    1:17:38   Text mining - twitter data analysis

⭐️ Credit ⭐️
This course is created by:  Orange Data Mining
YouTube: https://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g
License: Creative Commons Attribution license (reuse allowed)


⭐️ Join Us ⭐️
Join our FB Group: https://www.facebook.com/groups/csles...​
Like our FB Page: https://www.facebook.com/cslesson/​​
Website: https://cslesson.org/",2021-03-08T12:44:43Z,Data Mining Tutorial For Beginners || Data Mining with Orange,https://i.ytimg.com/vi/NEEEUOmYRd8/hqdefault.jpg,Geek's Lesson,PT1H22M29S,false,7194,277,4,0,8,hello and welcome to orange tutorials my name is ida and i will be a guide in the world of data science let's begin you have probably already downloaded and installed orange if not you will find the detailed installation guide in the link below when you open orange the first thing you will see is a welcome screen from here you can start a new data analysis workflow open a recent one or explore tutorials ok let's exit the welcome screen orange starts with a blank canvas this is where all the magic will take place but every magic needs some secret ingredients our ingredients are called widgets widgets are computational units of orange they read the data process it visualize it do clustering build predictive models and otherwise help you to explore the data you will find them on the left hand side of the screen most data analysis workflows begin with the file widget with which we load our data click on the file widget and it will appear in the canvas open it with a double click go to browse documentation sets and load one of the existing data files that come with orange you can naturally load your own data but for the sake of simplicity let's go with the famous iris data set now we would like to see what we've just loaded click on the data table and the widget will appear in canvas widgets communicate with one another they have an input channel an output channel or both to feed the data from the file widget to the data table drag a line from the right output side of the file widget to the left input side of the data table widget now open the data table wow that's a lot of numbers in columns and rows let's instead plot the data we will use scatter plot and connect it with the file widget much better and with this you've just set up your first workflow congratulations creating a data analysis workflow in orange is simple we start with opening the data in the file widget some pre-loaded data sets are available so let's select iris and explore it let's check out the data in a data table i will select the data table widget and connect the file widget to it here we have 150 rs flowers from the famous fishes data set flowers are described by four features the length and width of sepals and the length and width of petals each flower is labeled with one of the three classes a species of iris arizotosa iris versicolor and rs virginica now let's visualize the data i'll connect distributions widget to the file widget in this way the file widget sends any data it loads to the distribution's widget in distributions we can walk through all the features in the data pattern length and pedal width seem to ask to separate different species of iris we can additionally inspect the data in the scatter scatterplot the plot that we see is a bit messy r is versicolor in red and virginica and green are not well separated i wonder if there's any pair of features that would nicely separate the three classes i can click rank projections to score or feature pairs a higher score indicates a better separation of different species of iris the best chord scatter plot with paddle length and paddle width really nicely separates data instances of different class but there's some overlap of rs versicolor and iris virginica i'll select data instances in the overlapping region scatter plot widget automatically sends the data to its output now i connect another data table to the scatter plot to inspect selected data instances no surprises here they are all either iris versicolor or respirginica and seem to have similar values for all four features we can expand the workflow with other widgets or save it for frequent use but for now that's it we've learned that orange widgets communicate with one another and the changes in one widget are immediately propagated through the workflow today i'll show you some great functionalities of orange data mining widgets and communication channels in orange there are several ways to add widgets to the workflow a click on the widget in the widget pane and the widget will appear in the canvas b click and drag the widget onto the canvas to place it exactly where you want it to c right click on the canvas and the widget menu will appear select the widget from there or even better start typing its name in the filter select the widget and press enter d drag a communication channel from the output of the file widget and orange will suggest the widgets you can connect your original widget to remember orange will not allow you to connect incompatible widgets to get more space for the workflow go to view and deselect expand tool dock i for instance have the widget toolbox minimized at all times and use the right click to add new widgets you can also tell orange what kind of data you would like to send from one widget to another here is a simple example we will use iris data from our previous video i will first connect scatter plot and data table to the file widget let's see if the data was received by the scatter plot yes it's here let us also connect data table to the scatter plot the idea is that i will select some data instances in the data table and these instances will be highlighted in the scatter plot i will open data table and select some instances the widget is sending my selection straight to the scatter plot see how scatter plot marks selected instances i have indeed just created a visual data browser now double-click on the link between data table and scatter plot orange correctly guessed that the data from the data table will be used as a data subset in the scatter plot we can change that and remove this connection by clicking on it now we can drag a new line from other data to data subset and the plot will change all data instances other than those selected in the data table will be highlighted this is how you can define what is communicated between widgets you don't need to do this often as orange matches the inputs and outputs automatically or infers the right connections from the order in which you've added widgets to the workflow a right click on the communication channel allows you to remove the connection today i've showed you the many ways to add widgets to the workflow how to adjust the input and output of a widget and of course how to turn a few widgets into a cool data browser preparing your data so the programs can read it is probably the most important task in data mining today i'll show you how to load your data in orange orange can read several data formats such as excel tab and comma separated files the data is normally a table where data instances are in rows and data attributes are in columns but why just do the talking and no walking let's make our own data i'll use google sheets to create a simple data set say we have a group of people and we would like to know whether we can predict the gender based on their physical characteristics okay our people will have a name so we know who's who then we also know their gender height and weight and we know how they look like so let's put down also their eye and hair color i have this data set for my friends jill jack mark and and so on their names are all strings that is text gender won't be a string but a categorical value because our people will belong to one of the two groups male or female my friends naturally have different heights and weights which are numerical values some of my friends are tall others short some slim and others a bit chubby the color of their eyes and hair are again to categorical values since the eyes can be either blue brown or green and the hair black brown blonde or red now we have our data still other than providing the data i have not explicitly specified attribute types so let's hope orange will guess them correctly now we load our data in orange let's copy a shareable link and paste it into a file widget let us first view the data in a data table orange correctly assumed the first column with names contains our meta attributes but it incorrectly made the hair color our class variable maybe i should have gender as the last column in the table but let us fix this in orange we can rearrange the data with select columns widget we'll put hair color attribute into features and gender in target variable box one quick check in a data table and we're good you can also save the data to your computer with save widget it's best to save the data in orange's native tab format since it automatically appends header annotations for attributes you might want to define your data locally i will use the same data as before only this time i copy them to excel i will add two extra rows under attribute names set variable type in the first one and variable kind in the second for the attribute type i will use c for continuous that is numerical attributes d for discrete or categorical attributes and s for string values for the attribute kind i will write class under gender and meta under the attributes that provide some extra information now our data is all set for the analysis today we've learned how to prepare data manually annotate it and subsequently adjust which feature is considered a class attribute by now we know that we can visualize our data and browse the data subsets but what else can we do with our data instances perhaps put them in logical groups we will use the good old iris data settings in our previous videos we've already observed that flowers are different but how do we know whether they're just one single group of flowers like one single species or they belong to different groups of viruses i will show you how to discover groups and possibly subgroups and subsubgroups using a method called hierarchical clustering so how does this clustering work naturally we would like to group the flowers together so that those with similar leaf measurements would belong to the same group for two flowers we can check each measurement and compute the difference between the measurements square it to make sure it's positive and then sum the square differences across all four measurements at the end we can compute the root of the sum to match the original measurement units i've just reinvented euclidean distance still we can see from here that the smaller the distance is the larger the similarity flower distances can now be used to construct hierarchical clustering connect the hierarchical clustering widget with the distances widget hierarchical clustering displays a dendrogram which is a tree that reveals the structure of the discovered clusters and the distance between these clusters let's make this dendrogram more telling and annotate the branches with the species of iris it looks like clustering indeed made sense of the data as flowers of the same species are clustered together however there's an area with some mix-up let's mark it selected flowers i mean data instances will be on the output of hierarchical clustering widget to check these instances we will send them to the data table and voila here they are looking at this data table i'm not much smarter wouldn't it be cool to see these selected flowers in some visualization and say in the context of every other flower in our data set we've done this before and we'll do it again we will use scatter plot widget to visualize all 150 flowers no not this visualization i want the interesting one here it is now connect hierarchical clustering to the scatter plot the mix-up is naturally in the bordering region between iris virginica and iris versicolor we can now browse through clusters in hierarchical clustering and observe their mapping in the scatter plot just like we did in the previous video it helps if we have both windows open at the same time to observe the results this is how orange became a tool for cluster exploration there is so much more you can do with clustering in combination with other orange widgets but for today this is it we've learned that hierarchical clustering requires information on distances in the input that it displays a dendrogram in the visualization and that we can select data instances in the clusters of the dendrogram to output them to other orange widgets one of the great things about being a data scientist is being able to predict the future of course i'm not talking about an old lady in a crystal ball but rather predicting from solid data with some degree of certainty today i'll show you how to use predictions widget to predict class labels for instances in our data set this time i'll be using data on fruits and vegetables because i will use this data to train my classifier i will refer to it as a training data set we have nine features in the training set including the calorie count proteins fiber vitamin and mineral content based on these features we would like to predict whether a plant is a fruit or a vegetable of course we're interested in which features are the most important for our classification what tells us best if something is a fruit or a vegetable we'll check this with classification tree viewer here we can nicely visualize which features best split the data to pure subsets where one of the classes prevails in our case the most important feature is the calorie count and then the content of vitamin a and proteins so most likely these will be the deciding factors in our predictions okay now it's time to make some predictions say i have my own free plants for which i would like to know whether they're fruit or a vegetable i know their potassium vitamin and calorie values so let's write this down in google sheets the one thing i need to be careful of is to use the exact same names for the features in my test data set so that orange can match them correctly now let's load the data in orange and read it in the data table widget ok all of our data is here connect the file widget with predictions do we see anything yet of course not we need to give the widget some classification model first actually i've already built a model from my training set with the classification tree widget now all i need to do is connect the classification tree to predictions i can now view the predictions directly in the widget seems like two of my plants are fruit and one is a vegetable of course i can use other classifiers as well a fast and simple one is logistic regression i wonder if its predictions will be different let's check again i will connect logistic regression widget to the file widget and then pass the predictor to predictions seems like logistic regression agrees with predictions from classification tree as a matter of fact i have used the actual data for a kiwi asparagus in a raspberry that is to fruits and a vegetable predictions were indeed correct today we've learned how to classify data with classification tree how to build prediction models and finally how to use them on a new data set while we have made predictions we have not really evaluated how good these prediction models were in the upcoming video we'll talk about model evaluation and scoring in the previous tutorial we've made some predictions on fruits and vegetables but you've been probably wondering how do we know which classification model works best today i'll show you how to assess the quality of various prediction methods this time we're going back to our good old iris data set first we will train a model with one of the simplest classification techniques a logistic regression to avoid overfitting we are first building the model on the training data and then testing it on a separate test data we want to repeat this procedure multiple times and then report on average accuracy this is what cross-validation does and it can be found in the test and score widget in our case cross-validation splits the data into 10 subsets uses 9 for training the model and the remaining subset for testing it it repeats this procedure nine more times each time using a different subset for testing cross validation results are reported on our right what do these numbers mean the simplest one to understand is classification accuracy in the second column it reports on the proportion of correctly classified data instances looks like classification accuracy was 96 so what are the four percent of the data that were misclassified we can check this with confusion matrix looks like our model had no problems with classifying irisetosis still it got a bit confused with versicolors in virginicas we can easily observe misclassifications by selecting misclassified instances and sending them to the data table just by looking at this table it is still hard to interpret the results and say much about misclassifications instead let's visualize them in scatterplot most misclassifications are at the border area between the two classes the red for iris versicolor and the green for iris virginica we expected something like this right logistic regression is of course not the only classification method we can apply on iris we could use say random forest this is a more complex classification technique but often has better accuracy however not in this case where the accuracy of logistic regression is already quite high besides accuracy test and score widget reports on four other scores we really like the area under receiver operating characteristics or auc in short but that will be the topic of some other tutorial we can also check where miss classifications lie for random forest but i'll leave this to you today we've learned how to use cross-validation in orange and how to compare several classifiers on a single data set i've also showed you how to select miss classifications and visualize them by now you know orange quite well but how about if you want to do some specialized research in say bioinformatics or text mining add-ons are coming to the rescue you can find add-ons under options add-ons this will display a package list from where you can install the add-ons you want say we want to go with bioinformatics select the package you wish to install and press ok now restart orange and you will see an additional widget set on the left okay let's explore what's in the add-on there are some visualization widgets some specialized data analysis techniques and a few widgets that offer access to external databases one such widget is geodatasets that offers access to a large repository of gene expression profiles there are a few thousands of data sets you can load from the widget say i'm interested in researches on smoking we first filter the data on this keyword now let's select a small data set this one looks nice first we want to see what kind of data we got these are gene expression data for five smokers and five non-smokers we have gene expressions in columns and samples in rows now we want to visualize this data to quickly check if there's any difference between smokers and non-smokers based on their gene expression profiles there are so many features to choose from so how do we make sense of it multi-dimensional scaling will help this technique projects multi-dimensional data to a 2d space we see there are two nice groups blue for non-smokers and red for smokers and there is one sample from a smoker which is an outlier this could be very helpful even if we're not molecular biologists we can see that smoking leaves a trace on a cellular level but enough biology for now multi-dimensional scaling and other embedding techniques are a great way to understand the underlying patterns in any kind of data we will talk more about 2d embedding in the upcoming tutorial as for today we have learned how to install add-ons in orange and how to use new widgets in combination with the existing ones data can be more or less complex imagine a data set with many features say a hundred how do you know which features matter the most and how could you possibly project the data onto a 2d plane one of the popular techniques to answer this question is principal component analysis or pca in short pca transforms the data into a new attribute space where features are uncorrelated and ranked by the degree of explained variance let me show you this by painting some data although this data is in 2d we could identify the position of each point just by knowing its coordinate in a new tilted axis this would also be our principal component its direction is defined by the vector pc1 if our data lie in a many dimensional space maybe only a couple of principal components are enough to explain it let's see this in action this time i'll be using wine dataset with 13 features a 13-dimensional space is difficult to grasp so we'll be using pca to transform the data into fewer dimensions how do you know how many principal components to go with the best choice is to select the first few principal components that explain say 80 percent of variability orange shows the proportion of explained variance in a scree diagram five principal components in my data set already explained slightly more than 80 percent of variability i can check the transform data set in the data table now let's see how our transformed data looks like in scatterplot i will plot the data using just the first two components the three different vines are really nicely separated turns out that chemical components called flavonoids are those that define the first component the most followed by phenols today we've learned how to transform our data into a set of linearly uncorrelated features with principal component analysis next time we'll show you another way to rank features with rank widget in our previous video on principal component analysis we used wine data set these data are the result of a chemical analysis of wines grown in the same region in italy but derived from three different cultivars or vines the data includes 13 features reporting on quantities of chemical components how do i know which chemical components are the most significant for differentiating between these binds with rank of course this widget's scores features with several scoring methods based on the relation with class connect file widget with rank rank displays two scoring methods as a default but we can display more if we want say i want to see the scores for gain ratio genie and relief f now i want to select the features with the highest relief app score by default the top five features are selected and are already on the output now i want to see how well are these features related to class let's use some visualizations for that connect the box plot to rank and inspect the first feature use group by wine to see the results for each class separately boxplot reports on mean median variance and quartiles of each feature min is displayed as a vertical blue line median is yellow the blue highlight denotes variance while the dotted lines display the first and the fourth quartile the wines seem to have very distinct distributions of flavonoid concentration seems like this feature separates the class very well but there must be an even better way of inspecting our features how about distributions let's see distributions widget displays a value density plot for a given feature we can display value distributions for each class separately for flavonides these three distributions seem to be well separated are likely one of our most important features in the data set as the separation is less pronounced with other features rank widget can score and rank features both for classification and regression say we want to analyze a big dated housing data set where we would like to check which feature best correlates with the house price in boston suburbs seems it's the economic status of inhabitants and the average number of rooms sort of obvious but it's still great to see this directly from the data today we've learned how to determine which features are the most interesting in our data set and how to use feature scores for plotting interesting visualizations almost every data mining problem describes the data with features thus making feature scoring one of the best love techniques in the field in data analytics one often needs to find interesting groups of data instances may that be segmentation of customers based on their shopping habits finding similar documents or grouping tweets based on their content especially when the data abounds we can find clusters using a method called k-means first let us paint some data to see if clustering really works we will make one two three groups of data points now let's connect paint data with k-means this widget finds clusters so the data points in the same cluster are close to each other that is the distance between them is small here we told k-means to find three clusters now we can observe the clustering in scatter plot wow this worked fine and the k-means really discovered clusters where we had expected them we can interactively change the number of clusters and observe these changes in a plot we can ask for two clusters four five and so on for our data the choice of three clusters works best k-means requires us to specify the number of clusters but in orange we can also ask it to find the right number of clusters we can tell it to vary the number of clusters score each clustering and return the best score but how do we score the clusters with silhouette silhouette scoring reports how well each data point on average fits into its designated cluster the higher the score the fewer data points we have where clustering membership is not clear let's instruct k-means to use the silhouette score and guess the best number of clusters it's three just as we expected let us add a few more clusters to our data four five six now this looks simply wonderful every time the k-means with silhouette scoring correctly guess the number of clusters is it even possible for k-means to make a mistake let's see we will draw three clusters in the shape of a smiley face let's see what silhouette suggests four clusters that can be right obviously there should be three this is one of the drawbacks of k-means it works well on compact spherical shaped clusters and fails on shapes of a different kind now let's use k-means on a real data set boston housing prices for example we will ask for two clusters and then observe the differences between clusters in the box plot looks like there are major differences between the clusters in this data with respect to the crime rate pollution and age of the houses we can even check if clusters make sense in the mds data projection well they do this data set indeed has two distinct clusters today we've learned what k-means does and how to use it on a real data set in the next two videos we will explain how k-means and silhouette scoring work and how silhouette can find in liars and outliers in the previous video we talked about k-means clustering and how we can find interesting groups in our data but how does k-means actually work that's easy to find out we can use interactive k-means from our educational add-on this add-on is designed for teaching machine learning and it includes some wonderful instructive widgets to install it go to options choose add-ons and select educational you will need to restart orange to make the widgets from this add-on active we will first plot a data set with three groups of data instances to see if we can retrieve them with k-means now connect paint data to interactive k-means besides the data the widget also plots the centroids marked with squares each one in its own color centroids are the assumed centers of clusters interactive k-means places them randomly notice that each data point is associated with the closest centroid in this way the centroids define the clusters but while we have free clusters they're not the ones we would wish for first it would be better to move the centroids to the center of data points we do this by pressing recompute centroids button the centroids moved look at the red centroid some green and blue points are closer to it than to the other two centroids we need to reassign centroid membership so that the data instances are labeled with the closest centroid we do this by pressing reassign membership we can again move the centroid to the center of its data instances using recompute centroids and again reassign the membership we repeat these two steps until convergence that is until the position of the centroids does not change for our data the convergence took only a few steps and the algorithm found the appropriate clusters now let us try to place centroids so the algorithm would fail here i will put the red centroid in between two groups of data points and the green and the blue centroid so that they share the remaining group now press recompute centroids and reassign membership a couple of times the algorithm converged but the clustering it found is not the one we had wished for today we've learned how k-means finds clusters with the help of an interactive visualization we also learned that the initial placement of centroids is important this is why k-means normally uses some heuristic for smart placement and reruns the algorithm a couple of times to report only on the best clustering in the previous videos we talked about k-means and how to find a good number of clusters in our data we mentioned silhouette which is a score of cluster quality and helps us find the k to our means understanding how silhouette score works is quite simple here we have three clusters green blue and orange now we would like to know how well this data point belongs to the blue cluster first we will measure the average distance between our data point and the points in its own blue cluster let's call this distance a second we will measure the average distance between our data point and the points in the closest green cluster let's call this distance b if our data point is well grounded in its cluster b needs to be large and a small so that the difference between them b minus a is as large as possible to normalize this score we have to divide it by the maximum of a and b the silhouette score for our data point will be quite high since it lies close to the center of its cluster a silhouette score for a point that lies in between the two clusters will be close to zero let me now paint some data i'll pass it through k-means clustering and visualize the clusters in the scatter plot i will use silhouette widget to find points that are close to the center of the red cluster to observe where the selected data instances lie i will connect silhouette to the scatter plot let me select a few top score data instances in the red cluster wow they are indeed in the center of the cluster and those with the lowest scores they are the borderline data points i can use silhouette plots on any data that include discrete class or attributes say on an iris data set the biggest outliers are in the overlapping region between iris versicolor and iris virginica and the inliers most of them lie in the well separated iris setosa class today we've learned about the mechanics of silhouette score in the previous videos we had used it to score the clusters but what a great tool it makes for finding the inliers and the outliers data can come in all shapes and sizes it can be a simple spreadsheet or complex text audio video or even an image today i will show you how orange transforms images into numbers and in this way enables machine learning first we will have to install image analytics add-on go to options add-ons and install image analytics restart orange for the add-on to appear now let's start with an example i have collected 19 images of domestic animals and place them all in the same folder i can load them in orange with import images open the widget and select the folder now connect image viewer to import images looks like we have all the animals here inboard images actually constructs and outputs a data table but how does this data table look like let's check this in a data table widget all i have is a bunch of meta information image name image file path size of the file image width and height in pixels nothing that would help me with machine learning we need image descriptors that is numbers which describe the content of these images we will transform raw images into their vector representation using a deep neural network which was trained on millions of real-life images retrieved vectors are also called image embeddings we embed our images into a multi-dimensional feature space connect import images to image embeddings the widget sends images to the server and computes embeddings remotely let's see what we got back now we don't only have meta information but also 2048 additional features to profile the images great with embeddings we can compare the images and compute their similarity let us pass the data to distances and select cosine distance as it usually works best for images then we can pass the distance matrix to hierarchical clustering and observe the dendrogram seems like embeddings indeed make sense all the chickens are grouped together and all the cows too i can even check each cluster in an image viewer simply wonderful but i know what you're going to say you think i'm trying to trick you that i intentionally selected similar photos so they would get clustered together let's double check our procedure with the most famous cow in europe the milk a cow this image is quite different from the others it is a frontal photograph with digitally enhanced colors i have retrieved the image from the internet and added it to my domestic animals folder if i press reload in import images the new image gets loaded and the whole workflow is instantly updated let us again open hierarchical clustering and image viewer seems like milka is correctly clustered with its kind today we've learned how to work with images in orange profile them using deep model embeddings and perform clustering in the next video we will go a step further we will be building predictive models for image classification in the previous video we talked about how to transform raw images into vectors and use these embeddings for clustering but we can do much more with images we could for example use them for classification say i am an owner of an online flower shop i have many images of flowers from roses and daisies to tulips and orchids can orange tell a tulip from an orchid let's check i have created a folder flowers with nine subfolders containing images of different flowers we already talked about spreadsheets and images how about text could we extract any meaningful information from a set of documents of course we can first we need to install text add-on go to options add-ons and select text restart orange for the add-on to appear now let us slow the data place corpus widget on the canvas and open it go to browse documentation corpora and load green tails selected we have 44 green tails on the output of the widget what are these texts about connect corpus viewer to corpus corpus viewer displays text and enables us to browse it for example we can output only those documents that contain the word king another widget for visualizing the text is word cloud this widget displays word frequencies in a cloud the more frequently the word appears in the text the larger the word will be but our word cloud shows silly things such as punctuation and uninformative words we will use preprocess text to get rid of these this widget will transform all text to lower case next it will convert text into individual words and omit the punctuation individual words are called tokens finally it will filter out stop words the effects of pre-processing can be visually explored in the word cloud after pre-processing this visualization looks much better we retained only meaningful words and now we can better understand what our corpus is about grimm's tales talk about kings fathers and wives but some words are still a bit annoying such as could would and said we can filter this out as well let us write our own custom stop word list open a plain text editor and type each word you want to filter on its own line then save the file and load it next to the preset stopward list the changes are now propagated through the workflow and the words we defined in our stop word list no longer appear in the word cloud pre-processing is the first and a very important step in text mining we defined our tokens and filtered out the bits we didn't need now our text is ready for the next step in the following video we will use pre-processed data to find interesting groups in green's tales in the previous video on text mining we talked about text pre-processing now our data should be ready for machine learning right well not quite after pre-processing orange still sees only lines and lines of text for machine learning we need to transform text into numerical representation and a simple way to do it is to count how many times each word appears in the text this approach is called back of words let us reuse the workflow from our previous lesson corpus reads the collection of text documents and text pre-processing removes stop words and delimiters now we will extend the workflow with the bag of words widget back of words outputs a data table where word counts are the new added features you can always check the output of a bag of words in a data table great now we have our data matrix and we can find interesting groups of documents connect distances to back of words here we will use cosine distance as it normally works best for corpora we feed computed distances to hierarchical clustering to estimate the distances between clusters we will select word linkage now drag a line at the top of the visualization left and right what is the appropriate number of groups two seems to make the most sense the nodes in our dendrogram also have a label in folkloristics grimm's tails are labeled with arne thompson uther index which defines the topic of the tale if the tale talks about animals it's an animal tale if it's more about dragons and princesses it's a tale of magic looks like the tail type corresponds quite well with our clusters except for one part where animal tales and tales of magic are mixed can we figure out why they're mixed select the cluster and connect corpus viewer to hierarchical clustering seems like some tales of magic still mention animals perhaps clustering got it right after all clustering is a great way to uncover similar documents in unlabeled text but here we actually have labels the atu topic in the next video we will talk about classification and try to predict the type of detail on fresh data we already know how to pre-process our corpus and how to find similar documents with hierarchical clustering but grimstales also have a label the tails are either animal tails or tails of magic can we use this data to predict the type of a new unclassified tail we read grim tales selected and check the data in corpus viewer day labels are provided in atu topic field we prepare the text with pre-processing and turn it into a bag of words which represents each tail with a vector of word counts now for the classification connect logistic regression to bag of words logistic regression constructs a model to predict whether a tail is an animal tail or a tail of magic we can even see how our model looks like we will use nomogram which visualizes the logistic regression classifier connect it to logistic regression the widget displays top 10 words that are important for the classifier at the top of the list are the words that most contribute to the prediction seems like the word fox can tell us a lot about the tale if fox appears often in the text it's an animal tale if it doesn't it's probably a tale of magic now we know how our classifier works and it's time to see if it also performs well connect test and score to bag of words we will use test and score to cross validate the logistic regression model not bad the area under our c curve is over 0.9 when given two tails of a different class logistic regression can correctly distinguish between them in over ninety percent of the cases but we said we want to predict the tail type right and we don't want to predict something we already know we will place a new corpus widget on the canvas let us load three new tails from hans christian anderson we will ask our logistic regression model to tell us what are the types of these new tails connect corpus 1 to predictions now provide the grim trained logistic regression model and observe the results in predictions our model says the ugly duckling is an animal tale and a little match seller a tale of magic seems quite right predicted class probabilities were high as well the probability that the ugly ducklink is an animal tail is 90 percent today we've learned how to inspect our logistic regression model with nomogram how to reuse the familiar classification workflow on text and how to predict the type of detail on new corpus working with text in orange is just as simple as working with spreadsheets in the previous videos we talked about text pre-processing clustering and classification we worked with greemstales a dataset i have prepared in a spreadsheet but working with spreadsheets and long text can be a pain is there any other way we can import texts into orange of course there is this time i will work with kennedy's speeches i have 17 of them in a kennedy folder each in its own file files can be word documents pdfs or plain text files here for example is a speech from democratic national convention to load corpus into orange open text add-on place import documents widget on a canvas and open it click on the folder icon and select the folder you wish to import let us observe our data in a corpus viewer here's the speech we've seen earlier now we can do some clustering i'll use pre-process text bag of words distances and hierarchical clustering i was fast for details on text pre-processing and clustering you can check our previous videos looks like i have two interesting clusters one on nuclear arms and the other with kennedy's presidential addresses clustering is fine but what about classification can i tell orange some documents belong to one group and others to the other let us put kennedy speeches into two folders say pre-1962 and post-1962 now reload the folder orange recognized subfolders as class categories if we observe the corpus in a data table we can see that orange put pre-1962 and post 1962 in the gray class column you can check our previous videos on text classification to learn how to proceed import documents makes it so much easier to organize your files and your research today we've learned how to import our own data for text analysis and how to define class values from scratch we love our data but most of the time our data is big with many variables and making sense of it is a difficult task well not so difficult in orange let us load the iris data that we have used in some of our previous videos and connect it to scatterplot scatter plots are great they can show us the relationship between two variables but our data rarely has only two variables even iris data has four how could i see the relationship between more than two features with freeways freebies is a multi-variable projection method for uncovering feature interactions in a data with class variable if we pass iris data to freewheez the widget would initially place the variables on an invisible circle making them equally important press optimize and freeways will rearrange the projection and find the one that best separates points of different classes by doing this optimization exposes the most relevant features of the data variables associated with longer axis are more important for a specific class value and those variables that lie closer together are more correlated too iris setosa flowers which are marked with blue circles have high values of sample width and large petals are distinctive for iris virginica on the other hand sepal length does not play any role let us see this on a different example the zoo dataset contains 100 animals classified into seven groups including mammals fish bird and so on we will use previous projection to investigate relationship between all 15 variables and see whether there's some structure in the data in the file widget simply change iris to zoo with browse documentation data sets in freeways we instantly see the unoptimized projection let's use optimize again to reveal the relations between features and animal classes we can even use show class density to observe whether the data projection in freewheeze really managed to distinguish between different class values it looks like it does having hair and giving milk is a distinguishing property of mammals while being aquatic is a property of fish on the other side of the projection are those animals that lay eggs and have feathers this means mammals do not have any of those two properties since they are placed opposite of the two features finally at the center of the plot we see features that are not very informative such as animal domesticity we can exclude these features from the graph by increasing the blackout radius visualizations make data interpretation so much easier in this video we learned how to uncover interesting relations between classes and features in a class labeled data set and how to interpret the freeways projection every machine learning needs numbers b day in a spreadsheet form from the beginning via feature construction or in the case of text mining with bag of words today we're going to have a look at an alternative way of describing documents with vectors by using document embedding first let us prepare the data we will start with corpus widget and the preloaded data set of grimstales a quick look at a corpus viewer gives us an idea of what the data is about these are the fairy tales of the green brothers such as cinderella little red riding hood and snow white now we wish to describe the content of the tales with numeric features but first we need to prepare the core units of our analysis this will be a quick setup for a detailed guide see our text pre-processing tutorial whose link is in the description below the presets that we have are already quite good our text is transformed to lower case and split by words without including punctuation we also removed english stop words that is words that don't have real meaning finally since we have long texts we will remove words that appear in less than ten percent and in more than ninety percent of the documents now we are ready for word embedding embeddings are a low-dimensional representation of high-dimensional data they start with a single token which in our case is a single word they are based on pre-trained models for the selected language which looks at a word and places it in its corresponding vector space in other words it embeds it in this way words that have similar meaning or come from the same family will be placed close together and will have a similar embedding vector once all the words are embedded the procedure then averages all the word vectors to produce a single document vector in practice the procedure is simple connect document embedding to preprocess text and orange will compute the result server side you can change the language of the model or the aggregation of word vectors a quick look at the data table shows us we now have 300 additional numeric features that describe our documents let us now do a simple clustering with cosine distances and hierarchical clustering with word linkage i will label the tails by their title a quick glimpse into dendrogram shows us that animal themed tales are clustered together based on the type of animal they talk about document embedding is a great tool for describing documents with numbers it is usually more accurate than bag of words since synonyms are placed close together and as such the model doesn't rely on words alone but also on their meaning bag of words on the other hand is easier to interpret the choice of the technique is up to you ever wondered who is leading the debate on social media who is that one person that everybody mentions and that keeps coming up on our feed today we will learn how to construct a network of mentions and find the most popular account in the bunch we will use the twitter widget to retrieve 500 tweets in english with the hashtag machine learning this will output a subset of the debate on machine learning with all the hashtags ad mentions text and so on a quick look in a corpus viewer gives us an idea of what the corpus is about great now we are ready for some pre-processing our aim is to retrieve only app mentions first i will remove everything i don't need lowercase transform and filtering for example secondly i will select the tweet tokenizer since we are dealing with tweets this will keep the add mentions as they are finally i will add a regular expression in the filtering section which will keep only words that start with at the expression can be copy pasted from the description below now our data is ready for the final part i will use corpus to network widget to compute the network of mentions the widget outputs two things one is a network of documents where an edge is created between two documents if they share at least the number of words specified in the threshold parameter the other is a network of words where an edge is created if the two words appear in at least the amount of documents specified in that same parameter the latter is what we will use in this case as we wish to observe how words go occur in our corpus also we will set the window size to 100. this will create an edge between two words if they both appear within 100 words from each other for tweets this is sufficient to capture co-occurrence of ad mentions in the same tweet to observe this network we will use the network add-on which you can install in options add-ons i will connect network explorer to corpus to network besides sending in the constructed network i will also add node data which will give us interesting information about the nodes in the network explorer i will set the size of the node to word frequency that is the number of times the word appears in the text finally i will label the notes with words the network i get is interesting seems like there is a larger debate here with several people talking to each other however the most mentioned person does not get mentioned with the others today we have learned how to retain only mentions in pre-processing how to construct a network of words from twitter and how to explore the constructed network stay tuned for more text mining videos social media are full of interesting data on human behavior and sentiment is one such thing sentiment analysis gives us a quick glance into emotions in any kind of text and can be used for brand monitoring review analysis observing story arcs and for recommender systems in this video we'll learn how to extract sentiment from text with orange first let us get the data we will use twitter widget to retrieve tweets with hashtag machine learning to see how people feel about his field you can use any other keyword or even browse by add mention we will keep the analysis manageable by retrieving only 100 tweets in the english language let's go i always like to check my data in a corpus viewer we seem to have some podcasts articles retweets and so on now it's time to compute sentiment scores sentiment analysis can be lexicon-based semi-supervised or supervised in orange we're using lexical methods which means we store lists of positive and negative words then compute how many occurrences of each there are in the text the approach we'll be using vader is a little smarter and can work with phrases negations and punctuation so three exclamation marks will count more than a single one also it doesn't report just on a single score but we'll report on a positive negative and neutral score and on the final compound score okay we have computed the scores for our corpus the easy way would be to use data table then find the scores and sort by frequency but this is just no fun let us plot them instead i will use heat map which shows all four attributes and where the blue represents the lower scores and the yellow and the white the higher scores still our data is unorganized let us use clustering to put rows with similar scores together perfect now my negative scores are at one end and the positive ones add the other select say the negative cluster and observe the text in the corpus viewer it seems like some are indeed negative but others not so much certain words such as problem are considered negative yet in the language of machine learning they're totally normal we deal with problems every day to finish on a positive note let us select the positive cluster and inspect it just as before we have some truly positive tweets and some that are ambiguous today we have learned how to extract sentiment from text in orange how to plot it in a heat map and how to explore the results stay tuned for more text mining videos social media is not just for fun it can also be useful for understanding people's behavior especially on twitter with an open access to its content it's easy to follow and analyze topics political opinions friendship networks trending hashtags brand sentiment and so on in this video we'll learn how to retrieve data from twitter how to pre-process it and uncover interesting topics from the corpus twitter provides a way to retrieve tweets but you will have to first register and get an api key the link to the twitter developer site is in the description below next you will have to apply for a developer account and create an app once you have created an app go to keys and tokens then copy the key and secret key and paste them into the orange twitter api dialog now the twitter widget is all set up say we wish to see what is trending in the machine learning community i will enter hashtag machine learning into the query box and set the language to english i will go with just 100 tweets to keep things simple but you can retrieve as many as you like the best way to observe text data is with corpus viewer widget here i have all my tweets for a concise view i will select content in display features to see only the content of the tweet then i will use ctrl a to select all the data perfect now i can read the tweets one by one but of course we won't do that if we had thousands rather than 100 tweets it would be impossible so instead let us look at the most frequent words to see what a corpus is about we will connect word cloud to twitter widget and see what we got oh boy lots of useless things our query and some punctuation let us remove this with preprocessing we already have some presets here we will keep the lowercase and add remove urls the word cloud had https ranked at the top but this is not an actual word so let us remove it the preview in the bottom left shows the first few tokens so i can see how my data is changing the next step is setting the right organization instead of splitting by word we will use a pre-trained tweet tokenizer which is able to extract hashtags mentions and emojis the downside of this tokenizer is that it also returns punctuation which we will remove with filtering by regex the preset regular expression will remove most punctuation characters a quick glimpse into the word cloud shows us our data now makes much more sense the top hashtags used with machine learning are ai artificial intelligence data science and deep learning finally let us uncover what these tweets are about we will use topic modeling to uncover latin topics in the data there are three methods for topic modeling we will use latin dirichlet allocation which is a generative method based on word co-occurrence we are asking for 10 topics in the widget we see the defining words for each topic an even nicer way of observing topics is in the heat map widget select clustering with optimal ordering to cluster topics by how frequently they occur wonderful now i can select a subset of documents with high topic frequency and observe them in a corpus viewer today we learned how to retrieve data from twitter how to reprocess it how to extract interesting topics and how to plot them in the next video we'll learn how to perform sentiment analysis on twitter data 
n2MWaAAUfgY,27,This video gives an introduction to general data mining and notes some of the applications of data mining that various industries use.,2020-03-18T15:38:08Z,Advanced Database - 026 - Data Mining - Overview,https://i.ytimg.com/vi/n2MWaAAUfgY/hqdefault.jpg,Brian B,PT14M43S,false,161,N/A,N/A,0,N/A,in this video I'd like to give you an overview of data mining and just to kind of give you an idea of what you can do with large amounts of data after you have a data warehouse you tend to want to learn more information about the data that it contains and data mining is one way of doing that but when you consider data mining itself it is not simply querying data praying the data is only telling you information about what is currently inside the database and reporting on what's already stored in there you're not looking for trends you're not looking for things you're just looking for facts that the database has another point that you should realize is that data mining is not OLAP so online analytical processing analysts when they're when they're doing OLAP queries they use the database to test hypotheses about the relationships or the patterns in the data so they've already formulated the hypotheses and then they look at the data to figure out if what they think is actually true data mining on the other hand is different than that it is you studying the data for particular patterns without first having hypotheses developed for it you're uncovering relationships by an induction process using an algorithm based on the data so you're exploring existing data finding important factors that you might have never even thought to include in a particular hypothesis so the definition or several definitions of data mining first its discovering new information from very large datasets and that's kind of a key point there you're discovering new information things that you did not know previously or that you can't really see from looking at the data itself the second point is you study the data without first forming a hypothesis you do not know what exists without reviewing the data itself so there are a lot of techniques and algorithms that allow you to do this we'll look at some of those as we go forward to make data mining useful it requires a large database a lot of data a lot of information because without that large amount of information your your data mining process may not be that accurate so sometimes that requires you to collect data in a way that kind of lends itself to the data mining process if you don't have a large database or you do not have a data warehouse set up then you may have to create that there's some sort of cleansing integration and formatting process before you can even begin to mine the data so some knowledge areas that you need when you start to perform data mining is database technology itself you need to know how to query data and how to query it accurately and that's very very important if you are developing a database an ETL process for example then you need to know exactly how that data moves from the source databases into the target database and you need to make sure that it moves accurately and correctly so data technology is very very important for data mining another thing that's important for data mining is understanding at least at a basic level statistics there are several statistical techniques that you can apply to a set of data that will give you information about that data that you may not have known for example regression and Bayesian probability so these are some techniques that use statistical analyses to determine trends and patterns in the data another thing that is quite often used is artificial intelligence and artificial intelligence is an enormous field but there are several techniques in there that are geared specifically toward using data and finding patterns within it so if you are designing data warehouses then you should be designing them with data mining in mind you might choose to include summarized plus raw data and I know in a star schema or a snowflake schema set up then you have facts tables and those fact tables tend to contain raw data but you could also make it easy to query summarized data as well and you could create views for example to do this you might also consider creating a special data Mart and if you recall your data warehousing terminology then a datum data Mart is a set of tables or like a mini data warehouse that is set aside for a specific use and in this case you would use a data Mart that's set aside specifically for data mining data mining also requires domain knowledge and data mining know as I mentioned before some of the things that are required are database technology knowledge as well as statistical and artificial intelligence knowledge so that's the data mining knowledge that you need but you also need to know about the data itself so domain knowledge is very very important before you can even begin to do a data mining process often the format of the data when you begin to perform data mining is not normalized the data itself is usually a flat file that has one line per record and there's no normalization to it everything's in one row and it gives you the information that you need within that row but when you do that that allows for easier analysis with statistical packages are is kind of the open source standard for this also Python is used quite widely with data mining other statistical packages like SAS and SPSS and MATLAB are also used and also available so a couple of things about the purpose of data mining first is prediction trying to forecast future behavior or trends from a given set of data you also have classification which is when you attempt to look at data rows and you place those rows into the correct category based on facts so that you know about the data so classifying things putting them into the appropriate buckets also data identification looking for existing activities or events such as insider trading fraudulent insurance claims also things like intrusions into a network if you're in the computer science world so a lot of a lot of different things that you can identify and note that those are anomalous within a data set another thing to do with data mining is creating optimization so creating scenarios that will allow you to determine things like equipment placement advertising how to invest money efficient time management efficient equipment management even things like placing items on a shelf at a large store so these are the types of things that data money aims to aims to do aims to solve so some applications of data mining in retail CRM is very important customer relationship management looking for trends in customer data is something that data mining allows you to do and as a result then you can do advertising campaigns and targeting specific customers for specific reasons banking and finance banking uses things like credit scores to determine creditworthiness so credit worthiness is actually used in a lot of cases data mining is used to determine your creditworthiness so fraud detection is another thing and I know you've probably received phone calls from your bank that asks you if you have made a certain purchase or completed a purchase in a certain location or something like that so fraud detection anything that's out of the ordinary in a in a banking transaction they aim to use that to prevent prevent fraudulent transactions so data mining is important there in manufacturing using data mining to optimize a set of tasks and manufacturing is very task-oriented then it goes from point A to point B to Point C but you may find in your manufacturing process that point B is unnecessary or just extraneous so you could go directly from point A to point C so Dana mining allows you to see things like that and then optimize based on those findings product design also uses data mining there are lots of ways that you can optimize a particular product making things more efficient for that product streamlining a car body for example so all of these things can use the data that you have and and make decisions based on that applications in science and medication is extremely important the recent kovat 19 pandemic there's a lot of data mining that goes on with that looking also at treatments for different diseases and how effective those disease treatments are as well as medications drugs for those particular diseases or particular ailments that people might have looking for relationships between different entities often data mining finds relationships that we did not know exist so that's important astronomy uses data mining to do things like find planets that surround stars that we didn't know were there mathematically you can locate planets by looking for anomalies and data weather prediction is something that that is beginning to use data mining more and more and more looking at trends over time trying to predict how things will happen bioinformatics is also something that uses data mining using the human genome for example to find certain things about an individual that you might not otherwise fine so looking for cancer treatments and things like that using biological data some other examples homeland security looking for terrorist activity in different areas of the globe and then identifying and tracking individual terrorists using data that you might have search engines have used data mining for many many years PageRank was an algorithm that was created by Google and it was simply for web crawling and then classification of particular web pages so data mining has its applications all throughout every industry really and it's very very important for many of those industries so in the next video I will go into a little more detail about some of the specific ways of doing data mining 
QizU-n7ank8,22,"Introduction to database, Dataware house and data mining",2020-05-29T06:59:12Z,Introduction to database | Dataware house | and | data mining,https://i.ytimg.com/vi/QizU-n7ank8/hqdefault.jpg,Computer Corner and Machine Learning,PT30M23S,false,103,10,0,0,2,the lecture information systems that what is the need of information systems and why the information systems are necessary for different type of businesses now if we analyze that what is the purpose of information system basically the information system provides us lots of facilities for example manual system may be summarized with lots of efforts but if a digital system we have in an organization we may summarize the data very quickly so whenever the data is summarized it is known as means the process data is known as information that's why we ask him information systems so the definition of information systems is very much clear from the previous lectures then when we say that process data is information and now when we have the information when we will process the information that will be converted into the knowledge so the second part of this lecture will be about the data mining techniques and data warehousing as well means the machine learning techniques and the artificial intelligence techniques will be used so it is a brief intro we will not go into the much more detail so data Resource Management is done through the information systems so the learning objectives of this lecture is to provide examples to illustrate each of the following concepts such as major types of the data bases then houses in data mining then logical data elements then fundamental databases structures and database development moreover we will see some of the concepts of data warehousing and data mining as well who in the abstract level because in this lecture it is not possible that I will show you the data warehousing and data mining in a detail this is a completely different discipline if we have been offered a course that data warehousing in businesses we may think about this and are the data mining in businesses are the machine learning in businesses we may think about it as well so what are the major types of the databases we have to see first of all what is Theta is management system actually data is sucked in files for example in the MS exit if it is in the form of the ms excel well it is sufficient but the second part when we see that we have the tolerance of the rules can be a completion how can we take in such data so actually the data resources must be structured organized in some of the logical manner so that can be exist means retreat processed and managed easily so basic concept of the database management system is just like we have the excel file but we don't have the database management system but in database manage the database when the management system is involved then loads of DQ l DD l TM l and other languages are there these are the part of the SQL structured Q language so they give us some sort of lots of her facilities to access the data to process the data to manage the data very easily and accessing the data by using the relational database management system which in which the relational algebra means set theory has been involved we know in class it said theory is very much given to section Union and these operations are there so same has been followed here so in this way we can see the data responses must be structured in a some manner so these are the fundamental data concepts that what is the character Italy character is a data the most basic logical data element that can be observed a single alphanumeric or other simple represented but one bit actually in the computer memory as we have learned that bit by bit information is same but the character occupies one body space and what is the field actually the field is also known as column and combination of fields is known as row so in ms-excel when we see that there are some columns in database there are some columns but data we say in two different concepts and the here we if we see the national or the other type of the database management systems empty other concepts because primary key M secondary keys are discussed so we have to just build a concept that field a guru of related characters as last name or a celery represents an attribute of some entity now fuel is also known as attribute and color and entity is known as some sort of we can say that it is a table as we see in the ms-excel so general purpose application programs it is used and they perform common information processing jobs for end-users it is very much clear that it is used for the common purposes and the record a grouping of attribute that describe an entity actually record is combination of fields and we can say that it is a row means if we have name F name salary canal like these things if we have related data a grouping of attributes that describe an entity is a record that's why we can say that a row is a record so it is very much then the database is a collection of logically related data elements give us lots of combination because the database is not a related so in the lecture last lecture we will see that what are the fundamental concept and data concepts are built in China as well so employee record and record to record three is there now actually these are the rules and these are the fields as I said number fees now we can build a record as well so in this way we have this some sort of record think a while give it in time that there are three type of the databases now in these fundamental concepts we have to see that T Joel's summaries SSN number insurance fears now these are the basically files related to the human resource databases so the bear on file and benefit files of the error but it may contain lots of data it is one of the sample tests and then we can see the database structure the hierarchical structure tree like structure and structure one of the manic parent-child relationships each child can have only one parent we will see in the graphical form and then we will dismiss and the network structure is similar to that but it allows many relationships our child recorded more than the national structure as well the most widely used database model is national data base model and through this we can say that data is represented as series of two-dimensional table cells we have seen the previous figure that tables are called relations each column is named attribute of the entity and each row is unmanned instance of the activity now this instance is can terminology that could be understood by us it is instance is also a row a row is also called instances streams it is called record as well now we have to see that what are the consequence is that what are the differences between these data types and how can we say them and what are the two dimensional tables we have seen in that previous picture as well now what are the relational operations select join project and the select operation we create the subset meets the criteria means if we ask a Curie inquiry that how many what is the maximum salary of implying in their particular type of the organize the horror what is the range of the salary and we can join report one record for the view purposes and in case of the project to kill create a subset for the temporary tables as well and we can see that these are the assumptions of the data and we can summarize the data in a more structured way so the multi-dimensional is structural variation of the rational model using the multi dimensional structures to organize that express relationships now what are these now here if we see that this is the hierarchical structure department now project even acknow employee is working in imply one and two in project a now in this there are direct combinations and international structure we have the department and we have to imply now we can see that this is the relationship this is the primary key and this is the foreign key and we can find that imply one is in the department a and the implied two is also in the department a and it implies three is in the department B so if we draw here that imply three blocks will imply three plans to project P so these are the fundamental concepts and this is a cube this is taken basic concept to build the being about the data warehousing a sample data cube is before you now these are the aggregations summarized data now how it is summarized we will see in the schemes so VCR TV and pc are the products first quarter the second quarter these are the date and then again this is the summarization into the USA Canada Mexico and notes also this is a basically dice before us a cube we can scroll it by see the data from the multi-dimensional ways so in data warehousing the all dimensions could be seen now here we can see it has been in scrolled TV VCR February annual budget whatever in the January now this is a timeline year and here we can see that yeast waste knocks out whatever thing actually what was a perfect it and into the fortress we can scroll the six dimensions as well so this is the multi-dimensional and we can think that it gives us lots of varieties as well so the object-oriented is structure combining the data of the interest and the process that ate their data into the structure called an object now when we see about the object now object is a place is some sort of the table it is an object if it is an object now what could be the method through the data is to be sent what are the restrictions what are the rules and regulations by following that they tries to be safe so it is encapsulated means it is embedded in that and defined very well in that particular object and inheritance means the upper class functions could be tried if we want to say that allows a user a or employee the department he implied one is belonging to Department a two is also belonging but three results means we can inherit the among classes through the programming by using these things and another thing is polymorphism which is not written here means one object could be used with many shapes so evolution of the database structure have grown more complex as the needs have a change and complexity and finally it has touched to the Big Data and what is a big data via seen in the previous lecture that the speed volume variety was there and by using these we can see so this was a little bit about the database development now this is what are the ethical responsibilities are for database administrator it controls development and administration of the database it defines the data definition language used to specify the contents and lightships structures of the database for example what would we database will contain and what is to be how the law business lot of it will be converted it is to be described by the database administrator so when it will describe it will create data dictionary directly containing the metadata which is known as data of the data as we have discussed for example we have a dictionary in that we can see the synonyms of that data and we can easily understand that what are the synonyms antonyms of that particular type of the data and how it can be utilized so how the definition would be defined into the meta data so data planning and database design is also responsibility of the database administrator so data or modeling entity relationship and diagrams logical models and data itself must be done before choosing the database model and the data BC scheme self to be defined by the database administrator that it is the physical internal view of the system and this is a some logical external view of the system now we have to understand that what are these what the database development if we see the database lifecycle it is also done by the followed by database administrator that data planning develops a model process and then defines the business documentation then requirement is specification and the name the description and then conceptual design is term there and the physical data models of the earth in that particular type logical design is also there and we have to see them so entity relationship diagrams this is one of example that this is one this is many means parties Arden has relation one-to-many with purchase items and products have one too many orders supplies have many too many products are healthy so this data is stopped here backup is restored through the data warehouses now in that data warehouses what we have to say that it is a separate data is which is stored into the database now here the duplicate data is allowed redundant data is allowed and data in is to be stored as a pickup as well and for the organization as well so data are organizational schools that must be managed by another resource for example the if there is a disaster if we have the backup of the data we can restore data and if we are working on the online transactional processing means operational database we can do anything but offline we can do so types of the database used for example we have the distributed database systems or the network database network passed so data warehouse is their end users is their and external and internal databases and data Mart's are subjective that particular type of the database data warehouses now operational database means store detail data support business processes operations means it is operational but when it is it becomes passive and its backup is still stored into the data warehouses same likely if we have a vacuole if we have some sort of the materials and thought of the products we always save them into the warehouses but in case when we store the data then we can say that this has been stored into the data warehouse is because it is a data so distributed databases many organizations distribute their databases for the multiple locations replication complex processes updating the data and application simplified and many of the updating data is here so external database is outside the form free from the base and hyper media database are VRI the media means for example we have the YouTube another like these applications and we have to follow them so data warehouses is to read I expected from other databases means try it from the operational databases and it becomes secondary data it into what are the data marts subset of the data warehouses focusing on the single topic for example on the selviana power on the on the time one particular type of the country these are the data marts who that can be defined by using the metadata and data mining uses the analyzation as you have seen the summarization warehouse reveal the hidden patterns of the data transmitted how much would product how much sold in which country and it what time means we can aspect that this product can be celled next time can be sold next time in that particular type of the region we can if we have a key Canton we can see that we have a tuckshop what is the train of the customers in the summer and winter we have to keep this in mind and we can prove by Excel these products as for the specification that we use what for example in a person if we analyze the data in the morning someone else came and it is purchasing the eggs definitely there is a possibility it will go for the milk and then for the slices then for the other items so we have to keep these things in the morning in a sequence that he can choose he is for its need very easily but what is the at the evening and what is at the 9:00 so we have to focus upon these businesses that how we have to arrange the details so this could be done through the data mining techniques so operational external databases are there data acquisition is the dieter management is there and meta Tector detection Holies there feeding the food is there so this is the basic concepts of the data warehouse that we have discussed but this is a graphical so what is the role of the database administrator what is the role of warehouses and what is the oil DP what is the database and what is a data warehouse we have to see these are the subsets ERPs having and these are the data Mart's from different perspectives as we have seen now in the data mining expects the business knowledge from the data warehouses from the very start we say that data is processed data is known as information but when the information is processed through the data mining techniques it may give us homology and actually this knowledge is also known as decision support system and by which we can understand that what is the pattern of the used customers in the morning-time and other times and how we have to generate the data so in that we have to select the data target it then we have to project little slag data and then data mining techniques are there and then we can find the business knowledge so LDP we can see the user side are mostly IT professionals in there where let me go ahead on the oil epi data warehousing operations are performed and this is our operational database means this is the secondary data this is the primary data so functions day-to-day decision support system is there DVD application oriented and this this subject you learnt aired and this is the historical data as we have discussed in detail so we have the some differences as well so what are the star schemas snowflake schema and that consolidation actually these are these schemas which are have the dimensions and measures while modeling the data warehouse is star schema a frack table in the middle is connected to a set of the dimensions and then the sin of lake is little refinement of that particular but to the fact consolidation has he one then more than one dimension 10 now this is the sample of a star schema we'll try it in those terms this is the aggregation summarization and here we can see these more years and these are the products of which these summarization has been made so that table is there now little bit refinements of hierarchy now the supplier details city has been also derived in lots of Fineman can be done also but here we can see that we have the multi perfectible shipping fact table and the sales rep table and this see these are the mayors and here these are the mayors it can be refined more so data we are housed in we can say that traditional file processing data was stored in independent files resolved about the needs of the data just like ms excel load customer two three into the very beginning of the computing and then the profits missing of file was converted into the databases and database seek to solve the problems later redundancy with duplicate data is not allowed the same data is kept more than four locations and database seek control not reduce redundancy this lead to the data inconsistency where the element data has came and the same multiple locations but values are different that's why if the values are different it then it is not redundant it is not duplicate so laid out the data integration these are the problems of the database and then the data dependency problems are the error in with the tie data is tightly coupled and the data integrity is some of the interests and relation that are to be defined very carefully by the database administrator as well and the consolidated data separate from the into the database accessible in multiple programs and data base management system is action of programs create man time who's received a talented as we have seen this is some sort of first light that we have discussed and the database maintenance is very much feminization data was need to be updated continuously means this is the bulk of Chile database a Devastator and the application development this is also the responsibility of database man administrator that it facilitated by the data manipulation language provided by the DBMS so database integration QE our state away history allows us here are the case of the databases we have discussed and one more thing which is very much crucial in this that boolean logic 3 logic operators and or not for example a and B items are present then it is true but if I say that a or b are there either one of them means it is true but when I see a and B are not in the seed then we can say that it is true otherwise it is false so graphical and natural cues have the methods of the SQL Curie languages so thank you very much on world after participation I am highlighting pull to you for the ingesting and participation hope you will 
ze5m_dVibMA,27,"Data mining is the process of finding anomalies, patterns and correlations within large data sets to predict outcomes. Using a broad range of techniques, you can use this information to increase revenues, cut costs, improve customer relationships, reduce risks and more.",2020-10-19T19:00:06Z,What is Data Mining and Why Data mining: Introduction to Data Mining Course (ITS632),https://i.ytimg.com/vi/ze5m_dVibMA/hqdefault.jpg,Charles Edeki - Math Computer Science Programming,PT24M51S,false,109,5,0,0,0,so again welcome to introduction to data mining this is our first lectures and this course is its 632 class what is a data mining and why data mining that is our main objective in our unit one lecture one so first we start with the definition of data mining and also we try to understand what is a data mining and here we say data mining is the process of identifying patterns that exist within data so the whole concept of data mining is try to find a pattern in a large data set now with these patterns at hand analysis can apply them on other data sets now we should think about actual mining as the search for the data patterns so our main goal is searching for a pattern or who can use the term knowledge searching for knowledge in a data set so a data mining process may involve the use of again statistics database queries visualization tools traditional programming and machine language algorithm or machine learning organic so first we should understand what is a machine learning the concept of machine learning and data money they are almost the same because data money uses machine learning your guardian to perform its tax but there's a two major difference between them as we saw the definition data money is to try to find a knowledge or a pattern in a data set but machine learning is more or less action oriented or to perform a specific task so for example i may use a machine learning algorithm to program a self-driving car or to program a robot to do a specific task based on the data the model that we have built so we can say machine learning is smaller action oriented we are using data to perform some specific tasks while data money we are trying to find a lot of knowledge from a data set and we may apply this knowledge in other problems that we may face but both of them use almost the same algorithms so a machine learning is the use of data pattern recognition or gallery which allow a program to solve problems such as clustering categorization predictive analysis data association without the need for explicitly step-by-step program instruction to tell the algorithm how to perform tasks so here what we want to see the difference between data mining machine learning is that they are using the same algorithm classification cross story in machine learning we use the term supervised learning which is classification clustering on supervised learning we also use categorization association rules which is data association in both machine learning and data money but as we said earlier machine learning is more action-oriented to perform a specific task such as a self-driving vehicle whereas data mining we are trying to find knowledge in a data set and management can make a decision based on that knowledge maybe for a prediction so in this way we say machine learning solution can solve complex problem by using data to drive discovery and using only a few lines of code so the whole concept machine learning is that instead of writing a program to perform a specific task let's say we are going to write a software program so that application can drive a vehicle it will be very complex to write that kind of program so the the main objective is to use machine learning algorithm so in this case the machine will be in this case the software it will be learned by doing the work by feeding it more data it will be learning the practice now these are some of the common data mining tools and we have the database such as mysql or mongodb this has some databases that can be useful useful in data mining excel also can be used visualize visualization tools such as tabroid also business intelligence tools such as microsoft power bi can be used by data mining programming language solutions such as our uh wake up rapidmine applications all these can be used with the common machine learning tools is the python and r is the most common again tools that are using machine learning but visual programming such as weka rapid mana or orange also can be used excel with a third party in heading such as a sofa service application third party adding to excel you can use the performance of machine learning such as supervised learning unsupervised we also want to compare data mining with data science the main three things that comes to when it comes to data analysis is again data mining machine learning data science they are all using almost the same algorithm but the tax centers are different so data money is the process of identifying patterns that exist within data set as we said earlier but data science is the use of statistics programming scientific methods machine learning to extract knowledge from data sets so we can see that data mining data center was similar but the main difference between now data money data science data science would can deal with a very large data set and a very complex like big data analytics and uses more complex [Music] concepts like distributing programming and concept computing concepts now the definition of data mining data sets as we said data science are very similar and the two sometimes are used interchangeably so a data scientist therefore is an individual who analysis and also interpret data why data mana also the same analyze and interpret data so you find the term data scientists and data analysis quite similar and also both use data mining tools to gain insight into one or more data sets now data money vessels statistics when we say data money again is the process of identifying patterns that exist within data but in terms of statistics it's a collection analysis modeling and also presentation of data so statistics can be one of the component of data mining because in data manual so we need to call it data we need to do our data preparation phase and analyze the data and then build the model if we are doing classification so having knowledge and understanding of statistics normally will make us to understand the concept of even machine learning data science data mining so the main background of this data mining will be statistics and also linear algebra and the good news is that you don't have to be again a statistician to use the tools we don't need to have a great background in certainties in order to become a data miner also excel remains one that most widely used data analytic tools and he has a many built-in statistics functions now while mining data we look at the commercial viewpoint and we will see the main reason why we do data mining data science and machine learning because there are so many data available so we know in this data stored in a data warehouse or database system we want to see if we can find some knowledge from this data set again we may have a data set about patients that have disease a or let's say cancer a now we want to find knowledge from this problem which is the cancer one we can go and analyze this data and see what all these patients have in common maybe their gene sequence or their nutrition or their physical activity lifestyle contributed to the disease that they have so this will be the concept of again data mining trying to find some knowledge in a data set now since we have so much data available we end up trying to find knowledge from this data and that's the main ways of mining data so here we see a lot of data is being collected and white house uh web data like e-commerce online shopping online business transactions now purchases at the department store grocery stores there are so many data doing transactions using your credit card or bank card we generate data even going to make an observation for hotel observation we generate data paying our toll booth we generate data using the internet and other twitter facebook we still generate data using our cell phone every day communicating we are generating data so again this is the main reason why we mine data we generate data and data storage tool is becoming very cheap and cheap and cheap and also very powerful cpus available to again process all these data so computers are becoming cheaper and more powerful so why mine data in terms of scientific viewpoint and we say remote sensors on a satellite they are generating data also when we use the telescope to scan the skies we are generating data in terms of computational biology or bioinformatics we can use micro errors to generate a gene expression data these are gene expression is how the generally add to environment their selection can be stored in a in something like a computer chip and we call this a microarray we can only maybe sequence uh maybe few amount of genes let's say 200 [Music] nicots but today we can do five thousand ten thousand necrotizing again we plant them in a cheap asset microarray and do the studies on it yeah not only in the computational but also in engineering field we generate so many data so data mining may help scientists for example classifying and segmenting data or in hypothesis formation also so many large data sets that's our main motivation behind data mining when we look at the graph here we can see that how data has been increasing exponentially from 1995 to 2000 so we can see that the total new data is terabyte since 1995 and we can see from the graph how the data is increasing exponentially so what is data mining we saw the definition before the whole concept of data money is trying to analyzing a large data set and trying to get a knowledge or get a pattern from the data set so data mining which can be knowledge discovered from data is the extraction of interesting non-trivial implicit previously or more and potentially useful patterns or knowledge from huge amount of data and we have so many names for data mining and we may come across some term like knowledge discovering database kdd or knowledge extraction data or pattern analysis data archaeology data dredging information harvesting or business intelligence again all these are have the same concept of data money trying to find a knowledge or a pattern in a large data set now one thing we should watch out is everything a data mining of course no sometimes we may do some tags and these styles can be a query example uh we have amazon database system amazon want to know which state in u.s have more customers that purchase items of a thousand dollars this is not a data mine okay you write a query that's sql query to again trying to get the result that i need all customers from each state that spend over 1000 and count each state for me this is not a game data might the reason why it's not data money always the data money we are expecting some result but we don't know it in this case we know the results we are looking for so it's like we have the hypothesis we are looking for customers who spend over 1 000 and we still have more customers at that amount so again it's not data money we also have an expat system that's a deductive concept so many definitions available we saw our first definition of data mining another definition is a non-trivial structure of implicitly previously unknown and potentially useful information from data also the exploration and analysis either by automatic or semiautomatic means of large quantity of data in order to discover meaningful patterns or we can say extracting meaningful patterns or knowledge from a large data set and this is the process of doing data mining first we need data we can get data from different sources then second we need to select which again we can use all the data let's assume we have a population population with everything if i'm going to do a research i may not use all the population because of time and also the cost so the same thing applies to data money i may have a large data from different sources but the first step i need to select what data i want to use after i select the data which would be like a sample then i'll do my data pre-processing this is a process of trying to eliminate errors in their data duplicates in the data or trying to replace a missing value with some default value or find the mean mode to replace the missing values so data preparations there are so many techniques to use for example we can use the singular value decomposition or principle component analysis analysis we may discuss all this as time goes on then after we do our pre-processing that's the transformation where we do the principal component analysis uh similar value to composition etc so again selection pre-processing with transformation together then we moved now we have a data that is ready for analysis now we apply our data mining algorithms and then we get a knowledge which will be that pattern management can use this pattern to make some decisions so that's the process again we have the inputted data first thing we do is our data preprocessing so here data preprocessing the attacks we can do can be data integration to gen 2 or more data together or normalization to reduce the large or to increase the minimum value to majority of the values so we normalize our data the most common algorithm we use is the log normalization then we do a feature selection we are not going to use all the data so we need to select something it's like taking a sample then we do our dimension reduction this is the transformation phase next we do our data mining so in data money we can do different types of tax pattern discovery association and coalition classification cross story and outline analysis then post processing after we get our result again we interpret the result then we have the post processing this is when we do the pattern evaluation pattern selection pattern interpretation and also pattern visualization so what is data money and what is not data mining so we have a couple of examples here what is on data money for example i'm look looking up phone numbers in a phone directory it's not a database or query a web search engine for information about amazon it's not a data mining but example for data mining can be certain names are more prevalent in certain u.s locations such as o'brien or rocky or rally in boston area so this can be a better money and i want to know if each part of us we have more last name of o'brien or o'reilly here we can do clustering order reading so group together similar document returned by the search engine according to their contest this can be amazon rainforest or amazon.com also we have a multi-dimensional view of data mining for example data that can be mined data in a database system such as relational database or extended relational or object directory database system we can also mine data in a data warehouse or transactional data most data money tax is done on data that has been stored for a long time so like example with the data warehouse a transactional data depends on the organization an organization will say that okay every weekend every sunday all the transaction we do for the whole week we transfer the data to our warehouse so which means that transactional data will not be more than a week old but if we go to the data whereas it can be over five ten years so it depends when again we have the data so knowledge to be mine as we said earlier classification clustering association and characterization afterline analysis or normally deviation again so all these are different tags that we can perform we also have a descriptive base of spreading predictive data mining you may go details on this so descriptive data money can be association or clustering we are trying to find similarity within the data predictive data money means we're going to be the model and based on the model we can predict future knowledges but also the techniques that we can utilize they can have to be data intensive or data warehouse olap means online analysis process the knowledge of machine learning statistics pattern recognition visualization high performancy so data money again is an interdisciplinary field application adapter can be retail telecommunication banking industry computational biology medicine rate stock market analysis web money etc so here we say data mining on what kinds of data we can have a data mining as we said earlier in a regular database system such as relational database or data warehouse or transactional database also advanced data set and advanced applications such as a data stream can be cctv live video sensor data time series data like says example for time series data can be the stock market data because this data changes based on the time also structured data graphic graphs social networking such as facebook twitter we can mine all these data multi multi link data object rational database and database that deal with space and time tempo so we have spatial temporary data or spatial data multimedia database test database and also the world wide web so we can see from this list we can mine almost any type of data so again in this lectures we discuss about what is data money and the main reason why we do data mining and so we discuss these two our next lectures we're going to discuss about different types of data mining tasks again thank you for your time and wish everybody the best 
DLfh8pv4-yQ,28,".
.
.
.",2016-04-14T00:59:15Z,Lecture 58 — Overview of Clustering | Mining of Massive Datasets | Stanford University,https://i.ytimg.com/vi/DLfh8pv4-yQ/hqdefault.jpg,Artificial Intelligence - All in One,PT8M47S,false,41971,215,6,0,6,hello everyone welcome back to mining of massive datasets in this lecture we are going to cover a very important topic clustering we're going to kick off our discussion of clustering by looking at some applications of clustering they'll tell you why we need clustering in the first place then we're going to look at the overview of the most common methods used for clustering the basic problem of clustering is quite simple we have a cloud of data points here you see data points in two dimensions and we'd like to get a some understanding of the structure of the data points beyond just seeing them in the two dimensions for example it's intuitively clear by looking at these points that there are three groups of points here there's a group of points that group together here there's another group here and there's a third group here and it looks like we have two outliers that don't fall into any of the groups the goal of clustering is to find groups like this except in much higher dimensional spaces than two dimensions more formally given a set of points and a notion of distance between points we want to group the points together into a number of clusters or groups and you want to group them together so that members of a cluster are close or similar to each other using the notion of distance that you've defined previously and members of different clusters are far away from each other or dissimilar from each other usually the points that we'll be dealing with will live in high dimensional space or space with thousands or hundreds of dimensions and similarity will be defined using a distance measure from from among the recent measures that we have covered earlier such as Euclidian cosine shekhar or edit distance going back to our example here we have points in a two dimensional space and let's say our distance measure is Euclidean observe that points in the group that are highlighted are close to each other by creating distance and are closer to each other then they are two points outside this group we will call this group a cluster similarly we have two more clusters based on the similar notion and finally we have these two outliers that are not close to any of the clusters now if that example looked easy but in general clustering is a hard problem for example here we have a group of points and you can see that the different colors actually denote different clusters but the problem you notice is that unlike the previous example where the clusters were cleanly separated from each other in this case the cluster is actually overlap with each other for example there are some blue points that are among the orange points here and some among the green points here and you can notice that the clusters are kind of smeared over and mix into each other it's hard to find clear boundaries between the clusters as in the previous example so these are the kinds of real problems that we have to tackle when we deal with clustering in the real world so why is clustering hard clustering two dimensions actually looks quite easy clustering small amounts of data also looks easy and in most cases looks are actually not deceiving clustering in two dimensions or small amounts of data is actually very very easy the trouble starts when you increase the number of dimensions many applications don't involve two but tens or hundreds or thousands or even tens of thousands of dimensions and high dimensional spaces are fundamentally different from low dimensional spaces the problem is that in high dimensional spaces almost all pairs of points are at approximately the same distance from each other and so it's not intuitively clear how to group them together these problems will become apparent as we work with high dimensional spaces let's look at some real applications starting with skycat skycat is a catalog of 2 billion astronomical objects and each object is represented by its radiation signature in 7 dimensions or frequency bands the problem is to take these 7 dimensional data points and cluster them into real-world objects such as galaxies stars quasars and so on which are more similar to each other than they are to other things as a second example let's look at clustering CDs now intuitively music divides into categories and customers prefer a few categories or genres of music but what a category is really right we'd like to represent a CD by a set of customers who bought it and we'd like to say that similar CDs have similar sets of customers and CDs naturally group or cluster together based on the customers they have for example there might be is a group of CDs that are you know there are classical music and they have a group of customers who are classical music aficionados and there's another group of CDs that are punk rock that are bought by a different set of people another example is clustering documents we'd like to group together documents on the same topic but what is the topic really a topic is just a set of words that appear together frequently now documents with similar set of words might be about the same topic right so we want to cluster documents based on their similarity in the space of words a dual problem is to find topics instead of documents a topic is just a group of words that Co occur in many documents so we could instead look at words in the space of documents and cluster those rather than clustering documents in the space of words and when you do that we will find topics instead of document clusters let's talk briefly about distance measures we've looked at various distance measures such as cosine jacquard and Euclidean and depending on the way we represent the objects of a clustering one or one or the other of these distance measures may be more appropriate for example let's look at our examples of documents or music and different ways of representing documents lead to different distance measures we might represent a document as a set of birds or a bag of birds that appear in the document in this case the appropriate distance measure uses a jacquard distance since we are dealing with sets alternatively we can think of a document as a point in the space of words here there is one dimension for each word and each document is an n-dimensional point a dimension I is one a word I appears in the document and zero otherwise since you've represented documents by points the natural distance measure here you see clearly in distance between the points in space alternatively we can think of a document as a vector in the space of words a document is a vector from the origin to the vector x1 through xn where X is 1 if bird I appears in the document and 0 otherwise and when you think about documents as vectors the natural distance measure is the angle between the vectors or the cosine distance notice that each different way of representing the same object leads naturally to a different distance measure and depending on the application a certain representation and distance measure might make more sense than another now that we understand distance measures here is an overview of the important methods of clustering the two important methods of clustering are hierarchical methods and point assignment methods and within hierarchical methods we can either go bottom-up or top-down whatever methods or agglomerative methods start with each point in a cluster by itself now once we have each point in a cluster by itself we repeatedly combine the two nearest clusters into a single cluster and we stop at some point and we have a set of clusters divisive or top-down methods initially place all the points in the same cluster and then keep recursively splitting the clusters and to be end up with the desired number of clusters point assignment methods work differently in point assignment methods we always maintain a set of clusters let's say K clusters at any point in time and then we repeatedly assign a point to its nearest cluster and we proceed until each point is assigned to some cluster you 
bz0N-WP2FQE,27,"#DataMining | What is Data Mining? What are the applications of Data Mining?  In this course, you will learn the basic concepts and fundamentals of Data Mining and more.

About the Speaker: Raghu Raman A V
Raghu is a Big Data and AWS expert with over a decade of training and consulting experience in AWS, Apache Hadoop Ecosystem including Apache Spark.

He has worked with global customers like IBM, Capgemini, HCL, Wipro to name a few as well as Bay Area startups in the US.

#BigData #DataMining #GreatLakes #GreatLearning

About Great Learning:
- Great Learning is an online and hybrid learning company that offers high-quality, impactful, and industry-relevant programs to working professionals like you. These programs help you master data-driven decision-making regardless of the sector or function you work in and accelerate your career in high growth areas like Data Science, Big Data Analytics, Machine Learning, Artificial Intelligence & more.

- Watch the video to know ''Why is there so much hype around 'Artificial Intelligence'?'' https://www.youtube.com/watch?v=VcxpBYAAnGM

- What is Machine Learning & its Applications? https://www.youtube.com/watch?v=NsoHx0AJs-U

- Do you know what the three pillars of Data Science? Here explaining all about the pillars of Data Science: https://www.youtube.com/watch?v=xtI2Qa4v670

- Want to know more about the careers in Data Science & Engineering? Watch this video: https://www.youtube.com/watch?v=0Ue_plL55jU

- For more interesting tutorials, don't forget to Subscribe our channel: https://www.youtube.com/user/beaconelearning?sub_confirmation=1

- Learn More at: https://www.greatlearning.in/

For more updates on courses and tips follow us on:

- Google Plus: https://plus.google.com/u/0/108438615307549697541
- Facebook: https://www.facebook.com/GreatLearningOfficial/
- LinkedIn: https://www.linkedin.com/company/great-learning/

- Follow our Blog: https://www.greatlearning.in/blog/?utm_source=Youtube
Great Learning has collaborated with the University of Texas at Austin for the PG Program in Artificial Intelligence and Machine Learning and with UT Austin McCombs School of Business for the PG Program in Analytics and Business Intelligence.",2019-01-30T09:26:53Z,Data Mining | Tutorial for Beginners [Part 1] | Intro to Big Data | Great Learning,https://i.ytimg.com/vi/bz0N-WP2FQE/hqdefault.jpg,Great Learning,PT14M39S,false,10213,71,6,0,3,[Music] so yeah but if you say that you're working on Pais path and that typically means you are already working on big [Music] enough to say that I'm coming from the feet don't ask any questions or information like give me the definition or something no it is that big data I mean okay probably not I mean I don't blame you already part of something related to big data but maybe are not completely convinced that you are working on big data all right okay so this term is actually a very elusive how did I say so big data this term was very popular I would say around eight years back ten years back even today it is very popular but the meaning has changed a lot okay so when I started working in big data it was when somebody is a big data it was like oh that's something really great stuff that you're working on but you say that you are in the Big Data diamond that's like Oh chatter let's so it will happen initially so something like that so before we understand what is Big Data or what is industry aspect of big data except like cetera you need to have a slight idea about what is the world of the third big data right so before I talk about Big Data I have to tell you what is the world looks like without Big Data so traditionally if let's say you are working for a company I don't know ABC corporation any company right and let's say this company is having a lot of products right so let's say I was with this company called insurance company AXA right there is a AXA right so I was working with them as a consultant for some time so they are into life insurance business right so take an example of a safe just as an example ok so this company what they do so they have a product called a life insurance which they sell to all the US customers in US insurances big deal not like in India and in India even if you hit and die nobody cares about insurance when yes it is million dollar business right so excess lot of customers in India as well as in u.s. so in u.s. what they have done they have a platform where people visit and buy insurance so you have a portal where you can log in and say that I want to buy an insurance for my bike or myself or anything like that right so basically they have a website kind of thing okay actually buy insurance right so if you are buying an insurance online or if you go to them and physically purchase an insurance copy these transactions that are happening in the website actually handled by an AR DBMS system so I am assuming that you at least know where is a database and all right so I don't have to tell you what it is like okay so except I was actually using Oracle handle all the you know insurance transactions so if you pay some money to their insurance or renew your insurance anything that you do this website will take care of it and all the data that website has is being dumped into Oracle so Apple is their database number one axle we so has a customer portal they call it as their CRM score the customer relationship management system wherein they keep details like the customers name age and ethnicity you know all these customer related information so the c IM also dumps the data but CRM is using my sequel for some reason another I do BMS so CEO done is using my sequel right and then they have an internal system which is used by this HR and other folks they have an internal application software application and all this is getting managed their office data and this is using some other a DBMS so let's say this is using I don't know PostgreSQL just as an example so you see a problem right and this is not a new problem but in the traditional world one of the problems that we have is that if you look at any organization they will have multiple applications not one application and each application will be probably using a different database right so doesn't is using a writer where this guy is using first Chris twin right and now what happens if you are the CEO of AXA or some big shot in AXA you want to pull a report right so the basically interested in understanding how many customers between 30 to 50 age group have bought an insurance from this country etc etcetera so to fill that report you actually need the data from all these guys because your transaction data is in Oracle where the customer data is in my sequel and some of the data is here so if somebody needs to be some sort of analytics or one together approach combining all this data the only way is that I need to get all the data from these three guys which normally is not possible why it is not possible because usually these databases are very busy they are serving their customers right so this database is ready BC this is BC this is PC you can touch them that is where in the traditional world we do something called ETL there is something called EPL maybe some of you have heard about it ok it stands for extract transform and load so basically ETL is have to you connect this with all these sources and you say that in the data midnight 12 o'clock and ETL job will run these dyes will be pulled towards let's say a data warehouse so here you are going to have a data warehouse this is called a data warehouse so basically the data warehouse is a place where all this data is getting dumped right so that you can look at the data and analyze the data very simple right now and this is something which has been happening in the industry for like 30 years it's not something I invented ok this is something this is already there in industry and people are using this for quite some time now once you have all your data in your data warehouse the beauty is that this data warehouse is internal to the company that means the external customers are having no access here and I can connect my business intelligence tools things like tableau I think you're learning tableau in your course so I can you complete a right so you can just connect your table to a data warehouse and then create visualizations right you complete the tableau so you they really know this I mean this exact picture right I don't have to tell you by the way right so one of the tories which you can use to visualize your data right and so this is how the industry has been working for quite some time and this is perfectly fine no troubles but the real problem is so this was a nice diagram in 2002 or 2004 I don't mind that when I give you 2010 no 2015 if I fast forward my problems are bigger so one of my problem is that I don't just have data in my database I have data everywhere right for example when was the last time you actually went to a mobile shop and purchased a mobile phone I don't remember I read it online by tactically your balance I believe at least so you are everything starting from groceries to pockets online bite I mean I'm not from Daniel so we still go out to show that you guys are there everything online by so what does that signify if you can order everything online that means all the businesses are online and all the customers are also online everything is online these days right that means that also means that um I was also working with the flip card for that sometime flip card was one of my customers right so I have the chance to work with them for a variety of their use cases so if you look at a company like flip card the amazing revenue is actually from the website I mean now the app however you purchase stuff from Flipkart so what they do the moment yet sell you open flip cards website either on your laptop or the app they start tracking you from the Charpy address are you browsing with city are you browsing how much time you're spending on a product page which items you are zooming which items you are adding to cart all this activity you do is being captured so this is captured in multiple ways one they collect all the log files so whenever you browse the website and leave you create log entries second thing there is something called clickstream data so now when you click that score click stream like the report which icon you picked and what was the effect except etcetera so extreme data then long data they capture for every customer okay now I don't really have a way to dump any of that sheer clickstream data and log beta not really something is I can sit in a table our DBMS table it's not a column format I don't know what to do with that data question number one so my traditional architecture may not really work in case a flip card prime number one every day they have millions of log entries practically millions of love interest so that data is coming in and they don't know what to do and from the app they also collect the data like how much time you are using the app and a lot of statistics about the app the way you use your app right so that data they collect in a format of examine the data comes in XML format not in the normal format so that data also I cannot put it anywhere here to put it very simply then you can also collect a lot of social media activity right around the city around India assets like so it is Diwali right the values over right so during the holiday season they conduct this what you say sale festival called big billion days and so basically of a to empty your pockets right so to put it in action but because huge discounts and profit for them right so so but how do these sales things happen so when they billion days were there for the first time I was at that when I was working for Flipkart for some time so what they do is first they will analyze the market so they will look at the trends and also what people post in a social media and all to understand what is the interest so they will generate something called interest talking so that means what you guys are really interested how do they generate it there is and emails and websites selection getting launched that ex-something XR the next iPhone right so they will collect social media data from Twitter and Facebook whether this particular model is trending how many people are really following this model how many people will you like this model etcetera etcetera so first I capture all this data right and then they decide on the actual day of the sale watch what they should sell on how much amount the solution they should sell etc etc so if you are looking for something like big billion days the amount of data they capture is in terabytes actually so that comes in the format of images if you collect from let's say public fields in Facebook and all and news articles comments and tweets the Twitter data right the tweets that you can get in JSON format there is a format called JSON data you can typically get in JSON format this is key value pair again all right and then from Facebook if you are practically downloading the data it comes in a typical format called graphs grass format okay Facebook uses something called a graph API to represent the data so it pretty much is at a graph the data comes in and then the text data like what people commended what people wrote what news article they read etc etc so if they are getting all this data first of all they cannot feed it in this such a structure even if they're able to fade it if you dump it in a data warehouse you have nowadays to analyze this kind of a data 
ZPFZ9gDlZQI,27,"http://eu.lib.kmutt.ac.th/vod/content.php?subject_id=21
Chapter 1 Outline of Data Mining",2009-09-15T09:27:17Z,CPE751 - Data Mining : Chapter 1 Part 2,https://i.ytimg.com/vi/ZPFZ9gDlZQI/hqdefault.jpg,eKMUTT,PT10M1S,false,115,0,0,0,0,other exam will be open book open notes open everything just but ask your friends and the prerequisite like the basic database and some set which I think of you have okay so any body has have any questions so far the this book doesn't doesn't contain any foundation or review about dated database actually when people look at a data mining topic then they'll think about that it may be related to database somehow right is like through one pie is it related to the database because it has to access allocution off data but that is not the whole part it also involved in using some technique on statistics majority of it is steps statistics so when we go through the class you will see that most of the algorithms is not quite related to all database but that is a part it's like like accessing and the data and that the type of data that will be related to database but besides that there will be from pattern recognition from a I community or form statistics community okay so okay um I'm actually okay maybe I'll let let me tell you about myself firstly I'm my research is not in data mining my research is in computational geometry and I want to change the topic so this is new to me as well and there's a cold I haven't decided yet but I might go of doing the project here is try to make each of you familiar with the technique of the data mining by using some software you can try to you I can say that the project should not be programming you may have to program a little bit like doing some script for further and package that you are using are you are you will choose to use the lovely the project will be defined a problem that you want to mine the data set that you want to money and then defy your own software that you want to use I have one software okay maybe I'll go back but can you can you look at them maybe I'll go back to this one you just write it does so dumb the webpage which contained the syllabus and then you can go to that web page and then see all the information okay so about the project so you can choose all the software that you like there are many of them but most of them are commercial so you cannot access that but some of them are free so you can just download and then apply the software to the data that you chose and look at the pattern the combination or whatever and dryer report in to send a report that is the kind of project is not programming in tennessee's that is what I have in mind but it may be surveying the papers but I don't think that is likely because that one you have to each of you have to ask who serve a lot of papers and then presented and it takes so much time I think that I like to have something like hands-on experience on data mining the steps in the remaining that you can you can start form define your problem okay and because i am new here the I just for the policy from the department that the the faculty members will not talk to you in Thai and you will not talk to us in Thai it's ridiculous but I think that anybody understand high but we cannot talk the RAM in the class outside the class I don't know I my English is not so well but I think that you are understand even though you have problem with English feel free to ask a question I think I can guess the problem why is the meaning of the question anybody make mistakes but is to be okay so also that's done about the maybe that is that the last wishes the last the last page tells you the roughly double schedule which is fast form today which we will cover some introduction and then it's about the related topics like them the data warehousing information with you for a little bit like that and we go to data mining techniques classification and clustering actually classification clustering association rules will be the three majority of the data mining task and if you have time we will discuss something about advanced topics like special my special mining is likes to special database or web mining or text mining and then it will be project project presentation and a final for those who can't late just look at your friend syllabus I'm not going to go back to that patient anymore just go to the webpage usual you can you can find the other incident information about the syllabus there so then we will talk about the definition of data mining the definition there are many definitions actually but roughly for example from the book of view we will be using this book is there simple just finding hidden information in a database so but some books they try to elaborate that definition a little bit like trying and the analysis of often large there's the arm one of the stress that it has to be a large data set right and try to find and suspected relationships that if get then the same thing and summarize the data in a novel ways so that we can understand and the data will be useful to the decision maker or another one is the same that is try to this once pressed on automated it has to be automatic process and hidden information and the information should be predictable non unpredictable is like information will be used to predict something so the key is like hidden predictive large automatic should I move on if you okay the everything on this slide and all this life will be available but you can write you can write all the description I just want to let you know and i'll try to make the verbal one day before class the data set is like a database unknown today the data so what happened data explanation you you are expected to to hear the data set is why the right database is a collection of data there's that is like you can look at it as a database but it can be from one database it can be for many databases like that but basically it's like this collection of data is set of data rain when the term little bears may be used like besides the collection of data set it includes the method of managing the data set as well like retrieve the data specific data of the big data set that isn't like the term database actually that is lab database management system but other this is just like collection of data okay so this is lovely what is about data mining so what 
iB_1MQmb4r0,27,,2021-03-14T14:02:07Z,5.3 Data mining applications,https://i.ytimg.com/vi/iB_1MQmb4r0/hqdefault.jpg,OU Education,PT7M33S,false,259,4,0,0,0,hello guys we are back with our next lecture in this lecture let us go through the applications of data mining guys okay so you can expect this question in our examination guys because this is most important right so when you read a subject totally you should know where it is applied in reality right yes so that is what we will be discussing i'll be going through 14 different applications guys i think i have taken the print right yes so i'll be just giving you an introduction here if you want you can just go through the theoretical concepts also inside it it is written really clearly guys because i think it i collected it from a research paper or somewhere or some blog where it was written really clearly so that is the reason i have just copied it and printed it guys okay yes so initially i wrote the first three types guys because in one examination they asked specifically for three so i already wrote the answer for it so i'll be just going through it first after that we'll be moving on to the rest of the eleven explanations okay yes so the first application is nothing but customer segmentation guys okay so assume that you are running a shopping mall okay yes so your shopping mall is such a huge shopping mall that you are selling everything so you sell electronics you sell furniture you sell mobile phones like gadgets you sell groceries right yes so you sell all these things then your customer count how much will be there because there will be a lot of customers right yes so now you got some sale for electronics right so do you want to send it to each and every one or only to the target audience of electronics guys it will be better if you send only for target audience right so your budget will also be a bit safe right yes so this segmentation or dividing of a customer so based on their behavior or based on their activities is nothing but customer segmentation is i hope everyone got a clear idea here right yes so traditional market research may also help us to do segmentation or division okay customers into groups or clusters or segments okay so using data mining aligning the customers into a distance okay into a distance a segment and can tyler the needs according to the customer so basically based on their needs we will be doing it okay so based on the cluster one can show one can showcase the product to the audience guys to the target audience so basically only to the target audience you can showcase it okay so in simple words we can say here customers are divided into different clusters and based on their interests we will be taking and will be sending the messages and any operation you'll be doing only based on their interest so you are not going to spam to everyone right yes so if i took an example here of a few customers buying ac and buying tv right yes in that way you can write any kind of example guys okay and the next application which i wrote is a market basket analysis guys okay so i hope we discussed about this already in the frequent the patterns i think so in second unit i think so yes yes so market basket analysis so market basket analysis is a modeling technique based upon a theory that if you buy a certain group of items it is more likely that you will buy the another group of items so assume that you went to a shopping mall to buy bread so you are not having any thought of buying any extra thing with bread okay but you saw their butter okay so instantly you got a thought that okay so let us buy bread and butter to have some breakfast so in that way you assumed so this is nothing but market basket analysis guys so basically the the shop owner or the uh organizer has assumed that most of the customers who are buying a bread could buy butter so in this way he thought and he kept both of them nearby so he observed that and the customers are buying it so that concept is nothing but market basket analysis guys so that's what it is written here this techniques allows the retailers to understand the purchase behavior of the buyers okay so based on the collection of information to store layout can be changed accordingly okay so buyers buying a bread and butter okay so that's what i have written here okay yes and the third one which i have written is a fraud detection guys so basically even fraud detection can be identified here so assume that you are a common man right right so you get salary of around 50 000 assume in that way so you are getting every month salary and you will be spending only 25 000 assuming that way half of it you will be spending it and always 25 25 000 you will be keeping in your savings assuming that just an example guys okay yes so in this situation you are doing this for around two years till date guys okay so from two years you withdraw only 25 000 every year so your company will be dropping directly into your bank so in a month a total at max you will be withdrawing 25 000 okay yes so on some day there is a there is a and some transaction occurred which required one lakh amount of money guys so your transaction per month is only 25 000 so this one lack is a bit suspicious right so why do someone who is saving that much of money will spend one lakh at a time so that is a suspicious thing so at this moment the fraud will be detected guys so in this way the mining of the data is done so billions of dollars have been lost due to this frauds so traditional methods of fraud detection are time consuming and complex so data mining adds in providing meaningful patterns and turning data into information so any information that is a valid useful is knowledge so completely based on the previous history i told you right so any time when a transaction occurs it gives it is given that whether it is a fraud or non-fraud okay and depending on its future so assume that a user always spends ten thousand dollar ten thousand rupees or dollars every year for past three years suddenly he withdraw fifty thousand dollars so in this situation there are some kind of issue right yes so these kind of things will be detected okay yes so that is a fraud detection guys okay yes so even in future health detection so basically based on your health doing some visualizations and all those things you can get to the future of your vision of your health right so that can also be done similarly market mass analysis we discussed in education so basically in education if someone is predicting your marks your future marks how you are studying now how your future will be there those kind of things can be mined and observed guys okay yes similarly manufacturing indus engineering so even in that the knowledge could be used and crm okay that is nothing but customer relationship management so even in that you can use fraud detection which we wrote intrusion detection so intrusion is nothing but whenever something bad is occurring with your system some hackers are attacking and those things can also be detected like detection is also possible okay so based on your pulse and all those things it will be possible guys okay yes similarly customer segmentation which we discussed and financial banking so basically whether you are going to take the loan are you going to refund the loan all those things can also be identified cooperative that also can be identified research analysis can also be done crime investigation can also be done so basically just to go through the theoretical parts once guys like you you need to write whenever they ask for four to five examples you need to write them applications okay yes so i hope everyone got some basic ideas so in the next lecture we'll be going through the trends of data mining guys okay so let us meet in the next lecture thank you thanks for watching you 
XOW1bZRePGQ,27,"Subject:Management        
Paper: Management Information System",2017-09-01T12:53:45Z,Data mining for Decision Support,https://i.ytimg.com/vi/XOW1bZRePGQ/hqdefault.jpg,Vidya-mitra,PT35M7S,false,381,4,0,0,0,"you [Music] hello students welcome to apt pad shala I am dr. Ashiya heads pal assistant professor from University Business School on job University original centre Ludhiana today we are going to discuss about the module data mining for the season sport under the paper management information system after completing this module the students would be able to understand the fundamentals of data mining understand the scope of data mining understand the data mining architecture list various methods of data mining get an overview of how data mining aids the decision sport and to understand the industry wide application of data mining data mining is a process used by enterprises to turn raw data into useful information it uses software to look for patterns and large batches of data businesses can learn more about their consumers and develop more effective marketing strategy as well as increase sales and decrease cost data mining is dependent on effective data collection and warehousing as well as computer processing data mining is primary used today by companies with the strong consumer focus retail financial communication and marketing organizations it enables these companies to determine relationship amongst internal factors such as price product positioning or staff skills and external factors such as economic indicators competition and customer demographics data mining helps them to determine the impact on sales customer satisfaction and corporate profits finally it enables them to drill down into information to view the detail transactional data data mining tools can answer business questions that additionally have been quite time-consuming to resolve this scrubb databases for hidden patterns finding predictive information that experts may miss because it lies outside their expectations data mining tools enables to predict future trends and behaviors allowing business to make prompt knowledge different decisions the perspective analysis often by data mining move beyond the analysis of past events provided by retrospective tools typically of decision support system data mining techniques can be implemented rapidly on existing software and hardware platforms to enhance the value of existing information resources and can be integrated with new products and systems as they are what online let us take an example of Walmart Walmart finded massive data mining to transfer its supplier relations Walmart captured point-of-sale transaction from over two thousand nine hundred stores in six countries and continuously transmits this beta to its massive 7.5 terabit Teradata data warehouse Walmart allows more than 3,500 suppliers to access data on their products and perform data analysis these suppliers use this data to identify customer buying patterns are the store display label they use this information to manage local store inventory and if I knew merchandising opportunities way back in 1995 Walmart computer processed over 1 million complex data queries these students now we'll try to understand how does data mining works the technique that is used to find solution in data mining is called modeling modeling is the act of building a model and one situation where one knows the answer and then applying it to another situation that one doesn't data mining consists of five major elements the first extract transform and load transaction data onto the data warehouse system the second is to store and manage the data in multiple dimensional database system third provide data access to business analyst and information technology professional fourth to analyze the data by application software and lastly to present the data in a useful format such as a graph or a table data mining provides the link between transaction and analytical systems data mining software analyze relationships and patterns in store transaction data based on open-ended user queries several types of analytical software are available that are statistical machine learning and neutral networks generally any of four types of relationships are sought first what we define it as classes store data is used to locate data in predetermined groups for example a restaurant chain should mind customer which is data to determine when customer visits and what they typically order this information could be used to increase traffic by having daily special offers or offers designed for certain days of a week then is the cluster's data items are grouped according to logical relationships or consumer preferences for example data can be mined to identify market segments our customer affinities there is what we talk about is associations data can be mined to identify associations ladies who came shopping for dresses were likely to purchase cosmetic to match their attire this could enable stores to mix and match various accessories and cosmetic combinations on display to enhance customer purchases lastly there is the sequential patterns data is mined to anticipate behavior patterns and trends for example an outdoor equipment retailer would predict the likelihood of a backpack being purchased based on customer purchase of sleeping bags and hiking shoe the following diagram depicts that how the data is transferred into a more related information so that depicts a sequential pattern let us now understand the scope of data mining data mining is an important part of knowledge discovery process that analyze large enormous set of data and gives us unknown hidden and useful information and knowledge data mining has not only applied effectively in business environment but also in other fields such as weather forecasts medicine transportation healthcare insurance government and etc data mining can discover new correlations patterns and trends in vast amount of business data stored in data warehouses data mining software uses advanced pattern recognition algorithms mathematical and statistical techniques to shift through mountains of data to extract previously unknown strategic business information many companies use data for performing market basket analysis to identify new bundles to identify new product bundles find root causes of quality or manufacturing problems prevent customer attrition and acquire new customers cross sell to existing customers profile customers with more accuracy data mining automates the process of finding predictive information in large databases questions that additionally requires extensive hands-on analysis can now be answered directly from the data quickly a typical example is targeted marketing data mining uses data on past promotional mailing to identify the targets most likely to maximize the return on investment in future mailing are the predictive problems includes forecasting bankruptcy and other forms of default and identifying segments of a population likely to response similar to a given events data mining tools sweep through databases and identify previously hidden patterns in one step an example of patron discovery is the analysis of retail sales data to identify seemingly unrelated products that are often but together other pattern discover problems include detecting fraudulent credit cards transactions and identifying anomalous data that could represent data entry King errors data mining techniques can heal the benefit of automation on existing software and hardware platforms and can be implemented on new system as existing platforms are upgraded and the new products developed when data mining tools are implemented on high-performance parallel processing system they can analyze massive databases in minutes high-speed makes it practical for user to analyze huge quantity of data larger databases in turn yield improved production dear friends now we shall be looking upon the data mining architecture the data mining is a very important process where potentially useful and previously unknown information is extracted from large volume of data they are number of components involved in the data mining process these components constitute the architecture of data mining system the following diagram depicts an architecture of a data mining system let us first understand the data mining architecture what actually means the major components of any data mining system are data source data warehouse server data mining engine pattern evaluation module graphical user interface and knowledge base let us understand what are data sources database data warehouse world wide web text file and other documents are the actual sources of data large volume of historical data are required for data mining to be successful organization usually store data in databases or data warehouses data warehouses may consist one or more databases text files spreadsheets or other kind of information repositories data may reside even in plain text files or spreadsheet worldwide web or the Internet is another big source of data the data needs to be cleaned integrated and selected before passing it to the database or data warehouse server as the data is from different sources and in different formats it cannot be used directly from the data mining process because the data might not be complete and reliable a number of techniques tend to be performed on the data as part of cleaning integration and selection let us now understand database or data warehouse servers the database or data warehouse servers contain the actual data that is ready to be processed hence the server is responsible for retrieving the relevant data based on data mining requirement of the user understanding Ola tea server that is online analysts processing server it enables a more sophisticated end-user business model to be applied when navigating the data warehouse the multi-dimensional structures allow the user to analyze the data as they want to view their business summarizing by product line region and other key perspective of their business the data mining service must be integrated with the data warehouse and the Oh L AP servers to embed ROI focused business analysis directly into this infrastructure and advanced process centric metadata template defines the data mining objective for specific business issues like campaign management prospecting and promotion optimization integration with the data warehouse enables operational decisions to be directly implemented contract as the warehouse grows with new dimension and results the organization can continually mine the best practices and apply them to future decision data mining engine the data mining engine is the core component of any data mining system it consists of number of modules for performing data mining tasks including Association classification vectorization clustering prediction time series analysis etc next is the pattern evaluation model the pattern evaluation module is mainly responsible for the measure of interestingness of the pattern by using a threshold value it interacts with the data mining engine to focus the search towards interesting patterns the graphical user interface the graphical user interface module communicates between user and the data mining system this module helps the users use the system easily and effectively without knowing the real complexity behind the process when the user specifies a query or a task this module interact with the data mining system and displays the result in an understandable that is the knowledge base the knowledge base is useful for guiding the search or evaluating the inference of the result factor the knowledge base might even contain user beliefs and data from user experience that can be useful in the process of data mining the data mining engine might get input from the knowledge base to make the result more accurate unreliable the pattern evaluation module interact with the knowledge base on a regular basis to get input and also to update that further let us look upon the data mining methods the most commonly used techniques in data many are artificial neural networks nonlinear predictive models that learn through training and reassemble biological neural networks in structure the next technique is the decision tree tree shaped structures that represent set of the season these decisions generate rules for the classification of a data set specific the season three methods include classification and regression trees elevated us cut CA RT and chi-square automative interaction detection that is CH a ID the third technique in data mining is the genetic algorithm optimization technique that use process such as genetic combination mutation and natural selection in a design based on the concept of evolution the next method is the nearest neighbor method technique that classifies each record in a database we based on combination of the classes of the K record or records more similar to it in a historical data sometimes called the K nearest neighbor technique lastly the rule induction the extraction of useful if then rule from the database on statistical significance the CPP group has a database of seven million customer but until recently knew little about them to help target its marketing resources the CPP group employed G B groups analysis data inside to carry out and major customer modeling exercise the first step was to cleanse the data and then augment it with demographic and lifestyle details over 300 critics were augmented down to around 30 which were picked specifically to other the best chance of predicting their customer behavior the aim was to establish which of the four outcomes would be the most likely in each case whether the individual consumer good response to offers by making a purchase cancel within the 30 days cooling-off period cancel at end of the year or cancel after three years the chosen critics who were run through a shape analysis to create a hierarchy of importance this enabled model is to develop a scorecard that would predict a propensity for buying and lapsing among the strongest factor to emerge were those that were product related such as whether a claim had ever been made which would increase the likelihood to renew the analysis also revealed that product appealed to the people in the 1824 group a fat noodle CPP group now instead of offering a single marketing message for the product different treatments can be tailored to suit the individual segments now we'll be talking about the data mining for decision support system this is in sports system are defined as interactive application system which are intended to help decision makers utilize data and models in order to identify problems solve problem and make decisions they incorporate both data and models and they are designed to assist decision maker and decision making processes they provide support for decision making they do not replace the mission of decision support system is to improve effectiveness rather than efficiency of the decision a decision support system can take many different forms and every decision support system is developed for a specific objective and basis on a particular decision process and setup methods techniques and approaches DSS can be developed for the purpose of simulation analysis forecasting and optimization the design of DSS is very dependent on decision making process and the season problems which the DSS is going to support the objective of data mining is to discover relationships patterns and knowledge hidden in data data mining is the process of analyzing data in order to discover implicit but potentially useful information and uncover previously unknown patterns and relationship hidden in data data mining is interdisciplinary field which encompasses statistical pattern recognition and machine learning tools to support the analysis of data and discovery of principles that lie within the data integration of data mining and decisions food can lead to the improvement performance offices and support systems and can enable the tracking of new types of problems that have not been addressed before they also argue that the integration of data mining and decision support can significantly improve current approaches and create new approaches to problem solving by enabling the fusion of knowledge from expert and knowledge extraction from the data detecting frauds using data mining fraud detection is a topic applicable to many industries including banking and financial sectors insurance gammad agencies and law enforcement and many more fraud attempts have been a drastic increase in recent years making fraud detection more important than ever despite effort on the part of effective institution hundreds of millions of dollars are lost to fraud every years since relatively few cases show fraud in large population finding these can be tricky in banking frauds can involve using stolen credit cards forging cheque misleading accounting practices etc in Insurance 25% of the claims contain some form of fraud resulting in approximately 10% of insurance payout dollars frauds can range from exaggerated losses to deliberately causing an accident for the payout with all the different methods of fraud finding it becomes still data mining and statistical and statistics help to anticipate and quickly detect frauds and take immediate action to minimize cost through the use of sophisticated data mining tools millions of transactions can be searched to spot pattern and detect fraudulent transactions an important early step in fraud detection is to identify factors that can lead to fraud what specific phenomena typically occurs before during or after a fraudulent activity what other critics are generally seen with fraud when these phenomena and characteristics are pinpointed predicting and detecting fraud becomes a much more manageable task using sophisticated data mining tools such as decision trees machine learning Association rules cluster analysis and neural network predictive models can be generated to estimate things such as probability of fraudulent behavior or the dollar amount of fraud these predictive models help to focus resources in the most efficient manner to prevent or recuperate the fraud losses thereafter we will look upon the application of data mining a wide range of companies have deployed successful application of data mining while early adopter of this technology have tended to be in information intensive industry such as financial services and direct mail marketing the technology is applicable to any company looking to leverage a large data warehouse to better manage their customer relationships two critical factors for success with data mining are a large well integrated data warehouse and well-defined understanding of the business process within which data mining is to be applied some successful application areas are discussed as follows a pharmaceutical company can analyze its recent sale force activities and their results to improve targeting on high-value physician and determine which marketing activity will have the greatest impact in the next few months the data needs to include competitor market activity as well as information about the local healthcare systems the results can be distributed to the Salesforce why a wide area network that enables the representatives to the view the recommendation from the perspective of the key attributes in the decision process the ongoing dynamic analysis of the data warehouse allows best practices from throughout the organization to be applied in a specific situation yet another example is that of a large consumer packaged goods company can apply data mining to improve its sale processes to retailers data from consumer panels shipments and competitor activity can be applied to understand the reasons for brand and store switching through this analysis the manufacturer can select promotional strategies that best reach the target customer segments optimizing the prices for all products in a store is of course a very difficult task a number of factors such as consumer demand price demand interaction etc need to be considered while modifying the price of any product normally price increase leads to lower sales and customer of alternative products data mining can be used to identify customer demand for the products and also to understand how the price range of a particular product affects sales of other products and another example a credit card company can leverage its vast warehouse of customer transaction data to identify customers most likely to be interested in a new credit product using the small test mailing the attributes of the customers with an affinity for the product can be identified recent projects have indicated more than twenty fold decrease in cost for targeted mailing campaigns over conventional approaches data mining can be applied to check which segments of the customer respond positively to a promotion how effective the promotion could be in terms of cost and benefits which media channel have been successful for different campaigns in the past and so on for analyzing this kind of information a retailer can come up with more effective and fruitful promotion and advertisements a diversified transportation company with a large red cell force can apply data mining to identify the best prospects for its services using data mining to analyze its own customer experience this company can build a unique segmentation identifying the attributes of high-value prospects so students let's now summarize what we have learned in this module obtaining knowledge from human expert is a time-consuming process and in many cases the expert do not have the requisite knowledge solution to these problems are promised by data mining techniques data mining is process of securing and analyzing data in order to find implicit but potentially useful information it involves selecting exploring and modeling large amount of data to uncover previously unknown pattern and ultimately comprehensively information from large data sets today multinational companies and large organization have operations in many places in the world each place of operation may generate large volumes of data corporate decision makes corporate decision makers require access from all such sources and take strategic decisions data mining uses a broad family of computational method that include statistical analysis decision tree neural network rule induction and refinement and graphic visualization the combination of business domain expertise with the power of data mining techniques can help organization gain a competitive advantage in their efforts to optimize customer management thank you "
9WQzZVv5H1U,19,"The A-priori algorithm continued.
Measures of interestingness for association rules.",2020-09-26T09:00:18Z,Data Mining Lecture 5: Association rule mining continued,https://i.ytimg.com/vi/9WQzZVv5H1U/hqdefault.jpg,Debapriyo Majumdar,PT45M1S,false,95,2,0,0,1,okay so we are starting the first two three minutes let me just recap uh where we were in the app a priori algorithm in the last class so what we did was we started with c we started with l one the frequent items the items with a minimum support and then we generated c2 the candidate sets of size 2. hello anybody has a question can you please go on mute please [Music] okay uh and so on so essentially iteratively from l k minus 1 we generated c4 and then we pruned uh the obvious cases where the subset uh the the anti-monotonicity property is not satisfied and uh then what we wanted to do is we wanted to check um that we wanted to calculate the support of the new candidate item sets right so support of each item set in ck and to do that we have to go through all the transactions and for each transaction we have to see whether this item set is a subset of the transaction if yes then the support count increases by one right okay now so that would have been a bit naive approach what we did instead was this so let me just go through this again so if we have let's say 12 candidate uh 12 candidates in candidate item sets of size 3 in c3 so coming from l2 to c3 if we have 12 of them then what we do we define a hash function k goes to k mod 3 and now i have actually colored them to be more clear so the hash function what it does it sends numbers into one of the three buckets let's say the red green and blue the left side is red the middle one is green and the right one is blue uh and then once we have this then given the set of item sets c3 we can somebody is not on mute can you please all of you check and go on mute okay so um given this what we can do is we can hash according to this hash function we can hash all the item sets uh into this tree right so the left side is 4 1 4 and 7 so ah 1 2 5 first goes left then goes to the middle again goes to the middle so red green green kind of a thing so 1 to seven goes like this one three nine goes like this and so on right so this is i hope i have colored them properly now so after hashing what happens is each item set is a leaf of the tree now given a transaction what we will do is we will hash the subsets or subsets of size 3 of the transaction because we are interested in item sets of size 3 now so we will kind of create all the subsets of size 3 of the transaction and hash them in the same way and only some of them will match with these item sets and will only increment the count for that item set right so what we will do ah suppose this is our uh transaction so somebody has bought one two six seven and eight these five items at once uh and uh we the subset start and the i the transaction is ordered we can make sure that it is ordered like in some order ah i one comes first um so subsets where one is present have to start with one and they will go left if it starts with two then it will go middle and so on right ah starting with 1 7 have to go left left on 2 will have to go left middle and all that so for example this gives us the possibilities of the subset starting with 1 2 and so on now what we see is that for this particular transaction i have 178 that is one possibility if it starts with 1 and 7 it has to be it has to end with eight because it is ordered sorry if it starts with one and two there are three possibilities one two six one two seven one to it now again i can actually branch them into three different branches but just you know to keep the picture simple i have not done it here and uh 1678 also uh it will go into two more branches but uh have not done that but it's just explain this sir yes so in this picture from the root only we're following the hash function right means if uh say instead of 6 we have a 5 then we will not have the third branch instead of 6 if we have 1 2 5 yes yes then we will not have the third branch right yes i mean if the size is only three the size is only five right one two six seven eight then you won't anyway anyway don't have too many possibilities in general what might happen your size is much larger and then it will not only end with nine it may actually have many more so you will accordingly you know expand that yes but in this case if we have one two five seven eight then you will not have the third branch because we are looking for only subsets of size three okay so similarly so we have done that and now having done that one two six seven eight we have we have two hash trees now we'll simply overlap the two hash trees and see whether we have anything common right so what we will see here is uh we'll see that the 678 is the only common so only 678 which is one of our candidates in c3 can only be a subset of this particular transaction so what we do for this transaction we increment the support count of six seven eight i we don't increment the support count of any other then we move to the next transaction this particular hash tree remains all we have to do is we have to hash the transactions uh into a new history and overlap right so essentially that's what we have today all right so now uh this is recap from the last class so where do we stand now into the algorithm so what we have done is we have computed frequent item sets just to remind you we have iteratively from very small item sets like size 1 to size 2 to size 3 and at some point of time the when the item sets sizes are large they will not be frequent anymore and that's what we have done we have finally got we are essentially we have let me just go back to this thing once more to remind you yes we have finished this pseudo code part right so we have calculated the frequent item sets the next step then is uh so first of all how many uh so next step is each frequent item set each frequent k item set means each frequent address of the size k gives rise to several association rules right so if i item set as size k and we remember that any uh binary partition will generate one association rule right one two six seven eight if that is frequent then one two implies six seven eight that is one one two six implies seven eight is also another items uh another association rule so if we ignore uh the whole thing implies phi and phi implies the whole thing then there will be two to the power k minus two such rules so too many rules right so again if we um yeah so too many rules and what we have to do is essentially generate the rules from the frequent item sets but they also have two rules have to satisfy our minimum confidence criteria now all of the rules definitely satisfy the support criteria because the item sets have the minimum support the rules also have the minimum support we have seen that before but obviously now the challenge is first of all there are many item sets and each item set of size k generates 2 power k minus 2 rules we don't want to you know generate all the rules individually and go through all the transactions to compute their confidence right so we will have to do something bit more clever again so yeah so let us look at the following property let us see if x is a subset of y for non empty item sets x and y ok then x to y minus x is an association rule ok um so then we have the following theorem so first of all let's validate that okay so x two y minus x is an association rule why because x is a subset of y ok ah um for uh non-empty item sets then y minus x is whatever remains of y right so ah so then x to y minus x is association rule in our case so the theorem is that follows is this if x prime is a subset of x and is a subset of y then the confidence of x to y minus x is greater than the confidence of x prime to y minus x prime that means if you have a smaller set in the left side of an association rule your confidence has to go down intuitively bread salami implies ham if that's a rule with some confidence then bread implies salami and ham will have a lower confidence so yeah so for example if we have one two three goes to four five that confidence has to be less greater than equal to one two goes to three four five so that means if we calculate confidence of rules higher up then at some point of time the rule is confidence will fall below our threshold and then we can discard other rules below that threshold okay so the proof of that is ah this that observe that the confidence of x to y minus x is essentially the support count of x union y minus x by support count of x and that is same as support count of y by support count of x and similarly the if you do it for the same if you do it for x prime you will get support count of y by support count of x prime but x prime is a subset of x so the support count has to be greater than equal to x right so your denominator goes up so your confidence goes down okay so then the corollary is if x to y minus x is not a high confidence association rule then x prime 2 y minus x prime is also not a high confidence association rule by high confidence it's a terminology similar to frequent item set high means it's not a relative term here how it means a conf and a rule that has confidence at least as much as the minimum confidence threshold okay so high confidence means some rule which has constitute confidence at least as much as the mean con threshold so then if x to y minus x does not have that confidence then x prime to y minus x prime will not have that confidence so then what we will do is we will do level wise rule generation so let us say we have a frequent item set one two three four after our you know first pass of first phase of a priority algorithm we have generated frequent item sets suppose one two three four is one such item set now this will generate lots of rules the the first one with the biggest uh subset on the left is one two three four implies null set okay so that's a trivial one it's not interesting but that's the first one now so suppose this is our x to y minus x here x equal to y right so essentially let me just uh so this let me try to annotate yeah so here y equal to one two three four and x equal to one two three four equal to y okay so this is this is our first step sorry okay so this is the first step now in the next step what we will do is we will reduce x and pass that portion which we have reduced that means y minus x to the right side so 1 2 3 4 2 4 1 2 4 2 3 and so on right so this will be our next step and similarly we can actually build a graph not a tree it will not be a tree because there will be you know there will be multiple parents of some rules so one two three goes to four will give rise to one two goes to three four and one three goes to two four two three goes to one four uh ah but one two four goes to three will also give rise to one two goes to three four and so on right so it will not be a tree but one level can completely generate the level below it and so on obviously by simple combinatorics you can figure out if we have four item set of one size four then the only possible subset of size four is one so the first level will have one rule the second level will have four choose three so that means three rules the third level will have six rules and the fourth table will have again four rules and the fifth level will have the trivial rule phi goes to everything phi implies everything right okay so for not equal to four bigger numbers you will definitely have a graph which will kind of the first level is the thin one then in somewhere in the middle layer it will be the thickest uh like lots of rules and then again it will decrease now what happens suppose suppose we we so what we do we first calculate the association and the confidence of the rules in the upper levels ok so as long as all the rules are passing our confidence test we are fine but somewhere it will fail so suppose one to four goes to three fails the confidence bar if it doesn't have enough confidence then everything below it okay all rules in the sub tree under one to four goes to 3 can be discarded right so this means these rules as well as all of these rules can then be discarded right so this is how one can prune yeah this is how one can prune uh the um the association rules and we don't have to really check the confidence of all the rules okay just a moment let me yeah so is this part clear okay i hope so i'll just then clear this clear all my drawings okay yeah okay yeah so this is done now so now there are a few interesting concepts um associated to this uh which uh which people use and they can be derived from the from whatever properties we we discussed so one is maximal frequent item set so one item set is called maximal frequent if it is frequent but none of its immediate supersets are frequent that means this item set is frequent but if you put any more item into this it will not remain frequent so in this particular ah graph for example uh suppose these items sets are not frequent okay suppose the orange ones are not frequent then which ones are maximal out of these one two three four is not maximal because several ah oh sorry 1 2 is yeah 1 2 is actually so yeah 1 2 is not maximal because 1 two three and one to four uh i'm sorry i'm sorry we are not looking at maximally infrequent we're looking maximum frequent so let me go back to the previous picture yeah so which of these gray item sets are not maximal so they are all frequent the orange ones are not frequent so the gray item sets are frequent but not all of them are maximal so let's look at uh this thing called one right so let's look at this one this set this is not maximal because although this is a superset and this is not frequent it also has you can still add three to one and still it will remain frequent so this is not maximum so which ones are maximal here these are not maximal here 2 3 is not maximal because it super sets none of the supersets are maximal 2 4 is also not maximal and 1 3 4 are not maximum all right so so all frequent item sets are subsets of one of the maximal frequent items that's that's an interesting observation that look at all the gray and yellow item sets which are all frequent they have to be subset of one of the maximal frequencies so which means either they are by themselves maximal frequent item set which means its a subset of itself or they are subsets of these ok because if it is not if it is not subset of any of the maximal frequent item sets ok then for example um yeah so if it's not subset of any of the maximum frequent item sets then um okay yeah so for example if let's say 1 4 was not a subset of this then as soon as you increase this it would have become not frequent right so so it must have some closure where if you if you add any more element to it either it is maximal by itself or if it is not maximal by itself then it has a superset which is maximal so that is why all frequent item sets are subsets of one of the maximal frequent item sets ok so ah so why are maximal frequent item sets useful because they are valuable compact representation of the frequent item sets so essentially you can instead of giving someone the list of all frequent frequent item sets what you can do is you can simply list the set of all maximal frequent item sets and that means the subsets are frequent anyway right so it is kind of giving you the boundary of frequent item sets however they don't contain the support information of the subsets right so obviously the maximal frequent assets have the minimum support but the subsets may have more support so so it says that all supersets have lesser support and all subsets may have more support but does not say if any of the any of sub any of the subsets also have the same support so what is so this gives rise to another concept called closed frequent item sets so the closed item set is an item set x for which none of its immediate supersets have exactly the same support count as x okay so if x is not closed then at least one of its immediate superset will have the same support and closed frequent will be one which is closed and frequent as well so in other words a closed item set means if you increase any more item into it your support will decrease so if the concept is similar to the maximality but its maximality ah under support condition not under frequent maximal frequent item set conditions so here what it means is that let us say bread ham and salami are bought together and it has certain support but if you add any more item into this then the support will decrease so then then we will call this item set closed and closed can also be a set of all items right so that is not interesting to us so closed frequent is the one which ah is closed as well as the support is greater than the minimum support so support for non closed frequent additive sets can be determined from the support information of the closed frequent item sets because close frequent addresses tell you that if you ah if you go any further then the support will decrease so it gives a boundary or it gives a bound of the support to a support of its supersets so in when we look at uh the topology of item sets so we have lots of frequent item sets and under that there will be some of them will be closed frequent and within that also some of them will be maximal frequent ok so ah you can actually try to prove these things if if you want uh you can consider that ah take a maximal frequent data set to prove that it is also closed uh frequent and frequent item set of course it is frequent because uh that is uh given by definition but a maximal is also closed and all you can actually try to prove okay so ah now what we have done so far is we have talked in terms of support and confidence and support and confidence as if you know they will solve all our problems but are they the only interesting measures of uh association rules right ah the answer is no there are lots of them and lots of statistics have been developed on top of these subjects also so let us look at some of them let us say even from a small data set a very large number of rules can be generated right so for example supported confidence conditions are relaxed so if you decrease your threshold slightly then you have a hume i mean the number of rules that will generate or that will pass the threshold will suddenly increase a lot now that means so many rules cannot be interesting to you because you cannot take business decisions on so many rules right you can only take targeted decisions so some other uh some more interning interestingness measures for patterns or rules are required ok so there can be two kinds of interestingness measures one is ah support uh yeah so a measure that uses statistics so support confidence correlation etcetera etcetera uh what we what we so two kinds are one is objective the these are objective because these you can calculate it's not that someone will look at this rule okay so bread and salami implies ham i can look at it and say well that's obvious because you know these people kind of eat western breakfast okay but that's not an objective measure right that's a subjective measure so support confidence correlation they are objective measures they are domain independent you can actually run ah programs and calculate them and they do not require human involvement and the subjective measures are let's say suppose the rule salami to bread is not so interesting because it is obvious i mean if you want to eat salami you probably eat it with a bread so but it is a rule such as salami to dishwasher detergent or salami to diaper these are less obvious well i'm just i have tried to make it intuitive in the sense that you know people who are used to western lifestyle right so but it's a less obvious implication than now the problem here of course is subjectively subjectivity means you need human human intervention but the good thing is they can be more interesting for marketing experts and they can generate non-trivial cross-sell and so on okay so so method for subjective measurement what can it be obviously if you are given a huge database you are not going to put in 5 experts there and tell them to look through all the data manually so what can be done is you can do some objective measures and then provide some visualization with a human in the loop or something or you can put some constraints for the rules some more filtering and things like that essentially you know make it interactive so that yes the objective measures are there but the business decisions are not taken uh completely uh automate in an automated way right so that would be the subjective subjectivity part into it you know or filter obvious and non-actionable rules and so on so there are some more measures one is contingency table this i am i'm sure many of you have seen so let's say ah this t and not t coffee and not coffee right so so this is frequency tabulated for a pair of binary variables how many people have both t and coffee how many people have t but not coffee coffee but not t and so on and the columns actually sum so the first column is the sum of number of people as a total number of people who have coffee and first row total number of people who have t and all that so this is an useful evaluation ah useful for evaluation in last illustration now generally ah this is just an example right generally these are the the notation is the following if you have two uh attributes a and b and then not a and not b right ah then f one one is the frequency of both being present if zero one is not the first one but the second one and so on and then the sum is f plus 1 means sum of uh the 0 i mean some of the sorry sum of the second one if plus 0 sorry if zero plus is uh if one plus is sum of the first one if plus one is some of the second one if zero plus is um sum of the not first one and f plus zero is some of the not second one now um yeah so a prime or b prime denotes the not a naught b ah now you can see here that f 1 plus essentially is a support count of a and if plus 1 is a support count of b right yeah so that you can see from this now what are the limitations of support and confidence just as they are that is tuning that support threshold is very tricky if you have if you choose the low threshold then too many rules will be generated if you choose a high threshold and then your number of rules will be small but you would never know what are you missing out on right so it may be possible that some product which not too many people buy but still quite a few people buy and you are actually missing out on ah discovering interesting rules similarly limitation of confidence let us look at this example that we have so consider the rule t to coffee so t to coffee what would be the confidence of what is the support and confidence so you see the support will be the union of t and coffee right so union and tea and coffee essentially is sorry union means the number of transactions or number of people who have both tea and coffee that is 150 and the total is one thousand so fifteen percent that is the support of tea and coffee tea2 coffee confidence of tea to coffee is out of 150 people uh out of 200 people who have tea 150 of them have coffee so that is 75 so now let's look at this very carefully this is a low support yeah support we said that support thresholds are typically low in this toy example also low support high confidence rule so it looks like that we can infer if you have t you also have copy well from the data okay don't think what happens in your life from the data if you have t you also have coffee but overall 80 percent people have coffee so out of 1000 800 people have coffee anyway right so the rule nothing to coffee has confidence 80 percent so that means if you don't tell anything about a person the chance of that person having coffee is 80 percent but if you tell say it's a conditional probability right if you if you say that this person has tea then the chance of that person having coffee is 75 percent which actually goes down right so among tea takers the percentage actually actually drops to 75 percent so this is a bit counter intuitive if we just think of support and confidence right so a rule is interesting when it actually implies right so if if you have t it should be more likely than random that you have coffee then only the rule is interesting but in this case it is the other way right so where does it go wrong why does the confidence measure gives us a wrong intuition here because the confidence measure ignores the support of y for a rule x to y yes confidence measure is support of x to y by support of x well if x has very low support y has much larger support then we have a problem right so so the the the other side of the story is captured by some other measures like interest factor just check the time yeah we have some time so interest factor and lift okay so lift is just the other way so confidence of ah x to y um i am sorry it should be i think support of it should be support of x two y i will correct it later so the lift should be ah support of x 2 y by support of y just like confidence is support of x 2 y by support of x lift is support of x 2 y by support of y for binary variable lift is equivalent to the following which is called interest factor so i x y ok that is support of x union y by support of x times up out of y which is nothing but the total number ah of f 1 1 and f 1 plus times f plus 1. now this is similar to the baseline frequency comparison under statistical independence assumption uh we'll not go into much deeper into this but yeah so these all these measures are actually statistical measures and what you want to look at at what point of time the decisions have to be made wisely now if x and y are statistically independent then their baseline frequency that means expected frequency of x and y both occurring is anyway f 1 plus times f plus 1 by n okay all right so ah intuitively what happens is if ah interest factor is one [Music] so interest factor becomes one if if x and y are independent interest factor becomes greater than 1 if x and y have a positive correlation interest factor becomes less than 1 if x and y have a negative correlation ok so let us verify for the t coffee example the t coffee the same thing the interest factor of t coffee is okay so 0.15 by 0.2 times 0.8 so that is actually 0.94 it is slightly less than so actually they are slightly uh the correlation is you know slightly below uh one so they're slightly negatively correlated uh if they are independent then having tea should have no impact on somebody having coffee so you pick once if pick a person randomly the probability of that person having coffee is 80 percent it would have been the case that you pick up you pick a person who has tea but you don't know anything about him or her other other than that and the probability of that person also having coffee would have also been 80 percent if the interest factor would have been one but the interest factor is less than one so it's actually a bit of negative correlation limitations of interest factor so yeah again this is also not invincible so let us look at this example so let us say text analysis so suppose we count number of occurrences of words in scientific articles related to nlp or data mining okay so text analysis well text and analysis both are very common words right and suppose we have something like this and graph mining well graph is very common mining is also very common but graph mining may not be that common now the interest factor of text analysis is 1.02 interest factor of graph mining is 4.08 now i mean it's somewhat subjective but you can probably feel that text and analysis are more related than graph and mining obviously graph mining is a field but it's not as common as text analysis may be so the confidence ma but the confidence measure here will probably do somewhat better text to analysis will have 90 or around 95 percent confidence and graph to mining will have around 28 percent confidence so in this case again the confidence measure does better ok ah so what goes wrong here i will leave this you can think about this uh you can think about this question and yeah i'm i probably will not ask you in the exam or i might but yeah just think about this if you don't get it then maybe in the doubt clearing session or in the next class we can talk about it okay so there are more measures a correlation coefficient for binary variables ah then we have is measure which is a combination of the in the i s and s measures combined so this is mathematically equivalent to the cosine measure for binary variables so cosine cos cosine measure is like the dot product by the norms kind of thing so mathematically equivalent to that so um all right so what should be the properties of objective measures uh so objective measures as we saw different measures are different measures are good for different kinds of things and it is not very clear when that will tally with our intuition or what we want to do so um just a moment yeah okay so ah so one is the inversion property is a measure invariant under inversion operation that means if we exchange f 1 1 with f 0 0 and f 0 1 with f 1 0 will the measure remain the same what is f 1 1 both are present okay so that means if we just say yes he called i mean if we swap yes and no okay will the measure remain the same right t and coffee i have t i have coffee the relation between these two will i also have the same thing between i don't have t and i don't have coffee now so inversion property some measure has the inversion property if the value of the measure remains the same under inversion then null addition property so null addition property is invariant under addition of counts for other variables that means if you increase f 0 0 so let's say i keep the number of occurrences of both occurring the same okay so number of people having both t and coffee remain the same but i keep increasing the number of people who don't have either of them if i simply so seemingly that should be more like unrelated right so that's like unrelated to what we are thinking about so that's null essentially so null addition property if we increase f 0 0 a lot or whatever will my measure remain the same so again one exercise from the formulas of the support confidence at least the simple ones please do it at home you don't have to really think deep into these statistical measures because that's not the main goal of the course or our our goal was to actually understand the algorithmic part and support and confidence and so on so the exercise for you will be that yes support confidence interest factor lift etc which of the measures have inversion property and null null addition property you know just look at it and verify okay all right so uh um so with this uh i i would uh finish the this chapter of frequent items in mining or association rule mining there are some further reading because we are actually going to have smaller class hours and total number of class hours will be a bit small we will not discuss in detail the following item so look at the third and fourth item there are some improved versions or rather improvements after a priority algorithm okay ah especially there is this frequent pattern growth algorithm so the priority algorithm makes multiple passes through the database right each time you have some l yeah somebody has something uh can you hear me folks is my voice clear yes yes sir okay i think neeraj is writing something maybe with him but anyway so if others are able to hear then the recording is going fine all right so the a prior algorithm makes multiple passes like you go from lk minus 1 to ck and then you have to go through the whole database again to do the pruning and all this confidence and support calculation right so for each k you have to go one pass the fp growth or frequent pattern growth algorithm makes two passes using something called fp3 it doesn't generate candidates it does things in a different way however it may have more memory requirement but in modern times epi growth is you know more commonly used if people still want to mine association rules ah so the third the last point here the this is parallel fe growth and for a recommendation system whatever but basically it works for parallel free growth and this actually uses mapreduce it's also implemented in spark libraries you don't actually have to implement it so ah i mean you can if necessary you can actually simply run spark and then use this epic growth algorithm all right so so these are the references for this chapter that i have covered um if you have questions right now let's discuss that and after that what i will do is we'll talk a little bit about the quizzes that we're going to have later this week yeah questions if you have right now okay if you don't have immediate questions we can of course ask later so essentially take away from this takeaway from this chapter is obviously the details of the priority algorithm it's a database oriented algorithm ah and the support the concepts of support and confidence in detail the anti-monotonicity property and all and these things okay so and uh what can be the limitation of support and confidence uh when to use it when to not use it kind of thing so that's pretty much the takeaway uh from this chapter uh now in the next chapter we will go into the modern version of recommender systems okay so obviously uh market basket analysis came up because people wanted to do marketing marketing is not shopping marketing is advertisement right so people want to do marketing and yeah but nowadays advertisement is everywhere and in multiple of uh the chapters in this course throughout the semester we will actually see different different you know technical aspects of advertising or recommending products to people so in the next chapter what we will do is uh in the next two chapters actually first one will be similar item finding similar items and then it will be actually core recommended systems with dimension reduction collaborative filtering and stuff like that all right so um i guess if you don't have immediate questions right now let's uh let's actually open the following let me then sir if i have questions later like i'll go through the slides if i have questions later then when can i contact you yeah so there is a doubt clearing session okay yes friday 2 15 to 2 45 
T6JszjTZTnM,27,"Check this- https://www.atozsky.com/ NIELIT- National Institute of Electronics & Information Technology
http://econtent.nielit.gov.in/login/index.php",2019-10-22T04:14:01Z,DWDM 3 Data Mining Primitives,https://i.ytimg.com/vi/T6JszjTZTnM/hqdefault.jpg,NIELIT Original,PT59M18S,false,225,1,0,0,N/A,"learning objectives in this chapter the user will learn the following in detail data pre-processing including data cleaning data integration there are transformation discretization and concept hierarchy generation definition and specification of a generic data mining task description of data mining query language with few example queries concept hierarchy generation concept hierarchy generation for nominal data nominal attributes have a finite but possibly large number of distinct values with no ordering among the values manual definition of concert hierarchies can be a tedious and time-consuming task for a user or domain expert fortunately many hierarchies are implicit within the database schema and can be automatically defined at the schema definition level the concept hierarchies can be used to transform the data into multiple levels of granularity for example data mining patterns regarding sales may be found relating to specific regions or countries in addition to individual branch locations four methods for the generation of concept hierarchies for nominal data as follows pacification of a partial ordering of attributes explicitly at the schema level by users or experts concept hierarchies for nominal attributes or dimensions typically involve a group of attributes a user or expert can easily define a concept hierarchy by specifying a partial or total ordering of the attributes at the schema level for example suppose that a relational database contains the following group of attributes street city province of state and country similarly a data warehouse location dimension may contain the same attributes a hierarchy can be defined by specifying the total ordering among these attributes at the schema level such as street belongs to City belongs to province or state belongs to country specification of a portion of a hierarchy by explicit data grouping this is essentially the manual definition of a portion of a concept hierarchy in a large database it is unrealistic to define an entire concept hierarchy by explicit value enumeration on the contrary we can easily specify explicit groupings for a small portion of intermediate level data for example after specifying that province and country form a hierarchy at the schema level a user could define some intermediate levels manually such as Alberta Saskatchewan Manitoba proper subset of prairies Canada and British Columbia prairies Canada has proper subsets of Western Canada specification of a set of attributes but not of their partial ordering a user may specify a set of attributes forming a concept hierarchy but omit to explicitly state their partial ordering the system can then try to automatically generate the attribute ordering so as to construct a meaningful of concept hierarchy a concept hierarchy can be automatically generated based on a number of distinct values per attribute in the given attribute set the attribute with the most distinct value is placed at the lowest hierarchy level the lower the number of distinct values and attribute has the higher it is in the generated concept hierarchy this heuristic rule works well in many cases some local level swapping or adjustments may be applied by users or experts when necessary after examination of the generated hierarchy specification of only a partial set of attributes sometimes a user can be careless when defining a hierarchy or have only a vague idea about what should be included in a hierarchy consequently the user may have included only a small subset of the relevant attributes in the hierarchy specification figure shows the automatic generation of a schema concept hierarchy based on the number of distinct attribute values cleaning real-world data tend to be incomplete noisy and inconsistent data cleaning or data cleansing routines attempt to fill in missing values smooth out noise while identifying outliers and correct inconsistencies in the data missing values imagine that you need to analyze all electronic sales and customer data you note that many cupid's have no recorded value for several attributes such as customer income how can you go about filling in the missing values for this attribute let's look at the following methods ignore the tuple this is usually done when the class label is missing assuming the mining task involves classification this method is not very effective unless the tuple contains several attributes with missing values it is especially poor when the percentage of missing values per attribute varies considerably by ignoring the tuple we do not make use of the remaining attributes values in the tuple such data could have been useful to the task at hand fill in the missing value manually in general this approach is time-consuming and may not be feasible given a large data set with many missing values use a global constant to fill in the missing value replace all missing attribute values by the same constant such as a label like unknown or minus infinity if missing values are replaced by say unknown then the mining program may mistakenly think that they form an interesting concept since they all have a value in common that of a null hence although this method is simple it is not foolproof use a measure of central tendency for the attribute to fill in the missing values a measure of central tendency indicates the middle value of a data distribution for normal symmetric data distributions the mean can be used while skewed data distribution should employ the median for example supposed to the data distribution regarding the income of all electronics users is symmetric and that the mean income is $56,000 use this value to replace the missing value for income use the attribute mean or median for all samples belonging to the same class as a given tuple for example if classifying customers according to credit risk we may replace the missing value with a mean income value for customers in the same credit risk category as that of the given tuple if the data distribution for a given class is cued the median value is a better choice use the most probable value to fill in the missing value this may be determined with the regression inference based tools using a Bayesian formalism our decision tree induction for example using the other customer attributes in your data set you may construct a decision tree to predict the missing values for income methods three through six bias the data the filled in value may not be correct method six however is a popular strategy in comparison to the other methods it uses the most information from the present data to predict missing values by considering the other attributes values in its estimation of the missing value for income there is a greater chance that the relationships between income and the other attributes are preserved noisy data what is noise noise is a random error or variance in a measured variable given a numeric attribute such as say price how can we smooth out the data to remove the noise let's look at the following data smoothing techniques winning bidding method smooth assorted data value by consulting its neighborhood that is the values around it the sorted values are distributed into a number of buckets or bins because winning methods consult the neighborhood of values they perform local smoothing in this example the data for price are first sorted and then partitioned into equal frequency bins of size three that is each bin contains three values in smoothing by bin means each value in a bin is replaced by the mean value of the bin for example the mean of the values for 8 and 15 in bin 1 is 9 therefore each original value in this bin is replaced by the value 9 similarly smoothing by bin medians can be employed in which each bin value is replaced by the bin median in smoothing by bin boundaries the minimum and maximum values in a given bin are identified as a bin boundaries each bin value is then replaced by the closest boundary value in general the larger the width the greater the effect of the smoothing alternatively bins may be equal with where the interval range of values in each bin is constant regression there a smoothing can also be done by regression a technique that conforms data values to a function linear regression involves finding the best line to fit two attributes our variables so that one attribute can be used to predict the other multiple linear regression is an extension of linear regression where more than two attributes are involved and the data are fit to a multi-dimensional surface outlier analysis outliers may be detected by clustering for example where similar values are are organized into groups or clusters intuitively values that fall outside of the set of clusters may be considered outliers shows a 2d customer data plot with respect to customer locations in a city showing three data clusters outliers may be detected as values that fall outside of the cluster sets as a process missing values noise and inconsistencies contribute to inaccurate data the first step in data cleaning as a process is discrepancy detection discrepancies can be caused by several factors including poorly designed data entry forms that have many optional fields human error in data entry deliberate errors such as respondents not wanting to divulge information about themselves and data decay outdated addresses this capensis may also arise from inconsistent data representations and inconsistent use of codes other sources of discrepancies include errors in instrumentation devices that record data and system errors errors can also occur when the data are inadequately used for purposes other than originally intended there may also be inconsistencies due to data integration where a given attribute can have different names in different data bases discrepancy detection look out for the inconsistent use of chords and any inconsistent data representations for example 20 10 12 25 and 25 12 2010 for date field overloading is another error source that typically results when developers squeeze new attribute definitions into unused bit portions of already defined attributes for example an unused bit of an attribute that has a value range that uses only say 31 out of 32 bits the data should also be examined regarding unique rules consecutive rules null rules a unique rule says that each value of the given attribute must be different from all other values for that attribute a consecutive rule says that there can be no missing values between the lowest and highest values for the attribute and that all values must also be unique as in check numbers an old rule specifies the use of blanks question marks special characters are other strings that may indicate the null condition example where a value for a given attribute is not available and how such values should be handled the null rule should specify how to Kord the null condition for example that has to stole zero for numeric attributes a blank for character attributes or any of the conventions that may be in use for example entries like don't know our question mark should be transformed to blank there are a number of different commercial tools that can aid in the discrepancy detection step data scrubbing tools use simple domain knowledge that is knowledge of postal addresses and spell checking to detect errors and make corrections of the data these tools rely on passing and fuzzy matching techniques when cleaning data from multiple sources data auditing tools find discrepancies by analyzing the data to discover rules and relationships and detecting data that violates such conditions they are variants of data mining tools for example they may employ statistical analysis to find correlations are clustering to identify outliers correcting data inconsistency some data inconsistencies may be corrected manually using external references most errors however will require data transformations that is once we find discrepancies we typically need to define and apply a series of transformations to correct them commercial tools can assist in the data transformation step data migration tools allow simple transformations to be specified such as to replace a string gender by sex ETL extraction transition loading tools allow users to specify transforms through a graphical user interface GUI these tools typically support only a restricted set of transforms so that often we may also choose to write custom scripts for this step of the data cleaning process the two-step process of discrepancy detection and data transformation iterates this process however is error-prone and time-consuming transformations are often done as a batch process while the user waits without feedback only after the transformation is complete can the user go back and check that no new anomalies have been mistakenly created typically numerous iterations are before the user is satisfied another approach to increase interactivity in data cleaning the development of declarative languages for the specification of data transformation operators data discretization let a be a numeric attribute within observed values v1 going up to VN min Max normalization performs a linear transformation on the original data opposed at min a and Max a are the minimum and maximum values of attribute a min max normalization Maps a value VI of a - V - I in the range Neumann a - nu max a by computing VA - equals VI minus min a / max a - min a into nu max a minus nu min a + Neumann a max normalization preserves the relationships among the original data values it will encounter an outer bounds error if a future input case for normalization falls outside the original data range for a discretization by binning binning is a top-down splitting technique based on a specified number of Bin's these methods are also used as discretization methods for data reduction and concept hierarchy generation for example attribute values can be discretized by applying equal width or equal frequency billing and then replacing each bin value by the bin mean or median as in smoothing by bin means our smoothing by bin medians respectively these techniques can be applied recursively to the resulting partitions to generate concept hierarchies binning does not use class information and is therefore an unsupervised discretization technique it is sensitive to the user specified number of bins as well as the presence of outliers discretization by a histogram analysis like bidding histogram analysis is an unsupervised discretization tick because it does not use class information a histogram partitions the value of an attribute a into disjoint ranges called buckets or bins various partitioning rules can be used to define histograms in an equal width histogram for example the values are partitioned into equal sized partitions are ranges with an equal frequency histogram the values of partitioned so that ideally each partition contains the same number of data tuples the histogram analysis algorithm can be applied recursively to each partition in order to automatically generate a multi-level concept hierarchy with the procedure terminating once a pre specified number of concept levels has been reached a minimum interval size can also be used per level to control the recursive procedure this pays for is a minimum width of a partition or the minimum number of values for each partition at each level histograms can also be partitioned based on cluster analysis of the data distribution discretization by cluster decision tree and correlation analyses clustering decision tree analysis and correlation analysis can be used for data discretization cluster analysis is a popular data discretization method a clustering algorithm can be applied to a discretize a numeric attribute a by partitioning the values of a into clusters or groups clustering takes the distribution of a into consideration as well as the closeness of data points and therefore is able to produce high-quality discretization results clustering can be used to generate a concept hierarchy or a by following either a top-down splitting strategy or a bottom of merging strategy where each cluster forms a node of the concept hierarchy in the former each initial cluster or partition may be further decomposed into several sub clusters forming a lower level of hierarchy in the latter clusters are found by repeatedly grouping neighboring clusters in order to form higher-level concepts techniques to generate decision trees for classification can be applied to discretization such techniques employ a top-down splitting approach unlike the other methods mentioned so far decision tree approaches to discretization are super wised that is they make use of class label information class distribution information is used in the calculation and determination of split points data values for partitioning and attribute range intuitively the main idea is to select split points so that a given resulting partition contains as many tuples of the same class as possible entropy is the most commonly used measure for this purpose to discretize a numeric attribute a the method selects the value of a that has the minimum entropy as a split point and recursively partitions the resulting intervals to arrive at a hierarchical discard ization such discretization forms a concept hierarchy for a because decision tree based discretization uses class information it is more likely that the interval boundaries split points are defined to occur in places that may help improve classification accuracy measures of correlation can be used for discretization I'm merge is a chi-squared based discretization method the discretization methods that we have studied up to this point have all employed a top-down splitting strategy this contracts with Kai merge which employs a bottom-up approach by finding the best neighboring intervals and then merging them to form larger intervals recursively as with decision tree analysis Kai merge is supervised in that it uses class information the basic notion is that for accurate discretization the relative class frequency should be fairly consistent with an interval therefore if two adjacent intervals have a very similar distribution of classes then the intervals can be merged otherwise they should remain separate Kai merge proceeds as follows initially each distinct value of a numeric attribute a is considered to be one interval chi-square tests are performed for every pair of adjacent intervals adjacent intervals with the least chi-square values are merged together because loci is quite values for a pair indicate similar class distributions this merging process proceeds recursively until a predefined stopping criterion is met data integration data mining often requires data integration the merging of data from multiple data stores careful integration can help reduce and avoid redundancies and inconsistencies in the resulting data set this can help improve the accuracy and speed of the subsequent data mining process the semantic heterogeneity and structure of data post great challenges in data integration entity identification problem data analysis task involves data integration which combines data from multiple sources into a coherent data store as in data warehousing these sources may include multiple databases data cubes of flat files there are a number of issues to consider during data integration schema integration and object matching can be tricky how can equivalent real world entities from multiple data sources be matched up this is referred to as the entity identification problem metadata can be used to help out errors in schema integration the metadata may also be used to help transform the data when matching attributes from one database to another during integration special attention must be paid to the structure of the data this is to ensure that any attribute functional dependencies and referential constraints in the source system match those in the target system redundancy and correlation analysis redundancy is another important issue in data integration an attribute may be redundant if it can be derived from another attribute our set of attributes inconsistencies in attribute or dimension naming can also cause redundancies in the resulting data set some redundancies can be detected by correlation analysis given two attributes such analysis can measure how strongly one attribute implies the other based on the available data for nominal data we use the chi-square test for numeric attributes we can use the correlation coefficient and covariance both of which access how one attributes values vary from those of another chi-square correlation test for nominal data for nominal data a correlation relationship between the two attributes a and B can be discovered by a chi-square test suppose a has C distinct values namely a 1 a 2 going on up to AC V has are distinct values namely V 1 V 2 going on up to be R let AI VJ denote the joint event that attribute a takes on a value AJ AI and attribute B takes on value bj that is where capital a equals small AI capital b equals small bj the chi-square value also known as the Pearson chi-square statistic is computed as chi-square equals Sigma I equals 1 to see Sigma J equals 1 to R o IJ minus e IJ whole squared divided by e IJ where oh I J is the observed frequency actual count of the joint event a I BJ and E IJ is the expected frequency of AI V J which can be computed as e IJ equals count a minus small AI into count B minus small BJ divided by n where n is the number of data tuples count a equals AI is a number of tuples having value AI for a and count B equals B J is a number of tuples having value BJ for be the chi-square statistic tests the hypothesis that a and B are independent that is there is no correlation between them the test is based on a significant level with R minus 1 into C minus 1 degrees of freedom correlation coefficient for numeric data for numeric attributes we can evaluate the correlation between two attributes a and B by computing the correlation coefficient also known as Pearson's product moment coefficient named after its inventor Karl Pearson this is our a/b equals Sigma I equals 1 to n brackets a I minus a bar into bi minus B bar R of a and B equals summation I 1 to N AI minus a bar VI minus B bar divided by n Sigma a sigma B equals summation of I equals 1 to n AI into B I minus n a bar V bar divided by n Sigma a sigma B where n is the number of tuples AI and bi are the respective values of a and B into ply a bar and B bar are the respective mean values of a and B Sigma a and Sigma B are the respective standard deviations of a and B and summation AI bi is a sum of the a/b cross product note that our a B lies between minus 1 and plus 1 if our a B is greater than 0 then a and B are positively correlated meaning that the values of a increased as the values of B increase the higher the value the stronger the correlation hence the higher value may indicate that a or B may be removed as a redundancy if the resulting value is equal to 0 then a and B are independent and there is no correlation between them if the resulting value is less than 0 then a and B are negatively correlated where the values of one attribute increase as values of the other attribute decrease correlation does not imply causality that is if a and B are correlated this does necessarily imply that a causes B or that B causes a covariance of numeric data in probability theory and statistics correlation and covariance are two similar measures for assessing how much two attributes change together consider two numeric attributes a and B and a set of n observations a1 v1 going up to a n BN the mean value of a and B respectively are also known as the expected value on a and B that is expected a equals a bar equals summation I 1 to n AI divided by N and expected value of B equals that is V bar equals summation I 1 to n VI divided by n the covariance between a and B is defined as covariance of a and B equals e into a minus a bar into B minus B bar equals summation of I 1 to n AI minus a bar into bi minus B bar divided by n if we compare above equations are a and B equals covariance of a and B divided by Sigma a sigma B where Sigma a and Sigma B are the standard deviations of a and B respectively it can also be shown that covariance of a and B equals expected value of a into B minus a bar b bar this equation may simplify calculations for two attributes a and B that tend to change together if a is larger than a bar the expected value of a then B is likely to be larger than B bar the expected value of B therefore the covariance between a and B is positive on the other hand if one of the attributes tends to be above its expected value when the other attribute is below the expected value then the covariance of a and B is negative if a and B are independent that is they do not have correlation then expected value of a dot B equals e of a into e of B therefore the covariance is covariance a and B equals e of a dot B minus a bar b bar equals e of a into e of b minus a bar b bar equals zero however the converse is not true some pairs of random variables attributes may have covariance of zero but are not independent only under some additional assumptions that is the data follow multivariate normal distributions thus a covariance of zero imply independence tuple duplication in addition to detecting redundancies between attributes duplication should also be detected at the tuple level the use of denormalized tables is another source of data redundancy inconsistencies often arise between various duplicates due to inaccurate data entry our update some but not all data occurrences for example if a purchase order database contains attributes for the purchases name and address instead of a key to this information in a purchase database discrepancies can occur such as the same purchases name appearing with two different addresses within the purchase order database data value conflict detection and resolution data integration also involves a detection and resolution of data value conflicts for example for the same real-world entity attribute values from different sources may differ this may be due to differences in representation scaling or encoding for instance a weight attribute may be stored in metric units in one system and British imperial units in other attributes may also differ on the abstraction level where an attribute on one system is recorded at say a lower level of abstraction than the same attribute in another for example the total sales in one database may refer to one branch of all electronics while an attribute of the same name in another database may refer to the total sales for all electronic store in a given region data pre-processing data have quality if they satisfy the requirements of the intended use there are many factors comprising data quality including accuracy completeness consistency timeliness believability and interpret ability users of database system have reported errors unusual values and inconsistencies in the data recorded for some transactions the data that is to be analyzed by data mining techniques are incomplete lacking attribute values or certain attributes of interest or containing only aggregate data inaccurate or noisy containing errors or values that deviate from the expected and inconsistent containing discrepancies in the department codes used to categorize items elements defining data quality accuracy completeness and consistency inaccurate incomplete and inconsistent data are commonplace properties of large real world databases and data warehouses in accurate data reasons for inaccurate data the data collection instruments used may be faulty they may have been human or computer errors occurring at data entry users may purposely submit incorrect data values for mandatory fields when they do not wish to submit personal information for example by choosing the default value January 1 displayed for birthday this is known as disguised missing data errors and data transmission can also occur then technology limitations such as limited buffer size for coordinating synchronized data transfer and consumption incorrect data may also result from inconsistencies in naming conventions or data codes or inconsistent formats for input fields for example date duplicate tuples also require data cleaning in complete Dera reasons for incomplete data attributes of interest may not always be available such as customer information for sales transaction data other data may not be included simply because they were not considered important at the time of entry xi data may not be recorded due to a misunderstanding or because of equipment malfunctions data that were inconsistent with other recorded data may have been deleted furthermore the recording of the data history or modifications may have been overlooked missing data particularly for tuples with missing value for some attributes may need to be inferred data quality depends on the intended use of the data to different users may have very different assessments of the quality of a given data base timeliness also affects data quality suppose that your own the distribution of monthly sales bonuses to the top sales representatives at all electronics several sales representatives however failed to submit their sales records on time at the end of the month there are also a number of Corrections and adjustments that flow in after the months end for a period of time following each month the data stored in the database are incomplete however once all of the data are received it is correct the fact that the month and data are not updated in a timely fashion has a negative impact on the data quality two of the factors affecting data quality are believability and interpretability believability reflects how much cell data are trusted by users while interpretability reflects how easily the data are understood the post at a data base at one point had several errors all of which have since been corrected the past errors however had caused many problems for sales department users and so they no longer trust the data the data also may use many accounting codes which the sales department does not know how to interpret even though the database is now accurate complete consistent and timely sales department users may regard it as a low quality due to poor believability and interpretability major tasks in data pre-processing the major steps involved in data pre-processing are data cleaning there integration data reduction data transformation data cleaning data cleaning routines work to clean the data by filling in missing values smoothing noisy data identifying or removing outliers and resolving inconsistencies if users believe the data are dirty the unlikely to trust the results of any data mining that has been applied furthermore dirty data can cause confusion for the mining procedure resulting an unreliable output although most mining routines have some procedures for dealing with incomplete or noisy data they are not always robust instead they may concentrate on avoiding overfitting the data to the function be modeled therefore a useful pre-processing step is to run your data through some data cleaning routines data integration suppose that you would like to include data from multiple sources in your analysis this would involve integrating multiple databases data cubes or files that is data integration yet some attributes representing a given concept may have different names in different databases causing inconsistencies and redundancies for example the attribute for customer identification may be referred to as customer ID in one data store and cust ID in another naming inconsistencies may also occur for attribute values for example the same first name could be registered as bill in one database William in another and be in a third having a large amount of redundant data may slow down or confuse the knowledge discovery process clearly in addition to data cleaning steps must be taken to help avoid redundancies during data integration typically data cleaning and data integration are performed as a pre-processing step when preparing the data for a data warehouse additional data cleaning can be performed to detect and remove redundancies that may have resulted from data integration data reduction data reduction obtains the reduced representation of the data set that is much smaller in volume yet produces the same or almost the same analytical results data reduction strategies include dimensionality reduction and numerosity reduction in dimensionality reduction data encoding schemes are applied so as to obtain a reduced or compressed representation of the original data examples include data compression techniques such as wavelet transforms and principal components analysis attribute subset selection for example removing irrelevant attributes and attribute construction for example where a small set of more useful attributes is derived from the original set in numerosity reduction the data are replaced by alternate smaller representations using parametric models that is regression or log linear models or nonparametric models such as histograms cluster sampling or data aggregation data transformation in data transformation the data are transformed are consolidated into forms appropriate for mining strategies for data transformation include the following smoothing works to remove noise from the data techniques include binning regression and clustering attribute construction or feature construction where new attributes are constructed and added from the given set of attributes to help the mining process aggregation where summary or aggregation operations are applied to the data for example the daily sales data may be aggregated so as to compute monthly and annual total amounts this step is typically used in constructing a data cube for data analysis at multiple abstraction levels normalization where the attribute data are scales who has to fall within a smaller range such as minus 1 to 1 or 0 to 1.0 discretization where the raw values of a numeric attribute such as age are replaced by interval tables for example 0 to 10 11 to 20 etc or conceptual labels such as youth adult senior the labels in turn can be recursively organized into higher-level concepts resulting in a concept hierarchy or the numeric attribute concept hierarchy generation for nominal data where attributes such as Street can be generalized to higher level concepts like city or country many hierarchies for nominal attributes are implicit within the database schema and can be automatically defined at the schema definition level data transformation by normalization the measurement unit use can affect the data analysis normalizing the data attempts to give all attributes and equal normalization is particularly useful for classification algorithms involving neural networks or distance measurements such as nearest neighbor classification and clustering in using the neural network back propagation algorithm for classification mining normalizing the input values for each attribute measured in the training tuples will help speed up the learning phase for distance based methods normalization helps prevent attributes with initially large ranges such as income from outweighing attributes with initially smaller ranges such as binary attributes it is also useful when given no prior knowledge of the data there are many methods for data normalization we study min max normalization z-score normalization and normalization by decimal scaling and specification of a generic data mining task data mining paths can be classified into two categories descriptive and predictive descriptive mining tasks characterized the general properties of the data in the database predictive mining tasks perform inference on the current data in order to make predictions concept class description characterization and discrimination that I can be associated with classes or concepts it can be useful to describe individual classes and concepts in summarized concise and yet precise terms such descriptions of a class or concept are called class concept descriptions these descriptions can be derived via data characterization that is by summarizing their data of the class and a study often called the target class in general terms or data discrimination by comparison of the target class with one or a set of comparative classes often called the contrasting classes or both data characterization and discrimination data characterization is a summarization of the general characteristics or features of a target class of data the data corresponding to the user specific class are typically collected by a query there are several methods for effective data summarization and characterization for instance the data cube based oil AP role of operation can be used to perform user control data summarization along a specified dimension an attribute oriented induction technique can be used to perform data generalization and characterization without step-by-step user interaction the output of data characterization can be presented in various forms example include pie charts bar charts curves multi-dimensional data cubes and multi-dimensional tables including cross tabs the resulting descriptions can also be presented as generalized relations or enroll form called characteristic rules data discrimination is a comparison of the general features of the target class data objects against the general features of objects from one or multiple contrasting classes the target and contrasting classes can be specified by a user and the corresponding data objects can be retrieved to database queries the methods used for data discrimination are similar to those used for a characterization our discrimination descriptions output the forms of output presentation are similar to those for characteristic descriptions although discrimination descriptions should include comparative measures that help to distinguish between target and contrasting classes discrimination descriptions expressed in the form of rules are referred to as discriminant rules mining frequent patterns associations and correlations frequent patterns as the name suggests are patterns that occur frequently in data there are many kinds of frequent patterns including frequent itemsets frequent sub sequences also known as sequential patterns and frequent substructures a frequent items had difficulty refers to a set of items that often appear together in a transactional data set for example milk and bread which are frequently bought together in grocery stores by many customers a frequently occurring subsequence such as the pattern that customers tend to purchase first a laptop followed by a digital camera and then a memory card is a frequent sequential pattern a substructure can refer to a different structural forms such as graphs fees or lattices that may be combined with item sets or sub sequences if a substructure occurs frequently it is called a frequent structure pattern mining frequent pattern leads to the discovery of interesting associations and correlations within data classification and regression for predictive analysis classification is the process of finding a model or function that describes and distinguishes data classes are concepts the models are derived based on the analysis of a set of training data that is data objects for which the class labels are known the model is used to predict the class label of objects or which the class label is unknown a decision tree is a flowchart Lag tree structure where each node denotes a test on an attribute value each branch represents an outcome of the test and tree leaves represent classes or class distributions decision trees can easily be converted to classification rules a noodle network when used for classification is typically a collection of neuron like processing units with weighted connections between the units regression is used to predict missing or unavailable numerical data values rather than discrete class labels the term prediction refers to both numeric prediction and class label prediction regression analysis is a statistical methodology that is most often used for numeric prediction although other methods exist as well on regression also encompasses the identification of distribution trends based on the available data classification and regression may need to be preceded by relevance analysis which attempts to identify attributes that are significantly relevant to the classification and regression process cluster analysis unlike classification and regression which analyze class labelled training data sets blustering analyzes data objects without consulting class labels in many cases class label data may simply not exist at the beginning the string can be used to generate class labels for a group of data the objects of clustered are grouped based on the principle of maximizing the intra class similarity and minimizing the inter class similarity that is clusters of objects are formed so that objects within a cluster have high similarity in comparison to one another but are rather dissimilar to objects in other clusters each cluster so formed can be viewed as a class of objects from which rules can be derived clustering can also facilitate taxonomy formation that is the organization of observations into a hierarchy of classes that group similar events together outlier analysis a data set may contain objects that do not comply with the general behavior or model of the data these data objects are outliers many data mining methods discard outliers as noise or exceptions however in some applications such as fraud detection the rare events can be more interesting than the more regularly occurring ones the analysis of outlier data is referred to as outlier analysis or anomaly mining outliers may be detected using statistical tests that assume a distribution or probability model for the data or using distance measures where objects that are remote from any other cluster are considered outliers rather than using statistical or distance measures density based methods may identify outliers in a local region although they look normal from a global statistical distribution view description of data mining query language with few example queries data mining query language the importance of the design of a good data mining query language can also be seen from observing the history of relational database systems relational database systems have dominated the database market for decades the standardization of relational query languages which occurred at the early stages of relational database development is widely credited for the success of the relational database field although each commercial relational database system has its own graphical user interface the underlying core of each interface is a standard relational query language the standardization of relational query languages provided a foundation on which relational systems for developed and evolved it facilitated information exchange and Technology Transfer and promoted commercialization and wide acceptance of relational database technology the recent standardization activities in database systems such as work relating to SQL 3 and so on further illustrate the importance of having a standard database language for success in the development and commercialization of database systems hence having a good query language for data mining may help standardize the development of platforms or data mining systems design our comprehensive data mining language is challenging because data mining covers a wide spectrum of tasks from data characterization to mining Association rules data classification and evolution analysis each task has different requirements the design of an effective data mining query language requires a deep understanding of the power limitation and underlying mechanisms of the various kinds of data mining tasks primitives for defining a data mining task in the form of data mining query the set of tasks relevant data to be mined the kind of knowledge to be mined the background to be mined the background knowledge to be used in the discovery process the interestingness measures and thresholds for pattern evolution the expected representation of visualizing the discovered patterns based on these primitives we design a query language for data mining called DM QL data mining query language TM QL allows ad hoc mining of several kinds of knowledge from relational databases and data warehouses at multiple levels of abstraction the language adopts an sql-like syntax so that it can easily be integrated with the relational query language SQL the syntax of DM QL is defined in an extended BNF grammar where square brackets represent zero or one occurrence curly brace represents zero or more occurrences and words in sans-serif font represent keywords syntax for tasks relevant data specifications the first step in defining a data mining task is a specification of the task relevant data that is a data on which mining is to be performed this involves specifying the database and tables or data warehouse containing the relevant data conditions for selecting the relevant data the relevant attributes are dimensions for exploration and instructions regarding the order or grouping of the data we tried d MQL provides clauses for the specification of such information as follows use database database name or use the data warehouse data warehouse name the use cloth directs the mining task the database or data warehouse specified from relational cube where condition the from and where classes respectively specify the database tables or data cubes involved and the conditions defining the data to be retrieved d MQL double : equal to d mq l statement d mq l statement that we'll call an equals d remaining statement or concept hierarchy dimension statement or visualization and presentation a series of data mining statements are shown in the subsequent slides in relevance to attribute or dimension list this Clause lists the attribute or dimensions for exploration order by order list the order by clause specifies the sorting order of the task relevant data group by grouping list the group by a clause specifies criteria for grouping the data having condition the having Clause specifies the condition by which groups of data are considered relevant you example this example shows how to use D MQL to specify a task relevant data the mining of associations between items frequently purchased at a B company by Sri Lankan customers with respect to customer income and age in addition the user specifies that the data are to be grouped by date the data are retrieved from a relational database the example can be seen on screen characterisation mine knowledge specification double colon equals mine characteristics as pattern name analyzed measures this specifies that characteristic descriptions are to be mined the analyzed clause when used for characterization specifies aggregate measure such as count sum or count percentage the percentage of cupids in the relevant data set with the specified characteristics these measures are to be computed for static characteristic found syntax for concept hierarchy specification unsub hierarchies allow the mining of knowledge at multiple levels of abstraction in order to accommodate the different viewpoints of users with regard to the data there may be more than one concept hierarchy per attribute or dimension for instance some users may prefer to organize branch locations by provinces and states while others may prefer to organize them according to languages used in such cases our user can indicate which concept hierarchy is abused which statement use hierarchy name or attribute dimension otherwise a default hierarchy per attribute or dimension is used syntax for interestingness measure specification the user can help control the number of uninteresting patterns returned by the data mining system by specifying measures of pattern interestingness and their corresponding thresholds interestingness measures and thresholds can be specified by the user with a statement with interest measure name or threshold threshold value syntax for pattern presentation and visualization specification how can user specify the forms of presentation and visualization to be used in displaying the discovered patterns our data mining query language needs syntax that allows users to specify the display of discovered patterns in one or more forms including rules tables cross tabs pie or bar charts decision trees cubes curves or surfaces we define the DM ql displaced statement for this purpose as shown in the subsequent slides an example of a DM ql query suppose as a marketing manager of a b company you would like to characterize the buying habits of the customers who purchased items priced at no less than rupees hundred with respect to the customer's age the type of item purchased and the place in which the item was made for each characteristic discovered you would like to know the percentage of customers having that characteristic in particular you're only interested in purchases made in Sri Lanka and paid for with Visa credit card you'd like to view the resulting descriptions in the form of a table the data mining query is expressed in DM QL as shown in the screen conclusion in this chapter we have covered the following in detail data pre-processing including data cleaning data integration data transformation discretization and concept hierarchy generation definition and specification of a generic data mining task description of data mining query language with few example queries "
xBSqrkxiuKk,22,,2020-04-07T11:36:30Z,"Data Mining Stages, Applications and Technologies | Data Mining and Warehousing | KTU",https://i.ytimg.com/vi/xBSqrkxiuKk/hqdefault.jpg,Shreya's Tutorials,PT7M33S,false,968,26,0,0,0,hi guys welcome back to my channel if you are new to this channel please do subscribe to my channel and it developed and for all the notifications so here we are going to discuss about data mining data mining and warehousing is a subject in semester 8 computer BTech students so this is the first module of the subject we are going to discuss about data mining the stages of data mining data mining applications and technologies for data mining first of all let's see what is data mining in school days we have learned mining of minerals right from the Earth's yeah that's what meant by data mining here we are extracting the relevant information from a large amount of data here in the case of minerals we are mining minerals right the old is the large amount of data and the minerals is a mind data that's what is example practical examples so here the large amount of data can be the database data warehouse the web other information repositories next one is data mining stages the atomic stages are data cleaning data integration data selection data transformation data mining pattern evaluation nourish presentation so let's see what is data cleaning removing noise and inconsistent data we know cleaning means what cleaning means removal of unnecessary things yeah that's what meant by without attaining here here we are removing noise and inconsistent data in a software image in a data what is there unnecessary thing no is right there so next one is data integration we know the normal English meaning of integration integration is combining so that's here combining multiple data sources next one is data selection from the data base we are retrieving relevant data we are selecting so that is men might add a selection now data transformation the relevant data and transform that consolidated how my performing summary and congressional operations by performing the submarine aggression operation we transform the deterrent relevant data like this data transformation next one is data mining I explain it in simple terms right so here there is a little technical definition essential process where intelligent methods are applied to extract data data mining is an important process and here intelligent methods are applied to extract the data path evaluation we extract from the extracted data we identify the truly interesting pattern and Noel's presentation knowledge presentation is the last step what is that display or presenting our mind knowledge she uses how my applying visualization and knowledge representation techniques so these are the stages of data mining now let's see the applications of it data mining is applicable in bioinformatics that mining business data analytics business intelligence market basket analysis classification estimation and prediction so I have given this the definition of prediction estimation and classification so let's see classification we know what is classification classification is defective Esha so in nearly example in a loan database we classify an applicant as a prospect II or defaulter an estimation estimation means where we estimate predicting the attribute of a data instance example estimating the percentage of mark of a student whose previous marks are known from the previous marks we estimate the percentage of marks of the student let's estimation prediction prediction is predicting future robco outcome rather than the current behavior we always predict current thing right but here we are predicting the future outcome printing example predicting the next week's closing price for Google shell price per unit next one is the technologies for data mining let's see the technologies autistics machine learning pattern recognition database systems visualization data warehouse algorithms information retrieval applications high-performance computing are the technologies of data mining can you see the arrows are all not going to data mining so let's see what statistics statistics studies the collection analysis interpretation or explanation and presentation of data data has an inherent connection statistics this for for data mining the statistics really important for data mining this div' really used statistic model I this were really important to classify the data a statistical model is a set of mathematical functions that describe the behavior of the objects in a target class if there's a target class by with the help of statistical model we can describe the behavior of object and in terms of random variables and their associated probability distributions statistical models are widely used to model data and data classes this models are really used to model data classes and next one if machine learning we have learnt machine learning and some Australians might have learned machine learning new semester seven it's an elective subject and machine learning is it is an application of artificial intelligence we are making the machine thing like we human beings really the machine do not have the capability to think what we program they give the output right so here we are not explicitly programming we get experience to it and from the experience it gives the output that is what meant by machine learning so machine learning see the first point investigates how computer can learn based on data application of artificial intelligence third point it gives the system the ability to automatically learn and improve from experience without being explicitly programmed you have without explicitly programming the computer it gives the output from the experience there are three types of it supervised learning unsupervised learning and semi-supervised learning supervised learning is a machine learning technique here we here we make the machine learn using supervision supervisor will be there for this for machine for in this supervised learning and unsupervised learning we do not give supervision without supervision we make the Machine learn and if the semi-supervised learning it is the combination of both supervised learning and unsupervised learning so this is what is machine learning so this was a little explanation of data mining I have said about that remaining the application soft with the stages of it and the technologies of that domani so come up with the next video thanks for watching bye bye 
45StT34-6LY,27,"This short course will help you to understand some data mining techniques for knowledge discovery and knowledge presentation. At the end of the short course you should be able to use the skills for knowledge discovery and future prediction from a suitable dataset of your interest.

This course is coordinated by Associate Professor Zahid Islam and Dr Michael Bewong. This second webinar is presented by Associate Professor Islam - you can find more videos here - https://www.youtube.com/channel/UCrcz90CHdK6GrtnMvk_vVXA/videos?view_as=subscriber 

This short course is also a taster of the Graduate Certificate in Applied Data Science course at CSU - https://study.csu.edu.au/courses/technology-computing-maths/graduate-certificate-applied-data-science",2019-11-13T04:16:10Z,Free Short Course: Knowledge Discovery and Data Mining - Webinar 2,https://i.ytimg.com/vi/45StT34-6LY/hqdefault.jpg,ITMastersCSU,PT1H15M45S,false,643,10,0,0,0,hello everyone and welcome to knowledge discovery and data mining second webinar of a short course presented by Charles State University my name is guy coward and I'll be your emcee your mentor yes I hear Islam before I begin as usual the housekeeping we encourage questions and the use of chat during the webinar and we ask that you direct all questions relevant to course content to the Q&A section particularly and that you send all administration's have questions to the support team in chat so we can sort of questions about dates or quizzes or resource availability for example you can also just chat with your fellow students and you can choose to do that by setting your chat settings to all panels and attendees using the Dropbox once you have opened the chat log there are almost always very experienced industry-based attending so it'll be most helpful with any queries you have and will be able to augment the content that we deliver tonight on that content we'll have a few videos tonight and a lot of Q&A if it comes up and if a question is particularly relevant I'll into it interrupts I'm in the middle of a slider life is as ever here in an administrative and technical support role for IT masters he's responsible for the course page that learned I think artists edu which is where you'll find all materials and webinar recordings if you have any questions about the course page itself please feel free to contact us using the details on that page no guest lecturer tonight we have a full hour of full salvo narrating out with Syed which i think is very nice can you please welcome make him comfortable thank you very much thanks for again a nice introduction guy and I'm just double-checking you can hear me my voice coming through yep loud and clear sounds good out and clear okay and thank thank you very much to all participants for coming for the second lecture and I hope you will be enjoying this and today we will talk about knowledge discovery a little bit more use of decision trees and forests how we can use decision trees and forest for knowledge discovery and we will also see a couple of applications of data mining some short videos etc and look in this in this subject we actually teach also some background information on how decision trees are built and etcetera at least some basic idea basic concept of the theoretical side but in this lecture I will try to avoid this because we want to give an overview and a good understanding of the some subject met materials etc okay so I don't think I need to introduce myself a lot again but you you know that I have a colleague who will be giving the lectures from next week so next week and the following week and he is dr. Michael beiong I introduced him last time he's a wonderful data miner and he will be discussing knowledge discovery decision foresters not Assad whatever what is that decision support system and he will introduce those topics now as you know my name is Shahid and we also have a data science research unit here at Charles Sturt University and that data science Research Unit our colleagues are working in various research areas and that data science research unit has five different groups advanced networking research lab data mining research area imaging and sensing research group cybersecurity research group and health research group so our colleagues are working on various research topics and we are actually part of I think around 150 million dollar cybersecurity CRC where there are about seven or eight universities collaborating each other with many industry partners and with government funding and our university is is also part of food as ility crc with a similar amount of funding for ten years and then also soil CSC so what I want to say is that this this group of researchers are actually actively working on various research topics various research projects and that generally helps them to convert their experience and knowledge into teaching as well so it is a very very nice achievement that we have nice data science Research Unit and in terms of computer science and information technology research wise we are ranked Atwal standard by Australian Research Council AARC and in some parts some subdivisions within the computer science and information technology we are ranked above all standard so overall we are at well standard but in some of them we are above world standard so is it's a very active research group and so the information for example personally I work on along with my other colleagues of course I work on various industry projects and that knowledge and experience is is I find it useful in teaching as well okay now the next slide is a small introduction about our school here school of computer science and School of Computing and mathematics we call it and according to University reviews here is the web link we were ranked third we were ranked number three in terms of best computers universities in Australia as a computer science school we were ranked number three and that ranking was actually based on number of students student satisfaction the grad salary grad our students and that these three factors contributed towards pointing system scoring system and we are in in that sense we are ranked number three so it is look this ranking is not very important but what we are trying to understand from these that possibly we have good number of students and our students are reasonably happy with us we have so we take it very seriously and we add just our teaching about course materials etcetera accordingly we try to understand the student needs and also to in order to provide better education better service etc and also one information I have that is in 2017 in terms of the headcount enrollment our school had six thousand twenty-five or so enrollments which is the highest in Australia so in terms of number of enrollments headcount enrollments okay so we have in that sense we had the largest IT education provider in Australia so now this this course the graduate certificate in a flight data science this course has many subjects one core subject and many optional subjects elective subjects so the course met the subject materials that I am delivering through these lectures is only from one of those subjects ITC 573 data and knowledge engineering but this is an elective subject so you can choose this subject or you may not choose this subject if you are wanting to study that graduate certificate in Applied data science then it's up to you whether you choose this subject or not but the way we design the subjects - is aiming to you know quatre for different groups of students and their needs so that some students may want to learn more statistics and statistical techniques etc so there are statistical subjects there as well instead statistic scientific statistics SDF 5:01 some students may need to learn a little bit more about database database structures schema design ard entity relationship diagrams and those kind of things so there there are various various different options for this subject for this course okay now coming to our lecture today and there's a brief introduction to the school to our data science research UDN and the course now coming to our lecture today last week if you remember we were talking about the data lifecycle and we introduced what is data collection very briefly of course and then we also briefly introduced how to do data pre-processing remember we were showing missing values and how to impute missing values and we also discussed the corrupt data how data can get corrupt and etc and there are techniques for automatic detection of Corrib data as well and also we discussed briefly discretization you know scaling etcetera so that these are pre-processing and then we discussed briefly about data analysis how to now analyse the data set data collected data and we were showing you very briefly a decision tree we used Weka Luke Vega is just one software tool you can use any other software tools for data analysis commonly used software tools Weka is one of the most commonly used software tool for research and education purpose you know because there are lots of freely available software packages or software algorithms programs there in Weka already so generally researchers use this but there are many other more commercial version of programming or data mining tools other other commonly used tools could be orange is another one where you can actually use various data mining tasks you can apply various data mining tasks there so that sort of thing so we are using Weka here because it's easy easy to demonstrate things and it's good to understand how data mining works in real life and we have some algorithms there as well so today I will show you a little bit more about what is the decision tree and what is a decision forest remember last week we discussed that there is a thing called decision forest which is ensemble of trees you can build more than one trees and how those things can help you to discover knowledge so that knowledge discovery thing and also a little bit of idea about how this knowledge can be conveyed published or kind of shared with with non data science people as well so we will discuss those things and and and last week we also briefly discussed about some industry projects just to see as an example of the possible use of data mining and knowledge discovery in various real-life projects so look this is not a complete list it's just an just an idea one of my student who was studying one of these subjects he told me that he actually works in a high school as a high school teacher and with this subject now he can now actually analyze his students and they're there and the reasons why some students are performing well and some students are not because he has information about lots of lots of students and many of them were performing well many of them somehow were struggling so now he can actually dig out more information more knowledge from those type of data so this kind of thing you can do in your in your work place as well if you are not already using data mining after this course after this subject a couple of lectures possibly now you know how to do this and he will little bit more but I understand many of you are quite experienced in data mining and you're using it already so like these industry projects are like health monitoring system that we developed for patient welfare and better hospital management the quality irrigation project was for irrigation water demand prediction etc and so on and so forth so we discussed those projects last week now today I will actually show you a quick video on another possible application of data mining and this was we published a paper on that in Ostia Australasian data mining conference in 2016 where we actually developed a decision support system that collects information from various sources like Bureau of Meteorology New South Wales water and weather zone and then it combines the information processes the information and displays the information in a decision support system for better understanding of the data and a user for example a water manager or a damn manager these particular paper was using dam management as an example what a dam management as an example and a manager can actually try to understand the weather condition the water flow etc and also then he can build models like the decision trees and decision forests that used little saw a little bit last week and though then use those forests and decision trees in order to predict future so he can predict and he can actually choose various target attributes like damn what a level or dam in flow level or etc and dam outflow and many other things so he can choose target variables as he needs and then he can actually develop models and he can predict them in advance okay so these this sort of thing and here is a small video for that and I hope this is only one and a half minute long video hi my name is Marco I'll be walking you through CSU's new water dmz this is only one and a half minute long video and I hope this will briefly explain what is a decision support system and how data mining can be used in that in in a decision support system okay let us run it hi my name's Michael I'll be walking you through CSU's new water diem system web-based decision support system powered by data mining what is water DM what RDM allows everyday users to experience the power of data mining by selecting some simple parameters and clicking a button the user can then explore the data mining model using a powerful new visualization system that provides statistics and useful information on the fly for time series data the user can graph data points over time and compare variables as well as check the accuracy of a particular model over the data collection period the system can be used to predict the numerical or categorical class values which is displayed on the prediction page the system shown here has been used to predict a variety of variables relating to burundanga dam in the central west of New South Wales every day the system fetches weather predictions from the Bureau of Meteorology weather zone and pass data from the New South Wales Water real time data site the flexibility of the system allows us to customize it to use any data set the user require we can also add any additional functionality due to our systems modular design if you're interested in more information email dr. Sahib Islam at Zed Islam at CSU ddau thanks for watching okay so that that was an example how data mining data analytics can be used for water management if you are interested you can have a quick look at that paper that I showed in the previous slide and you know how to find them on my web page right so you can Google search me go to my web page and go to the publication page and search for that paper you can control F and such I showed how to do it last week so I'm not I think there's nobody needs that again okay another example of use of data mining could be smart forms so data mining on smartphones so these smartphones today that we use they are very powerful and for example most of the smart phones are not the high-end mostly smartphones like reasonable priced medium and lower end smartphones and can have eight core processor four gig ram etc so they are quite powerful so we were thinking that whether we could do data mining complete data mining on smartphones and at that time we did not see any smartphone data mining app like you are using way cahier on desktop machines or laptop machines PCs for data mining and you can actually choose various algorithms for classification for clustering for data pre-processing etcetera but for smartphones we did not see any app like that so we developed an app called data learner that that is actually that mimics actually Weka but for Android phones and so that and then we also studied the impact of that how the speed of the forms compared to PCs and then also temperature user experience in terms of complexity of the of the of running the data mining algorithms accuracy of course and all these things and we found it actually very encouraging it's doable and the app called data learner is freely available is is freely available for downloading from Google Play just search for data learner and also the source code is also freely available from github here is the link for that so there we have implemented around 40 different algorithms there are in data learner so you can choose an algorithm you you can upload your data set and on in the in data learner app and then you can actually choose an algorithm and just run it to discover knowledge to get the models etc so that is that is doable and I'm now showing you as small again less than 1.5 minute video to introduce their data learner very briefly but again we have a paper on that you can search for that paper on my web page and the preprint of the paper is freely downloadable you can read that okay let's see here is the video [Music] okay so this video introduces that app and you can actually download that app from google play and play play with that on your on your smartphone now you may be thinking like why we need to do data mining on smartphone when we have powerful desktops and laptops why and how much we can actually do on smartphone so there are quite quite a few advantages possibly yes there are disadvantages as well it will not be as convenient as a laptop as a small screen you can do limited you know like applications they are for example you can't open the access maybe their microsoft access for making more queries or you cannot use Excel that sort of thing but the advantages are also there for example it gives you more flexibility you can do your data mining at some time when you are not on your desk at your desk and you are not using your laptop on top of that smart these smartphones this data mining on smartphone actually does not require your data set to be sent to the central server you can actually do the data mining on the phone and your data set is not needing to be you don't require to release your data set to somebody else you know that sort of thing and an obvious application of that is if you are collecting data related to you using your phone and then want to mind that and the following slide will possibly show you an example of that an application of smartphone data mining we published this paper recently in possibly ostium 2019 this year that conference is going to happen in Adelaide from I guess 2nd of December to 6th of December and in that paper we showed the readiness of smartphones for data collection and data mining with an example Apple in mental health so what it does so these smartphones they actually have lots of lots of sensors many sensors quite a few sensors for example they can they can sense the the light the how bright or how dark it is and depending on that actually it deems down or it brightens up you know your screen so it can send slide so if someone is living in dark for a long time it can actually sense what what is the light if it isn't very bright or not it can also it has accelerometer so it can also sense movement so if someone is moving or not if he is carrying the phone and sitting somewhere it can sense that it can also sense proximity for example when you bring your phone close to your ear then all and possibly when you are talking although you are touching the screen it the screen is inactive at that time it does not change the app so it doesn't drop the connection so it has the proximity things and it can also store information about say your web browsing or your you know your messaging how much how many how much message you are sending or how long you are talking how many calls has cetera so it has lot of sensors so what we did in this paper we use the sensors to collect information about an individual the individual who is using the phone and then add after that okay these are the sensors that we used accelerometer screen activity proximity phone call log SMS log as such a light sensor which I said explained before etc and then we collect we said that we created a app we developed an app called mindful and that can actually collect information using the sensors about an user and then it can create a data set every day it collects it keeps on collecting the information and then gradually it the data set and also every day we ask a question so this is this is how the interface of that app looks like so every day that a user will need to answer these phq-9 questions which can assess his or her mental health condition you know feeling depressed or not that sort of thing so what we are essentially doing we are collecting all different sort of user activities movement and all these things and we are also collecting information about these phq-9 questions to to determine his or her mental health condition and then we relate the other activities with mental health condition and after we collect this data for a few days maybe a month two months three months we have a reasonable-sized data set that can actually then the phone the smartphone app will also have a data mining capability that our algorithms are installed there so then it mines the data set and understands the relation between different user activities with their mental health condition and then it tries to predict in future so after a couple of months even if you you or the user does not that's the idea even if a user does not does not record record or does not answer these survey questions phq-9 survey questions the app can still identify their mental health condition because it has launched that relationship from the previous collected data their user activities and mental health condition so in future just watching the user activities they can predict mental mental health condition of that particular user now mental health condition can vary in terms of the relationship between user activities and mental health condition can vary for example for can vary from person to person sorry for example if if if a doctor works what's in a hospital at night then at night he is not sleeping he is moving around or a security officer working at night so at night he is not sleeping he is moving around he is what he is he turned on his light he's in a in a light area in lighted area so those kind of thing is not does not reflect his or her mental health condition being poor whereas for many other people who are supposed to sleep at night they are not sleeping at night may mean their mental health condition is not so good they are struggling so it it is the the relationship between various activities and mental health condition can actually vary from person to person so therefore these individual because it is individualized for an individual user and it only learns from that user data can be very applicable for that user only so we then tested this with a synthetic data set and we found that it's actually it can be very useful look the reason we are not releasing this app because we that that requires serious ethical ethical ethical ethics approval and the data set we used is not real life is a synthetic data set so we cannot guarantee that this will be useful or this can really identify mental health condition but we find it really very interesting to see that mobiles form smartphones are ready technology wise they are ready for doing these things they can be used things for things like this so that's another example of data mining application in real life right ok so after that now we are deep into today's lecture maybe we are now showing how decision trees and decision forests can be used for knowledge discovery ok so in this case last week I was showing you a very small toy example that I said but in this case I will be showing you a real-life data set called diabetes data set and using that diabetes data set I demonstrate how you can discover some knowledge what generally people in real life use how they work in order to discover knowledge we did couple of a few industry projects on knowledge discovery to discover information to discover knowledge and report that so we use similar sort of approaches there as well in those real-life projects so I will demonstrate that now and I possibly have a video here again and this is a very recent video I just I was basically doing it for this lecture purpose and I have uploaded this video on YouTube as well I will share this I will show this video now run this video now it's a little longer than the previous two videos it's 15 minutes but the reason I am showing this through the video is because then it will be very systematic because this this was captured carefully and then I will show you some some screen sharing from here to show you a little bit more about this so this video is basically introducing how you can use decision trees and forests for some knowledge discovery let's enjoy welcome everyone to hits data mining channel once again my name is ahead Islam and today I will be discussing knowledge discovery and data mining and we will also talk about how we can actually discover knowledge and I will demonstrate a real knowledge discovery from a real data set okay now we published this paper called 4x plus plus a new framework for knowledge discovery and decision forests and in this paper we discussed some initial ideas of how to discover knowledge from a data set what is knowledge discovery and how to also present the knowledge in an interesting way to some extent in this paper and in a following paper we then took this to further details or further improvement I will discuss that also very briefly so to demonstrate this knowledge discovery in this video I have just downloaded a freely available data set called diabetes dot rff data set diabetes data set it not err FF is the format for wake up as you all know and you can see these things he can learn these things a little bit more from my other videos if you like so in this data set there are about 9 attributes 9 variables and one of them is a class attribute which is diabetics test result whether it is tested positive or tested negative so this is the data set you can see and these are the variables like preg means pregnancy how many times the patient was pregnant then plus press keen ensue insulation related information etc age and then finally class now I have opened these data set in Weka this is you can see where K Explorer and here you can after through this open file you can open any data set here I have diabetes Tata air ffs and we can open that data set and now you can see various variables now for knowledge discovery this information can also be useful that you can actually see the distribution of different variables how many people what is the minimum what is the maximum etc and if you go to class you can see there are 500 negative tested negative people and 268 tested positive people in this data set but often people will build decision trees and forests to discover knowledge as well so here we can choose a decision tree algorithm say for example in this case I am choosing j48 and we can actually build a decision tree and this is how the tree will look like that will give us lots of information I can choose some tree parameter to make the tree a little bit more sensible for example I am saying choose at least 50 records in a leaf so that my records are more like sensible and then from that I can discover some knowledge people will also use decision forests instead of a tree where they can have multiple trees to discover more knowledge ok so let me show you an example and then I will demonstration for si I'm using sis 4 there's our ohms algorithm that we built that we proposed few years ago and then we can possibly choose minimum number of records etc information so that we can build some sensible trees so here we can choose some parameters forces for say for example we are choosing 50 records in each leaf so that every rule that it discovers contains at least 50 patients in it so that the rules are more kind of reliable is not based on two patients and it that can be a pure coincidence so 50 patients there and the number of trees you can choose a suitable number of trees the more trees you choose generally you are trying to discover more knowledge so 50 by default it creates 60 trees so now when you build it forests like that then you can see there are 60 trees tree number 60 there but the problem it it can be huge you know that's one problem people are suffering from and that is huge number of trees and lots of lots of rules or lots of knowledge and then it's becomes difficult to discover which knowledge which rule sets are more important more useful for example a little bit of interpret interpretation of this is it says that if place value is less than one hundred and sixty six point five and then mass value is less than twenty six point three then these patients are tested positive their diabetes from these data set and there are 159 such records that have plus less than hundred and sixty six point five and mass less than twenty six point three and only nine of them are negative are positive so 150 of them are negative so it's a predominantly negative people in terms of diabetes test now thus the paper that I was discussing actually presents some interesting idea that how do you now discover more interesting knowledge so it says that you first find out the rule sets that have accuracy above the average rule sets accuracy and then that has coverage also above the average coverage and the lengths are also short that lengths are smaller than smaller than the average length of a room so smaller rules are more understandable interpretable something like that so to understand the the proposal in the paper let us build a small tree small forest of three three trees number of trees will be three and then so that we can demonstrate this and now we can see that these are the trees 3 1 3 2 3 3 so following the suggestion in that paper we first write the rules on an excel file as follows so here is the excel file you can see and there's three one rule one so the first rule is plus less than 160 6.5 and mass is less than 26.3 you can see here and then we are saying it is negative the there are 159 total records only nine of them are positive so hundred 50 of them are negative and then we can calculate the support support means how many records fall in this group so there's 159 you can you know and confidence is what is the accuracy level confidence level so it's 150 or 159 159 which is ninety four point three so this way we can write down all the rules okay so that's the reason why from 60 trees for demonstration purposes I just I said that we better choose three trees so we can write all these rules now in real life you will have lot more and then you know the support and confidence so as a result of these you can actually then rank the rules in terms of their confidence and support or whatever so here now I can see that true tree to rule number seven that gives the highest confidence so that's the 94% now if you are interested in only positive rules so you are interested in those people who have diabetes then you can see this rule which says if PLAs is greater than one hundred sixty six point five gives the highest confidence for the rules that identify or that predict positive diabetes positive patients so here 86 percent accuracy now this is how you can actually discover and you can play with interesting rules and you can find out rules so you can possibly find out top ten rules in terms of support in terms of confidence or in terms of lift and you can choose many other criteria now you can also after you identify some interesting rules you can also explore surrounding those rules to see what are the information there so to explore the other side of the rules or when you identify some rules you can then decision trees and forests can help you to identify interesting rules like this and this you could find without to any pre-pre assumption any domain expertise in in that in that sense so this knowledge this these rules are coming out automatically from decision forests and then you can identify that maybe this is one interesting rules because it says plus greater than hundred sixty six point five means positive now what is if it is less than hundred sixty six point five what is not hundred sixty six hundred fifty so can I discover knowledge and then for doing that here I'm I have opened the file the data set in Microsoft Access and let us write some query to see what is the last distribution when the class when the place value is actually not greater than rather less than hundred sixty six point five so I'm writing some basic SQL query here it is dot plus is actually less than equal to hundred sixty six point five instead of greater than because that rule says greater than so I'm now checking if this is less than what happens dot class it was just it negative so I'm trying to see like how many records how many records are actually tested negative an how many records are tested positive when place value is less than hundred sixty six point five where as this rule says place value is greater than hundred sixty six point five so once I now complete this this query and run this query I can see that there are four hundred eighty nine records with negative class values and similarly I can run this query again to see how many of them are positive positive and then I can see there are only two hundred positives so from these I can see a class distribution when it's actually different and the glass value is different and then from that I can write an excel file like this with the new class distribution so when case one then this is the percentage of positive and negative case 1 means this rule and case 2 is this other query and I can see the distribution difference so when plus is greater than hundred sixty six point five then there is a huge number of positive compared to negative whereas when class is less than hundred sixty six point five then then there is a very big number of negative compared to positive so I can actually discover this sort of knowledge from this so this is and this is a simple demonstration you need to do it more and more to actually learn more about the data so data set data mining and knowledge discovery is more mostly about making sense of data so you can actually keep exploring these things and learn more and more knowledge so as a demonstration of knowledge discovery we in this paper 4x + + paper we actually demonstrated and applied this approach on some real-life data sets for example this was on a osa oasis data set which is on dementia patients and we discovered that this these are very interesting findings we discovered that when education level is low say in this case less than equal to 8 we can see that the number of demented patients is very high compared to number of non demented patients at least in this data set but the for the same data set when education level is high big so the patients had long years of education then amongst those patients the number of non demented patients is not higher than the number of demented patients so education has influence on dementia it looks like at least from this data set so this is interesting and similarly we can also see SES socioeconomic standard or background also has influence on dementia at least according to these data set and here we can see for similar in condition with social better socioeconomic standard generally people have less percentage of dementia so this sort of thing can be actually discovered and this knowledge discovery is an ongoing process and many people say it's data science or data arts because you need to be creative you need to do those kind of queries those kind of analysis decision trees and forests can help you to build it but then you need to continue exploration to find more knowledge dig out more information and knowledge from your data set we then presented another paper on knowledge discovery and we actually in this paper identify presented a full framework for knowledge discovery and we identified a new definition of interesting rules and this framework you can apply to discover more more interesting rules and knowledge so feel free to read this paper and you can find these papers from my webpage as usual you can search me on my on Google and you will find my page go to the publication page and just search for the paper so here is the 4x plus plus paper and you can also search my other papers thanks for watching ok so that's the end of the video now I I can promise you this this was the last video no more video now you can see me live and very interesting and we've got a really fantastic couple of questions during it kemon asks about the difference between correlation and causation and how you correct for that hmm so that's a good question Kevin my understanding about that is correlation does not always mean causation correlation is simply if we have two attributes two variables are they changing similarly if one decreases does the other one also decrease or if one increases does the other one also increase or decrease but it's always the case so their relationship is very defined so one increase other other one either increases or decreases all the time for example age and height right so in in high schools for students in high schools the age increases the height also increases so there is a strong positive correlation there but among the entire population of Australia the correlation between age and height may not be as strong so that's correlation but correlation always does not mean there is a causation so causation is possibly one causes the other ones happening okay so the correlation can be can be happening just just because of other things there are many other things for example I can think of I'm trying to think of a good example say the city size and the number of cars in a city possibly they are correlated the city size increases the number of cars also increases but it is not the number of car is not caused by the size of the city it's caused by the population in the city so when the city size is big that means generally the population of the city is also big and that is causing the number of cars increasing in the city but when you see look at correlation between city size and number of cars in that city it looks like they are strongly correlated but one is not causing the other one does it answer the question it's a very good explanation of the difference but with respect to that particular study and the relationship between for example education and dementia to what extent is that causation or correlation that's correlation that's that that that is that is not a causation possibly it's difficult to say whether it is a causation without strong evidence but we can see that there is a relationship there is something happening so data mining generally explores this type of information and as I showed you then you can make some more queries to actually dig out more information but then it is for the domain experts to actually think about this like the doctors clinicians nurses in that exam or you know psychologist you know social science people to actually then think whether there is a causation or not whether really studying more makes brain act differently to and more more resilient to to dementia or not you know or more resistant to dementia not but simply just doing these data mining we possibly can't conclude that it requires further study but with this data mining with this knowledge it can promote it can support that further study fantastic thank you so then who said in the chat we'd better sign up to the full curve full course so we can avoid the manager yeah you might be inoculated but probably still best to just sign up just in case they sleep sore questions so we might just keep going and then have a nice long extender okay thank you okay so that was the last video as I as I promised now next few slides are actually explaining exactly what I showed you in the video so I don't need to give the lecture now you see like or at least for this part so this was the excel file and exploring few things and doing some microsoft access queries SQL queries this was the paper now after that we will talk about so that that was the video now it's a live one so in this subject we also teach you or we also teach the students about the theoretical background theoretical justification or theoretical knowledge on the various data mining techniques it's not just we show tools and we just demonstrate that it can be done but we also try to give the basic understanding why it can be done this way what really happens in the back end because I have a feeling that you will possibly agree that when you know exactly how a tool or a technique works then you will have better control on it right so you just don't know how to use it but you know the inner mechanism of this tools and technique and that way you can actually apply it more sensibly in different situations I can share with you some vague examples because if I give the detail example it will take long that there are many data miners or data analysts we're using data mining techniques in in a very incorrect way in non sensible way so possibly I don't know it's just a human mistake human error but possibly better understanding of why things happen according and how things happen or what is the inner mechanism could actually give better understanding so this will be taught as well but in this short course I will not go into deep details for example here I'm just showing couple of slides on this here that was the toy example data set and if you ask Weka or orange or you know other software data mining softwares there are lots of data mining libraries in Python as well scikit-learn numpy scifi this kind of Python libraries are very very useful for you to write a small program to be to implement an algorithm or to use an algorithm the advantage of Python coding is that is it's there is there is a community contribution sort of thing many people are developing different algorithms libraries are freely available you can use it and it's also a very fluid language in a way you can it's very flexible and also the graphic part of that language is very useful you can draw different graphs and plots etc very easily compared to look I'm a big fan of Java but when we first learn Java it was very difficult with using bridges swing swing etc to draw something lots of lines of code just to draw it since seeing a straight line but Java has improved from that law but pythons is also very good so there are many other software tools that you can use to to build you know decision trees forests or do some clustering here is an example of a decision tree that you can just build we didn't mean blink of an eye if you are using a software but how does it actually happen so what really happens in the background so that is also taught in this subject in this course for example I mean on a at a surface level it it actually there is an algorithm behind it and it is a systematic approach it actually tests attribute one by one so for example here we have two attributes qualification and salary and the third attribute is of course your class attribute which is position in this case class attribute remember there's the target attribute for example in the previous diabetes data set it was that diabetes diabetes test result tested positive or tested negative and you can actually choose any attribute as a class attribute so here is again one one example of your requirement of understanding of data analysis so you know which attribute to choose and that that requirement of understanding is important from day one when you start collecting data when you design your survey questions and when you know which type of data you want to collect because you want to get that sort of analysis done or if the datasets or databases are already there how to build a data set how to combine them what sort of information you need you cetera and I can give a long list of examples on that but to be brief okay so how it builds how how the decision tree was it tests the at reveals one by one first it tests the attribute qualification here in this case it checks what are the possible values MS and PhD there are two possible values here and then it one it tries to understand its suitability the suitability of this attribute qualification in classifying the class attribute in differentiating in distinct distinguishing the values of the class attribute how clearly this attribute qualification can classify can divide the class attribute into two homogeneous groups one group will be positive only now one group is senior only in this example another group will be junior only so can it do it and how how accurately it can do it so it will it will analyze that for this attribute and then it will also analyze that for all other attributes in this case salary and it will also do the similar analysis for this attribute how does it assess the suitability or accuracy of classification of an attribute there are many different ways there are many different metrics I should rather say for example information gain gain ratio Gini index etc so so there are various techniques to assess the suitability zhh you and different algorithms will use different and in this subject we will also explain in details like exactly how it works so in to the level that you can actually without using Weka or any other software tool you can actually build a decision tree and forest by yourself and similarly you can do missing value imputation you can do classification clustering etc so that that is one one slide and the other slide and I will not go into the details of how it actually works because of the limitation of time and the next slide and we are almost towards the end next slide is about decision forests you you possibly already know what decision forest is all I was trying to display here is that from the same data set you can have a tree that says if qualification equals MS then the position is jr. and if qualification equals PhD then the position is senior and if you look at this data set now you will see that that's 100% true so whenever the qualification is ms the position is jr. actually there are six such records we're qualification is m/s and position is jr and nine records we're qualification is PhD and position is senior so that's a that's a very acceptable or confident rule set of rules but at the same time from the same data set you can actually see some other type of pattern also exists for example if salary is less than eighty six then the position is junior if salary is greater than eighty six anything more than eighty six then the position is senior and that's also equally true for this data set so from a data set you can actually explore multiple patterns okay so one one set of patterns may be one decision tree one decision tree may present one set of patterns but if you build multiple decision trees then you can see multiple set of patterns sets of patterns right and there are many work on this on this there are many different research works there on this and also there are research on or techniques on how to optimize the number of trees so how many trees are so we want less number of tree but maximum knowledge so how do you do that something like that so those kinds of things are also discussed in this subject now I will just take two or three more minutes just to introduce clustering because last time last week also I wanted to talk about clustering I could not just to let you know for in case if you do not know I know many of you already know what clustering is so clustering is like imaging this is a data set age and height in this case so if imagine that these data set has only two attributes okay so I carefully chose eyes two attributes so that I can plot them now if you plot these values so for example on the x-axis you you consider the age fellows and one on the y-axis you consider the height values then if you plot each record as a dot for example age 23 and height 6.5 so that's the first record is this dot then you can get number of dots right and you can clearly see possibly from this graph that there are four different groups of people so these these four four people are kind of similar to each other these four people are similar these four people and these four people are similar so there are different groups so that's the clustering if you have a data set and now this was a simple data said two dimensional only few records so it's doable but when you're dimension is high say you have 50 60 different variables and you have thousands and millions of records then how do you automatically I find out clustering so clusters of groups of people and that's another way of discovering knowledge and here is an example you can actually if I have time I can demonstrate this on on unreal real-life in terms of on my I can share my screen and I can show it but here is an example you can choose a clustering technique on waker and you can cluster the records something like this and that's it for today's lecture but before I finish the lecture I will ask guy-guy do you have do we have couple of minutes few minutes to demonstrate clustering sure and you know pick a few questions there okay so basically I will I will be very brief I I think we all I have already you are now share seeing my screen it's not a video anymore so you can see this is the work I Explorer and I have opened this this file called diabetes file that you have one thing I wanted to also tell you that when you have this Weka Explorer and you open a file in this case diabetes dot air FA file you can actually look at these attributes to also see various information learnt various information for example class you can see tested negative 500 and tested positive 268 so you can see that the blue is negative and red is positive and there are 500 patients with negative test result and 268 patients with positive test result and then you can choose another attribute say for example age and you can learn various information about that attributes the minimum value was 21 maximum was 81 in this data set more interestingly you can see when age is small low like 21 then you have more tested negative records remember blue is just a negative then tested positives so it shows that diabetics chance at an lower age at an early age is low chance of have being a patient chance a chance of a patient being diabetic is low but when age increases then here you can see the gradually the proportion of red is increasing as well something like that so just this is one thing and as an example of clustering so if you choose from the area previously we used to choose classify to build trees etcetera in my previous examples you can also choose cluster and then you can choose some clustering algorithms one of our algorithm Gen class plus plus is implemented in Weka and you can download that how you can do that please watch my youtube videos and in those YouTube videos on my youtube channel za he'd start mining channel I have demonstrated how exactly you can install simple but I will it will take two minutes three minutes to me explain but I don't have weed I don't think we have that time I am rushing and you are also rushing I know so so that's that that's the algorithm if I choose that algorithm cluster and then if I if I choose that one and then if I choose cluster start cluster then here is a clustering results so it shows that clustering results I can see that there are two groups so it automatically divides these diameters patients into two different groups and it is saying that for one group group the average values are like this for Prague fellow is four for the other group the prague value will be 2 and class value is 140 plus value is 170 for the other group so this is how it tells us how the two groups are divided on an average one group has this on an average the other group has that you can actually see that sort of try to see the visualization as well you can visualize these things distribution of the records etc you can choose on one hand on as an x-axis maybe age and as y-axis maybe something else maybe mass you can see how records are divided in different groups etc so that cluster visualization is is slightly you know like for exploratory purpose and also you can generally you may ask why people need to do clustering sometime and and a quick answer to that will be sometime you have a data set that does not have any labels okay for example I can give you the records of everybody or records of the residents of a city say Bathurst and I can just give various information their ages their number of dependents or their household income or etcetera and then what do you do with this that that data set there is no class attribute as such so sometime the records are unlabeled so in situation like that generally people use clustering to first group the pupil group the records into different groups and then study each group carefully what sort of group is this and then they find possibly I'm just giving an example wow this group is having a say religious group okay so this is a religious group and this is a possibly less religious non-religious groups or they can find maybe this is a very sporting group they go for sports various types of physical activities etcetera and this is kind of a little bit of relaxed group and then they can label those records according to those different under groups understanding of groups and then they can build a classifier a decision tree to see the patterns of being a what is the pattern or what are the patterns of lazy people I mean look I'm I don't know if my examples are I'm being silly let's say okay let's let's go go with this lazy group of people and very sporting group of people right and we can then learn their characteristics and etc so clustering may be in that sense is the first step if you if you have a data set without any labels you can do clustering and with that maybe I I can stop my lecture almost today and because that was the last slide yes that was the last light please thanks I will just go for a few questions Hamilton's asked where can I get the half test files mentioned in week one and two I'm not under scim certain what that is talking about a a RF test files they'll make any sense to you I think I think I think the question is where to get the test files the Dottie rff files so when you install Weka then it comes with that installation as well so when you download Weka is free download if you can google such Weka and download and then if you store it save it in your hard disk then go to that folder you will see there is a folder called data and within that data folder you will have this dot erfa files as they start start play play to play with beauty okay thank you on Weka there's been a few questions about whether it'll be used a lot in this subject and also Robins been talking about different tools that can be used is there any particular reason why you use one tool over the other not really I mean any of the tools can actually do the same thing at the end of the end of the day the algorithms are the same algorithms that are being used differently through through different tools now I mean this this the the reason for choosing Weka was it's very common among the academic academic world so they generally use it because there are already lots of lots of algorithms implemented there but in this subject we also briefly demonstrate how to use Python to actually do similar things in Python but that requires some sort of programming understanding and programming knowledge if you do if if one does not know what object-oriented programming is and how to do programming in Python he may find it little bit difficult so that's why for the demonstration purpose and to our main goal here is to convey the message how data mining works what what are the things that happen in the background in the as a technique and huh and also we teach those things look one advantage is the subject the subject look this is only one subject in the course there are many other subjects so you can actually choose various subjects together these are optional subjects so the way we design the subject is basically we are teaching how to do various components of the data mining lifecycle and we are demonstrating that through the algorithms that we developed here at CSU for example for missing value imputation we demonstrate DMI or simi we had around 10 or 12 missing value imputation techniques that we proposed but we are teaching one or two and then we compare that with some other existing techniques so the advantage is you can hear about a technique how it works why it does it that way why not other ways from the mouths of the authors from the authors themselves so the people who actually develop the technique are teaching you the those techniques so that's why we chose those techniques and we implemented these techniques in Weka so that we can use it for teaching purpose as well that is one reason we are using Weka to demonstrate but you can use any other similar tools okay and for more questions when the dawn comes from David talk he's talking at the the tree where he had the junior and senior according to celery he's sort of noting that there's a gap between 86 and 143 celery yeah and wondering whether there needs to be an additional branch or or whether that is just a function of its classification okay good question David look David this is a toy toy data set and in this data set possibly I deliberately wanted to have two distinct group there is a there is a clear gap if it if the data set data set records are more contiguous more close to each other the tree will actually try to find out the split point the best split point where it can divide the record into two different groups and the trees here the trees are also very simple because the data set was very simple but if you go to my previous examples you can see like here you can see this tree it did not choose just one level to reach to the leaf so here is the leaf but for that it needed to check plus and mass both right so it can actually n4 to reach to this leaf it needed to check age plus mass three attributes out of nine so to reach to this leaf so you are right that sometimes there will be more attributes needed more splitting needed to reach to a as homogeneous leaf as possible but because that example was very simple and the data set was a very simple data said there is a big gap between these two and the tree easily finds that within this gap if I divide the records one group one group of records a one group of records is fully jr. group another is fully senior group well I hope I hope I answered the question David but yeah I send it good to me but that doesn't mean much it's gotta be fairly students we can't have more of an idea about that statistics in I do okay thank you so much ahead another very interesting and complex lecture thank you everyone for coming along and just digging around a little bit late again no I'm quite happy to keep going for 75 minutes if that's what it takes like thank you thank you next week so hey buddy you weren't gonna be here for the next couple weeks are you yes guy I will be I will be attending a conference in China from next week so that's why I will be out of out of office out of country but my my colleague dr. Michael Byung will be here and he will be giving actually he is now in China he is attending a conference and he is coming back on Sunday maybe and then he will be giving the remaining two lectures the conference that I am attending is on data mining they call it admah advanced data mining applications and we have maybe two or three papers there to present so that's the reason it's a very good conference yeah well I hope you enjoy it thanks for thanks so much for the two lectures it's been it's been really interesting and I hope to see more of what you've been up to keep keep us posted in the forums thank you very much it was a pleasure and please watch my youtube videos so I will be up updating things in YouTube videos and please be in touch with me if you have any questions 
quQmnnb09yQ,27,"Learn how to explore, analyze, and leverage data sets of any scale in this 60-minute webinar with Google's Search Scientist and Stanford Instructor Rajan Patel.

Learn more: http://scpd.stanford.edu/courses/data-mining-courses.jsp",2011-03-22T21:57:42Z,Data Mining: The Tool of The Information Age,https://i.ytimg.com/vi/quQmnnb09yQ/hqdefault.jpg,stanfordonline,PT52M6S,false,6855,33,0,0,2,"now I'd like to introduce our speaker professor Rajan Patel PhD is a search scientist at Google where he has led several successful projects resulting in the integral use of live traffic experiments and search quality evaluation is also a statistician on the team that developed the Google Flu Trends obviously been busy of recent times as a powerfully powerful available tool which mines Google search logs for flu related activity and estimates influenza-like illness rates in each of the 50 US states he developed a model which uses queries to predict influenza-like illness rates a paper on this research detecting influenza epidemics using search engine query data was published in 2008 in nature part of joining Google dr. Patel worked at Angie and incorporated as a senior bio statistician and then as a biostatistics manager while he was at Amgen he conducted several statistical analyses for early phase clinical trials and preclinical trials dr. de Patel is currently an adjunct professor in biostatistics at Emory University where he received his PhD from biostatistics Department Wallet and rekey developed novel statistical methods to analyze functional connectivity of the brain using functional magnetic resonance imaging fMRI data he's also a visiting instructor here in the department of statistics at Stanford University now I'd like to turn over the floor to dr. Patel hello welcome to data mining the tool of the Information Age of revolution so to motivate today's webinar I wanted to start with a quote from one of Google's co-founders Sergey Brin every year Sergey writes a letter to shareholders in the general public called the founders letter and in this year's letter he explicitly mentioned data mining so Sergey said just a few months ago we released Google Flu Trends a service that uses our log data to predict flu incidents weeks ahead of estimates by the Centers for Disease Control it's amazing how existing data set typically used for improving qualities can be brought to bear on a seemingly unrelated issue and can help to save lives I believe this sort of approach can do even more going beyond monitoring to inferring potential causes of tears and disease this is just one example of how large datasets such as search logs coupled with powerful data mining can improve the world while safeguarding privacy so hopefully that shows how important data mining can be and what what great tools can come out of a guide of mine so what is data mining depending upon who you ask you'll hear many different definitions so I'll just provide one coming from a statistical background my definition might be different than someone else's but but here's the definition that's used in the textbook that we'll be using for the stats 202 class data mining is the product process of automatically discovering useful information in large data repositories so the key one of the key words there is automatically and another key where there is a large data repository and those are typically the two the two aspects most commonly associated with data mining so on the bottom right-hand side of this slide I give you the process of a typical data mining solution typically when people talk about data mining or teach data mining in a class they focus on the the aspect of it where you have your pre-process the data and you apply different methods such as those that we'll talk about today are like clustering classification regression Association analysis bayesian methods etc to find to build a model interpret data and evaluate your data however in the applied world when you're really doing data mining the process is a lot more involved and a lot more in-depth you have a large data repository from which you need to select appropriate data pre-process that data and develop the attributes that you'll use to actually data mine before you can actually apply some of the algorithms that we commonly talked about in data mining so if you notice there's an arrow pointing from the problem question question of interest to the data repository and that arrow points after the data repository has been generated so why does the problem formulation come after the data and this is one thing that separates data mining from from other hypothesis testing frameworks or learning from frameworks the data is often there before you you formulate the question of interest this is the difference between hypothesis testing versus hypothesis generation and data mining we often generate hypotheses where as for example in clinical trials research or or other areas you might test hypotheses whether a drug is good for a patient or not you would test that hypothesis collect the data and analyze that hypothesis so you often don't have control over what your data looks like and because of that the selection the pre-processing the transformation steps are all critical in transforming that data to a format that's most useful for you so before moving on I wanted to give the example of Google Flu Trends so our goal in the flu trends group was to estimate flu rates in 50 US states using Google search query patterns so Google has this large database of search queries issued by people just like yourself from all over the world and we felt that there's a powerful signal there that could be mined and could be it could help the general public for the College of Public Health so the data in our case and this is of course an abbreviated view of the original data is each each search query that we receive the time span that it came in and the location from which it was issued so typically with some granularity were able to determine with a level of confidence where a search query came from so our original data looks something like this but before going forward we wanted to select out just queries that came from the US because our initial goal was to just look at queries or queries associated with flu that that comes from the US and not only that but we wanted to look at queries that occurred a certain number of times because what our interest was was to find patterns of query patterns Accords that had patterns that matched flu like illness rates and during the pre-processing phase we transformed our data into a time series for each location where a location was a US state so that each query had a time series of how many times it occurred over the last five years in that state and then we rolled that up so that each of the counts were done had a weekly basis because the the training data provided by the Centers for Disease Control had that time resolution as well so we did all of this we actually did quite a bit more in terms of spam removal and spike removal News News removal as well before even beginning the data mining process at this point we restarted running our data mining model which which is similar to a linear regression model to find queries that correlated with our I oh I raised truth data and then finally showed that the model actually can estimate flu-like illness very well over the course of an unseen season so this is the process we used to develop Google Flu Trends it's a fairly standard data mining process but I wanted to illustrate that once some of the important steps that we took happened well before the data mining step itself where we we actually use these correlative methods identify flu related queries so before we go further I want to discuss a little bit what we'll talk about today first we'll give some some more industry and academic examples of data mining then talk about what is and what isn't data mining and to briefly discuss the origins of data mining identifying types of data mining and introduce some methodology often used in data mining we'll go into more detail on the methodology in the stats 202 class this summer but I just wanted to introduce the methodology a little bit today finally we'll talk about some of the challenges remaining in that money so what are some examples of data mining so Safeway which is a big retail grocery store you can use your purchase data and so if you've noticed Safeway can provide you these discount cards where when you when you swipe them you often get a discount on some of their products and what that helps Safeway do is track what you're purchasing and and help present you with relevant coupons when you've purchased certain items so they use a data mining methodology to provide those coupons to you and they use and and provide those those discount cards to users to help better serve better serve customers similarly Amazon uses your browse history and your purchase history to identify items that you might like or items that you may want to purchase and that's that's helps separate Amazon from its competitors in terms of in terms of converting the page views into purchases State Farm oops State Farm and which is an insurance company as do all insurance companies build models of your likelihood of filing the claim and how much that claim might be based on people like yourself so predictive models so that they can help better estimate what your premiums should be furthermore an academic example of data mining that I was involved with a few years ago was is to find the functionally connected brain regions from functional MRI data so the scope of data mining is far and wide from neuroscience insurance retail groceries Google searches other searches there's quite a bit that can be done so the term data mining is very very broad in scope what is and what isn't data mining so so remember the definition again is given at the bottom data mining is the process of automatically discovering useful information in large data repositories but the process is often quite complex so looking up a phone number in a directory is that data mining well a directory is a large data repository and looking up a phone number could be automatic if the directory had some search functionality but but that's not really data mining it's not we're not just discovering useful information from a large source we're just finding getting returned at some of the data that's in that repository and another example what data mining is and is issuing a search engine query for the for or the query Amazon well the search engine might be using data mining to return those results but you're not mining for data by just issuing the query there's something more complex to data mining that makes it what it is so for example if you use information and directory to find which names are more prevalent in certain US locations that would be an example of data mining so if you were able to find that oh oh Connally is common in Boston or Johnson is common in Alabama or something like that and that would be an example of data mining furthermore grouping together similar documents returned by a search engine so let's say Google returned documents relating to the Amazon rainforest and amazon.com how could you use the information in those documents to group them into like categories that process could be considered by the money obviously the definition of data mining and what is and what isn't data mining might differ depending upon what what perspective you're coming from but but from from the perspective we're coming from today those are the examples we'll use so some of the origins of data mining data mining draws ideas from machine learning artificial intelligence pattern recognition statistics computer science in general and the databases so if you look at the Venn diagram on the right so I'm a statistician so so for me data mining comes comes from the statistics from a statistics perspective from computer scientists that comes from a machine learning perspective so you might learn a little bit more about boosting and bagging and various machine learning techniques support vector regression etc that come from from this side from the statistics side you learn a little bit about logistic regression and linear models etc on the database side frankly I don't know too much about but the database side contributes to the enormity of the data and help solve the problems with regards to scale so some of the traditional techniques in each of those three areas are may be unsuitable because well in statistics when you have too much data or it has too high dimensionality a lot of the statistical methods begin to break down so that's why combining with machine learning artificial intelligence database systems data mining can help answer some of the problems with the data of that size so standard data mining tasks can be broken down into two groups descriptive tasks and predictive tasks so descriptive tasks are typically defined as when you are looking for human interpretable patterns that describe the data so you're not necessarily trying to make predictions from the die but you're trying to understand the data better so for example clustering if you have a set of observations and you want to group them into different groups but you don't know necessarily what those groups are in a predefined manner clustering can help Association analysis we'll talk about a little bit later as well as clustering where you find patterns in in sets of items that are in your observations and predictive tasks which are which may be more commonly associated with data mining where you use some variables in your data set to predict unknown or future values of these variables of other variables so for example can you classify whether someone is more likely to cheat on their taxes based on the data you have about them or regression is an example from the flu trends regression is the type of diamond we use for food trends where we estimated the influenza-like illness rate given flu or given search query data so we'll talk a little bit about clustering Association analysis and classification today in SAS 202 we'll go into all four as well as anomaly detection and we'll go into quite a bit of detail in the regression talk about different regression approaches classification we'll talk about different classification algorithms with clustering we'll talk about hierarchical clustering k-means clustering how to identify the optimal number of clusters in your data and we'll spend a little bit time on Association analysis as well so we'll go into far more detail on not only the algorithms behind each of these data mining tasks but we'll go into examples and learn how to actually how to actually implement these data mining tasks in statistical programming language called our all right so so let's start with classification which was one of the predictive type tasks for data mining so given a collection of records or a training set where we know the class of a particular record can we find a model for one of the attributes that's a function for the other attributes that classifies new records or classifies that new records as accurately as possible for that class attribute and then typically we split our data into a training set and the test set where we're able to use the class attributes of the training set to train our model and then we see how well we're able to predict our predict observations in the class attributes that we want to predict for using our test set so let's start with an example so each record in this data set is information about an individual and what we'd like to be able to predict is whether they're likely to cheat on their taxes or not so the IRS builds models every year and or they build a model and try to identify who is more likely to cheat on their taxes than others so that they can target their audits more effectively if they randomly audited people chances are they they would get a lot of false positives so this way building a model they're able to allocate for those their resources more effectively so some of the attributes they use to build their model are well did the user or did the did the person get a refund this year and that would be a categorical attribute did is the person single or married or divorced is the what's the taxable income of the individual and that would be a continuous attribute so in our data set we have two categorical attributes in one continuous attribute and the class attributes that we're trying to predict is whether that individual was likely to cheat on their taxes or not so in our training data we actually have truth data whether those individuals did cheat on their data or on their taxes or not and we would like to build a classifier and essentially in this case it would be a logistic regression model to determine whether unforeseen observations would or unforeseen individuals would cheat on their on their taxes or not so we split our data into a training set and a test set and see how well we were able to predict whether individuals cheated or not on the test set so the classifier in this case from my perspective logistic regression you can also use support vector machines there's quite a few different classifiers that are out there and we'll talk more about that and stats two or two unfortunately we don't have time to go into specific classifiers today another classification example that I worked on while at Emory is classifying whether microcosmic microcalcifications in the breasts are benign or malignant so in this example the data is initially a digital mammography image which are very high resolution images that where each pixel is often about 50 micrometers in size and the image is about 2,500 by 2500 pixels and from these images it's not difficult to identify where certain micro calcifications are using existing techniques but it is a little bit more challenging to identify whether those micro micro calcifications or belong benign or malignant and it's critical for the patient to identify whether they're benign or malignant because that could be the difference between having to undergo surgery or not one of the most important steps in classifying micro calcifications is coming up with the attributes that you'll use to classify to build your classification model so one particular attribute you can use is the grayscale intensity of the micro calcification another one is what's the circumference to area ratio the micro counselor so for example the most malignant microfabricated Moshe calcifications often have a very high circumference to area ratio whereas benign microcalcifications tend to have lower micro have lower ratios another attribute that you might derive is a major access to minor access Radio ratio where the major axis is the length of the longest point in the calcification and the minor axis is the width of the micro calcification and then also you could use the pixel intensity histogram of the micro calcification there's there's quite a few attributes you can use and you don't have to find the attributes that certainly are correlated with whether the tumor is benign or whether a micro calcification is benign or malignant for you the the goal is to find as many attributes as you can and the classifier can help determine which ones are useful and and how useful each one is this is an example of where a lot of the work goes into finding the attributes and not necessarily building a great classifier or given attributes so a descriptive technique for data mining is one of the descriptive techniques we'll talk about today is clustering so what's clustering so given a set of observations each of them having a set of attributes and a similarity score or similarity measure among those sets of attributes find clusters that observations in one cluster are more similar to another and observations and separate clusters are less similar to another so the goal of clustering is to group observations that have similar of a similar set of attributes into different clusters and/or into into clusters where each within each cluster observations have similar sets of attributes there's different similarity measures that you can use to determine whether how disparate a set of attributes are Euclidean distance is one mahalo novi distance which adjusts for the correlation across pairs of attributes and other other problems specific type distance measures would work as well in fact 202 we'll go into more detail as to what euclidean distance is how it's calculated how it's used how what mountain Mahalo know be distance is and go into other distance measures as well but coming up with a distance metric to define how disparate attributes are is an important part of the clustering process so here's just a visualization of clustering of three clusters of observations where you want to maximize the intra cluster distance so that the sorry the inter cluster distance or the distance between two clusters and minimize the intra cluster distance so the distance between observations in the same cluster in such - OH - we'll go over a couple different clustering methods one of which is k-means where you pre specify the number of clusters in the in the data set and k-means will will arrange the observations in two clusters that best minimize inter cluster distance and maximize inter cluster distance and then there's another family of clustering algorithms it's called hierarchical clustering and we'll talk about agglomerative hierarchical clustering in the class where you don't pre specify the number of clusters but they're clustering occurs in something called a dendrogram and eventually you cut off the dendrogram at the number of clusters that is most appropriate and we'll talk about how to do that in the class so another another clustering example is in the in Google Flu Trends we wanted to be able to cluster search queries that have similar volume patterns so we wanted to be able to understand well which queries have similar user in times which korie's go have increase in volume at the same time and decrease in volume at the same time so on this plot I'm showing the the normalized search volume for the query flu symptoms as well as the normalized search volumes for the great influenza and you can actually make plots like this yourself if you go to Google Trends which I believe is at google.com slash Trends you can actually then download this data in CSV format and do your own data mining with queries of interest as well but the idea behind clustering similar search queries is well if can I find the queries a set of queries that might be news related or can I find a set of queries that have some specific pattern that I can I can attribute to to some other effect and either use those queries in my in my flu model or if omit those queries from my food model and so we did use quite a bit of clustering in Google Flu Trends and in this when clustering flu queries the attributes are the query counts or the normalized score accounts at each time point so you have quite a few attributes and they're highly correlated with each other so you would want to adjust for the correlation across time point but the attributes so we actually had weekly data going back four or five years so we had two or three hundred attributes in our data and that's when the scale of the data begins to grow beyond what there's typically typically used in statistics so finally Association rule discovery is another descriptive task where given a set of records each containing some number of items from a collection can you produce dependency rules which will predict the occurrence of an item based on the currencies of other items so the Association rule discovery is important for so for example amazon.com or so let's save this example a Safeway a grocery store where user one purchases a subset of all the items for sale at Safeway he purchases bread coke and milk and user 2 or a shopper number 2 purchases beer and bread shopper 3 purchases beer coke diaper diapers and milk and shopper for purchases a set items shopper 5 purchases a set of items and what we'd want to be able to do is discover relationships between the items when somebody purchases milk are they more likely to purchase purchase coke when somebody purchases diaper and milk together are they more likely to purchase beer so let's let's go into an example from from from Safeway so let's let's see if we so let the rule to discover it be like bagels - potato chips so bagels is in the antecedent and potato chips is in the consequent so having potato chips in the consequent allows us to be able to determine what can be done to boost its sales so potato chips in the consequent we see that bagels is in the antecedent if we sell more bagels then we can sell more potato chips with bagels in the antecedent we can be we can use that to see which products would be affected if this store discontinued selling bagels so if Safeway note no longer sold bagels then the sales of potato chips would be effective as affected and finally if you had bagels in the antecedent and potato chips in the consequent you can see what other products should be sold with bagels to promote the sale of potato chips so for example if coke and bagels were in the antecedent then both of those would be would affect the the sales of potato chips so this is how Association analysis and Association rules are often used at at retailers so not only safe way of course but Amazon and then almost every retailer in the world and it helps identify which other items might be my we might be a shopper want to buy or to present items to them as in the form of coupons or you know in Amazon's case as you know items you might you may like they use an association analysis to do that so finally so some of the challenges remaining in data mining so data the data is often very complex and very noisy as you remember from the beginning of the talk the data is often often existed before is you are you developed a problem or question of interest so you aren't able to collect the data yourself in the format you want and and have complete control of the data and converting this complex and noisy data into something that's useful and usable to you in terms of what attributes that it has and and and so forth is a very important process in data mining and it's one of oftentimes the most difficult and overlooked steps the same goes with data quality the privacy preservation is often is recently become very important for example with the launch of Google Flu Trends we're using individual search query data so search grades that individuals issue and you have to be very conscientious about the privacy of those users so we went to great lengths to anonymize the data and make and aggregate the data to make sure that no one individual could be identified from from any of the data that we use lately streaming that has become more and more important there's a microblogging service called Twitter which is exploded in 2009 and streaming data in the form of micro blogs and and Twitter searches are becoming more and more the I guess more and more prevalent and Twitter is going to Twitter and other similar services are going to use data mining quite a bit to identify what hot topics are hot trends are what useful webpages are etc to better serve their users and in an issue that Twitter will have to deal with as this Google and other and other companies is scalability of their services so it's important for data mining especially with the size of the data sets and the increasing size of the data sets that scalability of the algorithms and of the the platforms with which you implement those algorithms can grow to meet your needs and then finally from a statistic that efficient perspective the dimensionality of the data often causes the greatest concerns with the methodology so developing methodology that scales with the dimensionality of the data is critically important so I want to thank thank you for attending the class today I believe Paul will follow it will follow up with some some comments and then I'll begin to answer some of the questions that have been queued up over the next five to ten minutes thanks dr. Patel thank you very much and now what I'd like to do is go over some background on the Stanford center for professional development and how you can actually enroll in dr. patel's course data mining and analysis first of the Stanford center for professional development is in delivering education to industry as the bridge between Stanford and industry for over 40 years now we deliver a number of graduate courses certificates and programs about which I will speak in a few moments and also professional education our goal in doing this is not only to extend the Graduate Stanford level content to maintain the vitality of the Silicon Valley but also to allow you as individuals to stay on top of your game we have a number of different offerings and I'd like to walk through those our Center offers about 230 graduate courses every year those can be taken in a variety of different options that you see here the first is the non degree option where students without submitting transcripts as I sorry without submitting GREs and actually applying to the Department simply submit a transcript to SCPD and gain access and take those courses for graduate credit at Stanford University students who pursue this can actually accumulate up to 18 units which may be transferred in to variety of different graduate master's degrees that we offer at Stanford students can also bundle those courses the SCPD has worked with various departments including statistics to create graduate certificates data mining and analysis for instance is part of the certificate in data mining and applications offered by the Department of Statistics these graduate certificates are focused in a specific area such as the one I just mentioned and comprise typically of three to five graduate courses this allows the individual who may already even have a PhD to get some additional education in a domain we know one thing is certain that education and these various areas continue to evolve and this is a great way to demonstrate how you can remain up-to-date to your teammates as well as to your boss and others in your group finally we offer two different to a master's degree which is called the honors cooperative program if you're interested in this candidates must apply like any other graduate students to the department of interests and the SCPD allows this candidates that once accepted to participate on a part-time basis students can take up to five years to complete that degree and as I mentioned if you had participated in the non degree option up to eighteen units can transfer into that master's degree we also offer an audit program where students who want to just observe the content can participate and there is no credit associated with that the Stanford center for professional development is one of the leaders in distance education since 1999 we've been offering master's degrees in engineer and other disciplines at a distance we do this by creating lectures which are an offered in nine television classrooms here on Stanford's campus we capture those and extend those on a streaming basis those typically are offered about an hour after the class has occurred students engage with faculty via email and also if the class has homeworks and assignments those are completed typically online and through the Stanford center for professional developments distribution unit finally if there is a test required that we do require that companies identify an exam monitor who may not be your spouse or best friend to provide some secure way of making sure that the exam is taken by the student and the time allotted once you have been able to identify the the course of interest as say the course that we've shared with you today I'm going to walk through some of the ways you'll need to engage with us for a course enrollment the summer quarter is now open and will remain open until June 17 students can enroll in a variety of different ways the most common of which is the non degree option category if you are as you can see here if you belong to a member company as identified on scpd's website you have a slightly better per unit cost than if you do not so would encourage you to look at that if you're interested in auditing again the member company has some slight advantage the nice thing here is that this allows you to participate and observe the course just as you might have when you were once on the University campus not engaged with the faculty member or the class per se but get a chance to experience and deserve the content once you have decided you would like to participate we actually have in scpd's new website something called the my Stanford connection we encourage you whether or not you are actually actually interested in pursuing a graduate course to enroll there we often extend free seminars webinars and archived webinars such as the one you've experienced today and my Stanford connection will allow you access to those it will also allow you the ability to register for courses graduate courses through a CD what you do is you go to our website once you're logged in and have your user ID you can I do a quick search for instance on stats 202 and you can simply click enroll in this section and you'll see the various options this will drive you once completed we encourage you to click on the carts very much like an amazon.com experience where you can click on that cart to make sure you have the right items and proceed to checkout your page will look very much like this or you can identify the correct category based on member company or you're taking this for non degree option credit an audit option and so on and then you can proceed to checkout to the tuition payment screen I've given you some sense of the SBPD and how we operate what I'm interested in knowing from you right now is how much how many of you are are interested in taking data mining and analysis stats tool to be helpful to know that I see some people registering or voting right now I'm going to give you a few more seconds to click in before I close the polls again the the categories are I'm interested in taking this course this summer make sure you didn't take in the course next time that is offered I have some questions please contact me or no I'm not interested at this time okay the selections are still coming in I'm going to close the polls now you can see that eight of you are interested in taking this this summer seems like eleven are interest and also eleven are not interested we certainly can create these webinars to give you a glimpse into the state of the art and we also hope that this may drive some interest to the courses that SAPD offers I did want to highlight two upcoming webinars on May 14 we have a program with my colleague Belinda quo which will give you some perspective not only on the graduate programs about what you heard today but also the professional education offerings where we address content for engineers technology managers professionals and executives so we have a full complement of offerings that's tomorrow May 14 at 11:30 and then in two weeks we have a webinar featuring Professor Simon Wong and Stephen Boyd who will be talking about future opportunities of Electrical Engineering Yee matters that's at 10 a.m. Pacific daylight we hope that you might you or your colleagues and I join us for one or more of these webinars to learn more about our offerings finally we encourage you to enroll in dr. patel's course this link will be provided here for a few moments we do appreciate your taking the time here today we do have a number of questions coming in and we would like to actually try that to answer those I'm going to actually turn the floor over to dr. Patel who might be able to answer a couple of these questions live we have a few more minutes and if you do have questions for dr. Patel please send them in now we have an active Q&A session so the first question that we received was with regard to the data to data filtering is your data skewed or swamped once an event like the current swine flu occurs and numerous us reviewers conduct related queries through Google so this question is about the Google Flu Trends product and weather issues I guess news related events such as the current swine flu affect the data in the model so part of the model before we actually use the model we filter our query stream for news related events and we have a separate machine learning pipeline that identifies queries that may have been likely issued due to due to due to a news related event and we filter those queries out so oftentimes you know most of the time this works pretty well sometimes we let some things slip through but it's a very important goal of ours for the system to work not only when there isn't a particularly unusual and news related event going on but even during during something like this swine flu that we've seen over the last month or so the second question is for steps 202 is a prior background in our or other general programming language recommended for the course so experience with programming languages is recommended however experience with our in particular or any particular programming language isn't isn't necessary and if you don't know our if you haven't used our before don't worry about it but hopefully you've used some some statistical tool before maybe it maybe it's R or MATLAB or SAS even Excel more advanced Excel or other programming languages so that because we'll dive into R but I will provide a background within to R as well as Excel but hopefully you can get off the ground running in terms of how programming languages typically work okay the textbook first that's two or two will be the same textbook that was used last year it's called introduction to data mining by Tenma Steinbach and Kumar so again its introduction to data mining by ten Steinbach and Kumar if you if you go to Amazon or Google and type in introduction to data mining time ten Steinbeck and Kumar you'll find the book listed there the next question is what pre-class reading can a student conduct prior to enrolling in the stats 202 class so I think in order to prepare for the class and to get yourself up to speed as much as you can get familiar with our as a statistical tool that would be optimal because then you'll be able to focus on the data mining methodologies themselves and not necessarily have to worry about having to learn programming language while trying to learn new statistical methods and data mining methods other than that I don't think there's there's too much that you'll have to read and if you're interested in getting a broader background on clustering classification and Association analysis for aggression etc I would just say that you know start searching on the web maybe Wikipedia and reading a little bit about what's out there might help what are the differences between sets 202 252 and 315 B Paul can you answer that one well I we have a variety of different courses in in stats and what I would encourage you to do is go to ETSU pd's website and do a compare and contrast against those topics I can tell you that that's 252 is quite a popular course and features a number of guest lectures talking about how they implement and use statistical methods in their own business we are offering 252 in the spring but with the others I would have to refer to the various descriptions on a securities website which are reasonably robust I would also encourage students if you have questions we can direct them to our course advisor for statistics who is quite good and will be quite quick on the uptake regarding any questions or issues Ashish is a PhD candidate within statistics and is reasonably responsive within the 24 hour business day so if you have further questions I would direct you there the one of the next questions are actually the two next questions has to do with the level of math background or staff background required for stats 202 will there be a review of key concepts so the FAFSA 2 is an introduction introductory course so we will go over basics like you know how to make a histogram how to visualize your data well it will really take it from the top so I think that the level of math background will be will be quite quite minimal I wouldn't worry too much about not having used advanced math concepts in the past however concepts like probability and perhaps even conditional probability might be worthwhile to review but beforehand especially as they come up in terms of in association analysis and and in logistic regression etc as an SP CD student can I come to the classroom for stats - OH - generally speaking it is up to the faculty member in the space in the room what we do ask you to do is submit your question to the SCPD customer service link which is on our website and then we'll get that to dr. Patel often the rooms are quite small in the summer and so we are just generally not sure about the room allocation of an availability it is indeed up to the professor generally speaking students in the non-degree option CAD Gouri are encouraged to take courses at a distance we have quite a robust enrollment in that category and if every student showed up at one time we have a real problem with capacity for those students who are on Stanford's campus the master's degrees candidates are typically allowed to come to Stanford's campus if they're local and again if if the classroom allows with that what I'd like to do is thank you for your attendance and your questions today's presentation will be recorded and we'll be sending out the link to all of the attendees including those who are unable to show up we encourage you to extend those links to colleagues who may have missed a session we do also welcome in choirs and follow up there were many good questions we'll be taking some of the questions offline and we thank you for your attendance today on today's webinar we look forward to seeing you in future SCPD webinars and we wish you a very good day thank you very much "
OtFImwvpQ3s,27,"This Data Mining training video is part of the CISSP FREE training course from Skillset.com (https://www.skillset.com/certifications/cissp). 

Skillset helps you pass your certification exam. Faster. Guaranteed. https://www.skillset.com

Topic: Data Mining
Skill: Information System Vulnerability Fundamentals
Skillset: Security Engineering
Certification: CISSP

Join the 40,000+ candidates in over 58 countries that have found a faster, better way to pass their certification exam.

+ Unlimited access to thousands of practice questions
+ Exam readiness score
+ Smart reinforcement
+ Focused training ensures 100% exam readiness
+ Personalized learning plan
+ Align exam engine to your current baseline knowledge
+ Eliminate wasted study time
+ Exam pass guarantee

And much more - https://www.skillset.com",2016-05-03T21:00:55Z,Data Mining (CISSP Free by Skillset.com),https://i.ytimg.com/vi/OtFImwvpQ3s/hqdefault.jpg,Skillset,PT3M20S,false,2915,15,1,0,0,welcome to our data warehousing and data mining module data warehousing is combining databases into a single large database in order to analyze the data the merged data can be used for information retrieval reporting and statistical analysis data mining is analyzing large amounts of data to locate data that you previously did not know data that was previously hidden or useful information data mining tools produce metadata and this sorted aggregated data is often able to show you previously unseen relationships and patterns you should be familiar with the difference between data warehousing and data mining for the CISSP examination remembering that both allow you to combine large amounts of information but data mining allows you to find information that was previously hidden or unknown data warehousing is the combination of data from several different databases in order to better retrieve and analyze the information you're extracting operational data and making it into informational data a screen scraper is a tool that will pull data from multiple websites and add it to your database you should remember this term screen scraper for the CISSP examination with data warehousing you are not just mirroring the data you are normalizing it by reducing redundancies and inconsistencies with a data mart you're gathering and sorting data in order to meet some defined criteria this data can be used to meet your immediate business needs or this data can be sold to other individuals who are interested in it there are tools that can be used to analyze a database or a warehouse to look for anomalies or trends and also to identify patterns and relationships between different sets of data this creates metadata which is data about data which can help the data become more useful to the user it can also be used to test for inference vulnerabilities where an individual is able to deduce information about an object by collecting information from several different sources we have an example at the right of your screen where database information from database a B C and D were all placed into the data warehouse the metadata or data about the data within the warehouse was extracted and can be presented to the users in a more usable format than if the user just looked at the data warehouse itself this concludes our data warehousing and data mining module thank you for watching 
Qx4Wupf1okc,27,"BIDS Data Science Lecture Series | February 5, 2016 | 1:10-2:30 p.m. | 190 Doe Library, UC Berkeley

Speaker: Matthew L. Jones, James R. Barker Professor of Contemporary Civilization, Columbia University

We cannot understand the programs revealed by Edward Snowden and other whistleblowers without understanding a broader set of historical developments before and after 9/11. With the growing spread of computation into everyday transactions from the 1960s into the 1990s, corporations and governments collected exponentially more information about consumers and citizens. To contend with this deluge of data, computer scientists, mathematicians, and business analysts created new fields of computational analysis, colloquially called “data mining,” designed to produce knowledge or intelligence from vast volume. Facing the growth of the Internet and the increasing availability of high-quality cryptography, national security lawyers within the Department of Justice and the National Security Agency (NSA) began developing what was called a “modernization” of surveillance and intelligence law to deal with technological developments. In addition, in the Clinton era, concerns about terrorist attacks on the United States came to focus heavily on the need to defend computer systems and networks. Protecting the “critical infrastructure” of the United States, the argument ran, required new domestic surveillance to find insecurities and opened the door to much greater Department of Defense capability domestically and new NSA responsibilities. Tools for assessing domestic vulnerabilities lent themselves easily to discerning—and exploiting—foreign ones, and traditions of acquiring and exploiting any foreign sources of communication prompted the NSA to develop ever more invasive ways of hacking into computers and networks worldwide. The job of the NSA is just “to exploit” communications networks—to make them available to policymakers; to do this, its lawyers “exploited” the law as well as technology. ""Great Exploitations"" tells a history of how we came to exploit communications, law, bureaucracy, and the fear of terrorism and how we might choose to do so differently.",2016-02-05T22:43:21Z,"Great Exploitations: Data Mining, Legal Modernization, and the NSA",https://i.ytimg.com/vi/Qx4Wupf1okc/hqdefault.jpg,Berkeley Institute for Data Science (BIDS),PT51M11S,false,694,6,0,0,0,well thank you all for coming on a sunny Friday afternoon I just want to begin by noting and that i'm going to make extensive use of publicly available but still classified data so if you happen to have a security clearance and it's not extremely high security clearance you should probably leave i want to begin with three major changes in the posture of the national security agency from the mid-90s to say the middle of the last decade the first concerns the severity of volume in the mid-90s the volume of communications was one of the foremost problems for the NSA talked about constantly in their internal literature but by the mid 2000s we've found out about email titers where volume is thought to be our friend in which leveraging volume has been turned into an ally leading to what by 2010 they called the Golden Age of SIGINT signals intelligence their job of listening to signals that they are not supposed that that are not intended for them so volume secondly the legality of kinds of queries that they can do on kinds of data in the late 1990s it towards the end of the Clinton administration the Department of Justice ruled that it was not legitimate for the NSA to do what is called contact chaining that is producing social graphs on the basis of people's telephony records I cannot get access to this ruling they have not succeeded in foyer of them but quickly it has changed or to go by 2008 a secret DOJ Department of Justice memo rules the contact chaining and other meta analysis do not qualify as interception or selection of communications and because they are not interception they do not violate the Fourth Amendment thirdly so we have volume the legality of certain kinds of queries and thirdly the hacking into computers around the world as a practice in 1997 after an internecine battle with other parts of the Armed Services the NSA is given the role to be the primary the primary aspect of the US government to produce and instruments for what it was called computer network attack and instruments for exploiting computers by hacking into them by the mid-2000s there's a worldwide database called xkeyscore which is a distributed set of Linux clusters which allow you things like the option to show me all the exploitable machines in country X by 2013 the US government states bluntly that the United States government has mature capabilities and effective processes for cyber collection at scale hacking had become banal a standard practice and a huge scale throughout the united states throughout the world so this is indeed a drawing this is one of the key Snowden documents and every yellow dot on this diagram is an instance of a place where the United States government has implanted something into a computer a router a printer in order to access communications or prepare for a computer network attack this is a sort of probably a radical underestimation of what it is but it is a worldwide phenomenon the name they used internally was that they washed to own the net and they did this through passive listening a worldwide platform of past listening of managing hacking and strangely they are a profoundly Janis faced agency on the one hand and many of you probably know this they've the fundamental job of protecting the communications particularly of national security and military communications but also in the design of things to protect all kinds of communications so-called communication security and on the other hand they have a fundamental job of exploiting those communications and indeed they fully recognized for many years that this tension is a problematic one the current director of the NSA just announced this week that these two divisions were going to be folded into one another making the contrast even more problematic one of the aspects of this is that it seems as best we can tell that overall the exploitation of signals and of computers has taken a priority to the security needs of other parts of the government and indeed in all of us that secret risk analyses have decided that national security is best served by weakening encryption in many cases and by allowing a large number of malware exploits to exist the eff has been doing a lot of work to get into the process of these vulnerabilities so by 2008 one talk at the NSA said that they were going to be in your interwebs splitting your data okay you might say and many a commentator has said this well duh their job is precisely to listen to communications to collect and analyze what else can you expect them to do so many people have rightly said for all the all that's come out as a result of Snowden and other whistleblowers maybe we're missing we're just making too much of this so what has changed is there been a change I'll argue that there's been a there's a sort of radical two-fold difference that we need to look at here and that difference has an element that I call and an element that I call depth now by depth what I mean is this that in the post 9-11 world the NSA has become far more involved in in accessing communications of people far beyond its traditional domestic target of foreign targets of leaders and militaries and intelligence not just communications between phones and computers but access to the full contents of computers phones and routers that diagram of the full range of computers around the world that they've exploited is an indication of the depth of this the other side and this is what's absorbed most of the attention in the media is the NSA's domestic presence that it is involved in in the now slightly changed but systematic collection of domestic telephony and until recently internet metadata that it is actively involved in collecting all kinds of communications in the United States subject to a very complicated legal regime but nevertheless a very important one and it is involved in one which it can potentially keep all encrypted communications indefinitely so just to clarify some of the axes of debate and these often get confused by the different communities that are involved i think there's three that are important and it's not just privacy and security privacy and security is rather crucial and usually the debates cast this way we have one set of values of our privacy and another set of values of the security that's necessary to confront that security we're seeing this played out again in the current debates about encryption and I'm the idea that the FBI is very concerned that they are going dark this concerns of course are the take of most of the energy in the media but there augmented by a kind of bigger debate one that I think is central if you want to understand what happened with the encryption wars in the late 90s which is very much one about security versus security this is very much in the minds of those cryptographers outside of government who are adamant that the creation of any back doors for governmental purposes undermines the seekers the security and fundamental ways a final way that this debate plays out and this is not one that we see publicly but internally within the intelligence community in the military there is a massive debate between operations and espionage between attacks and collections this is a history we know very much about but it's it's hinted at everywhere on the document so what kind of history do we want to do about this now 911 is in sort of easy point and it's very easy to say pin this on someone like Dick Cheney or others but it doesn't offer the possibilities of understanding the conditions under which these transformations came to be and it allowed it makes it too easy for us to forget the series of decisions technological and legal and societal that enabled and indeed set up things in such a way that the events that followed in the wake of 911 and subsequent other attacks could happen so what I'm interested in as a historian is in some sense giving a structural account of those transformations in technology and science in the law and in governmental postures that will then actualize that we're available to the people in the bush administration in the wake of 911 and why I'm a historian of science why does that matter well one thing historians of science and technology are very interested in is is sort of pushing back against claims when someone says this technology exists in this way therefore society is going to have to change in a particular way and in this case didn't the creation of new communications and analytic technologies require us to think differently about the law now my sources are varied and everything i'm doing here is very much reasoning under uncertainty because we don't know a lot about the NSA we know a lot more because of Snowden in fact one of the things I'm operating under is a rather grave challenge that on Tuesday a document came out that is about data mining at GCHQ and NSA that document I've barely had time to digest i'll talk about it just very briefly but i'm using declassified documents open source intel in some cases forensics on weapons in the wild and indeed the Snowden documents all of this is publicly available many of the things from the Snowden documents of course are still classified but I'm reasoning under uncertainty and I'm often horrified to look at a Twitter feed before I give a talk because I'll find that I'm wrong about something ok exploitation so exploitation is a word is a term of art within the intelligence community and within the signals intelligence community and to get this I need to tell you a little about two kinds of Arcana some of you will be very familiar with this some of you won't know anything one in telecommunications and the other in national security law a lot of us have been getting a quick education in this since Snowden started coming out a lot of this involves what how Calvin called the verbing weirding language there's a lot of strange language use that is central to the way things work in the intelligence and national security world that is very odd to us in particular there's a lot of actors categories that are not the way we would understand something so collection for example in the ideo lack of the intelligence community is confined to that those those those data that have been accumulated and have been received quote by an employee of a DoD intelligence component so when clapper testified before Congress he typically is using a particular idiolect and so within that frame of mind he wasn't in fact lying to Congress about within certain of his claims but it's a highly it's a highly specific vocabulary that has legal significance and it's not I think exclusively rhetorical not only about pulling wool over someone's life but it's central to the world view and the legal categories of what's going on the central term that's exploitation is used for within the intelligence community is the phrase exploiting signals to exploit a signal roughly means to make it available for analysis to enable so to exploit angle a miracle cellphone is precisely to make all the communications on it available to us to exploit a keynote in the internet black bone is to be able to capture all the as the evidence for it now what I'm going to argue is something very much like this is happening with the law and that's much of what we've seen in the last period that the law has been exploited I mean this in the ideo left way it has been made available for the extension of the collection and analysis of signals and this is nearly an actor's category so in the documents we have about the evolution of the law in the wake of 911 they often talk about the need to create legal theories that will allow allow different kinds of collection and analysis and this is a that happens in the executive branch in the judicial branch and then the legislative and it also happens in the parts of NSA and DOJ that are actively involved in rewriting the legislation in countries around the world such as the German privacy law Australian laws constricting this and it's like I said almost an active and actors category so in an important sigint strategy document of 2012 ok so an actor's category i'm sorry i'm using my own idiolect wonderful question actors categories typically used by anthropologist to mean the categories in which people were studying talk about things themselves rather than the categories we would frame them in so instead of some imposing some external vocabulary of analysis using their own and so I'm using exploitation I would say almost as an actor's category so they say this is under keith alexander for SIGINT to be out the effective legal policy and process authorities must be adaptive as in dynamic as the technological and operational advances we will see to exploit we aggressively pursue legal authorities in a policy framework mapped more fully to the information law information age so what I'm going to talk about in most of the rest of the talk is two moves about modernizing the law and then the adoption and transformation of some of the analytical technologies captured in the umbrella term of data mining which is something that's actively used within the intelligence community's now modernizing the law there's two major things that I'm going to be interested in one with its received a lot of attention in the media another you have to be in particular sets of their media but the first one is the transformation from the legality of intercepting without a warrant the dialing information of a single phone of a person single phone line this gets transformed into the ability to capture bulk metadata worldwide including us metadata there's been a change in how this is done in the United States but it's still something is done warrantless Lee and secondly a transformation in how we understand espionage from a particular set of communications or breaking into a building and getting something to cracking computers at great scale now why this is why we need to put some attention to this is because we will find vocabulary constantly that says the technology has changed and therefore the law must be modernized and i want to say we need to step back and look at the decisions that made that a thinkable inference because modernizing the law in this way is playing a funny kind of game internally the great interest in collecting things that bulk is driven by something that almost everyone in this center is probably greatly aware of the way in which scale enables you to do certain kinds of analytical operations you couldn't do otherwise but legally the transformations justifying that scale are often promised on ignoring the implications of the scale to say that a law that was about individual things can be extended to talk about bulk things so internally volume is our friend because it allows us to leverage scale internal externally and particularly in the legal realm a very strong argument that the scale doesn't change a thing about the way we understand the Fourth Amendment so let's start with a paradigm shift about volume 1996 the deputy director of the NSA William crowel says let me add to that the third biggest challenge facing us and that is volume I could end the sentence there and everything is said then there's a huge paragraph redacted this gives you some idea of the daunting challenge volume presents in a report issued by Jim clapper then a private contractor on the scientific advisory board we're not allowed to see the volume growth because I don't know it's top secret for some or even though it's taken from a corporate report but it mandates a new paradigm in the collection of signals intelligence indeed it was a shift to what they call digital network intelligence which meant collecting the communications on the internet worldwide it involved an exploitation of a vast number of things routers servers fix satellites GPS phones modems faxes in a transition document 2001 written for the administration coming in either Gore bushed like everyone else they didn't know who was going to be they said the Fourth Amendment is as applicable to electronic cig and as it is to the Sigma of yesterday and today and then I want to put some attention this the information age will however caused us to rethink and reapply the procedures policies and authorities born in an earlier electronic surveillance environment the information age it continues make no mistake NSA can and will perform its missions consistent with the Fourth Amendment but senior leadership that means president and his close advisers must understand that today and tomorrow's mission will demand a powerful permanent presence on a global community telecommunications network that will host the protected communications of Americans as was the targeted communications of adversaries so before bush or Laura comes into office NSA's internally making an argument that the information age is going to require a modernization of the way we understand the Fourth Amendment and that's going to require collecting protected communications their concern here and this is again this is a diagram that shouldn't be top secret because it's taken from a public set but it's top secret nonetheless the point being that large numbers of communications on the internet that are foreign foreign are transiting the United States and in order to collect those it's the NSA was going to need authorities to collect things transiting but along the way they were likely to collect large numbers of signals of us persons in the homeland so to modernize the law meant being able to capture phul in communications and it also was going to require domestic launching points for information warfare all of a sudden domestic law enforcement and foreign intelligence we're going to have to be mish mashed in a way and this was going to be require this transformation of the Fourth Amendment now they said the information age and at the end of the 20th century this was a common way of speaking but it's a very loose and imprecisely do ages have causal effects one fact there is a major argument that goes on within military and dimdim at defense intellectuals in think tanks and in a large number of bodies that begin thinking through what the information age means for traditional divisions about intelligence and law enforcement other things as one report notes the issue of domestic intelligence gathering and surveillance needs to be this is the framework in which the NSA is thinking before nine eleven of the transformations it needs in a presentation internal to the NSA and then disseminated in its internal sort of house magazine crypto log it's available for download i recommend it to you a form of this argument was set out that we had overcome the agrarian and industrial ages and we were entering at radically transformative information age in which the ultimate destructive capacity is going to be critical information deletion and we were entering a phase of information warfare this was everywhere in a book that caused many people embarrassment to be admitting was a central source but was everywhere read called war and anti-war about the cheap new wars that would be coming and the need to accommodate the information war and it was it was set on a grand view of history that from 1648 until the end of the 20th century we were living in a so-called westphalian order in which we had sovereign territorial nations with clear divisions between foreign and domestic that was no longer true in the domestic in in the information age we were now in a d territory alized post westphalian they said information age an age were law enforcement and demand foreign and foreign intelligence were going to be mixed and this it was involved with a deep concern about asymmetric home asymmetric warfare in the homeland in report after report the language that became part of all of our lives in the wake of 9-11 of homeland security of the loss of sanctuary of the sorts of concerns was set out it was set out by bodies like the defense Science Board but also centrist groups like the heart Rudman Commission it saturates novels like drunk a clamp sees debt of honor and it feel figures centrally into the creation of the central infrastructure protection directives of the United States government dfb the defense Science Board and Rand play that work this out how there was going to be no frontline in in information warfare and the US homeland is not going to provide a sanctuary we were there for going to have to develop a new legal regime in one dsb report they noted that there was a lot of resistance within the Department of Justice and among civil libertarians to any of this thinking but they noted a sophisticated attack on public and private networks will likely make cooperation not just politically acceptable but politically necessary when this happens the legal regime needed to respond to the attack will likely be put in place quickly by politicians anxious to be seen part of the solution well they were right so what I'm talking about is some internal moves within the government and within advisers to the government to think radically about the possibility of domestic intelligence now many of the things that these people were calling for didn't happen in the 90s there was great resistance to but much of it happened in the Patriot Act so it's not accidental the patriotic emerges within weeks of 911 because everyone has the stuff in their desk drawer so it's not just little bits of legislation it is a world view but in the meantime it was profoundly contested the law enforcement intelligence access lost the so-called crypto Wars at the very end of the 90s and I saved too many people was seen as a cold-war relic and it seemed unlikely that NSA was going to get its modernization and indeed the Office of Legal Counsel in the Clinton era gave several legal setbacks to this in the very in the very end of it this just didn't seem very likely and it rejected the plan to contact chained us persons but nonetheless it's set in motion a particular set of technological developments as well as legal ones in one of these defense Science Board this is like an internal thing it's an internal advisory board of scientists and industry leaders to the Defense Department wrote in 1997 and a report on the transnational threats the call for quote the application of evolving techniques that are already employed widely in the industrial sector for searching merging sorting and correlating data in multiple independent databases can be applied to the transnational terrorist problem the so that the highest advisory board to the scientific research of the DoD called precisely for leveraging the developments that were widely known as data mining precisely about these questions of terrorist threats domestically and in foreign intelligence and hence my story comes to data mining I'm sorry it took me a little while to get here now data mining is the term that's very widely used in the intelligence community up to today and it has many glosses the one that was very important in the 90s is knowledge discovery in databases I was talking about this yesterday with a bunch of great people at at the ant lab in one form it particularly grew out of the concern within the database community that they had produced wonderful forms of storage of data they had produced ways of securing transactions the entire conception of the acid database that they had been able to build that in robust ways but as one writer wrote a large data store today in practice is not very far from being a grand Rite only data tomb data mining drew on the resources of multiple disciplines database and it's concerns with the storage of data and the efficiency of operations statistics and machine learning for a whole series of algorithms and just as the NSA and many people in the defense intellectual community made technological determinist arguments for why the law had to change within the broader data mining community an argument was made that the sheer amount of data both the number of points and its dimensionality would mean that we would need to transform the way that we produce knowledge that is a new paradigm with new experts and new expectations of privacy the promise of threat is very much that data mining would provide not just better truths about aggregates and a kind of traditional very very traditional undergraduate statistics way but rather better understanding of individuals as we all know from recommender engines and political campaigns and for the NSA telephony metadata it is exactly this that this defense Science Board called for the defense a research community and particularly DARPA the research wing of the DoD to begin funding and what they called for was something like this an interactive multimedia distributive exploitation and analysis Network this provided the support and intellectual infrastructure for something that became notorious almost instantly total information awareness which had this sort of unbelievably unbelievably ill-thought-out diagram of Masonic Temple viewing the entire Earth now I call this the complete irony unawareness office but i think that lack of irony about doing this it's much more understandable when you think about this framework i've given you for how common it was for certain kinds of intellectuals and policymakers in the middle of the 90s to think that we were going to need something like this so total information awareness included a whole series of efforts including or with a sort of i think most probably important in the long term to the development of the data sciences an effort called evidence extraction link discovery which was about automated discovery extraction linking of sparse evidence and large amounts of classified and unclassified data sources it will link together related items that comprise potential terrorist groups or scenarios and learn patterns of different groups and they had many many diagrams of what they were going to do in particular as this slide says they're going to extract evidence from unstructured text data and discover relationships and learn patterns of activity this is probably very familiar to all this it funded a wide variety of academic centers most of them doing very high quality work including here at Berkeley so it funded all sorts of things Mallett if you do anything with topic modeling is something that gets heavily funded mining graph dad is an early textbook funded heavily Kojak I don't actually know how many people use this Total Information Awareness envisioned individual analysts having access to the full range of machine learning and other statistical algorithms in databases that were efficient and it was particularly focused as you might expect on doing sort of graph analysis and it funded a wide variety of efforts both in government contractors but also in so this is various attempts to do say graph-based supervised learning unsupervised learning but ti a wasn't the only part of the government internally the defense community I mean the intelligence community had another effort which was called in the most bland name ever the advanced research and development activity novel intelligence from massive data in which massive data would be investigated but also the analysis the analytic tools would be applied to the analysts at the NSA who were apparently all white people from the 80s it was precisely about human interaction with information in a way that permits intelligence analysts to spot the Telltale's tiles signs of strategic surprise in massive data sources and so what's very interesting about these efforts whose exact ramifications in NSA or opaque to us is that they focused very heavily on building databases but also enabling individual analysts to draw upon a whole suite of algorithms for their development that was the goal a kind of highly personalized platform for individual data miners in the analytic people in the intelligence community now internally we know a lot was going on the NSA the NSA of course had been more concerned with large volumes of information they anyone else since the Second World War unlike most of the computing work that had been done say in the nuclear an atomic energy areas the NSA was had large volumes of data at least varge volumes by any sense and had been looking at that for a very long time indeed building up what well we don't actually know what they developed internally they're very worried about precisely the sets of questions of data mining and they're very worried about things like anomaly detection so one paper I've tried to FOIA this they're stonewalling me well maybe in five years I'll get it but they have a whole series of papers like things two things to be careful about when doing anomaly detection or why I pissed it epidemiologists are almost always wrong now as I said a few days ago Cory Doctorow on the website boing boing released this a very highly classified document from the analog of the NSA in the UK called GCHQ and it is the data mining research problem book and it gives us a sense of where things stood within the research community of NSA and its partners in the Anglophone intelligence world and i can i this is a long document i haven't fully digested but there's several remarkable things that I'll mention for those of you in the data community that the NSA was very early on friends of random forest through direct contact with someone perhaps someone right here that was advised to be an important algorithm for them um but they were very deeply worried about interpretability of random for us they were worried about the black box in nature and thought that analysts needed something else but above all the document reveals that they continue to try to be pushing the edge for looking at large scale graphs now we know from a large number of sort of national security journalists accounts that in the wake of all the new data that they were accumulating after 911 they did not have they had the storage capacity but not the analytic capacity to make sense of all the data they were finding they pushed the edge very hard and continue to do this in this 2011 document it is the biggest focus is anomaly detection on streaming graphs in real time it's equally worried about data mining about finding suspicious activities about botnets of a internet I mean information warfare or espionage around the world this is an active project and it's all done and this is more public after the sort of about say 2008 I believe it's all being done with an implementation of of Google's big table that is customized for the NSA so that they're doing Hadoop analytics on distributed clusters around the world okay now let's come back to the law so I'm going to talk about two things metadata which has been so much the focus of the conversation and then computer network exploitation which is hacking into computers in stealing data so metadata well we're not allowed to know the definition but in 1979 there's a crucial Supreme Court decision that took back some privacy rights this decision Smith versus Maryland said that tell if you sirs of telephony have no reasonable expectation in privacy in the numbers they dial so the decision said that for a given copper landline you could do warrantless wiretapping only of the numbers going out and the numbers coming in you couldn't get the content without a warrant but it made a sharp distinction the word metadata is not uttered but the idea the conceptual die did there and they argue because you're giving the phone number to the phone company you cannot yet you do not reasonably you cannot well as it says you could not have been calculus you cannot calculate to preserve your privacy it is this way of dividing up communications and information about communications that gets hacked in order to justify bulk data connection and this happens in the executive branch where metadata is deemed not to be interception it happens in the judicial branch where the fisc the secret court comes to agree with this and it also happens in the reworking of statutory law so what happens is the device that you use to capture capture phone numbers called a pen register and it's it's acceptable form of warrantless wiretapping so what happens is it gets rewritten in the Patriot Act so the Patriot Act is mostly really fine rewritings of really arcane bits of legislation but they rewrite the definite change the definition of pen register such that it is no longer just about a telephone on copper but now become something that is about dialing routing or addressing and changes signaling information processing and transformation of wire electronic communications so as not to include the content of any wire electronic communications what happens here is a potential illicit transformation in which something that is about old kind of telephone technology now becomes extended to a vast array of other telecommunications technology it in some sense creates a category of metadata out of that earlier division of telephone technology which is about giving phone numbers over copper lines to one company you've a bifurcation of communications metadata does not appear in the Patriot Act but the demarcation is here the FBI says and it's very defensive fact sheet that it updated the law to the technology so this meant if you read Smith versus Maryland the users of telephony have no reasonable expectation of privacy has moved from telephones old-style telephones to all of communications metadata a totally non innocent transformation have the technology changed yes did the technology mean this inference is clear not accidental that the definition of metadata is classified now the way courts understood this is that they denied that scale mattered so this is from the fifth court there right so long as no individual has a reasonable expectation of privacy in metadata the large number of persons whose communications will be subjected to the surveillance is irrelevant to the issue of whether a Fourth Amendment search or seizure will occur scale doesn't matter bulk is the same bulk needs to be understood as just a sum of individual if there's no privacy violation and any of those individual there is no privacy violation they say it again put it another way when one individual does not have a Fourth Amendment and just grouping together a large number of similarly situated individual not result in the Fourth Amendment interests bringing into being X nee hello again they transform the definition and then deny the salience of scale in different now one of course the classical the major transformations in sort of an older-style kind of statistics between aggregation and the kind of things that Data Miner promises that data mining precisely is about often learning to know individuals better but what's remarkable is the NSA were the great pioneers of this long before anyone else was the reason was is that they undertook in the wake of the inability to decrypt Soviet communication something they called the traffic analytic revolution which they saw now traffic analysis the this revolution was the recognition that cryptologic tack can reveal information of value even when it is successful only in recovering the external of intercepted communications in other words the great success of the NSA in the middle of the Cold War was precisely learning how to use what would later be called metadata because they couldn't read the Soviet they couldn't get the Soviet full Tet in other words aggregation really matter because aggregation allows you to understand something in SIGINT there's a huge division between Crypt illogical where you decrypt plaintext and traffic analysis where you reconstruct networks of communication without access to content this is a central domain for them and it is central to intelligence and as I say again and again in their internal house magazine they celebrate their real achievements in this traffic analytic revolution even when unable to produce plain text they could provide valuable even life-saving information to consumers revolutionize the field and in a recent history they explained it it's just like if you all you had to do if all you had with the outsides of envelopes but you had them at scale you know a lot about people now that form of reasoning is rejected by the United State the Judiciary of the United States so this is an argument the traffic analysis works precisely because of scale that logic is currently not operative in our judicial branch it is not operative in our executive branch and is not operative in our legislative branch but it is central to the practice and successes of our intelligence agencies since the war from before the the world war and they do this very well in a classified annex to the executive order issued by president reagan which is called 12 triple three it's the fundamental document that regulates the intelligence community it's incredibly interesting but in its classified annex they're very clear when they define interception that it means the acquisition by the system but not including the examination of the technical characteristics of signal without reference to information content carried by the signal so the practice of looking at the external communications then its legal foundations are set in stone long before the very salience of that importance of scale is denied in the law in the wake of 911 so now why does this matter well traffic analysis in the Cold War was about military diplomatic and political communications but after 911 it was about threats in the homeland it was massively in symmetric and went both deep into America and deep into international communications and deep into American communications okay last section then I'll wrap up the other place where there was a radical shift involved hacking into computers known as computer network exploitation so when pushed on this the US says something like this this is from the Obama administration the United States has made clear it gathers intelligence in the same way as any other states we hack they hack and this is part of the reason when OPM was hacked and the personal data of millions of government employees several of the directors of the NSA kind of begrudgingly admired the hack because it's not illegal it's espionage this too is given a technologically determinist narrative CNA that's computer network exploitation evolved as a natural transition of the foreign intelligence mission as communications moved from telex to computers and switches NSA pursued those same communications but I think we all will agree that listening to a transmission a wireless transmission is something different from hacking into a computer and copying a hard drive doing that at scale is something again their project for this is called tailored access operations and you build custom malware to enable access into routers and computers and printers and all kinds of good things and you shoot it using something called quantum but 1997 it was an obvious to anyone that this was a standard form of espionage indeed in the first document that set out the information warfare posture of the US government information the concept of warfare precisely included hacking into computers because it didn't seem like espionage and over the course of the next few years the hacking into a computer as opposed to destroying the computer deleting data those begin to be divided legally a new legal category is created one in which computer network attack a comes to be separated from intelligence this new category computer network exploitation comes to be enshrined into law and it's a fundamental division in which an attack on a computer deleting data or something like that is part of the laws of warfare but hacking into a computer to acquire information is held to be simply espionage in a document which is paid they pay an outside contractor to think through the law and they argue it's it seems oh the treatment of espionage entre international law may help us make an educated guess as to how the international community will react to the information operations activities information operations is that at that moment the current idiom for information warfare which seemed to violent if the activity results only in a breach of perceived reliability of an information system it seems unlikely that the world community will be much exercise in short information operation activities are likely to be regarded much as is espionage not a major issue unless significant practical consequences can be demonstrated so a radical technological difference that is hacking computers is flattened out and put into a category of espionage which is something that is domestically illegal but doesn't have a particular status in international law and is separated from activities of attack this is the posture that we currently exist in but we now do it not just of individual computers but it's scale and we can ask things of our massively distributed database called xkeyscore which captures large amounts of the total internet activity at various places around the world to say show me every computer that I can exploit in an entire country and we have a system called turbine which is precisely about managing this an immense scale and the documents some of the Snowden documents show that there was a push to make this hacking happen an even greater scale to make it not individualistic now many people have noted that this is a very funny kind of espionage in one article two authors note espionage used to be a lot more different cold warriors did not anticipate the wholesale plunder of our industrial secrets the techniques of cyber espionage and cyber attack are often identical and cyber espionage is usually a messa sary prerequisite for cyber attack they continue cyber espionage far from being the copying of information with system ordinary requires some form of cyber maneuvering that makes it possible to exfiltrate information this maneuvering requires this exactly the same kind of operations that an attack does so this difference is a distinction without a difference even though it is enshrined into our law now who are the tenured radicals arguing this a colonel in the US Air Force and a lieutenant colonel in the US Air Force the do not accept this partitioning of the technology into an existence kind of law this argument stems back from this this divide between the services and the intelligence agencies about things but it gets at something really fundamental that the legal transformations that have made hacking banal are not technologically obvious so in a remarkable document and one thing I'll say is that it is many of you know the NSA is filled with not just skilled mathematicians and skilled computer science and skilled engineers but people are deeply reflective about the pathologies of the agency of its failures as well as its fights with other parts of the Institute of the intelligence community but a remarkable document about the changing transformation of NSA culture from the 80s to the 21st century and author whose name is redacted wrote and as they valued in the 1980s accuracy deep knowledge thorough expertise productivity and reputation reminder that people working on the Soviet problem in the 80s were Soviet experts as well as mathematicians but then this author wrote NSA valued in the 2000 speed getting at eighty percent right now could make all the difference in saving lives and then often this really I think amazing reticle of course if it were targeting information that would mean killing innocents twenty percent of the time in his report in 1999 calling for transformation of the NSA Jim clapper concluded like this a concluding conclusion to do nothing is not an option I think he was probably right thank you you 
yvrzQ3t5jyw,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-22T23:39:43Z,Data Mining  (Spring 2016) Lecture 11,https://i.ytimg.com/vi/yvrzQ3t5jyw/hqdefault.jpg,UofU Data Science,PT1H20M42S,false,142,0,0,0,0,this the stuffing class up through up through this week okay um you can you can bring any notes you want to print you out notes you want from class but no but you can use any sort you can use any sort of electronic devices so there should not be need to be any calculations other than like adding mom that that you'll you'll need to do so you shouldn't need calculators or anything like that so i'll talk more about that later on but just to give you a sense of how to start preparing food and then the intermedia report is the next step do for the project which is going to be due the Monday after spring break and also on that day we're going to we're going to do some peer review in class as well so I'm going to have you paired up with a couple of other groups that hopefully are doing projects in it's somewhat similar space that they can in and you guys are going to talk with each other and you're going to see each other's reports your commander ones if if you have some data that is sensitive for whatever reason and you can't share certain aspects with down can with me let me know at a time and he'll figure some number I it's Monday March 21st that so let's so expect everyone in class for that awesome if you can't be in class for that let me know how to talk ok so we've we're starting so that's kind of administrative business we're now starting a new kind of small section of class on on streaming algorithms so that's going to be covered this week but then we're going to revisit some of these issues in getting the regression or the class and maybe in the draft analysis product class as well so so okay so who's who's heard of a streaming out instead of this term produce okay so few of you for this right so the main idea here is that you have some so you have some large data when you don't have access to it as normally what it's not sitting on the disk of your computer or maybe it's Leon the disk but it's definitely not just sitting in the memory of your computer in fact the data could be arriving more data all the time so what's going to happen is that you are going to be you can think of this data actually as some log string and each data element is in this this this long list of this array et al is it in this array and up to time I up to so the time hi you have access to data points say let's call this a 1 a 2 a 3 up to a five you have access to this part of the data only okay and ink and so there's other data that you're going to get access to and but you have to process the data you've seen so far and the constraints is that as your processing it your offense sitting down here and so there's sometimes you draw a picture of something that looks like a like a cpu but really you think of this cpu plus the plus of small memory on the side and the memory is small compared to the data so you can't store all the data you can store some part of what you've seen but you can't possibly straw everything okay and so you need to process this data up to this time I and and you need to fit whatever you're processing inside of this part of inside of this part of the memory and then you're going to get another data point and you have to process that and you have to deal with it and you you have a limited amount of space you have to deal with okay so kind of where this was first you know at least as I understand from our community where this was first really kind of investigated heavily and found applications was where this object here is like a it's like a router and the data all the pack is going through the route ok so the router can't store all the packets going through it doesn't it's not a storage thing it's a router it just sees them it does something with them very quickly and then it moves on to the next one it can't possibly store anything it can't so you have to so if you want to keep some basic statistics of what's happened on your data you have to do it in this model where you have a limited memory that you've allocated maybe neither the full memory of the router use some memory on the side that you want to kind of used for keeping track of what's going on with all these packets ok so this is one application another one is where this data that you have this big data set lives on disk or lives on the somewhere else on the network it lives in the cloud and you want to and it doesn't all fit in your memory so one efficient way to deal with this data is to read through it once you get one chance to read the data you don't want to you don't have random access anymore ok so a traditional model if you're looking at a basic class and algorithms of you so that you can go and access any piece of the data at a point and maybe do a modification or a comparison and this takes a constant amount of time when your data is really big this is no longer true ok so on one way of dealing with this service is not anyway is it saying I'm going to only deal with the other ones that make one house or maybe a few passes over to David okay I'm just going to read it once you bring it into memory you read through it and then you replace that with something else remember you've is blocks in your memory okay and so maybe you can keep a few blocks in your memory but you might want to have other stuff running and so forth so you want to keep the data in your local memory much smaller compared to the full data citizens okay um it turns out that the sorts of things you're going to want to keep in the memory are going to useful phrase and broader sort of the applications where you're doing things in in parallel and you want to kind of summarize the data and send these summaries back and forth we won't get into this so much now you know in a little bit into the to that to the regression part of class we will see some more advanced sort of these techniques ok so does it everyone roughly understand understand them out we'll go through several examples ok so let's let's talk a little bit more about what these data can look like and we'll stick with this router analogy um it's often good to think of you need to be careful often and thinking what does this this it actually need because you need to be careful with the memory in this case right so we're going to think of each of these data elements as coming from some from some universe and ok so again we're using this notation where there's it's from some set of size N and I'm just I can use the numbers one through and represented this could be all IP addresses this cookie all words in English language and and so forth are some there's there's some universe there's some universe of of the data that it comes from and the one thing is that I need to be able to distinguish between the two different things in in this universe so this is going to take two so the space of one this is too small of one AI is going to be roughly log up again okay so I mean this many bits in order to represent something any universe of size it okay so maybe you know if if I do various encoding schemes or maybe a more efficient maybe a mock my sub constants or something but roughly you want to think i have logarithmic in n the size of this the size of this coming now now often we'll think of this login is fitting insane 11 word up of data but what kind of you know so say it's it's you can usually map something to an integer or two to a float or or something like this and this will often be sufficient if you're using some hash of techniques just destroy these things and you maybe don't worry about these collisions but you want to know roughly how big this is and log in right right think that to think about here okay um the other thing that we want to worry about and I'll get into some examples of this this universe in a second another thing to think about is that we're going to have our data set a go from A one A two up to a Emma so this n parameter here the size of the data set and we may not need to know this thing getting in advance we need to know roughly how big this phase is going to get this tells us how many if we're going to keep a counter or something how many items we've seen that we need something that is and so that a counter is going to take log of M space okay so we're going to do things like keep counters how many things have we seen that takes log chemist ace and hump and each individual thing that we want to keep track of takes log n space these should both be drastically less than and the space of all IP addresses which we don't want to deal with or the space of all of all of our data set which is gonna be too big to fit in memory but a log n along animal are things we can do right so these are things like a single work okay so so so in an hour so what could be so that this is this universe of data you know again we can think of IP addresses both what can we possibly do with this a common goal is to keep track of the the frequencies of various titles how many times have we seen something for almost pacific idea does IP address are going to a specific ideas say there's a certain word we want to keep track of maybe it has to do with its flag for terrorism we want to keep track of that specific word how many times the depth word appear in this long string of texts that we're seeing over these documents were nuts secretly right so so will will index so we'll index into this it will say for fourth index J in this domain head will say that F J is is the frequency because it's the frequency and this is going to be let's write this in the set notation so for all AI in our in our data set so for all AI in the data set such that a is equal to jail okay so this is the set of all things we've seen in the stream that are is this jay item it's the specific IP address which is sending out up to Paul this all the spam or something you might keep track of if it happens too much it's probably something that is going on there so you care about the size of this so then this tells me the size the size of the set okay so this is the frequency okay and so a common thing is we want to maintain the frequencies of all these items to see if any one of them is getting too big if something is getting too big that's interesting okay um another another view of this is that you can cut up if you if you normalize this so if you do f of j / m which is the number of things that we've seen so far or the number of items in the data set this kind of gives us this gives us the probability of seeing item J this is the probability of seeing J right I've seen this Jaden or at least our empirical fabula okay so and what's interesting if you've seen something once this this might just be noise we can maybe unless we knew ahead of time this with something specifically want to look for with some filter otherwise we would just say that the things that occur infrequently are probably noise we care about the things that have large mouths and turns out only the things with really large mountains or the things we're going to be able to detect in the in small space okay so we're not be able to really find things that occur infrequently but things that occur very frequently we should build a pickup only using a small amount of space okay um okay so so this is the problem or the thing I guess this is is this is the setup on there's some let me mention a couple of generalizations of frequencies that kind of fit there to generalizations and then we'll talk about some algorithms and we'll hit on one of these generalizations today and another one of them in a few weeks so addition to the frequencies it's sometimes useful to look at what's known as the quantiles and this requires um um this requires it ordering it specifically in ordering on on n on this universe okay so instead of thinking about this now as the IP address of the packet which is the screen object two different IP addresses give the right next to each other needing it definitely does mean something but it doesn't necessarily say one is better or less than the other you could think about this as something like the time it takes a packet to reach from here I'm I'm from from here from the previous router some other something more continuous right or you can be looking at this data and this is the amount of money of customers spent on a website or the amount of time they spent on it before they clicked on a link right what's the amount of time they browse before they actually bought something these websites are actually trying to measure these quantities and they see lots and lots of customers so they don't so keep track of all this data they want to maintain this statistics on this on the server so you have it ordering on this and then it's two items are close if two times are close to each other that's useful to know okay so instead what we want to do is think of these on the space n kind of this is like equals to time or it could be something else like money something some continuous value and then you have these instances which you see on here which have different time values right so people spent a different amount of time on the website okay and and now it's useful not just keeping track of these now these may not ever be exactly the same you probably have some finite resolution so they probably will be the same sometimes but you don't have to think of them as be exactly the same instead you can think of this as a cumulative density function here so if you get some thing that looks like like this where it goes from 0 to 1 and this is the height of this is the probability that I've seen that the next item for the item that I've seen took this much time or less oks you can write think of this as a cumulative density function we did this you did this fear your homeworks and so far you built these so now we can think of this as the underlying object instead of just these individual counts and then you think of approximating this Q it's a density function and you want to approximate it with something else it turns out it's going to be also discreet but ahead of time you know just for the purposes of this you won't care such that the difference in the height the difference in the air between this at all times is valued so this will be some bad denair epsilon so say it's like one percent right so you might want to approximate this sort of quantity okay so this is a different generalization and we'll talk about a very simple way to do this towards the end of the lecture and then the third the second generalization is that in the first one we just had a set of items so this is where we're going to view the data as a matrix and now this elements and n this is going to be replaced by a point in in our d ok so that's a D dimensional vector so now you have more you have more than ordering you don't really have an order you between them anymore because we have a higher dimensional values but you want to think of kind of like you can think of this as being these higher dimensional points representing more complex objects ok and so then you're going to need to use different techniques it turns out that the techniques will talk about today are going to generalize towards dealing with these points in higher dimensions but has to use some machinery from SG det CH first so we'll discuss that in a couple weeks and then we'll revisit these talk about how to do it in a statement large-scale sentence so these are some of the kind of the classic problems in the second that study and these are kind of used as building blocks for lots of other things you can get your data into one of these formats you these are things you can actually approximate so okay so so let's go into talking about just the frequencies now and see how this how this works any questions for it okay so the first thing we're going to talk about is is is a really simple problem called so call the majority problem and this basically is if some FJ has a count greater than n over 2 I wanted out with Jay else I can output I can do anything okay so I want to know if there's a counter that has more than half is it sorry this should be this has more than how strictly more than half of the data okay so you know with with all the elections happening now you only care about if someone has more than half the votes if there's a tie or someone is less than half the votes I can up with something arbitrary prepare okay so this is one view of it another thing is you know this this threshold it over to will be able to relax that in a little bit but think of this as your monitoring traffic on a router and say you only care about telling some specific IP address that they have a problem if more than half of the data coming through is from there I theaters it's less than half or equal to half you say well that's bad that I'm you know like that one I don't like it and maybe this over a short period of time or something like that okay so these are it's kind of a toy problem this will use this to build up to do more coolers but you understand probably only care if something is greater than half okay so this problem should be easy seems like it should be very easy to do but we're going to do it with only 11 labels in one counter okay if you remember this label took log n space and this counter took log M specs right so a counter ism is because if they're m items possibly in the data set need log M space for counter roughly that much and log in there n items in the universe so I should go so I'm gonna be able to solve this specific problem using only log n plus plus log up space ok so now my space restriction is very very small and I can only see each other once once I see if I have to giving you a hint I have a label and a counter so I have to update that label possibly update the counter and then that's it I get the next item I have two other than what's in the label in the counter I forgotten what the islands okay okay so how do we do this so we only have one label every one captures so there's not too many things we can do right so what yep start with the first time you see what counter the counter reaches zero start with the next item please oh yeah that's it that's it so you keep one counter do you see this the same thing you keep one label encounter if if you see the item that matches the label you increment the counter you see something that does not match label your detriment the counter if your counters at zero you can put you put in the next side of you see as the new thing in the label right so let me write this down on over here so so we'll start with the counter equals zero and the label equals the empty set and you say and you can think of the streaming out as a it's a for loop before i equals 1 to m right on if a i equals L then you're going to do a see people see counter plus plus okay else c minus minus if c equals 0 or is less than or equal to 0 then you'll set C equals to 1 and your label equals okay so the first time through your only get a decrement but then you fix it with the first item counters wanted that label is that item every time else you just keep track of this one label right so let's go through an example here let's say that the 10 items are a be be be c be okay so now be has occurred four out of seven times so this is a majority was this how are we going to do so i'm going to have about a counter and a label ok so the counter is going to after each round I wouldn't count will be 1 the label will behave the counter will be to label a counter one label heah decrementing that I saw a bee it didn't match I go to zero the label is going to be the catcher's give you one label still be actually I will but clever i can actually because the counter was zero i can make this a one and then it tues is a clear what happened how I switch this to a one I think technically you might not want to you know even if you don't do that are will still work but so I can I've gotten to a decrement ative now with less than or equal to 0 so now i can change it to change the label and the counter to be one year now we get the counter to to the labels filthy he goes back down to one because see is something else they will still be and back up to two labels be so now at the end I'm going to how quickly I should say output son so I'm going to return to hell if it wasn't majority I would have gotten if it wasn't majority then I may have returned something here but okay I went to checks and may be verified or something how useful is the doesn't always return the majority well so if there is a majority will return it it was what's gonna happen all go through argument a second if there's not a majority it can be turn anything right so if instead it give instead I had said that I've written 20 just on top here all right place this with an a okay so then when nothing is the majority here instead what I'm going to get as we the same here at from this point on I'm going to get a 2 a and then a 1 a this is going to give me a C or of us I guess we'll be of one and a see because it went down to zero and then it'll be a one and A and a B so let's get return be so it's at the end of the stream so it returns that CL B is not the majority okay so I don't make any promises of STDs so that this is basically if you have to make a promise then you need a lot more space to solve this out so there's we'll see next how to generalize this and how you can make out stronger stronger promise with a little bit more space yeah so this doesn't tell you whether or not there was no no it's not telling you that but we'll get to something next that generalizes this and gives a stronger guarantee but this is just a very simple warm this is a very simple thing the very small amount of space you can still solve this specific column so think of if there was something if there was a bad event you always find it there no they're not going to any there any true negatives bring it you don't want to have someone be majority and not work work that's the only thing without protecting its ok just quickly let's go through an argument of why this this has to work if you had a so um ok so if you all we need to show is that if you did have a majority you found it right and so for everything that means if you have more than half of the items are the same value let's say be on that that means you can essentially pair up every be with the previous or later thing that was not to be okay and so that every be either and there's going to be at least one left over so everyone that comes after a number thing like one of these aids is going to decrement that a and get rid of that counter everything in em and so that one is used to decrement another counter everything that comes afterwards will that one was incremented when you saw it but Mike it might get decremented late and then you because you have a majority you have more things that you can pair up so there must be something left over which I have a positive County okay so this is kind of this is the argument why this this works there's a little bit more detailed argument in the notes if you want to see it but hopefully it kind of roughly believe this room's okay so this is capable this was the warm-up let's talk about now we want to do something stronger and this will be heavy hitters but we want to now so we turn okay so you want to return all FJ such that f j is greater than some fee x panel and sophia's give me a threshold now that that that we get the set so instead of M over 2 so he was one-half we can set this to be like 1 10 or 11 hundred the larger we said at the more space one here so all of them that are greater than some threshold but we also want to return a stronger and so we we don't have these false positives no FJ such that F J is going to be less than the m minus epsilon okay so we're gonna have this parameter epsilon M which you can think of me either exactly p.m. or something smaller so if if Fiat is fievel is like 21 so ten percent epsilon might be might be one percent so it turns out the space why actually depend on epsilon will enforce that epsilon is going to be less than or equal to feed and the space will need will leave something like the space will be roughly wonderful four cups like this we'll see ok and then but we still have some region of things that we that we can't control about if FJ is in the range the m minus epsilon m to the M then we then we want to maybe return FJ so we're still going to have the space of things where we're going to have some slack we're not going to be able to do anything about this okay if there's there some gap here which we say okay maybe I can return it maybe not but it's going to be close to our threshold ignorance so you can either set you know a bottom threshold that vm minus epsilon f of the top threshold depending where you want so so thats so this will be the goal now this seems much hopefully more reasonable but this seems more powerful thing to you okay so what we're going to describe is what's called mishra Reeves out of them and this is a very simple algorithm that's kind of an extension of this of this majority however the innotek there's it's going to solve a slightly more general problem and in which is going to be enough to solve this heavy hitters power that's not a more general problem is going to be too it's it's going to read return FJ hat so that approximate value of the frequency for all J in town right for all J in this is such stats j minus epsilon is less than half their cat is less than so it's always going to be less than the true frequency but never too much less and so if I have all these approximate frequencies you should go see how he can answer that if it is if this value is greater than p.m. I always return it if it's less than p.m. then I don't ever return okay so so so I might write so if it's less than I got these mixed up this is a little tricky so he say's for that and b hitters with and epsilon so if FJ let's see if it's greater and this is an FJ hat is greater than p.m. plus epsilon M at this if it's queer than p.m. then return if it's if it's a less just touch a hat is less than pm let's see so it always a I'll ask you something like this on the homework and this always confuses people in this you can confuse guys so if it's if it's tri-state it's less than V minus EF minus epsilon m to not return yes so let's say then so don't be minus epsilon okay good good all right so here's what we should do so if it's greater than fee n minus epsilon am I should always return because if it was if the true value is greater than p.m. it must be greater the proximate value must be greater than this right because if I hadn't epsilon M to the true value then up to the approximate value so if I looked at F hat j plus epsilon M and an added that to me right here I looked at this inequality I want to say I always return some value if this if the if the if that F hat is greater than FJ n minus epsilon M ok but if I want the threshold to be at p.m. where I always return it if it's greater than phenom this was based on this heavy hitters problem I always return it then they need to add epsilon M come to on to both sides then this is TM and is TM plus this is PM minus epsilon plus s right and I get to right-click apology okay so you have to play with these inequalities to kind of figure out where you want to get these these expressions and and we might see another algorithm I think I took that out just there's another algorithm think it's these inequalities differently okay so but this thing is kind of it's a little tricky to wrap your head around when you return it when you know but you're off by this epsilon M hair and so you say your threshold FTM with being careful with these epsilon a pair of parameters and then you can solve this heavier okay before we before I tell you the actual algorithm or before you are going to tell me the algorithm there's something else strange here that then that I claimed i can do yep what wire restore you frequency the way to allocate space for each item i plan to see why not just use actually good good so this was loves going to point out i'm claiming i can return a value for every value for every j right that this this should seem strange okay so i'm not going to do it explicitly I'm going to do it implicitly it turns out that for a lot of the values I can return something that's that's going to be in this range without storing it so I'm only going to store a set number of counters and labels just like before but for the ones where I have no labels those are implicitly going to be valued 0 okay so if the true value is small enough then I can be between the true value minus this air and if this range contains zero at a lot we return 0 so I don't actually return it I essentially if you don't have a data structure where you can ask right where I just tell you all the big ones the ones I don't tell you implicitly those are going to be 0 ok so that's how I get around this thing and this holds for all j so if actually you should think of this as i'll return a data structure where you can ask this question for all j i'm going to do something this is a little bit better and then i'll return to you all the ones which are large enough instead of giving you a data structure you can ask they'll be something we'll talk about Wednesday which only gives you the data structure access so if you wanted to know which ones are large enough you have to ask all of them it turns out you can do bookkeeping so you don't have to do that but but which the bookkeeping will look like this arrogant but you can um so you uh so it's important to say you actually it tells you which ones are large not you just have the data structure access to it does that make sense to understand that distinction that distinction so one is I can have a data structure so for every Jay I return a value I can ask inquiry a value but if there are lots of possible questions I don't know where to ask which ones are watching this is I'm going to return you a set of values and all the ones I don't return are implicitly going to be 0 this is this is a more powerful and we'll see something that I Wednesday that does something we could that ok so now let's describe the actual average how's this going to work I'm going to I'll tell you how much space it uses and then you're going to tell me they are ok so my space for this acronym is going to be k okay here k times log in us log jam and K is going to be equals to 1 over custom ok so let's break this down when I say I've k times log n plus log M what is that telling me how much what do I have space for you're right right good so this is these are the labels and these are the counters so I'm going to maintain a data structure and which is going to be these two arrays this is going to be C and this is l and it goes access into a 12 up to k or 0 to k minus 1 you know i don't care so it's at this data structure with this access into into here and I'm just going to maintain these these counters and labels okay and so initially i will say that counter let's index them by r is going to be 0 for all par and label are going to be equal to the empty set so now how's how's this algorithm going to run again there's not so much I can do they'll be one step which will be a little tricky but let's figure out the basics first and then we'll figure out this one last step that's triggered what can I do with Katie counters and K labels yeah you're streaming writing to start like filling up with labels and counters these if you're out what we're going to do what you did yeah right good so it's going to be just like the majority algorithm except with more counters and labels if I see something I have a label for i increment the corresponding counter right and if I have an empty if I if I don't see something but I have an empty label that's not being used or its counter values at 0 well then i can create a new label in that place and then start the counter off at one now the question is what happens when all the counter the labels are filled and I see something on my set okay so um so let's write down the arm up to that point just make sure everyone's clear then that will then we'll discuss this last point so for I equal 1 to M say if AI is going to be equal to some label are then I'm going to do is new be here see our + + um else and now I need to be a little bit careful if there's some see our which is going to be equal to 0 then I'm going to say now r equals to a I and SI r equals to one okay right so the first one is if I sauce saw an item that matches a label i increment the counter the second one is if or some some counters at 0 that I can take over the over the label so if I don't have a matching label and there's a counter which is Iran taking over the label saying 201 now the question is this else case what do i do at this else case where I don't have all my counters are greater or 1 or greater and I don't have a label that matches what should I do that so the good news it turns out there there are two right answers there's there's 10 different algorithms well we'll just talk about one that matches closely the majority well what did i do it in in the majority on it what do they do there I what I had threw stuff away yeah so how do i how do I throw stuff away here I decremented the counter night which is long so I want to throw out so I want to so let's say all my counters are at set I have 10 counters they're all except Carter maybe after at eight after at seven so old not boy bra yeah so so turns out you don't there's okay there's some version of this we're like you don't want to throw away because that's too much information you don't want to throw a count of seven or you want to instead what you want to decrease the couch founder like six I don't want to have what outliers so I penalized I completely throw away everything right so instead of throwing them away I want to just detriment the labels turns out i'm going to do this for all the labels and this is important if you turn also view for half the labels things will work out okay with slightly different properties but for all the labels i'm going to decrease them by one and it definitely okay so i'm going to be much partial majority i just had one label if I decremented maybe not a big deal now I'm going to decrease all the labels so for our in hey I'm going to do z CR minus models so I'm going to decrement these and then i will return my data structure okay so for all of the items I'm going to decrease the labels the counters for all of you what was the fee so these are insulting language yeah sophie was by heavy hitter threshold right but all I'm going to do now is I'm going to maintain all the frequencies up to an air of epsilon and then i can say let's say the absolute is one percent and i want my cut off for fee to be ten percent so then I throw away I only return things that are not very things 10 reviews of now it turns out you what one over epsilon you want to make sure you get accuracy on all of all of your counters so all of the counters are going to have epsilon am accuracy and then later so this is solving this approximate frequency problem and then if I want to I can apply this heavy hitters adaptation on top so I'm going to be solving this epsilon approximate any energy on the top here no here we're going to solve this this problem this mission grease pump so for all of my account my labels J I'm going to have a counter value which is a spider's property and so then to solve that any hairs with the fina epsilon I'm going to filter this all of the all of the counters that are greater than PM minus epsilon M I'm going to return those all right so think he is ten percent epsilon is one percent anything above nine percent in my in my Contras the end I'm going to return those rights oh yeah so for the heavy hitters you need this extra parameter to filter but but often it doesn't take any more space to solve this general problems he might as well return this more information okay so but this is again is the whole algorithm here this is Venetia greedy algorithm okay and so again the strange thing here's what to do when why am I decrementing all of all of the county so for all here so so for all of these of these values I'm going to decorate this isn't this getting claws too much too much air it turns out this is the key to making it work because I can't do this this step I can I get to this up too many times ok so this snap this stuff every time i do this i have i have all my counters must be full and I'm decreasing k of them so I can do this step at most at most M over K times because I know my the sum of all my counters has to be at most M and most and that's the number of items in the street and every time I do it I decremented one from every counter in the value still not negative because they were all they were all all greater than 0 okay so habits at most M over K times and so if I set K equals to 1 over epsilon then this is equal to epsilon L okay this is from setting tables 1 over epsilon makes this epsilon M meaning that for any of the counters I'm keeping track of I've increased it in most excellent times so my air is that mostest much it's it's never over count right I guarantee that my approximate counters are never too large which is true I only increment the MIFF I see something that matches the label and it's never too small because I decrement them at most this many times so the fact that I'm decreasing all the counters allows me to bound there it's pretty cool yeah what have you detriment then you have several counter yes so then i can return those but I know the labels I can skip the same but the calves are returned are zero so you can do with them as you like you know they they are very meaningful yeah so you can still return them and then you use this this feed value to filter them yeah so this is a really cool how if it turns out if if you need this much space in order to solve it if you want to solve this approximation guarantee i had on the on the previous page you need to have this much space you need to have this many labels you can't do any better and they spit a straight but um but it could be that if if they and the way to think about it i can have it most k iOS with this large of account I can't have more than that any lips right if I had if I want to return items that are occur more than ten percent of the time there are false can such items and that's how much space on i'm using right so roughly or hopped up to that much air right if so if i hit so the air depends on this is the size of the number of iOS and you can have that have this have the counter of the amount of area so the proof you need this much space requires this you know information theory arguments that we're not going to get into but you can't do any better than this space for this this problem if you're doing this mystery it's the very simple cute it turns out most algorithms in the streaming setting are going to be extremely simple but kind of figure out what they are and and how to analyze them can be can be complicated this one is is also fairly simple hand lines there are some more details i didn't over but hopefully you roughly by this argument that decremented most and what with the times and that means the errors of the most epsilon yeah do I know the number at Jade upsetting as I spider country and it can we make sure that there is a release of KF j is greater than the gen- itself good good as so let me repeat the question so can there be more than k equals 1 over epsilon J such that F J it took how about answer this question what come on laughs Oh greater than epsilon okay can I be fewer than that yeah so if there are fewer then my counter will be smaller and I can filter it later however I want to do I have some some number we go one back some something that's actually a n yeah yes so it's possible that i will output i will output a label that does not have a very big counter right and so then i can do with that like what i want right so let's say my total air on the count of these things is going to be it's like one percent right so I so I'm going to keep 100 counters and labels and so that means i have one percent air and all of the frequencies so let's say I return a frequency and I know total number of things that has that only occurs is the last half of a percent of the data has this and to our return on counter and a label that has half of a percent right so I know that okay it has at least half of a percent the true cab is larger than if i go back up to my guarantee so this is my remember this is my guarantee here for everything I return it's going to be between the true value and the true value minus epsilon so think this is like one percent right so if I reach if I return if something comes at the end and so it shows up because one of my encounters got down to zero and I filled it in with something that came at the end but it was it did occur that often then my associate encounter with it is going to be scary turn something that has to be smaller it's going to be smaller than the true values right so if the true value was only like point seven percent of the time right so if there was a fluke J the true value so this is the true one if this is equal to 0.7 percent then I know then it means that after jhat must be less than equal to 0.7 percent I'm not going to overestimate so if I want to filter at one percent or a five percent even if I return this value let's say it's so maybe this will be equal to 0.5 of the year so if i see this value i know i don't need to worry about I know this is in the north I know in my epsilon M tram partly equal to the algorithm so I know if it's less than that this isn't the noise part of the data and I can just kind of drop it okay great yeah but ok so this is another interesting question let's just come back and make sure we answer this I only have cave 1 over epsilon counters and so can I have more than J values that have account later than epsilon L then I wouldn't be able to do this right is this a problem yeah I think this is the problem okay so let's say I have K equals 2 10 that means that epsilon M is going to be equal to m / 10 and this is roughly as n % okay so now if I have 11 things 11 item so that are this is this is jae and this is the percent of the data so if this one's at eleven percent this one's at throw they're all at eleven percent so this is j equals 1 up to j equals 11 right so they're all at least ten percent or let's say it's exactly ten percent then the total % is going to be eleven times ten percent which equals one hundred and ten percent right so so unless you're playing in the NFL you know this is not possible you can't give under ten percent of something so we're okay we can't possibly have more items then we have space then yep oh my missus is like a so the fee is a modeling choice so it's up to you what you want to filter it he tells you what you care about things that occur then more than certain percent of the time and you should talk to people in networking or whatever model these sorts of say denial sorts tax whatever you're trying to financing system so so if you're just using this to say approximates on probably distribution I would probably do something like setting p equals to 2 epsilon right so then you're so then you're you're throwing anything you can say it's probably noise if it's less than half of the if what I return is less than half of my official but you might as well actually keep everything because you know you have no over counts so it doesn't really harm even hold on to this other stuff the heavy-hitter problem was something that the data people in the database community said this is an interesting important problem we need to solve and then these streaming algorithms solved it by using this more general tickers and they come up with deep parameters that our various problems but I don't it's problem specific good okay so i have i have about 10 or 12 minutes left and I wanted to show you another cool algorithm so I'm just going to do this briefly and so this is for this let me just review this old picture I have for this where you go for this quantiles color okay let's look now at this quantiles problem right so now i have things coming and they have an ordering to the think of these are times people spent on the website they are not coming in the time order they could be instead of key money instead of time some continuous variable okay and now I want to approximate things in the ordering okay again if i want to have so then i get this error is epsilon here the heights rate this value everywhere is between 0 1 so epsilon tells me a probability em off by so if this is if epsilon is point zero one then I'm off by you know a probability of point zero one okay and so there's a way to do this offline with with um sighs exactly 1 over epsilon to get this everywhere and the idea is that that you take all of your data and you sort it and you take evenly spaced boats right so I'll take every you know if if my hair is one-third then what I'm going to do is I'm going to evenly spaced points in this sort of over I'll just the orange ones I circled and then this is going to be an approximation I can we just return these points translate a stream this is much harder to do in a stream it turns out that in a stream you're going to need the number of items you keep track about is going to be 1 over epsilon times log 1 or Absalom and in just this last year someone figured out how to do this there are cracks my algorithms that are a little bit larger than water so in a stream you need a little bit more space because you're going to get some air if you accidentally throw a way a point it turns up maybe that is the point that needed so that these are bugs are a little bit complicated and I definitely can't solve them in in 10 minutes okay so instead I'm going to tell you a different problem thats related and I'm going to show how to solve that problem in 10 minutes and this is going to be kind of like this majority thing is going to keep a single counter in a single actually just just a single value in my might okay and the goal is I'm going to try and find so lets me draw this picture so um so we'll call this is called a frugal algorithm because it takes a very it's only using one counter so Trueba out of it however them for a meeting right so if I have this picture of the cumulative density function going from 0 to 1 then the point where hits 0.5 this is whatever data point is right here is the median of the data points that means that of my data I'm going to have half of it so half of us can be on this side and half let's give the other side okay so this this median tells me something about these contexts that if I know the median that's kind of like the point five quantile so that the point 0.5 quantile is equal to the median and if I could keep a lot of these instead of the point five pawns out like you keep 2.1 quantile the point to quantile and so forth then that's going to actually tell me a lot about this this cute of density function but basically I've got this value here but if I also had it and all these if I also had at all of these other values and so forth if I have all of these values here that I know it accurately not the whole distribution is in fact if they gap between these if the gap between these was epsilon this was going to be sufficient okay so but it just gonna solve a problem just just for the median and then i'll mention on time how to generalize it to other values as well and so for the median i'm only going to get one counter here this is a very kind of cool cute cute a burner okay so i'm just going to keep one counter an s that's it so I'm just going to keep a counter hell okay and let's just say it's in a discrete domain could be a large donate it can be discreet one and um and so I'm going to consult and initially let's say the counter is 0 but it might be better than start someplace else if you do something but so i'll start with the counter that's 0 or this sorry this is a label label zeros in the domain hit n so this is log n space okay and so now I want to approximate the median with it so how would I do that but the only one counter that's all i have i have to catch it at zero so i'm going to write an algorithm for i equals 1 to m okay and then going to have a simple if condition if AI is greater than L then I'm going to do well + + else ki now less than hell minus minus I will be turnt up okay so this again like super simple if I see something greater than my current estimate I increment the counter it's less than the current estimate I decrement the camp okay so if i can add the true median then all the things i see half them should be greater half of it should be less red so this should it should move up and down a little bit but basically shouldn't I stay the same and so the air bounds on this algorithm are not as strict as the other ones this is the more challenging problem you can see you can say something like so the air analysis of this one is going to be something like that my let's call I'm going to say that the L my estimate that are returning is going to be need to define this so I need to find the rank of so I'm going to get an estimate of the fraction of data that's less than some rank is the number of items which are less than or equal to help is the set any sub stack come to the Bell alright so this is instead of the frequency this is the count of the things which are smaller equal Adele and so rank / rape of l / M is going to be my approximation of the of the height on the y-axis here right so this is going to give me so I'm going to guess some value so this is the true median i'm going to guess l and now the rink is going to be over here so this is going to be the rank of hell over hell it's going to be this value and i want that to be close to 0.5 and i'm going to say that this is in one half minus epsilon 1 plus epsilon as long as so with the probability and this is assuming something on the distribution 1 minus delta 4 m so so this is this is going to be the the true median times log 1 over delta by a backup solar ok so i'm going to get so I'm going to get some other air valves that so after after this many steps okay so as I take more and more steps i'm going to get more and more accurate representation in the sauna and and so because i started 0 it's going to take at least the median number of steps for me to get there so if i start out at something with n equal something larger than this valleys connect it's going to be slow so if annuls very large this isn't going to be isn't going to be as useful because it's can take a larger steps to get there and so the more steps i get the accuracy is gonna is going to come down and then there's going to be some analyze like a random walk around the true value I think that's what these pounds come from so the one of the issues is that I increment exactly by one or decrement by one there's a version of this where you can keep an extra counter that tells you at the beginning take bigger steps and as you get closer take smaller steps so there's a version that takes two counters that does that but this was one catcher is already getting a good it's already getting some sort of guarantees and then there's a version so let's say instead that I didn't want to get the point five approximation instead I wanted to gets a point seven five then then what I'd say is that if so if I want a value safe e equals 0.75 and I want to estimate this value instead then I do this this step if I get here then I say with probability 0.25 which is one minus B and I do this with probability 0.75 so I i take this increment of decadence that proportional to how far away this so if I'm try to have something that is is is is large backwards but if I say that's a large I expect to see something smaller than so I don't always increment Thanks so if i said if i want to find a smaller quantile i expect to see something larger than it so if I see something already don't always increments and you can play with this and you get similar bounds for these sorts of contents again so it's a very simple queued up so do choose what probability you wanted via two galaxies curves Gap Road median so so this is trying to learn the distribution so what I'm doing is this fee that I edit I mentioned here at the end this is trying to tell me which of which quantile I'm trying to estimate so i can run a different one of these estimators for each quantile this if you want to estimate all the quantiles this is not quite the right way to do it if you just care about a couple of them because you want to filter say do some confidence intervals or something then that then you can just maintain a couple of these this very simple q powder if you want to maintain all them you need a little bit more space you need a bunch more counters and the housings are out more complicated object so okay so this is it for today on wednesday we'll talk about some completely different ways we'll talk about these sketching approaches towards towards the same a heavy bigger problem and also talk about some other ways of happening these freaking things so we'll use these randomized hashing approaches to solve some things I'm the be different modeling and computational yes 
g5ZUFkBLlTE,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-03-09T23:56:51Z,Data Mining (Spring 2016) Lecture 15,https://i.ytimg.com/vi/g5ZUFkBLlTE/hqdefault.jpg,UofU Data Science,PT1H22M5S,false,186,1,0,0,0,"I should find you my basically track outs like the traffic in drugs that Oh just two drafts yeah exercise of that symmetric difference where you can think of so this snippet special a long track to pass so I you're saying if there's like a GP yet repeat of something else that tracks vessels that's what I was thinking give you some and you could cost your peers he could just going to do something significant bulletins baseless your clustering be friends this is another manga you the other one there's like three canoes permission things sorry yeah it's that typing Susan experience yeah okay so let's get started so so we got some new screens and projectors these are great can actually see the grid cells the color green is working you know I've two of them so I you know figure I can burn burn the actual candles we both end I'm not sure if any to but I guess just for today I'm so excited I can I feel like using both of them we'll see how this goes okay so so so I'm so I wasn't able to be here for the midterm but from what I have glanced at them this morning and from what I heard from the pious it sounds like everything went great yeah and it's it seems like people worked where you know some people use the whole time and that's that's good but a lot of people were able to finish early and you know that's great i think the midterm was actually harder this year that it was last year but people took longer last year so so you guys must be kind of like like you know like very smart learning a lot so that's good so so any hey I kind of want to hold off discussions about that till after class so I and give any questions about particular questions or something we can discuss it after class will will grade them at some point over break and can so they'll be back to probably the Monday after after break okay so monday after break also is when your intermediate project ports or do so so you love you'll have a fun spring break up sure well you know hopefully giving you an like enough lead but so again the point of these project reports is I want to see enough stuff so I can tell you this is what you should do towards finishing it to have a good project right I want to be able to give you feedback on that and so that should be your main kind of motivation in in in trying to work on these reports try and give me enough information so i can give you feedback and so part of what we're going to do on on monday after break is we're going to have like a half class lecture and then and then we're going to i'm going to assign pairings between groups and you're going to give each other feedback on your projects as well okay so we're going to need class even more feedback before I and I give you some and so I think this will be important not just so you're going to get more feedback but you also will get to see what other groups are doing and this kind of get a better sense of maybe calibrating what makes sense to be doing or get some new ideas and you know I find you know I on I have I have certain ideas for these projects but all of you are so bright and and and creative you probably come up with even cooler ideas to give your like all of your all your classmates so so I I think this will work well okay so they'll be I'll explain how that will work out monday but so yeah so so you'll have to talk to these other groups about the project reports you have to turn them in online through through through campus but it'll be good to have something tangible to discuss with with with other groups okay so whether it's on a computer you can share with them directly classroom print something out that might also be easier it doesn't have to be just a project report you goes just showing what you're doing you know I'm flexible okay a couple other things some people have questions about the key the King mediums problem on homework 3 and this sounds like you all had a lot of fun with that question I like I I don't want to give too much away on on camera so so I will be happy to discuss kind of what you could have done after class so after class i'll be around be happy to discuss the details after we turn off the camera off of the permanent record so you saw people next year can't say right if I give a similar question okay so and then I said I'd be taking office hours by appointment tomorrow morning no one's emailed me I probably will build a making my my son sick and i think i need to say hello from him tomorrow morning so but i should be around more or less during break so few questions about projects during break you can't wait till intermediate reports hopefully I can find time to meet with you okay great so on to the edge of the lecture so 20 the other one more thing so the the notes were we're updated the start of the semester up until this lecture this one is now updated for this semester the ones after break have not been updated yet i'll update those probably over break so if you've been printing them out or something they the new version can will hopefully be off by the for the rest of the semester but they do break great okay sort so we're talking about this matrix sketching and it's a kind of again we visit what what this is about is is this we're thinking of taking this large matrix that we wanted to approximate this using something like this singular value this so this using the singular value decomposition and remember what that gave you was a decomposition into these three three matrices where this is you this is a s this is the transpose and the kind of the key thing was that the only interesting part of s was along this diagonal here and everything else was going to be zeros so what that meant was basically this whole part of you also did not matter because this was all zeros and it got x this part so this part didn't matter and then what you often want to do is take say the best rank a approximation of a and what you would do is you would set a bunch of these guys up to up to zero and so there are only going to BK things on this diet which are non zero and what that does is basically this part of this matrix and this part of this matrix now these are blocks of all zeros to which makes the lower part here not interesting and also this part that interesting so the dimension here is only k and the dimension here is only k and the resulting matrices are then called UK sk + VK transpose and the product of these you're going to get is will call AK is UK s k transpose okay and you can get the US and the matrices by just calling the SPD operation in MATLAB or many other places and it will do this and then you can get this approximate rank K approximation to a which will again be an n by D matrix but will have rain k or the data from D dimensions will now live on this cute original subspace okay so remember this is what we talked about last week on Wednesday ok so then what is the matrix sketching problem well what we'd ideally like as a kind of as a statistical sense of what's going to the data we're going to think of like these end data points as being observations of some phenomena and this view matrix is just mapping this information from s and V and log back onto the data points so this is telling us how to map from the data point so this s and the V matrices are really what's capturing this kind of big picture of what's going on and so we're going to what we'd like to do is to create a sketch or an approximation that's just sk x VK transpose so this is kind of this would give us a if we multiplied by the relevant parts of the matrix is going to give AK by D matrix and we'll call this matrix let's call this right so we can get this k by D matrix M alright so this would look like this is K Phi D and this is m and this will use to approximate a from the sense if you multiplied by D dimensional vector it's going to look similar and f as it looks if i multiply by some sex i'll be more precise about with that okay so this seems to be like what you'd want as a good compressed representation of what's going on it captures the right subspace each of the rows of here is kind of a d-dimensional basis vector and the norm of the row tells you how important it so if i look at one of these one of these rows one of these rows here i see i could even use this color before this is great this row you know tells you some direction in a d-dimensional space that the data lies in and the normal that tells you how how important it is okay so there are couple reasons why we don't want to actually calculate our approximation this way in all settings in some settings if your data isn't so huge then this is just what you would do but if your data is is a is bigger their reasons why you would you would not want to calculate it in in this way it's kind of list out a few these reasons so what is that this is on this is slow for large n and d okay so calculating the SVD of a takes oh and d squared for his section of minimum of head n times n times d squared times n squared times B so in all of our settings and was bigger than these so it's going to be 0 of n times d squared okay so if the matrix is like it's like 100 million by 10,000 that this takes about ten thousand times as long as as reading the data and this is can be too long on these very big data sets that they have inside of like Google and Facebook and Microsoft obviously so they they don't want to do the sort of operation so we want to get something that the runtime is much closer to just as fast as as reading the data or maybe reading at a few times so something closer to intensity this that's one reason another is for and interpret ability okay so if I if if i if i'm looking at i'm trying to use these rows of em this output matrix as its kind of telling me what are the important directions of the data it turns out each of these rows is going to be a linear combination of all of the original rose right so think of one what model of this matrix could be that you know each each column represents a system so it could represent a certain thing that's uh let's see each each column could be I guess something that's that's a expressed in a in a certain gene and then each of the rows as a gene or you can think of each of the column is a is some item you might buy a grocery store and each row is a receipt at the grocery store so I bought two gallons of milk and six boxes of Cheerios you know I really have the munchies or something right and you know more k it could be be something else but if I take a linear combination of these it's hard to interpret interpret what that means right so what you might want to do instead is fill up this matrix with rose from your actual data set if these are actual genes that kind of our give examples of describing of suspects so they're actual receipts from a grocery store or if this is a word document frequency those are actual documents you can interpret these in these a little bit better so I'm so can we you actual rows of the public aid okay um another reason is that the computation might not be centralized so non um some computation and this means a few things so one could be that we're processing this data in a stream so we're seeing one row of the day at a time so that so we talked about the streaming model with mainly with this frequent items problem um where we saw like one IP address at a time something like that but what if we're monitoring these these documents that were meeting on the web and they're these kind of the term frequency documents as is each row well we're looking at these receipts at a grocery store so each bros r is a receipt and we want to kind of maintain what is a good single value decomposition or for pca or for something else as this data is coming in in strength how do we do that this SVD operation you would call inside of inside matlab you give it the whole matrix at once right so if you want to update this how would we do that or um let's say the data is too big for for one machine so then their their various techniques actually to compute the singular value decomposition when it's kind of as precisely as you would on one machine if it's spread across multiple machines but a different way of doing this is to create an approximation on each machine and then combine those approximations together on a single machine the approximations should be something like this k by D matrix which hopefully you can this small enough you can fit a bunch of those on one machine so if you're doing really large you you may want to kind of pre-compute parts of it and then and then move these on to one machine for the for the different parts of that the matrix right on so so this would be another motivation and their various other settings where the data is not all in one place where you can't calculate it all at once where you would want to be able to do something like this okay all right so so these are these are kind of the motivations for various types of matrix sketching and there's some other ones as well so the lecture today will be about a couple ways of doing the sketching and what you're going to if you look back at this picture is we're going to create another matrix will call this one B and this will also have D columns and we'll use a parameter l for the number of rows and L might be larger than K but then you can sometimes reduce it back to K a little bit or it's in sum 41 the techniques l will actually a lot larger than Cain another one it will be you can you can make a change so but i'll have very very soda properties that our next patent so it will help satisfy these other these other concerns about calculating the caesar value decomposition is this black box ok this any questions there no okay okay so we're going to kind of the picture the stood you have this big matrix a and you want to map it to a smaller matrix B which is l by d here okay um and so we're going to talk about three ways to do this in two of them are going to be today the first one is going to be called row sampling and so if you read about this and like some papers or even text books the this mmds textbook so that link to talks about this technique but they call it column sampling they sample the columns instead of the rows and they think of this matrix being longer and fatter or longer and shorter but it's just a transpose transpose the matrices you end up dating the same thing i'll talk about the this a second technique will be this this iterative technique that you can think of updating this kind of one row at a time and we'll talk about it how are them called up so this is called freak the directions and so we'll talk about both of these today the roast sampling this will be really simple you're just going to essentially sample rose from this matrix and put them in this one but i'll talk a bit more about how how do you choose to do that this one is a little bit more complicated but it's actually going to look like this it should leave so it's going to look like this v she agrees algorithm for grooving frequent items and that's why it's called frequent directions and so it's going to look like this other streaming out and we talked about but as other mixed properties and then on monday the half lecture is going to be about on so random projections which where you're getting kind of and we've kind of uh we mentioned some of this in this locality sensitive hashing will be randomly projected onto a one dimensional vector we're coming in and do that in a larger scale in order to in order to map from to a smaller matrix p and each of these will have different set of advantages in properties so that they're all kind of worth knowing they're all very kind of cute and simple okay so let's first talk about the so the role sampling okay um so the kind of so what we're going to do is we're going to so we're going to have F T and T is going to be I'll write the asymptotic bound is going to look something like k over epsilon squared and I'll explain what KX how what k epsilon turned out to meet in a second there's some lower order terms i'm not going to go over so we're going to sample this many rows so I'm going to sample this video's independent right so what the picture is who's going to look like is this is a and each row some subset of rose is going to go in to be right all but another one here right so that like sample four rows independently here and they've gone someplace in to be okay so so the kind of the question is how do you sample these roads how do you do this and so we're going to set for each row of a so we'll say for a row in a and this is going to be a row we're going to give it a weight equals the norm and this is the to norm of ro ham okay so if you remember it's a row it's a d-dimensional roadie dimensional vector we can take just the distance of that vector let me look at that square distance so it turns out you want to give each is a weight and then you want to sample the Rose proportional to that way bro Porsche it all to wi ok so so we had to do this before right we hit the sample rose proportional to a certain weight right to remember his pony where did it come up it it came up in yeah he means bus bus that's right so what you could think of doing is you can turn each of these weights into a to a probability so W is the sum of all of the weights and then you just normalize them into a probability and so then you think of having going from 0 to 1 and you have p1 p2 p3 before p 5 and then stay he's 6 they each take off part of this probability space this Connolly I'm six-year right and then I can generate some random so you is a uniform value between 0 1 and if this is safe point 4 5 it goes to p3 and this is one I slipped yeah so effectively boxing for into vectors oh that's right exactly so on yeah so that's a good question right so think of the axis being here and our data kind of looking like like this and so what I want to do is I want to kind of capture the subspace that this data lies upon its like will be drawn in small dimensions but if I picked so let's say totally taking one of these points and I picked one with the small norm at the small norm so it lines somewhere here it might end up being this point and then the direction i picked would go through this point this is the wrong fix if i pick one with a large norm it had to be say one far away and now if i picked this direction this subspace is is closer to what you think is going through all the points so the ones further away tend to capture where there's the more variation along your name that's what so i want to kind of type um this is essentially the SVD remember if you if you go back to pc A one A Center the data first to do this and after that model is if the data is centered at the origin and the more variation occurs with points further away so I want to find this sucks I want to select points that kind of our end up being further way these are capturing the subspace that that tends to have the most variation along okay and so because it's the variance that's ends up being the squared distance that I'm trying to trying to minimize like the sum of the squared distances and so so that's why I pick the square norm instead Justin yeah losing count for a grade so if it's you know if it's really if you have one outlier that's really far away like if I did have one data point way up here right so it's really far away up there then maybe it's not so bad actually to minimize the sum of the squared errors I may actually want to have us the subspace go up to that point if it's even further away but the amount it turns out essentially the amount i want the subspace to go through there depends on the squared norm of that point if this one's really far away but there are a million points that are not Colleen you know not nearly as far away but there are a million different right here then I really want to go through this one even though there's only one and so what's going to happen is you may have one big thing here and a million small ones but the millions small ones kind of takes up the main problem okay um but this this works okay you know I can do this for it to generate T of these points or tea rose i should use bell here we use L of these rows because this is how the schedule work right sighs hell if I have all of these rows and I can just I built this once and then I can generate L of these he's made in veracruz this you know this is also doing this works but it's not you know this it doesn't like fit the streaming setting but there's actually a clever way to fix that they'll tell you in a second what it is doing is now each of kind of the vectors I have in B is one of the row so i can interpret it i can say let's look at the data and this data kind of tells me thesis or should be a somewhat representative set of data points so if i look at the other data points are probably close to some linear combination of these data points I mean each of those I can kind of interpret what they so sometimes that's a little bit more useful people have used this for like some understanding some gene expression data where they wanted to understand which are the actual genes which you know that correspond with these so patients that these linear combinations of it okay so how would I do this in a stream though how would I do this in a stream and let's just worry about choosing one row I just want to choose one or honest I could so I can just keep track of the furthest away row but again say I had a million points here and then this row is twice as far but off in the wrong direction then that's not going to give me quite I want to sample proportions probability just like camions plus plus yeah Ranger rose yeah okay cool so this is a whole idea that actually comes up in in streaming pretty office so you think of this is your matrix a and it could actually keep going down right and so you're going to divide up a into a chunk of size that's that's not too big once I've got in this chunk maybe we can maybe fit this old chunk in memory I'm going to create one random sample okay and then i will get and and this will just give me one row so let's call this just a one hat because it's it's not the originally one it's the first droid pick maybe i'll call this be one all right so this is my b 1 hat and then this other chunk which is not too big i'm going to random sample again this is v2 hat but i don't want to keep both of them I just want to keep one of these rows and so then I choose one of these of these two rows arms I'm so proportion maybe proportional to the probabilities right if if this one if this idea guys would have to solve all the scarred norms here and say this was a hundred and the Sun was here was 200 in this bucket so that I picked me to with probability two-thirds right so I proportionally sample one of those two in order to get this new rock and so you can you can do do something like that you can also grow them so let's say if the rows are all roughly the same same size then then you can kind of ignore this weight let's kind of ignore the details because this this technique on the scribe comes up a lot in streaming and so then you'll just pick one of those at random ignoring the weights and so then what happens is I'll get a third thing and I can't immediately combine this one because it has a different total wave the squared norm here was 100 and 100 and this is 100 and so I can pick one of these uniformly but I can't combine that one so what I do is I wait until I have a third one or a fourth one and then I combine these together and I'll say the 11 and B are 2 1 and this is 2 2 and then I combine these two together right so what you see is you kind of get this binary tree going on here and what's neat is that if you know your jump size is small enough in the size here is always at most n then the number of active elements i have in the binary tree i only have at most log in of these actives the height of this is going to be equal to log in which is much smaller than it and so maybe that's okay right so if I for instance if I had this other chunk here be five hats then the only ones i need to have active wear this one and this one there's at each level of the tree there's only at most one of these active in my in my data structure now what would happen is like there's there's really another path here right and then eventually this one would were together right there's the rest of this binary tree but I haven't seen the rest yet but i only need at least one thing from each of the levels so so so it's kind of this is kind of a as a neat trick ok so if you end up looking at a bunch of these fruit jobs you'll see tricks like this come up over and over but there's actually for droid a random sample there's a simpler way to get it in order to do this there's there's a good simpler trick that's called sir reservoir sampling and so I probably spelled reservoir wrong I've written this word like a thousand times of paper and in papers and I think over half of my Spelling's is it wrong it is wrong okay that's that's not too bad I've been much worse before like yeah like put the are on the wrong well not the first arm the second or yeah usually the second are gets in the wrong spot okay great so this this technique is called muscles called reservoir sampling and so what you do is you maintain so say wait hi this is an uppercase W there's going to be the sum of so if you're at the height on one of the stream it's the sum of all the ways you've seen so far okay and you also comes so this is something you maintain in the streaming this is easy to do you just keep you just keep us some and you add all these things up right and so then the other thing is you maintain a point so that you give your sample so you have a row a AJ which is is your current to sample and then if you see when you and so then when you see when you see rope AI a new row from a you you set let's call this B so this is our sample you so with with probability say though way to buy over this sum of all these weights then you set set B equals a I ok all right so so with probability the weight over all the way to seem so far you set a I to be this sample B they have okay so now if you and and how do you do this well you can say I can choose a again some random value you if that value you is if i multiply that W that value you yeah if the value is less than this fraction then I then I keep it if it's greater than this fraction I don't keep it right so let's say this weight was really big it's one half ours is a let's say it's one-third then I keep it with probably one third well if I generate a random value between zero one one third of the time it's going to be less than one-third so i can easily keep this with this profit that's easy to do so so why is this correct well there's there's a nice argument that if you if you write down the probabilities they all work out on have I'll give you a little bit more intuitive view that's a little bit shorter so if this was the end of the stream if this was the last element that the probability I should have kept this last item AI as my sample should have been exactly its weight divided by the total weight that's going to be its probability of it right if I look at how I define these is these probabilities up and green up there is exactly the weight over the total weight right so so I've kept it with the last one with the right pop ability okay and so then if I did not keep the last one then I should have kept one of the priorities right and so if I assumed inductively that be was a random sample from all the high- one previous ones then I keep that with with the right probability i should have kept one of those samples and so that I just need the base case that the first item I see I keep the probability 1 and so everything else works abducted so there's there's a little bit more to that arguments you can you can live the details to work out but that's basically so this reservoir sampling very simple very useful trick okay and so so then how this are going to works in a stream right I keep these rows i select each of them with probability P I and so I just I'm going to keep all of these independent of these out independent of these reservoirs samples so i might select a road twice you know so that's how to baked into this k over epsilon squared term that's why i need i need that many i'm going to be kind of that many rows and i'll explain shortly what k epsilon turn out to be so this is I'd like it to only nikkei rose right I'd like to get K rows and approximate this best ranking kind of approximation but I need this extra facts or one of rap's 1 squared and part of this because I might sound Paul wrote twice I you know I should really sample this direction that I sampled something slightly slightly lawful but there's some probability of error or I may if I may have selected this outlier with some probability so I need to over sample to make sure so but it's you should see this reservoir sampling is very very simple to implement right so very very simple bit so this is a very very easy algorithm ok so what does this give me there's like some formal air bound that lines up with why you get these camera app salon so basically what you end up getting is a matrix B and what you want to you get this drop property and I'll write this down and then I'll explain what this means okay um so you can get this property for matrix B so what does this mean okay so let's start with the easy terms so this is the Frobenius norm of the matrix you know what the Frobenius norm is this is often written with where this will make a dish simpler so the Frobenius the square root for videos from of a is it is a type of norm it's just going to be the sum over all IJ so I look at all entries in the matrix J I square you to them and I some hoes up that's the squared for vehicle so this a staffer for being storms usually we're seeing one to one half on that on the quiz those work for vectors when you have a matrix how you take norms is is going to be is is going to be different right there's there's different ways you can do it this is essentially one version of a 2 norm but if you write it to it means it means something else okay okay so and then what is this matrix here this is again a frobenius norm but is a minus AK so it's kind of annoys part of that matrix right it's the part that's not in this best reggae approximation so this is kind of the air you get by doing a rain k approximation it's all of the square of all the elements after you factored out this best ranking so you would think if this is a good model for the data then this is a pretty good then this should be pretty small okay if so now this is the term that depends on my matrix B and what this hi Abby this is a projection a so projection of a onto B is is what this is here so why does this make sense to compare well so this is really defining a subspace this is equal to it turns out that you can interpret this as the projection of a onto V okay right so this are the top K the top cave right singular vectors I got out of the SVD this is the best rank a subspace and a sub K is actually the projection of a onto those substances and so this is the same thing but using the subspace I found with d so if I took be and then I took this all the singular right singular vectors of heat then this is essentially i projected am to be instead of on to the best ones i thought okay so i want this so if i do a that as this I want this to be small if he is is a red k it cannot be any smaller than this but it can be smaller than this plus this arrow here okay so that's kind of the bounce is that the subspace I found with v is close to the best subspace that's how they kind of interpret this okay and so you need kind of these k over epsilon squared rose in order to get this sort of property alright oh this this bout is actually a little bit conservative but you end up this one over epsilon squared term is kind of annoying you'd rather build to not have that term again okay so there are ways to get slightly better battles in this but they end up being slower typically any any questions about this before I move on to the next technique you don't necessarily actually means but this kind of tells you what as what is possible with this it gives you about the right subspace by sampling the rose um okay so let's talk about this next out of the now called this is called the frequent directions algorithm and and so this this is a little bit more complicated i'm going to write down now with amit pseudocode it's going to be about five or six lines and so and then then we can kind of discuss all kind of narrate as I go so we're going to start with the sketch of E which is going to be it's going to be all zeros and will write this as 2 l x times d so it's going to be i'm going to i'm getting kinda just straight line to the fast version of this algorithm so it's going to be a matrix so this is the matrix B and D is here this is 2l there's there's a Winnie and so this is going to be our sketch matrix there's a way to do with only L Romo's but it's going to end up being is going to be a lot slower okay and so what we're going to maintain an invariant that beat is always going to have some row that's all zeros so it's going to have some row this is just going to be zeros and if okay so so now I'll write this as a streaming algorithm but you can do it in this parallel sense as well for all a I in the rows of a right so we're going to have a for loop for thing of a for loop over the over the rose I'm going to insert a I in to be and when I say insert I here I ensure there was always at all zeros row so I'm going to put it replace those all zeros with the new bro yeah so if i had a matrix a and i had to be that edit was big enough i can just transfer the rows over until it gets full once it gets full that i need to do something so so now i say if no all 0 rows in it be then i need to do something to make some of the rows at least one of the rows all zeros and so well what do we know how to do we're going to take the SVD of beat so let's take the svdp okay now this will we wanted to avoid doing but B is much smaller than eight doesn't have an intern so the runtime is now going to be roughly l squared times D and now is going to be so a small okay so this is not a color here this is roughly oh look l squared times D time so even though we're doing that CD it stops it's not too bad now let's set delta i equals s health square so this be it's the health so it's singular value of d squared right so i took because i did this singular value decomposition i looked at the alt sealer value there are 2l there could be up it was all the rows around zero so it could be up to two Ella home I took the health one okay and now i'm going to change this singer the singular value matrix s to s prime i'm going to create another diagonal matrix or diagonal upset padded with the zeros at the bottom and so this is going to be a diagonal matrix but the elements are going to be s 1 squared minus Delta I s 2 squared minus Delta I the square roots up to that's how minus one squared minus Delta service okay so up to the l minus one singular value i'm going to subtract off this Delta term I'm going to shrink the singer values a little bit and then I do that for this term this was SL squirt already so when i subtract down to I this one goes to zero and all these would shrink to something less than zero if i subtracted up Delta I so I'm just going to set all these last month's 20 and now i'm going to set my sketch be is going to be s prime times V transpose okay this is the part okay I want to approximate with the new s prime so I've shrunk it a little bit and now i've multiplied by the transpose when I get back my sketch and because i multiplied by this diagonal matrix which is mainly all zeros now this is going to have l plus 1 rows in it which are going to be all 0 so I'm going to freed up a bunch of space that means that it's going to be at most l steps before I need to go through this this if this if condition again so i took the sut but it hammer ties it out to health steps so this l times l squared ideas amortized by help so this costs about L times d times per row okay so mom so this is the whole album right it's not too bad and hopefully it looks kind of like this Misha Greaves algorithm for frequent items there I had a bunch of these these uh these counters and labels and if if I saw something I headed i increment the counter but if all my counters were full I Shrunk all of them right so if if instead of having to outros I only worked one row at a time I don't get this amortization but that you look closer whenever I add a row and then I do the singular value decomposition and I shrink the last row 20 and I shrink all of these and and so it kind of works kind of kind of like that what typically happens is if I draw a picture where this is on is j and this is the the singular value SJ these are going to kind of tend to behave very quickly right and so that's why you know usually set these 20 and the best renke approximation okay so let's say that that animal was right here and maybe these are slightly nonzero I just when I drew them can only draw so fine on here right so this was hell so basically I wanted this quantity here this little gap is going to be Delta I and my new thing is shrinking everything by this gap so these are my new cingular values in s prime so these are the values in SF on paper yes I'm just heey drew this picture of this ellipse this was the original matrix B and then I shrunk it a little bit to get the output this is s prime times B transpose so I don't care about the U matrix I just care about the s and the V and I've shrunk it in a little bit and so if this decayed really fast this should actually be a small gap on a shrinking should be okay and so it turns out that the air ballons are going to be similar as well to the mish agrees algorithm but you right you right the mountain terms of this this matrix form you can see that for all matrix for all vectors XD such that X plus 1 that and this is at the end of the sketch I can have this property here okay so now for any vector X so for any direction I look at these these ellipses that and think of this as like the frequency and this as the approximate frequency but now i get this continuous space of frequency direction to the triple x then it's bounded by the sum of all the frequencies is the Frobenius norm is turns out is the right term to use here divided by L so if i had more counters or more roads i get a better approximation it turns out you can say and this is actually with a paper with with one of my students that we show this that you can get a better balance so you can have it depend only on the tail part to air only depends on me on the tail here on the aim is a day you / a slightly smaller term but you have it depend on the tail and so then if you set the felling equals two k minus 1 over epsilon then you have it's less than or equal to epsilon times so if you said l equals k k+ k plus 1 over epsilon so you plug that in for L you get 0 1 minus whatever epsilon here and this is epsilon times a minus a cake so you get you get any area 1 times the size of the table with a with only one over epsilon rose so this is a works much better so if you if you run this algorithm state you get a much better approximation of the matrices it's a little bit more complicated you're not just sampling his rose you don't get the interpretability you had before but this is going to be a faster runtime and computing the full SPD the overall runtime here what we have the overall runtime of this is going to be 0 of n times d times L instead of O of n B squared so if d is like 10,000 and l is like 20 which is might be a typical value right you get a big savings here it turns out you can also do a process where you have a really big matrix a and i can break this into chunks and for each of these I can I can create a sketch okay and so say these are so big they fit on different computers or that you can capture them at different times and then I can think of these sketches has been stacked on top of each other and then I can compress these again into a single L by D matrix in by this by the same process on on this matrix which is now linked k times hell and all i get this i get the same error bounds it doesn't make the air any horse i do it this way i'm just delaying when I do this compression step actually so the air does not get any worse you can also say something about the subspace as well you can also get a bound that says a minus the projection onto V k of a and so this is setting l equals to 1 over epsilon plus this is a k plus K over epsilon okay so if i run the algorithm with equals k plus K over epsilon i get this care about and now I'm not just projecting them to be but I project on the first Kalos of be it turns out so I ensure that I'm only projecting on a rank a subspace the other one projected on this k over epsilon square right subspace and you get this slightly stronger air battle okay that's and so this difference you see in these bounds ends up showing up in practice we actually run the skin much tighter much better approximation but it does not have those same interpretability things I'm again not getting the actual rows here that I I got before so with the with the other the row sampling you're getting the actual rows which adds some interpretability oh ok so let's look back at this again so just so this might be something you would implement in your homework so I need any questions on how you actually she do this so how many people have not used math not use math okay so the first question you should have is oh my gosh I need to program like I need to create a data structure for a matrix right that seems like a pain right yeah so like python has some classes for matrices see you really need to build them yourselves probably I mean there are libraries but in matlab and then you know i'll try and do this kind of the week after break i guess i'll show you a little bit matlab code it's a is it's built for the corn thing you work with as a matrix so instead of having just a variable that's a number you have a variable that's a matrix that's full of these numbers and so you know if you insert a row you can just actually there's you know inserting a rose this is just one line I'm at one calling the SVD into outputting three matrices it's just one line amount right this is one line event lab I mean you can write this as one line over you if you're careful about it and and this is one matrix multiplication is something you can do right so each of these is one line and it doesn't look too different than I originally ok so the implementing this really is not it's not too hard so if you were to write this and see it would it's not too hard you need to kind of have a good day structure for a matrix who's written a matrix data structure and see before good that's it problem um so how do you do this what do you like how do you store your data yeah so so you would you know you can have a data structure that has and an array here that indexes each of the rows and then the row is another struct that your calls as a it's just a vector which is just another array that's that's good that's that's a nice kind of extensible format if typically what you want to do is call these libraries there's these libraries that are written in Fortran that that everything uses matlab uses in the back end you can call them directly from see their calls from python and em and dump I and this library is called close called lot pack and so on mod pack you actually don't start as the structure you store the matrix as a single array you just index into it at different points when you want to get to a different road you jump d elements in the raid to get to the next row so you write as a single array and then you in order to use lot that you have to figure out whether it wants it sort of rows or columns in this it's kind of a pain to use of C but you can do all this and see as well in if you use numpy and Python it has bunch of this functionality built in and in that lab everything that's just natural are I guess you know it very easy to do all this is mom okay but the homework will be meant i think we'll probably written so it's very easy to do with that okay so um so let's there couple of points I wanted to bring up about this algorithm and the other other one maybe some questions you would have so I kind of explain roughly well maybe I'll kind of alluded to the Miche agrees algorithm to explain why this works why you can get this bout here black it's not actually this should be in parentheses it's this found instead of this found this is not less than this okay so this should look like the Miche agrees found maybe I've got a roughly explain why this works it's not too hard to see you but if you're not used to the linear algebra kind of the details can be easy to get lost in but so what's what's happening is that you're shrinking a little bit in each of these in each of these directions but because I Shrunk these singular values it kind of represents an orthogonal vector here in fact if you have a matrix because there's this cool property that all right here has this is this a sign here it lets me let's write this on the side so let's say that I took the so s these are going to be the singular values of a right so pass is going to be equal to s 1 s 2 and so on okay then the frobenius norm of a the squared for video store is going to be equal to the sum up to the rink where those singular value stopping zero of the squared singular values okay so it turns out that therefore be alarm you can write it in this weather the reason why this works is because these are kind of orthogonal components of this matrix and they tell you how much variance variation is going on in this direction and the by breaking them into these orthogonal components with SVD and kind of decomposed this for media storm into these parts okay so what's happening is what I'm shrinking what I'm doing this s prime step I'm shrinking the frobenius norm of of my matrix B by exactly l or by at least L times di of the times Delta right so this this shrinks the shrinks the Frobenius norm find more than Alan times Delta I and as Delta I was this kind of some actually doesn't matter so much what Delta has that some ends up being one of this single guys but they're at least level of these directions the first L minus 1 and then the health one that all gets shrunk by this amount okay um but that means if I shrink them all bye bye bye shrinkable by this amount and i right if i read this big delta equals x by this then I'm saying I'm shrinking in total across every every step I'm shrinking the total for media storm of a B across all steps so that the total shrink is it's more than big Delta program right because each time I do a shrink its little when the little dell 2i times out so it's that most big it's it's more than big downs over Alec this is the sum of all the shrimp so that's the total shrink open up for me okay but I also know that the total shrink could not be more than so this must be this must be less than the total for squared for pinas Ron oh no this total strength three times sometimes ok because I each time I shrink by that okay so then the total amount i shrink must be less than the total for media storm otherwise I would have shrunk more than I have okay but every time I did a shrink every time I shrunk one of these these are one of these objects on I shrink it in an n orthogonal direction I shrunk a little bit in this way that's worked a little bit this way but those for any vector those add up those the total amount they contribute is a sums up to one essentially i could buy the Pythagorean theorem I can decompose the amount of shrink in each vector and I know that it shrinks by at least amount i shore can eat any one of these directions so I know I shrink any one direction across everything from A to B for any friction acts by it most buy it most Delta right Delta is the sum of all these shrinks I know I shrink any of these by post Delta so what I end up doing is I can say that this is less than Delta is less and then Delta it must be less than this if i divide l on both sides of this inequality then I'm going to get this so this is how the proof works if you're a little bit more careful and you can get you can get this baptism okay so I didn't do I go through all the details but if you remember the Miche agrees algorithm and my network then this is a similar analysis but you have to generalize it to this matrix very cool okay um well their points i want um yeah well let me make a few comments on the column sampling then we can wrap up and then all discuss the bikinis Tom if you're interested okay so with this column's with with this row sampling there's just kind of another obvious thing to try and do which is you know if I found just picking these you know I'm picking each of these rows independently of one another it seems strange if it does should you know we did the same operation in k-means plus plus but there we changed the distance function every time we picked one of these things right if I knew I already picked this subspace that I want two big things that tend to be away from the subspace next yeah so so people haven't looked at that because it's harder to implement in the stream it's not clear how to do camions bus bus in a streaming set and if you're if you're willing not to do this in a stream it turns out there are other techniques you can use that you sweat any better but there is typically roughly as slow as as doing the pause and think and only like really there one set you can kind of do fast but they don't do much better than just doing the normal sampling it most sex there are some algorithms they analyze always pick the appointment the largest leverage score and then factor that out and try and do this and there's some version that try and do this in the stream or a few passes over the data and those sometimes work a little bit better sometimes they don't so this simple trick of just picking these these rows based on the norms works pretty well if if you can do them so you don't pick the same road twice there's this cool trick which which is kind of an extension of reservoir sampling to sample multiple things but with weights and this is called um and this technique is called priority sampling or there's a various optimal that's slightly better than this but this priority sampling is very cute and it handles taking multiple things without replacement when they have weights if you're doing it unweighted it's pretty simple to do in the weighted case if you want to use this priority scheduling trick and if you do that instead that in certain cases it doesn't do any worse than this and in certain cases it will show some significant improvement um so ok I think that's basically all I wanted to say so that's a maybe we can and the official lecture here and then I can discuss some stuff about the k-means africa the sun goes up on something fun can you turn the shut this down yeah and then I will I don't want "
9PGtvZHgOIk,27,"These components constitute the architecture of a data mining system. The major components of any data mining system are a data source, data warehouse server, data mining engine, the pattern evaluation module, graphical user interface and knowledge base. #DataMining #Architecture #Warehousing

Follow me on Instagram 👉 https://www.instagram.com/ngnieredteacher/
Visit my Profile 👉 https://www.linkedin.com/in/reng99/
Support my work on Patreon 👉 https://www.patreon.com/ranjiraj",2018-05-03T09:32:45Z,Data Mining & Business Intelligence | Tutorial #2 | Architecture Of Data Mining System,https://i.ytimg.com/vi/9PGtvZHgOIk/hqdefault.jpg,RANJI RAJ,PT9M54S,false,15248,151,9,0,8,hello viewers welcome to yet another interesting video on data mining so in my previous video I discuss regarding the kdd process that is the knowledge discovery from data so today we'll be seeing an architecture of the data mining system so let's see a vertice architecture or how it looks like so this is the diagram so focus here so this is a architectural typically determining system like how it looks so here there are different layers basically so the important layers which are involved in this architecture that will be seen in this part that is on the right hand side so let's see a quick overview of what this architecture basically consists of so at the base level you can see there are many other repositories and other storage mechanisms like you have database you have data warehouse worldwide from the internet and other information repositories your grids JTP C files Excel files and all these things so this becomes the base layer all the information are stored in the databases or where the other kind of repositories are present so when we go to the next level that is from the ktd steps we have the data cleaning integration and selection activities that remains constant for this then we move on to the higher layer that is of the architecture like how it gets constructed so there we have got a database or a data warehouse server from where we can connect or while we do some querying or manipulation activities will be performed DML or DCL command activities it needs to connect to a server so that's why you have a database or a data warehouse server to be kept or implemented in this architecture so this is the role what it does in the architecture of a typical data mining system next we go up we have a data mining engine that is a DMP which is the most of the part for all the characterization and all classification activities then we have pattern evaluation module so this is basically basically used for the interestingness measure to extract in order to refine your queries and to extract that because pattern from your database and then you have the user interface where you can communicate with the data mining system like you have the end user and the data mining system at the other end so you can write different different queries to communicate with the other end system and then you have a knowledge base which is kept here so this is the primary you have the domain knowledge like past experiences or any other reference material or you have to input your current data or past history for so those are all things you can extract or you can refer from the knowledge base and you can very much use in this architecture so that's a quick overview about this architecture what the typical data mining system architecture looks like so then we now have the main components so to talk with we have this knowledge base and data mining engine pattern evaluation model and then this user interface so right so we have the knowledge base here so DK stands for the domain knowledge which is used to guide the search or evaluate the interestingness of the resulting patterns like you have different different queries or with different different motivations we extract different kinds of knowledge from the database so for that we need to have certain kinds of past experience or transaction histories or something like that to drive our particular query search so for that we have this knowledge base basically so that is kept here and it basically consists of concept hierarchies so what concept arc is does is it organizes different attributes into different levels of abstraction so you can get a view from at different angles or different sides of a cube you can get to know like bottle parameters or measures are required to segregate or to classify this kind of particular search and then you have user behaves which assesses patterns interestingness based on its unexpectedness this is something like you have a new scenario which is given but there are no particular algorithms but only you can have trial and error mechanism to evaluate that particular processor activity so it's like consider the case of human being like we have got some new scenario like we don't have any particular algorithm or steps to solve in that scenario say for that particular case now what as a user what does a human being will do he will assume it some perceptions like take so-and-so constant says so-and-so and then he will try to solve we will try to approach that particular problem so what he does this he basically search for this knowledgebase search for that particular query in that knowledge base and then he will try to extract certain and soft patterns or interestingness measures and then he will try to apply it to its his own process for which he is intended to so that's where the user belief is there and if we get some kind of knowledge from that or violence user belief is proven to be true then what he does is he updates into that knowledge base so what this arrow basically this bidirectional arrow loop at an evaluation is that it basically inserts anything which the user gets during his journey or it can even refer so this is basically a reference referenced from knowledge base for any transactions to proceed further it basically refers said knowledge base and then it applies to the pattern evaluation process similarly to the data mining itself he can also refer that particular knowledge base and he can drive this social cells so here know insertions or push requests is there but only cool he can only pull some kind of information of knowledge from the knowledge base here you can do both the push and the pull activities so then he can have interestingness constraints thresholds and metadata so all these things can be then in knowledge base so metadata is nothing but heterogeneous information from different different kinds of sources of data you have the pitch data you have Word file data PPT did PDF data then database then grid daytime so and so so all those kinds of informations are metadata so it's like data or information about information or data about details called as metadata next we have the DME that is the data mining engine which is the heart of this architecture so it has a set of functional modules for tasks such as characterization Association correlation analysis classification prediction cluster analysis out layer analysis and evolution analysis now characterization is used for characterizing on base four different categories then association or correlation analysis it's like a kind of relation like if parameter is equal to this and if this parameter is equal to this then you apply some composite rules or singular rules for extracting that kind of patterns then you have classification classifying based on some set of constraints and prediction then cluster analysis then out layer analysis in there there are any kind of noise or inconsistent data within that database you use out layer analysis and evaluation analysis finally there are any kind of regression activities which have been previously done it is not reflected then you have to do evolution analysis like if there are any changes which are done in the database then you have to evaluate it as an impossible next we have the pattern evaluation so that is the layer about the DME so pattern evaluation employs interestingness measure like I've said and interacts with a data mining module and it filters out certain discovered patterns based on its previous history of previous transactions and this module that despite an evaluation module you can actually integrate it with the data mining module so that all the push activities are the push data can be confined to search for that particular pattern like whenever any new data is like extracted from your journey what you have to do is just have to push those data to the data mining engine so that your search is confined only to that particular process which are relevant to an analysis task so you will not go anywhere diverted from the task but you will only be focused to your search only next you have the user interface so that's pretty much simple it communicates between the end users and the data mining system via some queries like data mining query you can have detailed data control language or data manipulation language DML commands or any other commands which you want and focus on search the exploratory data which is there or you can either browse the data warehouse or data stream that is vs and evaluate the mine patterns and finally you can visualize those mine patterns into appropriate forms like in aggregations or some very resinous so these are the different blocks or the important building blocks of a typical data mining architectural system so well that is all about the data mining system architecture so hope you enjoyed this video if you got educated by watching this video please the like button and don't forget to subscribe to my channel thank you for watching my video 
NcKRg44bPKE,27,WEKA Lecture 1,2019-07-23T16:50:20Z,Data Mining Application WEKA Lecture 1,https://i.ytimg.com/vi/NcKRg44bPKE/hqdefault.jpg,Charles Edeki - Math Computer Science Programming,PT44M1S,false,146,2,0,0,0,welcome to Sunday so in this we are going to use so the objective of this assignment discrete analysis to analyze this application and we are we have one person using classification to question so there are different questions the moment we are so what we do is that in our Senate Allah it is already we are half as editor but next language we are going to use a lieutenant analysis again sorry for the sorry after foster so as we said in this in our files in there now you have a use for assignment and as you may get a figure that comes to the Vita is already a our performance a read on YouTube but normally if you have a test by NTU they are a performance this is a sentence so first we start with a keyword at relation then the name of the file so the name of these valleys as easiest sympathize with the dashes no file extension there is the name of the file so it start with a keyword activation then they may notify then you know all these side reaction this is now a key or at attributes the name of your attributes and also the data type so here we are th the data type is numeric so in this case these are the attributes now in the next section which is the pronunciation is the king words at data so one important thing is that the order of the data or the record should be the same ad system sex consequence of sex is more chest pain I can tell a general cholesterol levels numeral so you have to exercising use argument no [Music] [Music] so now the process and prepossession of the data would wake up with chronic filters for discretization discretization means messing we have 100 patients and we made like age varies from 10 to 60 and we have each but discretize into only three levels so if you are between the age of 10 to 10 so what I'm doing than putting in the age this is what we call discretization getting a height between 42 so we also have normalization normalization as a federal ace is when we have values that intervals are buried in example other value in one arching would be 20 I mean the defense's didn't so the place each contains transform and also so when we started look at this is what we see so we found the preprocessor preparing your data which we call the preprocessor then do your trimming and I classify cross for Association and after that so pre-processing also we see the temperature yourself ranks so the first thing we're going to do of course we need our menu which is [Music] [Music] we have [Music] [Music] [Music] [Music] so instead of using the unique values each first we finish the preparation and the second step we are going to do the classification expectation so in fact we were continuous a last time but looks like so she seems so when he saw the classifies we discussed about this discussion [Music] [Music] [Music] [Music] so the next thing is systemic water die available materials decision tree when is the Magnusson [Music] [Music] we constantly we are going to use this - what is the percentage so Els I use in Sochi allow us to use too I'm going to [Music] [Music] [Music] [Music] [Music] this is good [Music] [Music] [Music] so so so we could start [Music] [Music] [Music] [Music] you have a data set you want to find as we have similar separator so my claim is Christ the King king most important values distance only certain patients in this medium going to conserve affinity and this one should be very small so that's our concept so here we choose classes request our valuation [Music] so the choice so I'm going to end here and again if you have any question about the assignments often I tell you I'm going to do this designate just using they're not work yes so again thank you very much if possible 
J6u7TsP9Zbw,22,This video introduces students to the concepts of Data Mining and Data Wareshousing,2020-06-22T21:02:43Z,Data Mining and Warehousing Lecture 2,https://i.ytimg.com/vi/J6u7TsP9Zbw/hqdefault.jpg,ZDS content,PT1H50M16S,false,12,0,0,0,0,at a place even in this country where we don't have to wake up and go to the office for us to work but we can work from wherever we are and those are thoughts that we may need to think about so even as we continue that if you have any question you can raise or if you have any comment you can you can chat with us in our Chapel but also the chief digitization of data and the establishment of data repositories has created a new term in the field of information system and that is basically that because we had we have digitized our way of operations in organizations we can be able now to think how else can that data be used or how how useful is the data that we are using in the organization's then the other thing but I need maybe some few definitions that I need to mention is now the key definitions here is on data warehousing and data money because the cost that we are to talk we are talking about in this in this class is just about that how can we store historical data but also how can we mine or how can we get retrieve data from multiple sources and so key definitions that I want to mention here is on data warehousing which is the auditor house is a subject oriented and we'll talk about features of data warehouse subject oriented integrated time variant and non-volatile collection of data in support of management decision a management decision-making process so we are not just storing data here data warehouse is not just putting data you know helter-skelter but it's also being king on how do we store that data so subject oriented such that we are looking at that data from for example marketing department data around vanity data around sales data around our customer data around this product its subject oriented but also its integrated because we are getting that data from multiple sources the other thing is also time variant that as time goes data keeps on changing and so we can we also want to view that data with respect to when that data was mined or when that data was thought non-volatile also because every time we store data in warehouses we don't want to have that data altered and a good example would be Kenya National Bureau of Statistics for example would not want a case whereby you get sensors like the one that we had recently and then ten years after that the data that you collected has been altered because then how how would you be able to compare the two because the point here would be gather historical data and compare and they do some meaning out of that so we want the data stored in data warehousing to be also to be non-volatile allow me to just check to by the end of this unit are we going to learn how to collect social media data say someone is interested in knowing how many people used their hash tag okay what I would say around this is out I will introduce you to various platforms that I used for data collection Aida we are talking of text data or we are talking of of text data or we are talking of numerical data I'll introduce you to various platforms and then also various platforms that we can use for visualization and then now depending on your needs you be able to determine which platform or which one works for you for whichever need that you have and that that's how I would respond to your question Nelson yes so data warehousing is key in this in that sense that we wants to have a way that we can collect data data that is subject oriented so marketing sales customer care all that as opposed to data about Z Tech University because that would not be a subject oriented data but the way we store data is in a sense subject oriented and even what Gugu is doing and I mentioned this last week what who is doing to be able to store the data that will restore it or rather to gather data from multiple sources so you sign up in code Academy and you're asked to sign up with with your Google email accounts and they pick that data and that data is translated into various subjects but the primary key of each and every of your records or data about yourself the primary key becomes their email address and so subject oriented is very important but also integration of data because the data that we are talking about here is coming from multiple sources and so we may need to know if we are collecting data from 10 universities in Nairobi and that data is the data is is for example coming from Z Tech University K u USA U K CA and all that the way we name registration numbers so for example we are talking about data for around students the way we name registration number in our institution we know necessary be the way Jake watt names or calls registration number it may be admission number or unique number or whatever yeah and so how do we integrate those two when we collect data those are key key you know concepts that we may need to even discuss in this class but also how can we be able to look at data from the perspective of or in relation to time when the data was collected and how does that have an impact in the decisions that we make burrows own and volatile of course like I see the other thing that I need to mention is data mining I mean not mention about all the all the amino be able to mention about the okay I can see our proof is here welcome professor Django Molina thank you for joining us for this session we are learning about it a mining and so you're very much welcome so so what I was saying is data mining also it's a non retrieval process of extraction of hidden previously unknown and potentially useful information in large databases and what are what does that mean what we are talking about here is that yes we have from various sources data from data warehouses data from transaction or databases data from let's say even transactional databases data froma whatever sauce that you are talking social media like Nelson was talking about and all that data we want to be able to extract that data but also not just to extract but also get the hidden and previously unknown information something useful from that data so let's give an example that data from us award let's say from Z Tech University yeah so we have collected data from Z Tech University and that data is about students and we are looking at enrollment of students in in Z Tech University and we want to get to understand how do students enroll or why why choose Z Tech in the first place for example and we can be able to analyze data about students get to know for example whereas we have the major markets where have we taken the major market share from Yahoo students in various you know areas of this country or where do most of our students come from yeah what motivates them to come to Z Tech University those may be you know hidden previously unknown and potentially useful information for z-tech invested to make strategic decisions around around around around their next move as far as probably marketing is concerned or as far as their their business is concerned and so we need to so it's not just pulling data on getting data from whatever sources but also we want to be able to analyze that data and get some useful or potentially get information that can inform the next move that we are making as an as an organization and that's very important and if you think about it even from that angle then you may want to understand what I are saying that this class is practical and I remember some of us who are asking so if it's practical then how are we going to learn online and I was saying the sense it's practical in the sense that we will have to or rather it's very very applicable in the real world way we are in and many of the even examples will give our examples that relates to the day-to-day affairs of I mean the day-to-day affairs a tree we run each and every day and and of course you have feedback and comments will be very variable in that sense so data classification is also very very key term characterization and classification here is a task of formulation of models functions the best described or distinguished data classes so being able to know this data can be classified under this because also we want to classify data on subjects so being able to know this better fits in this subject and this and this that's also very very important Recker ization also which is summarization of general characteristics and features of the target class of data so you are able to characterize characterize data that you are getting and be able to summarize this data should fit here and here and here because also will be learning about outlier analysis and we will need to law so whichever data that does not fit where we want it to fit what we do in each and that's also very very important discrimination is also very important concepts that we want to talk about and this is comparison of general features of the target class of data data objects again is the general features of the objects from one or multiple contrasting classes so being able to know that this is a general generally we would expect that this curve looks like this but this is the data we have and how can we be able to compare the two and see what is the discrepancies or what is the difference between the general curve how the general curve should look like and how it is and those are also concepts that we will be looking at prediction also and tradition here is after we analyzed data and we have gotten some useful information can we predict how the future looks like based on the data that we have currently that's also important the other thing is outliers and outliers here ism is very very key and we'll talk about outlier analysis because every data today I think there are many many algorithms that are coming in and they are trying to to demystify the concept of you know doing away we go out the outliers because it's known as AI that all the outliers are wrong because outlast is basically this is you know data objects that they eat from what exactly you would expect or the generic behavior of the data model you are using and they are new models that are coming in that are trying to demystify the fact that all the ultras are wrong because I may not be the case that all the outliers are wrong there may be some of the outliers that we can be able to deal with some some of the reasons why outliers would be there within the data what do you think why would we have outliers probably you can type in if you have what are your thoughts around why do we have outliers within data's data sets or data the data that we are collecting what causes outliers for example yes you can type in there let me see some few comments you see also it's possible that in such a forum I can be talking to myself I know that the moment I get feedback I know I am NOT alone so I will actually be sampling so let me get feedback from the following people so that I know while still here with me this is a huge class Paul Mutunga Paul Mutunga please give a comment there give a comment Paul Lujan ISA give a comment Edward give a comment if son keeps away keeps on keeps I'll give a comment so quick comment I'm receiving is one Hasan is saying data entry errors and I want to agree this and I want to agree this because data entry errors in this case would be for example we are human and sometimes you may want to enter and you may have talked about this in system analysis and design where we talk about concepts like minimalism and all that but the way we design even piece robust interfaces we have today we may design in a way that we try to minimize all the errors that may come with that interface but still we get errors he actually get errors and that could be someone was collecting data for Kenya National Bureau of Statistics and they were told that the age of a person is 30 yes but when they were entering that data probably they there was an additional zero on the data entry and that can be an issue there could be data processing errors and those could be now based on the algorithms that you are using or the functions you are using if the function has an issue or the function is wrong then you would expect some errors in data processing they could be the tools that you are using as well and that could cause some errors the other one is variable variability of measurements that's an interesting one key prop and I don't know the variability of in measurements what you mean by this but in my opinion I think water what I would pick and you can confirm or rather you can type your thoughts yes I can see you see you're talking about the tools that are used but in my opinion outs yes see but the truth the measurement tools but also there could be I'm using this particular SI unit or I'm using meters and they have a person a system is using centimeters for example in that variables or the change of variables can actually change or can alter the end results that we get good thoughts right there good call and we never answered Edward are you there okay you are there honey sir are you there if you had a SAS if you're not then I will assume you give me a screenshot I mean a screen saver and I will act appropriately Cammisa idea do a mixer boo can you hear me who can you hear me oh that's not I hope that's not a screensaver anyone else I mentioned and we have not responded so I know I'll be something as we continue the other key one that has been just mentioned by Felix is changes in circumstances during development what would that mean please type in okey oh you're here please type in what your thoughts are around that for me to get clarity on what you mean yes as we continue but what the next thing that I so I want just to ask us to have a bit of introduction on both introduction to data warehousing Barroso introduction to determining so NDT warehousing is a relational database which is developed with an aim for query and analysis rather than transaction processing now remember data warehouse is not a transaction or database for example if you have a point-of-sale system in Toulouse my screen I able to see my screen ok let me know let me get back now ok its back so what I was saying is processing is that for example in a point-of-sale system you would expect that data that you know work a customary prison or or a salesperson Luise what we call these people the people that serve us you know on the counter that that person is using a transaction or database but we can pull data from transactional database and take it to our data warehouse and so that data becomes a non-volatile data that we cannot make changes on that data we are just storing the data but of course we also storage not on the way it was in a transactional database but we store it based on the subjects that we are talking about the some things that we are talking about and that's very very important so remember data warehouse contains historical data that we have committed over time that is derived from a transaction or database or from multiple sources various sources that are there but also data warehouse is a single version of truth truth from a single version of truth for the organization and it's created for the purpose of helping in decision making market market analysis for example forecasting prediction of the future forecasting the future those are the reasons why I would have a data warehouse and remember now we are linking this data warehouse with determining because we are mining data from a data warehouse we are mining data from a database or rather then data that remain we mined one of the sources of that data is a tutorial it can also be mined from a transactional database for real-time analysis where we want to see what is currently happening and yeah and the last time I talked about waldo meter which was which i mentioned while Tornetta come this side i talked about waldo meter which actually gives us real-time data on how many people are like today in the whole world how many people have died today in the whole world how many people are have been born in the whole world and they have added a feature for coronavirus india various gives us quite good analysis there I also talked about the features of a little house and I talked about these three at least four features so subject oriented integrated time variant in non-volatile so and this is just what I was talking about I mentioned this so subject oriented focus is on a subject rather than an ongoing operation so focusing on sales on marketing or distribution one particular area integrated which is which basically means that you're able to match data from multiple sources non-volatile that data was loaded in the warehouse do not change but also time variant that focuses on change over time hence enabling organizations should study the trends and changes and provide objective facts are based on the same let me try and just move to the big the big question here is how do we analyze data because we can't avoid or we can't run away from data within the organization's today but the question is is that data becoming variable to us is it is it helping us to actually meet the objectives that we have and that could be very very important for any organization of course yes there is the operation of data bees and I think this is something I would want to put emphasis in our next next class but before I do that let me just mention that there is the operation and this is something I've mentioned also is the operational database porosity is the data warehouse and we need to differentiate between online analytical processing systems which deal with data warehouse and online transaction processing systems which actually deal with the transaction audit and so in many cases the data that we have in warehouse is most really part of that data comes from transactional databases and that's also very very important the okay if you have any question or any comments please type in as well wow I didn't know this this class has grown to this to this level um quite a number of us I know for years are keen on finishing school so we have to make it count by all means but it's good it's good that we are here so the differences between operational data base systems and data warehouses okay please just a comparison and let me just go to this because online transactional database is actually an operational database but online analytical processing system represents a data warehouse and just to differentiate the two to differentiated Israel by an all LTP online transactional processing is based in a customer oriented kind of a system so all kind of data the data we deal with in online transaction processing is customer oriented but in analytical which is the data warehouse is actually market oriented so we are looking at markets and so it's possible that the marketing or if we are storing data about you know marketing of learning institution like GTECH it's possible that we don't have just data about our university z Tech University but we can pull data from other sources and see a comparative analysis of the two because it's a market oriented it's not a custom oriented also of course things to do with system integration so that you can be able to integrate various systems for an online analytical system and Link data that is coming from multiple sources but also in terms of data content we manage current data in online transactional processing systems Paris but in online analytical processing we manage historical data so we summarize aggregate and store information from various levels from multiple sources and we keep that over time I won't be tech that of God for therefore the country but the country or the Kenya National Bureau of Statistics with the national data hast data you know dated back in 1950s I would expect that that we still have our data somehow reliable to capture if we are keen on what we are talking about because that allows us to make informed decision that we can see how our growth the growth of economy looks like by just comparing how that trend has been over time the database design in online transaction of processing this you know very well that we use entity-relationship model erd models especially we are talking about a relational database but we will talk about the models that we use in the data warehouse and some of those are like a star model snowflake model you know a subject oriented design as opposed to you know an entity relationship model or an application oriented model then you will focus on current data for an online transaction processing but for but for for an online illogical processing it is pans over multiple versions of database and schemas so we are able to integrating from and that originates from different data stores and bring it close home a quick question from Baraka and this is the rockin is that your name like your call Baraka miracle that's an interesting one I've never seen that before okay I hope so what we are saying is the question that Baraka is asking is on which category do we have the CRM let me get your responses before I give my view just bring in your responses and our sample a few guys Wilson give your response on this what do you think you're okay give your response Antoni Matua give your response prudence give you a response and Osman please bring in your responses what do you think does a CRM where does a serum how can where can we place a CRM is it an online transaction of processing or what is a CRM a CRM is a customer relationship management system okay so it's used to manage you know how you connect in YouTube a very very very common one is for Dietrich's the bittley's which is available on YouTube YouTube Newton able to see totally a customer relationship management system so Newton things it's an OLAP it's an online analytical processing system aha Ali thinks the same keep them coming aha I like you are your thoughts Nelson coos et actually explains why are we saying it an online analytical or rather an online analytical and in an OLTP yeah it's fetching data from so the question is if you say OLTP fetches data from an existing database does I make it's an OLTP or all up okay okay please even if you are typing if you are seeing it on on up give us your thoughts why are you saying that so inner let me let me actually open this and you give me I'll try and finish this in because we were late bye-bye 15 minutes to 4:00 I'll be done but I just wanted to see if I can be able to paint a good picture one of the things I would want you to check is Beatrix I talked about Beatrix which is a good example of CRM so I am actually torn in between on how to to define this because for a CRM on one side for a you know a typical CRM a serious CRM on one side we actually captured data real data from the customers Posse RM but on the other side of a CRM is that we can be able to visualize data that has been stored over time from a CRM it's possible and another I wanted us to probably get to understand especially even those that are not very clear on what a CRM is cuz like a promising CRM manages current customer data it's it's true that current customer data is actually managed within a CRM but also it's possible to visualize the cleanse of them buckets from a CRM some of the CRN's of course yeah it's possible that we can visualize how the trend is going and given a large to someone on I think we are doing well beside and this other side we are not doing well so out really out place it more of an online transactional processing system because most of the data is current data that we use in CRM most of it is current but also it's I think it's there is a comment from Ali with ring as data in OLTP is operational it is used running and controlling for the mentor business tasks please let's see let's see and I need us to understand what we are talking about let me get a little bit back here just for us to understand here so operational database systems is actually what we are talking about when we say online transactional processing systems and data warehouse is online analytical processing systems and so just try and get the difference yeah but this for online transaction processing systems we are talking of real data so if we say that a CRM is an online you know transactional processing system then what we are saying is that that system is actually using real data yeah and more most of them out see even including the one that I am talking about beatrix most of them they actually help you manage your customer in many cases they help you manage your customer sorry I lost my screen again we help you manage your customer and so in many times I see a rim if I was to answer that out place it more of an online transactional processing system okay just before I finish to this session we'll talk about the architecture of data mining system and if you look at this you can see that we have that the GUI at the top whereby we want to visualize we want to visualize data you want to visualize data so we have a CRM I mean are a graphical user interface at the top but also we can be able to get the mining eg in the pattern evolution and also get the database where the data is told and so we've we gather data from the data warehouse the transactional data database and then we are able to get that into the mining engine um you know categorize whatever you're getting also use the knowledge that we have the knowledge base that we have from previous information that we had or from other sources and combined with that so that we can visualize whatever we want to see at the very end of the day okay the last part of our discussion today is is also now they just understanding what or what we've just talked about so the database which houses the inputs that we get into the mining engine we need data cleaning data integration because we are getting this from multiple sources so we may have a transactional database we may have a bit of a house and we want to to read to remove any inconsistencies that may come because of data coming from multiple sources the knowledge base of course which evaluates the resulting patterns guiding the data mining process and pruning because we have a historical behavior that we have seen in the past as well as the data is concerned and that is important the mining engine which forms the core of the system and implements the specific data mining technique so we talk about classification clustering addition rule mining among other algorithms that we can use on a data mining engine and that is where now you can be able to cluster data you can classify and get to bring out the meaning that you want to get out of it the other thing is button pattern evaluation component which incorporates various interesting measures to single out the outliers very important - that whatever you are giving is clean data so you have been able to single out any outlier that exists within that but also the interface which helps us to visualize the data that we have already been able to mine yes so according to gopalan and this is a book I talked about and I said I'm also going to incorporate some of the thoughts of this author with a new book I got an e-book that I'll be sharing with us after this class on data mining and another one on data science and yeah according to him this just phases of data mining we'll talk about most of them but also we'll talk about if we talk about data mining or a determining what are the algorithms that exists in there and some of those would be as initially brutal mining algorithms clustering algorithms classification algorithms and just get to understand what and how can we use them from a conceptual point of view and then now also see where they are used in day-to-day life so these are concepts that I am just introducing but I can assure you that they are concepts that we look at it at them in detail as we move on I have some review questions in there and these are some of the questions that probably you may expect in some of the assignments and I mean some of the positions that we have as we move on and so please try and attempt them but also I'll sorry I'll give you I'll give you some discussion forum immediately after this where you can throw in your your thoughts around what we have just discussed today yes and I'll be able to also share my thoughts as well question I'm getting from Antoni is how will we submit the assignment the assignment is going to be most of the assignment to be submitted by elearning so if it's a written assignment you will I'm not saying whatever is here is an assignment and I know the last slide was showing assignment 1 let me see let me see that what I meant what I will give you a multiple choice question but actually a verb it's a verb it's just you understanding so don't take this as your assignment one for now I'll give I will translate these to an assignment that you can do on eLearning which is going to ever with the same thoughts as well and you'll be able to give answers yes I can see the topic of questions yeah these are actually topical questions Nelson that's typically like it's these are just topical questions to evaluate for example and talk about what are the three major research issues around data mining that you can talk about that if you had to to do a research paper right now on data mining what are some of those things that you can bring out yeah how does determining differ from conventional retrieval of data for example what are what are the differences in the area so those are just review questions to jumpstart us into the discussion that we are having ok in case there is any question please bring it on before we end but I want to take a roll call so let me let me prepare that in one minute and then I will require you to either respond by armed on your mic or I'll require you to to type in as I attack their own quarry so let me set that briefly for just one minute I hope my network is the winner a little true if you need to unmute yourself please you can now that we have just finished feel free to unmute yourself if you have an a question you can also ask either by speaking to us or even as I now let me get changed today we captured or gaining confidence that this is doable but we will learn because the worst thing that can happen for a four point one student is to be told that you have to eat for another so this is doable right now we can how we will be willing put on this foot part but if we actually put our best foot forward and I think the best thing about this platform is also that that's for each party it's possible for us to monitor how am i engaging and in learning that's important keep rope you're here I saw you Antoni I saw you on Maundy and boy are you here Lisa V well type in hiepro keep quiet  come on D is it in come on you [Music] absent okay does your chain a chain world a muddy brand he prob Clinton please if you hear your name type in all the present yes let me just read our names keep rope Clinton keeps keep keep cause k : skip to buckle areas Sleeman present omadi brand prop Christopher Clinton because K : - Buckhorn areas all those people are not here so if you're not here to back on areas yeah keep your comments coming I am seeing them but I just want to finish this I saw Edward layman I haven't seen you ever in Korea Jamal monkey LD yoga Eva post K faith telex I saw you drawings here escape please type in that your presence Paul I can see you I need to know that you're here well if I know I screw you money hustling because such an easier yep Manya [Music] [Music] Franklin Tsonga class joy lean approach Justin son I saw you killing Joseph Joseph among yourself the rain is not here just in Korea one boy Linda lucky Collins I see you when I see you from boy Linda Brandi here's some presents type in type in you can type in here I'm waiting for brand puggy you to type in okay I can see you on the audio this order to confirm oh is this Kevin Malaya [Music] mark ISO unison I'll show you CBI Mara 9 my comic I mean God is not here my comic are you mommy shall I show you  I show you jameelah I show you my monsters haven't seen you earlier I saw you moriki I saw you when Temagami you're here on Xmas Melinda honest mass Makita qiyamah Moss's Wow let me go to the last page this is huge but it's fun the last page has okay type in your name if you haven't heard me talk talk about you so new Torah here Casper I haven't heard about you to do my co Dorito Naphtali Paul Mutunga TC are you here Brandon oil Wilfred let me mark the start up of taking oh can you turn I can see you [Music] Brian I can still exist Stacey you're not here Mildred Paul Mutunga is here and afteri not here Michael dorito not here karaoking digger not here Nelly kill not here not here not here Mildred not here AB buckle osmond think I saw you yells huh Oh catch Paul not here remote moaning the am Bregman oh okay of the humble plenty here Robert Morgan see he bet Victor Jacobs Emile Kevin Shaka bogus robots so anyone who has not had their name Stacy are you just logging in now I've called you a name so many times we just logged in I will confirm that from the system the time you logged in Jason okay if you've been here come from but let me take for now okay I'd only there's anyone I'm just about to save and then I will exit I will allow you to leave okay so yes I had about a question or on on the issue of what the issue of little bundles I think and I just I I can follow up Lots then follow up that and let you know yes I will follow it up and let you know by either what's up or before the next class so in case you need to leave I can excuse you if you need to leave I can excuse you if whoever is calling is part of this class allow me to finish fast then I will I'll get in touch what about was you no one can confirm if sorry I found some few guys I left out Kevin I bet Victor kibbutz was was Vika here in one can confirm [Music] [Music] done here I hope that this is gonna be fun as it has been today thank you for making time for this I want to now please make sure you do this in believe you don't have a question every lecture I put a discussion forum make sure you comment but also the level of engagement for you is being evaluated that's what I say for those who here I'm evaluating your level of engagement so make sure you make a comment for every discussion forum that I create for me to be sure that you are actually active in this learning process yeah so that's what I expect I am still here up to be in the next 20 minutes in case you have anything you want us to discuss please remain as I finish up on this attendance and also as I try to create a discussion forum that we can be able to use we have finished so what I'm not expecting us to okay I am expecting that we can interact with the content I share more but also I am now creating a discussion forum or on the same lecture which is lecture 2 and so you can get in there and put in your comments if you don't have any question please comment and if there are comments you have I don't have to tell you each comments but just comments so that you are lever of an judgment I am actually evaluating you on that [Music] yes I have created a discussion forum let me add a topic in return for you to engage [Music] so few minutes yes it's a discussion forum yes so in case you need to not in case you order you have to comment and I have just added the questions that I have shared on this forum in that discussion so you can be able to yeah you can be able to engage in there okay Kevin I hear you you're asking all this current disrupting the Kevin you're asking I did not understand what you meant by outliers I would be glad if you use a kiss a kiss study outliers what I mean Kevin is that for example we are collecting who do my number data yeah and we come and ask what is your age and then you say your age is 20 years but the person who is collecting the data puts the data at 200 as 200 so when we analyzing that data it's going to be an outlier it's it's not within the scope there is no one who is alive right now that has 200 years in your in your data I mean in your sample you would not expect that so that is what I would mean by outlier that that those are and just like given our definition and I can take you back where what I meant by outlier I see data objects that they did all do not comply with a generic behavior of the data model and what does that mean is that the data model we should we expect that a typical human being is between a you know 0 or 1 year and and and let's say 150 years maximum yeah so if we get a negative number is a data entry on that particular on that particular entry then that could be an outlier because it's dieting or it's not complying the generic behavior of the data model that we expect I hope that answers your question Kevin yes I am here I'm here and even if you want me to be here up to five and I'm in my house so I don't have parrots we can still ask questions and I'll be willing to answer them so does it mean out of scope as per the subject matter yes it's out of scope as per a subject matter oh joy in your here I didn't note that okay okay I will revisit that so yes Kevin it's it's out of scope as per the subject matter it's not for any other any other questions bring them on but also go to the discussion forum on lecture two of this course and give your thoughts give you thoughts as well on those questions the review question that I in this lecture give you thought around them and I'll remove all the review questions from the lectures and I'll put them under under the forum so that once we have a discussion here then you can go into the forum and give your thoughts around the review questions okay any other question any other question so if no other question then I will end this this online or this collaboration forum and I will I will I will expect you to go to the discussion forum and give in your comments in the discussion forum Kim you even know I don't know what you are asking about is even still here I don't know what you are asking about not very clear if you could help me understand okay okay okay generally the in some minute or it has been a very vibrant very vibrant I'm sorry I actually didn't note that it's excellent time is already gone so much okay okay our legend so in kiss of anything else we learn that now we need to Paul you need to be careful about what you see anyway allow me to end this forum now please feel free to go to the to the lecture lecture 2 of this class and throw in your discussion or rather us your comments around the review questions in this lecture and I'll be also revisiting them and seeing those that are made their comments and that's what all your participation in this class so it will be amounting to their parish called to be part of the evolution for this class thank you for your time I have really enjoyed I am actually shocked it's it's 4:20 I was hoping to finish this in a now it's it's it's almost two hours but I'm glad that we are engaging I'm glad we are loving it I'm glad that we are yeah we are having fun so let's continue in the same spirit I am very optimistic that this will work for us and I'm optimistic that we'll get the best out of it and of course the management is putting every effort to make sure are these ones so thanks thanks very much and all the best [Music] [Music] [Music] [Music] we recorded but Raja waiting [Laughter] [Laughter] [Laughter] [Laughter] my stories were last week man [Laughter] [Music] when you certificate you got a place [Music] [Laughter] [Music] [Music] [Laughter] [Laughter] [Laughter] [Laughter] [Music] [Music] [Music] automatically [Music] [Music] tonio mine anyway [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] so I'll apply this a monkey buddy Dubai yeah mine we don't never be a bye hang on oh hang on 
U-hUnxuaw7w,27,"Steps in the KDD process. Knowledge Discovery in Databases (KDD) is the process of finding valid, novel, useful and understandable patterns in data, to verify the hypothesis of the user or to describe/predict the future behaviour of some event.
#DataMining #KnowledgeDiscovery #KDD

Follow me on Instagram 👉 https://www.instagram.com/ngnieredteacher/
Visit my Profile 👉 https://www.linkedin.com/in/reng99/
Support my work on Patreon 👉 https://www.patreon.com/ranjiraj",2018-05-02T19:14:06Z,Data Mining & Business Intelligence | Tutorial #1 | The KDD Process,https://i.ytimg.com/vi/U-hUnxuaw7w/hqdefault.jpg,RANJI RAJ,PT9M44S,false,33504,331,17,0,21,to use today I am here with a new topic that is data mining and business intelligence so let us see what is the basic concept of data mining or from where we can extract this kind of data so for that you need to understand one concept that is called as the knowledge discovery from data so it is a process which is short from desk ad D so here you can see it's a diagram for this so I've made this diagram with lot of difficulty so you need to concentrate here so basically in the process of K DT you have seven different steps like I've listed here so let's go into the detail with certain example of my own I will give you so first let us understand what this diagram is so basically this is the kdd process so in this you have at the base level you have different databases databases for in table format you have SQL pl/sql or other Oracle forms then you have flight files you have so all these goes to the data warehouse where data from different kinds of sources are stored in this so then it goes up to the higher level so at the very first layer you have the cleaning and the integration process which goes hand-in-hand so in this I have listed in different steps so at the first step and the base step you have the cleaning at the integration activity then after the data is moved up then data from data warehouse it goes to the higher step like there you have selection and transformation only data which is relevant to your application or only data relevant to your analysis tasks are returned from that so that's where selection and transformation comes into picture then with that data which should have you will extract some kind of pattern or interestingness measure from that so that's what data mining comes into picture so that you do after this after selection and transformation then you go hide the ladder then you have evaluation and present techniques where you represent in different different patterns of the extracted data and finally you say that I have got knowledge so that knowledge is nothing but the from the base level till the upper level what all things we have done in that right from analysis integration selection transformation cleaning and all activities so then it's said that you have knowledge of that particular topic now these are the different steps which are involved in kdd activity or the k2d process for data mining now data mining it's a term which is given so data mining is also a less data dredging so like you you have to extract that kind of data from this huge knowledge base or store bits or flag base or something like that so first in the first step you have data cleaning so what it does it removes noise and inconsistent data that you can't read but let me give you an example so like in your college project or in our college presentation activity you have been given to present one kind of topic like for example let's say block sheets so you go to the internet you go on google you search for block chains and like and it may not be like always be possible like at the very first instant you will get the current results that you want to be but then you will refine your searches by removing all those grammars and you will concentrate only on the main phrase which you want so that you get the exact searches so at times when Google does not get your key but it will have a strikethrough so that's basically your keywords are not been getting searched so that is the place where you do data cleaning you will remove those noise and inconsistent data from your query so that can be related to your query you search to your like for you to get that particular knowledge so that query which we put is also kind of data which are giving to the system or to the algorithm of Google so that's where data cleaning comes into picture it removes in inconsistent data so whenever you search any kind of thing you should do minimalistic involvement of grammars and all then you have to consider it only on the keywords which were searching for like etherium or blockchain bitcoins then you will get more knowledge from that or more relevant results you will get to extract from those then after that while you go to while you enter those particular keywords or the query on the Google search engine you will get a list of different kinds of websites no now let's open one website and it will have some kind of web pages a lot of information has been given like a literature writing and all then on the other website you get to see some PPT then on the other this thing or you will get to some videos or excel sheet or PDF or Word or something in it so you have data integration now you have to collect or download all these data and put into one deposit drill or create one folder so that's what I did I integration is you will visit all these sites and you will you will probably visit five to seven different sites for your particular thing and then you will integrate all those into a particular folder you have different kinds of multi-dimensional databases or data sources then finally what you do is you select those particularly d-type which are level to your analysis tasks or what you want to present yourself in that presentation session so now in this what you do all the data which are relevant to your analysis tasks are read read from the database so this is in context to the data mining but when I'm giving the real-life example you want only those particular keywords or those particular thing which you are concentrated or which thing you want like for example in blockchain there are hundred different kinds of parameters or entities but then your instructor or presenter might have given you like you want to construct a particular presentation based on these these parameters parameter a parameter B parameter C so you will be more on working too on those parameters rather than on those hundred different kinds of parameters which blockchain has then you have data transformation now after all this we will do the transformation or consolidation of data into appropriate forms for mining by performing the summary or aggregation operation so what you do is you basically have all these information from different different kinds of data sources what to do you open our word file you paste or you just die or you construct a new document now this document might have different kinds of or all these different kinds of data sources might have different grammars and also or - do you eliminate or you try to give minimum grammars in that and you try to represent in bullet formats or na like in a point wise form is like point one point two point three so that becomes more over like a easier end of interpretation to the user as well so that's how you do the data transformation you have this kind of paragraph then you are converting it into smaller smaller number of bullets or small points like 1 2 3 4 and all like that so that's our determines formation is then comes data mining so this is the part we are interested in in kind of data knowledge discovery from data so what happens here is all different kinds of intelligent methods are applied in order to extract the data patterns now let's talk about blockchain so in blockchain there is one concept of creating of blocks so that's like in the entire context of blockchain the part where you want to construct a blog in order to generate a Bitcoin so that is a kind of interestingness measure or interesting thing in blockchain so that you will identify in this that is data mining by using different intelligent methods that you have you have to work your brain on that and then pattern evaluation then some kind of patterns will be there then identify interestingness pattern based on some interestingness measure this means that like from the pass data from the past 10 years or 15 years what kind of activities of what kind of different kinds of changes happened and so that you basically represent in a pie chart format or on a bar graph or some other kind of representation techniques then you have after you extract into your pattern what you do is knowledge presentation then finally on the day of presentation what do you do you open one projector and you put all those Word files are in a PPT and then you start to explain so all those kind of visualization techniques are used to represent the mind knowledge so that at the end while you do the presentation the teacher will come to know that you have got knowledge on that particular topic like here you have got on Bitcoin or blocks so that's all about the knowledge discovery from the data that is kdd process in data mining so hope you enjoyed this video if you found you got educated by watching this video please hit the like button and don't forget to subscribe to my channel thank you for watching this video 
TSsG6NJDQZg,27,"Data Mining Concepts and Techniques — Week 1 —
Copyright © 2020 Wael Badawy. All rights reserved
This video is subject to copyright owned by Wael Badawy “WB”. Any reproduction or republication of all or part of this video is expressly prohibited unless WB has explicitly granted its prior written consent. All other rights reserved.  
This video is intended for education and information only and is offered AS IS, without any warranty of the accuracy or the quality of the content.  Any other use is strictly prohibited. The viewer is fully responsible to verify the accuracy of the contents received without any claims of costs or liability arising . 
The names, trademarks service marked as logos of WB or the sponsors appearing in this video may not be used in any any product or service, without prior express written permission from WB and the video sponsors 
Neither WB nor any party involved in creating, producing or delivering information and material via this video shall be liable for any direction, incidental, consequential, indirect of punitive damages arising out of access to, use or inability to use this content or any errors or omissions in the content thereof.
If you will continue to watch this video, you agree to the terms above and other terms that may be available on http://nu.edu.eg & https://caiwave.net 

Chapter 1.  Introduction
Why Data Mining?
What Is Data Mining?
A Multi-Dimensional View of Data Mining
What Kind of Data Can Be Mined?
What Kinds of Patterns Can Be Mined?
What Technology Are Used?
What Kind of Applications Are Targeted? 
Major Issues in Data Mining
A Brief History of Data Mining and Data Mining Society
Summary
Why Data Mining? 
The Explosive Growth of Data: from terabytes to petabytes
Data collection and data availability
Automated data collection tools, database systems, Web, computerized society
Major sources of abundant data
Business: Web, e-commerce, transactions, stocks, … 
Science: Remote sensing, bioinformatics, scientific simulation, … 
Society and everyone: news, digital cameras, YouTube   
We are drowning in data, but starving for knowledge! 
“Necessity is the mother of invention”—Data mining—Automated analysis of massive data sets
Evolution of Sciences
Before 1600, empirical science
1600-1950s, theoretical science
Each discipline has grown a theoretical component. Theoretical models often motivate experiments and generalize our understanding. 
1950s-1990s, computational science
Over the last 50 years, most disciplines have grown a third, computational branch (e.g. empirical, theoretical, and computational ecology, or physics, or linguistics.)
Computational Science traditionally meant simulation. It grew out of our inability to find closed-form solutions for complex mathematical models. 
1990-now, data science
The flood of data from new scientific instruments and simulations
The ability to economically store and manage petabytes of data online
The Internet and computing Grid that makes all these archives universally accessible 
Scientific info. management, acquisition, organization, query, and visualization tasks scale almost linearly with data volumes.  Data mining is a major new challenge!
Jim Gray and Alex Szalay, The World Wide Telescope: An Archetype for Online Science, Comm. ACM, 45(11): 50-54, Nov. 2002 
Evolution of Database Technology
1960s:
Data collection, database creation, IMS and network DBMS
1970s: 
Relational data model, relational DBMS implementation
1980s: 
RDBMS, advanced data models (extended-relational, OO, deductive, etc.) 
Application-oriented DBMS (spatial, scientific, engineering, etc.)
1990s: 
Data mining, data warehousing, multimedia databases, and Web databases
2000s
Stream data management and mining
Data mining and its applications
Web technology (XML, data integration) and global information systems",2020-08-27T17:00:10Z,Data Mining Concepts and Techniques    — Week 1 —,https://i.ytimg.com/vi/TSsG6NJDQZg/hqdefault.jpg,Win Your Brand,PT52M31S,false,645,5,0,0,1,welcome to week one data mining concept and techniques this video is subject to copyright owned by Nile University to maximize the use of this session please connect a headphone and mic if you can stay in a quite place adjust the sound of your computer close all other programs running on your computer the instructor will direct you to open any other required program to use closed chat programs in social media such as facebook twitter x' messengers what's up etc maximize your screen to see the four corners on this slide on your screen explaining what is data mining we will provide multi-dimensional views of data mining what kind of data you can mind what kind of pattern can be mined what is the technology used currently what kind of application you may introduce data mining to and what are the major issues in data mining at the end we will provide a brief history of data mining and data mining society at the end we'll summarize this presentation the first topic is why we do data mining with the increase of number of terabyte to beta byte we have a significant amount of information and data that may provide decision support system some knowledge so people can use it the increase of this data comes from the fact that we have an automated data collection we have database system we have computer system which works on its own as autonomous which generate lots of data there is also business ecommerce transactions every activity produce massive amount of data in science we have remote sensing bioinformatics science nation everyone right now is digital so you may exercise yourself and the experience the amount of data you have on Facebook on Instagram on Twitter in terms of images and text and what have you so we are in a what we call it data pollution or lots of data however we would like to extract useful information or what we call it knowledge out of this data and as a necessity is the mother of all invention data mining is used to have automated analysis of massive amount of data to serve or to provide enough information to specific users if we will understand how the system evolved you have to first to look at the evolution of the science before 1600 we have empirical science around 1600 to 1950 we talked about theoretical science where we provide lots of theory and lots of models later on we developed what we call it computational science where we compute patterns we compute data and we provide models based on competition competitions usually means simulation and simulation can come from mathematical models and verified by empirical date around 1990 we start in the era of data science we do have lots of information lots of data coming and this data start to build a serious question is can we use this large data to build theory and science we start to see effective approaches and economical approaches to store this data to manage this data the exposure of being online with the cloud and different aspect of the technology evolves we have lots and lots and lots of data we start to build what we call it internet which we believe it's for free however each one of us is paying for its own Internet which produce more economical set accessible business data acquisitions and different organizations simultaneously brewed using multi data start to echo a need to segregate compare analyze and understand what this data and now we have what we call it data mining which means we mined data for useful information is no different than mining ores inside caves or inside mountains we have a reference here from the ACM which you can refer to if you like if you want to look at data data usually stored in what we call it database and we also we also have to look at the database evolution so 1960 which start with data collection with information management system Network database management systems databases have been created and you'll be lucky if you can have a database not an excel sheet around 1970 we start to see what we call it relational database and the regional database management system where you have different relations between different cyclic tables or entity or schemas and this relation identify specific techniques for indexing and so what have you around 1980 we start to see advanced data models like object-oriented programming object-oriented base relational database management systems there is some applications for database such as spatial database which include GIS or geographical information system multi-dimensional viewing as a matter of fact any of the computer graphics games is a multi-dimensional database which is something many of us don't know that if you play any of this computer animated games in 3d space you have more than 3d data representation in the back end around 1990s we start to look into data mining data warehouses it was mainly driven by a special interest to track the behavior of people it was we don't want to say it it was primarily for looking at suspects to fight terrorism and the terrorists the international terrorism will come by comparing data to understand how the transactions will come through around to 2000 there are a stream of data management systems and the mining has been occurring because you have all of these streams which is available on your phone so if you look at your phone right now you have the Twitter account the Facebook account all of these accounts it provides a massive stream and this massive massive stream of data it can be analyzed to provide useful information so with the evolution of the science evolution of database infrastructure to support this science we start to see what we call it data mining and its application also we start to see some interfaces and some different languages we use like XML representation like Tyson like different coding schemes which support data mining and we start to see a framework of data mining or how people who process this data in the next chapter or the next section of the chapter we will talk about what is data mining we will start to define the meaning of data mining data mining called knowledge discovery from data how to extract more knowledge than data usually the knowledge is not trivial it's not obvious so when you look at patterns and it's or some data and you figure out how this data can be interpreted into knowledge to give you an example here you may be looking at eldery and you may look at a specific hospitalization days and you may be also look for geo grey and between the geographic the age or the elderly age and the hospitalization you may evolve some new knowledge about the reasoning of having that much usage of hospital bed if you try to track movement of people or try to track activity it also can be done in the same way we have also to understand that this data mining may be supervised or unsupervised supervised means I know how to extract the data and I expect what the results will look like I just need the quantity unsupervised means on the other hand that this data is not known as a pattern and looking inside patterns which is beyond the human knowledge data mining is a misnomer because everybody have been using data mining however we have some data some alternative names and the alternative names comes such as knowledge discovery knowledge representation data and pattern analysis or recognition data archeology data did the road aging information harvesting business intelligence and so on so when we talk about what do you mean with data mining it means that I in mine data to extract knowledge that I wouldn't know obviously by looking at the data alone I have to examine some pieces of the data some parts of the data and that is what we will be explaining in the next part so this data mining will serve what we call it expert system to provide knowledge and expand support system to decision-makers so if you look at knowledge discovery we have data bases which hold data then we have data clean cleaning and data integration to integrate between the data data cleaning to clear what we will look for and then we can move ahead to data warehouses for the data warehouse which is a traditional name to data unstructured or unstructured data where you don't know exactly the structure of the data you just have accumulation of buckets and these pockets will have data then we have selections and then from selecting this data you can have a Tasker driven a task relevant data where you can look at specific parts of the data you do data mining you extract different patterns and then you build knowledge and this is a feedback loop where data mining will will be used to discover this knowledge an obvious question usually is do the data miner know what they will get to the answer is usually the student and they experience data miners will if even data miners would be a term because sometimes we call them data scientists and if this is a scientist we'll be able to push into the knowledge is based on experience of how you handle this data with the evolving of the web we start to see so much into this domain because there's so much information you can extract from the web there are so many uploading data like there are some statistics demonstrate about maybe 30 to 50 hours of video uploaded every minute on YouTube and so many statistics about how massive this data is so web site web mining or database mining or data mining from Internet and online presence it becomes an interest to so many people because you have independent concurrent sources of collecting data producing massive amount of data and this data is always stored on data storage so usually data mining involve data cleaning I should clean the data to understand what I am looking for an example of data cleaning will be such that if you look for individuals crossing a specific street between 6 to 7 o'clock in the morning so in this case you will be able to draw asked process this particular part sometimes when you acquire signal from analog sources this signal will have some noise so sometimes it's noise reduction or denoising will be an example of data cleaning sometimes when you look at reputation and multi readings from different sensor providing the same data at that time data reduction is also considered data cleaning data integration from multiple sources means I can correlate the data between different sources and then I will have warehouse this data and build relational knowledge between this data called data cube and as well at that point I can produce what we call a data selection and I will form a data mining framework and in this series of tutorials or lectures we will evolve and we will describe all the steps and I also remind you that within this particular series you can also access it through YouTube or through webcast or through podcast if you have to make sure that you have all kind of resources around it and then we do the mining after data mining we would like to present this data so visualization of this data is one part of the data mining so how to visualize this data how to present this data in front of somebody then we will talk about patterns and knowledge has to be used to store into a what we call it knowledge-based this is all the evolving techniques we will describe through the series of these lectures different steps on each one of those if you look at data mining in data in business intelligence you will can classify it as follows we have data sources look at the number of sources you have for to access data through your business you can have from paper from file from experiment from somebody else you can by then you will look at data pre-processing and the integration and when you process and integrate this data you store them usually in what we call it data warehouses or sometimes we call it a specific order time and driven view of the data then you will have data acceleration which is a statistical summary queering and reporting you can have data mining and data mining means information discovery then you visualize this technique data and you provide a decision-making support system so if you want to increase the potential support business decision you have to be higher in the pyramid and at the lower level you have the data base data analyst the business analyst and the end-user who is the actual decision maker who is get benefit out of these systems we want to provide an example between the what does mean data mining and data exploration so if we want to look at business intelligence for you I will see the warehouse data like I'm just accumulation of data I can see data cube which provide relational shape between the different data entry I would provide a reporting however on this reporting I have no much of data mining because I'm not creating knowledge there is a difference between business objectives and data mining tools because I can if I want to provide the tools it will be just a supply chain tool I'm providing data if I want to present data I want to explore data this is our the difference between data mining and exploration in knowledge discovery domain I will have data pre-processing I will have data mining which means I will be able to produce patterns associations clustering classification and outliers analysis in this case I will be looking at how to make sense of this data by extracting trends or patterns or similarities I can associate events and I can correlate between the different data for example if you have increase of revenue you will have increase in cost or if you have interest increase you will have the efficiency in stocks or in basic expenses I can also classify data based on classes I would know I can cluster or connect similar data together to build a cluster and I can also analyze why there are some elements or events are not compliant with my clustering or my classification after that I will be able to produce better evaluation and I will select different patterns I can interpret the patterns and I can visualize the pattern so if I wanna evaluate a pattern for example I would like to see for example on medical application I can see different MRI or scan or x-ray scan with different buttons inside the tissues and this will be correlating into a specific disease and I can also interpret different patterns I can say if I see this button it means so and so for example if in financial statement in financial transactions if you see somebody who is doing a small transactions on a regular basis or frequently small transactions suspect Li this individuals or this account will be looking to hide something because it doesn't make sense for somebody to transfer let's call it $10 every one hour or $10 every day or below $20 every day so this kind of interpretation for specific patterns it can be interpreted based on human knowledge and then when we represent these patterns and you button information in this case this will be right the knowledge here is an example in a medical data mining where health care and the medical data are open using statistics and machine learning so we pre-process the data such as we look at feature extractions dimensional reductions we would like to extract and label this data then we can classify or cluster this data into specifics so for example we look at x-ray or MRI and they look for a specific pattern or a specific connectivity we call it age and then on this connectivity present a specific when this connectivity present a specific mass we can say it's malignant or it's true mur or it's whatever it is and from that point we will start to post process for presentation in front of an expert in the next stage we will talk about the multi-dimensional view of data mining data to be mined can be coming from data bases data warehouse it can be transactional it can be stream it can be spatial temporal data it can be time series it can be sequence you can also look at text and web so we can mine data coming from Twitter accounts or from posting on Facebook we can also mine data from videos and cameras and the movies we can look at graphs social and information network where we can connect between individuals and we can build the clusters or cells of individuals who have a specific pattern against or toward a specific decision or a specific process the receiver here or the audience here should understand that we are speaking within a limitation of what terms you can use so from that perspective you should be able to understand that we can get data from different sources now what are the knowledge that we would like to be mined this knowledge can be characterization it can be discrimination we can  we can classify we can build trends we can see what kind of information people are usually saying it for example to give it to built into perspective you may be able to drive along a highway and suddenly you see that moon so when if you can have a way to know what people will text or will tweet when they see this moon so when somebody will say hours the moon is nice you have a white wall it's look cute or I wanna buy this I want to buy that we can look at trends on how people accepting socially different constructions when you accumulate this data around a specific funnel and then you have some incident or events which we call it outliers we can analyze why individuals or why events will not comply with the majority in this case we talk about the 80/20 rules usually we have an 80/20 approach where we assume that 80% of the time 80% of the people comply with a specific trend however you may experience twenty more or less 20% are outliers or they don't comply sometimes our client indication that you don't have enough amount of information or enough datasets thats can provide a specific trends so the knowledge or this is describes knowledge then we can talk about two different of data mining one of them is descriptive and one of them is be reductive descriptive means I can look at data mine and I can describe an event happening in the past or a pattern or I can describe and extract a specific amount of information that usually is not known because nobody analyze this data before I can also do some prediction of data mining because if I can build a waveform or a diagram or infer may not information but basically a trend and I can fit the data into a specific curves I can come and say from data mining prospective I can predict that these things will happen I can also multiple and they have integrated function and data mining at multiple levels because I can mine data at multi levels and I can provide them usually we use data and intensive techniques we use data warehouses machine learning statistics pattern recognition we can use high performance computing visualizations and different technologies can be used for doing data mining there are so many applications right now and the applications can range from retail to all the way to Homeland Security banking and stock market analysis text mining for behavior all of these are different applications which can be adopted by data mining the most important piece of information here for you is data mining is a big huge more than huge tool you have a limited number of data you have a limited number of sources you have a limited number of techniques it's an opportunity for you to use your innovation to come up with answers for unknown questions and this is the way I would like personally to put it together the question after that is what kind of data can be mined mind we have database oriented data sets and application so we have structure we call them structured data so you have a record of different fields and this field will provide what we call it data base and then you can have indexing and you know what the field is about you may experience missing fields you may experience collect garbage or noise in some of the fields however it's still structurally still you know that this term is the name of somebody this is a database of purse this is what this is what on the other hand we have advanced data sets and including data streams including sensing and time data series sequence of data it may be structured or not structured it may be related or not related it can be stored in object related database it can be heterogeneous database it can connect to legacy database you may have a spatial temporal database which means you have some notion of geographical information system or Maps and this maps will change with the time you can also have a multimedia database which basically video audio and stuff like that you can have a free text databases and you can have also the World Wide Web so what we want to say is almost any data a human being is producing it has the likelihood of being mined for information this is what the summary of this slides about the next question is what kind of patterns can be mined so if I want to look at different data I want to understand what kind of information and what kind of matter of information I can mind so I can have information integration I can look at data warehouse construction I can transform data integrate data build multi-dimensional model of data I can have also a data cube technology and the data cube technology will provide it's a scalable method method where I have multi-dimensional relation between each node and from data cube I can build relations and I can do what we call it online analytical processing and we will have a specific section I believe it's lecture six or week six we'll talk about this particular data cube I can also have multi-dimensional concept description which basically characterization and discrimination where I can identify I can construct a contrast I can compare I can belt drains out of this data I can also associate data and correlate between them so I can ask myself what is the frequency of this transaction to happen for example what is the frequency that you go by from Walmart or from what we call it local store or grocery store or Carrefour I can also look at what the frequency of you drive in this street what's the frequency of you visit this place I can associate between data for example if you go to the mall most likely go to like Carrefour or like Walmart I can I can also also associate in this particular cases here they talk about associate diapers with beers that if people will buy diaper most likely they will buying beers with the 50% potential was 0.5% potential and 75 percent confidence which basically usually people who have diapers which means they have kids at home they don't buy beer then we can also have strong association so for example if you buy Nescafe you will buy milk or dry milk if you buy meat most likely you'll buy butter or oil or or seasoning stuff like that so this association rules it can be looked after from data mining and the question is always how to butter how to mine such a pattern and how to build this correlation this is one of the tasks which we'll know in the next lectures how do use such button for classification and the clustering this also will be part of this series of courses if you talk about classification and production we always look at how we can classify so if I want to classify I need to know what's the classification about so one of the techniques we use with neural network and artificial intelligent machine learning whatever you call it is we can understand subset of the data and we can label them to specific and then we will train our system if they see this similar type of data to produce this class and then from that point we can build a predictor where when you provide this data into the classifier the new data will come to know how to classify this data there's also typical methods and the typical method is like decision tree naive Bayes a Bayesian classifier support vector machines neural network rule-based classification so for example if you see a big potato most likely it has more time in the field it means an ageing food not necessary because you may see a small potato and because it did not get fertilized enough in the field the small potato did not grow as the one bigger so this kind of rules if you put them together we can well decision tree remember this is an introductory lecture in a series of twelve session course each one may exceed two hours so here we try to put an umbrella we try to provide large information to be able to show you what we are exploring in this field typical application for this kind of classifications will be marketing to understand the behavior of buyers you can classify different disasters you can classify different diseases you can look at different webpages how to maximize the reach out of it you can do a potential fraud detection on credit card because you see behaviour patterns so one of the functionalities of data mining to summarize it is a classification and classification means you you classify specific events or specific set of data or specific individuals behind the data into a specific known class the most interesting part of data mining is called clustering analysis or clustering and the clustering means I will see battles of behavior or battles of values and I will classify them into a class and this class does not have a characterization is just have a label from this label I will start to explore what kind of characterization and what kind of performance or behavior I can attribute or or label this cluster at this concept is very important because I can look at similarity of the data and the dissimilarity of the data and I can analyze how this similar data will get together there are many techniques and many methods have been used in the clustering schemes we will cover some in the future lectures however if you understand the scope of clustering you realize this is a goldmine for people because you will be able to build different clusters in different way the outlier analysis is another approach we have and the outlier Liars is data object that don't comply with the general behavior of data now the question is what is the general behavior of data so in a general behavior of data we always talk about 8020 rules like majority of the data eighty percent of the data or more will follow specific patterns so for example you have pedestrain e'en and if you look at behavior of Mediterranean around buildings they will basically get from specific doors however you have these specific ones who will roam around the building round and round and round and we always ask so what this Mediterranean is doing around the building most likely he the security guard or most likely the suspect so these outliers if they don't comply they may be noise or exception they may be noise because they may be room wanders or people who lost their ways or they may be exception because they have a specific task so we will introduce specific techniques to produce clusters or regression analysis we will experience 'full for fraud detection for rare free event analysis the next step we'll talk about this time and ordering sequential pattern and trends and evaluation analysis you look at sequence trends and evaluation you can look at time driven you can look at regression you can look at specific behavior we can look at specific sequential patterns for example somebody will buy a digital camera he will go buy an SD somebody will go buy Tylenol or a medication for her for headache he will also go and buy vitamin somebody who will go and buy vegetables fresh vegetables or he will go and buy cheese some of these patterns can be also analyzed we can look at political periodical analysis one of the projects we worked on early on is how often you go to visit the washroom and when you go to your washroom how often a day and this pattern or periodic analysis may result that you suffer from specific disease and this specific disease or specific potential disease did not have a symptoms you may be able to treat treat earlier than when you have the analysis you can also look at modfet biological sequence analysis you can look at approximations consequence you can look at similarity between these sequences based on how similar you you do in a battle for example every weekend you will go to the mall and buy specific grocery in a specific route 80% of the grocery or most likely will be similar at that stage I will be able to provide use advertisement on the similar product you will always buy or I can send you a reminder that's next within your visit today that you missed these particular products which you got in the last visit this kind of similarities it can boost our economy it can boost our sales it can boost our understanding of the crowd you can also mind data streams to understand trends so for example when you have a curfew people will have a specific trends when you have a specific anxiety a specific I don't say like a specific lack of specific products you may be able to see these streams you can always order them you can look at time variation and so and what have you there is also structure and the network analysis so we can find in this case we look at a more complex set of data like graphs figures structures so if I want to find frequently chemical compound I can look at three structures I can look at fragments I can build the information network analysis such as network of people which is I always love to talk about so if you look at specific actors the specific relation if you look at author networks in a specific environment if you look at terrorist network how people are sending hidden messages I can look also that some people can be in a different heterogeneous network and I can label them based on friends family classmate I can lick link and carry a lot of semantics like link mining I can do what mining and in web mining I can take lots of data and then for example if you have a page which has a higher rank on Google this page most likely will be a desired product or a business which people like to buy from so I can always analyze this behavior and look at how I can increase other pages based on what people like in these particular images so this kind of structures and network analysis is part of what we call it data mining the question is are all mined knowledge interesting not necessary all mined and all all knowledge mining are interesting they must have a commercial viable need so you have multiple patterns you can talk about different dimension different correlation between dimensions however do they really need to be together so I would like also to highlight here that we always look at mining only interesting knowledge so I want to look at either descriptive or predictive descriptive means I can describe phenomena happened the reductive mean I can predict what will happen in the future I can look at coverage to make sure that people have been covered or how many people have been covered I can look at typical versus movility I can look at accuracy of knowledge and how much accuracy I can get and on accuracy we have to talk about one more thing is the beauty of working in the social media data mining is there is no accuracy required however on the other end we have something called critical mission application in critical mission application you will need high accuracy in real-time when you allow a specific response time you can also look at different time lines and how sensitive this data from space eight important information so in the evaluation of knowledge you have lots of data you can have multiple knowledge dimensions the question is which one is more interesting to you the following section we will talk about what kind of technology are used inside data mining so you have data mining we have applications we have machine learning we have pattern recognition we have statistics and statistical tool visualization we have high performance computing to process all of this data we have database technology which is structured or unstructured and we also have different algorithms and these algorithms will be able to demonstrate to you what kind of information you can extract so this is the conclusion of multidisciplinary research so as a data scientist performing data mining you will need to know unfortunately a little bit in each one of those sets and this little bit will make you deaf dangerous will make you be able to explore more so in this particular introductory course we will be able to provide you a little bit of information in each one of these sections so there is a large amount of data so usually you need algorithms to scale them down we have high dimensionality of data which basically we have ten thousands of dimension data are usually complex they come from data stream from sensor data they may be temporal they may be sequential they may be spatial data can be structured can be graphed can be social can be heterogeneous can you have legacy data new data they change the versioning of however they collect this data so you have a different format you may have multi dimensional data multi nature data like video images text and you may have different software and different simulation as well where you may integrate to understand the legacy between models and the data itself so you have a new and sophisticated application which we call it data mining what kind of application you are targeting with data mining we have web page analysis is a common one for marketing and is usually whenever you talk about data and need for knowledge you have usually do disciplines one is sensitive Homeland Security level protection and commercial money making machine so between these two I believe everybody fall in everything will fall in so when we analyze bit webpages based on classification based on ranking we can also do recommender systems we can build collaborative systems we can build basket analysis for target marketing we can on the other hand look at biological and medical analysis we can look at classifications cluster analysis biological sequence analysis so this is what covers the medical field there is also some kind phears now they call data mining and software engineering where we will have individuals who will just specialize in not in research in and application in data mining and extracting information then we will have different tools like server analysis like Oracle database mining unless you have some invisible data mining tools you can also have what we call it right now as pison and you can connect a different library to do data mining major issues in data mining the largest one usually is the methodology how you can mind what you don't know will be getting so mine mining various and new kind of knowledge the dimensionality of the space because remember each one new attribute will increase the complexity of your space so if I have first name last name is two dimension if I have rid of births it's a third dimension if I have salary is a fourth dimension so by having multi-dimensional if you add one more attribute you'll you will increase the complicity of the detection because you have more dimension the other part is data mining it's a multi centered similarly effort because you may have a statistical message which will work with image processing like edge detection which will work with pattern analysis with color format so you will have the mining methodology will evolve and will create more interesting future because multi similarly more knowledge you can also look at the computational power required you can look at how to manage noise and the uncertainty in completeness of data is another aspect which is usually now where we look at when data is not there how we can fill it with more appropriate information and if you work in real-time data collection you might find an interesting part that if you project this data to be X and with the time accumulation it becomes Y value you may be created from the original and it is what we call it real time or semi-supervised data mining where you can unsupervised for some time and once you have more clarity so the beauty of data mining is like human brain you don't know what will happen you don't know how will it change how it dynamics the other part is a user interaction and the user interaction can be the interactive the background knowledge how you can present data and visualization we will talk about lots of visualization scheme within this particular course efficiency and scalability is also known as one of the major because when you will get the information and how the information delay will happen when you scale the data remember when we talk about one extra dimension it will increase the scalability of the data you can also talk about parallel and distributed streams you can look at incremental mining method like whenever you get new information you can discover there is also diversity of data how you can manage complex data heterogeneous data different data for sure data mining has some other aspects where we talk about the social impact of data mining because if people will know that you collect this information about them and you analyze it may impact their social activity there is also a bigger question about the privacy and who captures the data and I will introduce module in the model where we talk about the complicacy of collecting data and what kind of legislation surrounds it and then there's an invisible data mine mining situation where you can look at data and you extract knowledge which cannot be seen from the data there is also a bigger question right now in some places in Europe where if this information and knowledge is generated by a computer not a human who have the access who owns it and who can own the rights to we want to talk about finalized here the brief history of data mine and data mining society this is a different workshops UK I will not go into it in 2d but I can say that around 1989 we start to see the workshop on knowledge discovery in database and then it start to evolve and we start to have some transactions these are the set of conferences which you can look at in the slides and journals if you want to find references you have access to all of these resources which is right now on the screen individuals on the podcast you may refer to the associated the PowerPoint slide or you may assume a refer to the actual video at your leisure time I don't think it will help anybody just to go unless this particular information to summarize this particular session which we discussed data mining we talked about discovering interesting pattern and build knowledge from a large or massive amount of data it's a natural evolution within the database technology and with the ability of more sophisticated processing techniques and processing platform we have a knowledge discovery space which include data cleaning data integration data selection you have transformation you have data mining butter evaluation and knowledge presentations and more or less the flow of the following sections or the following lectures if you want to call it that it will follow this particular pattern mining can be performed on variety of data heterogeneous data different set of data it can predict data it can substitute for missing data it can handle few things so we can mind basically anything data mining functionality it includes primarily characterization when we characterize this data discriminate this data means classic identify specific data we can associate data together we can classify them we can cluster them we can look at outliers and trends where we can extract the one which does not follow a specific trends however we may not know the trends so this is ours a functionality where data mining will follow through so if you wanna understand data mining technologies and the applications you have to understand that if you know one technique and the definition how to apply each one of these functionality you'll be able to pass the first level of data scientist with data mining experience you have also to understand the major issues in data mining which we'll discover which will cover it very quickly and at that point I would like to thank you and looking forward to see you and the next level there is recommended reference books I have on the screen here for you you should be able also to find it in size PowerPoint slides and looking forward for your feedback I would remind you that this particular session is also available on broadcast on iTunes and the Google Play for any more information please email me at Badawi at Badawi dossier or Badawi at high wave dotnet looking forward to hear from you again and write down right in the comments here your feedback or send them to me thank you and have a nice day 
EBdJUyq1CG8,27,"Data Mining - is the art of mining knowledge out of the huge amount of data. This channel mainly concentrates on the aspects and concepts of data mining with suitable examples that describes the concept clearly. 

Subscribe to https://www.youtube.com/c/IdaSamsonEasylearn",2018-11-13T08:35:23Z,Concepts of Data Mining Association Rules - Apriori Algorithm,https://i.ytimg.com/vi/EBdJUyq1CG8/hqdefault.jpg,Ida Seraphim,PT10M45S,false,3928,95,1,0,3,so we are going to see about Association rules now so as we know Association rule learning is a role based machine learning method so when we go for Association rules we will find all set of items that have support greater than the minimum support and then using the large item sets to generate the desired rules that have confidence greater than the minimum confidence so when we call a particular rule as a valid Association rule is that whenever a particular rule satisfies the minimum support as well as the minimum confidence so we are just going to see an example by using the a priori Association rule algorithm so you have so many associations of algorithms like a priori of P growth and health court algorithms so now we are going to see about the a priori algorithm we are going to see an example with a priori algorithm so just look at this example so in this ayah I'm having a transactional database which has five transactions so each transaction has a transaction ID right T 1 T 2 T T 3 and so on so each transaction ID has a list of items associated with it so you have five transactions and each and every fight transaction as a list of items associated with it so here if you see they have given a minimum support a 60% and minimum confidences 80% we need to find out the frequent item set and you need to generate the rules using the a priori algorithm okay so the first step is I am just taking the minimum support so here the minimum support is given a 60% when it is given in percent you have to convert it into your numeric value so what I am doing is I am just taking that 60 divided by 100 into the number of transactions in the example here you see the number of transactions in this particular example is 5 so I am taking it as 5 so when I cancel it I am getting 3 so the minimum support this 3 okay whichever element is having the support count of three and greater than that can be included other elements will be discarded so when you see first step is I'm just gonna have converted my percentage into a numeric value and I have found out my minimum sup and the next one is I am just going to generate the candidate one item set so I'm going to this transaction table and I am seeing what are all the elements are there in this transaction table I am just listing down all the elements that are there in the transaction table so I have listed all the elements so here if you see for example I have taken em okay and I'm seeing the EM is present in how many transactions so if you see 1 2 3 it is present in three transactions so I am taking the value of 3 so for each in every element I am just going to find out the find out in how many number of transactions that particular element occurs so when you come to this one when you for come to this particular example sure if you have seen here the o is repeated twice so you may have a doubt better to take whoa as one count or as two count okay one main thing you have to note that is you are just going to see the number of you are just going to see in how many transaction workers okay it is not that how many times Oh occurs you are just going to see in how many transactions Oh is occurring it is not that how many times always occurring so if you see you have to count this as one okay you should not count it as two you have to come to test one because you are seen in how many transactions over o is occurring okay so the value is three here so I have just like that I have taken for all the elements okay so what can it eight items it's all the elements I have taken and I am now comparing it with my minimum support so which other element is not having this minimum support count should be eliminated so I am eliminating again da you see and I because all these elements who are having support count which is less than the minimum support come so I have eliminated all these syllables and this is the list of list of elements that is having the minimum support company okay once this is done I'm moving on with my to candidate item 6 so if you see I am just going to take the combinations of two two item sets so I am with whoa am with km with TM with y just like that I am finding all the combinations of two item sets and I am finding out the support counter feature everything so once again once it is done I am just going to compare it with my minimum support which ever path is not having this minimum support count I am just going to eliminate those pairs okay so I have eliminated everything and my l2 is this one okay and now you see after this I'm going to generate my candidate three item sets so here if you see you have okay and oae so here the first two elements are same so I am just going to use the concept of self join so when I do it it will be okay ii and then here also you see you have K and K in these two things K is the first element once again I am going to apply this self join concept and then I am using ke y ok and then I'm whenever you do this you may have so many other list of item sets so I'm just seeing the support count of these things so this is 3 and this is 2 so if you see ke while he's not having the minimum support um so I am eliminating this one so my final frequent item set is the okay II okay this is my final frequent item set that is having the minimum support counter okay once this is done this is for first half of the problem is over with minimum support I have found out my frequent item set once this is done I need to generate the Association rules that generation of Association rules must be done with the help of the minimum confidence value that is given ok suggest different stuff so I'm just going to generate the Association rule using the minimum confidence that is given that is 80% okay whichever is not having the value more than 80% or equal to 80 percent those rules will be eliminated whichever is having more than the 80% will be accepted so if you see the frequent item which we have generated is okay he once the frequent item is generated I am going to write all possible subsets of this particular item set so when I write it you will be having these mini sets you have some eight cents so in this I am just going to eliminate the empty set as well as this full set okay I am just eliminating it I'm just going to consider these six sets for my role generation okay so the first tool I am going to generate with my okay okay my rule one is woe and K implies e if someone is going to get o and K that in place you may get ye also okay that is what this Ruth implies so I'm just going to find the confidence you know the confidence from lightest support of a union B divided by support of a so here in this case this will be your a and this will be your B so support of Boquete e divided by support of okay so just take the count of these two so that is 3 divided by 3 that is equal to 1 that is 100% so now I'm come comparing this with my minimum confidence value that is 80 this is more than the minimum confidence so this rule is accepted okay I'm moving on with the next one that is rule 2 I am taking ke ok K and E implies whoa so same way support of ke bow / support of ke so take the count it is 0.75 which is less than the minimums confidence 80% so I'm eliminating this role same way I'm moving on with oh ye okay it is also giving me 100% so this rule is accepted I'm moving on with oh okay if someone is going to purchase all that implies they may go with K and E so support of o ke / support of whoa okay this will be your a and this will be your B so 3 divided by 3 that is 1 that is 100 person so this rule is also accepted I'm moving on with my next one thatis kale kale implies whoa and he so here you see it is 0.6 that is 60% so this rule is eliminated because it is having less less than 80% so this rule will be eliminated I am moving on with my last own that is with E he implies O&K support of kale o e / support of e that is 3 by 4 that is 0.75 75 percentage which is less than 80 percentage so it is eliminated so if you see your rule one rule 3 and rule 4 or accepted and rest of the rules are eliminated because it is not having the minimum confidence value so my final rule that is generated this three rules and that's that is that that satisfies both the confidence as well as the support okay so I'm calling these three rules as the valid Association rule this is how your hair priori algorithm works okay just see to it if you like it just like the video and just comment on it thank you 
f4lkbBW-9Bw,22,For downloading ppt:https://www.slideshare.net/secret/16VCQOCr06bgr,2020-06-04T02:40:12Z,LECTURE 2||DATAMINING AND WAREHOUSING||Module 1||DATAWAREHOUSING and OLAP TECHNOLOGY,https://i.ytimg.com/vi/f4lkbBW-9Bw/hqdefault.jpg,Nimmy's KTU TUTORIAL,PT11M33S,false,1376,11,1,0,3,Hana students next topic is data warehousing then the difference between oil TPN all been chosen question an oil taping all happen double difference data warehousing means normal data mining the large amount of data and lysine and that large amount of data large database runs toward a new power a larger database not a data warehouse snoring the number usually you see the troller databases some data mining even do you see in the data warehousing the mean difference under okay so actually data warehouse means the data repository that which shows the data for data mining and which is maintained separately from the normal database normal database little separate Archimedean Jason again they needed and then the reason and laminate stupid Ananda then he data warehouse in Ambala story in a data Salem historic data sorry and it is not frequently updated but either data base Carnegie Mellon data's and you know then the melody not not delete you know then Adam cardi updated it to be in them using depression but they did warehouse anomaly date and just add you know like happened in the Palomar through another day to delete insane modification elaboration performs in and he data you see return amicable a decision making power person but but a decision-making purpose no Elliott in the mercury data warehouse or data use it mainly for prediction classification Exeter then it is not updated regularly but it is appended regularly but no transaction data under numbering and IO transactions then again the data is loaded he but he shorted theta converting means the definition of a data warehouse it is a subject oriented integrated time variant at non royalty that means subject on in limits aramean particular subject in a base is a color date lyric normal data warehouse will store in a then integrated means pala sources no no date yet he compiled a heterogeneous forces no new data they're the normal databases the random flat file so condom transactions online transaction record Salman Khan when a pile of files metadata for him had also in his files metadata anomaly data warehouse Lee you seen then time where in that means the historic perspective at lighting and data at the store in the area element is no key to the point with no occasionally the key structural kicking any monitor contains an element of then non-volatile means non volatility means the power of hidden data relating chain under the data attentional economy is another just reloading and operations then just reloading and accessing that means creating multiple functional operational new perform chain so data appearing out of a tender imitate a load you how should a summit at eight in number in lysine element it ecstasy then what is data warehousing it's the current structure on frame data warehouse data warehouse to construct a membrane data cleaning integration and soul additional transformation methodically you see around in what is the need for data warehouse in the Nano data warehouse from you seen so mainly is used to promote high performance of both online transaction processing and analytic process the data warehouse quarries are complex at animal a normal database you see in the chorus Nepal area Catholic complex network or a 13 data mining using song complex core economic normal database Slovakian Battaglia atom is Makoto data warehouse done event then operational database like normal database non-animal concurrency preserving on the kind of conduct security measures of anomalies yield pusher concurrency control measures on the mandatory techniques on the number a data warehouse Lee usage in the national data mining rolled a taking using the now schmira so separate data warehouse from UC then ending up a database and database Lola data structure content sillim entirely different area weather data business story in the data salem and the data type is a memory and either put a employ embroidered it is now then a language student details on the same data type setting music in question data warehousing different types of complex subtler data's are getting a different varieties of data Eric Nam line at the store in then main motivation or in the business profitability inaudible in the face of customers not buying pattern at identify the sales occur and improving in the bedroom then products name and click position when detail laughs I'd say sweating again I didn't faded grow up products nary position rain detail animal data warehouse in New Zealand okay the next Easter difference between OLTP and data PhD degree means data warehousing data deduplication my lab online and he'll take processing anyway so operational database is a lengthy nombre oil the oil DP only transaction processing in the main a man or another only transaction and quarry processing and at the day-to-day transaction like mucosal concentrating the data warehouse you know remain a menorah detector analyzing and decision making Eric him and the layer date does not really it can't stop a darkness and present data and various moments in order to accommodate diverse needs of a different user super aloe same type of uses nitrogen animal a data warehouse water in fact in also different types of users community different types of data occurs in artist arena but a data base Langley overlap processing indicates learn under sorry OLTP processing you a little a memory you see an illness same type load a to type transactions a then difference is a preacher to the question is happening elevated amputee laden with nothing to do 17 so our fritters about another elemental dead fetus anybody can main features nor another in the case of oil tape in the middle operation process in a transaction processing larrikin concentrate in this tradition relation and another process in my throat well happened occasionally number analytical processing an important in then oil people occasionally I think they user no another cloud so DPS very commercial application or Umbra take a high-level top level manager certainly then only type in a main function day-to-day operations very keen materialism a long time Allegra decision supporting operation self then database design then they turn out use in the data will people current a data today date a mighty overlap not historical dated him then added up you then even the accessing OLTP reading right Yolanda lap not arrayed operation Metro loop then polity because the ink dated mainly concentration overlap not the information about that mainly concentrating then number of users we need people to give thousands of users of animal happened occasionally top-level execute this mark on a so number of users is very low camber to overlap well KP then database design Lelaina capacity codes in the burlap net then data warehousing architecture and OLAP technology first animal operational they tell and collect here then as not national data mata makes right there then transforms reloads here no data warehouse storage locally and then Lena are they in America radiator cycle acted or not the ETL no applause sapling he tell you is not exactly a virtual data matrix right here the near particular formula transform change in Lodi loading means nominal data warehouse locally in data warehouse launched a that the other directing a nominal data mining see the business visas improbability viola our anomaly in Terminator 2 OLAP server keeping them a lot server and animal next buddy then traitor acting a chopper data we're hosting architecture which again anomaly architectural but I cannot treat our architecture modulated activity known yet the children elite Lee it's not money so prepare a petition together on dangling Amala first rendering different forces narrator operational database makes journal so signal and Dayton Amla extracted the clean to the transformative lowly you know then refreshing Parliament data warehouse Latin keep another then data by using a miniature form on a data marts Malaga then adding an amulet he data warehousing energy mighty damage will cumin the formula to converting an Anglophile observer neuron after college server that server for Madrid to convert either Yannick Nepali operations performed a curricular sponsor I had another output the mcurtin and he said Lord putted mining volleys a problem I'll output some theta then what is elapsed elapsed or means it's a multi-dimensional cube like structure it look dimensions no connotation another number area entity Snelling attributes nine for example I'm a little subject oriented on the Purdue data warehouse for example say a sneer I got in the d3 son angry say listen Eddie came over the main thing is not anemic indica attributes very I'm telling Linda T's where I'm a tunnel dimensions for example sales in the gates run underneath time in you item translocation and embedded in the moolah hidden a schema representation the case you ordered a man shouldn't core attributes even about our dimensions nail and I'm the dimension table one died came wrapped us and another then either loot central teen gun a central theme in a matter of fact a below and I can represent either a fact a black or a numerical measures like I'm not involved in it then the first economica controller two-dimensional p1 adage of a particular location lay all over time period roll over items in it remodel cell lipo in alas all typo in another person in her you do just over locationally metonic he thought if ignore the multiple location about their mighty damage will be you wanna create in our first one was to Table two dimensional beings second wiggler three-dimensional view it ready Donna actually nominal data warehouse snoring new data warehouse me a Multi multi dimensional cube uniform at low tide in converting another matrix formula to three dimension multi-dimensional matrix uniformly converting he did not name a couple of operation server forms here the Terek maha bua result is Corinna Mullen me with multi dimensional cube Allan Horlick server walk he returned him another couple of operations about from say the tearing in the Macau modular dating a my interrogator what in the steps in partnerships onion first different oil TP selling transaction details and Alex black tea again a transformed through the standard is a normal easier then our new OLAP database totally nominal data warehouse loading both here at even the normal multi dimension to observe the 80s then finally to smelter probability 
U18SiyBK12A,27,In-Database Data Mining Using Oracle Advanced Analytics Option for Classificaton using Insurance Use Case,2013-04-19T14:36:10Z,In-Database Data Mining Using Oracle Advanced Analytics for Classificaton using Insurance Use Case,https://i.ytimg.com/vi/U18SiyBK12A/hqdefault.jpg,Charles Berger,PT11M52S,false,5649,15,0,0,1,welcome to this presentation and demonstration of in database data mining using the Oracle advanced analytics option an option to the enterprise edition of the Oracle database for predictive analytics and classification problems specifically use in an insurance use case this our legal disclaimer are always required to show and now the Oracle advanced analytics option is an option to the database where where we've brought algorithms to the data so versus traditional to data analysis platforms where they require you to pull the data out to separate servers and separate analytical and statistical engines at Oracle we've decided to bring those algorithms to where the data resides thus eliminating that's that whole step of moving the data back and forth we feel this gives you a better simpler more simpler more capable scalable architecture for delivering better decisions and deeper insights using predictive analytics and in doing so provides a lower total cost of ownership there are different ways of using these Oracle advanced analytics option I'm going to primarily focus on the graphical user interface which is part of sequel developer that is going to be using nodes they're going to drag and drop together to drive the sequel and pl sequel API that's going to drive the data mining algorithms that run natively inside the sequel kernel of the Oracle database and you also have the option of using the popular open source our language to drive those same algorithms and a growing library of our packages the problems of classification are exemplified I think using this diagram which tries to show how difficult the problem is with just two variables of finding patterns hidden in the data if I do a simple linear model I can perhaps separate the guys in the red down here on the lower right from the people up in the upper left that are blue that are the more loyal customers if I do a more complex model with some quadratic or cubic terms perhaps I do a better job of picking up some of the blue people down here in the lower left and the red guy up here but a decision tree would do a much better job and actually find these cut points and a nice byproduct of decision trees one of the for classification techniques that we support natively inside the database is that it provides these English like rules that say well if customer months is greater than a certain number that was discovered by the machine learning techniques and if income is greater than 90 another number that was discovered through iteration of all the various possibilities and cut points then we found a pattern and that pattern says that based on the historical data based on eight out of eight people I can say with confidence of a hundred percent if I find somebody new that looks like these people that I can say they're going to buy they're going to excuse me they're going to churn and the amount of the size of this segment that we found is eight out of 39 total records or about twenty percent here we find another pocket of customers that are of interest to us possibly a slightly lower confidence in our predictions there but also having a different behavior so based on these profiles and based on these predictions I could have more proactive marketing strategies to combat sure and retain these loyal customers so what we do with Oracle advanced analytics is we provide insight using the data that you've managed in inside the Oracle database and we provide prediction so in using the graphical user interface ultimately we're going to build a workflow that looks a little bit like this at the very end of that workflow we can right mouse click and generate the whole sequel script to go directly to deployment we think that's one of our really advantages and strengths of the Oracle in database platform and we have a number of different graphical user interface model viewers and different nodes that help you assemble this methodology so with that let's get into this and go straight to the demo so what I'm using here is Oracle sequel developer it's a free graphical user interface for people who work with the Oracle database you can download it from the Oracle technology Network and there's just a simple post installation configuration for setting up the data mining capabilities and if you google Oracle advanced analytics and data mining you will find those instructions so the first thing I'm going to do is pick a data source and what it does it prompts me for those data sources that the DBA has given me access to I can include tables and schemas tables and views from other schemas but I have this insurance customer lifetime value data set that I'm gonna use here and I can choose which variables I want to pull over for my analysis I'm choose to bring in all of those variables and the next thing I may want to do is I may want to explore this data by setting up some sort of exploration of techniques here where I'm going to actually build a number of different histograms thumbnail histograms and calculate a number of statistics and all that data now I've already set some of this stuff up in advance here so I am going to go directly to to that right now and scroll down here just a little bit and I have the same data set and I've already run this so let's go take a look at this first if I view the data here I see thumbnail histograms that I can click on each one and see what's going on for each of the different variables and I've grouped them by our target field here which is by insurance or don't buy insurance we find only twenty-seven percent of our population has actually bought insurance so we're keen to go find other prospective customers who fit this profile who would be likely to buy insurance so the next thing we're going to do is we're going to do some preparation and cleaning of the data this is one of the nodes we use over here in the transform area and let's take a quick look at this if I edit this node I can see that we have used the settings to automatically deal with nulls missing values too many constants like a phone number or a zip code we've actually nested in here and attribute importance which is going to find those variables which have the strongest correlation with the target field I can do stratified sampling and it's really a lot of automation and machinery that's going on here and after doing this what the output is that tells us what those variables are that have the strongest relationship with my target field it tells me which variables are those ones that have too many unique sore too many Knowles and it gives me suggestions over here on whether i should include or exclude those variables from my analysis and if i simply like this all up and go up here and take the recommendations that it'll go off and do all that but i've already done all that and a sense of saving some time here let's move on to the next step now I've dragged and dropped a classification note on workflow here so we've set this up it will automatically build the four different classification types of problems that I that we support natively inside the database if I want to add more I simply click and I could run 10 or 20 different models rather easily and Oracle advanced analytics will take advantage of the scalability and performance of the database the variables that I'm going to use are going to be buy insurance from my target field and I can turn on or off these different variables and have them included or excluded from the model and I have different mining types and there could be a lot of automatic data preparation so I don't really have to worry about a lot of the things that you typically do with more manual data mining processes we realize we're going to be mining large amounts of data and hundreds or thousands of variables inside the database so we want to be as automated as possible if you want to override those you always can we've run our models already let's compare the results there's different ways of doing this so we're going to look at the lift chart one of the techniques which shows me the predictive advantage of the the the incremental value of using predictive models that says here's my random guess if I'm trying to find those people are likely to buy insurance but if i look at this model up here i think it's the decision tree modeling it's a much higher lift that says if i'm going after the population thirty percent into at thirty-five percent i found thirty-five percent of the people who are likely to respond which were only twenty-seven percent of the total population but using this predictive model i found as many as eighty percent of the people i'm looking for so I'm doing that in a much more efficient way going back to our workflow and moving along rather quickly through this demo I want to now show you that we have different types of models we have things like support vector machines here i'm going to show a generalized linear model with just saying for the target field yes here the standardized coefficients showing those variables that have the strongest correlation so I can sort these by absolute value or not however I want to set this all up and that's another model one of our four different techniques that we support natively inside the database the one that a lot of people like to look at is a decision tree because it's very easy to interpret it's very visual so here we have the total population as we can see twenty-seven percent of the people did buy insurance most did not and as I start sifting through this data sifting through this decision tree scrolling down further and fur I find ever more homogeneous sub populations that have been discovered through the data mining techniques that in this case if bank funds is greater than that number checking amount is less than that and so on then I have people who are very likely to buy in fact over here they are eighty eight percent likely to buy here there's sixty-three percent likely to buy here there eighty-five percent likely to buy so so why bother white might go after those people so i can actually apply this predictive model two new sets of data make predictions maybe take the prediction the prediction probability x a dollar amount and prioritize my customers based on that so I've done that here I've taken my customers a new set of customers have taken the the output of that down below here so if I wanted to show my view my properties down at the bottom so I've built several different models here and I'm only going to actually flow one model for just for simplicity here so I've done that I flow the model through to here and I've set this up where I'm just looking for the prediction probability I can set this up to be friction caused prediction details a number of different options here to set just I want to I want to see in this case those people are most likely to say yes i will buy insurance I've done that I've also added some other data and if I look at the results then I get a table which again is just a table or view inside the Oracle database and here's the customer sorted in order or I can sort those now if I want to sort ascending and this is just a table or view inside the database which I can join two other tables and views I can also take this whole process that I've done I can generate this all as a sequel script so if i want to go straight to some sort of deployment then i can go off and just take that whole flow that I've set up and deploy that as a sequel script and immediately go to production so I can set that all up there and the last thing I wanted to show was that this of course could be part of an application and here's an Oracle communications data model where we've done up streams and clustering which I'll cover in another YouTube demo but if I've done different clustering I can apply the clustering model to the customers sort them and different sort of unsupervised learning clustering bins and then for each one of those I have different probabilities that the customers going to leave or stay I can actually get the description of who those people are in greater detail so there is it all in sort of a nutshell and if you are interested in learning more about this there are training materials that are out there on the Oracle technology Network just Google for them and you'll find them you can also send me a personal email if you'd like and i'll send you my favorite links with links to more information and there's no real for matter of capability for questions here but thank you for your time in watching this presentation and demonstration of Oracle advanced analytics thank you 
GUawpOlUFOY,27,Database and Data Mining Introduction Video,2017-08-30T06:36:08Z,Database and Data Mining Introduction Video,https://i.ytimg.com/vi/GUawpOlUFOY/hqdefault.jpg,GaryBoetticher,PT57S,false,176,3,0,0,0,hi my name is dr. Gary betta jerk and I teach databases and data mining as you know the classes for the first week have been canceled because of Hurricane Harvey I hope you're all doing well and safe and dry wherever you might be I felt really bad about missing the first week of classes because in both classes we're going to be doing quite a lot and as a result I've decided to create a set of YouTube videos to make up for the class so we can stay on schedule so you'll get an email that mentions these videos and the order how to watch those videos and I do request that you watch those videos before the first meeting next week which is September 6th I believe so thanks a lot and I'll see you next week bye bye 
bCfnWsYHEok,27,,2016-08-03T06:02:27Z,Mining Real World Data,https://i.ytimg.com/vi/bCfnWsYHEok/hqdefault.jpg,Vidya-mitra,PT12M3S,false,23,1,0,0,0,so far we have seen different kinds of data mining tasks and their different applications when it comes to actually applying these data mining tasks on real-world data there is a whole lot of different issues the real world data consists of many different kinds of formats and many different kinds of issues and each of them contains their own fine points to consider before we actually we can actually apply the available data mining algorithms in these different real-world scenarios real-world data not only comes into in the form of relational databases most of the data in the world is used to be present in relational data until of course later the World Wide Web started to become very big and also a lot of data happened to come as multimedia data so because of the fact that lots of people have digital cameras and so on so but relational data itself is very huge and how do you mind relational data for different kinds of data mining tasks let the association rules or classification or clustering and so on basically you can see very easily the relational data is very amenable to mining for classification and clustering because it automatically comes in the form of a table of records where there is a set of columns there there are rows and columns that's what relational database are all about right so now one of the columns you can actually treat as a class column especially if it has you know well defined classes it's a categorical attribute then it has well defined classes and you can try to predict those classes based on the values of all the other columns all the other attributes okay so that would be a good classification problem now you can use these records separately and try to cluster the records based on some distance function you need to come up with a good distance function for each for the current application that you have in mind and using that is function we can apply any of the available clustering algorithms to cluster these records ok so now obviously there are other complications to consider because the relational database normally does not consist of just one table the lots of other tables which are all interlinked to through keys foreign keys and so on ok so now in this complex structure you might need to before you actually apply classification or clustering you might need to do you know get all the data that you want further classification purpose or plus wing purpose you get all the columns that are required from all the different tables do all the kinds of joints that you want to do form a nice big table and then do the classification of clustering so these are the kinds of issues that you come up with in relational database now in transactional data bases data is very amenable to mining association rules okay so data is of the form where you have different records and each record is a set right so in a transactional database each record is a set of values now so there are different elements that is a set of values so this could be people who have bought different products from a store okay so now they have seen that this kind of data set is very amenable to mining association rules and frequent itemsets but obviously you sometimes want to do you know mine you want to do clustering or classification on transactional data bases on the other hand sometimes you want to do frequent itemsets and association rules found relation later business so obviously you need to transform the data sets into the correct format but you need before you do these before you apply the correct data mining task okay so then we have issues related to distributed databases in in the real world many times I database is not just present in one location but it is present in many different locations so there might be a business activity at different stores throughout the city and each of them might have a separate database oh there might be multiple businesses different companies who want to share the data in order to get good mining results okay so now one issue with the latter kind of example is there's lot of privacy concerns so generally companies do not want to reveal all of their data to other companies they just want to make the data accessible to some algorithms and the algorithms will work in such a way they will only produce rules okay so the data shouldn't actually be shifted from one location to another it's only the algorithms which work and the rules that are output and might be shifted from one location to another okay so these are some of the kinds of issues that you have it distributed mining okay so the other kinds of things you also need to bother about whether you are going to transfer too much of data from one place to another if it's allowed that you can transfer data whether how much of data is that you want to transfer or it is better to do the mining in the different locations separately and only transfer the rules okay so these are the kinds of issues that you have here then in spatial databases a spatial databases the database where one of the columns is actually a special column okay so this might be for example that is the address of a person okay so there might be the address of a person and the address of a person is the location in some space okay so that's the meaning of a special date facial attribute okay any data any database which has a special attribute is a spatial data set ok so now given address column like this you want to do certain you want to find certain kinds of rules okay things like maybe people who belong to a particular City so address consists of different parts right there is a city there is a state and so on the country and so on there is a hierarchy of locations in in an address okay and you want to find rules of the kind people who belong to this state they have this kind of property ok so our people who belong to the city have this kind of property ok so now or you want to say that people who belong to let's say this whole collection States which all belong consecutive states they have certain kind of property okay so when you are dealing with locations or spatial attributes you have to do all kinds of different tasks for example you might want to look at polygons okay and talk about the properties of all points within the polygon okay are you want to say that you know suppose does the river you want to say something about all the cities which are on the bank of that river ok so you want so you need an attribute your call let's say adjacent so you want to find all the cities we can adjacent to the river and mine rules from them okay so these are different kinds of special operations but you want to do so these are the kinds of issues that you come up with in spatial data set ok so in multimedia data sets like you might have a collection of songs in your data okay in a database or you might have a collection of movies or a collection of video that you have recorded yourself and so on and so there's large collection of such data available okay and how do you mind data from this kind of data set automatically okay so now the key task here is how do you mark you know the kinds of things that you normally want to do is find do clustering at first you do plus drink so you want to find that so you have a video you want to find you have one snapshot in the video and you want to find all the similar snapshots okay so now let's say you want to find all the photographs which are which have a scenery in the background okay let us say natural scenery with let the mountains and so on those kinds of photos you want to separate out okay and photos which are taken in cities you want to separate them out differently okay so you want to have a nice similarity measure a nice distance metric and do clustering which will be able to separate out these kinds of different photos okay so and there's also classification task suppose you want to do face recognition okay in photos that is a classification task because you already know that okay these are different classes based on the number of people that you have and given a new photo you want to put them into one of the different classes after doing face recognition okay so these are the kinds of things issues that you come up with in multimedia data set then you have data streams data streams are actually you know it's kind of data set which is actually very huge it might not be huge in the beginning but the data is actually coming very soon for example stock market data okay in stock market data the each record comes one after the other very soon okay throughout the day the large number of records being created and they come as a stream okay and the stream is really so big you cannot store it in any particular place very effectively so assume that you cannot the steam steam is very huge that you cannot store it and then mine it has a stream is coming you have to mine it on the flight okay so you need special techniques to do data streams when you have time series data okay so basically let's say things like weather prediction okay so you have the temperature of a particular region and then as a time increases how the temperature has changed okay and given a time series of this kind there are things that you might want to do you might want to study the kinds of trends that are there in this time series you might want me you might want to predict what is going to be the temperature tomorrow in a particular place what is going to be so similarly what is going to be the weather tomorrow whether it's going to be sunny or raining or whatever in a particular place these are all time series kinds of application finally you have in the real world text and web data so text data and let us say digital libraries lots of digital libraries and there are lots of HTML documents all over the web okay and this is a huge repository of data and how do you mind this data effectively and there are whole lot of issues you know from actually mining content okay so which is actually to basically in X data okay so and then you can use the links that I present in the web that one document 1 HTML document links to another document how do you use the links to do better mining okay an how do you use the usage so people who actually use these upsets how many people have seen this particular website after seeing this website how many people shift okay move on to this other website okay so things like that so basically the usage of these websites ok so the different kinds of data that is available so how do you mind is different kinds of data on the web or in text digital libraries in order to get interesting kinds of patterns that are very valuable to the users so this this all that you have different kinds of applications on real world data 
251682USGKQ,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-24T23:40:54Z,Data Mining (Spring 2016) Lecture 12,https://i.ytimg.com/vi/251682USGKQ/hqdefault.jpg,UofU Data Science,PT1H26M7S,false,179,0,0,0,0,yes we don't know how yeah yeah but still sentiment that way interesting ok so this tomorrow you are so six hours directors is also actually the subtraction actually does this brother spectrum the wording it's fun so my question see so starting at all you see take a point that's farthest so that's that is correct yeah SME sign points don't factor it we're not going to meet at the point is this one works even if you begin to using jakarta's because you know sometimes you get out aces so let's get started let's see okay so we're going to talk today again about streaming confidence so again it's going to be and we're going to talk about variations on frequent items again putz so but we're going to change it in in a few weeks so so again i'm going to talk about frequent I know that's weird way weighted writings but frequent items and frequent itemsets as well but also we have time we'll talk about just finding collisions with with a bloom filter as well so we're going to talk about kind of three things that mid count sketch can't spell today the end ok so the sorry I woke up this morning I woke up in different states are a little out of sorts then it was an intentional don't worry like i was in wendover this pretty or does it happen to anyone they like okeydoke amid wind over even if you're intentionally dis driving the san francisco you really here wind over it's kind of a little bit but okay so so sorry the first thing i was thinkin of the count being sketch this is going to solve actually the same base more or less the same problem we talked about on Monday the frequent items problem but it's going to use balloon different approach and have slight and various other other advantages on top of the beach agree and there's there's a very similar how we called the count sketch which also talked about on this will be much more similar to the countenance catch you couldn't tell by just looking at the games but also have a little bit different properties then we'll talk about these these a priori the what's all is the a priori are benign and this is kind of like in classical machine learning if you took a machine learning class 10 years ago this would have probably been something you spent maybe uh maybe a few weeks ago they're still machine learning I mean sorry the data mining class with data mining took it 10 years ago you know even five years ago you may have spent a few weeks on this topic there's still some classes taught around the country that spend a lot of time on it I'm going to spend half electronics I don't know just personal preference but some people find it very useful so which is something i mentioned this is the for frequent I know sets so hence that's how I got that weird-looking title this is a parenthesis all right so your good items and assess okay so so this is a kind of a it's a kind of seems like a very useful problem these one is their kind other approaches which kind of come to tackle this for a little more statistical perspective happen later on and it's kind of related to this frequent items but is is um is different in some ways and you know I it's kind of a from a theme how that are going to works is also a bit different and then if we have time in the lecture today you'll see there's quite a few things on this list i'll mention bloom filters which are going to be very similar to the structures the count min and the count sched we'll talk about earlier just to kind of put this in perspective so people I guess you know usually some people ask me how this relate to bloom filter so people is anyone heard of bloom filters so these the count min sketches get a little kind of like a loon filter and so people say is that a bloom filter it's not a woman filter but its shares a lot of similarities so i'll describe this and the differences have been given i have time or ok so hopefully we'll get to that all right and the the notes that I wrote on here I usually save on the webpage the surface crashed and didn't save them last time so you know their loss and on forever there hopefully the lot tech notes I have up there are already enough information but sorry about that ok so let's let's review the streaming streaming setting again so um so for streaming you have a have a data set a and you're going to be presented this in some border up to usually some AI and will write the last element as a I think that using hem and is so there M elements in the stream but at any given time we've only seen up to some point a I so this sometimes we'll call this set a big a I saw the stream up to this point and we have we're processing this we see this one item where we get to do something fast with our CPU and we have a of a memory which is of limited size so we're trying to keep some statistics on this with using a small amount of memory and so then for the frequent items problem we're assuming that each of these AIS comes from some universe of bounded size n right and so then the frequency F J of the Jade element in this and the set so jay is in the set n is we look at the set that we've seen so far all day is such that a is equal to j and then the size of this set so this is means a such that it's a slightly shorter than the ones on the outside this is if you see it in la tech document to this is the notation used and so these are all the elements that are the jinc items so these are think of IP addresses this is the IP address of your computer that's the some secret agency might be might be monitoring if you're sending a lot of suspicious messages then they you know then they want to know about it right so we want to find these activities that are hot so some IP address which occurs very frequently and so we're going to get some sort of guarantees and I'm going to write this slightly differently today because this will fit what the harbor lunes is going to do it's going to have the property property we want to come up with this F J hat for all values of j and how we deal with the all values is gonna be different today than monday last time we limited space how do we solve all values all the ones we don't record to artistically 0 we're going to do this a different trick in this class and so it's going to be we're in this time they're going to be greater than the true value but not but not too much great okay so it's always going to be larger than the true value our estimated value but not larger by too much before it was smaller than the true value but not smaller but by too much and so you can you can also estimate whatever this quantity is and you can add or subtract that from your estimate so these are you could get even you can get either one of these these meds okay n okay so this is this is the goal today is to provide these estimates what we're going to do today is we're going to explicitly construct a data structure s of a and so then we can ask reddit a data structure so this will be a data structure so think of this as you're creating a date structure you can insert up into it and you can ask certain type of queries you have an API for your data structure and you're going to ask some value J and it's going to return to you FJ hat so you're only going to have access to this data structure this time you're not going to kind of have these counters in the same way that we have a full you're just going to be able to ask where is it okay so and this data structure is going to be random so it's going to turn out that this bound is going to be on all ways true this is going to be true with probability at least one minus Delta for some other delta counter that's going to go into the sides so we want this to be true with really high probability and so this will be something so it will always be larger we'll never underestimate and we ought to be too much larger with some probability it could be that we got unlucky and things were much larger than our worst-case scenario but in in general it'll be okay all right and so the trick we're going to focus on today thats can make this work which we did not use on monday is going to be hashing right so our old friend hashing from the first kind of from we talked about a lot earlier in the course and so again just remember a hash function is going to be is a deterministic function from in this case it's going to go from n to a smaller set okay okay and this is going to be hash eight or in this case let me use a different symbol will do this music is H in my doubts oh it's ancient lowercase H let's let's use a are you okay so r is going to be random so H R is going to be drawn from the distribution of hash functions so when we pick this remember we pick this hash function from this distribution now it's deterministic but this choice was ready so like if you use the built-in hash function you used a random salt or random see to it and you as long as you it's the same random seed you got the same result every time right so this is remember I hash function and so in this case again we're going to pick some number of hash functions i use index are because we're going to have our be go from equals from the set 1 2 up to some variable T so we're going to duty of these hash functions okay alright and so okay so now this is kind of the ingredients we need and we've seen all this before so and again we're just going to use or you use hash functions and counters but we're not going to need labels so let's go into the count min sketch so the count sketch okay so instead of keeping just a set of K counters we're going to keep a table of towns draw this also will do this we see counter 11 both counter 12 counter 13 up to capital one T I mean longer this way supposed to be counter to one count of three one two counter k 1 and so forth in this would be counter k t yes we have this table of all of these counters and so so to initialize see i j is equal to 0 so we're going to start with all the counters equal to 0 okay and so I said we had T hash functions so i'm going to put h1 h2 h3 up to HT here so each row is going to correspond to a hash function and this is the setup of the data structure okay now that I told you instead of having kind of cave counters and labels now I have K times T of these counters and T hash functions so now what should I and my hash functions serve you my hash functions go from one decay so I can get an item in and it tells me it goes to one of these hash functions right so I've seen i J 3 J okay so now how would I do this what should I hows about any ideas so you know there's that there are too many options here so and again they're going to need to to write answers the count min sketch in the count sketch or you can use the same roughly the same structure so so what would you do with an incoming point right so remember the algorithm for the comp and sketch is going to go for a I a right so I'm going to loop over all these owners right written it for I equals for i equals 1 to M red so I have I loop over all elements my screen okay and and so then what should I do with each each AI each element of the stream a hash it good house which hash function should i use use all of them good right so if i take an element if i have an element AI that comes in here i'm going to push it into the hash function h 1 so each one of AI is going to map to some value will call this our which is going to be in this go from guess this was J to a value J that goes from one 2k okay so that means it's going to go let's say it went to three let's say h1 went to 3 i'm going to go to this counter and h2 will go to h h 3 so this will go here h3 maybe we'll go out to the ice thing for the the outs of the Jade right so in each of them is going to go to a different right two different one of these these these counters in the scent okay good good so so the first thing I do is I'm going to hash it to one these counters and then I should what should I do with the catheters once i found increment good so i'm going to increment the counter all right so let's try and write this down algorithm so then for r equals 1 to t so iterate over my hash functions try and write this down so counter the Jade so this is the Jade row and then the counter I hatch to is to hash of j of a I so this tells me what counter to define in here right so I've hatched to the Jade counter which one did I ask you H if this is our so see of our H of our okay so rs3 here the third hash hash to the Jade counter jay is this quantity here and salami this quantity is jay so I passion this counter I'm just going to increment this counter that's it so far for each every time something comes in for each of the hash functions I find the counter associated with it and I increment it that's that's the whole it's the whole data structure I don't have to worry about if I run out of counters I don't have these if conditions I just increment these counters actually yeah yeah so it's going to depend on how accurate you want right so these parameters and all it turns out what you'll want to do is you're going to set k equals to 2 over epsilon and T is going to be log of 1 over Delta okay this log based YouTube and so so typically typically this is like between between five and ten it's going to be enough to have a really low drop low population fill yer if I want so if i set this to be like five or ten it means that Delta is going to be very very small this this grows if Delta girls small this really quickly gets its for this this goes very small as a function of of 1 over Delta right so so if Delta is a saying one out of one out of a thousand right so I feel one out of a thousand times right so then this it's log base 2 of a thousand 1024 log base 2 1024 is in 10 so if I feel roughly one out of a thousand times on that means the whole data structure is correct this was going to turn out to be one out of a thousand times on doing this I need ten ten rows so think five or ten and then k is going to determine my gear this is going to end up being the right here and we'll we'll see why this is okay so you said the number of rows so this you may want to be like 200 or something like that to give a one percenter okay so good so this is the whole data structure this is how you do the updates I still haven't told you how to do the query right so this is the this is how i do the update how do i do a query right and my query i mean if i have my data structure SMA and i asked for Jake right so I asked for the value as for the value J how do I get this is supposed to be this F F hatching how do i return this maybe someone like a person counters yeah good so I want to look at all the counters right so I want to how do I know which counters to use right so if I hash j if I hash the value J it tells me what are the right counters precisely these hash functions or deterministic ones i've chosen them so every time I think I saw something with the value J I know exactly which counters i increment so those are the countersign that then I want to look at so I want to do something for all over all the counters are hash of our j right so so for each value of R with went from 1 to t this I this tells me the right counter so I can index into those right counters and then I somehow need to average them or something right this is this average the right thing what ah the minimum good right from the title of the sketch yes it turns out you actually want to find the minimum so columns so why is the minimum of the day very very well by having to sing sing box right so so use it different values ash into the same counter it's the only the only the only when yeah okay okay your pink clothes you enclose the minimum is good why is so yeah the idol course always go to read post falls every time you see that I on the water okay so many numbers is that most of them right so here's here's maybe it differently looking at it is impossible to return a value to see a counter that is smaller than my truth out right good I can't I can't ever underestimate in any of the counters none of the counters were will underestimate it so they're all some form of estimate and they're all larger than the true value so I might as well take the smallest one right so the smallest one is it will give me the one that must be closest because they're all larger because everything every time I saw Jay or every time I saw my query Jay youth JD twice here okay so let's let's change this in notation otherwise to query so every time I saw q every time I saw query q it's it's going to go and increment this now i might get air I might because there's could be hash collisions there probably will then some other things might have to the same location and also incremental but they're never going to decrease the batter they only increment so so I get air with their collision so I can only get something larger so my best thing to do is to take the small as possible value okay so if i take the minimum i already know is going to be less than the value of return right size it's going to be an over estimate and so if i take you know the minimum over a bunch of over estimates this is going to be my my best friend right so now the question is how much how much of an over estimate is this is going to be right and this is going to go into why we chose these eyes of Katie okay um right okay so this is going to be less than the true value f of Q plus some w and so now how large is the stuff all right so um okay so algorithm and how you query it clear it's like you should be like it's a fairly simple algorithm very simple are going to implement very simple to use right why it's true turns out is this is also not not too complicated Bachelet's let's let's go through this we have to use some now randomized analysis okay so let's so so we're going to describe big a random variable will call this why are cute okay so this is going to be for the are throw so this is the art which which hash function and so the Q is going to be a specific we're okay so so for this random variable but this is going to be one this is going to be defined as the amount of the over cat so so technically VCR of H R Q minus the true frequency husky rights ahamed extra stuff is going to be in this counter and it actually it will be simpler to actually analyze just just this so we'll ignore the fact of this variable sup we'll just analyze this disk captain if you you can add this back in and it gives a slightly better so so so now again we're going to use that that's so why of of our cue oh let's let's change this to actually change this actually use J so I'm going to look at the j okay let's let's actually ignore this price so so the amount of of of stuff so it's going to include all the counts in in here so this is but this is just going to be the effect of all elements with so this is in heaven so I'm going to say the amount of the amount of of counts from from q + n so of the of the objects we see that have a that have a value q that go into this counter okay so if they go into this counter we're going to see that it's either going to be the value f of q with probability 1 over K or 0 okay so if you look at all the VAT all the elements that have value Q these are going to are going to hash sorry I'm getting so this is going to be it's so good this is jay but this this Q is based on so the query I'm looking at the query that that I care about so the particular counter that is affected by my query on the Rope bar and this is the Jade element which let's say is different thank you so this is set minus Q so one element J that's not cute that Ashton to this counter okay so it's going to either have a receptive F of F of G here see they're going to have an effect of f of j with probably one over k if it has a collision or zero if it does not go okay so that means that the expected value of Q is half of j / k so the for any other value other than Q for any other threats I see other than my own IP address the expected effect on my counter is going to be the total number of times this IP address occurs / k because that's the probability of clothes okay so now if I look at the sum over J in n right so from jay from 1 to n they set minus q of these expected value of y rj so the expected influence of all of these other values by by the linearity of expectations it's going to be QR j equals sum over j of f of j / k and this is exactly going to be m / k right so if I ignored so the sum over all of these terms all of these counts of everything is the total number of counts I've seen now I'm fine I mean a little bit more careful i can subtract out half of cute right but this part is is the part that I can I don't I need for my ultimate now yeah would tell us with the having a greater number of distinct elements in your head and we difference turns out it's not going to have any difference for this one for the count sketch something like that is going to it is going to make up it's going to make a bigger difference it's just that the total sum of them all this one is not affected by the bearings caused by this distribution it's just easy to see that each each each thing is correlated and it's it's effective is this essentially random on the probability of a bit falling in this band is one over for each other right okay so the expected so the expected effect expected over count of all the other counters so now I can say something like the what this is saying in word it is expected over count in row r is going to be m / k which if remember by setting k equals to a perhaps lon this is going to be epsilon M over 2 and so the expected over count is epsilon M over 2 so this is good this is less than what we said was this over account which occurred with very small probability so we're on a good good truck so this is basically saying that all the items are you are going to be evenly distributed across all that the bins so most you can get yeah the big ranch um the expected number we get yeah so if they were totally evenly distributed distributed we take those slightly tighter analysis onymous actually but because they're kind of clumped together you could have some very heavy items one item if it has the wrong collision could put us over our thresher so we otherwise we could use a turn-off mount about this it turns out we then there's going to be a weaker bound we can use there's this Markov inequality which we're going to say the Markov inequality says that the probability the absolute value of some random variable alpha of the ceramic variable X is where that alpha is less than the expected value of absolute value of x actually it should be yes you should actually just enforce if we know that X is always greater than zero so it turns out that this over count is always greater than zero and so the probability that it's square than health is less than expected value over alpha so this you can you can say if it's always greater than zero is expected I'll you must be greater than 0 and if too much of it is very far away then the expected value could be so big so if you've never seen this before you can kind of think about that sit in a little bit and have we derive it but if we're just using this what we're going to do is we're going to set this now but we want this to be used to be epsilon m right we want say that the probability the you want this to be one half and we want alpha another color so there you can set this to the one-half by setting alpha to the epsilon down and this works out because the expected value was epsilon M over 2 and this was divided by alpha which was epsilon house and that's going to people or whatever right so that we say the probability is greater or some random variable which it turns out is the sum of all these all these counts right that random variable right right here this was our random variable X so we say the probability that the sum of all those over counts is greater than epsilon M is less than the expected value which is epsilon n over 2 divided by epsilon M we get one half s what this says is that probably that any one of the rows is greater than epsilon M is at most one half okay so we're we have T of these robes and each of the rows was independent so now we can say the probability that the that all of them were greater than we r greater than epsilon m so the men of the rows of of all these you know so that the are men of our in t of the sum or j of y rj is going to be greater than epsilon hound right so now if the minimum being all of them are greater than this than epsilon out this is going to be because they're independent this is going to be less than one-half to the power t and member t was equal to log base 2 of 1 over Delta right so this is going to be equal to one-half to the log base 2 1 over delta which is going to be equal to delta p right so now the probability that were too big probably that we've too much air is going to be in most delta which is what we wanted okay so we just prove the correctness of this randomized algorithm typically you papi just like we did you set up your random variables correctly you apply some tail down this case the Markov inequality we talked about the Chernoff tufting any quality which is more powerful before and then you plug this in and you get some not so we've so this that makes sense you believe this works so each of the estimations over estimates is spread out so the expected value is pretty good is small about the air so if you do it a bunch of times over and over again then the minimum of them this should be too big okay so this was this is the countenance caption how it works very simple to use okay so how much space how much space did we use for this thing so what is the space for the count min sketch so how many counters the number of counters was equal to k times t which is equal to 2 over epsilon times log 1 over delta and each of these counters headspace log m but we didn't need any label so is this how do we avoid using labels yeah the hash functions turns out a hash function takes costs about as much as a label so we at T equals log 1 over Delta of these of these hash functions so this is the space of one of the hash functions okay so how does this compare to the missionaries sketch it's more or less than the missionaries sketch the mg sketch then use 1 over epsilon times log n plus the bar now so probably log in and log n are probably about the same they're probably going to use like a single integer or whatever fur for both of these right so then you know it's to be roughly the same size on the 20 brats lon I've got this long river delta factor which is about five or ten them so I'm probably facing the space by about a factor five or ten so this one about log 1 over delta 5 to 10 times bigger so its bigger so so why would I want to use to countenance catch there's something cool I can do with it what happens so one thing I can do and this is probably not obvious at all we've talked about so that they count min sketch can do a subtraction so you can subtract something out you can remove an item that you put in before so let's say I accidentally I I the typo into my my sketch can I go back to fix it I candid with the mish agrees once I've done this uh this um this thing right i subtract something from every encounter you know I don't necessarily know what I had any more I can't go back in the fix things but with the count min steps it's easy to go back and fix I just I just go to all my hash functions in an update I I just do i do a decrement instead of an increment four minus minus on subtraction so I can just I can decrement these counters if I'm doing subtraction and as long as I don't take out more than I put in then everything works so like if you if like if you're putting it a bunch of in a bunch of documents and you say no I don't want to include this vacuum of my days anymore I'm going to take this out or you could say something like this is often called the so subtractions often called the on the turnstile model so you're at like a like a football stadium people walk through the turnstile when they go in and then they come out so if you're if you're keeping track of things passing in or how to beset then whenever something leaves you want to subtract it so I'm getting air based on only the things left over so I could think of what we use this as so so say you're you're you're you're sending some messages back and forth or you're trying to reply to all your emails and and you have a bunch of emails coming in and when you get a reply it it removes something from the set right so i don't have to ha my email so i have I now apply to you right so this put these string algorithms you only have access to the data at the time the Univision song yes so thank you have another part in your data set that tells you here's an item you're removing this from the set so I only see I have something some firewall setup and it sees emails coming in and or from up some IP address coming in and going out to an IP address right so if I know that I have more coming in than I have going out these are the things that I still have that my set then that I can keep traffic's if it comes up with some various things so one thing you might want to do and to do this as a bit more to get this to work you require some extra layers of stuff I'm not going to get into but you could think of this as your screen and you want to look at so typically we've been saying we're interested in in this it is that going back to the beginning but there's an ocean dog what's called a sliding window where if you if you change it to go here then you shift it forward a little bit so you always keep it of a server side so I want to know what happened over the last day so the there's there's some way to keep track of this information internally and then you remove it from a sketch at some point and being able to subtract so you need to keep some more detailed stuff inside of your to get this to work but being able to subtract out of the sketch becomes becomes useful there are ways to do without subtraction but that this helps oh it could be that you're saying I'm keeping track of say the you know there are things where you get an update like otherwise this could be a subtraction event otherwise those so otherwise you would you could see this subtraction and then you go and you know to decrements the victims so that means the everyday appoint twice up to twice up to twice you you know you you can't subtract one you had in in this city there are other techniques to do that they usually get the air battle to get more complicated state because then you don't have a total number of things you have to talk it's something to do with absolute values but it gets Messier okay how are we doing alright so let me spend 10 minutes just talking about the account sketch alright so the accounts so historically the count sketch was invented a few years before the count min sketch but it's a bit more complicated so again you have the same setup as you did before c 1 1 c 2 1 up to see k 1 c 1 2 up to c 1 t see ktc have all these counters you also have these hash functions and you're going to also have a second hash function which is going to be simpler and so now this this is going to be a sign hash function s of pets of our and this is going to go from your set and again but just to a minus 1 for a possible and so I'm just going to for every item that comes in with along with each regular hash function H I'm going to have this side hash function it's going to give you a random sign along with it okay so now my algorithm is going to be for I equals 1 m for each hash function for R equals 1 1 to t I'm going to do the sea of our H of our of I it's going to get sea of our H of our AI plus the sine of our and so I'm going to increment or decrement depending on the Sun so if it's positive I get positive x increment the counter if it's negative i'm going to i'm going to decrement the counter pencil then so this is going to be this is my update and so now my query is going to be similar except i'm not going to use the sum of money my estimate now s of a query which is going to be my F with Q hat is going to be equal to the median of the set see of our age of our Q times sine of our you visited for our and so sub taking the minimum I'm going to take the median half of the set this is the median of assets right so so I sort them and I take the one that has equal number of things on your side so I'm adding or subtracting here so I'm not going to go through the full analysis so I can get to the other stuff again but this intuition what's going to happen is what the property I'm going to have is that the expected value of F Q hat is going to be exactly equal to F 2 i'm going to have the right expected value the reason is because for every other item the ones which are not Q i randomly either which everyone want to happen to Clyde I randomly either add them or subtract them at random based on these scientists functions so my other my collisions are could be positive or negative so on average there 0 so the expected error is going to be 0 what they expect two ears not zero but the expected effect of any one of them is here now but there's variance in here and so the error bound is going to pension and on the Barrens instead so you take the median now because the if I took the average of them if I had one really bad one it's going to pull it off the median is more stable in fact the media is going to be that I would have some probability of having care and it's only going to be bad if over half of them are going to be bad in the same direction and so this is probably you can use the media instead of saying over all of them are bad I'd say over half of the lux and so you end up doing so T now as a need I think it's a I mean something like to log of 1 over Delta instead because i have to say half of them are bad and either the directions and then k i think is again going to be is now going to be 2 over epsilon squared so now i'm going to get a worse the condensed on epsilon in fact this is this is much much worse than before fact it's there's even going to be some some constants in here there's there some constant times 10 grams long skirts so I'm going to get this code 0 1 over epsilon squared here but the air balance so the error bounds I'm going to get our that f of Q minus f of Q hat it could be larger or smaller is going to be less than epsilon x f2 and f 2 is going to be the sum over J FJ square square root okay so what is this is called the second moment of the frequency counts so instead I had just the sum of these before that was am the total count to them it turns out that this this f2 moment is typically much smaller yeah this is typically going to be much smaller than them than the total count um what's going to happen is especially if the distribution is skewed it's going to be making a much smaller so this turns out to be a much tighter power than before but I need a larger size and these work roughly the same in practice as it turns out they were about the same I mean that's not too surprising because they look pretty similar but they work about the same in practice and if the distribution is very skewed and as we'll talk about these these lights and heavy tail distributions later in class then actually if it's yeah if if it's actually if it has a heavy tail then this is going to work much better if it's get all of the masses in one item then who they're going to work about the same but if if you have a very heavy tail on your distribution like the words in the English language fall off their frequency falls off pretty slowly then this will actually work work better because this have to mountain turns out okay so I'm not going to talk too much more about that but it's kind of a simple trick on the variation on the count count sketch count vintage okay so let's let's talk about these this on his frequent itemset problem for the last part and so the a type of a primary algorithm and so why is called that's not really sure just it's probably written written down someplace but there's there some parts of like science where they feel like they need to give some catchy names every afraid of the out of them and sometimes sometimes it's clear with those names come from sometimes it's not okay so so this is frequent itemset so this is going to be different than the frequent item instead of getting a stream and you can still pick up this in the streaming sense but often it's streaming only in that often this data gives me really big and it will live on the disk of of your of your computer and it's you can make you can access it on disk but you only want to read it a few times so you're going to do this with multiple passes or video instead but it's almost no we can think of it as as a data set a where a1 a2 and so forth am but the size is not so important is each of these objects is going to be a bunch of elements it's going to be a set of elements so each of the items on the stream is going to be a set of elements let's call this the i won di to the 30 so far okay so now each element in the stream is a set and you should think of this this is often called Tom's these are often called these market basket data sets so a common way to see this as if you're you're going is going to a grocery store and these are the items on your receipt right you don't buy one item at a time you go and buy a bunch of things at once you buy milk and bread and other stuff right so each item of the stream is now set and you want to know which items are typically typical are typically bought together so you will know they're frequently occurring sets of items so it doesn't have to be everything in one particular basket but a subset of many of the baskets so you know people buying milk eggs bread and thence their their favorite specific brand of cookies right those cookies may not be a curve but the fact that they brought milk and bread together a lot it happens to many people okay so um so the goal is to find the home hey and so then we'll say all of these bi JS each of these occurs from our our University so now this is the set of all items in the grocery store right so you gotta find all subsets of which co-occur more than some fee in more than fiom of the of the baskets and so more than a fee fraction of all the baskets are alignments not quite right here so more than a fee fraction of all about contain this substance guess it comes on so before knew that there can be no more but now anvil is the number of baskets so you might have a different set of subsets kind of made in sight is that say milk and eggs occurs you know the m x then both milk and A's occurs feed em right so if we see the combination milk and eggs together lot then both individually milk and eggs as a single item set must also occur lot occurs at least this much right you could have bought eggs without now if you could about not without eggs but if that pair occurs a lot together in each of the individual things must also occur so this this tells us two things so one there may be a lot of these combinations of these subsets occur we're only we're only going to return only return the these maximal sex okay this will cut down greatly on the number of sets we have to return so if eggs and milk both the car with more than fiom frack of the sets that I don't need to return just eggs or dis enough okay so I only return the maximal sets I necessarily have a bound on the number of these but it turns out in practice it's always much much smaller ok so it's rolling to return the maximal sets that's one implication the other is this tells us how to run this a this gave primary algorithm so how to return how this if ur alamos is uses this fact it's but instead of he uses head of the he uses a contrapositive of this so so it works in these rounds let me just described described the algorithm in rounds so in round one and so the input to it is this value fee for this this total size p.m. and we want to find so the first round we're going to find all individual items occurring more than feed vm Thomas ok so I'm going to just look for one item at a time so I'm going to first find so I'm not going to worry about pairs of items this is a much bigger search space right if they're n items and there are roughly n squared pairs of items so I don't want to try and keep track of all these edition ok so I'm going to find all of these and you know and i can i can use something like these other sketches to find them they don't get quite the right guarantees even if i set them so that the lower bound is there because the meaning of em is different here I need to know the total number of items that occur over all of the all of the receipts all the market baskets so if i set M is equal to our uppercase M if you go to the Sun over I hi the size of a I right so these were the size of each basket then i can run say the Meech agrees algorithm with or like some version of the heavy hitters algorithm we're using this parameter M as the as the tools as the total size of the domain and I set and I only return heavy hitters so let's say can return one of these heavy hitters where I use the total size as M I just kind of chained together these market baskets and I set the threshold at feed pm- the uppercase L minus epsilon hat so then I can cut it I can kind of use these heavy hitter techniques well just reading over the data once in this first round find all the things which are individually just these are larger okay and so this is going to get me a set of these items so be the one up to be back okay right so this is going to be a subset of it so this is an outfit around ok so this ultra round one so then in round two I look at all pairs the set on these steps open e 1 e 2 e 183 the K minus 1 but so now I'm going to have all these pairs with this number of pairs is much less typically than n most of the items don't occur that freedom right so I I only look at the piers which I know all the individual things occurred frequently enough and so now this is typically so this is usually much smaller than n it's usually actually much much smaller than so I could keep your right to less than sizes means it's much much smaller that's a semi-formal okay so so in round two I do this and now again heavy hitters on well so you you can actually do is just keep some counters say for all all pairs or if this number of pairs is very big you can actually run on you can keep track of each of the individual k and then and then run is on for the heavy hitters again with the headlights absolutely that's so but now if it's not one of the items that's it might set that I've kept track of one of the K items that I found from round one I can ignore that completely I don't need to worry about that I only care about things that occurred in both of the pairs that occurred where I see a market basket things that occurred on you know at least anything interesting that the ones that were interesting after the first filter okay and so then and what happens is then at the end of this I'm going to get this on a set of these heavy pairs right so this is going to be a set D 1 DJ so you know the b1 b2 may not be big but be one bj b7 is going to be big red so and now this set of pairs is going to be much smaller okay now in the third round I'm going to consider only triples but so every triple has each of its individual pairs have to be in the set for round two okay and so they're a lot there could be M cube triples or if there were case sets in and around one there could have been k cubed triples but now I only consider the ones that the mold is also home so then this filters it again and then you keep doing this in each brown the set of things you keeping track of tends to gets much smaller and smaller that typically the largest set have at the end of round one or maybe round two but after that it usually gets much smaller very quickly okay so I I didn't you know I didn't go through this in full detail this is the main idea to kind of that if you go on the if you go in the notes there's a there's kind of an example we go through with this but this the small set here I'll just let me just show this to you quickly for the notes all right good so here's here's my market basket data set each of each of those t 1 2012 is a basket this is a set of items that I saw okay and so now in this example we go through I want it to be in at least one-third of the baskets which occurred at least four of the baskets what are the objects which occur more than that okay so in the first pass here the ones which are involved a Kurd these four times so to occur is 5 times 3 years 4 x 6 and 7 occur 84 times and ninety five score times right so I have five things left out of out of 10 after the first round okay so two three six and seven and nine survive so I'm going to get these these pairs and then I make another pass over the data again over the pair's to 3269 occurs at least four times now I only have one pair so I can't have any pairs at each triple to occur that many times so I can stop I can return this maximal set and also kind of the one fights are two bites of three by itself and nine bucks so kind of have a clever algorithm but that's basically there's been a lot of other work kind of using this the same idea for counting we're finding clicks and grass there's some version that works kind of like trick is to filter stuff okay so I'm just going to quickly mention the relation to bloom filters for two minutes to wrap up grab a bus and then I'll do this and then acting probably I went through some stuff quickly um that the difference between the bloom filter so in the count min sketch is that a bloom filter it doesn't keep a counter it just keeps its only keeping a single typically a single hash function sometimes you can use to two hash functions on the same thing so it will keep a single row a single hash function each one and this will go from our set n 2k but the difference is each of these items so in here will say this b1 b2 b3 up to be okay so each VI is going to be 0 or or one is a bit as you get compression by storing so this is only for storing a set when you want to check whether something's in the center note and so Steph's drawing a counter which has this log in this log an overhead and you like to sort bits in each of these things okay and so it's so then initially you said all these bi is equal to 0 pencil then on the on the update step you do so for all elements in a set you want to put load them into the blue enter by tour it's actually may have a few of these hash functions so for j equals 1 to k you set the the use of you set the bit Patrick Jane of x equals to 1 so if it was already won it stays 1 if it was zero before ok and so I patched multiple hash functions into the same same domain right so this ash function might end up here and this one might go here so i'm going to set both of these values to one ok so now I I want to ask is X in in the set and so I say true if and only if from all J in t that be of hash j of x is equal to one so only if all the bits are set one do i do I say it's in the set okay so if you just use my hash function you may it's not that a comment to give these collisions but if you use them if you use multiple hash functions then you need everything not to be an occlusion and so you kind of get to amplify the analysis of it there's kind of a sketch of the analysis in the notes but the actual analysis is actually fair bit more complicated and it's a little hard to analyze but the the sketch gives a rough approximation of what's going on but you kind of overload multiple hash functions into the same set so instead of having hash functions hatch separately you kind of push them all together and then you put all your bits in the same rough so then caves would be a little bit bigger maybe the same space but the hash functions will set in multiple things to work okay so if you further it's similar to what we talked about but not exactly the same it's not keeping house is just keeping it whether something is in the sense it is going to have nose else negatives but might have a false positive okay so that's that's the very quick bloom filter just right so we'll start talking about high dimensional data and refreshing and stuff on next week monday I'll also talk to you a little bit about the first midterm which will be a week from Monday next week and kind of what to expect in there all the questions should be easier than four more questions and things that you can do with that calculation so don't start panicking about it but thank you cheers you 
F7lfqlyPib4,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-04-13T22:53:46Z,Data Mining (Spring 2016) Lecture 23,https://i.ytimg.com/vi/F7lfqlyPib4/hqdefault.jpg,UofU Data Science,PT1H22M31S,false,85,0,0,0,0,"hey so it's three so let's get it started star little bit late because of the hats so get some let's see some people managed to turn in their projects and skills stumble into class pumps that's good you didn't see I I post on canvas this morning after getting some questions about it if you turned in a reasonably final version of your your project report you want to polish it a little bit you can do so off until midnight with no extra penalty you need to have something something by now should have been turned in that could have been a fun report you know eighty percent way there at least for not kind of late but you want to polish a little bit more than okay that's that's fine i think that's that's better for everyone especially me so so that works the thing is on on on Monday the poster is due so there's there's a lot of stuff like I wrote a bunch of stuff about how to do the posters in in the project document so please look at that kind of just a couple things to point out highlights um you know just there there are two main types of posters in the world so there's an and be here where a is meant to be kind of stand-alone you put them up on a wall and you can just you don't need someone there to have the poster be effective havanese have a ton of texts with people go up there and they'll read like a paragraph on there the other type is one where they're meant for you to stand next to it and explain what's going on and really they're they're meant to be and eat for your explanation they're not meant to be stand alone you want to explain something you wish we had an equation you wish you had a figure or a chart that you could point to while you're explaining it that's tight be these posters should be tight beef ok so don't don't make too much text on there just just um kind of make make the big things is a cave for present right so you know focus around built it around the charts right illustrations oh okay so then there are there are lots of ways to make these these posters what is if if it really nervous about this for some reason but it you should be fine but the way which is acceptable to okay is just to make like eight or 12 slides and then print them out and then put them on together if you want to do that that's fine it's not going to look this good but it works get your idea across the better ways to create one big image and in that case we'll print it for you okay you can there are lots of programs you can use to make it like PowerPoint or keynote just make one slide the right dimensions and put everything in that one side and then you can if it's the right dimensions but that scale will scale it up and reprint okay so don't worry too much about that and the more fun the more while you know the better there's going to be a voting by your peers of who is the best posters and you can get some some bonus points for having really cool poster so the hopefully this is a fun aspect now that you know you fish with the reports now you can actually do something even even more fun of the little artistic side there have been some really really cool posters in the past so someone did like text analysis and made it look like a whole post newspapers versus something like that I don't know but there's there's there's lots of crazy fun stuff you can do so so you know the I I hesitate to show you other posters because the best ones are the ones that don't look like the other closers I've had secretly so so so going and kind of go wow but remember not to put too much text on the poster avoid full sentences stuff like that and have the title and the names match that of the project reports okay that will also be important okay so finally there's a version of them due on Monday and then we'll try and very quickly get you some feedback and maybe ask you to make some changes if they're very far from what we were hoping they would be and then I think I I'll work this up I'll work this off at end of the week but I think by next week wednesday i'll have to you'll send the final version 2 to chris coleman who works in the cs main office and he will print all the posters for us if you're doing the beginner slide slip you can go print those on your own if you are but if you want make one thing image will print it it'll be two feet by three feet or three feet by two feet either way so either orientation is fun you could you can make it 45 degrees as well if you wanted to yeah well we'll print the PDF to give us and then it's up to figure out a hang it on and so yeah so well and i'll put up details exactly in this document and i'll post on canvas about the final details about this this will depend on the date we decide to an actual poster session okay so on this morning I posted two options for when the post-recession would be one would be the which i have scheduled is the normal exam time for this class which is main second i think 3 30 to 5 30 and and no one should have any conflicts that right because exams are scheduled so if you're in this class you can't add new conflicts and so that's the safe date the other one is in april through 27th which is a week from wednesday and that corresponds with me the same day as the senior projects and so the reason for doing that um would be that you'd get a lot of foot traffic from the senior projects we'd scheduled so it's basically starting as the senior projects are winding down and roughly the same area last time this was near the atrium and that and the upper part of in the upper part of this building and and so it works really well um it's kind of it's busy for the seniors day but then it's over with so you know if anyone has any objection to they have something else scheduled from like four to six that day with and I actually can't make it to be be be 4 3 is 6 30 please please let me know it seems to be that on canvas so far people seem to like the April 27th date so just show of hands anyone like the april twenty seventh day tour the main second date so April 27th may second okay okay so so it's about even okay so if if you like the maze second date how so on canvas just so I see representative people there as well right now everyone on canvas is cindy april twenty stuff so so if that continues if you have a conflict april twenty seven then please let me know and then it's then i probably will if there's a good reason because enough people more than one person as a conflict will probably move it to may second just because otherwise it's not gonna work okay alright so discuss that on canvas and all decide by the end of this week but that's why the posters are do so early because we need to give enough lead time to for their printer okay and uh great okay so any questions on posters animated yeah that's right sure yes there a few groups that what what they're posted to be or some aspect of the interactive or animated that's um you know that's also fine there'll be some spots that will have outlets and I'll try and have some tables set up last year we had it in kind of the part of web in between where those big classrooms are and so there are couple tables to decide it kind of had this semi reserved and those tables have power behind them and you can set stuff up there so so that's absolutely fine that's great as well yeah so you don't need just the poster but this is a great visual aid for having a discussion ok great um and feel free to turn in posters earlier to will try and give feedback earlier you don't need to wait to the last minute although i know everyone likes wait to the last minute yeah ok so today we'll talk about some MapReduce and also sums and also distributed file systems ok and so a night ask this earlier in the class how many people I've heard of MapReduce before this class and all my people took this class because MapReduce was on the syllabus yeah so in the past that those are numbers have been drastically dif I've reduced a lecture on a MapReduce because it's it's no longer like you know in the zeitgeist as a very important like is the important thing about about about dealing with big data so part of this will be the story about MapReduce and also these distributed special type of distributed file systems which are outliving MapReduce itself ok so you know this has to do with comes some big data and MapReduce is kind of this system that was was created internal to Google which is very integral and how they handled and done with with big data but there's have a larger story going on here and that's that you know when you think of this this big data I want to kind of mention a couple of you know this means different things to different people and often you ran something big data just to get funding or some sort of recognition for it but there's actually some some truth to some of this um so there's kind of a couple properties of this that we will kind of focus and that have spurred things like these MapReduce and these distributed file systems so so what is is um that the data is actually very busy it is actually enormous with it this is it is is this means that your data is at least in the terabytes or in larger than this all right so this means that it does not just fit on your lap a few of you ran into problems when dealing with data because you said well if I manipulate it this way it's not good for my laptop or not I remember anymore but that produces meant for when it's needed to fit on on hundreds or probably thousands of machines okay so this is really really big gear and so this there's data like this inside of these these internet companies they're collecting all sorts of stuff the National Security Agency probably has the state of this bit so they're they're places where it's you can collect data automatically in some way and the storage is an active in one machine you need many many machines to destroy the state and so I'll explain why this this motivational talk about today the other thing is that the data is um is pretty static so that it's not really changing over time each you've collected data over time or if it's changing its usually so so usually the updates are just um these updates are just depends where you're you're getting new data use add this on your data set there's not a lot of rewriting of the data once something is part of the data set it's part of the data set and so this means that we don't necessarily need to go in and change something we can store it and then it's there and then we want to play with it from afterwards and into and the beds are going to be easier to deal with than actually going to changing each each of the pieces of data you could go and do that usually with these systems but it's not kind of you often don't have to worry about it which makes it nice in certain data structures if you guys have get into advanced data structures even things like like dealing with like a red black tree doing deletions are often a lot more tricky than then they seem to be at first and this gets often you don't have to deal with deletions only attending game okay and so there's thinness part of this and because of this data has been what I you know you could call like the data science revolution and this is kind of a way of thinking about about how this enormous data has changed how we do science in general so I you know I sometimes give these he's over V talks at all I'll started with this this the story about how classic science so if you've learned in high school you learn about like what the to learn about like the scientific paradigm right remember remembers with the scientific paradigm is observation hypothesis experimentation uh yeah good good so I like to write it a little bit differently so so maybe you start with observe but then somehow the first step here is the hypothesis you come somehow think in your in your head you know you think of this hypothesis about the world and then there's the experimentation but often what this means in terms of from a data point of view is then you go and you and gather the data so once you have the hypothesis you then go in and gather this data and then once you have the data you suppose you draw some conclusion and so in order to draw this conclusion you often run statistics statistics too many teasers alright so you run some statistics in order to draw these conclusions okay and there's and like this goes back I mean 100 150 years in until the last decade or so most science was done in some manner like this and the fact that you make the hypothesis and then gather data is often essential for the real fundamental hypotheses within the statistics for those to work there's some assumptions that this data is not correlated with with other things this is strong specifically for the statistics you're generating and so you would calculate some statistics and then draw some conclusions and it could be that your hypothesis was wrong and you go back right and you you make a new observation may be based on this conclusion they can do hypothesis and start this over again but then when you do it over again you should in principle gather more data right um and so there's there's some kind of you know flexibility would that fifth people often do with this but this is the classic paradigm the data science paradigm usually starts by saying you have moms on some enormous data that you started with that has been just collected maybe you've done this a bunch of times and you keep pouring this gather data down to this data set that you just kept around these log notebooks and so forth or maybe you're an Internet company that is or Google the goal is to just gather and organize the world's information at some point they had a few models don't be evil the second one I think was gather and organize world's information right and so that that was for like a long time after they built this rich age and that's what they were doing they were just trying to figure out how to do the information they figured profits and other stuff would follow him and if that was true there are these things like the Sloan Digital Sky Survey where the point that this this used to the Astronomy was Miss follow this thing that someone make I patha sis they go and they they request time to observe a specific part of the galaxy with the telescope they'd wait weeks or months to get a chance to observe it and then based on what they saw they would decide whether their hypothesis was correct your hands and they also would not share the data because it took him so long to wait to get it one telescope so the Sloan Digital Sky Survey said well let's just go and get funding to have one telescope dedicated just to collecting data and then we'll share with everyone right there's there's also like on the Human Genome Project right thing the goal is instead of testing individual people for diseases we would try and put together example of the whole human genome and then make a base of this and make this public and this would kind of spur silence and so you started with these enormous enormous datasets start at around two thousand and once you do this then this step is you trying to do some slow data mining on this on this data and then this was also to draw some right so you're still drawing try to draw the same sort of conclusion scientific conclusions but you've kind of replaced the hypothesis in the statistics together in data mining supposed to be with this class is about I guess right now that there are certain issues with how this interacts with classical statistics and these are not completely resolved yet and you're but you're starting with this data and then you're trying to sense you for my hypothesis while checking whether it's significant at the same time right and so this is kind of this new view of data science okay um this was supposed to be about MapReduce okay so how does MapReduce fit in here well Google for instance had all of this this this data just this enormous data and they wanted all of their engineers to quickly be able to access it but it was so big it was a team to use if you have the entire the entire web breath where you have all of the texts from all the web pages how do you and you have a hundreds of Engineers how do you let them all an experiment with the Train create new products with it it takes so much time to work with this these big data sets they don't fit on one machine remember these are these needs to fit across many many different machines so how do you make this accessible and so you know at the time there's these classic database engines if you take the 5,000 level database system class you'll learn on all about these and these were designed around these highly structured data that often fit up on machine yes there are some extensions that are paralyzed able but they're kind of that's not the thought process the thought process was to take this very structured data and find out paralyze it where are these these distributed file systems so these are distributed file systems were thought from the start of flexibility and parallelism about them right so before MapReduce and I think this will be more of a lasting kind of thing is these distributed file systems that were these that we're based on these key value pairs so in a traditional database you would have tables or every every object has ID and a bunch of attributes here you have these key value pairs and so you're going to store these and these are because they're not a structure they can allow for different sorts of data to fit in the same the same one big system ok so these distributed file systems these these key value pairs could be things where you just be key is going to be some sort of ID let me have some examples here so then the key could be something like a log idea and then the value is the actual blog right so if you're running a system it occasionally spits out air messages or these log messages and this big log table is gonna be like this so it just has a ID you describing it then whatever you want in the law it can be processed to scan and look helmets later you can put more structure within the value part that's very flexible flexible it could be that the the Kia is like a web address and then and then the value would be that HTML or if you're just going on like the web graph you might just want the outgoing links right so if you're just care about creating this doing pagerank send for every web page which has a unique ID if Kia's web address then you have the outgoing links other web pages right so then that would be wasn't the value or if you wanted to process it for building an inverted index HTML somehow just made the text on that contains all the information you so so or if you have a document ID so you have a corpus of documents there are these collections of these news articles for instance you may have come across then you know then the value might be just a list of words inside there maybe you're using the backwards model maybe it's just the text in the documents right and so more you could flip this around you could have that the key is the word and then the value is is something like the list of the of the documents that is containing the okay so i have a list of all the documents which maintain this work so this essentially is the inverted index to some extent right if I this is a search query than if I find this key in my database then this is the list of documents or the list of web pages which contains so you can think of these as these p value pairs yeah question about possums was just like you see like a different cut this we talking about here or you're talking about like just like you have all these files and the keys are in the veins of the files as it wouldn't matter right yeah so some of these so that no sequel databases are kind of usually have even more structure on them than this but that's the right thing to think about this is like these new SQL databases yeah yeah so that's right but you can think of this more generally more flexible so we're in this for here we're just gonna say okay it's just just a key value okay it's a pencil down these and and so I'll talk a little bit about how this is stored in a second but just there's kind of some kind of evolution here there was I mean there's some stuff that came before this but we'll talk about there was this this this this Google file system and then sitting on top of this was MapReduce and this was inside of Google and then there's this open-source version though the dupe file system which then a Duke was sitting on top and these are open source versions of the Google file system and an MapReduce correspondingly you know they weren't quite the same and because the Google file system was internal there's this was kind of the first view of it and so this part was inside of Google and it was talked about in some research papers but people didn't have access to this and this took maybe five years or more later to actually develop in a stable form so this was talked about even in research comedian we'll try to study it before this was actually stable what's happened since then is that there's been a lot of other stuff that's been built on top of HDFS most notably the spark system which is it looks operates basically is the same sort of processing goals and behavior as Methodist in in and also I do it's kind of this replacing them it's a there's a little bit more complication and getting stuff to work into implementing it but it's gonna be much more efficient and we'll discuss why that is there's some structural difference between spark and do men and MapReduce and so this is one prominent thing but there's a whole other ecosystem of these other tools which sit on top of here there's something called on pig which is supposed to in game is right which is yeah so big I think is is the SQL this kind of a way you can query HDFS in a way that looks like SQL and this is a horrible mismatch it's not the data is not organized so the same things that would be efficient in a traditional SQL database would be a fishing here but you can write them anyways and so these tend to be like can be really really slow but people are familiar with SQL so they wanted this so they built it and then like oh why is this so slow and hopefully you'll realize through this lecture why that's going to be the case but there's all bunch of these other tools builds on top of the HDFS and some kind of a dube has kind of is kind of not being used as much as being replaced by spark and some more specialized things there's a variant that sits on top that does some stuff with with graphs especially for graphs and that you could have done that and people trying to do them they said well i can write a separate that's a slightly different interface for a lot of graph based things and those are going to work more efficiently than they do so this is how to not be used much anymore and I've heard MapReduce of being phased hours mainly is fees that was inside of Google there's more specialized tools for certain tests but again we'll talk about with the system because they kind of give a general overview of what can be done and then you can imagine specializing these for certain sorts of more specialized tax you might want to do that what else becomes yeah so I uh saw MapReduce is more of a concept system but here's a historical system it's yes so it's it's both it's a system that people studied and as a concept because they didn't have access to it and so the concept of MapReduce is the same inside of the Duke and it's almost the same as inside spark except for one or two very critical differences and then there's these things that are inspired by methods there are these graph-based libraries and thanks for dealing with logs better in specialized ways that are inspired by batteries there's been some computational models a way of thinking like how we talked about streaming algorithms it's a different way of thinking about how efficient an algorithm is people have looked at that for the concept of MapReduce and try to formalize this I taught in class a few times a few years ago where we looked at some of these algorithms but there's there's like a bottleneck it needs ram complexity of MapReduce which seemed artificial to me five years ago and now I mean enough not the only one but now spark has basically gotten rid of most of that that overhead so that things that were most important in a caduceus and hadoop are no longer as important in the model so this model itself in some ways is still this still around in some ways it's changed dramatically what has stuck around a lot more is this HD offense and I'll talk a bit more about that yeah so part of it is this key value pair which is influence these no SQL databases as well they're built with more structure on top of this but the other thing is how the data is stored and so I mean one way you could think of it as you just have this one file right that this is you know this is terabytes right terabytes of data and it just has a key and value he value just have this whole file down here filled with this you think okay I need to store this file and I need lots and lots of space get this right but it's you don't just want to think of it this way what you actually want to do is to break this up into into these blocks interest into some blocks of data and usually they're not too small like you would think of it like a cash but they're like 64 megabytes each block okay and then you're going to think of storing all of these blocks separate and they're filled with key value pairs okay and so what starts to get get interesting is that you think of now having these different machines call this m1 m2 let's say three up to it's called us ml right see of all these L different machines and this remember this is like hundreds or thousands of machines I instance and so what you would do is you would put for each block so you'd have B 1 B 2 B n okay I'm so you'd put the one across your replicated on a few different machines so each VI you put roughly on two or three copies okay and then if you look at block e to which might be right next to it in the ordering so then if i look at this block on p2 maybe on one it's right next to it but that's here and then it's on this this other location here so it's there on the same machine here but then there otherwise not on the same sheet there's not locality between these plants so no locality so traditionally if you take it i like to think about data structures often the key to a data structure is to put data locally next to each other so that things that you would need to work with each other you would put next to each other so it's easy to do comparisons like if you sort something or to look stuff up for rain searching we talked about these hierarchical range trees if you organize them hierarchy and spatially things that are next to each other in the same part of the tree here you're you're doing the opposite of it you're making them non-local right I'm putting blocks that are right next to each other in the war during so this was this was v2 and this was the one I don't necessarily put the next to each other on machines in fact this is often done at at random right so you put each block you replicate it two or three times and then you randomly put it on each machine okay and and so this this is how the data is stored so you think you have this huge value break out of these blocks of 64 megabytes so now we huge you replicate each block towards of you x and hard drives are cheap right so even though it's terabytes you're willing to go to two or three times a big because at least Google hard drives are such a small expense of their total operation and then so they replicated two or three times and then they don't store things look ok so so why does this structure make sense why is this what would be what are the advantages of having these these blocks store with toppings and not the same location seems to be um yes let's say so one is going to be redundancy so why um so speed yeah so so we'll talk about speed is more complicated and actually speed is not usually the goal they're like it's going to be it's gonna be faster than meeting over the data set one by one but they're more focused on scalability and we'll get back to the widest is redundancy in four words this yeah so okay so um how many see maybe they're like 30 20 30 people here today how many people have had a a machine crashing last year okay oh boy it has some bad look in this putz okay so about five people so so maybe this twenty percent in the last year now let's say that let's say out of let's say instead of having 25 machines there are a thousand or ten thousand machines and your data is sorted all of them right and so if if in the last year then let's say it happens once so so so what's it that was twenty percent of people that means roughly every like 70 days of a machine to run time it crashes right and so that means if you have a thousand machines that you expect one to crash every 70 days but that means that when you've way more than 70 machines right youths let's say it was 700 machines so that means you would expect 10 of those the crash every day and it means 12 crash every two or three hours that's probably a little a little pessimistic most you probably have laptops where laptops get drops data center computers probably don't get dropped as much but they still crash right and so it's a common occurrence when you scale up data to this may machines that you're going to have these crashes and you don't have to restart everything event happens so when if you're trying to so if you're trying to process something over all the season and the fur machine fails and you were using black one and walked two of this machine well what you can do is you can then look up lock one on Michigan three and black to a machine to and you have this redundancy okay so it's already built it you don't have to wait for that machine to be actually actually rebooted the data centers are so big what they typically do is they cycle through them and it takes one day to do this and for all the machines that have gone down the last day they reboot them so this machine may be down for a day you know they're like that's fine we have backups right so no actually probably when they know the mission is down they'll actually replicate these on another machine so they have they try and have three active copies so it's so then also because these are not on the same machine because this one went down you may have finished processing what you did on the other machines when you when you are canal when you realize this is now you don't have to wait for this machine to hit the exact same stuff you could then say well do what I needed walk to on this machine and do what I needed block unblock one on this machine I can do them in parallel and that was because I did store them with the same that I didn't see story the whole machine identical to another way i did this basement blocks okay so the fact that i spread this out makes this failure much easier to recover from and much faster to recover yeah so well say wants it so um so the other thing is that so hetero janaya t that's a big word that I may have spelled right depending on my of my penmanship so it's I can do a lot of different things with this format as long as it fits within I can the data within a walk is enough to like capture a few keys I I can do this with most sorts of data all these different examples I before I can break down a block so i can i can do it in this way the format is very flexible but also when you have a query what you want is that the computation is carried out in parallel and so if your data is all stored say all my day was stored on machine one that was relevant to some process well then the first thing I need to do is either i need to run it all with machine one or two do it in parallel I need to send all the data across couple of machines to do that parallelization for now if I start looking at it I can say well I can I can run on machine to and machine three at the same time because the data is already spread out across their duties so I can the parallelism is it's already set up to run a parallel yeah are these just clowns that tops of their both right so the idea is you can bring the competition actually to the data and separating the data to the computation the the cost of transferring the blocks across machines is a pretty sizable and you want to avoid that if you can and so that the data is going to be in this format but fairly permanent on these machines the machines go down every every so often but each machine it takes it could be a year easily a year before it goes down and and so you don't need to update the data on each machine right if you're just appending you data you can put them you can spread those across different machines but mostly data stack so they're sitting there and now a lot of different engineers can ask queries of this system and when the machine is free from the other queries that can run this it doesn't have to move the data to do so the data is kind of static there it's a capsule the whole web they have all the laws of some of their their whole search they're all the search box or stored across this way and what page people link down all that it's sitting there and they can ask these queries and the parallelism is already built in okay so this is kind of this principle of this very perilous of a file system is very flexible and it's it's redundant because the disk failure and this redundancy breeds a resiliency so when the disk fails the whole system doesn't have to stop there's a there's this other view of parallel systems that's often the people work in these high-performance computing system so they also deal with parallelism in this and it's much more care is to keeping into allowing the machines to stay up for a long period of time that the code is very very robust if but if one machine crashes they often have to restart a lot of what they've done they know there is some stuff with some some checkpointing so they can try and recover partly through the operation but this becomes kind of a big pain to do a lot of systems and so this is the opposite saying if it fails I can keep going and because I don't need I can make it much more parallel across different machines I don't each machine to be as fast it's okay you know there's a lot of cost between transferring data but often I want to avoid transferring the data or I just I'm going to do it at one step and this will be MapReduce which we'll talk about next and and so uh and it's like if I have just more more machines that have to process less data protein right I think just I could scale this out very fun I can make this very scalable across many many more machines and it doesn't really slow things down so it's easier to buy more cheap machines than fewer really fast machines and so at the time when this was coming out they showed the breakdown of like one really fast you know she in the head like maybe many cores within it versus a bunch of each cheap cheap machines and at all now computational power very cheap machines you get for the price is much much more and so this kind of went into it but I think this is no longer be kind of it's still a big cost at Google but not as much as as like the as I think just Engineers is pretty expensive so not sure whatever that where the big costs of these these companies but it's very small compared to the profits that it is for yourself okay so so this is the system and this you know as I mentioned this is kind of this process of storing this huge data is outliving MapReduce sent to Duke itself but let's talk about kind of MapReduce and this is essentially the same concept as as Duke here okay so the idea is that you're going to be sitting on top of this data in this distributed file system and you're going to access it in a few fairly simple ways and then all the stuff with dealing with the parallelism is going to go on behind the scenes okay so you're going to do these these are you can kind of think of it as these three steps and you actually the first step is called map there's kind of a 1.5 step that's called combined which is optional there's a step called shuffle and and and then there's reduce okay and if so as a programmer you would just write what's in reducing what's a nap shuffle is done by the system okay so map is is um so let's let's break these down this this operates on the data in the distributed file system okay so what it's doing it's going to read the key value pairs in each block by block and it's going to somehow process these key value pairs so it's essentially mapping so it's it's going to there's me some map operation which is going to take in one key value pair and it's going to output another key value pair okay it's gonna be the same basic format um but what the key means and what the value names could be different and I'll go through a bunch of examples of this it could be the key is the document as a web page and the value is where the key is the web address the value is the actual text on web page and then here it is a one word that you can found on web page and the value is the link to the webpage so this might actually be a set of these of these key value pairs so it might it's going to read each of these as going to have good potentially a set of key value pairs so what the what the shuffle is going to do is it's going to the shuffle that's going to put all key value pairs such that they have the same key on same um machine so this is where the data is moved this is where the data is moved in between machines so it's going to output a bunch of key value pairs from the map step this is done on parallel across all machines and then you've got a bunch of these key value pairs and all the ones with the same key are somehow going to get moved on the same machine hopefully the same block if possible but they could be in consecutive blocks in the same machine they might be coming into this machine in a stream there have been a few different ways to think about this and and and and then this reduced step is going to essentially get this a key and then a bunch of these values and it's going to output a set of these key value pairs ok so this what this does is to put all the things all the key value pairs of the same key on the same machine the reduced step then says ok for each key i see i'm going to get a list of these values it's going to somehow all of this is now going to be sufficiently local so it started that everything was non-local then the mapping tells you how to localize it the shuffling which goes on behind the scenes on actually moves the data and then the reduced step now has stuff localized by this key value and then you can process on all these values together to get some say some new key value pairs so every intermediate step you think of it in these key value pairs and so these are the three steps and as a programmer this always happens so you know it's it's kind of combined based on the keys so your job is to write this map operation and in this reduce leverage and they're often very very simple just a few lines of code each of these the hard thing is paralyzing this across on machines if one machine fails you want to you know you have to go get the data from someplace else and all of that is the system MapReduce for Hadoop that handles this behind the scenes and it does the movement of this data and if one machine becomes slow then it or it's it's taking too long it will send the data elsewhere so what might happen is you might get some key that has a ton of values and you don't want anything else the machine that has a process these you don't want to have to process anything else so you would send those other similar keys other keys you don't want to wonder if on the same machine and the MapReduce system to Duke should take care of that behind the scenes for you there's certain things that will be hard to avoid them you need to think of Kevin out from our algorithm design perspective to make sure those workers okay um and then this combined phase here this is basically reduce before shuffle okay so it would you do this combine this map can output this set of key value pairs which are all in the same machine this shuffles with the data moved sometimes these are kind of these are repetitive and so you don't want to send all these across the network if you don't have to so you can combine them before you send them across them so often you're going to redo the reduced step locally before you shuffle and that's what the combiner feels okay so let's so let's go through some examples and then I'll talk about a few few issues kind of as as we go okay so let's so the simplest example and Matthews which is like which is like the hello world of mattress is is the word count so if you look this up online for example this is going to be the first thing thank you fine let's just there's a reason it's like that you just you just want to let's just it's a really good first example okay so what you have is your as your as your input is is going to be a large set of some of some documents and so each of the document you're going to think of as a as a set of of words and so for the output what you want is for each word how many how many times does that word occur so they counted that work okay so so if you're looking at the word apples how many times is the road app will occur in this huge document right so think of all of all of Wikipedia okay so so now so how would you write so now we just need to write these map and reduce steps right so we have these documents which is just the document the key doesn't it's not so meaningful but we just have this list of words how do we how do we break these MapReduce steps earth yes so what we're going to get is the input essentially the value is just going to be word one or two or three squared n right and the map phase is just going to convert this into word one in account one so the value now is going to be account or two and that I see if once accountant right so the advantage of of this thing where I'm thinking of I do think very simple I'm just reading through this document I process it through memory once every word i turn it into this key value pair okay very very simple and this this reduced step so now once I have all these words and accounted one next to them this reduced step is going to get some key which is is the word and so a bunch and so it's also going to get up a bunch of these values right so it's going to get for one key the shuffle phase will put all the same words on the same machine reducer will be able to see for this word you know bird is the word can meet then help one for every time it occurs and so then the output here is just going to be the word is the key and then here is just going to be one de que vea so I so I'm just going to sum up all of the cats so just going to write a some of these things right this is where kept very simple the map has an input grab a key the key here is it's it's the document it's you have a set of these it's a weekend aim of the wikipedia it turns out it's not going to be important so you just care about the values here and often the initial the initial setting maybe you may have many things door down on this distributed file system basically the key saves if this is a Wikipedia article so if the key says wikipedia article then do this if is the key is some the all the HTML links out of a web page then ignore it right so you only operate on the key value pairs that satisfy the initial victory so it's kind of a yeah so it's it's often when you think about these you think about okay I only not only have the data in front of me that I care about but often on the system there's lots there could be other stuff their dead residence so this is is very simple um so you know it's good because the whole thing is it is incredibly simple but at the same point this will not work there's there some big failure point here something something horribly wrong as like it will work but it's going to be it's not going to work as well as you would think what's going to happen if the word equals the yeah yeah it's essentially it's going to blow up so this occurs say seven percent of all works okay so keep mind my dad was supposedly so big that I needed thousands of machines to store it now one seventh of or not what said but seven percent of a thousand is going to be something like like 30 machines but I went to all this on one machine so that means my shuffle step is going to send all the bell words haces this one machine which then has to process seven percent edita so I've lost a ton of my curlers this is doing this sucks alright this is going to take way way too long okay but there's also a simple fix you do just yeah so you could say well if if this one machine is getting overloaded I need to send it across different machines and kind of in general you could do that in principle you should be able to this like in this way it's designed in this reduce operation should it may need to do some something global that required all this data to produce an answer let's say you're trying to find if a graph is connected your free missing one edge I mean no longer be connected and so it's harder to in general do it so in practice you could do that right you could say okay you would instead making the key just the word you would say the word and then also if it occurred in the first ten percent of her first one percent of all the documents that's a word and then an ID for the first one percent for the second one percenter so it has more information than in the in the in the in the key and so then they get said to say a hundred different machines and then you combine them together you could do something like that with an extra round and we'll talk a little bit about rounds but there's something simpler you can do and I mean it's basically in mind with what you're saying but you can have this combiner phase which which sits right here and so this combiner phase is just the same as is just going to be the same it as Canis the reduced phase and for each word it just it just is combining them the same way that you do on this reducer so basically means before you send the data off of the map off from the original computer it's on up in the map base before the shuffle phase you do this reduce that and then when you're adding up these VI is if they're not just ones anymore this still works and this works because adding is on is uh was we're late is it commutative was it's it's associative whatever you can you can add up some parts and then add them again again later and any sort of operation lectures like like doing a union or doing the max where then then you can do a combined face first and it will save a lot of communication so not only will you not have this last reducer the overwhelmed but then now of data moved in a shuffle step is also going to be much much smaller and so the bottleneck and the runtime is often either in the shuffle step if you have to send a lot of stuff if you didn't design it well or in this reduce debt so it's not finished until so there's this notion that happens here that's called the the curse of the last reducer so where you send a data often there's one reducer one machine that's trying to reduce that is taking a hundred times longer than all the other ones and sometimes this happens because the machine got slowed down because some other process sometimes because you designed it wrong and the word that is seven percent of all the words right so if you do this combine your face this will help with us sometimes you need some algorithmic thinking order to do this there's kind of a there's nice examples that we do we won't get into about counting the triangles in a graph and there's a way to design the algorithm to avoid kind of this curse of last reducer that kind of like it seems like a very simple difference but it can really change the that they are really we won't have time to get into that today okay so 10 minutes glass let me let me say a couple things quickly let me give another index oh so another classic palm is the inverted index and kind of their two main computational things that as understood that we're going outside of Google when when MapReduce became so important one was doing page rank which I have time off quickly sketch and one was the inverted Indians and this was this creating this index where for every search word there was a bunch of these relevant web pages rancid so that's the case here the input is is the web right so I have all these web pages and they have words on them right and I want as as the as the output I want to have for every word I want to have the list of pages which has that work right so for every word I want to have the list of all the page that has it so then I can just read off that that list then maybe I want to sort them by some searching criteria that may involve a drink or something like that so with each page I can also pass along if Hayden I can like sort things and some okay so then the is I'm going to go from say and the HTML into a set of these so there the words can have a word and then the page and some other info which might include this the patron gets me where to that's going to the same page and another info on here that's so maps I just turned the HTML into this other thing I'm just text processing and then the then the reduce step and so I can do a combiner on this if the word occurs multiple times and then maybe in the combiner can also have a number of times the word occurs that might be helpful again and then he yep I'm going to have this page 1 and n h2 h3 right and basically the output is is going to be almost exactly the same except i'm going to sort these page 1 h 2 up to paycheck so i'm going to have them in sub sorted order so that and this is maybe the search order and really all it would really care about maybe is the top 10 or maybe a hundred words in this order but there's some way that I've converted it into this output just now mind ridiculous ok so again very simple here ok so a couple of things to point out MapReduce is really good when the input is is a large key value here just document las accumulate hairs and the output is too okay if if the output is giving a value say is Buster's is or the single best page or just a count of how many web pages are there in the web right single value MapReduce is may be quite this this regular there are other tools that live on top of HDFS but you can do this a bit better and some bagel had a tool they call like I think they called they called a dremel like a drill I guess and and outside of house it was meant for having a small output where this has been for a big dog and there's slightly different in how you'd implement it but again a lot of the structure and understand how this works is the same okay so a couple of other things in the one to mention I've got a couple more examples in the notes you can look at if you're interested is rounds okay so what happens inside of a round of MapReduce you basically have all your data and you do some sub map phase on this data and it moves it inside and then you can think of having these other machines that are going to do this reduce and this shuffle is going to send them send them to all the right machines so that you can do this reduce and then off further reduce is again a key value pair that's what you think well I'm just going to plug this back into the map phase and then do another one of these of these shuffles over here right so you can do these rounds you can chain these together maybe I can't get my final output in one man your space I may want to first map it to some keys do some computation then bad example would be like if you're trying to load a geographer k-means clustering what you would do is you would you would you would matter be dated to its center right and then for every Center would be the key you would have all the centers that on each machine and you'd map it to the closest centers you would send all of all the data points to the reducer where the key is the center you then re a verge the center and then you were to repeat this again so for several steps okay you could kind of think about doing this in a remember you know like eight years ago I was looking out for trying to understand this stuff I was looking for examples of MapReduce and someone broke this example this is this is a horrible way to use it the reason is each round takes there's a big overhead inside of madness and in also in an ado and so a lot of the complexity announced that people try and develop these algorithms was to reduce the number of rounds needed you needed to do something why did it take so long to do around well each Brown you have the shuffle shuffle step which is very expensive where you're sending on the data and you basically you you don't really this the same machine here too may not be the same machine here you're sending all of your data once maybe the amount of data goes damage down there some like think of Hyrule likes often the data reduces every step and maybe you think okay it should take less and less each shuffle step but what's really going on is that in between the in between the reduced step and the map step this goes back to disk and then back out again every time it finishes we do step it writes the data back to disk when you read it you have to bring it into memory off the disk and you read it but it rented back to disk in each step and this is for the resiliency if the Machine goes down what's a memory might be lost but the disk probably is going to be okay and so they're really worried about being able to recover this information and so it kept getting written after this and this takes a long time just because it's on the run machine does not mean it's going to be in memory to do another mapping room even though in principle you would think this would be possible but this really causes a big slow down and so the big advantage in spark is that it's kept the same resiliency but avoided writing back to disk in order to do this it does something where you can it kind of is able to keep track of what happened in the previous map steps and if something crashes it just recompute sit in a quick way in an intelligent way it figures out what would have been lost and it stores it in a way they can easily be recomputed and it doesn't spend sum up back to disk image we rounds it'll do this in the background so so that it has some summer buses properties but it doesn't do this it doesn't wait for it to go to disk and then back again treatments and this make things like running Lloyd's out and there are other kind of iterative things that are very common machine learning it makes them feasible within such a system like this so this is so this round oak head was like a really big deal for a while and now with spark it's not a big deal but you have to be a little bit you have to specify some data structures call these on RT DS inside of this and to make this work sometimes so some stuff that sits on top of those so you can be oblivious to it but to develop cool stuff need develop these is a resilient where the coal forget what this our TV schedule but there's these certain type of data structures like it quickly recovers from stuff on crunch okay so if you see spark this is the big difference it's not letting back to this so the round complexity is greatly improved but it's built on the same system ok so I have two minutes left let me just really quickly sketch doing page rank on MapReduce um so the key step and pagerank you would actually want to do rats you want to do like 50 rounds of the step where you're doing qi plus one is going to be equal to maybe this was one minus beta this probably transition matrix times beta times on this fix is this cube all ones matrix which is basically just a 1 over n for every step that sounds Qi all right so you basically come want to keep iterating this step you want to do 50 rounds of this and each round of this is going to be one math and reduced its ok and so what's stable this is in the probably transition matrix is going to be in the distributed file system and what's passed in is going to be this Qi matrix and you're going to iterate this for this process okay and so basically this what the basic idea is to take this P matrix and then you're going to the simple idea is to stripe this P matrix and one have to free up to n and um and then you're going to similarly you're going to hit this with with Q which you can think of living here this is the Qi and each each part of Q part of Q part of it only affects this part of the P matrix and that's going to map over to this entire thing all right so what you're going to do is in so you're going to store each of these stripes each column is a web page and has like an average 20 links to it right so I can store a bunch of web pages on each HDFS block and so then the map is going to take i'll call this is going to do something like let's call this our is it going to get this is going to calculate this RI which is going to be my Qi the is Jane rj m j qi j so this part here is gonna be q IJ you're going to multiply this together get this RJ and then this we do is is going to just sum up q hi plus 1 is going to be the sum over J of rj times one minus theta plus beta over N and so this is dealing with it teleportation this is done inside the reduced up the map step is just multiplying this part of the Q matrix times this stripe of this whole matrix which you can store it and so there's a way to encode this carefully and I work through an example in the notes of how to encode this in these key value pairs and because this is really sparse even though this is really big each row should not be too big so you can store a bunch of these together and there's a way you can encode it the key value here so that the consecutive things are going to be next to each other inside the flock and you can do these together and so you can then multiply these pieces and you're going to get a bunch of these are Jays and the sum of them is going to be this q IJ that's going to work if you're not going to teleportation but you're going to have this other data or interim which is essentially tell according beta % be like Oh point one five divided by M and that distributes that part of the mass random okay so you can do the review stuff this way there's some issues sometimes some parts of this are going to be really heavy again like the word does going to curl up and so that's going to cause some of these these these columns to be much heavier and and to get around that sometimes you want to do he is blocks instead so you want to break it down into this way and and then you have some some replication of this part on each of the nodes but this kind of takes care of these really large cases of effect like the words the occurring so that occurs on the mass side instead on the reduced side but so this structuring it this way can have some more advantages and deals and with the sparsity of your data this will help a little bit so that these are the two main ways to do it in and this comes up a lot if you're dealing matrix multiplication in a in a HTC style setting as well again they use similar sort of these are the two main ideas these blocking and the striking of the matrix and that allow you do these matrix vector multiplication is very still somewhat scale with efficient close all right so hopefully projects went well and and have fun making the posters feel free to send me on examples if you're not sure if it's going well be some feedback before the formal "
Bj9eB97rKdQ,22,DataMining Introduction Class | Explanations with Examples | DataMining Notes,2017-04-05T12:50:38Z,DataMining Introduction Class | Explanations with Examples | DataMining Notes,https://i.ytimg.com/vi/Bj9eB97rKdQ/hqdefault.jpg,Share Knowledge,PT1M13S,false,3,0,0,0,0,hi friends this video gives a clear idea about the data mining and how the data mining has evaluated and how it started and how we are just using the data and what the necessary and why is data mining is important so if we see before thousand six hundred there is an empirical science where there is no computational science so in the year nineteen fifty-two 1990s the data signs the disk computation computational science as a ball and from 1992 till now there is a data science necessary so for which we can say that there is a evolution of a data okay because when we take an example as hospital or supermarket there is a data we have to collect data we have to collect the information about the customers and how many storage wave wave so there are confusions and we how to just order the data in the order so rebels are necessary for determining hope so we were clear with this thank you 
KRpeDMRr0Xc,28,"With the data explosion occurring in sciences, utilizing tools to help analyze the data efficiently is becoming increasingly important. This session will describe tools included with SQL Server (Yukon), and Wei Wang will describe the MotifSpace projectΓÇöa comprehensive database of candidate spatial protein motifs based on recently developed data mining algorithms.    One of the next great frontiers in molecular biology is to understand and predict protein function. Proteins are simple linear chains of polymerized amino acids (residues) whose biological functions are determined by the three-dimensional shapes that they fold into. A popular approach to understanding proteins is to break them down into structural sub-components called motifs. Motifs are recurring structural and spatial units that are frequently correlated with specific protein functions. Traditionally, the discovery of motifs has been a laborious task of scientific exploration. In this talk, I will discuss recent data-mining algorithms that we have developed for automatically identifying potential spatial motifs. Our methods automatically find frequently occurring substructures within graph-based representations of proteins. The complexity of protein structures and corresponding graphs poses significant computational challenges. The kernel of our approach is an efficient subgraph-mining algorithm that detects all (maximal) frequent subgraphs from a graph database with a user-specified minimal frequency.",2016-09-07T16:08:40Z,Datamining in Science: Mining Patterns in Protein StructuresΓÇöAlgorithms and Applications,https://i.ytimg.com/vi/KRpeDMRr0Xc/hqdefault.jpg,Microsoft Research,PT1H19M18S,false,202,0,0,0,0,"each year microsoft research helps hundreds of influential speakers from around the world including leading scientists renowned experts in technology book authors and leading academics and makes videos of these lectures freely available you we started welcome to the station the topic of this session is called data mining and in fact there are two ruthle couple sections first topic is about secure server 2005 a platform for data money and the second half of dependency is about applying determining protein analysis so my name is rock weekend and I'm the kroger manager for sequence of determining team and join me is the dev manager broad or team Jamie McNerney so I work Cove a few slides and then Jamie will all give you something more help if you have a direct understanding of our product so as many of you already know a datum and secure server is way beyond the traditional relational database today there are lots of components in sequel server business intelligence is one of the key part of the secure server so let's see what we have in the deputies inheriting field so first the requirement for doing business intelligence is you need to integrate data from various sources you need some tools to do data acquisition from Japan where GPU data type you also may apply some data transformation Anna like this for example you may do some relations before the data is saved your destination or you may derive some new variables or new columns so that's the part or called data integration or et al once the data is gathered you apply some analytics on top of your data warehouse so the typical technology here is dimension modeling motor dimensional database also called OLAP irons data mining using statistical machine learning techniques are to analyze your data and the thought patterns laughs you want to do some report you want to publish the findings to a nice formatted way and send to your cut consumers the poor consumer that is the emails through websites so we need to have a way to present data and to format the result of data so those three areas integrate analyze and report we have different components inside secure server 2005 one box by a lots of different components so for the easier part we have the secret services on five integration services for the analyzed face we have an elected services and for the recording part we have reporting services 2005 and the focus of this topic is on the red path theatre discovery by determining so it's the data mining part of analysis services so Microsoft psycho server is the first pet relational database product that introduced data mining inside the relational package relation to edit package so we enter the game in secure server 2005 years ago and the most work we have done is created an industry standard as that time called Oh ddd 40 remaining inside there the important language called a DMX there are many extensions for sickle the release is targeted for develop audience and it was a vp product one product with the two algorithms when is Microsoft this increase the other is Microsoft crabtree first of them come from micros of the research so there are lot of research that some research articles written on the logarithm during the past five years we have invested a lot of data money and we will come out a product called secure server 2005 in a couple months that's five years ind so we really want to be the leader in the data money space we continued our efforts on theta minus standard HD I for developers develop air for we also have comprehensive features that we have data many algorithms cover all sort of determine Tod including classification association clustering sale of forecasting all those areas we have different algorithms and will also want to build a determining ecosystem which include data mining algorithm providers tools providers application developers as well as academic researchers who do interior many algorithms researchers on top of secure server platform so we think we're bitterly saw the leadership in this area so here are two key message first as I said we have a complete set of tools we have lot of UI code as well as algorithms we have each other ISM has a data mining viewers and those viewers is embeddable in your own applications so if you your students want to view that their money application you can host our viewers inside so they have a lot of time for the presentation layer we have integrated solution so we consider data mining as a key member in the various intelligence family other members including relational data warehouse or app integration services which reach here and the reporting services so we have tight integration we are one of the few product which provide all the data abilities intelligence functionalities in work package and we have group integrations there another message is embedded data mining we believe in the future there are more numbers there will be more more applications which embedded in our money inside and makeup it gives much smarter just like amazon.com today if you buy one book it gave you the combinations on other books right its data mining application embedded instead of hcp application on our website and the last is data mining platform so we think what our approach unlike other small eyes these years we r platform approach we want to make our secure server analyzer services the d-pad form for data money in order to that will have to be extensible and flexible we have algorithms previ interface which allows to the project we develop algorithms and plugins a sequel server and it should be considered it equally as the rest of secure server building algorithms we also have a lot of men of g models managing Authority model development of g model those orbitals allow users developers to build applications on top of the platform build web service for example so this let's use the overall component architecture of super silver dinner money so dinner money is part of an elected derivatives there are two components in a tunnel activities window left over the other data money and this on top of the slide the set up hang on applications or cry of joy models you can use o ddd traditionally db2 connector server use a do or use XML final itís actually if you use old only do it Aereo its action actually implicitly use XML for an elective so the server and the client shall communicate slow XML message and you kind the rest is your client and it doesn't need to be our windows you can use a Linux or embedded as a client that communicate with windows server we have a set of interfaces which will integrate with determine if example if you have a data many hours and instead of secure server you can use ETL to invoke data money to do our table magnaflow you can also use the potty services to do the report based on the result of a determining prediction queries and bottle here is the dynamite interfaces we have a set of algorithms nine hours from Microsoft its most of them are joint developed between sicko 17 and micros of the research we also have a KPI of H I for third parties so there are some iced we specialize in determining algorithms are working with us to provide some algorithms at the plug in and it will you are your students interest in do some projects we can provide some technical assistance here as well which allows your average to be seamlessly integrated with secure server and of course once you are with is plug in tight secure server you've got a lot of advantages you get advantages are all the API models so developers developers can build applications use standard HD I connect you are determining of algorithm and you can use standard query language which is DMX industry standard also we also provide provide layers to connect with different database source databases so you don't need to worry off with the sort in oracle DB q a tree to the theme that Leigh is covered by our old ed so let me talk about dmx determine extensions for the data mining permits for expansion for sickle this is the initiative from Microsoft started five years ago with a number of ice V and we try to build a query language which is similar to SQL for the determined users so developers data especially happy developers will be very familiar with this API will be easier to butte creditor many applications so they are sweet type of statement first create a nine model so this data one is very simple just like red table to create a money model called credit risk and we have a set of columns you specify the data type the key is customary and you have gender income profession and the last ones risk you have texted text for data type discreet and predictable and the last part of the settlement is which are going to use so if you look at this statement it's very easy to understand it uses gender income profession to predict credit risk based on Microsoft decision trees and the statement statement is training once the model it will create it is still empty so you need to train it and just the algorithm where this color patterns and those patterns will be predicted inside my data mining container so i will use in virtue into statement inverting to credit risk you specify the list of colors from the money model and then you use a trick statement this is a part of spinach eco the Select training dataset from any data source elephanta call or access or from Excel or Oracle for the data mining and an activity if our eco so training usually time consuming after training you want to do produce you and the prediction part we moderate as a select statement so for program here I select three columns the customer ID credit risk the risk this is a pretty predictable column from this money model and also predict probability which is a pretty fine prediction function pretty predict ability equity risk and enjoying to objects where is the money model it's not relation object but you can think about it a traditional table and the other is a new customer table is specify the column magnet here so this statement returns three columns customer ID critter it and what's the probability of the credit risk and you can stand issue this prediction query using a do and the result is in a tabular format so it's very similar to any database applications connect your database and a relational query and again get result back in tabular format so there are many models and I set up cases so these are the temple cases I have a sweet students I know the agenda parent income IQ parenting argument and the last color is is the pretty pretty blue color with a college player in some people are yes and people have college pain no so these are very simple most dynamic applications does support simple cases and we have a proposal another way in which is more complicated cases we call the next to the cases the reason is there are the many tickets we find you have multi value attribute customer bought our state a product or watch the state of movies they want to model those multivariate repeats and this gives a lot of expressive power for example dispirited I have three kids customers their demographic information but the list of movies they like press the scholar reg reg the disease movies I can predict customers gender based on the list of movies they watched first the their IQ and the age for example oh I can predict the list of movies they like based on their demographic information oh I can predict the list of other movies they like based on the movies they already put purchase or watched so next table helps you to more the data complicated business problem easily and you don't need to worry up the pivoted the algorithm not accurate of the layer before before I window they have any infrastructure code we have you to pivot it your tear and give you all the coloration comes so you are poised for you to develop algorithms you can directly consumed the converted data for you so it is expressive power as well as there's a lot of time for each year you don't need to manually create a very wide table with all those colors finally columns we have a rich set of algorithm as I mentioned we don't have time to go each of them but most of them we have research papers that the first wave Microsoft at entry it's the pod bus classification and the liberation clustering we have to building clustering algorithm one year the kidneys the other is am expectation-maximization we also the support automatically discover the number of clusters within a data set we have time series wisdom and the algorithm is based on research paper written by david hackman it's called up auto regression tree so to use using regression tree technique to forecast the future values of a time series it also find correlations among different series for example you can build a model with 5,000 not stock stock tickers for the past two years and it finds out quite a bit dare stop value has strong correlation to Microsoft future value it may end up using death period previous value but Michael totally Microsoft previous value to predict future value for Microsoft we have sequence clustering algorithm which is a hybrid of sequence and a clustering algorithm this is very useful for creek street anna like this so each webpage click web page is either LG is a state of the sequence and the transition is a model using the sequence and you can group customers or web visitors together based on their navigation patterns you can also analyze it for purchase as well or in the academic you know students which caught it they take some people take the course in this order some students take courses the other order and you can group them faizan the similarity association algorithm the one we use is very classic appear clearer type of algorithm but we have very efficient cementation then we have now you've is 16.9 you base a new low network mercadeo position puts at home and also the rapid we have text mining technique which is also picnic transport from Microsoft Research Asia it is analyzed your English text and the extract na and na praises was convert the o structured data to the structure variation of all night we can apply a naive Bayes boiled up with you classify your documents so now my colleagues anywhere show you some demos of our product see if I'm leveraging switched over here we go my name is Jamie mclennan and as always said I'm development manager for analysis services what I'm going to do right now is just give you a flavor of what we're offering sequel server 2005 so you can see a little bit maybe how you can apply it back at home in your research or your class work or also have maybe you can plug into this environment and why the advantages and some of the choices that we made the first thing that you'll see is that our environment here it's actually this environment visual studio so our data mining environment is visual studio and the reason why it's visual studio is that most of these projects we're doing data mining or if you're doing any kind of kind of business intelligence like an OLAP cube or reporting whatever you're actually getting a development project you're developing something which you're going to want to share out so just by being part of Visual Studio that allows us to do some things like integrate with source code control and a lot of other kind of neat features which you have for that kind of environment another thing lets me do is if you're familiar with visual studio as visual studio has a solution concept then you see it says solution up there and we have one project in there with that called AC I can have multiple projects in there i could have a recording services project in there which is where i'm going to show my results I could have some code projects in there which on maybe our algorithms I'm going to integrate into heat into this system or stored procedures I'm going to write as other projects or have a web front-end project all sides of same solution so it keeps it all together for the end user or the end development team that's going to be working on different parts they can all share the same thing so it's kind of a really interesting feature just to begin with before we even get into what the data mining part is now what we're looking at right here I already started the project and I created something called the data source view the data source view basically where you select your tables that you're going to do mining on and what I did here these are actually tables from a sequel server database but they could just be easily table from a db2 database on oracle database or even access database intentionally you know Excel and many other different backends we also support data mining from any source because you can do data mining in integration services which allows you to pull data from any data source and push it into your mining models I'm not going to be able to show that now but if you want to do data mining of a flat file you can do that directly integration services and also with some tricks you could do it in this environment as well so the data port view environment allows you to manipulate your data source and kind of look at in different ways it also lets you we have some rudimentary at data exploration tools so the first table is what we call case table and it just has a demographic information this data is a movie survey i did about people from people in microsoft just about their movie watching behaviors and you have columns such as their marital status who and their family pics what movie they watch you still love doing microsoft erwise pick the models and why's their husbands pick the movie they watch how often they watch movies and so on and so forth that kind of information and then we had these other tables in here such as this hobbies table and if i look at this table and see what it looks like it has basically two columns the customer ID and their list of hobbies right so one customer has many hobbies so it's that you know kind of a one-to-many relationship in this table and this is how we do the nesting so this is how i can say i have a nested table so for when i create my model i'm gonna have all the demographic information in there but i also had the list of hobby that this person likes so it gives me a lot of expressive power it's actually very interesting the kind of things you can do it for example quality health services in israel they built a model to determine to predict whether or not a senior citizens health is going to deteriorate within the next year using the previous two years of data and to do that they had all the demographic information and previously using other products what they had to do is search all the other information manually do you know quote unquote data mining you know where you're really digging through the data to come up with attributes that they had to figure out which other thousand attributes are going to use and put in their database and do mining on well what they did with our product is they said oh you know i'll just create a nested table of all the medications will use they're using i'll take it make a nested table of all the types of specialists they're seeing I'll make a nested table of all their chronic illnesses that we know about and use those as inputs to my data to predict whether or not their help and they ended up having ten different nessa table so they actually essentially they cross their information across 11 different domains to solve one problem so it gave them a much bigger expressive power so skipping ahead i'm going to go right into our wizard which help you create models and i'll right click here and say new mining structure which pops up our data mining with it the first thing you asked is whether want to data mining against relational source or a pube and we'll just leave relational sort selected and now I don't actually get list out the techniques so here list out all the algorithms that we have in our server now actually what it does actually goes to the server and discovers a list of albums that are on that server so if you wrote your own plugging algorithm to your students or a plugin Elron already bought a plugin out going for a third party it would just show up on this list you don't do anything it just shows up on this list on the client side and everything else works the same so I'm going to leave it with decision trees for now click Next ask what data store if you I want and now it says how I want to use the tables the customers my case table and let me just take a couple take the hobbies on that they have and their favorite channels as as nested tables now click Next and now it tells me how do I want to use these columns like to make this window bigger this is actually a wonderful new feature within the head before which is just making dialogues bigger it's an amazing feature so I'll just going to click here and just make everything input here and what I'll do is I'll make the the channels and hobbies inputs as well and what let me go is say look let's try to predict whether or not somebody's going to own their house let's predict how often they go to the movies at the theaters and we'll also predicted favorite hobbies phone thing is kind of interesting is one thing we've done to make data mining easier just kind of wipe out a lot of the predefined kind of the pre ideas about data money so normally in the alignment bottom I couldn't go and say our free printable targets right but here i can say creepers will particles of handle another thing we do is like our clustering of them this is the same dialogue that you'll get it either clustering diagram algorithm our association rules algorithm it's not specific so we can actually do predictions with clustering algorithm if anybody's use clustering before they know it's not very great for prediction well you can do it it's not something we prevent the user and then down the road we have accuracy charts that tells you that the question I can probably do a very good job unless you're predicting gets your first data so I'll just go ahead and click next here on this data on here it lets me select the content types let it know whether things are categorical or continuous and the data types and I can go in here hit detect and it'll run a little heuristic which says you know it only had two values and it only get discrete even though the date along a numeric data types I'll click next here and give it a name so my mind is structurally the same name my mind Marvel inside the structure I'll give it another day customers dt4 decision tree now I would I have here the reduced view of my data source to you just what I've seen before in the column joined model I'm not gonna go to live here but this is the mining structure which is a new term we also have this idea of a mining model a mining model is inside the structure and it says take those columns and those data bindings that it shows before and associate them with some data associated them with an algorithm right so I can go in here and say here's my algorithm decision trees I can do funny stuff like oh instead all our our nice set of advanced parameters for people who like to tweak of that kind of stuff and change how the columns are used I can go in here do something but let's create another mining model and i'll call it customers CL and i'll use the clustering algorithm and click ok oops i use association rules no I don't want to kid you you try it again customers CL and use Microsoft clustering there we go and then I'm also going to create another one and i'll call it customers naive Bayes naive Bayes advertise in its metadata that it doesn't support continuous variable so it tells me that these variables aren't going to be supported so it will actually continue yes and you see that those columns that are continuous that markets ignore so what I have here is one mining structure i define kind of the domain of a problem and then i can have many different instances of models in that in that solution now what happens actually is it reads the data once and put that into a nice compressed cash and then it feeds that to all those algorithms in parallel so it kind of gives some advantage of trail of processing just at the top end and then if you have your own algorithms you can put those in here too just like anything else they're not there's nothing special about a Microsoft algorithm versus a third-party I'll road so another thing I'm going to do here exactly for the clustering I'm going to go ahead and ignore these nested tables just for our sanity sake and then what I'm going to do is deployed it through the server so really what I've done so far that created the file to my hard drive which just have the XML metadata of those models and of those objects and I'm going to deploy it to my server it creates the objects on the server that reads the data and then go ahead and process all the models and this is the point in the presentation or we just a have tell birdie watch a spinny yeah hypnotic little device that we have here to say bye sequel server by six we're actually come up with a whole line of spinny t-shirts and a stuffed animal so yeah it created it process all the models and now let me go ahead and browse some of these models so the first thing I do is browse my decision tree model and that's I know and here's our decision tree viewer as that we said you can take these viewers you can put them in your own application since you're creating your own algorithms to you can actually write your own you wrote your own viewers and plug them in here I can actually write my own tree viewer and have it we place this tree viewer for the user who want to or I can take the tree viewer and view my models with it with using my algorithm or write my own completely new viewers as well so we have a lot of nice little features here you can you know go back and forth you can shade things by where where this hobby is this is acting in theater the tree for acting the theater and you can see that they're very that the most concentration is where the hobbies Museum and arts exist and one thing also you see here that they look at it we have museums and art with and hobby movies intelligent with the hobby though we have marital status also play then so you can see the interplay between the demographic information and attends actual information right here in the model now there are multiple targets like I showed you I click three things as brick table well it's not just three things actually every hobby becomes a predictable target when I look at here I still have a tree for every possible hobby so I can go down here for business and investing and I see you have a tree for business and investing I can go to tree for comics animation and so on and so forth I can see all these different trees well another thing we already have the dependency network which kind of let you see the forest for the trees and just take seconds them up I always have to use that line because it's tough decision trees and you can zoom in here and you kind of see the relationships you can drag this down use the strong relationships like what's related to museums and art bring that up you see books and literature photography acting a theater or the strongest things followed by news and media and so on and so forth so you get some good ideas of how that would how that works let me go on browse our clustering model our clustering viewer question while it's really nice because it kind of you look at it you're like okay well with that that's neat but so what so what it really does is it take alert clusters which are kind of abstract and throws them on the surface and said let's try to get the clusters that are similar close to each other in the ones that are different far apart so cluster sticks is probably not very similar to cluster 5 but you know cluster 1 and 5 might be closed and also you see the cluster 1 and 2 have more people in them I can also go in here and say for example where are the people who own their houses well these are the people that own their house is these are the people that rent their houses where are the people with who let's look at theater frequency people will go to movies daily well that's nobody how about people who are the movies monthly almost every cluster as a good country good concentration weekly only these guys over here going weekly so you can kind of get a view of your clusters there and there's lots of other views as well I'm not going to go into them all just for crime snakes but do with a nice view here which says show me the difference in cluster one everybody is not in cluster 1 and this is applicable to everything actually there's a paper written on sequel server 2000 using our clustering algorithm where they did something scientific study based on the impact of underwater wood on greenhouse gases which I don't really actually remember the results of the study but they said they did this study previously using manual methods and it took several weeks if not months and they didn't using our clustering algorithm and they did the same research that the same results in about three days so it's very very nice now I'm going to skip the accuracy chart so we can move forward and this goes quickly over to our money model prediction pain now prediction is where we think that you know there's a lot of benefit you can do predictions you can embed predictions and applications there's lots of different things actually one thing that kind of always kind of mentioned is I think there's two sides of data mining there's descriptive analytics that's showing you what I just showed you now is the data mining did it just showed you a bunch of stuff that maybe didn't know before and then there's predictive analytics which is applying that against new data which I'm showing you right now so I'm going to switch this over to singleton mode which allows me to actually enter in value of what I want to do is I want to say what are the possibly the top two hobbies that people are going to want accidentally delete that week that and they say give me a prediction function where I'm going to predict Bobby's and i want to say what are the top two most likely hobby for a person and i'll just go in here and say the person's 35 they have a you know a bachelor's degree their mail their other hobbies are they camping and computers and they own the house what this does is actually generate the query like that we showed you which says select the top two hobbies from my model prediction join there's my input right so it's very simple very easy very easy to create this kind of stuff there's a lot of things you can send here i could send at this point i can send a singleton query like this i can send a query to relational database i can send a parameter from from the client so i can have my data gathered on the client and send that as a road set parameter to the server I can have another data mining query in there where I predict from another model and send the output to that model into a different model so there's a lot of flexibility you have there as well so I look at the result I know the results going to be and we'll find out that we have this nested result actually let me just go back here and just for fun so you can see kind of get an idea of what it looks like let's actually take the egg and put that here as well ok it's not let me do that let's go here and off a comma T dot age and if I look at the result now you see I have my row my face row and a nested table the results and it says that the this person is likely to watch like science technology and travel and if I go back to my design I can go back here and change my hobbies pages Nia add acting in theater fitness food and cooking and get rid of camping and hiking and change will stay make it a woman with a master's degree who's 40 and then his results and now it says books literature inside technology so you can get different builds can do a lot of what if kind of analysis here you can take these queries and embed them into your results so the idea here is give you an idea of the flexibility to platform there's a lot of other things you can do with the platform other than this when you right here on algorithms you don't use that you don't worry about that language our infrastructure basically takes all the data from whatever data source trace it stream of active value pairs and send those to your algorithm so all your element has to deal with the stream of active value pairs and it's it's it's up to you to do with that the only thing difference between I would say our implementation of an algorithm in our system and then implementing an algorithm on your own is that generally people don't have to worry about cases of different lengths so if you're running a data mining algorithm usually you think of the old model which is kind of a square model so you know that your cases are all going to be have ten attributes if it's the first page that set attributes well in our case you could have thousands of attributes and actually we've run models where we have 750,000 attributes they run just fine so um that's the demo of sequel server 2005 data mining and I wanted to transfer it over to our way long who's going to talk about her datamining work with protea now thank you yes currently now but you could actually write an algorithm which calls the other algorithm that does the vacuum boosting for you actually since everything is an XML for analysis everything is automatically a web service you just have to long open up the port de cv Thor to you're on you're on you're on your client and it is web server if you want to write this down there's a website called sequel server data mining calm and on that website we actually have an XML for analysis example where it allows you to it reads all the metadata and comes up with a query builder and you can click what you want a query and it will send an XML request then get the XML response any more questions yes it is reasonably easy it's actually it's much easier than we had in sequel server 2000 actually the the API level where you write your algorithms is that the exact same place at the eggs API level where the Microsoft algorithms are written so if they have no more or less functionality than the Microsoft algorithms and it's a much lower level API we have it right now the API is a common api's p plus plus it's not your comp stuff is just all in process but we also are working on a sample for a managed code of C sharp or vb.net kind of implementation as well that'll be released as a sample on how you can write code that way tickle server data mining that come one question ok so the text binding we have the question is on display more text mining the text mining that we have our two transformations which are in integration services and the transformation for term extraction which finds the noun and noun phrases in your in your English document it documents and then term look up which takes new documents and says you know this term belongs in this document it does word semi and does a couple other things that they actually find key phrases so it doesn't necessarily find this one term we can find many so find data mining is one term instead of two separate terms and then you can actually pump those into your your OLAP cubes or reports or on data mining models are you ready ok we'll take will take question Decker's ok so from for the rest of the next hour half an hour also I'm going to share with you the projects that have found is partly funded by the defense of the consumer award here and the project is called can you hear me the project is a cop motif space basically of course I will give you a offer you about her what this project is about and how computer science so actually research can help greatly in a intense magnification myself came from the background of a database and data mining and during the past three years I start to look into bio informatics notifications and the primary access agreement at start without her driving application of my research in computer science and math and the things that start to do that has become more more interesting and this is just the one project I have currently working on at unc-chapel Hill during the less ranked second I talked briefly about her a mining protein and mining hydrant from protein sir as I mentioned before that proteins and machinery of life under the mat the big challenge that foul it is try to addresses to determine how protein powders and and we all know that proteins function is determined by its structure and therefore the scientists the happy working on cataloguing new protein structures at a very excited explanatory and all the currently other structures are deposited me to a protein data bank this is a publicly available accurate escutia phase resources that anybody can who's interested in kind of came to that you can see that clear the rate has been growing exponentially so if you look at a protein structure as it is I'm going to share a little bit more detail with you actually it is a large molecule basis it's a long linear chain of amino acid and the according to some stable but dynamic three-dimensional shape so that can function and in a relatively stable sake we we also know that for many of the proteins actually only a small structure in the protein directly responsible for its function and we often refer the master visual motifs here is the US just a free sample of protein after they're coming I Brister pics free samples from three different protein structures and the highlighted area are the functional side I see some have called active side of the protein you can see that there are just performed by a very small number of aromatic and it served as their the functional side of the protein and from now on elektra further micra specialties and our our emphasis in this project is to develop computational method to fast and efficiently discover all those specialties so that the ballast can use them to form value to have positive for your test so the overall structure of motif space after consists of several conversational component and we start with the protein structures available in protein data bank and we all know that scientists have been doing a great job in cat in grouping protein structures according to their similarities in their instruction and the and the functions here is the on the right hand side asa it's just the high-level protein classification after in a coke its patients in this particular one is based on a structural similarity so came in any protein family what we can do is that we have a motif manner basically it take the protein structure out of the protein data bank and look for recurrent spatial motif so they think they are the motifs that occur murder at the very high frequency within his family and the next step is to apply a filter on those special move here just to put the lights out those ones that occur rarely in the rest of the protein work the notion here is that we want to fund something that highly associated with this particular protein family that we want to study because we want to find out what are the characteristics of this job working family that are responsible for for example for the common function shared by the school problem proteins we want to be able to understand how they function sand which park are planning to they're actually responsible for their function so we call them families difficulties and with the positive the motive green in the database so and then this motif can be used to construct a predictive model and to predict for example protein function so a new structure comes in and they'll go through this the country bar and then we will be able to tell whether or not sister how likely this this new structure has this particular function and we'll also look up at the literature in biomedical domain and look for a free to extract external knowledge that are has been developed by scientists and put them in a knowledge base and the order to make for yourself is the the the information or knowledge we discovered we will have keep navigator basically this navigator will talk to each individual component and enable a user to interact wisdom and if you look at the whole picture you can see that your several important components that can view to the whole project and if you look beneath each of the component you can see that I tricked this are the the area of research that involved each component okay you can see that in the motif manager I'm going to focus on today so it's nonstop worth mining and then for the next one is the motive filter actually associated with the feature selection Association discover it and the to build the classifier we used across education vampyre the next one Thea information retrieval and tecmo in the blocks it is a largely I mean that navigate her comeback a benefit from a normal realization technique after these are the errors that we're currently working out and to develop her normal computer science research so I'm just going to share with you this motif minor and if time allowed i'll show you a brief demonstration to see how that to show the idea so let's go back to the protein structure available in protein data bank in order to analyze them will have to be able to represent them in clean and precise way so in protein data bank if you look at the data file actually they are represented by point in a three-dimensional space was available there is for each I come in a protein you have 3d coordinate you can sync data what's what's the input to the whole system is via a set of point with your sweetie three dimensional coordinates but what you need to keep in mind is that they have four errors and know if they're it just like you have a protein structure and then you take a picture of that and then you for each point you have it location within some some error box so starting from that point and then what in the motive space what were the first step what we are doing to represent azria the protein structure so that we can keep the essential information for for the money off for visual motifs what we choose to do is to rather than by grass we're no corresponding to a mere hundred residues and edges corresponding to potential impactors and were investigating the trade-offs between the complexity of the boyfriend of the information we need to captures and you can imagine there's many different ways to construct the graph to represent the protein structure and hours to ourself we have tried several different approaches and next I'm going to share with you one of them so no but no matter what where you decide to represent your protein structure by class after you have fended actually a special motifs that I show you before actually one of them is also highlight data on the top left the figure as can be can correspond to a sub graph in the in the graph representation so basically you can see once we savage this correspondence the exercise of minus beta motifs becomes our funding frequent software in a set of graphs and the challenge here is the graph isomorphism which has been proven to be np-complete so next I'm going to share with you how we are going to ask you what what we have been doing to make a beauty of the variance of the protein glasses that allow us to develop a thermal efficient algorithm before I go into that i'm going to show you just the one way represent we have used to represent protein structures by glass it's called almost beyond accurate developed by my colleague texturing and his student at unc-chapel hill and their paper actually conversational geometry research and their favorite pub you lost your shoulder what we're doing here is that you can see one way to represent the before i show this done I want to I'm such that this is not the unique way to represent it this is just a reasonable way to do this I can to be I can give ourselves quite good result so what we're looking at here is that if we select a four point in in a three-dimensional faith and then we look at whether or not we can form an empty sphere that that those four points happen to pass through the surface and with of course no other point included in the Indus empty sphere if thats the case after we can connect each pair of this tattoo pointer with eggs so the notion here is that we want to define some kind of a spatial neighborhood with no interference of on another point though that's why we want this empty sphere and as I mentioned before well that actually that's a standard definition scale on interpolation and as I mentioned before there's noise Xena in the in the location of each point so therefore we want to allow some kind of error and what we're doing here we could all of them in basically we allow of a primary that we have additional parameters an optimal basic just defines the maximum area that were allowed per point in terms of their location so basically we allow is going to perturb cause any direction by an amount of patmos ethanol for doing that flexibility if we can still constructed mp3 then we think that it should be an actor so by this definition you can see that the traditional representation of design in translation suggest a special case with epsilon equal to 0 so this is just an animation to show you that that after you move allow yourself move the point later on and there's the possibility of for constructing new edges so this red way just a new acts that are allowed if you allow some perturbation so if you married is the restaurant started from zero to positive infinitum so basically you can allow them to perturb in any way one and then you look at the edges with distance less than a certain stress food then you have fun this country tsinghua basically want to alarm Ohio 30 you have fun you're going to have more and more edges sereno your grass and the our external 1.0 Axia I saw a default value so i can represent the protein structures at last now the next question is the how do we fund special piece which is how do we find sub graphs that are corresponding to the special kids and we formed a cesta as the block mining problem and in this case we want to find frequent sub graphs because we want to find special motifs that occur the almost ever cooking within the family but also up for the next a visa is had to occur rarely the rest of in the rest of the protein world but the first one who found out those one that occurred frequently within the family so this is a cure exactly the frequent sup of mining problem and for those of you who are not familiar with the problem basically just one card video oil was the problem is about if you start with the protein as for a black database of three graphs and there you set your threshold as the two out of the surface we are for all the sub graphs that occurring at least the cool out of this request so on the bottom of this these are all the sub graphs that occur at it is twice in your database so this is a definite working hope you'll think about the combination 300 it's very complicated and it's very time-consuming so a lot of our effort has been investigating how to develop a fast and efficient algorithm to allow this this kind of conversation when of our approach is the face of canonical we define some canonical form to represent via the graph in the subway and so that we can have some nice property on that is total out some combination of Judah so this is at a high level we we use the canonical adjacency matrix and the wiki find her so we firstly if we look at it and are adjacent to matrix is basically if you put on layout to the node in the graph along the end no and then if you entry this corresponding to the country to add to the edges that to come back to note and they will put 0 there is just no ads here so is the simple representation its adjacency matrix and you can see that for a given graph the adjacency matrix is not unique you can have many different adjacency matrix one corresponding to each combination of flow of your node along the diagonal of in the matrix and what we define the canonical one should be the one that maximized some some function and in this case is a code we define so we can coordinate nowise the additional metrics if you want you to track the one that maximize this coding and the yarder took put this other week which defines the in this particular example we will see well it's alphabetic order lexicographical order so that but but in the real water you would have preferred ways to define order between no acid so if that case the actually the leftmost one door that corresponding to the maximum Cody and we call it canonical adjacency matrix and look at this one okay so after that basically all the Sun for the preference of where we're interesting would naturally be represented as in the tree structure basically each of the two entry in the tree corresponding to a stop work and can be represented by canonical agencies world and from I'm due to the time comes from nothing to be able to share with you the detail the technical details of how this can speed up but I'm just to give you a high level of summarization so for the automation feel that we use the via the graph representation for Kurt instructor and where we used to add is truly encode potential impact on the plane rises so which means that the ground come from has some spatial locality and basically you will not connect amino acids that have further apart you all increasing those ones that are you some special neighborhood and this will translate into a of a nice property of a boundary degrees in in the graph also we use the a priori property that that can be applied to any frequent parapet remaining algorithm to help us to to guide our basically mining of broken sub work and the basics at the end super graph of an infrequent sub graph is infrequent so basically this CUDA a suggest that we need to check on small staff work for people go to bigger chakra and this would help us to eliminate unnecessary of a morphism cat and as I showed you the canonical form we were using which allow us to reduce the awarded redundant examination and we use that first explore original all comfortable with frequent stuff works so that we can use the original call sama incremental at maccas on track basically we want to reuse the result on smaller sub graph i have had two before so we don't we don't protect it and also this alpha last fourth of four petal memory of salvation so we stop our web developer several different version of our rock mining stock of many algorithms extra specially tuned for mining special motifs of health law protein structure basically we will utilize some special prophecy you can see that these are not general graph if its general Axton is already improving their surgery the cracks of graph isomorphism nice though it's an elite problem but especially due to the fourth property that we have we have this would allow degenerate interesting algorithm set to mount for ourself to be in such a dilemma so now what we are doing is we want to incorporate more interesting features to our algorithm one of the mixer we want to allow substitution of amino acids types in the sub graph we are considering and we want to allah i support more dynamics and the geometric constraint so here before i show you the demo i want to give you a few examples that we tried along our path and sort of for the past 23 years after the first family protein family we started is sermon positive and this in this table I think this is the 38 motifs that we discovered that her is specifically to this the protein family actually I felt them and now in one of the protein structures that you can see this the colorful ball ball they are VM I'm in an acid residues part of the stage normal kids and we have 38 help them you can see data actually many of the spectrum of peeps are heavily overlap with each other and if you look harder to send her part and productive the active site of this girl of this family of proteins this is the the pattern experiment easily discovered based on the scope acadia the set of protein we are using is classified by skolko version 1.6 65 released the patent offices on under three and the fifth then there's new structures become available and we use those one at the at the past and that this idea at seven proteins FATCA whose structure available after that and this are actually our protein classifier identified them as the label demise services and then we go ahead to check each one of them and the final actually on all of them are actually serve produces even though they were not originally labeled in the scope by the end of 2003 and our address some of them are currently labeled at seven cottages officially in esco there's a gesture a new release of horoscope I think it's 1.67 and a caddis success we go ahead and say well webinar his because we just had a lot in that one particular family are or this sir general Harran can be applied to all the coding family and they've that the case then undo actually there will be a lot of potential in health authorities so to design their well attack so we go ahead to pass another family which is CR 15 proteases and these are the family specific proteins of the family specific motifs which we put out and actually for if I don't know what amounts to a visible from from your side is the yellow highlighted the residue after dr d activa center of this family and i want to mention that actually for this exercise and we didn't incorporate any of this domain knowledge which is look at the structure of the protein and a desk are all the information we use for the mine exercise and then we look at okay what added vehicle motifs and whether or not cuz the result makes them in ambala t so on the left hand side actually these are dia i highlighted these are the active are active site the motifs in wolf investor cover those active site and you can see that and in this up anyone's function actually this exactly corresponding to it's just a different committee slightly different akin to the one put a figure in the in the right hand side after wiki rockets will have a favor from from this paper they recorded so some this idea the important structure that snoring active sites and so if we zoom back so that we can see all the stuff special more kids we put out all together and see what are they and what we just look at this park without active site and what are they spark then the definitional pressure and then we found out accurate these are the Hubble for hydrophobic finding pockets so which also carries a biological function so we're pretty happy with this if we have kinda member to show you a little bit just on this particular family so after that well we'll go ahead a tool to try something more interesting and actually more adventures which is the function of function difference of often structure so you know that yourself exists the many protein structures that people don't know just have no idea about you but what are your function is and we want to use of course we want to use our computational means to try to infer with how the possible functions they carry and and this and this is just one example we have tried on the left hand side is the family car hmph and we found 49 and special motifs that have specific to this family and this art piece highlighted in the in the mid of that and on the right hand side is a often structure that actually was a task aside target that people tried to project to the fold and a structure of the of the 13 now we know the structure but feel we don't know the the function and that because there's no good sequence and structure lineman to any of the known protein so still remains a puzzle and what we find out actually this particular protein contains 30th of the foliage I'm a fisherman tipsy we fund for that family and there's just a recent literature reporting data actually someone symbologies the fun in developed athleta this particular protein looks like carries functions similar to dr. family and they recording them in one of their present paper and this is the second example we have athlete it is just a different term protein family and we have 62 facial motifs pants and after this town is very small wine I only have four members at the end up from 2003 and this is again another orphan structure with a new function and still it's not classified yet and with the any function I fell far I think a few months ago and we find out how to this particular protein contains 46 out of the six to visual motifs we find it associate with that family and and the more interesting is like we find out there's actually several proteins new proteins that people just discovered after have worried I have some structure similarity with this particular from the enemy all already the official labeled ERISA with with that family so so there's some evidence to suggest that her the predictor model we're building can be served as a very good verbal model to generate hypotheses for validus to to be to design develop test okay so if we interest in the data mining and various magic the techniques that we are developing this other little papers that we published recently and of course the project the most part of a successful so that his participation flow of the collaborator from different field to make this happen so now I'm going to show you a little demo you ok so I'm going to load 7 so this are seven protein pharma 15 of 15 proteases and save the time i'm going to set my fresh food increase hi so I'm looking for special mopey that occurred in at least six of the seven and protein and the we we want the special motif have at least a Content six amino acid residues and I'm going to study the the job so what is happening here is that this education will go back to a server at UNC which is which directly caused the approaching data bank to retrieve all the structures and does everything is computer on the flag will replicate the effort will have to wait a little bit and kill dia the results returned so it does is that once it retrieved the structure busy target for each protein is a list of all coordinates associated with each item and and then we can track the grass and then we use the graph mining algorithm to mine freakin subclass and then we map them back to the to the core team work and they're not reading them and the middle block rhinos empty here those eventually hopefully show a list of motifs that actually is showing up now and that's shared by this at least six out of the seven a structure so so you wish to like one of them after we can see what haria other motifs that are contained by this stuff this particular family and then if we want to see where are they and how they look like just take a little bit one ok so this start actually this highlighted area they start the pasta special move here and there and actually discontent to all of this this group are the active center after of this earth and this funk this family of proteins and if you can look at it these are the lives of motifs and you can even highlight them out if they go down and look at the major one of her another so now i also want to show you that so if why i choose two of them i can say that what are the things share to buy this and if i want to just see whether or not i mean i want to show two protein structure together aligned according to the special motif i find out actually the Graduate feel the one that had just showed you before and the magenta one is the second protein so actually you can see that this this fear it may not be very visible here but ha so in that area dr. young that's the motive and the dispersion multi-effect her that I'm in 15 and use this to align it to to align this to go protein structure actually this particular motif that cover fear that you've signed healthier of the protein we can see that I can give particular example that see the second important has been aligned pretty well with part of via the first protocol the first line so is the big one and the second one is just smaller and then they happened to be aligned pretty well if we align both based on your ex the special motifs pathology exercise and in January for if you play with this and with many different without affording families and you will see that in general to proteins may not align very well and I mean globally so part of the milan pretty well and the rapid part of me maybe just the branch out and I know because the if they aren't very well it always along well then you I mean you can easily find them by global structure alignment so the beauty of this project is that it does not require that at last part of the protein structure show some similarities we just put that out you can see that particularly it's much much more difficult one to support this kind of local alignment and this is what we can support in this case actually and this is just showing one point example of one of the spatial more peeps that were able to find out and you can see that this part little heartedly the exit center so any questions working with biologists what about the culture that very good biology has harnessing the power of computing confirmed their clients you've been able to get your biology yeah I think that's a very good question so when we started project I talked to my collaborator from the other side were frequently and I'll see 94th on with higher the reactions kind of magnitude based a crash show them what i do and listen no we're not interesting this were interesting something else which is always different from what I show them even though I thought at all you said your interest in this lifetime I'll communion wine Lena know they always tell that was not what I mean laughs I'm so so basically there's a learning curve there that allow us to communicate so at the beginning of course that when I show it will show you a couple of examples like how it happened to sort of like the special motifs contains the active so basically all those proof of contact to give them the the confidence that you know this is a good system for you because Fordham who designed and implemented erick sermon is very costly and it's hideous and take long time and they don't want to go ahead take a random hypothesis so they won't have some handle for confidence before the go-ahead to do that but i'm happy to say at this stage are we we have developed some kind of found in the classical director that's why we that's why before the later part of the 1i showed to us out it's on the function or inference basically nobody know what to do the function of of all those offer instructors are and then on when we we were able to show them to go to the ballot is that they're very interesting that so they because intended because because of the all these other work that we have we have done in the past but just a tool so that rediscover what already what they already know so then now they feel more and more confidence data that they want to I mean use this one through as a way to help them to generate your hypothesis so I so I want to make sure that I understand your question so you we only met emotive this is a motive that are formed by multiple after we we have we have look at that and even I can't let me see what I did have something to see are you have a backup track for this so this RV yet if you look at a green or red there's our two motifs we discovered and if you look at your on this religious structure actually the Spanish to each other and the only reason that if they are discovered as to motif is that there are some flexibilities there dramatically so they occur if you look at different protein so that it won't will they won't be able to be covered by a single motive but they have fun with after after we find out of motifs we also look into a social agents between no amount of after what we do is that we can treat each multiplexer that's a feature right and simply way is the binary feature and then each basically each protein can be described area but by vector field wall emotive a thong and visa blender we can use association or feature selection of Augustana College we have tried to see you rather know you're strong association among different motifs and and we find some of them and that they have their now either we call them might have motive but yeah yes yes how does that how does that correlate with known active site have you tried to do a correlation between your automatic you're sort of computational method of finding small motifs because the protein data bank is already labeled with active site right and so there are many many active sites that are known and their functions are known and then and then I all just look for those active sites society what is the relationship have you tried to combine the two spatial motifs with biological active sites to improve your your discovery process this is a good good point and what we have been doing is that when we we did this intentionally just to prove to Ballard is that this is a quite effective way to combine the station tips and provide them with with some a nice hypothesis to test and so when we do the manually we didn't incorporate that knowledge we just read all the residue of the face and look at their genetic her features and and see whether or not we can find special motif exactly and then after that we validate each of our bigger more fish than that and again some domain knowledge and some of them are I mean for yourself think available at pvp them that either we look at them and some of my students will spend a lot of time to just look into the on the papers available from PubMed to see whether NASA there's some explanation is somebody's paper there happen to record okay here's the personified or hear some some interesting things going on and then that some and that what we have been doing our of course our next step would be like who we start to think about this the waist like to incorporate domain knowledge into the mining exercise so that's how something on our agenda but so we haven't forget it to finish via text actually we're a visit or a danger to what we're doing is the the the little demo i showed you if there is a way that we are learn to protein structures are and we are in the summer we have been starting to work like to build a nice girl and a a systematic way to align multiple structures and and with the which we go i'll provide a user with a nice visualization feeling so that they can they can for all the same motif thank you "
0e7hWIPSzf8,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-03-28T22:36:42Z,Data Mining  (Spring 2016) Lecture 17,https://i.ytimg.com/vi/0e7hWIPSzf8/hqdefault.jpg,UofU Data Science,PT1H11M14S,false,90,0,0,0,0,okay so okay so let's see the first thing is need to post the regression homework soon it's scheduled I mean I'm I'm happy to be flexible about this let's look at the schedule here okay so see we are here and regression work is scheduled to be due Monday if all 11 so it's again gonna be worth 100 points kind of like is it so let's get it's not going to be too as as challenging this can be all in matlab but I basically tell you what to do oh that specifically so I'm happy to have this dude later but the final report is due here and I can't move this date so much because I wanted the posters that I want to make sure you do this before you have to the post rounds so I can't really move this date so i can have to do later too once we can suggest the date you know there's there's another assignment due at the very last basic last possible day and again this will be half the size of this one okay so so if i don't hear any suggestions i'll keep it monday april eleventh but if you have a better suggestion i can move it okay well keep it there and after you posted if there's a big discussion to change your change my mind and this edge it'll be on in a in a few days hopefully then then we can maybe revisit this okay yeah so today is kind of our last lecture today on regression we've been talking mainly about 15 el paso which is is it's basically a l1 regularized regression and so rash right hands and so what l1 where God end of lecture but chris is technique called lasso like here in a rodeo meso although one date Avengers of this refers to as last soon I think it's announced Asia okay so however the I see also cross validation and so it's going to be important in this context so you now there's a high so for the first so this is a column here and so that this column is the first for the ice road the column is the first data point that I bro this where the second day to find so forth this first column is getting all ones so a0 is going to be the all one vector and so before we if you look we looked at this and one day you had this form where we're going to try and predict a X plus B was it was going to supposed to be equal to Y write this D is essentially we have two coefficients of this X is actually going to be the data here but this is wonderful efficient kisses the other one but this one was always x 1 do anything so now this column which is all one has taken care of it okay so oh ok so these are I screwed up a patient from the notes these are going to be the coefficients we're going to try and solve so this expression is going to be very very confusing these are the coefficients and we're going to try and find some function of s k of p is so why I swear now this is last term here this is going to be white I okay so we're going to try and predict this last value using some function that depends on this matrix a of all the first few values and this sum is going to go for a p.i.m.p and so these are just the first D plus 1 values um ok so so will be useful is to define this matrix and this will be the whole one vector and then it's going to have P tax and so this part of here we'll call this P X here so this part of this input data for the first B coordinates is going to go here and then us all one factors and so now we're going to have where this expression is going to try and find the minimum a so we're going to try to find a vector of coefficients a1 a0 through ed where multiply those by X the so linear expression here is going to try and look like y which is going to be the last values if you need a rest ok this is the norm is minimizing the sum of the square root this is I took a square root inside mr. on here but if you minimize it you minimize it the square root is is not changing what the foot the manholes ok so now too I i mentioned this before but there's a solution to this that this meet this vector of a you can write this as a X transpose X inverse X transpose Y okay so i can take this data matrix X and convert it by doing X transpose X inverse times X transpose and multiply this high values Y and this will give us the value a which minimizes this yes I said while is just philosophy traces this only minimizing the one direction yes that's correct yes I'm only trying to minimize the last dimension okay so there are advantages and disadvantages right the advantage the disadvantages I'm only care essentially about this last coordinate the the advantage in a lot of you know I'll get the feedback I'm finishing the feedback on the project's the new media reports but the a lot of you in the projects are facing this challenge or you have data it has lots of different units to it it's high dimensional but each unit is different okay so what's what's happening is we only care about the last term here you only care about those mutants the other units here kind of this part essentially makes gets rid of the units it's kind of normalizing by this term and so you can use this essentially as like the you can use this line is like the center of a cluster or you can use the distance of a data points that distance that line in this coordinate all right so you can um you kind of deal with these things where you have different units and in various ways by using this linear regression it also doesn't care how you normalize the unit the in each of other coordinates because it's is not caring about those newsmaker prediction well I just think this just like throw away all inventions yes so this is similar to what we talked about last week Wednesday we have lots of these you're going to have lots of these different dimensions here and lots of coefficients that tell you how important each of those is right and so this is telling you this a is selling you a model here and if you're using all these dimensions you're building some very complex model you'd rather have a simpler but there's lots of the generalization properties that we want to really get into in this class hopefully the machinery classical we're talking about these more but you'd rather have a simpler model to describe what's going on here and in this case that's going to correspond with the coefficients being 0 then need to be don't care about that dimension if i set one of these coefficients to be 0 that means i don't use the information about the second dimension and so when doing this compressive compressed sensing your kind of choosing these uh in some way trying to choose which of those to be nonzero and we're going to do something somewhat similar with with last we'll see so hopefully this will start with sort of making sense yeah so um this means only am important unit in D plus one for which which equals this why perfect okay and so you're going to get a function so this this red line here when I'm in higher dimensions this is where d is equal to 1 so in the case where d is equal to 2 the red line is going to be a red plane d is equal 3 it's via three dimensional subspace and so forth so I'm going to take all the data so this this dimension is going to be X and from every data point I'm essentially predicting a value for the Y coordinates so this is D this is D dimensional data here why is always one dimension there's a way they do this with multiple why dimensions of multiple regression we won't really have time to give this us okay so this is the set of this is ordinary least squares regression hopefully almost all of you have seen some form of this before um even before the preview lecture I gave a couple weeks ago what we're going to be talking today about is regularization okay and so the idea of the idea of regularization it comes up in we'll see one kind of on one form how to think about this you can also there's a couple of other ways to think about before I can get into it if you know like these Beijing techniques it's essentially some Asian prior which is trying to make your model simple so if you're using a purely Beijing approach you're going to have any prior on your model that it should be simple in this case that the coefficients are going to 0 is the prime and how much weight do you give them is going to depend on how what you use for your regulation proud that there's another way of thinking about based on this idea called regression to the mean so this is sort of regression to the mean before okay so moms so let me try to give an analogy which is macaroni totally fair here but let's say that there's have explained intuitively how this can work and then we'll see how it works with noise and in the case of linear regression so let's say that there's a test yes that's going to be true false and so every student in the class so let's say there's a larger students every school in the class um they know 50 questions and they guess on 50 questions okay so let's say it's it's a pretty hard multiple choice test but everyone like for half of the questions it's it's really easy right then the answer is to be obvious about the other half you have no clue what's going on so you basically make a 50-50 gates okay so if this is going to happen the expected score let's say the expected score is going to be seventy-five percent right fifty percent you get right fifty percent you get half of the ground okay so let's say that I take the top moms let's say the top ten percent or so of students and for less it's listed the top 10 students top 10 students have let's say that their average score it was eighty percent ok so the top students did better right there were some distribution of students where this was seventy-five percent and so the top ten students all scored the average score let's say was he students here the average score was 80 that have hit those top ten students okay sit Thomson elk let's say this first test was was the midterm and so now i'm i'm going to give the final and the finals just as challenging fifty percent of the questions you know fifty percent you guess how do you think i would take how how well do you expect the top students to do this students who averaged eight percent on the first test for nothing average the expected value of top students what's the expected score that the top student what the same is that eight years at 75 what's which 80 does it's going to be expected as in these of the top is going to be still 75 because these dudes they got lucky in the first test there's some distribution of the students that's the expected the fact that they did well the first one does not change how provision in a second one like if you are flipping a coin instead of if these students literally flipped a coin on those questions if they took them the vinyl on the same way and they flip the coin again they're expected values can be seventy-five percent right that's you're going to sew these students are going to regress back towards the mean even though they did well be beginning their expected score is going to be closer to the mean of the whole class afterwards now in real scenarios this isn't quite true there's students have different skill level but the ones who do well early some of that was going to be luck in some of it it's going to be skilled but if I were to look at this the top ten students on nom nom on the midterm I'd expect that their average score on the final would lower on the bright side the students did the 10 the 10 students who did the worst on the midterm I'd expect their score on the final to be higher they're both going to regress towards the mean which is the seventy-five percent right so because this was unbiased noise coming in here and if I sue unbiased noise from the true expected value just because they did well early doesn't mean they're gonna do well again the next time and so this is kind of what's called regression to the mean this comes up blood sports if someone does really well on the first half of the season they're probably going to do back towards where they usually do their second half of the season so this sort of thing happens a lot and there and the same phenomena can occur in in is going to curb in a linear regression as well and the rationale is the idea is I've got this data okay you know and let's say there's some data put up here and so I fit some model onto the data that tried to minimize all of these costs but I had this one outlier up here and if you remember this picture there was all this these x coordinates down here and I'm trying to predict these these Y coordinates and this red curve is predicting these white board is here and I get pulled off by this outlier I don't expect so I'm trying to predict if I got a new escort it where out where's the valley going to be if I saw an outlier in my original date oh I don't necessary expect to see that we're late and so it's hard you know often you see a picture like this and people drawn outliers that way or this far up the sound really problem it needs to be like really really far up like I'm just reading often people draw a picture that i layer down here and that will actually shift the thing down you actually usually draw it down here because it fits inside of a nice rectangle where it's harder to draw a really high up when really the hell tires you're worried about brother ones really far up there or actually really far down here and you want to bias towards having a slope of zero that means that the coefficients have no effect on the data outliers look like they have a very big effect you in a bias towards having little effect and so that's what regression the the regularization is going to be you want to do in when you're doing the linear regression ok so again let's we saw preview of this but let's see what this looks like again the first type is called see if I could spell this right the good thing about spelling names that are Russian is that if you spell it wrong probably someone else spelled the same way wrong before but I think this is the right spelling but I'm not enough so I don't I don't know so this is called so this technique will be called chicken organization or also called bridge regression and so the idea is that instead of solving a so instead of solving these coefficients that just minimizes X a minus y so it's instead deciding this right this was going to be my solution for least squares and there was the simple closed form of how to doing it Rachel instead doing this I'm going to add on an extra turn here so I'm going to I'm going to penalize by this parameter s the norm the to norm of the coefficients here okay so if the coefficients are larger than my total cost the total cost is is over both these terms I'm going to penalize larger coefficients force them to be smaller than larger the value mess this more I penalize large coefficients and so it turns out there's this nice simple closed form solution here I do s squared times the identity matrix the right size and I put it inside this inverse and I get a closed form solution okay and this is doing what I want you but I'm making the coefficients are going to be smaller the larger I've analyzed with this value mess ok this is tickin authorization and so this will make things more robust down and there's a nice solution to it there's a what we're going to talk about today's it is called the lasso and so there's and so this is slightly different and so I will call this so let's sew a hat s is going to be argument a so this part is the same um but now I'm going to add in instead of minimizing with respect to the tune are using the 100 right the one arm is the sum of the absolute value of the coordinates and so this is sort of 0 it's the sum of the absolute values for this one remember and say a and that was the sum of the square the coordinates okay so this seems like a small difference it turns out this one does not have a simple closed form solution like this people okay um but it is going to have to um it's going to really cool properties and we're going to talk about these properties the first one most of today we'll talk a little bit about the second one and then we'll continue on that on Wednesday in high dimensions it's gonna bias towards a sparse so most of this vector a is going to be zero in high dimensions and I'll try and explain why that's true um and so it's also going to force to consider the value s okay lovely you should been wondering is what value of s should we have chosen here okay I told you that the larger s the more you bias towards coordinates of 0 but it until you Tom how to choose that and so we're going to you going to choose s with cross validation and turns out but the kind of the good thing about the lasso and it's kind of a blessing in disguise is that there's no simple closed form solution for all single about a mess the way you solve it generally is a way that solves it for all values esas simultaneously this is called least ingo regression is a version of this and you saw all the ways of s and so when you're doing that you can come across ability as you go and so you can say which value of s is the best as your computing them for all values of s you can kind of score them and we'll talk more about that later okay any questions about lasso cookies that the reason why it biases towards varsity should should still be mysterious but ok so the next step is to realize that for all values s there exists a value T such that let me write this up in a slightly different form such that and so we can say for every value s there's going to exist a value T such that the solution a hat T is going to be the one that minimizes the sum of the squares subject or such that the norm the one norm of the vector a is bound in less than T ok so instead of the first value is like a soft constraint it's not really constrained I'm just some soft penalty this is a hard constraint and say it has to be messing this value T ok and so how do you see that this is true as s increases that means more of a restriction t is going to decrease as s increase then T is a decrease okay so how do I solve this well if i solve so T the T for some value of S is actually going to be the one norm of MAS right so I solve this expression for some value mess okay and then I take TS and so I know that this is that this this value of ay SI found is going to be is going to satisfy this condition is be less than equal to t in fact is going to be exactly for the teeth and that one is going to be the one that minimizes this expression so i could give this trade up so i can write it in either form and so the first one kind of seems more natural is an optimization problem but this will be much easier to think about and actually the least angle regression algorithm is going to kind of operate by thinking about this value fixed hard constraint that are you T so what so special about this that's cool why does this round time for a picture and I think this is kind of one of the cooler pictures in the class so hopefully I can I can do this right think about a let's draw so this is going to be this is giving the space of a okay and I'm going to bound this is going to be teased this was six steps okay okay so if i put if i put a and an l2 constraint on t if i put an l2 constraint on it then it's going to look like that a this vector a I can think of it in this D dimensional space or this D plus 1 dimensional space so this this space here is this our D plus 1 so in this case D was one because I can only really draw two mentions but a is a D plus 1 dimensional vector and I have a restriction that this is an l2 ball this ensures this orange one ensures that that a 2 is less than equal to T ok so if a is inside of this orange ball it's less than equal to t if this if this t is is this far this is supposed to be T this is a tea and a tea and so forth but this was a minus T but but that's the distance from the heart okay now if I draw an l1 bow instead how's the hell one ball get a look people remember it's the diamond right let's get a l1 ball is the sum of the distances the sum of the coordinates it's going to look like this and this fall is going to say that a 1 norm is lesser equal to T okay so using this l1 constraint in fixes that the a vector must lie inside the diamond I'll to constrain fixes and lies with inside this ball okay so what's the difference why why is this better so it's going to turn out that the corners the blue one that Adele one ball is pointing right and this is going to and it's pointing exactly the corners lie along these axes and this is what's going to be important let me draw another object here if I can do this as well let's this is going to be so this is going to be the solution for a in with with least squares with ordinary least squares right so I just solved the least square solution I have no regulars in turn okay so let's say that coordinate for a is like 11 and 3 right so those are the values I got for a but that has the norm is too hot the norm T in this case was six right so I'm not allowed to do that so the solution i'm going to get is going to be the closest solutions for a on inside of the space here okay so let's see what that looks like pickle and the closest in the note to sense how to sense beams inside of a ball alright so let's first do the closest point in the to the l2 ball I can think of drawing a ball around a scale look like this I'm going to find my ultimate solution will be here this is going to be a hat of T with the l2 solution with with that with the l2 regularization what is the solution going to look like with l1 regularization the hell one constraint I'm going to grow all an l two cents around a the least square solution until I hit this blue diamonds please try and draw this carefully here if I draw it it's going to look something that's going to be a little bit bigger ball because the l2 but then I'll one ball is a little bit smaller then I love balls a little bit smaller so where is a hint it's can hit exactly right on this corner let's get exactly on that vertex enter the closest point is going to be exactly on the corner and what's special about the solution here what's special about that solution what is the y coordinate with that solution the y coordinate 0 yeah the corners tend to be in places where the coordinates are 0 right now in higher dimensions this blue ball is actually going to have another pointy thing coming out this way right coming coming out of the screen here and more so than just having you can point to things coming out it's going to have like this kind of edge between here okay if the closest point is on the edge then it's a that is going to have the disease or the whiteboard and still it's at zero the Z coordinate would be nonzero if I'm on a corner two of the coordinates are zero if I'm on one of those engines one of the coordinates deserve okay and so if the true solution is very much further along one of say the x coordinate instead the white board is going to tend to make the whiteboard zero but as I rotated this solution up higher y coordinate these balls are eventually going to start touching along here and not going to get an e sparsity and eventually high enough des they touch that point right so when the solution has about all the same coordinates they're all about the same value it's going to tend to be dense but if some of the coordinates are more important than others then the ones that are less important are going to get zero dies right so this is why this l1 regularization tends to bias towards the towards of sparser solution that kind of make sense so one thing you're thinking well this l2 thing didn't give me the sparsity that one gave me more sparsity why not l0 right remember the some point I tried to draw one these l0 things or say L 1 half it looks kind of like that and you can see the yellow it's magenta it kind of looks like like this is this quadrant of the ball it's any harder harder maybe even if possible for for some now if you had yours a true solution with here that it's hard to get the closest point isn't in this this region here it's almost always going to be in the corners right but turns out this this would be like on ll say one-third right the smaller I make that value the easier i'm going to buy it's more and more towards these corners the problem is this shape is no longer this dish it is no longer going to be convex and that's going to make the fob really hard to solve I haven't told you yet how to solve this expression with this l1 constraint turns out for l1 you can there's a nice solution how to solve it it's kind of it's a bit it's it's a bit intricate it's not as clean as through the ridge regression but you can do it you can write there's code out there that doesn't it does it exactly right for this solution it's in the hard optimization problem there's lots of local minimum it's hard to solve this so the l1 serves as the best convex proxy for the ELLs 0 concerns l0 just how's the number of non zeros so l0 just catalyzes the counting the number of things that are nonzero that one is kind of a convex version of it okay um okay so hopefully you kind of have an intuitive sense of why this is working yeah I've seen you know I see if you look at other textbooks or other places you'll see somewhat different pictures of this but I think this is a good way of seeing us what's going on here all right so okay so now how would we how would we solve for this solution like how do we write an algorithm that that does this and we're going to do it for all values of x actually we can do it for all values of T instead but there's a there's there's a way to go back and forth between S&T or it doesn't matter which one you use so any ideas upon unity what's going to happen if T is very large it t is really large so T in this example is like six grid cells what if T was like was like 100 roots us then what would the answer be yes me exactly blue squares it's not your funny constraint that the least square solution is going to be guess so if T so for teeth large you get the ordinary these squares a which is going to be debts that's going to be this yellow non-zeroes unless you kind of get lucky and it happens to live right on one of the zero points ok so then what happens if T is small very small love t is basically zero what solution are you whoopty is exactly zero yeah if T is exactly 0 then this all shrinks down to zero and the solution has to lie inside of it so your solution is an all zero basically for every input you're predicting a y-value of 0 that's that's probably not the right thing to do unless you're an idiot really hard for Beijing or something like that no that's not how big it's it's a six works but I guess you're really strong prior of if you're a very strong bheja man you're also an idol space that's why the reality everything is here ok so if T is really small you get zero what happens if you make tea slightly bigger than 0 what's probably going to help you went through an eyeless face you're kind of coming out of that school yeah good so is the tea is very small you doing it switch colors I family can't write very sparse yeah so T is very small you get a very sparse solution and initially it's going to be so sparse so you can only have one 90 right one non zero okay so if we were going to pick one of the coordinates to be non zero which one would be in pic like if you saw this picture here kind of so you're going to pick a coordinate so we're going to we're going to start very small here so we're going to choose coordinates will call this j 1 1j one is going to be r max J so it's going to be the value J which maximizes the absolute value of x j so this is going to correspond with the column of X remember X was all the data points with the one column right so maximizes the column J which is most explanatory right this is just like orthogonal magic pursuit the first step we're going to take the column which is most explanatory of the data this tells us if we start moving in that direction we have to choose one direction to move in we're going to choose this direction so if you're continuing with this nihilist analogy and also at least someone's hoping I'm going to do this you're you you're from Lily Nautilus one thing the world makes sense right this is this one coordinate is that right so this starts making sense okay so you're going to start with that coordinate and then if you're doing that your solution kind of as this ball is growing your solution is kind of moving in this direction right so I'm starting at zero and I'm kind of moving in this direction as the ball is growing out moving along the x-axis okay then at some point you need to add another coordinate right in this example eventually get far enough out that the closest point on the diamond is actually going to be not on the x-axis so so so so we're going to start we're going to slowly increase T as we do this we're going to pick one coordinate and just like in orthogonal match pursuit we're going to be able to figure out what is the best explanation we can get with olympus one coordinate who actually the as we increase t the this coordinate is initially going to be cheap price okay a oh yeah this was a lower case so this was an element of a aj one is going to be equal to t as i increase t the value of his coordinates can be exactly team okay and so we're going to think again with this with this residual we're going to think about this in terms of this residual where r of t so this is the residual of what I have not explained left as a function of tease me y minus the sum j equals 1 to d from 0 to D of XJ may J okay so this is aj is a function of T that's we write this differently AJ is a function of T is equal to okay so what I only have one coordinate most of these terms are zero except for one of them and then at some point I need to add add one more of these terms okay and this is going to happen after point where and so this is going to be a linear function of T up until this point until some point where keep doing this until we're going to have x j 1 dot R T is going to be equal the XJ to our and so when with this term which is the first chord we found is going to have the same kind of explanation power of the residual of another term j 2 then this is the second coordinate we do and so if you think about in this picture what's happening here is eventually if i draw the the closest point in the green circle which is going to be shrinking my air because of regularization is going to be shrinking as the as the solution i wrong gets bigger the measure this diamond is going to be such that the closest point to the green circle is going to is going to touch down on the vertex but on me on the line and then there are two coordinates equally explanatory and so at that point in two dimensions these coordinates are going to have equal explanatory power and so at this point you're going to have then you're going to have this so half the after this happens you're going to have a I'll try and write this down and then it's going to look by confusing initially and hopefully not stay that way hey J minus one ah AJ one at at some specific time t2 so this happens at on T to this happens at time t2 + t minus T to sp1 and a j2 mt j 2 it's going to be t minus t2 okay and we have what's happening here is now I'm getting these two coefficients are going to be nonzero and they're growing linearly with respect to this parameter TV and so basically this first one grew until time T to when I found the second for guests and I had this value and then it's growing at the distance from time T 3 so as t get bigger than t2 on I've grown more at this fixed rate eat 1 and similarly after time T to this one is growing starting from zero now at a fixed rate p 2 and i have to have that the the sum of the absolute values of v1 to v2 is equal to 1 because as I'm growing up at the rate i'm growing tea i'm distributing the growth among both of these two terms okay then kind of working out how to solve for v1 and v2 you can do but the important point to realize is that the growth is linear in both of these so this expression is still a linear expression of teeth every time we find one of these events that's happening where these absolute value down products are the same i'm i'm i'm i'm getting a kink in some linear curve but in between its linear and because it's linear you can always solve for win this next event is going to and so now there seems to be lots of technical details you have to work out but hopefully you believe that something like this can be done okay and then if you look at the notes I've written out some comes from so i've written out some some pseudo code say how do this is more detail but this is kind of roughly powers so i also linked on the web page there's this book which is freely available online college elements of statistical learning by bunch of you statisticians out stanford and it has like maybe 50 200 pages of us in much more detail not just this out of explaining this whole effect and this and then that's then it does a lot of cool stuff be honest but so there's a way to solve for and so if you want to find the coordinates for a certain value of T then you're running this process you keep increasing T and you find these in bed points and you broke all the coefficients however many are active at some linear rate and eventually you hit the value of T that you want then you stop right so okay so so again what value of T did you want well the good thing is you solve this in the process you solve this for all values of T basically until they stop improving at some point to you you're all inclusive least square solution and all these coefficients essentially have all of these inner products are def one becomes Europe because they there on the residual there's nothing left to explain because this residual becomes here where at least it uh yeah so you should probably stop before you get but when you solve this for all values of T which implies you solved it for all values of s so now you can go back and look and see which one you want sometimes you'll do it based on cross validation sometimes you do and say I only want six coordinates right after i get to six coordinates i'm going to wait maybe right up until i get to seven coordinates i'm about to get to the seventh 1 i'm going to stop right there when i only need six okay um there are a little weird page banner here so this process which I kind of sketched over here is called that this process goes called least angle regression so there's a couple of things worth pointing out values of aj can be negative okay this should make sense if you're doing if you're doing linear regression you values your coefficients they can be negative if that coefficient is negatively correlated with what you predicted okay um what's strange is as you progress T as T increases value of AJ can oscillate between negative and positive and so it can oscillate between negative and positive this should seem like a little bit unintuitive it means if it starts growing positively it a not always grown positive it seems that this becomes if there wasn't this least angle regression is a pretty clever solution because there seems there's lots of local minimum that could be kind of going on here it turns out this this essentially is giving you the optimal solution there's one trick I didn't I didn't mention some some detail you have to do what can happen is as it goes between positive and negative when it hits zero you want to snap it 2-0 so if you want to say if AJ hits zero Smith snap and what that snap means is that if you're going to move it away from 0 again you need to reencountering these conditions with that hoarders so you take your unused neck cord that you no longer use that okay so what's kind of going on here is that you can think of there's one variable which does a pretty good job of explaining some phenomena by itself but as you get a better model there are two other variables which which combine do a better job of explaining it in fact that you have both of those you can get rid of the first quarter you can get rid of the first very this this can happen right so so this is related to this effect called collinearity where the first to this second two variables kind of had within their span or very close to their span the first variable I didn't add a lot more to it but if you're only to use one of them they did it was doing it was doing a good job okay okay so if this happens it's snapped so that means you then only change AJ if you get so if you get this condition again if you hit this condition with aj again that's the only time you groove hey Jay from zero so sometimes it will come hit the line and then at some point later go back up or go down okay so this makes a little bit Messier if if this was if you didn't deal with this case then the first approach was actually you can get to run I think in in in linear and basically linear climate in the data but with this condition you can actually go through some exponential number of different subsets and so that the runtime becomes X it's no longer a polynomial ography but you need this one to get the exact answer for a long time this was conjecture to actually have a linear number of times this could happen but there were some really crazy examples that that showed that this happen happen this we have an exponential number of times but it usually doesn't so so if you have an implication of this fly use it and not worry about it but kind of worried about things or things are not going right that that kind of say I don't the other one works well enough without this game um okay um what else everyone say so ok so we're going to going to jump I'm going to spend a lot more time on wednesday about cross-validation but let me just say in what sense let me just try and get across kind of this point is this better than ordinary least squares so I said this was going to be more robust Dow Liars but ordinary these squares was the best least squares fit I I fit my date of the best way possible so is this is this way that's regular eyes better than these squares in what way is it that yeah so it's like you're if your training set yeah okay I'm so good so I'll try and read explain this so this is so we're going to look at something called the generalization error and this is kind of a different way of looking at the air and under this generalization there there's going to be some S or T parameter which is going to do better on and so the way to think about this as your data this is D plus 1 and you're going to and this is going to get into the two doing cross foundation but you're going to train on this data right this is through this is through n1 or hit so the number of training data you have so this one is going to build a model hey so this was our data key here I'm just going to build a model pain and then how what you want to do you want to say you want to measure the care and we'll call this yeah this was this is P and then you have X where this was the y coordinate and so this is like missing you want to say from this x test times a minus y test you want this air to be small okay so I trained on the data up here but then I want to evaluate it on this other day that I haven't seen you so if you're just trying to see how much air do I have on my training data you can't be ordinary least squares so if you're measuring this sum of squared errors it gives you the best optimal fit for that deep but if I want to train on new data which is the whole point that's right I want to take some data build a model so then when I see something new i'm going to make a good job of predicting what happen what the value is going to be then that has been stimulated by taking the game set and splitting it into the training partners testing times and it's in this sense that doing this regularization is going to do better and this this is true both for the ridge regression with this easy to solve Lisa L to regularization as well as does this lasso which has the l1 regularization which is going to be harder to solve right so they're both going to give you a better solution in this sense it's no it's not clear what value of s to use right you're going to do this for some value of s and if s is equal to 0 then this is exactly score solution if it's something else that you're going to get that right so you're going to get some air as a function of s and you want to find the best s here and so essentially finding this best s is what is this cross validation them all talk more about it but if you want to prove improvement bounce and you have to look at it in this regime and this is really what what what tends to me okay so so that's all I have for today so we'll talk about cross validation and noise of all sorts of kind of views of noise training tomorrow Wednesday and then next Monday will be a really fun lecture on I'm dealing with privacy and data there's there's like if you're part of this big data certificate we we agreed every every class part of it should have some ethics component and so the next Monday is kind of ethics lecture by viewing it through privacy and I'll give some some some stern warnings about what not to do as part of that hopefully hopefully they'll actually make a difference so it'll be fun all right so i will see you on wednesday hopefully we'll get the feedback your private interview report and the next one workouts 
dRB_641k_aI,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-03-02T23:38:43Z,Data Mining (Spring 2016) Lecture 14,https://i.ytimg.com/vi/dRB_641k_aI/hqdefault.jpg,UofU Data Science,PT1H22M32S,false,148,0,0,0,0,I moved the deadline for the homework back to up to saturday at midnight sure you walk out slyzer there was there was a mistake where I signed from last semester we got propagates forward that said Saturday the seventh at midnight there's no Saturday that seventh this year so I I want to be done before you take the midterm which is monday so i put it on saturday night so so it's reasonable so so hopefully now you've enough time to finish the homework if you haven't already and a lot of you have turned it in already which is great alright so any questions on the little bar or the or the midterm yes okay so well well then we'll get into the topic of the lecture which is the singular value decomposition and so this is kind of a very powerful tool from linear algebra that turns out to really be that the core of what's going on in a lot of techniques that show up in in data mining from dimensionality reduction two things called principal component analysis two eigen value decomposition we talked about before to multidimensional scaling and these are all all related so what we're going to talk about today is basically it's so we're going to start with basically review of the SVD I'm not sure if it's taught in the linear algebra class here i think it probably should be if it's not but it may be taught there we're going to review the SPD and all the linear algebra that goes into it so and so trying to give some we're going to try and give some i'm going to try to get some geometric intuition about how to think about the singular value decomposition so it turns out when you're doing when you're using this for data mining there are there are lots of places where it comes up but a lot of different places are used use this in a way that's different depending on the set up depending on how you think about your data and so if you just know to blindly apply it in one case you may not understand its full powers allow you kind of get different views of this this decomposition so that you know your you'll be more comfortable trying out in more than just the standard way doing so hopefully that that that helps and then so then we'll talk about the the connections to pca which is principal component analysis to the to the eigen decomposition which we talked about of a spectral clustering and will come up again in the page rank and then into multi-dimensional scaling which maybe you've heard out some of these terms are defined differently in different contexts and different textbooks and they often mean roughly the same thing but sometimes slightly different and I'll give what I think is the most standard way of viewing them or at least the most logical way to think about them okay that's what use in this class but you might see them find slightly differently elsewhere for kind of various reasons okay so what we're going to be thinking about is our data our data is going to be a matrix okay and we'll all right this matrix as as a here and it's going to have these d D columns and it's going to have n of these rows okay and so now we'll think of every individual each row is going to be one data point okay each row is going to be one data point and then if I have a column right so this column going in here this is 11 attribute of the data point or you can think of it as one so one coordinate all right so what another way to think about this is that we have these these these data points AI and a and each is going to be some data point in our d so it's going to be innocent d-dimensional euclidean space ah the greens not working good not kind of see them okay anyways yeah that's not to try and fight it alright let's try this again so one column looks just like the purple let's try blue Ida looks okay and it's different than the purple all right let's let's try that so this is one attribute of the data for the one coordinate and so then for each AI and an a a is going to be in in our team so we're going to think about these as high dimensional points each row each of these rows so this is going to be ro AI this is going to be a high-dimensional point okay and so then if you think of it this way what the goal is going to ultimately need to find a way this G is going to like end is gonna be big these can be being we want something a smaller set of dimensions which is going to work nearly as well as these so often we want this between like two or three dimensions we can actually see what's going out we can flop right so even d equals 10 it's hard to visualize what's going on so going down to two or three would be helpful we'll give some different examples okay and so more specifically what the ultimate kind of goal is going to be is going to be to find a mapping from RD to RK and this thing so this this mapping is Gabi this mu mu mapping and actually there's a specific RK here this is think of this as lying in a subspace of our DS this is a little weird to write it this way but and you want to minimize the sum of AI and a of AI minus okay so so for every I would have sum over all the data points from every data point which lives in a high-dimensional space to a low-dimensional substance so I'm mapping to a lower dimensional space here think of k as maybe two or three and I want to do it so that the sum of the squared distances of all these these movements of the points to this lower dimensional subspace is is minimalist good so through my house because okay yeah all right all right so so let me that's it's good to have a mathematician in the room so let's define this to to a subspace f and will say that so let's get a map from our DS with subspace f and will say that F is is is is as a k-dimensional so substance okay cuz it through this a little better so below your filling in zeros are girls not really so it's a it's not what we'll see it what we're not it's it's a it doesn't need to I mean it fills in zeros in a certain in a certain rotation of the parameter space so so well I'll to find this more more clearly but this is kind of it so male try and draw a picture so we're going to think of these points and pretend that so will first start with D equals two and then i can draw the subspace here as f now f is now a one-dimensional subspace and i want to find this subspace such that if i look at all of these projections which you can't really see but i'll draw a lot of em so you don't want to anyways right if I draw all of these I some there squared distances right then I want to minimize the choice of this one dimensional subspace right so it was the smaller dimension now if d was hundred and this was two or three dimensions this kind of works again right it's in fact we're going to define a an orthogonal Euclidean basis that lives inside of that physical so it's actually gonna be it's actually going to look like RK exactly we're going to define this basis and we can give everything coordinates in this new K dimensional Euclidean suspects right so let's I i know i thought okay yeah all right so so so let's uh so it's you know it's it's very it's very cool how how this all works if you haven't haven't seen it before all right so let's so let's so so this is the ultimate goal of what we're going to be doing and it turns out that there's this very old linear algebra tool that will give us exactly exactly this exactly visible to find it more precisely and so it's going to we're going to start by introns we're going to we're going to start with this matrix a right this matrix a again and the singular value decomposition is going to be writing this as a product of three matrices so we're going to be a is going to be equal to u times s times transpose and typically if you call this in MATLAB which I'm going to ask you to do on the homework and so forth you would do SPD of a you can fly do this and other languages of the matlab octave is open source and free and use that instead we're going to product of these three matrices and they're going to look something like this so the first one you is going to be a big oh all right you is going to be this this big old square matrix it's going to be n by n you're going to have try this one nope and see the helmet at all ok s is going to be n by D again this is going to be s but it's going to have a property that it's all zeros except for except except for along this diagonal all right so so when I'm drawing these boxes you're not familiar with this kind of way of doing think of it as a matrix there's actually a bunch of beacon thing of like a grid in there and every all these elements in there and so then the last part is going to be B transpose so s was s what's n by D and then the transpose is d by d okay so this thing of value competition is going to give you this so you're taking a matrix which often is already big and you're making it even bigger but these are going to tell you a lot of cool stuff about a so let's talk about these now so u and v are going to be these orthogonal matrices so that means that what they're doing what you wish you can think of doing it'll draw some pictures to help you imagine this little bit better as their kind of rotating the space they're changing the coordinate system about the space and I'll describe what that means in this context a bit a bit more so so each so that's going to be a few things so each column of of U and V so the row of the transpose is the calm of a V is is going to have so for instance you I it's always going to be equal to one so what this means is that this is not changing the scaling of anything every time you multiply by any other columns it's not going to change it as a scaling and that for each pair so for a pair of of the columns UI and UJ we're going to have that you I UJ is going to be equal to 0 so what was it mean if the dot product of two vectors is equal to 0 means they're orthogonal to each other yes so so it's gonna there again need to have these these right angles this is you I this is jay so it means that so just like the x-axis and the y-axis are orthogonal to each other these are going to be a future but they're not they're usually not going to be exactly the same x 0 y axis there can be a different set of axes that are rotation of the original ones okay and also it's going to be that the transpose is the inverse so the transpose is going to equal to the inverse so that this implies that the transpose V is going to equal to thee is equal to the identity matrix so that means that you multiplied by V and then D transpose next to each other in kind of a string of products those are going to cancel each other so and we'll see this when we talk about relating this to the aggie decomposition this is going to allow us to understand alright so those are the abdominal matrices in that doesn't really tell you that much yet what's more interesting is when you start looking at this it's let me go down here and i'll come back alright so this so s is is going to be a diagonal matrix as I said and so will will write these s 1 s 2 up to sr4 where r is less than or equal to d is actually up to then so r is less or equal D and this is what's called the rank okay so what's going to happen is these values are going to go so this is going to be as I so the I one in there is going to be the entry on this diagonal and everything else is zeros in there is except for these on on this on this diagonal and we're going to have that s I is greater equal to s I plus 1 right so there's an order these are decreasing okay okay so now what's kind of illustrative is to let's see if I can draw this effectively without messing too much stuff up so this part of this matrix and this is a good use of the colors you can't really see this matrix dispersed all zeros and so when you multiply this by you that means that this part here is x all 0 so this alright so this part of you is doesn't doesn't do anything so so we can ignore this part of you now it has structure so it's you as an inner vago matrix that completes own thesis but this part of it doesn't matter at all because this is all Syrians sometimes when you see this written you'll see that you'll see s written as a square matrix just the diagonal upper part but then you can't believe product them together in a waitress okay and then if the rate is less than D right so now if if some of these other ones here are also 0 right if these are also 0 then this part also doesn't matter and that means that this part also is not important okay and this bottom part of V is also an outward okay so if there are more zeros here if the rain stops there then this part is not important and that part you can ignore isn't okay and so typically what you do when you look at this you're going to pick some value SK and say this is the last one I care about and then i'm going to make these all 0 okay so there may be some other stuff here but these are decreasing these are getting smaller and smaller in a lot these tend to be really close to zero this is kind of the noisy part of the data in some way so you just snap all these 20 here so you reduce the rank reduce and so that means that if you go back up to our up to our matrix you set more of these things to zero this is up to has k okay and so now this becomes 0 as well and so now you trim off even even more this right and this one gets trimmed and so what you have left now afterwards you're going to get this matrix would be called UK this is an N by K matrix the top part here is called SK and this is going to be a k times D matrix and this top part is the K transpose which is also going to be that you that wrong yeah that's that's still fine it's yeah the reason you chop off the bottom ones then kind of the it's because there's also these kind of these blocks here that you also don't care about you all right so that's why the bottom purpose doesn't matter in terms of it okay so then this this is going to be this top part here v k transpose is going to be a k by d matrix here ok right ok so now let's look at what's left here I'm not sure I got these ok so now we're going to get our a matrix we're going to call this now yes okay just end by D and this is going to be equal to you which is n by K make sure it gets think I'm one of tats and stuff of zeros here this is SK let me draw these metric at this rate this is the transpose K this one you want the rows of here this is going to be K times D yeah so then you can make this a K by K matrix yeah make a sk by kik so you shrink it shrink it so yeah so you just kick keep the upper k by k in this works so this is you like alright so now this is called you ok so now this is what's called a k now it's Estill it looks it's an N mighty matrix again all right this is you k sk + VK which are now these smaller parts that you care about but what you know and they don't argue to kind of watch it through the match max of this now that now each of these data points now so these are a one let's call them bar so so for all I'll cable for all a one bar these are going to live in some subspace F and the subspace f F is going to be so K dimensional and this can be a subset of rd right because this is still going to be have D coordinates to it this still has d coordinates and my knee matrix so so this has deep coordinates but it's going to turn out it's going to live in the subspace f which is going to be k dimension so I'm going to this gives me a substance it turns out that these data points are going to be this projection that I want to define okay so so I'm going to have this this mu of AI is going to equal a I bar that I get out of this where based on how I stuff is crammed for King right so this is going to get me this picture picture here where f is this subspace and I'm projected all the data points on here and this is giving the one that's minimizing the sum of the squared pairs ok so let's so this is kind of the magic of what's going on here and we'll walk through this a little bit more now but right so you just you do the Caesar value decomposition and then you can get out these big matrices but if you look at just the right components often this this tall skinny matrix this small square matrix that only has stuff on a diagonal and then this is just long and short of shortly trips if you put them back together again then it's going to give you this K dimensional suspects if you want so this is pretty cool it's all kind of loaded in this in this operation which is it's a there's these libraries that are written in like really heavily optimized Fortran that any programming language you you use at least any reasonable one will have this built-in if they all call the same subroutines Utley and they do this like very accurately very official Ames so usually when we call this without thinking about if this one time becomes a problem we'll talk about some techniques that can deal with that later well for the most part you can use this as a black box to calls okay so what is going on here what how do i how does this work how did I get this K dimensional subspace here so let's dive into the linear algebra a little bit more to see what's going on here okay so one important thing about these or diagonal matrices is that let's let's say that I can write V actually let me have some notation under here now these values here these are going to be called the matter with Joyce these SI are going to be called this these are called these are called the singular values ok these is elements on the diagonal matrix the rows of V these are called the right these are called the right singular vectors and these columns of you these are called the left and these are called the left singular vectors what's going to happen is then I'll draw some pictures up with this these right singular vectors are going to give us a basis a sub supply coordinates system that lives in F this subspace that we found these singular values are going to tell us how important each of those directions is and then these are going to navigant that individual data books so these tell us just about kind of properties of the data as a whole if you think of each of these as data flights they kind of tell you aggregate statistical properties of the day as pull and this massive back on an individual database so they all contribute towards these statistical properties and I like to think about you can do the same thing if you looked at the transpose of a you can you can flip this around and and this whole thing is is is up is on this whole pod thinking of D data points in n dimensional space and sometimes you want to think of it the other way so don't you know be willing that you can think think of them that way as well okay so what is why does this work so let's write these right singular vectors V as v1 v2 vd right so these are the columns of V or the rows of B transpose okay and so what we're going to be able to do is we're going to save for for any vector X that lives in our d we can write x equal to the sum of i equals 1 to d of elf I times d aye ok so there's we can write this vector as the sum of a bunch of these weighted vectors from the right singular values you can do similar things transpose fussing to the values with the n-dimensional vector but ok and so these alpha highs it turns out that there's that the alpha I is exactly is the dot product of X with VI so it's just the dot product ok so so i can write this in this coordinate system ok so now if my coordinate system my singular rights of the vectors was the original orthogonal chords this week's pair with the x-axis the y-axis and so forth then what would these vectors look like well then if this was the original coordinate system then um so the so this was the original coordinate system then so the eye and actually usually people call that e I then it's going to look like 00010 right so this is the ice entry okay so I needed each of these vectors to be have a unit norm so that means if I if I summed up all the squared entries that they would sum up to 1 and so I have only one entry that's one so that works these them all to be orthogonal with each other so they took the dot product right with this one in a different one that different entry is one is going to be 0 right so this works as a coordinate system and then if I added these up will the dot product with me vector and this just gives me the high court and the now product is this remember the sum of you just sum up the first coordinate with the first chord that plus the second for the second part in all of them are zero except for the item right so I get back this vector again 14 day at a time but with the new coordinate system the same thing works except these vectors are not like the all zeros except for one one end there's still a diagonal to each other it still gives me a coordinate system just a different room and I can still write Maya Maya Maya vector X in this way so this means it is giving me a new coordinate Macy's and so I could think of writing it in this coordinate system instead I could say that X in this coordinate system is equal to alpha 1 alpha 2 up to alpha D in this new coordinate system where I had V 1 me too if I wrote the coordinates in this space this is slightly rotated from the original coordinate system well then I would hear that gives me the point acts the same way as I existed before with the original court system let's go through an example of doing it so this is what allows the rights and of vectors to change the coordinate system and it turns out because i only have k of them i only have k coordinates so i can only live in a k-dimensional space because i stopped only using k i only have a k-dimensional space ok let's go through an example here and right so I got this example in the notes okay so go through a small matrix a which is going to look like 4 3 2 2 minus 1 minus 3 minus five minus two and so I've done something so that I I made sure that this was what's called centered so if I sum up each of the columns it's going to sum up to 0 that will make things look work a lot nicer otherwise something is going to feel a little bit some things might seem a little bit more strange ok so I'm going to get these these different data points so roughly the first one is roughly here the second one here it's not a good color 43 should be 22 then minus one minus five but minus one minus 3 and minus five minus two right so these are these are the four data points here okay and so if i do the SVD and I have it written out in the in the in the notes I lunch it just right out S&P so s is going to be 88.1 62.3 about zero so this is going to be the s-matrix the b matrix it's going to be minus 0.8 roughly minus 0.5 8 minus 0.5 80 okay so on each of the columns here so this is the first right singular vector and knows that these are zero sorry mom if I saw these these up and if i square these and add them up it's going to be one say here and these are orthogonal if you flip one of the signs it's going to be it's going to make those vectors orthogonal yes no s this I mean the top part of it is square this is not n by K remember if i needed to multiply these together and need it has to be n by k otherwise the coordinates don't work but the lower part i can ignore if otherwise you can't multiply a d by d by combining pictures so okay so and then you can write out you and so 4 by 4 matrix i'm not going to write out okay what's interesting is that the first singular vector is going to be the first column of V that's going to point roughly 2 here ok let's draw a line through here it turns out this line is roughly going to be half so if I were to project onto f here this is minimizing the sum of the squared errors these lines you can trees right so these this choice a path this first direction is minimizing sum of squared errors to a one-dimensional subspace okay and if I draw so this is v 1 switch colors v1 and then v2 is going to be orthogonal so this is going to be orthogonal yes this is d 2 okay and so the emission both lying on a and also see that they both lie on a unit ball it's going to go around the as well so they're both going to d these unit temperatures this is the picture and so if I wanted to write you know one of these data points say the first data point at four and three in these coordinates you know I'm right the dot product with d 1 is going to give them a negative value just be like negative 3 or something and the doc ock be too isn't give a positive value like plus 1 okay and so okay so when you think about this when one thing that's useful to do is now let's look at some some data point X right here okay so let's call this X and let's look at the process of what ax does and this is remember equal to us B transpose of X what we'll do is we'll write this out in parts step by step and see what happens when i multiply X through by hit okay what one way of viewing this drawing too much on this pick let me try doing this one way of viewing these are this well let me try doing that later I don't want to screw this up to okay so um okay so let's look at VX what what is the ex doing of V transpose X what this is going to do is it's going to change the coordinate system so if I look now and alright so if I look now at DX v transpose X I'm going to call this to match my notes will call this this exceed it's actually kind of what the Greek letter looks like it looks like a squiggle okay so what that's going to do is it's going to give I'm going to draw this in a new coordinate system now this is v1 this is me too okay and these extend out right this is a coordinate system and what and you know I can draw a remember X was chosen so it's up here so why on this minute ball that means that if i draw the same unit all again here absolutely one should also only gone out to here so now this point x is going to live where is this going it's going to live around out here so that this is exceed right so this is now it still lives on the unit ball so it still is still a unit vector but I've written it in this new coordinate system defined by a v1 and v2 okay so if i go see if i can get these on the same page at once but if you look at the v1 coordinate it's going to be some it should be it should be negative right and that's what I'm seeing here the v1 coordinate is negative the v2 coordinate is positive right that's what's happening here for X so I've just going to change of basis here I've written it in this new coordinate system okay what happens next well the next step is to write out this this this a town is going to be s x which is going to be s times to be try close of X ok so what's going to happen i multiply it by this s matrix now turns out is going to scale this out so this is where the scaling the scaling information of the matrix is all stuck inside of this s matrix so to draw this properly I need to draw so I still have the same coordinate system v2 and v1 but now my unit ball turns out to be scaled so if I remember more essential here this is v2 and this is v1 and so if I remember my S matrix the courts are eight and 2.3 so now this unit ball itself is going to get scaled out it's going to go out to about eight and two is it's going to be calm this week of this ellipse okay this should be a perfect ellipse you can't really see it here but the it's these coordinate axes go out to this ellipse let me try there now you can kind of see it right so this probably can't scan the video but they're great pictures and then the logic notes but it's kind of scaled out this unit ball so it's now skewed and so this point exceed which used to be on the unit ball is still going to be in on this unit ball but it now got scaled out to here so in this new scaled coordinate system plus the scaling now this is away from the unit bollocks on further away so now its coordinates are going to be something this is going to be roughly the v1 or delete something like seven and the v2 court will be something like to write so it's no longer it's no longer unit vector and then going and then there's one more step multiplying by you this is going to lift it to a to a four dimensional space it's going to give me four coordinates here and this is going to be victim fusing because I know any data point it started with must lie in a two-dimensional space so it's going to give for it's going to change this two-dimensional coordinate system it's going to tell me it's going to give you for orthogonal vectors but they're going to live in a four dimensional subspace so you're not going to draw a picture for that one but it's looking to look at summer so I think the first to the right singlet vector and the singular values are important one is changing the coordinate system the other one is scale ok so what they important to notice here is that any point that live units bottle for must live on this ellipse okay it must lie on this this ellipse here and it lives as a very special structure in that if it lies on the ellipse there are certain special axes of the ellipse that define it and these are exactly what the right and similar vectors are telling you they're telling you the axis of this ellipse for the scale so if i look at the norm of this red points here a tied and then norm does not change when i multiply by you so it turns out that being the norm and this is the to norm of theta is going to be equal to the to norm of x of any times X so if X is a unit vector then the to norv essentially tells me that there's some ellipse that is as describing kind of work any unit vector that the length of the unit vector is going to lie on the ellipse so if I go back to this picture again there's some sort of ellipse here as not going to quite the draw a big enough but it's going to look something like this time like this and so any any unit vector I choose X and I multiply that by a the norm is going to lie for a the norm is going to lie out here so this the length of this vector so this is going to be ax is essentially going to lie out here is actually going to be a four-dimensional point but the so the norm of this is going to lie to be described the direction x to the point on some ellipse and so this the singular value decomposition is very special in that this corresponds with disorder loose ok ok so there's there's probably more information that you strictly needed I think some of these tickets are helpful in understanding different aspects of working with the CBR value decomposition TX is little question that progress something my understanding of SVD was to go to us like Chris yeah you opsys that's right yeah you swing like wood great okay so it's it's mapping you to a four-dimensional space in my example but all the data all the original vectors X needed to line a two dimensional subspace so there's some two dimensional subspace that have lied on in there so the you view matrix is mapping this these properties on to the individual data points so I had four data points they started with and it maps the this kind of the effect of each of the singular values back under the database so that each of the four vectors are correct yeah so each of those that tells me how much how much does each data point contribute to each one of those singular values that's another way going back the other direction so the dimensionality reduction aspect is if you took this a matrix and you took just the first column and you just use that so that's this line f the subspace running through here that's how you do the dimensionality reduction so if you only took then everything if you only use the first vector then everything is going to line a one dimensional space inside the two dimensional space right so you know it's I can't draw pictures and illustrated if I started out with 10 or 100 dimensions but if I wanted to go down to dimensions I take the first two dimensions and then I can use this new coordinate space defined by these 2 v1 and v2 vectors to draw the data ports and essentially the best way for us so that's what you do the dimension a dimension so you basically occupied by the EU matrix you just take more than one nice you just take one of Markus's the way I like what started as you yeah so there's in sorted order so the first singular vector is up to this picture here the first thing that vector is the top row of V transpose that's associated with the first singular value s 1 that's the largest singular value that's the that's the most important rule so then they come to you in sorted order so you want to drop the lower bowls so another common ok the questions comments please please ask them yet the rows of the UPS the lines on the subspace of our team the mother goes out see if your reflective reducing the dimensionality would be lines on the subspace that you're mapping onto so um the columns that be the rows of the transpose those those those define the the basis in the new cytosine they may give you a coordinate based so that can your new if you rotate them then they give you the accidents that make sense right so so basically I've taken the data this example here I've taken these these four data points and I've I can I could redraw them in this subspace with v1 and v2 here I can try and let me try and do that here understand well I think this might be a host ready right so this this is a one and if I put it in a new subspace it's going to be something like a one here fight took a two it's me something like a to I do something and then this was 83 and then this is a 4 with something kind I might have not gotten quite right right but I can draw them in this new portal system yeah yeah so so I I you could think of for the purpose of this class for now we'll kind of mention how you would do it a little bit later in the PageRank part of class but you can think of just calling a black box function in matlab or someplace nice it won't be able make movies what are some black box yeah so their various ways we'll talk a little bit about the power method based ways of the PageRank part of class but but I'm not going to get into those details yeah so think of this as a black box that it's going to do it forward yeah ok alright so okay so this is let me say one more few more things about this let's say this best-ranked a approximation right so this is a so let's say I took the top cave or let's say K equals two or three in the example i was using k equals 1 because that's what i could draw let's say it was say two or three or ten what properties does this map have so this defines this mapping function on to the slaughter dimensional substance right so now you can kind of see how this subspace f it lies in rd but it has these k coordinates this coordinate system so you can really think of it as being our K this K dimensional Euclidean space so what what properties doesn't have it has kind of these two properties one is that I mentioned that it is if you want to minimize the sum for all pay ki a so if you want to minimize this so this is you can end up writing this as a minus this is equal to a minus AK so that pervy is known so the what the Frobenius norm of the matrix is is just so if i look at this is just going to be the sum over all entries squared ok so now AK is also in my d right-sided another end by d matrix so i can subtract it coordinate wise it's k minus AK it's a subtraction symbol there and so I give another majors a minus AK and this is kind of think of a noisy part of the data and if I sum up and square all of those all those entries then that's the same thing as this sum of the squared projections ok so it's minimizing this if I restrict that this is AK dimensional subspace or 88 as ranking then this minimizes this cost the other thing it does is it minimizes if I look at a minus AK 2 so this is the spectral norm so if i look at the spectral norm this is equal to the max over all x such that is it's a unit vector I'm actually don't well just it's nonzero basically this if it's unit vector then and the pass right so so it right before I remember this this M X vector if I remember what this norm the spectrum does i can think of it mapping it to this onto this ellipse it's a unit vector and appointing in some place in these lips and so if I minimizing this it's what's left over after I've taken out the long part of your lips I just have two left over so I'm going to take any one direction that's left over um how large can that be it turns out this also minimizes this this is equal to s k plus 1 it turns out so the k plus 1 entry of s so if I took if I looked at this guy here the k plus 1 that's the last leftover piece that's left so there's a leftover ellipse of all the stuff I didn't take out and the largest direction there was the k plus 1 entry and that's the spectral norman turns out and so this also process also minimizes that ok so these are kind of its its optimal in both of these regards now this is we talked about L these l1 distances that more robust outliers those are harder harder to work with in this regards there's not this there's not this SVD function you can call which solves it nicely for you as well so people usually use these that are minimizing this L to enter the sum of the squared distances right this is a little strange again why am I not minimizing just the sum of the absolute differences turns out that's a lot harder to ok alright so this is kind of this large linear algebra review of what the SPD is doing so I've 20 minutes left was planning to do is talk about now how this relates to pca tigony composition and the hem DX then these are kind of fairly simple transformations but if their questions about the SPD please please ask those as well about how to use this we'll see some examples a little bit of how to use this as well but you kind of want to do this thing of finding a lower dimensional representation of your data is kind of the main thing and you do that by mapping into this so if you want basically there are two ways of doing that one is if you want the data points to live in the same coordinate system so you want to understand what the original coordinates are then you do this uks candy cane if you don't care about the original coordinate system you just want to live in lower dimensional subspace that you can forget about this guy and you just say I just want to know what is this new coordinate system and how important it is it then it just vika ok then i can map all the points onto onto this this up onto this coordinate space using us by just taking the dot product remember i got the new coordinates by just taking the let's see by just taking the dot product right this gave me these new coordinates i just took these right secret vectors i took whatever my native length ones this could be wanted a eyes and I give the new court so i can just get the new coordinates that i don't i don't care about the old coordinate system ok so in the homework you'll see some ways they're playing with this and seeing someone how this works alright so let's talk about pca first so PCA or principal components analysis this is so PCA is finding kind of a subspace that best fits the data that's again in the subspace that's minimizing the sum of the squares what I kind of glossed over so is that so so that the SVD it it minimized these are these this something like a minus AK for instance such that this on this this subspace f which is equal to the K is kind of going wait thinking about the subspace includes the origin so it includes the zero zero point if I if I went back to this picture this half needed to go through the origin here this is the SPD finds the best subspace that's forced to go through the origin pca you might not want to do that you may have then your coordinate system orginal cordon system looks like this and your data points look kind of like like this and so now the best subspace that goes through the origin is going to be this is what's going to be v1 based on the SVD but that's not really capturing your data very well what you rather do is find the subspace here I'd rather find that subspace that doesn't much better job if I look at this this projection it's much better at capturing the data this other substance okay so so so how would i find so principal component analysis is to find this subspace that's not restricted to go to the origin so how I get so it turns out that the the subspace found by pistol component analysis is always going to go through the the mean of the data the average and remember the mean is something I can easily compute I can do it if coordinate wise if you have to mean of every coordinate and that's a point here and so if I know it must go through there what I want to do is I want to shift this mean to the origin and so I also want to shift all of these all these data points by the same vector here and you know and so forth so that all these data points now lie here and then i can find the subspace that I know goes through the meaning based using the STD and then I essentially I need to shift it back out by the negative vector of this meat okay now that the mean is going to be a data point but you can think of it as a vector and then you just subtract it from all the data so so basically the algorithm is first so one says first started so find mean up of the data let's call this M okay and then to for all AI shift to a I bar equals AI minus this mean after I do this now the mean is at the origin after this happens now I'm going to run SVD of a tilde just all the data points are shifted to the origin and this is this is going to get me my subspace half and then shift up to f + m shifted back at all right so this is step 4 you know this was a step one was here step two was doing all the shift step 3 was finding the subspace and stuff floor was now being beckoned okay and so it turns out that there's a specific matrix called the so this process of subtracting out the need is so this process is called centering and so there's a centering matrix which you can write the identity n-by-n identity matrix 1 over n the old one vectors x the old one vector is transpose and so now this i can write actually that a tilde equals CN times a and so this this matrix itself is against but this operation is sparse that you can do this very efficient because you don't have to you know this becomes on in all ones and by n matrix which you don't or which you don't want to actually create but you can multiply them peace models using operations so you can actually do this pretty quickly and then you just call the SVD on this a tilde right so you can do this edition so that so this is a principal component analysis it requires the century step otherwise you're not going to fit your data very well if it's not centered and so in particular a lot of data sets have things were like all the coordinates are positive if you know all your coordinates are positive it's not going to go to the origin and you really need to do this otherwise you're going to kind of waste the dimension going going out there and summarizing so especially if you're going down to a small number of dimensions like two or three this is really important yeah why why you should buy version okay I was being kind of like if you just care about the coordinate space then may you actually don't need to do this if you just care about putting it in a new organ space you can stop right here this tells you to do coordinate space and that coordinate space is centered around of whatever the origin in that coordinate space is good the day will be centered on the origin there the Minos will be the orange if you care about actually drawing this subspace along or actual data and need to it's a good question okay let's keep going and get through these eigen decomposition as well so the eigen decomposition okay so so if if we remember that for some matrix M then in the you get an eigenvector if this swivel this is a matrix in fact needs to be a square matrix and if this is a scalar so if this is a scalar value then if we find some vector where if you multiplied by the matrix you get back a scale of scale value of that vector then these as is league is called n eigen vector and lambda is the eigen value okay so how is this related to that the SVD well so one way of looking at this is let's start with our n by D matrix a and let's make it square I do a transpose a ok so if I do this at d by n matrix by N by D matrix and this is going to give me something that's going to be this is d by d no this is d by d and let's see what happens if i multiply it by the by the right singular vectors V I can expand this out the transpose of this of the square matrix of the of the diagonal matrix s is still um doesn't do anything because it flips things over the diagonal essentially other than making sure that the ignore things about the this is actually as transpose but this is u.s d transpose V okay and now this these this u transpose u cancels out because it's orthogonal this is the identity matrix so i can drop this and i can write and then i can ignore the zero part of this s matrix and i can write this as this and then also the v transpose the V transpose D and this also cancels out so i can write this as V s squared so I can then think of this as just the square part of s the D by D part of s and I get V the right single vectors x the squared single values okay and so this is actually i can write this because it's s squared is still diagonal this is s squared v that's just a scale actually I'm not sure that's true not till it sure that's okay so you can you get to that but for each individual what this is saying is if I looked at any individual column of V then I'm going to it's going to be orthogonal to the other columns so each on the right singular vectors vectors V I these are going to be the vector of a transpose pay and and s i squared the associated singular value is going to be equals to the hype the hype eigen value of a transpose F okay and and you can work out and do a similar thing that a a transpose you this is if n by n matrix now is equal to u s squared so now the left singular values or the less singular vectors of you are going to be the diving vectors of a transpose a and again the same single sink squared single goddess or again going to be the eigenvalues okay know if n is bigger than B I'm still only going to get Elliot and most D values here so there's still only going to be d I could guys that's because this is a tall strength e even those in head by n matrix okay so these are relationship between I can I ganong I ghen vectors and Singler vectors and eigen down so you can kind of get between them in this way so they're really the same structure how you compute them is going to be very similar as well ok um last thing talked about in the last few minutes is a MTX this is multi-dimensional scaling and so this is a silly different problem the the input now is going to be a a a a distance matrix D and so the entry so dij which is they it is going to be equal to some distance it's for some data point I and some data point Jay right so I'm given just this set of all these so think of it as an n-by-n matrix this is square and by a matrix and it's has includes all the distances in between points is stored in this matrix sometimes I'm only given a set of endpoints in a function to compute the distance and then i can compute this distance matrix sometimes I some I want to try and skip that step something right and I want to think of this think of there are ways to do this to deal with this without putting this is but I want to think of this as P i- pj so think of this as as the as the euclidean distance and now i want to find if I just have these distances I don't maybe this is like the Jaccard distance this is something that is more abstract I want to sell somehow embed these into some some into some subspace so I want to find a low dimensional subspace to draw these in so I want to embed these distances into a subspace so the distances are approximately preserved now well I'll talk about this called classical MDS there are other forms that actually preserve the distances what this does is it preserves the the projection the same sort of air I dubai by mapping to the subspace not the actual terms in the distances and this is a point of that's often miss done in the literature okay so I want to kind of use that if i look at the matrix a a transpose and say this is these are the original data points then the entry of this is going to be a matrix m and m IJ is like this inner product in the dot product and AJ and think of these as they're going to be like the p.i p.g here right and so then what I want to do is somehow convert this into the distance and so i can write out that a i minus a J squared is equal to AI squared plus k j squared minus 2 AI minus aj so what's up i have so if i have these I can almost convert into these inner products then I can treat the inner products as a transpose and then I can do the eigen decomposition take the first two or three dimensions of that to use that to draw these points okay um so but I have I am this distance and I want to get this but I don't Nestle know these it turns out I've only told you the distances so I can shift all the data points I don't change the distance so what I'm going to do is set a a 1 i'm going to set this to be 0 okay and now i can get that aj squared is going to be equal to aj minus a high so now I can actually solve for a I KJ is going to be equal to one half times a day i squared minus a 1 squared + KJ squared aj minus a 1 minus AI minus j j squared so after I've and this was a harbinger of choice and then i can calculate all these inner products map up into the mind i can then fill up this this n by n matrix a transpose from the d by d matrix and then i can do the eigen decomposition and you can just call like this ice function inside of matlab and it gives you the this this the eigen vectors which are the singular vectors and you can use those to math too low dimensional subspace so that's multidimensional scale but this is classic multi-dimensional scale it's just doing that so then do I of a a transpose and this will give you this mapping and then take the and the schedule laughing okay um so I've run out of time there are more details in the notes if you want to look this up if you have questions about this we can start the lecture next week wednesday on this if you have more questions and I covered a lot of material today hopefully some of this was review of linear algebra but if questions let's come back to class on Wednesday midterm remember it's next week monday no computing devices with all no calculators or computers you can bring in anything on paper okay all right 
nYCpKAE3JT8,26,Query optimization using Document Frequency,2013-03-20T15:48:56Z,Data Mining-Query optimization using Document Frequency,https://i.ytimg.com/vi/nYCpKAE3JT8/hqdefault.jpg,John Paul,PT11M32S,false,638,0,1,0,0,"now what do we do if our very is more general so for example let's take this query which is the and of two all terms matting or crowd and ignoble or strife so how do we process this kind of a query where we also have an all term well justice for the and heuristic we we said that the length of the postings lists the estimate of the answer if we do Brutus and Caesar we saw that the length of the answer is going to be less than or equal to the smaller of these two postings lists now when we have an all term the length of the answer in this particular case is going to be the length of this list plus the length of this list minus the length of the elements minus the number of elements that are common to both these lists so this is the familiar set Union formula where the size of the set a union B is equal to the size of a plus the size of B minus the size of a intersection B so one heuristic that we could use to estimate the size of an all term is to sum the lengths of the two lists which are being ordered so we can estimate the size of each or by the sum of its frequency and this is obviously a conservative estimate because in reality this intersection is going to be finite and the actual length is going to be smaller than the sum of the lengths but because we don't know what this intersection is unless we actually calculate it if we are just given the length of a and the length of B as given by the dog frequent the document free when she filled the the the best possible estimate that we can come up with for the or result is the sum of the lengths of the two postings lists so when we have an all term and we are taking the and of several of these all terms how do we know which sorry so this or this something or something handed with something else or something else ANDed with something else or something else how can we how do we decide which which or operation to carry out first again we can use the document frequencies of the terms these six terms we can estimate the size of each of these all terms by summing the lengths of the postings lists for the two terms involved in the or operation and whichever is the smaller pair of lengths for these three we are going to and that particular pair so if this length and this length both end up being smaller than this length that is this ends up being the largest value when we estimate the size of the autumn will parenthesize in this way calculate this result first which will which we will then end with this particular result so here is an exercise can you recommend a query processing order for this particular query tangerine or trees handed with marmalade or skies handed with kaleidoscope or eyes and these are the document frequencies of each of the six terms so eyes and kaleidoscope are grouped together because we are taking the or of that marmalade and skies have been grouped together in this diagram because those two are together in the or operation and angry no trees so how do you decide which pair of terms to take the and operation of anybody so whoever tries to answer announce your name just so that I know I know who you are because you know at this distance I can't see your faces properly so if you can tell your name maybe you know we'll be able to have a little bit more personal interaction that way hello so many yeah I think it should be eyes and kaleidoscope first because that's approximately 29,000 followed by tangerine and teal because at something like 36,000 and the last one which is approximately 38,000 so yeah yeah that's right so it's actually one order of magnitude more so this is 200 and I mean this is about 290,000 not 29,000 this it is there are six terms over here but you are right basically if the sum of these two is the smallest the sum of these two is the next smallest and the sum of these two is the largest among these three pairs so because this is the smallest and this is the second smallest we are going to do eyes or kaleidoscope handed with tangerine or trees if so I have parenthesized these two terms over here so we're gonna take this and operation first and then we are going to do this particular round operation is that clear to everyone now how about yeah so what's the reason for doing it in this order exactly like what's the real benefit so the real benefit is the same that we saw last time which is we want to minimize preferably as far as possible we want to minimize the size of the intermediate results because the intermediate results are going to be used for further operations and if the length of the intermediate results is small then recall that all these operations these boolean operations take time proportional to the length right at least the walkthrough of the pointers takes time proportional to the lengths of the two lists so if the length of any of the lists can be reduced then we end up minimizing the total amount of time to process this query so that's the motivation for solution I mean this is again this doesn't guarantee that this doesn't guarantee that you know this particular method will minimize the processing time again you can construct examples where this won't work so this is just a heuristic what were you asking sorry did I interrupt you I mean I was saying that the three or or patients they have to be done in all cases right so how does it matter what order we do it first yeah the three or operations will need to be done in any case right but we are talking about these two and operations ultimately after taking these these these three or operations you have these two and operators just like we had you know this query so the difference here is that instead of having single terms over here each of these three operands is an or result it's a more complicated example where you have an or result instead of single terms and so here instead if we just looked at the lengths of the postings list for these single terms directly in the dictionary now because it's an or result you have to calculate the length of each of these three operands and we'll have to do that by taking the terms inside the or bracket and adding up the lengths of the postings lists so you are right I mean we are not trying to minimize the the work by choosing between these or operations we have to do all three or operations we don't have any choice on that but we do have a choice on these two and operations does that answer a question yes sir okay now let's take another so one of the reasons why we are taking this and operation is because and queries are the most common queries on on the web and also you know in other cases so some of the other you know people tend to when the type queries they tend to think of all the terms as or at least most of the terms as far as possible as is necessary to be present in the document so let's look at this and operation friends and Romans and not countryman so now instead of an or result now you have a not result the third term is a not result so how do we decide which of these two and operations to do first we can estimate the size of the not result this time pretty exactly by taking the number of documents in the corpus and subtracting from it the size of the postings list for countrymen so the creek the length of the postings is four not countryman this result intermediate result will be the total number of documents in the car - the length of the countrymen frostings list and this length is something you know because you have the document frequency of countryman stored along with it in the dictionary so this pretty much finishes this section on query optimization "
H87VHjLwJHI,22,A student presentation about DATA MINING.,2020-04-10T00:31:57Z,Data Mining - MIS309 - IAU,https://i.ytimg.com/vi/H87VHjLwJHI/hqdefault.jpg,DBMS 309,PT14M21S,false,63,0,0,0,1,miss Miller I have a Honda machine Salam alaikum warahmatullahi wabarakatuh my name is de moi from Honda Takeshi and today I am going to talk and speak about that mining that mining is an important topic that is related to database and database management system so the course name is that a base management system the course code is mis 309 the assignment topic is that my link and the course instructor is dr. Maqbool Myrmidon learning objectives of this presentation the definition of data mining the applications of data mining how to do that mining the benefits and advantages of data mining the challenges of data mining the types of data mining data mining the trends and finally the conclusion that mining is the acceleration and analysis of large data to discover meaningful patterns and rolls it is under the data science field of a study data mining aims to predict the future outcomes also data mining techniques are used to build machine learning models that power modern artifical intelligence applications such as search engine algorithms and recommendation systems the applications of that are mining actually we have eight applications of data mining but I am going to talk and explain only four of them database marketing credit risk management a fraud detect and a qualitative data mining database marketing retailers use data mining to better understand their customers in fact that mining allows them to better segment market grooves and tailor promotions to effectively a drill down and offer customized promotions to different consumers a credit risk management banks deploy data mining to predict a borrower's ability to take on and repeat little bit again by using the personal information these models automatically select an interest rate based on the rank of the risk that is associated with a specific customer a fraud detection financial institutions implement data mining to automatically detect and stop the of roted transactions this form of computer happens behind the scenes with each transaction and sometimes without the consumer knowing about it finally the a clew DM the research can be structured and then analyzed using text mining techniques to make it good of the larger groups of unstructured data an in-depth look at how this has been used to study child welfare was published by researchers at Berkley how to do data mining business understanding data understanding data a preparation data modeling evaluation deployment business understanding the first step is establishing the goals of the project are and how that mining can help you reach that goal a plan should be developed at this stage to include the timelines actions and role assignments data understanding data is collected from all applicable data sources in this step that visualization tools are often used in this stage to explore the the data also to ensure that it will help achieve the business goals that a preparation data is now the Clint in fact the missing data is included to ensure it is ready to be mined the process of data can take a long time depending on the amount of data analyzed and the number of data sources therefore the distributed systems are used in modern database management system DBMS to improve the spirit of the data mining process rather than a single system data modelling mathematical models are then used to find patterns in the data using sophisticated data tools evaluation the findings are evaluated and compared to business objectives to determine if they should be deployed across the organization orphan deployment in the final stage the data mining findings are shared across everyday business operations and enterprise business intelligence a platform can be used to provide a single source of the truth for the sole of service data discovery the benefits and advantages of data mining automated decision making data mining allows organizations to continually analyze data and automate both routine and the critical decisions without the delay of the human judgement accurate prediction and forecasting a planning is a critical or process within every organization data mining facilities planning and provides managers with reliable forecasts based on past veterans and current conditions because the deduction that mining allows for more efficient use and allocation of resources or realizations kinda plan and make automated decisions with accurate forecast that will result in maximum cost reduction customer insight firms deploy data mining of from customer data to uncover the the key characteristics and differences among the dere custom the challenges of data mining Big Data a privacy and security cost of scale over fitting models Big Data the challenges of big data are every field that collect source and analyzes the data over fitting models this condition occurs when a model explains the natural errors within the sample instead of the underlying veterans of the population the cost of a scale as data continues to increase that volume firms must scale these models and apply them across the entire organization a privacy and security the increased storage requirement of data has forced many firms to turn toward the cloud computing and storage the types of that am i think actually we have two types of data mining supervised learning and it contains liner regression logistic regression time serious and I will be talking and explain these three for the other type it is the unsupervised learning and it contains the clustering Association analysis a principal component analysis the types of that mining for the supervised learning we have liner integration liner decoration protect the value of a continuous variable by using one or more independent inputs for the logistic regressions logística decorations protect the appropriate e of a variable by by using one or more intubated inputs the the time serious time serious our forecasting tools which use time as the primary independent variable for the unsupervised structure learning we have a clustering a clustering models a group similar data to cutter they are best employed with complex data the groups describing a single entity and then we have Association analysis Association Association analysis is also known as Market Basket analysis and is to identify items that frequently occur together a principal component analysis a principal component analysis is goes to restate hidden correlations between input variables and create a new variables called principal components which capture the same information contained in the original data but with this variables at now I will be speaking about that remaining trends data mining the trends we have for data mining Catherines number one language standardization number two complex data objects number three put my link number for increased computing speed occlusion in the future data mining will include more complex data types in addition for any model that has been designed further refinement is possible by examining other variables and their relationships we researched in data mining world resort in a new methods to determine the most interesting characteristics in the data as models are developed and implemented they can be used as a toll in enrollment management thank you all 
IdDVPKRCAYI,22,"Download Any web scraper from https://webscrapingtools.net/
Ultimate Extractor for Every Task
Whenever you need to extract some typical data from multiple web pages, Web Content Extractor is the ultimate solution. Extract product pricing data, grab real estate data; parse Forex and stock market figures; extract book, song or movie information; gather news and articles on a certain topic; extract web content on hotels or car rentals in a given country; collect information from dating sites or job web-resources - this is merely a short list of Web Content Extractor possible applications. Of course, you are not limited with the above; the tool perfectly works with any kind of web information and thanks to fine customization it can deal with any website whatsoever.

 
Powerful Web Crawler Engine Inside
Powerful, multi-threaded web crawler engine provides for quick and efficient data extraction. Web Content Extractor supports password protected websites and can access the Internet via multiple proxy-servers ensuring speed and reliability. Not only does the crawler support downloading with up to 20 simultaneous threads, it is also highly configurable. You can set it to ignore certain URLs or include them into the extraction basing on a title or a URL match. Such flexibility means accurate web scraping at high pace, as well as is an additional way to customize the process.

Wide Exporting Capabilities
In addition to its immerse extracting power, the program also features wide exporting capabilities. You can save gathered data into a plain CSV or text file, export to HTML or XML, as well as to put the data right into a given database format using the built-in possibility to export information into MSSQL/MySQL script or directly into any ODBC-compatible destination.

This allows you to apply the scraped data immediately - say, perform an in-depth analysis using spreadsheet application, create a summary report and upload it via FTP, or import the data into your own application or service's database.

 
Enjoy Automation!
Web Content Extractor provides serious automation of the website scraping task. Usually, you only need to specify a basic extraction pattern (done in few clicks too) and run the extraction process. The program automatically scans the provided URLs and scrapes all the info that meets the specified template. And command line options allow to set the program to work with any third-party scheduler.

The program doesn't try to outsmart a user though. Yes, it determines elements on a page and the type of the data field suggesting the extraction results as a preview, but you can always make necessary changes or adjust the program's choice manually if needed.

Reliable, highly automated, powerful web scraping software
Web Content Extractor is certainly a tool you need if your business is somehow related to web data extraction. Being a huge time-saver, this tool has probably the best value for money, plus you can try it for free! Download the free evaluation version now!",2019-01-09T12:18:23Z,"Learn Web Scraping, Data Mining Course , Lecture 01",https://i.ytimg.com/vi/IdDVPKRCAYI/hqdefault.jpg,WebScrapingTools,PT3M6S,false,53,0,0,0,0,this demo shows you how to extract data from listing pages using web content extractor first click the new project button to create a new project during the first step of the new project wizard enter the web address from which the program will start the crawling process then identify the links that the program should follow while basic rules allow you to identify links by the links position on the page advanced rules allow you to identify links by URL patterns if the links do not change position on any page of the website try using the basic rules click the plus button wait until the page is loaded and click on the next link you then you need to create an extraction pattern click the define button an extraction pattern is a set of data fields that define the positions of text and images on the web page to add new data fields click the plus button wait until the page is loaded and click on the text or image you need to extract the program defines the HTML path of the element that contains the title text and displays the new data field window which allows you to specify the other parameters of the new data field you can create other data fields in a similar fashion once all the data fields are created click OK in the preview window you can see the selected data fields on the webpage and you can see the extracted text you once the extraction pattern is created click OK enter the name of the project and click finish to start the extraction process click start the program starts crawling from the starting page follows the specified links and extracts data using the extraction pattern thank you for your attention 
dJh1MPiGka8,28,"This video is part of LearnItFirst's SQL Server 2008/R2 Analysis Services course. More information on this video and course is available here:

http://www.learnitfirst.com/Course165

In this video Scott will go over data mining, this is a big buzzword right now.  This video is a brief overview data mining, there is a chapter later in the course that will go into data mining more in depth.

Highlights from this video:

- Data mining and MDBs
- What is data mining?
- Finding patterns
- Manual data analysis
- Find ""known unknowns""
- Key influencers

and much more...",2013-02-11T21:08:52Z,What Is Data Mining in SQL Server 2008/R2 Analysis Services?,https://i.ytimg.com/vi/dJh1MPiGka8/hqdefault.jpg,LearnItFirst.com,PT8M22S,false,3314,15,0,0,2,okay so if we should get started talking about buzzwords and we talk about things that will get your resume noticed data mining is going to be way up there right so data warehouse big buzzword right now data mining big buzzword you understand the use of the term multi-dimensional database right you're gonna blow somebody's head off they're gonna think you're so awesome so I am kind of kidding with that but data mining is a big big thing right now so let's kind of get an overview of what data mining is and I really mean this is like a a very generic overview we have a whole chapter about this a little bit later now this particular graphic that we see here let me zoom into this cuz I think this is very important this is not a Scott Wiggum graphic this is one directly from Microsoft this is how the internal group at Microsoft considers the analysis services engine to flow it has two parts one part handles multi-dimensional databases the other part is data mining now this is actually kind of led to the ability to consider SSA s not just a multi-dimensional database server but a multi-dimensional database server and a data mining server now if you want to call that marketing I get it I'm cool with that however we we need to get into chapter 11 to kind of see the actual inner workings the fundamentals of how sequel server analysis services kind of works with that but I bring this up just to show you how important data mining is it is a core part of analysis services now what it is is it's the practice of digging through lots of data to find patterns probably understood that from the name right let's say that you have a hundred million rows and that data covers 30 years of trends maybe it is defect reporting maybe its seismic information like are you going to be able to spot trends into that data over such a long period of time I mean that's a lot of data to go through right that's a hundred million rows nobody wants to have to manually go to trend analysis against that kind of information so data mining you're looking for trends and patterns that would have gone unnoticed unless you had done the data mining and that's really the idea of it right so we're dealing with massive volumes of data you're not going to do data mining over five million rows she's just probably not enough information we need a lots of rows hey you when you've got a lot of data we need to spot the trends but because there are a hundred million rows in the relational data warehouse we can't easily easily spot all of these trends for a customer for example we might be tracking over 250 different data points gender are they married how many kids do they have income right we might have 250 different data points about that one thing called a customer lots of relationships between the entities spouses grandchildren hierarchies within the data itself you can only do so much when you just are given so many columns so many rows your manual analysis can only go so far an our usual way of getting answers is to ask questions right we write a query where you look at the result sets we run a report we look at the report which behind the scenes writes a query and returns the results out right however you have to know what the question is or else you can't ask it see that's the problem here manual digging through of this data is hmm what question do I want to ask okay what about I want to see the number of sales to people who were married to someone from another state right I mean you have to know what the question is and then you you the answer right how many tickets did our customer service team close over the last three years data mining though doesn't require such strictness data mining is going to help you by giving you answers and questions that you should have asked it's going to go find patterns it's going to go find trends in the data and bring them back to you and you're going to say oh I would have never thought to look at that so data mining is going to really broaden what you're able to get out of your data warehouse first off what we're gonna work with when we deal with a queue when we work with multi-dimensional databases when we work with data mining you're going to have built your database first your multi-dimensional database then we're gonna come back and work on the data mining portion of it so what we're often looking for is to find the to use the old donald rumsfeld quote for those of you who would know who he is you're looking for the known unknowns right what are the not so obvious attributes of people who are a bad credit risk like maybe in our data mining algorithms we want to stop marketing to people who are a bad credit risk and we need to be able to know what the attributes are are they generally married are they generally in between a certain age group are they generally have they usually completed university right what's the common patterns what are the things that are common for people who are a bad credit risk we're looking for the known unknowns we have this information in the database we may not be able to quickly identify that what we're looking for with data mining we're looking for the key influencers this is a good term a big term here and data mining here right if we could identify what those common attributes are of people with a bad credit risk then we could stop marketing to them or we could start marketing to them if we think that our product can help them for example now data mining will allow us to do predictions okay so that's a big part of working with data mining is to do forecasting okay we can run scenarios through data mining we can change a few things tweak it up run a scenario will this improve our bottom line Hey okay now I'm gonna change this little one variable let's run a new scenario ha that's what we need to do right sales forecasting any sort of forecasting can greatly be improved by doing data mining okay now the SSAS product features data mining as a core we are going to have an entire chapter on what data mining is in more detail how to do your own data mining how to do this how to integrate it with integration services we're gonna play with this in depth right so this is just consider this particular video the 10 minute introduction to what data mining is you want more information about that we're gonna come back in chapter 11 so really in between chapters 3 and 10 it's all about installing analysis services building your first database how to do cubes how to design your dimensions and measures how to publish this how to get it going with SharePoint or the various client tools reporting services and in Chapter 11 got everything Bill has come back and now talk about how data mining works so I'll see you in the next video you 
37fnf5Ec3Vg,27,This video briefs about various functionalities of data mining system.,2020-09-06T14:03:45Z,What does Data Mining System Performs- Data Mining Functionalities,https://i.ytimg.com/vi/37fnf5Ec3Vg/hqdefault.jpg,Florence Programmingz,PT14M48S,false,1204,N/A,N/A,0,N/A,[Laughter] [Music] hi friends welcome to florence programming in this video i am going to explain you about different functionalities of data mining system so data mining system is a tool which provide a lot of functionality to mine our data in the data virus in my previous video i have given what is data mining and what are different data can be mined in data mining systems so here i am going to explain you different functionalities of data mining systems i have listed six main functionalities of data mining systems that is concept class description mining frequent patterns classification and prediction cluster analysis outlier analysis and evolution analysis these are the six main functionalities we can perform on data mining system let me see one by one the first functionality is concept class description that is the concept or class description of data object in the data warehouse will lead to uncover characterization and discrimination of the data in the data variables so here if you take a student management data warehouse so here the student register number is one attribute that is nothing but the class here so every attribute has its own characteristics and own descriptions so how we are using the class concept and the class description for our mining purpose this is the first one so these class concept and this descriptions will help us to get characterize the data as well as discriminate the data so characterization of data object is nothing but we can summarize the data in the data virus suppose i want to find uh how many sales details i want to find so for a month what is the total sales for a particular product so here i am applying summarization operations that is roll up olap operations roll up operations we will perform so we are summarizing the class descriptions so the class descriptions help us to find the characterization of particular data object then discrimination discrimination is nothing but we can perform certain comparison studies on data object suppose for a particular product the sales rate is 10 percentage increase now i want to compare between products how many products sales rate has been increased and how many has been decreases so that comparison can be performed using the class concept or class descript descriptions so the first functionality here we are finding the characterization and discrimination of data object next how will you represent the result to the end user so the result we can represent either by pie chart or bar chart or curves which will be easily easy to understand by the end users the next functionality of data mining is mining frequent patterns associations and correlations so mining frequent patterns means some patterns or some model in the data virus will be frequently accessed or frequently mined by different users such pattern will help us to find different associations and different correlations so that is the second functionality of data mining see one example i can give here mining frequent button suppose i am searching some information in the google search engine so when i search it will list some set of results along with that you can see related search or you may also search like that so this is frequent pattern so along with our result we are getting another set of informations which will be frequently searched by other users so the mining frequent pattern will helps to collect associations and it will helps to correlate different attributes in a data virus so that mining frequent pattern will provide or will help us to uncover different associations and different correlations in the data were awesome so here we can use the mining frequent pattern we can represent our result to the end user through some association rules or through decision trees using that we can represent the results so the main purpose or main use of mining frequent pattern is we can get different association among attributes in the data virus suppose if you are taking online sales the customers people will buy computer the people will also buy printer or they also by scanner so these are all some association among attributes so how we are getting these associations so through the frequent pattern how many customers they are frequently purchased these set of items for their shopping so from that shopping data we are getting this frequent pattern so through that frequent pattern we are creating the association so also we are creating the correlations among the data so so this is the mining frequent patented associations and correlations and the third functionality of data mining is classification and predictions this is a very very important and mostly used functionality in day germany wherever if you are seeing any research people will use the classification and the predictions so the classification will help us to predict some new pattern or some new model so here classification means the data object in the data various will be classified based that classifications will help to predict or apply for unknown class labels so classification will lead for predictions suppose you have some set of images crop images so the crop images i can classify the crop images based on some patterns or based on some rules so once i classify all the crop images and i can predict from the classifications suppose the crop tomorrow image i have and another crop image brinjal images so i can apply some rule to classify the images so once i classify i can apply that images for my predictions so classifications will lead us for prediction so these classification will predict category labels so classifications also we can apply for numerical classification and categorical classifications but once we use the classifications with unknown label we can apply this for category based predictions next how will you represent the result of the classifications again we can use some set of methods visualization that is we can use if then rules decision trees mathematical formula or neural networks suppose we have some set of data here i am classifying the data as class a class b classy for this classification i have taken the attribute is age and income attributes so if age is belongs to youth as well as income is high i can classify such data as class a and age is youth and income is low i can classify b then age is middle age i can classify c then age is senior i can classify as so three classes class a class b class now i can predict here how many customers are eligible to apply for loan or eligible to avoid loan so these classification help us for predictions so the classification result we can give using if then rules and another one we can represent the classification result using decision tree like this or we can use neural network models so neural network model here you can use input input model input layer and this is output layer inside we have hidden layers here we can apply all our rules and we can predict these so these are some of the representations we can represent the classification result to the users and the next functionality we can see is cluster analysis so classification and cluster both are very similar but cluster analysis will be used we we are applying on data object without consulting the known class label that is the difference between clustering and classification classifications we will apply on known class label but here we we are not using the known class label so just we are not consulting a node class label so clustering just we can grouping the data from the groups we can predict the uh predict our results or we can apply the clustering for predictions so here in cluster analysis we are using two main principles the one is our cluster should maximize the intro class similarity and minimize the inter class similarity that is the data objects in the clusters should uh follow these two principles that is maximizing the intra class similarity that is let us consider if you have three clusters so the data object in each cluster's dimensions will must will have the maximum value but inside the cluster the data object will have the minimum similarities that is the similarity should be so all the data object in the clusters are close to each other but between cluster is it has more distance so this is cluster analysis and the next functionality in data mining is outlier analysis so outlier analysis means see we are maintaining some set of data here some data will be some abnormal some exceptional data but that data also we can use for our analysis purpose that is nothing but outlier analysis data will not fall into the regular behavior it will behave something different so such data can be used for our analysis purpose some example fraudulent uh deduction we can use such data see credit card fraudulent fraudulent mechanisms or network intruders if you want to find see network analysis you are using we are having set of ip addresses so we know the ip address will have the certain format here certain classes but in some time we are getting some new ip address into our network so that is nothing but outlier this is exception that ip address we can suspect that it may be in ruda so outlier analysis can be used for some abnormal data analysis or for like that fraudulent analysis or some uh detecting the purchase of extremely large amount of given number so that we can use the credit card has been used by some other persons so this is something exception and something different from our regular activities so this is outlayer analysis and the next functionality is evolution analysis evolution analysis means we have a long period data so that data can be applied for analysis and for prediction of next long term uh period or even next long period analysis that is evolution analysis so suppose we have last 10 years weather data so using that last 10 year weather data how the next year or next two years how the weather will be so these evolution analysis can also used by data mining so these are some of the mainly used data mining functionality for our research purpose so most of researchers they are using these six functionality of data mining for their research for classification for clustering and for predictions they are using hope you understood the different functionality of data mining system thank you 
oZZujcUXm_o,22,,2020-08-19T06:47:34Z,MKT5084 Lecture 9 Consumer Analytics Big Data and Data Mining,https://i.ytimg.com/vi/oZZujcUXm_o/hqdefault.jpg,Derek Ong,PT31M9S,false,28,2,0,0,1,hi this is lecture 9 of marketing analytics and for this topic we are going to look into big data analytics with a concentration on what is big data data mining and artificial intelligence now the learning outcomes for this lecture are we want to define what big data is and the challenges and the opportunities within the sphere of marketing analytics we want to understand how big data sources is stored and the difference between relational and non-relational databases and we want to understand the application of artificial intelligence to support data mining techniques so big data in essence is basically an extremely large data sets that may be analyzed computationally to reveal patterns trends and association especially relating to human behavior interactions and what this means for marketing analytics is that we are able to bring together lots of data sources to understand how consumer behavior is used for creating better marketing strategies so the problem is in the current situation that we have way too much large amounts of data and from many many different sources there is also a very inefficient use of this data to bring together and to even analyze it for some some uh to create models that are predictive and also useful when the models are created in malos cases there is a very ineffective translation of this analysis towards perfect towards uh effective marketing strategy that works so what is the goal of data mining we are trying to accurately report on the past with the data that we have uh uh gathered together so that we can analyze what's happening in the present to create models to predict the future better so we want to bring together many different data sources using many different technology and techniques so that we can use different measurement tools to measure the effectiveness of marketing strategy as well as create models to effectively predict the future now big data sources can come from very very uh different types of sources but big data is so much that it is sheer size of it is voluminous the the there is such a speed in which the data is generated so many variety there is the accuracy of the data in some cases are very accurate but in some cases they are also very hard to define what's accurate some of the data can be valuable but some data can be seen as junk in terms of validity some data needs better quality management there's so much of dynamism in the data in terms of its variability where the data is stored from multiple platforms and its vocabulary and also conventional things as well so where does these data sources come from these data sources can come from a customer database or in some sense it can also come from the actual behavioral transactions that the customer does when they transact with the company but you can also collect data from customer perceptions through surveys and interviews that you conduct periodically from the company as well as social media social mentions and trends i will also want to say that data can also be collected from now a lot of variable tech like your fitbit as well as your mobile applications mobile phone applications when they use mobile app now just to let you know that big data is useless to a company unless the company finds a way to meaningful um to to uh to find a way to uh something meaningful in the data in a quick manner because there's so much of data to see through so if they don't find meaningful patterns that will work for the company then the big data is useless the company also needs to find a place to store organize and analyze these data because they come from so many different sources and it must be easily accessed by any person in the organization that needs it especially the data miners the data supports assumptions of the company that has been made before analyzing it so for example if let's say there is going to be a spike in terms of the transactions especially if there are festive seasons these assumptions need to be made and also for holiday breaks as well as other assumptions as well maybe a new product launch or taking into factors of uh current events and social events also that the data is raw and unclean if the data has been aggregated or has been tampered with in some way then big data will be useless to the company so what are some of the different technologies and techniques the technology together the data online needs to be there offline as well with certain integrated softwares like web crawlers or data scrapers as well as data warehousing face recognition mobile and wearable tech these are where all the data sources are going to come from and there must be a logical way to put all these data together and gather them in one place there is also a lot of use and analytics techniques that can be used together especially for data mining you have seen in the previous lectures that you can do segmentation within cluster analysis decision tree targeting perceptual mapping for positioning and market basket for association and there are also many other things that you can also do like logistic regression for model building and in certain later lectures you will see sentiment analysis as well as text analysis you might also want to think about data mining as integrating technology with the techniques for example artificial intelligence blockchain ai machine and deep learning and internet of things these are some things that we are going to explore in this lecture so what are these different measures so looking into ctr and ir for example click-through rates and inquiry rates using search engine optimizations from big data will give you the understanding of how consumers are reacting towards whatever content that you put out there in your website or on your social media pages you might want to also look at primarily for social media analyzers or social media sentiment analysis analyzers for the text you might also want to look at higher statistical and data mining analysis so that if you have the conventional regression that doesn't work you might want to think about pattern recognition in your analysis as well this is a way for the different measurements to gain deeper gaps in the marketing which channels you uh the most gives you the most effective or even the least effective reach and these are information that will help you to decide on what best to do in terms of changes of your marketing strategy and finally you must think about building a model that is test and re-test and re-learn that constantly upgrades the model so that this model is a training model that tests the efficiency of a testing model so that you can improve strategy and performance every time you rebuild a new model for predicting uh analytics so what is it that you need to do data mining uh uh efficiently well first of all you need a good analytics platform and this can come in a form of either analytics platform that does any data mining analysis for example you've learned power bi there is also sas rapidminer as well as uh modular for spss you definitely need very good and rich data sources now this rich data sources can come from very very various places and making sure that these data sources are also in some form and way is uh accurate so that you can see through it properly so social media transaction and customer put them together to create a complete story rather than just seeing things from one side only you definitely need a good top down or technological and training support management support is important if you're going to build a system to help you to do your data mining or even the storage for the data that you're going to do for data warehousing later on you definitely need the right talents people who are data scientists data miners or statisticians and a good combination of qualitative experts and digital marketers and quantitative analysis and definitely think about a good fundamental data storage and strategy now this is where i'm going to explain to you the difference between what is a relational database and non-relational database now a relational database is basically a database of tables that organizes structures of data into fields and defined columns whereas a non-relational database does not incorporate this table mode instead the data is stored in a single document file using tags and using the composites of the informations that puts them together into a group so what are some of the famous relational databases oracle mysql and amazon uh and also the um server are ones are some of these very famous relational databases now a relational database as oracle puts it is a means of storing information in in such a way that information can be retrieved from it so it is a set of tables containing data fitted into pre-defined categories and these tables might include employees customers vendors products and other transactional information that is organized in some logical manner it is used when you're working with highly structured data and all the systems runs using the same code basically using either structured query language or we can call it sql so that we can retrieve data in some structured manner now the uses of this relational database as you can see here you notice that each database table represents a particular character that is a part of the ecosystem within the database now this database holds information of customers orders or the items and products and why it's called relational is because every single primary every single table has a primary key that uniquely identifies every single row and then this i uh primary key is then linked relationally to a foreign key in another table that serves as the connection between the databases so this is how a relational database works now for relational database you must always allow the users to link the information from different tables using these keys and indexes through the use of queries so that when you use structure query language you can link up these tables and find information for multiple tables at one go and a query is basically a question that allows the users to pull information from database to answer the same question how does this help marketing analytics it helps marketing analytics professionals to target specific customers store information about certain products customers and employees and use segmentation to create groups of potential customers especially if you're trying to figure out where are the best buyers or where are the best customer segmentation and products that are being bought what are the benefits of relational database some of the benefits is that it can store large amounts of data and creates easy access to information and this information can be manipulated at any time real time it can be used to target and segment specific customers especially when you have all the tables linked up together example if you're looking at targeted emails for your best serving uh customers with the highest ranks in contrast to relational databases are non-relational databases which you can find a lot on the web these days especially on social media now these relational databases can be found on google amazon facebook but they are facing big problems like the constantly changing of data decrease in development cycles and massive amount of users therefore to have a relational database online is really not effective and that's why they have introduced the non-relational database structure so the information that is stored on these websites are not stored in tables but in documents which are created each time a new data is created so it can be grouped into four different categories key value stores document databases and white column stores and even graph databases now for key value stores what happens is this is the simplest way of looking at non-relational database it uses an associative array for example a hash table like the value as the fundamental data model where each key is associated with only one and only one value in the collection for example k one two the triple a triple b and triple c and k two to triple a and triple b so unique key to unique value a combination of values or documents the benefits of this is that it is scalable it's reliable and it's simple and it's fast another type of non-relational database is the types of document store database now this document store database uses more complex data structures known as documents for storage and queries similar to what we call the key value but this key now is paired with a document instead of a value the previous one was sped to a particular value but now it is stored to attach to a entire document instead the benefits of this is that it's schema free which means you don't have to really design or identify it using a schema and documents can have different different structures whereby any document if it's attached to a key then you know that as long as you know the key you can also retrieve the document you can rush a fast right to performance and fast queries another type of non-relational database is what we call the white column stores now this is a database that is similar to a document database that uses column oriented data structure rather than a document column data structure so the store data tables is in columns instead of rows which means that if you want to look at the information for bob you have to look at the email address the gender as well as the age together and then each of this will have its own value that is hashed together now this each row can contain a different number of columns than the other rows as well so then you will not see two different types of hash values that are the same and they are uniquely identified within the sphere of the white column store and finally if we look at a more sophisticated way of non-relational databases are what we call the graph databases now the graph databases uses a structure relational graph of interconnectivity of key value pairings instead of relational tables this consists of notes and edges relationships and properties so for example you will see the relationship between one main value to another main value and therefore you also get to see what is the relationship between those two documents or interconnected pairs of relational tables so what is the benefits of using non-relational database as opposed to relational databases well there is a lot of flexibility when we look at non-relational databases because there are many different types of databases systems now that allow marketing analysts to manage data and how they wish so for example if let's say you have data consisting of sound and text and also videos how would you then use relational databases well the best way is to not use relational databases and use non-relational databases to store them as documents the other thing is looking into really it's relatively inexpensive to create because it allows companies to store high volume of data like logs call data records meter readings and ticket snapshots and even other things as well as i just mentioned just now it is affordable and they utilize what we call as the open source software or use cloud storage whereby a company does not have to come up with their own storage and they can use google storage or even the apple storage online to store all this information if they have an account and access to it it is accessible because marketing analysts can analyze semi or even non-structured data such as email archives texts analytics and xml files like the web pages and also documents and finally the scope of non-relational databases can crunch large data sets and help machine to machine data retrieval now because of non-relational databases popularity therefore big data now becomes even more important for marketing analytics the big picture here is that relational databases are optimal when they are dealing with structured data but when you're dealing with non-structured data or semi-structured data or unstructured data you will need non-relational databases so in that respect now we can see how data mining can happen in marketing analytics because now we know that there's so many types of sources of data out there data mining can be used to analyze relational and even non-relational database data sources into marketing analytics so the process starts by sourcing and analyzing these large data sets in different different forms and different types and different places and extracting usable patterns from these data so then they combine the methods from statistics and machine learning with database management to predict behaviors and trends and lastly when the models are created it allows marketeers to take proactive and knowledge driven divisions so what are some of the applications of data mining in marketing some of which that i'm going to present here you would have already seen in your previous lectures for example promotions to identify most likely people to respond to promotional offer using decision tree direct marketing identifying prospects that most likely will respond to a direct marketing campaign using the recency frequency and monetary analysis interactive marketing to predict what web pages and individual uh accessing a website will most likely be interested in viewing this is more like sequence analysis or like a click-through analysis and you want to look at what a consumer does online or a consumer behavior online to a web page we can also look at market basket analysis to determine what products or services are commonly purchased together within a purchase bundle we might also want to look at churn analysis like for example using logistic regression to identify consumers who are likely to drop a product or service and to shift to a competitor and lastly we can also look at fraud detection using regression identify which transactions are most likely to be for drilling transactions and to see if there are any mis-occurrence or out of the ordinary transactions especially when you look at the beta regressions which are abnormally high or abnormally different from the regular times now we're going to look at some of the techniques as i mentioned to you that facilitates data mining in its respect especially in the age of big data the first one i want to introduce to you is artificial intelligence or ai as it's commonly known ai is basically a simulation of human intelligence processes by computers it starts by learning acquisitioning of information or the acquisition of information and then creates rules for using the information and then it also works on reasoning after these rules have been created it applies those rules to reach the probable conclusion kind of like an if-else statement if you're doing programming so if something happens what will happen so one thing a good about artificial intelligence is that these rules can be self-correcting if you set the rules correctly then as it learns along through the process it will also change the rules and new set of protocols can be derived from new information and data the other thing i want to tell you and share with you is machine learning and machine learning is basically a branch of artificial intelligence that is concerned with the development of systems that can learn from empirical data now machine learning notes that systems learn to recognize complex patterns and performs tasks based on these capabilities it's kind of like doing your cluster analysis where you can redo your cluster analysis again and again and again when new data is added to the analysis you can train to classify different types of transactions for example identify fraudulent transactions or emails and distinguish what is spam and what is not the other way of looking at it is looking at two different types of machine learning optical character recognition or we call it ocr which identifies symbols and meanings and also natural language processing which you will learn in your sentiment analysis and text analysis lecture later on which we predominantly call nlp for short another branch is what we call as deep learning aka the deep structured learning or hierarchical learning now deep learning is the branch of machine learning where knowledge is structured as a form of a neural network connected to each other through a sense of directions and also connections just like your graphs the algorithms within the deep learning learn as they sift deeper down the layers of the network progressively accumulating and triangulating knowledge that is gleaned from notes into the outer layers so basically it tells you the interconnectivity of all the information and gives you what we call as a web graph to show you what is concerned with what and maybe to even tell you the strength of these connections as well and finally is pattern recognition now pattern recognition you've seen quite a lot in terms of correspondence analysis as well as logistics regression and also cluster analysis this is a set of machine learning techniques that classifies raw data according to a specific logic or learning procedure there are two types of learning which is unsupervised and supervised for the supervised learning it uses training data to attempt to perfor uh to learn and generate a model that attempts to perform as well as possible on the training data and generalizes it as well as possible to the new data in unsupervised learning however it works without a pre-label training data to find inherent patterns that is in the data that can be used to classify data instances just like clustering does finally another one that is very important to note when it comes to machine learning or ai is neural network this is a non-linear predictive model that learns through the training and resembles a biological neural network of a brain or structure it uses it can be used for pattern recognition and also most importantly optimization of this pattern the application of this can be looked into computer vision and speech recognition whereby you have a lot of information fed in the more information that's fed into the neural network the more it tweaks the network until the data or the model becomes more accurate the classification of customers example like identifying which are high risk high value customers or even the classification of transaction for example which are fraudulent insurance claims finally we have the natural language processing this deals with the ability of computers to understand what are natural languages in big data enabling computers to derive meaning from human or natural language input the application of this you will see in the the last lecture in terms of sentiment analysis on the web to determine how people feel about a particular subject example a brand or a company or even an individual now a sentiment analysis in short basically is the usage of the natural language processor and other analytics techniques to identify and extract subjective information from text and textual content such as the consumers generated media or their perception or their comments about a certain brand or even a certain event or person this determines the polarity of these comments whether they are favorable neutral or even their negative comments on the content and assesses the intensity of this sentiment net advocacy index is one of the examples of how sentiment analysis is used to assess how netizens feel about breath so what are the key takeaways of this lecture big data poses its own challenge because there is a large pool of data and there is inefficient analysis strategy but also its opportunities are vast whereby if we can have a way to understand this rich understanding of data on consumers and behavior then we will be able to come up with good marketing models that will meet the criteria and the needs of the consumers better the fundamentals of the big data is its sources and storage there must be a way to find to collect all this information from different sources and also what is the best way to store the data storage which you see the differences between relational and non-relational databases and finally artificial intelligence with all the different deep learning machine learning natural pro language processing and also neural networks drives marketing analytics and other high-level analysis to understand richer to for under for richer understanding and analysis of consumer data the tutorial for this particular lecture i would like you to search online and list different artificial applications in different social and different social media analytics especially when it comes to marketing analytics and marketing strategy analysis i would like you to compare and contrast on the capabilities of each of these methods and how they can be combined to produce a richer data analysis next i would like you to watch the video on digital transformation especially the one on measurement so that you get a sense of how data analytics is used especially online for big data additionally do read up on this particular article on brand marketing big data and social innovation as future research directions for engagement then from this article list down the ways that big data facilitates brand marketing and social innovation and identify examples from other sources for each item of big brand marketing or whatever social innovation examples that you can find from other sources on the internet this concludes the lecture for for for big data the next lecture we are going to break down into two parts where we're looking into data driven and digital marketing analytics and we're also going to look at online digital marketing and digital marketing analytics especially concentration on social media thank you very much 
RzXojsyYbXQ,27,"This short course will help you to understand some data mining techniques for knowledge discovery and knowledge presentation. At the end of the short course you should be able to use the skills for knowledge discovery and future prediction from a suitable dataset of your interest.

This course is coordinated by Associate Professor Zahid Islam and Dr Michael Bewong. This first hour is presented by Associate Professor Islam - you can find more videos here - https://www.youtube.com/channel/UCrcz90CHdK6GrtnMvk_vVXA/videos?view_as=subscriber 

This short course is also a taster of the Graduate Certificate in Applied Data Science course at CSU - https://study.csu.edu.au/courses/technology-computing-maths/graduate-certificate-applied-data-science",2019-11-06T05:57:45Z,Free Short Course: Knowledge Discovery and Data Mining - Webinar 1,https://i.ytimg.com/vi/RzXojsyYbXQ/hqdefault.jpg,ITMastersCSU,PT1H13M58S,false,1855,19,1,0,2,"hello everyone and welcome to the knowledge discovery and data mining short course president and buyer Charles Sturt University my name is guy coward and I'll be your emcee before we begin some just just a little bit of housekeeping all webinars for this course will be held at 7:30 p.m. Australian Eastern Daylight Savings Time with recordings made for those who cannot attend despite the recordings if you can make it we hope you'll attend the live webinars and contribute to a collaborative learning environment one of the great advantages of joining live is the ability to ask questions of the course developers and chat with other attendees during the webinar we ask that you direct all course or call questions relevant to course content to the Q&A section and you send all administration type questions dates and times and resource availability so the support team in chat and you can look at the icons at the bottom of your zoom window to to choose as I said you can chat with your fellow students as well and you can make that choice by toggling through the drop drop box once you open the chat log and setting your chat to all panelists and attendees there are usually some very experienced attendees who can walk into lecture as we go along with their personal stories we'll have Q&A sessions periodically and throughout the course but if the question is particularly relevant I'll interrupt during the lecture for those who have never taken part in a short course with us lighting masters is a training organization that exists as partner disease Charles State University Charles Sturt is providing this short course to showcase their graduate certificate in Applied data science but with that said we hope this course stands alone in its own right and is its own reward we want you to learn some useful information have some fun and hopefully make connections with some of your fellow students today over 2,000 people have been rolled in this short course so you know spoilt for choice just to who to talk to life is here in an administrative and technical support role for IT masters and he is also responsible for the course page or the Moodle page which is where you'll find for the other materials needed for this course links to study guides and meetings and discussion forums in any exercises if you have any questions please feel free to contact us using the details on that page well begin tonight with dr. Jason Harris who will provide a little context of this short course by detailing the grant said it was built out of Jason is helping to drive the transformation of online education at Shell ster which is appropriate given in completed a PhD on MOOCs he's been a great friend of IT masters short course program for many years nyan so hello Jason and thanks for coming I'll introduce a short Costco developer so hate Islam thanks so much go out that and everyone hear me just quickly it's my voice coming through great so thanks guy for the wonderful introduction welcome everyone to our short course tonight I will be brief with my introduction to the overarching program where this short course comes from a Charles to University I just want to spend five minutes talking about the graduate certificate in Applied data science which is offered as a new course from next year at Charleston University and just briefly explain how the short course that you're doing tonight fits in with this overarching program I will at the end of the short course series for those who are interested come on as the very last speaker to tell you more about the program and if you do want to learn more about how to enroll I can give that information to you then some really quick overview information on Charles Sturt University where a regional Australian accredited university with campuses all over New South Wales particularly in regional areas but predominantly Charles Sturt is an online teaching University so we have around about 44,000 students but our online cohort our totals 25,000 so most of our students are online which makes us a great University for people who are working who have families and other commitments so many of our students do study online and part-time one of the courses that we're offering and which obviously relates to the short course in data mining presented by Zahid commencing tonight is our new graduate certificate in applied data science so this course kicks off next year it's a four subject course only four subjects to get the award it is a postgraduate level program and you can see briefly there that it has one core subject found a of big data analytics which is all about the practical concepts behind big data and then you get to choose three electives one of those electives ITC 573 data and knowledge engineering is related to the short course you're studying tonight it's not the full subject obviously we're sort of sampling our concepts from the full subject the first few topics giving an overview of that material hopefully wedding your interest and potentially coming on if you like the idea of studying a graduate certificate with CSU or Charles Sturt so if you're wondering what a graduate certificate is and where it fits into the overall academic hierarchy it's a postgraduate award which means it's higher than undergraduate and it's nothing to our masters and it's certainly easy to complete a graduate certificate and then to move into the masters and potentially get credit for the units that you've done so another quick point to mention is you don't necessarily need an undergraduate degree to get into the graduate certificate so if you have industry experience it could be in different areas let us know if you are keen to study the graduate certificate because very possibly there is an entrance pathway for you all of our subjects in the course delivered online and so you don't need to set foot on campus it's all delivered through our learning management system everything is transacted online there's no disadvantage there as I said we don't even have on-campus students for this particular program and you'll be studying with like-minded people and are able to do it part-time and in a way that allows you to work around your professional and personal life so with that I will finish up I don't want to spend too long tonight detracting from tsar heads presentation if you do have any questions that you would like to ask either sabhi riemann who is the course director for this program these email addresses they're on screen you can also email me always happy to hear from potential students if you have any any questions whatsoever about the program or about Charles Sturt and as I said at the very end of the lecture series in a few weeks from now I will come back give a bit more detail about the program for those of you who might be interested in enrolling so enjoy your short course and I will hand back to guy and Zahid thank you very much Jason um what are you Jason just one quick question from that will there be a master's in science offered at the moment it's it's not there we're keen to test the waters in terms of how popular the graduate certificate is I suspect very likely whoo we will additionally have a master's it's going to depend on the demand for this course but given the demand in in industry for people able to manipulate and make sense of huge amounts of data I suspect the Masters is not too far away fantastic thank you alrighty associate professor as I hate Islam has been waiting very patiently and is director of the data science research unit at gel State University he's published 100 peer-reviewed papers you know I can't think of anyone more qualified to start this short course off even as I was chair of the 16th Australasian data mining conferences last year could you please join me in thanking associate professors I hate Islam running this short course in the chat box hello Sayid hello guy can you hear me now sorry I did I was trying to figure out how to unmute yep thank you very much and thanks dr. Jason Howard for a nice introduction to the overall course and guy mr. Geico thank you very much for introducing us and thanks to everyone who are participating I know your time is very valuable I hope you will be enjoying this short course we will try to introduce some basic ideas of knowledge discovery and data mining and I will also try to show you some software tools and techniques how to do some real-life data mining on some data sets through this short short course and in this subject actually when you if if someone studies the subject the full subject then there will be some theoretical understanding as well theoretical part as well so that you just do not know how to do things you also know why things are working this way something like that so let us start this short course my name is ahead islam and i will be presenting the first two weeks of the lectures for this mooc and then my colleague my wonderful colleague dr. michael beiong will give will tell if at the last two lectures for this MOOC he is a wonderful data miner okay so now the next slide is about myself so as you know my name is ahed islam and my area of interest research area is data mining I work on various types of data mining various tasks of data mining knowledge discovery classification we develop our algorithms so you know like generally data miners can be classified into different groups I have my experience one group is applied data miners like they generally work on problems and they apply exist in data mining techniques whatever technique that would be to solve that problem right so that is a very valuable type of research whereas another group and I possibly fall slightly on this the second group also develops own data mining algorithms to probably to solve some that data mining problems and then they can apply those algorithms so I possibly fall in that group we develops a classification algorithms and we possibly identify some challenging cases for example data set with some challenging properties and then how do we actually develop new algorithms to handle those kind of situations that sort of thing anyway so I will introduce the data science what is data science what is data mining etc today and in a in a minute and yeah we have a nice data science research unit here which is a faculty level Center at the moment and I'm the director of that Center we have many colleagues here in data science we do practical work and yeah you can actually learn a little bit more about me from this web link and I have a YouTube channel as well you can find quite quite some YouTube videos on various data mining tools and techniques and ideas and discussions so feel free to jump in and watch them at you at your free time if you like now my other colleague that will present the next to the last two lectures is dr. Michael being a wonderful data miner he his PhD was in privacy-preserving data mining on privacy side of the data mining as far as I remember and this is his webpage and you can actually see his details there as well Michael will introduce himself when he comes on the third lecture now as I said before the structure of this lecture today will be first I will discuss what is data mining what is data science why it is important and some basic concepts very basic concept about this data mining and then we will show some real data mining actions okay how to do this that sort of thing the introduction of the data mining we'll also talk about the data lifecycle you know that there is a life cycle for for data science and this can be one way to explain the life cycle it may start with data collection so first you need to collect data and generally people collect data through various different ways for example you can actually run a survey and survey participants may provide their information through survey and that way you can collect data sometimes people may collect data through sensors say weather stations on forms so these weather stations may send various with a related information like temperature humidity precipitation net radiation etc so those kind of things can be collected on farm and sent to the central server people may collect data through that way or you may actually collect data through wearable sensors and so on and so forth so that's data collection once the data are collected first first thing the second task will be data pre-processing and cleansing right so here generally your data may be signals time series signals or your data can be images so you need to convert the data into a two dimensional table sort of thing for data mining techniques to work generally and that's one step one task of the pre-processing so they're sort of pre-processing thing you need to do some time your data can be time series data and then you generally data scientists will convert time series data into a kind of an on time series two-dimensional table by capturing information time series information in a smart way so that is also part of data pre-processing sometime your data set when you collected the data could have missing values some values can go missing say for example you are asking you questions through a survey and a survey participant simply did not answer one of those questions or some of those questions so those value will go missing right or let's give you another example if you are collecting data through sensors say weather stations and your sensors may go may have a flat battery for some reason and it did not collect data for some time so for whatever reason you may have some values missing so that those missing value may need to be imputed for better data mining accuracy etc the main purpose of data mining is to capture the overall knowledge from a data set the making sense of a data set so that you can learn about the data set and then possibly use it for future prediction for strategy strategic decision-making for business enhancement business profit etcetera or better service provide providing better service so these are the reasons possibly and then your data set can also have quorum fellows some values can be incorrect right so then is there any way you can detect those incorrect values automatically say for example someone was asked to how to do to enter their weight body weight and the expectation was in kilogram maybe he provides he provides the information in pounds a hundred and twenty pounds whereas the expectation was in kilograms or somebody may may say he is 1.6 tall in meter 1.6 meter but the question was in feet so that sort of thing so if you if your data may may have incorrect values due to various reasons and those incorrect values may disturb your knowledge discovery and overall pattern discovery etcetera so you may sometimes need to rectify correct those incorrect values so these things generally are considered considered as data pre-processing so after you do the data pre-processing your data set is generally ready clean data pre-processing is also called data cleansing and then after data pre-processing and cleansing your data set generally gets ready for data analysis and in this step you may apply various data mining techniques for data analysis like stealth traditional statistical approaches can also be used along with these data science is also a kind of a creative creative area where some people makes many professors I have have seen they discuss in a in a in a funny way that is it data science or data arts because you need to be really very creative while discovering knowledge there is no one single kind of way of doing it for example you can apply a decision tree algorithm on a data set to discover the first set of knowledge from it and then from each of the part of the decision tree you may explore further you may make some queries and to discover knowledge you can do some clustering along with the decision tree etc so we will discuss those things in this in this short course one by one slowly and this thing these things will make better sense slowly but at the moment I think all we are trying to discuss here is that the first step is data collection then we do data pre-processing and after data pre-processing the data set is ready for analysis and after we do the analysis we generally discover some knowledge from the data set right so we generally want to see what it means actually and then after knowledge is being discovered then this is published knowledge is conveyed the message is conveyed we got to we get to remember here that look we are data scientists but our often our managers our end users may not be data scientists so we need to speak in their language okay so if we be if for example if we build a decision support system then the decision support system should display the information in a user-friendly way so that they understand what we mean and then after that knowledge is discovered knowledge is published our knowledge may reintegrate it so we learn few new things and that may require us to collect more data something like that so this is the life cycle now why is that a mining so important maybe I'm sure many of you already work in data mining many of you are maybe very experienced in data mining I have no doubt about that but some of you are possibly may more focused on a different area and may be exploring what this data science actually does so for for for a discussion purpose like what is the usefulness of data mining loop data mining is being used everywhere these days so almost every sector of life these days collect data right so imagine any any organization will be collecting data mostly almost and then what is the purpose of the collection of the data if they don't analyze it if they don't use that data for their business improvement and a profit improvement or etc so here I will give a few example on lis of some of the industry projects that we have done so far I thought this could be one very nice way to explain the importance or usefulness of data mining this that I thought like through practical example maybe I can convey that message very clearly ok so I will be very brief say one of the project the first one is health monitoring system this project was funded by New South Wales health Muhammadu local health history and in this project we developed a system that collects the data from all the hospitals and etc and then we applied our own data mining techniques our own means CSU developed data mining algorithms to discover knowledge from from that collected data and all these things are happening automatically in the back end in the background and then it discovers the knowledge from the collected data and those knowledge that knowledge is actually converted in a way user friendly way through graphs figures and if-else rules something like that so that clinicians nurses doctors and hospital managers they can understand the information and so that they can intervene or they can take action to improve the hospital service quality etc something like that so that's that sort of example that's an example of how why data mining can be useful and I'm very happy to share with you that that project after we finished it actually received an Innovation Award from the agency for clinical innovation in 2017 so you can see the value of data mining in health management now the second example here here is Kali M Lee irrigation system this was a this was an industry funded project as well funded by : irrigation area where again we developed a system that that collected data from the farms so in column B irrigation area there were around 500 farms from my memory if I can remember it correctly and those forms that our system was developed in order to predict the irrigation water demand for next seven days of for every individual farm so the farms farmers could order water and then they used to have a SCADA system and the irrigation company used to release the water for that farm but sometimes it could take up to seven days to reach the water after a farmer ordered the water so as a result they needed some sort of prediction system so they needed to know in advance how much water they will need to irrigate after seven days or so something like that so that's their system was done for that purpose good can we predict what a water demand for every individual farm we considered the farm soil type farm crop type cropping stage average temperature minimum temperature and maximum temperature weather etc so that system was also very useful we have couple of publications there you can find my papers on my web page and for most of my papers the preprint versions are available freely downloadable feel free to have a look if you want then the third one look the reason I am discussing these into industry projects just to possibly share with you that data mining can help in many different areas right the first one was health second one was irrigation the third one is was funded by Department of Social Services and this was in partnership with live better see SEO team where I was one of the team member as well for all of these projects and the project was around six hundred fifty thousand dollar project what we did from data science the project was multidisciplinary in this case what we did from the data science point of view is we collected information about the seniors elderly people in in regional Australia in our study area and then we try to find out the reasons for their well-being for their aging how we can improve their well-being how we can improve their social life etcetera and then we discovered many interesting patterns interesting knowledge that we could then we suggested the Department of Social Services through live better and live better as well so that they can intervene the senior people in the in the region to improve their life quality and I'm again very happy to share with you that this project was then assessed by Deloitte by the Department of Social Services they engage Deloitte to assess the project and it received an outstanding feedback from the assessment team then the fourth one was Hobart district nursing system so this was funded by the Hobart district nurses nurse nursing district and here we try to see the caregivers quality of quality of service and can we improve it so we had we came up with some ideas it again a multidisciplinary project but data science part of the project was to assess the the impact or the usefulness of the intervention plan and in terms of the improvement of service quality something like that and the fifth one I guess the last one here in this example list is sunrise and Agri futures funded project so through food agility crc here we are trying to understand the reasons of whole grain yields rice grain when did when they produce rice grains some of the rice grains can get broken so broken rice grains if a farmer actually produces lots of broken rice grain so then he actually does not get good price for his size okay so in this project we are trying this is the naan this is a current project we are trying to understand the reasons influencing factors okay it could be weather factors it could be farming practice it could be rice trait so what sort of rice it could be soil type it could be other what are the other reasons that influence it could be milling processor then what are the things that actually increase the break rice breakage or can decrease or increase the whole grain yield so and if if we can learn that then possibly we can share that with the farmers for a future farming year that and we can suggest them to do things slightly differently right so that they can improve they can increase their whole grain yield possibilities so we can actually guide their them as well given given a particular season we cannot control temperature we cannot control maybe rainfall but we can't possibly control farming practices or we can also at the very least we can predict how much whole grain yield they can expect so that they can make more informed decision so you can see that data mining can be used in white very variety of areas and there are lots of lots of other stories I can share with you I am mindful of the time but but just to let you know that for example Walmart in the United States they have actually they have their Walmart has shops in maybe it's a it's a supermarket chain it has in nine different countries they have their existence in nine different countries over maybe 3,000 shops so they collect each and every transaction of sales transaction and their mind those data so it is it's it's very well known that everybody uses the data collects the data and analyzes the data these days you know perhaps ahead we could we could collect one more set and just before we go to the next slide we were talking really about finding yet what your level of experience is or it's interesting to find out about the students so I launched a poll now yep what is your level of experience using data mining machine learning AI and knowledge discovery and if you'd like to talk so he'll about how that fits in your little flow chart there that'll be really interesting that's right imple data collection that's right it's very simple but very effective data collection okay so this is possibly what guy is doing is another kind of a quick lesson for us that for data collection data mining data knowledge discovery my personal experiences all we need to be sensible we need to be smart it doesn't always need to be complicated it's it's nothing nothing to make it unnecessarily complicated if simple things were for example here I can get a very good understanding of the cohort here of the audience here and I'm very not happy to see that the enthusiasm of the participants yeah so a lot of lot of the lot of the participants have less than one year data science related thing the and and some of them have more than five years event so quite experience data scientists yeah through that now we had about 75 percent people respond 72 brand-new are still learning on this one one year you know 24 percent one two three he so a good of experience taken in that 5% trader it's 4% I should say three to five years and one percent more than five so that's an example of some data collection right there and hopefully they will feed that into integrating the knowledge and maybe changing what we do with the course that's right this is very useful thank you guy so shall I move to the next next slide no okay okay so I was then so now let's let's discuss what a data set is so this is I mean whenever we write papers etc this is one one question that I have seen that people get generally confused because in data mining generally data set means a two-dimensional table like this right here rows are individuals or records and columns are the attributes or some or features something like that you can consider the first row is the first individual where columns are age sex so information about that individual and one of these features or attributes can be the target attribute like whether what is the diagnosis so let's do we are assuming that this is possibly information about some patients and their occupation their life pattern exercise how many hours their smoker or nonsmoker they drink or not as such a how much they drink and finally they have a particular disease or not this is just a toy example don't worry too much I mean this is the maybe this does not make sense or make sense but generally datasets will look like this a two-dimensional table and then you can apply data mining techniques on it but but the source of this two-dimensional table sometime can be signals like this okay so for example you can have a EEG signal from a patient's brain and the signal may loop like this our or the source of the data set could be images like you have the image of Michael beiong and Zahid Islam and you can actually have lots of loss of images of Sahid and Michael and then maybe you want to develop a technique that can identify the height and Michael from then on I mean this is just a toy example again in real life maybe people will be possibly a better example could be like there will be lots of images about shapes cows kangaroos and and whatnot goat and dogs and can-can the system automatically identify kangaroos maybe for an on-farm activity so that your say for example you are sending drones drones are taking they're taking pictures and can it identify kangaroo presence something like that so data set source could be images data set source could be surveys as I explained before data set source could be wearable devices these devices these days like wrist wrist sensors like wristwatch they can actually collect whole lot of data Kelvin all even what they could they make they mean like your skin temperature your your sweating level your body temperature your heartbeat your blood pressure etc and those data can be used for various purposes for example to assess your mood to you know like to see like your valence and arousal so how how stressed you are how frustrated you are those kind of things can be also done and that data collection can be done through smartphones as well today just today this morning I uploaded a video on my youtube channel just a coincidence on the use of smartphones for data mining these smartphones these days are eight core processors and two gig ram which was a quite a dream for even for a powerful laptop field few years ago data collection can happen through weather stations on farm weather stations etc now I think we can we I'm mindful of the time so I mean it's good to discuss it better rather than rush rushing it my purpose today tonight will not be to cover all the slides in a in a rushed way so in so in a way that people did not enjoy or did not like anything rather we go slow but we better discuss things better here this slide is not very important at this stage all I wanted to say that when you collect a data like a true table some of the attributes or features can be numerical numerical means numbers integers 1 2 3 4 they have natural ordering in it you can many most of the time you can apply mathematical operations on them some of the variables can be categorical like Sydney Melbourne Brisbane you cannot apply mathematical operations or there is no way you can say Sydney - Melbourne is equal to Brisbane or Sydney is greater than Brisbane something like that when I mean Zahid is greater than look when whenever you are saying Sydney's greater than Brisbane somebody may say yes that makes sense because they are thinking of population they're thinking of size that those are numerical attributes but just the word Sydney is not greater or smaller than Brisbane and that as it can be ordinal ordinal ordinal attributes are like categorical but they have a natural ordering like tiny small average so this is just a just a you know brief idea so yeah I we understand that variable variables features attributes synonymous things so they can be numerical categorical etc then here is a good good now we are getting into more interesting topics so data pre-processing as I have introduced that in your data lifecycle you have seen that one one of the tasks is data pre-processing missing value imputation I will give you an example here now you see the first table there on the left you can see let's assume that this is the original data set this is the pure pure data set but let's say while collecting this pure data set we have collected the table in the bottom okay bottom table we have collected the bottom table where two of the values are highlighted in red formed you can see so here the for the first participant hours exercise per week is 95 so we can clearly see that this is impossible nobody can exercise 95 hours every week so something must be wrong there and actually when you look at the table on the top yeah the hours exercise was 5 so there was a mistake maybe an additional line entered by by a data entry person or whatever so this is what we call corrupt data so in a big data set you cannot manually identify all these corrupt data so there's you often need some automatic technique and here also you can see a caution mark that means the value is missing the actual value was 8 but in this case the value is missing so that's how do you impute that missing value that's called missing value imputation core of data detection these are data pre-processing tasks other data proposing tasks may include discretization feature scaling etc so yeah let's let's see an example of missing value imputation and you may want to install after the lecture if you like if you want to play with this you may want to install Weka how do you install Waker Weka is a freely downloadable software I can see many of you have data science experience already if you have that if you have used it before no problem if you haven't it's very simple all you need to do is just write Weka on your friend Google and your friend Google will identify the link and you just download this software it's a freely downloadable software and if you are still confused or if you are you are struggling there are lots of YouTube videos and you can also visit my youtube channel and on my youtube channel there are few videos like these red color videos data mining software decision tree software these videos any video with the software at the end something like that missing value free software so these videos generally also demonstrate ha or dis or show or discuss how to download Weka and how to install Weka and how to install our algorithm CSU developed algorithms into Weka there are quite a few CSU develop algorithms freely downloadable you can use them now let's imagine this is a this is a data set and let's imagine when you have collected the data set these two values are missing so the original values was where PhD and 85 so the first record the first person is has a qualification ms and salary 85 and position is lecturer second person is PhD salary 145 position is professor so look this is just an example and somehow you these two values are missing now let's do it let's do it data mining and see how we can actually impute those values so this screenshot is showing what I will actually now do with you so okay so you can still see my screen I guess so here is the Weka after you download the wake up software you will see this icon and you double click on this and then the Weka GUI will will way quicker GUI will come up so you can see this and then you can actually open the Explorer way Chi Explorer and you will see things like this then you can in this one you can actually open any file any file you can actually load any file in this way cur system so here is a I have created a couple of files for you let's say here there is a file called data dot err F let's open this file I'm trying to open this file now on WordPad sorry I'm rushing a little bit because I'm I'm thinking of the time and I hope we'll we have lots of things to share but in case if we can't cover them in today's lecture we have next week's lecture and we will discuss them again in given its a free lecture I'm happy to sort of stay a little bit light here I can I can over over run it oh that's good that's okay yeah yeah if people need to go of course they're welcome to but you know get to the key points and you know like you the recording will be there for those that have to miss it okay yep okay I will try to be as precise as a straight direct as possible but it's good to know that I can I will not be short out after after 8:30 immediately but anyway yeah kick you out straight away okay thank you very much guy for giving me the extra time okay so here I have created two two data sets let's say this is the data dot err Fi err FF is the format that you Jen you need to use for Wicca so we can request err FF format that'sit is nothing but but but the actual data set with some special information at the top so the special information is very simple for example he says edge relation and I have said job so that means like is the name of the table is job or name of the data set is job something like that and then at attribute so now I'm listing the attributes the first one I call the degree and the possible values I said okay they can be MS or PhD masters or PhD and you can see the fast fast value is masters second well second attributes second records first attribute value is PhD third records first attribute value is PhD so the values on the first column can be MS or PhD so that's all this is saying and then at attribute salary so that means the second attribute the first attribute is degree the second attribute is salary and that can be a numeric fellow so it it can be any number so here you can see 85 145 145 85 so these are the salaries and the third attribute I am calling it class so I'm naming it as a class attribute you will you will hear this word a lot in data mining class attribute class attribute means sort of the target attribute and you can actually define any attribute as class attribute if you like but generally it means a special attribute that you want to predict for future for example disease diagnosis or for example in this case the position whether this person this employee is a lecturer or is a professor so that sort of disease or or maybe for banks may be fraudulent customer versus good customers for for yeah for insurance company or for banks may be loan default or good customer something like that so you know special type of attribute that you want to predict for future or that you want to use to discover knowledge around that you know so that kind of thing so people generally call it class attribute it also known as target attributes labels as well records may have labels so here class attributes I'm calling it Ln P so this is the data set but you can see that this is the original data set but let's assume that when we have collected the data set we did not we could not collect the original data set of course this is less - this original data set I'm using as a ground truth but let's assume we have collected these data set that has couple of missing values okay so when we have collected the data set the first employee or the part participant or the person did not mention his salary or her salary so that value is missing or somehow we did not collect that information so we have seen the study study in this research area has seen that if you mind if you want to discover knowledge from a data set that has loss of lots of missing values so often your knowledge discovery may not be as accurate or as useful as if it did not have those missing values say for example if you are building a classifier that can predict future if you build the classifier from a data set that has lots of missing values the classifier may not be as accurate on future records as if the data set did not have those missing fellows right so missing value imputation can be important for many many reasons you can think of many other scenarios where it is important now I'm demonstrating how you can do it using Weka so after you after you click on Weka you will see this wackadoo I then you click on Explorer you will see this way Chi Explorer and here you can actually open files so these are the files that I have on my computer hard disk and now I want to open this file called data missing and all you need to do is just use your easier I'm going to the folder use your directory to go to the right folder this is the one MOOC lecture week one I'm there and data missing okay I'm opening the data so as soon as I have loaded the data Weka will display the construction of the data or the distribution of that data for example it is telling us that the data has three attributes okay the underlying data I have uploaded this data and now we're Chi is telling me that it has three attributes degree salary and class and now it's also displaying what is the how many of so fourth attribute degree what are the possible values MS and PhD there are six people with MS and eight people with PhD and it also shows picture really here with some bar graph and we could do the same thing for salary salary is a numeric value so we can see the minimum value for salary maximum value for salary mean and standard deviation and some information similarly we could see class like how many of those different values are there lecturer six and professor's nine in this data set now what what are we doing we are trying to impute the missing values right so there are two values missing here we want to impute so if we want to impute missing values there are already laws of softwares that that are in Weka that can actually do it and you will be very happy to see even our software on this is also there it's called DMI so if I take the cursor on it you can see the details of GMI and you can see Raman mg & islam' MZ so Islam is my self side Islam so this is our technique it is also also there so I if we now okay if we now choose that technique so that D mi so DM I has been chosen for imputation and then all we need to do just apply once we have applied then it has actually imputed the it has imputed the data set DMI apply and oh sorry okay let me have a loop always it happens when I demonstrate something so what it's supposed to do now it should say that save it save the imputed file so somehow I'm having some glitch here okay let me let me try it again yeah okay once more that's really yeah I really like how the other bird sits down once it's finished let's try it again so why I don't know why it's this is because it's the rules this is how it happens that's right that's right let's see I'm opening this so some time I have seen that it may depend on the screen so I'm opening the same file again sorry for these bear with me so this is all happening on the other screen now is it yeah it's happening on the other screen because I have seen some time the screen may cause it you know like you know you know the Waker sometime you know gets a little bit confused okay might use this opportunity to just comment on a couple of things that have been said in the chat there was some talk about these short courses applying for credit for writing NASA's postgraduate courses it's not the case with this one because it's just a child State University only a short course to sample their graduate certificate it's not actually part of the credit system also there's heaps of good questions flowing in some of them we won't get to just because we'll run out of time but anything that we don't have a chat about we can always talk about in the forum it's always a great place to just rush these things out in more detail than we can in a given time yeah okay okay so he yep so this is what it should do now okay I'm not sure what's happening because when I have opened this missing Valley imputation so all I need to do is just apply and this board should keep on working and then it will just ask me to save the file so it will impute the file and then it will just ask me to save the file and I can possibly it's coming up somewhere else on my other screen but okay let me let me share my window my screen so I was just doing it this a couple of days ago when I was preparing this slide so look this is what it says that save this file and I save this file as data imputed error FF and I took the screenshot I opened that file so we are at this stage that we are seeing this whicka visualization thing in the background then we are imputing we are we have selected DMI we are applying this apply button and then it will say save it and I saved it as data imputed dot err FF and this is the data err imputed dot err FF and here you see that that missing value was there ms and the original value was 85 but now the imputed value is eighty five point two and the original value was PhD and the imputed value is PhD so for numerical value attributes the values can be slightly different sometime that's because the imputation uses some statistical sort of approaches you know to find out to make an educated guess basically okay and DMI is one way of doing it so there are many different ways of doing it so DMA is just one of them it makes an educated guess and this is a very nice technique is very successful we have used it in many many many places and also yeah so and for categorical values it has predicted the categorical value PhD so one thing I need to tell you that you should be as a data scientist you should be mindful that these are the techniques like that can automatically detect corrupt values that can automatically impute missing values but still we need some sort of if it is very critical then we need to be very careful before we make a decision based on the imputed values because these are algorithms and that in some particular given instance it can go go wrong so we need to be mindful of that so this is an example of missing value imputation sorry I can spend time on the Weka thingy and why it is not working but possibly that will waste your time but this is the next step it this is what it will do I can I can show you from the screenshot now the next slide I will show you about the next step of the process here the second step was data pre-processing and missing value imputation was one of those pre-processing approach the second one will be data analysis right so here now I will demonstrate again a data analysis action activity again through Weka hopefully it will work let's see should it should let's see what happens so now the same way after you the same from the same Waker Explorer option or Explorer window after you have imputed the missing values after you have identified column values etc you can also discover knowledge for example you can build a decision tree from it so I'm opening a file say data dot err FF file that does not have any missing value in it after imputation you will not have missing values you gotta set cans have missing values and you can still build classifiers so I'm now instead of the previously I was using filter and now I'm using classifiers okay there is an a button called or tab called classify and if you click on it and you can choose classifiers and see there are so many different classifiers Bayesian classifiers rule based classifiers I am trying to display our demonstrate a tree based classifier and forty-eight is a very commonly used decision tree which is actually Java implementation of Professor Quinlan C 45 C 4.5 C 4.5 is a very common decision tree algorithm and we also have quite a few of our algorithms in in Waker for example C's for this is we call it systemic forests as opposed to random forests random forest is Professor Liu broom ants technique a very very popular technique and insist for we called it we will not be building random forests rather we will be building systematic forests so that we can discover knowledge systematically that's the background for the name sis for and this forest PA is also our technique CSU develop technique now I am demonstrating j48 so when I have chosen j48 I can just start to build a tree this time it worked it builds a tree and you can actually see the tree here so the tree you can visualize it let us visualize the tree and then possibly then possibly we can see what is the knowledge that we are discovering so remember this was your data set open the data set again this was your data set where lots of values there were there were different values for degree and salary and then the class fellow either lecturer or professor now what a decision tree does is it discovers the pattern why sometime people are classified as lecturer sometime people are classified as professor and in this case it has discovered that if degree is ms masters then they are classified as lecturers and the degrees PhD they're classified as professors look this is just a toy example don't don't really don't believe this data set okay so this is just an example and this 9 means there are nine records and these six means there are six records so from this you are now learning that aha I can see okay this was the data set it was very hard to discover knowledge from it but when a decision tree bill say visualizing easy easy to visualize easy to visualize logic rules set of logic rules tree structure then it becomes very easy for you to understand for an end-user to understand you data set this one is a simple one this is a Toyota Tercel but in real life data sets imagined may have thousands and millions of records and hundreds and thousands of variables so it can be very difficult to discover any knowledge by plane I plane eyes and you also might have noticed that we did not need any pre pre assumed knowledge to discover this this pattern so we did not we did not ask show us the relationship between degree and position or show us the relationship between salary and position it actually automatically discovers these things from a data set so this is one example I can show maybe if and you can also discover alternate patterns say for example if you want to remove this and just want to just want to focus on salary then maybe there are ways you can actually do it I don't know whether that will work here I can build a forest here so let's let's just be in the forest and classify and start we can Dev we can build multiple trees and then we can have minimum number of Records so these things we can modify in order to build more forest so what is the forest in the next lecture I will discuss here it has now built forest here you can see degrees M s then its lecturer six of them are lecturers and no professors degree is PhD then their professors that's the first tree but the second tree says we could also discover this knowledge through salary not always through degree if salary is less than hundred and fourteen then their lecturers but if salary is greater than hundred and fourteen then their professors so there are multiple knowledge multiple views in a data set in the same data set so you can use this now at this stage I am happy to take some questions on this so is there is somebody asking questions guy is there any oh yes there's many questions ok let's go a couple will talk about missing records first what sort of point would you consider imputation to be unfeasible is there dancer that's a very good question yeah that's a very good question I mean look I mean data science is not a miracle so if if you're de if your data collection has 90% records 90% values missing is very difficult to impute you cannot actually often the understanding is generally in most of the datasets regardless how careful you are for generally for most of the big datasets 5% values can go missing for various reasons and in those cases generally you can impute them now the imputation also requires good relation in the data set so good pattern say what these imputation techniques are as I said not magic right so what they do they they generally will be very accurate when the attributes are strongly correlated and when the records are very similar to each other now this DMI technique actually in this DMI technique we realize that for a given data set you cannot increase attribute correlation and similarity of the records but what you can do is you can actually identify segments horizontal segments in the data set in a data set where the attributes are strongly correlated just give an example I'm giving an example if I if I ask you the correlation between age and height for the overall population in Australia you may see that there is no such strong correlation I'm a very old guy but I can be very very short and some young guy can be short as well and some older older people can be taller as well so there is no such strong correlation but if I ask you know do the correlation find the correlation between age and high height in a Highschool and then you will see in a high school generally there is a strong correlation so when when a population has aged less than 19 or 20 maybe there is a strong correlation but in the whole whole population of Australia the correlation maybe not as strong so this is just an example that in some areas of the data said the attributes can have stronger correlation and the records can have more similar to each other so we try to identify those areas so that we can make better imputation using an EMI AK equation that is used for they call it expectation maximization approach to to impute things but the question is very good sorry I'm going a little bit off track but the question is at what point it will be unfeasible it may go unfeasible very quickly if your data set has a very big number of missing values then it it can be very difficult you know quality of the imputation can go a little bit in a rough if you have a chance to have a quick look at our papers on missing value imputation those papers are freely available and you will see that we have actually presented experimental result on a massive combination of possibilities like percentage of missing values 1 percent 2 percent 3 percent 10 percent 15 percent and also are the missing values coming from the same column again and again or they are coming randomly from any column and what are the situation in terms of imputation so we have actually presented some empirical result on that that may give you some idea you can see clearly that with number of missing value increasing the quality is getting lower does it answer the question I'm sure it ends as many Aron is asked when imputing missing values can you unconsciously introduce a pre assumption or bias yeah I mean the missing value in petition can be rule-based safe for example I can say male and pregnant is not possible so if you in any of I can I can ask my software to scan the data set and identify any record having gender equals male and diagnosed is equals pregnant and and detect that as a incorrect value or if the value is missing say for example seller salary greater than say 700 k then the value imputation has to be CEO something like that so it can be rule-based and if we do it rule-based then it is possible to actually in introduce bias without you know like because because my rules are based on my knowledge and my knowledge can be actually incorrect or can be incomplete so yes that is possible but if you use the missing value imputation through statistical analysis like what these these imputation techniques are actually essentially doing they are studying the data set trying to understand the relationship and then among the attributes among the records and then when there is a value missing its trying to see for similar type of records what was the value for this one I will give you an example the basic the most naive missing value imputation technique can be think melt can be thought of the mean imputation so if this attribute the whole column of average is 95 then any value missing is 95 something that's a very nice very very old or very primitive sort of approach that that's not accurate but what I'm trying to say here is that it is based on the data sets statistical properties they the system tries to impute a missing value so it may not be as as bad as having some rules and the rules are guided by my knowledge so if this one is kind of like the approach is led data talk so stat it from the statistical point of view but as I said you are 100% right Andrew I think as the question you're 100% right if the if the pattern is incorrect and the discovery is incorrect it depends on the accuracy of the algorithm it depends on the on the complexity of the missing value etc many a times the value can be incorrect so there is no reason to be very confident that the imputed missing value is correct so it's just an educated guess thank you I too am mindful of the time and going a little bit over so perhaps we'll just answer one question more and then we'll start talking about what's coming up next week and maybe this might lean in to what's coming up in future weeks Neville asks what do you think about the crossover of data mining and AI or machine learning that's a very good question again ago I'm very happy to hear this difficult and interesting questions look I mean if I myself was confused like what do you mean by data mining what do you mean by machine learning where is that demarcation line look I will share one quick story I attended a conference I met a professor from some universities overseas and then we had good time we discussed various things you know conference people are relaxed we discuss about our research topics etc and then after I came back to my office after a couple of days few days I received an email from that professor and he said Sahid I thought you mentioned that you are a data miner but when I look at your profile you have lots of decision tree algorithms how come you are a data miner and not a statistician so look this is an example of overlapping areas so from when I talk about decision tree knowledge discovery I very strongly think this is part of data mining but he being a statistician he was thinking that decision trees are actually part of Statistics not data mining and he was confused so what I'm saying here is the lines overlapping the borders are very fuzzy because the fast decision tree algorithm was published in a statistics journal so it's very difficult now your question is the similar question like the difference between machine learning and data mining look different people will answer it differently my answer to this is data mining generally wants to ok let's start with ok yeah ok let me explain data mining will generally want to discover knowledge and then understand the data set and then will want to see why this is happening why this is not happening what are the reasons try to make sense of it and then use this knowledge for future prediction for decision-making for strategic decision etc right so that's generally that's what we think and think of data mining is and in many situation that is important for example if I go and tell my CEO tomorrow that we have to shut down this and these and these outlets otherwise we'll be losing this much money next year and the seal asks me why and I don't know why I just know my system is telling me that we have to shut them down and I don't know why so then my CEO of course will not be very convinced but in many situation we don't need to know why say for example I have thousands and thousands of images and I need to identify kangaroos or horses in those images I don't need to know why this is a horse as long as this is a horse and my system is identifying these as the horse maybe that's enough for me in that particular situation so machine learning is generally focusing on just identification classification and future prediction possibly it does not possibly worry too much about the why part so why it is important it's not some data mining is very kind of attached with business kind of analytics and decision making strategy making whereas machine learning is kind of is more attached with yes/no kind of things and AI is if you install the machine learning on some some vehicle robot etc then possibly as long as the robot if they for example you are sending a robot to to dig out gold from a mine and as long as the robot turns right when it needs to turn right you are ok with that right you don't need to know why the robot turned right and then turned left and then step forward to fetch the gold nugget as long as it has is the gold nugget we are happy so that could be a difference but I might be wrong but there's my explanation of the difference between and you could also ask what is the difference between data science and data mining so that sort of thing indeed some people have and and I think a lot of the point of these questions is the answer is quite frequently maybe you know and and and and you know it's a great place I think to ask these questions in the forums for each of the modules we'll put up all of the questions that have been asked and we you know there's still 25 questions outstanding and I've got rid of some already that really would be great to have asked anyway but we'll call it a night for now thank you so much so heed super-interesting you make me want to go back and do high school maths I can try and understand this a little bit more clearly what are we gonna have a look at next week sorry guy are you asking me yeah okay yeah next next week will be actually I mean in this week we also wanted to discuss a little bit of clustering what is clustering and show an example of how you can do clustering in Weka but we can do it next week very quickly and next week will be discussing more about ensemble of trays so we have seen in this week what is a decision tree and how come how it can actually discover some knowledge but we possibly will discuss that a little bit more that we if there is a data set how we can apply a decision tree and an ensemble of trays ensemble of tree is number of trees if we also call it forest decision forest one tree can discover one pattern but multiple trees can discover multiple pattern and more knowledge about the data set so we will be discussing those things in the next lecture look forward to it alrighty thanks so much everyone for coming along thank you Chantal life Rebecca and Jason for joining us and helping out tonight and thank you so much so it's a good night thanks guys thanks guy Chantel Jason everybody but particularly thanks to all the participants thanks for coming if you did not come I did not have this I would not have this opportunity to give a lecture thank you "
r08s-RaU9Xk,22,"Need more help with your HSC study? Check out my new digital study guides here:
https://christopher-s-school-bf2f.thinkific.com/

A look at the benefits of Data Warehousing & Data Mining. 

Data warehousing can be said to be the process of centralising historical data from multiple sources into one location. 

Data mining is the process of finding patterns in a Data Warehouse using technologies such as Online Analytical Processing (OLAP).",2016-07-19T22:41:18Z,Date Warehousing and Data Mining,https://i.ytimg.com/vi/r08s-RaU9Xk/hqdefault.jpg,Christopher Kalodikis,PT4M15S,false,11839,85,3,0,4,we're now going to take a look at the use of data warehousing and data mining in order to find trends in multiple database sources that have been archived so basically first we're going to look at data warehousing and this is basically the process of bringing together a lot of databases that's spanning a very long time and centralizing them all in one location these databases are not operational anymore so it could be the current years database but it would be a copy of it that is not being updated any more as we speak okay these databases are all read-only we're just looking at the data within these databases we're not adding new records to them okay these databases also need to be subject oriented there's some sort of linking factor that make all these data sources that were combining all relevant to each other okay so it might be for one particular store or about a particular product and all the data across the state that applies to that product okay and this data is usually historical so it's not just for this one year it might cover multiple years multiple quarters of each year okay so it's quite a large data source within this data warehouse that is pulling together all these data bases so we've data mining we're actually looking into this data warehouse so we're looking for patterns based on all these data sources we have in this data warehouse and what actual useful information we can get out of it so what we need to do is we can use online analytical processing or OLAP tools that can assist us in analyzing these data the data warehouse if we're going to look at four steps in how to actually look at this data warehouse and data miner we first need to get rid of all the irrelevant patterns so we've got to get used rid of all the data that is not relevant to us we then next need to highlight patterns that are relevant to us and carrying this might be based on formulas or statistics but we highlight those ones that we do need the next level is then developer hypotheses we develop a course of action based on these useful statistics and then the finally we review these patterns which are either going to confirm or predict beliefs okay so it function very similar to an expert system okay so that's how we do our data mining now data mining can be useful for the following areas it might help us determine what products sell best when placed next to other products in a storm it might help us determine the layout of an advertisement in a catalog and how it really emphasizes products and attracts attention to people it may help us analyze the expected increase in sales when a product is put on special okay and will actually show that although the product price is reduced by it being on special more people bought it would still increase the amount of profit we made because more products were moved and it also could help us forecast environmental issues based on historical geographical data from similar locations okay so let's graphically try to look at the combination of data warehousing and data mining so basically what we first do is gather all these historical databases together says you can see in here I've got store 1 store to in store 3 and I've got three years worth of database databases load it up for them ok what we need to do now is extract all the data from those nine databases into our data warehouse so they all all their data is now sitting in the data warehouse okay what I then use is our OLAP tools k2 data miner and now I can analyze all the down in that warehouse and hopefully find out that useful information for me so for example as I said on the previous slide what data sells best when it goes on special at what time of year does it sell best when it goes on special when I put it in the catalog do people notice it do they notice it because it's in conjunction with a certain holiday or it's a certain time of year by combining all this data based data together it helps me get even more useful information out of it so by data mining a data warehouse I can find some great information so I hope that all makes sense to you guys 
hfpkJs6GJUg,27,"In the last few years, collections of digital text have strongly increased in number, especially in the field of humanities. Digital libraries of full-text documents, including digital editions of literary texts, are emerging as environments for the production, the management and the dissemination of complex annotated corpora. The potential of Text and Data Mining (TDM) technology is enormous. If encouraged, TDM can become an everyday tool used for the discovery of knowledge, to create significant benefits for industry, citizens and governments. Because TDM involves certain acts of reproduction and communication to the public of (parts of) the texts in the collections, the enforcement of copyright and database rights in the collections may constitute a serious obstacle to the use of this new technology for the benefit of science. The intellectual property implications of the use of TDM has been brought to the fore at the European level, where TDM was declared one of the four topics needing further discussion in the context of the structured stakeholder dialogue led by the European Commission.The presentation will explain how copyright and database rights can be used to restrict TDM and how discussions are evolving on this issue at the European level.

The interdisciplinary lecture series ""Internet & Society"", organised by the Institute of Political Science and the Sociological Research Institute, as part of the Digital Humanities Research Collaboration, explores the social, technological and political interactions of the Internet and society. More information can be found under http://www.gcdh.de/index.php?cID=341.",2013-06-17T08:42:11Z,"Prof. Lucie Guibault: ""Intellectual property rights' obstructions to text and data mining""",https://i.ytimg.com/vi/hfpkJs6GJUg/hqdefault.jpg,Universität Göttingen,PT56M17S,false,346,1,0,0,0,I guess the subject did not draw that many that big of a crowd because perhaps people don't really know where and what it means and why people get so passionate about it these days and I don't know if you have seen here and there news reports about the clash between exercising IP rights like copyrights and database rights and the possibility or not to do some text and data mining and it has really created a huge controversy at the European level in the last few months especially since March well February March where the scientists in Europe and mainly mainly UK also in the Netherlands but also through the voice of big scientific journals scientists have been really crying out because it is impossible for them to make text and data mining and to use the databases and you know with which they normally work and they can't use those to really generate knowledge from the big data so this is one article in nature that really draw the intention in March about the clash over text and data mining and well in the coming minutes therefore I'll draw a portrait of well you know text and data mining you all know what it is because you're doing it and this is the center of digital humanities you know what that text and data mining is but you may or may not know we should examine together what it means from a copyright and data base right point of view to do some data mining and an impact of these rights the iprss intellectual property rights meaning copyright and database rights what their impact is on text and data mining and the current debates so I start off well I needed from self to give myself as a jurist the definition of text and data mining as again you you all work with the tools but as a jurist I don't yet so of course text and data mining are both based upon a statistical pattern analysis of content so robot will examine huge quantities of data to filter out according to a specific query all the knowledge that the researcher is looking for and it can be either text mining or data mining and there's a difference between texts I learned which requires syntax analysis or syntactical pattern recognition next to statistical methods of cool occurrence of terms which is much more technical I would say then the data mining which is more general general term and can cover interpretation of data as numerically expressed attributes usually based upon the algorithmic pattern recognition I guess you all know what this means for a jurist that's a bit more obscure but there we go what's the problem and the problem is that well text and data mining abbreviated as TDM involves access to and usage of articles in bulk so it involves using data and the data can be in the form of an article or pure raw data it's any any huge collection of articles or data so accessing it and using it to to generate the knowledge based on this filtering out and this therefore in corporate terms involves well can involve either the copyright or the database right and I'll give you a portrait of copyrights what's copra entails and what database rights entail and then we'll join again now this might be common knowledge to you all already but you all know that at least I have someone who who doesn't already know so I'm very happy good well I'm just checking on it is it's important for for someone to give a lecture and to know that you're not telling or commonplace things through to most of the people so I'm happy I'll I'll I'll tell you what copyright entails now the subject matter of copyright is well and this here would be nice to have a pointer because I could tell you but actually copyright protects the expression of an idea but not the idea itself so facts are excluded from copyright protection but even talking about expression of an idea copyright will attach to expression as long as its original so there's a criteria of originality under copyright and now in the respect of databases or corpus of knowledge you can see it in two different levels you can see it globally as a compilation and the combination as such as a gathering of data or articles or everything will be protected by copyright if the compilation itself is original and the originality of the compilation will probably be reflected in the selection and arrangement of the contents of the compilation now in the in the case of a big publisher there probably is no originality in the compilation because they don't arrange and selects according to originality criteria they just gather what they produce in a big database now this is the first level when you when you look at the aggregate level of the database you you also need to look at the individual level of the articles now articles scientific articles whatever other content in a database may also be it's by copyright again if its original so articles music videos will be protected by copyright if they meets the requirement of our sorry originality so meaning that a database may contain copyright protected elements but also may contain non protected elements such as facts and data so this is a subject matter for copyright if it's original either because it's a compilation or because it's an individual article then the rights owner has two sets of rights one is the economic right and the economic rights well if you look at the German core product you will see that there's a list of different rights contained in the Act and it's much more specific than than this one but generally you can categorize the the economic rights in two groups the right of reproduction and a right of communication to the public and the right of reproduction that has rather been harmonized at European level and that's any temporary or permanent reproduction by any means any any form so therefore a reproduction by a computer is a copyright related or copyright relevant Act let's say you might infringe somebody's rights if you make a reproduction in a computer now and the right of communication to the public is also a broad term that if you look in the German copper era you will see much more specified but it contains broadcasting making it available on the internet also distribution of copies of a work now this is the first part of the of the rights confirmed by copyright you also have more rights and the moral rights are less important I think in the context of data text and data mining but this still needs to mention them and that the rights to request that your name be named together with the work the attribution of your name with the work or your right as an author to oppose a modification of the work or mutilation exit ah so these are the rights that you get when you create something original and well a big element to know is that copyrights you don't need to register copyright you get the copyright protection as soon as you create something original so it's an automatic form of protection and the protection lasts for the life of the author Plus 70 years after his death and that's a very very long time and that means that if you if you think you know of a big publisher you think of El Sofia or you think of nature or any other big publisher they've been publishing articles of scientists for almost a century I would say and that the bulk the huge vast bulk of those articles in the databases of the publishers still is copyright protected because the protection has not left yet so only the very old works might already be in the public domain now if you work with with databases containing manuscripts of the the 1600 those manuscripts will be in the public domain so that those will be no longer protected however to remember that the individual records may not be protected but the compilation if there's creative workers if the compilation meets the criterion of originality you might have copyright protection on that compilation so not on the individual components but on the aggregate level if the compilation is original now there are of course on the copyright there are exceptions and limitations and you might think well this text and data mining fall under one of those exceptions and imitations here I listed the three most logical limitations that you might think of when you think of text and data mining for research purposes the first one is incidental and transient reproductions this is introduced by the information society directive in 2001 and it's uniform across the European Union and it the idea behind this was that purely technical reproductions to allow the system an automatic system to to function easily where they're absolutely temporary and necessary to execute the technical function those should be exempted from or excluded from the scope of the copyright and we'll see I'll see in more details those three limitations but this is one the second one is the right to quote and the third one is educational and research use now the three types of limitations have all been Armin eyes through the information society directive but they've been implemented you may or may not know that a directive European directive needs to be transposed in the law of each member state to become binding and so the directive usually gives you know contains norms that can be interpreted or transposed differently from one member states to another so you see indeed strong differences in the legislation of the member states concerning the limitations exceptions the wording is different and sometimes the scope is different so incidental and transient reproduction I took the text from the buhei gazettes to make your life easier because well this is probably the most widely harmonized exception because it was new so when when the information society directive was adopted at the European level in 2001 it was a brand-new provision that did not exist anywhere before in any of the Member States legislation so the Member States we could not really rest on their tradition or existing limitations or like existing legislation so they just decided basically if you look at all the acts they all copied the text of the day-after directive so and basically you see here this is the formulation of the exception but really what stands out here is that the reproduction to be exempted from the scope of the copyright needs to be really ephemeral really transient and there is some case law from the court of view of Justice of the Yee you that did interpret this provision but it's really limited it there is a bulk of case law in the member states that usually refuse the application of the of the exception because the reproduction was too permanent or too long-lasting to fall within the scope of this transient No yeah yeah and and yeah so this this excuse this clearly does not fall with the incidental and transient reproduction yeah so when again when you think of the text and data mining when you send the robots to filter out and gain knowledge throughout the whole the whole corpus of a database it doesn't fit the criteria of this provision so it's it clearly doesn't fall within the ambit of this provision the provision was thoughts in 2001 as allowing browsing and caching but not any caching it is really the very short short-lived caching so yeah so courts interpret this provision very restrictive lis now you might think that text and data mining might be a form of quotation well has been argued but not very convincingly if you look under the information society directive its Article five three B and quotations are allowed for purposes such as criticism or if you review and then you have the conditions provided that they relate to work or other subject matter which has already been lawfully made available to the public well that would be fulfilled that unless this turned out to be impossible the stores including the author's name is indicated well you have there an excuse not to indicate the source of the name of the author but and that their use is in accordance with fair practice to the extent required by the specific purpose now text and data mining a whole database would probably not comply with the last criteria the provision under the infrastructure active and even worse if you look at the Dutch text I don't I hope it's a bit clear enough but especially America and a pointer might be useful but the the one problem would be that antonovic and not a fair issue in n sub standards research roughly suspect excuse me for the accent but meaning that the right to code basically is to allow the author to integrate to quote inside a new work you know and this integration in a new work the text of the info so directive doesn't talk about it but the national legislation most in most cases do require that a quotation be integrated in a work of scientific criticism or other forms of integration in a new work you quote for a purpose and the purpose is to criticize or study or announce or depending where the formulation of the Act is but you always quote in something else and when you do text and data mining you don't quote in something else this the purpose and the it's the step too far yeah well at least for the traditional interpretation of the right to quote there are it does I mean there are many authors in Europe who try to push for a more flexible copyright regime you know push for reform in the European level and some people push for reform at the more national level like in the UK or in the Netherlands and they say well problem is just at a party between brackets that the information society directive contains a list of 20 some limitations most of them are optional but the problem with the infrastructure active is that the list of limitations is exhaustive that member states are not allowed to adopt any other limitation in their own national legislation then those that are listed in the information society directive so when you want to deal with new phenomena like text and data mining or user-generated content or any other form of use that's not covered by the list of exceptions then you're stuck at the European level unless you try to interpret the existing limitations a bit more broadly and that's what the scholars in Europe are trying to say that could the right to quote or the quotation exception should be interpreted more broadly to allow more users than what's currently the case in most jurisdictions but stretching the bounds of the right to quote in my opinion is not the solution that's my personal opinion yes because you can only go so far and and then the right to quote loses its its original traditional purpose so and I think it never fully meets the needs anyway so now educational use and research that's also our limitations that they included in information society directive and you might see here also a slight difference between the text of the directive and the text of the Hibiscus that's now under the directive you can use for the sole purpose of illustration or teaching or scientific research as long as the source including the author's name is indicated unless it's turned out to be impossible and to the extent justified by the non-commercial purpose to be achieved now this is a normal wording and this is the normal wording you-you-you find in most of the copyright acts throughout the United the European Union article 52 a of the New Hebrides gazettes is a bit controversial for the European Commission at least because it goes a bit further than article 5 3 any of the directive in the sense that it allows published parts of works to be communicated I'm translating to to a small group of scientists so and and and you see this is a clear distinction this is a language that you don't find in the information society directive and I know that the European Commission since well had to discuss this issue and with Germany when they implemented this provision and I'm not sure if I'm mistaking that this provision had a sunset clause what we called it was supposed to be abrogated after a while but they keep prolonging its application so it it was so controversial that this provision was put in the act with a certain deadline and but they keep prolonging the deadline because well no one hit the roof so far I mean there were discussions with the European Commission but basically publishers seem to be able to live with this provision in Germany and in other kind well it did not snowball in other jurisdictions so but if you look at it German scientists through this provision may do a little more in terms of communication among scientists than what they can do on the basis of the interesting information society directive however even if you look at this it still doesn't allow text and data mining because depends on you how you look at at the profession and how you interpret it but to you know to scroll and to to send a robot throughout the whole corpus goes way beyond this used for the sole purpose of illustration or scientific research yes not no legislation such such vague concepts they leave for the caseload the least for the judge to interprets what they feel in the circumstances of the case to be fair and to be the amount of researchers or the amount of a part of a work so there's no legislation whether there's case law there might be one case but I'm not sure anymore and at the European level the discussion on this on these norms usually is between what's public and what's private there's a lot of case law from the Court of Justice about showing TV programs in hotel rooms now this is totally different that what we're talking about but the distinction here is is showing a program in a TV in a hotel room is this public or private and so the court has spent a lot of time trying to examine you know when are we in a circumstance that it's private and when are we in circumstances where you can talk about public so and basically constantly the decision of the European Court of Justice to say well if you show TV programs in the hotel room via a decoder then you you're broadcasting at least you make an act to the public because it's unidentifiable potentially unlimited number of people in the public yeah so but to have this is a unique provision in Europe and 50 to a is unique there's no other provision in any of the copyright acts in Europe and so for the interpretation of what's a small circle of scientists and what's a small part you will need to look at and case law and please forgive me but I didn't have time to check if there are skates on this particular one I think I suspect there might be one case or two of lower instance but not not industries arrests off so yeah but I mean this the another jurisdiction you will also have case law on you know how far can you well not in respect of the educational use but for quotation in France for example and the the right to quote is limited to short quotations and so there is case law in France about what what constitutes are shorts causation and when when do you go beyond the bonds of over short quotation it's slightly different but it's it's the same rough distinction yeah but but if for example in in Germany you have a very interesting case of the Galman a dry case it was buckled breasts theater play if I remember had been reproduced quite extensively in another play and it had it has gone to the Bundesrat off to say well you know half because of course the the heirs of Bechtel corrects a claim he quoted much too far for the for the limits of the quotation right and basically bundes Christoph decided well it fits within the purpose of the play and it was a political or social statement and there was the right to create an article 5 of the lagoon Cosette's so he balanced all the rights in involved and the big AHA came to the conclusion that it it was allowed but it it's it's such a vague concept you always need to see what's you know what the circumstances are case are and how it's being argued also by the lawyers and how it will be picked up by the courts so it's difficult but in any case i reaiiy I'm pretty convinced that texts and data mining don't fall within the ambit of educational research use especially if you look at other jurisdictions but you don't have the unique provision of 50 to a in the in the UK for example you can make a fair dealing of a work for purposes of scientific research and they have criteria but a fair dealing entails it's never the whole entire work let alone an entire database so okay now now we go to the suti nearest database writes routine it is meaning it's it's a of its own it's a it was a newly created intellectual property right and it's the was created in 1996 but also through a European directive it's a brand new right and this protects it's it's not a I P right that protects creativity it's an IP right that creates in the protects investment so the criteria for protection under the the database right is if whether you have invested substantially in the court in the creation not the creation in the collection of verification and presentation of a database and the substantial investment will be evaluated either quantitatively or qualitatively so meaning substantial investment may be in terms of money did you invest a lot of money to to make the database or did you invest or or need to hire experts you know they did involve very specific expertise in the making of the database and that will be the qualitative investment in the knowledge no required to create so they didn't want to narrow it only to money but they also wanted to acknowledge the fact it's sometimes to create a database she needs a specific knowledge or other types of input yeah well basically so to come back to my two to two examples you know like publishers like nature publishers like Elsevier they invest and they gather and they organize and it presents to the reader so for me it's it's rather clear that they do have a database right on their databases and if you you you're working I guess with other databases databases that may contain public domain works like manuscripts of the 16th century for example however if the maker of the database has invested substantially either qualitatively or quantitatively in making the database he may have a database right on it even if the individual elements are not protected and just like copyright database right protection is automatic you don't need to register so as soon as you create or you completes the database then you have a protection that lasts 15 years from the creation of the database and a slightly controversial provision at the time of its adoption anyway is that if you invest if you make a substantial investment in updating your database this on its own also is worth a 15-year protection now and the thing is now we're in the database director was adapted in 96 it was implemented in most countries by 99 98 99 we're almost close to a 15-year first term for the existing databases we don't have yet case law on on how courts will interpret renewals or extensions of protection but that's that will come in the next few years where people will say well you know that database that was released in 99 Oh however you know it's it's almost unimaginable that the person who created the database in 99 did not update it and so it's so therefore the potential and that has been pointed out in literature many many times is that that you have a potential there to have endless protection because of substantial investments in collecting the data and presenting it and verifying it well that's that's where we will need case now on so that's what we don't know yet but we haven't you know the we haven't gone through at first 15 years that yeah it's well we'll get into the discussion in a few minutes I know you know you need to go but yeah but okay shortly so did that abase protection what the database right gives you it gives you the equivalence of the copyrights within other terms so instead of reproduction they call it extraction and instead of communication to the public they call it reutilization so but the idea is the same you reproduce and you can communicate to the public but with other words but what it also adds is that it's systemic and repeated extraction enrich reutilization of in substantial parts of a database may amount to an infringement of the database right but then the court of justice of the EU has narrowed this down you only it only creates infringement if the repeated systematic extraction leads to the creation to building up again the database the initial database so if you keep repeated systematic extraction of in substantial part and you never build up another bad database so basically it's not an infringement so that at least okay now limitations the limitations are with the database directive the thing is they're optional again and the only one that that is really relevant is the one for educational and research purposes but it's again rather limited and so there we go to the discussion and get hot your that's my first bullet point rights most probably subsists in database and or content of publishers yes I mean whether you look at the aggregate level or at the individual level and you're aggregating there will be a database right on it and if you're talking about the publication of articles and probably you also have copyright on the individual articles in the database if it's a database of pure facts or raw data then it might be database rights on you know on the database if the maker of that database can show that he invested substantially and that's debatable but basically all publishers they invoke the copyrights in their articles and then what they they're not too strong invoking the database right yet I I'm afraid that they'll use this as the back door backdoor arguments once we cleared the discussion we copyright because you know the article in nature that I showed at the beginning of the talk really mentioned only copyright and I don't understand why do you just you know don't deal openly well yeah they have a database right I'm just afraid that there kymmie keeping it back had to say yeah okay so now there's an exception for copyright to do text and data mining but ha by the way we have a database right I'm afraid of that because well they could be known to do that anyway so second conclusion is exceptions and limitations are hardly are clickable and we saw and and that scientific publishers do claim their copyright and sometimes their database right on the content of the database so the result is the text and data mining right now currently is not allowed without the prior authorization of rights holders so in getting organisation well you options are either you license directly with publishers but if you're scientists it's it's undoable anyway first of all you need countless permissions for countless database holders or copyright holders and that's provided that they would grant permission to do text and data mining which well currently it's about half and half half say no but the the biggest say no and the smallest say yes basically other option is to go through collective rights management organization and but so far this is not a mandate that has been given to the collective rights management organization so they offer themselves as their way forward they offer themselves as the solution because they can be the intermediary between the scientists who want to text and data mine and the publishers but it's it also entails paying a fee because there's you know that's yeah that's what they're about or another solution would be to introduce a new exception in a copyright act and then you have well you have a whole discussion at the European level that's why it bursts out at the end of February in March because it's it's way too tight so now just to finish this last bullet point the current policy of many publishers especially the biggest ones is to prohibit that text and data mining and or to set strict restrictions so and well this is information that I gathered from presentations made here and there but the biggest as you see here the biggest publishers actually prohibit text and data mining and you can add also I looked at River or international and also prohibit it and those really are the tools with which scientists work and this is really in those databases that you hope through intelligent query to really gather can a wealth of information so but it's explicitly not announced in those databases so the discussion really throughout the roof at the European level and one of the reasons you you may have seen right before Christmas in December 2012 the European Commission came with a green paper and said we're going to look at four different areas where copyright law is more problematic cross-border access of culture of cultural works user-generated content and audio-visual works with cultural heritage institutions and the last one was working group four is about text and data mining but the whole effort of the Europe of the European Commission is to bring the stakeholders to one table and to have them discuss however the the initiative is called licenses for Europe and well I see people nodding yeah the problem is there is that this is way too small but basically what they want is to bring the parties together to allow them to conclude contracts with each other so there was a reaction to this because scientists and groups you know civil groups and other stakeholders went there thank you for being here yo goodbye the the the went to the table and said well no this can't go on because we want yeah what they want is this and first bullet point is really very important they want to be sure that all evidence opinions and solutions - fela to facilitate the widest adoption of text in the data mining are given equal way and that no solution is ruled out from the outset meaning in other words that they want they don't want to be restricted only to licensing practical practices they want to discuss fairly and openly the possibility of introducing an exception or limitation in Copyright Act but this has been consistently thrown off the table it won't be discussed the licenses for Europe under the auspices of the European Commission is only there to talk about licensed practices and licensing options it's not there to discuss exceptions and limitations and as a result of this well I mean they also have two other points of contention that they wrote in their letter to the to the European Commission basically if I go back one you will see this is the league dead people you take a copy and dog ashesh Li bear you may know this of course they were the the papa hole they were the voice for all the groups who were dissatisfied and they wrote the letter that others signed and I also signed this as a as a scholar in copyright because I am the way the reason why I signed it is I wanted a fair dialogue and that license saying if you look back licensing directly with publishers is impossible it's it's an impossible burden to put on scientists licensing through collective rights management while also always entails money and for when you do research for non-commercial purposes in any field you don't want to have to pay a license or it has to be discounted somewhere else than in the pockets of the scientists so yeah also a second bullet point from the Lee bear is that they really want transparency in a process and this from experience is something that that such discussions lack drastically discussions of this type at the European Commission are not transparent and then they the last bullet points the the the put forward was that they wanted really the DG research and innovation to be part of the talks because when they they went for three meetings there was never any one from the D G research and innovation with the only mark DG intermarket DG connect and education but no no representative of the research organizations and the researchers there anyway now the there came a response from the European Commission but the response was basically well exceptions and limitations are discussed in another forum we're discussing a licensing rich DG research is invited all the time but it is their choice not to come we've heard other other views on that and they didn't say anything about making documents available more than what's to be found on the website so basically it was a rather colourless answer and the reaction was that most of the participants who were dissatisfied decided to withdraw from the talks so there was one meeting last week that took place with all these institutions no longer present at the talks so it's a gamble it's a gamble for those institutions who decided to withdraw they make a point of course and I support this point but the danger is that the talks will keep going and that there will be a result with which you're not satisfied at all and then you'll have to live with it and that you had no say or no input against it well it was being negotiated so it is also you know the danger of withdrawing to make a point sometimes you lose more I don't know we'll have to see I didn't hear yet what happened last week it's also not obvious on the website of the European Commission they usually post on the website if you're curious you you you can have my presentation at the end I hope it's clear enough but otherwise you just die at Google in licenses for Europe and then you get the the website of the European Commission so they published the presentations made during the meetings and they publish conclusions that are written up by someone of one of the DG's and that that's it but this also means that the discussions that are now taking place there's no more balance at all at the table so the only publishers will be present from now on and or collective rights management societies so that's that's a dangerous development there's one good news in the horizon is that the UK is very active with sex and data mining because they also see the wealth of information and the importance of doing text and data mining the UK has been engaged in the copyright reform exercise for at least since 2007 I think they produced the Cobbers review and then they produce the the publish their Hargreaves review and the Hargreaves review was really the starting point of the whole exercise of reforming and so the there they're a bit further in their exercise they also had an impact assessments on on a specific issue of text data mining and basically they thought that you know the the market would probably be able to sustain such an exception in imitation however they did get well counterbalanced reactions during the public consultation in 2000 Evon but in December 2012 the UK government published its document called modernizing copyright and modern robust and flexible framework and they're in there and that document there is a commitment to adopt text and data mining exception for non-commercial purposes so now of course you don't adopt a very overly broad exception a limitation you really want to you know allow things to happen as long as they fit within the ambit of non-commercial research so this this is the these are some of the safeguard that might be building the the provision that the the exception will not prevent a publisher from applying technological measures on their networks in order to maintain security stability because that's an argument that also has been made you know the publishers don't want an exception because that will allow anyone to do anything and they won't be able to to protect this their servers and the databases yeah well so during no because the the it hasn't been adopted yet now we're at the stage and that's already six months ago time flies they they adopted recently another legislative text but to deal with orphan works so I guess that was more urgent because there's a deadline to implement the directive on orphan works the deadline is 1st of November 2013 text and data mining they still I guess they still want to to take their time to to really do it because also they would be the first ones not only would they be the first ones but if you put it in a European context as I said before the information society directive contains a close list of limitations this limitation would go beyond that list so in the UK said well we don't we don't even look at what others do we don't really care what you're obsessed we want an a limit I think they're very courageous I have an idea of what kind of reaction they might get from the European Commission if they go ahead with adopting this limitation because they would certainly get a letter from the European Commission saying that there they're not in compliance with the European corporate framework that they would have to amend or repair or or adapt so there they're brave but but they see you know they see the point in in allowing text and data mining now they also say that exception would not prevent publishers from offering additional services of course and the exception would not allow redistribution of the primary work to substantial parts of those which would remain restricted by copyright but because this is also one argument that's always being put forward by the publisher saying well we're afraid that you know if we allow text and data mining that the content of the database that the articles will be just yeah yeah so of course now they have to find the proper wording to have these safeguards built in and that's linguistically and drafting technique is very complicated you know you give in one go and you take back in another and so it's it's a no there's no bottle to build on there would be brand-new provisions I guess III can explain why they take that time and then the last slide is the way forward now some arguments there are being put forward to support allowing text and data mining is that using technologies to find new relationships between facts does not undermine the primary market of access to research works and that's that that's I guess something that publishers deny to see the they are afraid of a primary market but they're basically afraid of any new change in their business model and any change in the way to carry out research or you know if you know the context anything that might affect remotely their business model but texted I'm not data mining is it does not affect you know the value or the the market value of individual articles or magazines they will always be purchased by libraries or by individual researchers should not make a difference and text and data mining have the potential to deliver significant public benefits in terms of increased research and lower cost or finding new new information in discoveries and making links and that's also what namely who's also recognized in in at least one or two public speeches that she made one way forward in science is to allow the use as you all know and as you all do at the CJD age of new digital tools to get access to this wealth of information problem is is that as I gave a portraits the stakeholders and the views are very polarized nothing new in copyright law they're always very polarized whether you talk about films or music or publishing they're always polarized so it will be a matter of finding the the middle ground somewhere I'm just not convinced personally that licensing is the issue or at he's sorry the solution and I would of course you you see me coming with the solution I would favor an exception in limitation but within within the bounds that are strictly necessary to allow proper scientific research on non-commercial basis but without affecting the business models of the publishers but that's a bit different and difficult so this is the contents of my presentation thank you very much you 
ePxZDuQdnW4,27,"Apriori algorithm, a classic algorithm, is useful in mining frequent itemsets and relevant association rules. Usually, you operate this algorithm on a database containing a large number of transactions. One such example is the items customers buy at a supermarket. In this video, we will solve a numerical problem based on Apriori Algorithm. #DataMining #AprioriAlgorithm

Follow me on Instagram 👉 https://www.instagram.com/ngnieredteacher/
Visit my Profile 👉 https://www.linkedin.com/in/reng99/
Support my work on Patreon 👉 https://www.patreon.com/ranjiraj",2018-05-21T13:43:09Z,Data Mining & Business Intelligence | Tutorial #21 | Apriori Algorithm (Solved Problem),https://i.ytimg.com/vi/ePxZDuQdnW4/hqdefault.jpg,RANJI RAJ,PT23M23S,false,6780,96,1,0,12,hey guys welcome to another interesting video based on data mining so in today's video we'll be solving a problem based on a priori algorithm for determining so before we solve a problem let's have a basic understanding about what is a priori algorithm and where it is used so a priori algorithm is nothing but is a algorithm for frequent itemsets mining in data set for relational as well as transactional databases like for example in market based analysis it is mainly used for example if you have an Amazon Marketplace and if you buy some kind of items like for example if you buy iPhone and then it's like more probabilities that you also buy a case and also like a tempered glass so it's like kind of a relationship is there and there are many Association rules like you map it and you buy those kind of frequent items and also you have your YouTube so in YouTube these kind of videos if you see like on frequent items certain pattern matching a prior you gottem so the relevant items which you can see can be seen by like kind of recommended for you and recommended to you kind of things so it's like the key tags which are actually put in my videos those which relate to other videos of certain kinds from other users can also be seen in your right side pin so it's all based on Association rules and it's worked on the a priori algorithm so let's have a understanding about how to solve the a priori algorithm in data mining so stay tuned so the problem goes in this way so you have been given the minimum support in some person is at the 60 percentage it can be wearing for different problems and then you have been given the minimum confidence 80 percent it so it's like minimum confidence for the time being you just have to ignore while solving the problem that we'll see later when we do the kind of Association rules so you have been given this chart or the information table so it's like like a mentioned frequent item set meaning and also for the Association rules so you have kind of transactional databases and narrator so for that you have certain kind of tid status tid 100 200 300 likewise and you have some certain kind of items word so it's like basically the items so this letters or the alphabets itis m o NK e my these are the items and since it is represented inside of braces it's called as the item set so it's like one item set in transaction 200 there is another item set bounty then make Lucy cookie and all those kind of things so this is how the data set or the data table is given then we have given this problem like you have to use the a priori algorithm to find the frequent item set from this and also list down there Association rules now from a basic analysis we can see that like some intuitions playing some intuitions we can see this oh-kayyy and okay from all this data for different transactional databases the okay items or the item set is appearing more frequently and so it's the frequent item said but then to to go in a constructive manner in order to consider certain steps and kind of certain formulas and all we solve it in eight different steps so there are eight different approaches or eight different steps to this to solve so it's basically easy there is nothing to worry about solving this problem so we basically start with generating certain kind of table item set and support count and then after we just compare those item sets with the minimum support we have so just I talked about minimum support or the support so that support is basically so in step one you have to generate a certain kind of clusters we generate in this so we generate C one cluster with the minimum support so like for this you need the minimum support or the support so how you can get that so support is given by this that is minimum support that isn't 60 percentage have to convert it that is 60 divided by 100 and multiplied by the number of items or the number of transaction IDs T IDs which are there so in this you have one two three four and five so we have five different Tran IDs and so it will be 60 into fives oh that will be three so we have the minimum support so I'll right here that is minimum support this three so this is the value that is min support is equal to three and so all the items which are there in that particular table or data set we have to prune or you have to drop down which are less than three and we have to consider items which are greater than or equal to three and so we first begin with the data set so we have this table so I will try to fit in this so you basically have items or the item set just right items and you have the support or the means of put anything is fine so first what we do is we write all the alphabets of all the terms or all the items without repetition from this table so first what we have we have M then we have o N K E and by and so now we have two so the first set is completed and so we need the second one so it's like donkey is there so monkey is completed then donkey so you just need to add D to it so it's like D o NK e Y so that gets completed next we have a so that makes make you have M a K and E then you have you and you have C that is down so it's like mu C that is mu C by and then you have finally as I think it's I yeah we have I so it's like cookie you have a CE o K I and E yeah so this is how the table goes now we will count the occurrence of each of these items from our particular I didn't say that we have from this kind of table yeah so this place is a little narrow so that's fine but then we count em s occurring what M is coming in this item set how many number of times that is where we will come the frequency of that particular item so M once then you have twice thrice so three times so support of M is 3 in this then you have o 1 2 3 & 4 then you have N 1 - yeah - likewise you have to fill this table so I already have this table so we will just fill it and so it is sorry 4/3 and you can just cross shake it this item set so I already made it so I just filled it now what you have to do you have to just filter out or you have to drop out all the values which are less than sorry greater than or equal to 3 so it will be greater than or equal to 3 so this is the threshold so what you have to do whichever value that is equal to 3 so 3 s equals its equal to 3 it's greater than 3 it is less than 3 so we will drop it this will consider this week instead of this will consider and all this will just drop so this is like you can consider is the initial cluster with so many data points of so many items then be founded of the outliers or the noise then we try to remove it or eliminate it now what we do we just transfer it to a temporary labeled disk item or cluster so for that we have these step two so what we do in this we compare minimum support with C to a bit C 1 so C 1 is our cluster so in this cluster we just put all these values which we have after we cancel this so you need not write this T can crawl this is just for demonstrative purpose and then we will be left with this that is items and then we have the support count and so tree will be just putting that as M then we have o then we have K E and by K E and Y and then yeah that's it so it will be this will be our next that is temporary so we will just label it as l1 so just a label given to that particular temporary cluster and so we'll write it support alongside it so it will be O 5 4 & 3 right yeah 5 4 3 so this will be our final after step 2 this will be a temporary cluster or the c1 that is item set now what we'll be doing step 2 it will be your 11 that we consider this set again so what we'll do we try to make a pair of 2 items there will be like 2 items will be there so what we will do be just trying to conjugate or we just try to relate it with other items so that we get the another data set so it will be like will generating these step 3 so we will have generation steps so generate C 2 that is second cluster what we have and then what we do we just create this table and so we have items here and then we have the support cont [Music] and so what we'll do it's very easy so just you have to write em o then MK m e my so it becomes m o MK m e and my so once this is done then it's okay Oh II and oh wait you just created one by one Oh II and oh by yeah and then k EK y ke k buddy and then finally we have ey now what we'll do we just do the similar like we have this we just compare this with the frequency of this like mo is occurring how many times once is there yeah it's one time then M K is there one then two then three is there then M is there one and two yeah two times so likewise I have this table again so I will just fill this values so it will be here too it will be three then it will be three and three to four three two three and two and so we do the same thing like here we compare greater than or equal to 3 that is a minimum support we just compare so this goes off this stage this goes this goes this stage this case this goes this stays this stays and in this goes so we will be left with this many items so what I will do now I will compare it that is our step four will be L 2 that is the new label or the temporary cluster which I have frequent items remaining and so comparing minimum support with C 2 so we do the same thing here so it will be L 2 here and in L 2 we have this items we draw this table we have items here and then we have this support counter yeah and so we'll just consider those items that does we have M K we have okay Oh II okay Oh II and then you have K E and K Y K E and K by so we'll be left with diesel and so we write the support count so m'kay m'kay is 3 o K is 3 o is 3 and K is 4 and K Y is also 3 yeah so next what we do we try to create one more to us tell that the c3 and so we will be generating a item set of having three different items so we will just wear it again so that's also very easy so we do not worry how to make a three pair items so that's also very easy so then it will be a three per item or then how can you identify or it's like if you think it's time-consuming then it's not so like if you have m'kay and you have okay so if there is something common between them then only it will be making a three item set that is m'kay likewise if you see and can be nothing is common in that because M K and k k is common intact so it will be m ke so likewise we'll be generating another set so we have that is step three we have let us step 5 we have so we will generate pairs for C 3 we have so we have the C 3 and we draw this table again with items and support count and so we just doing that way so it will be MK o remember we have to maintain it in an item set and not only just the items because they are asking us to find the frequent items it next we have that does MK e like I've shown here then you have MK y then you have okay E and finally you have okay way and then again you have to compare this along with this that is Mao is coming how many 10 so M K ovens then you have so m'kay once and twice yeah - MK y MK y yeah two times so it will be three and two so again what you do you just compare it with greater than or equal to three and just we'll consider this this this and finally this so in the basic analysis what we did we just have an understanding or intuition regarding like we have okay as a frequent item and this is the final cluster and so what will we do is we do these step six that is compare compare that is c3 with minimum support and so we'll be left with items and support count so item list will be so our final frequent item set will be okay II with support count of three so this will be our final frequent itemsets now you just did this part itis frequent item set you also need to lay out or list down the Association rules so that's also very easy nothing complicated in this so what we do is we just take this particular item set that is okay and we have this kind of minimum and confidence minimum confidence eighty percent and with minimum support so we create that the step seven will be creating Association rules with support and confidence yeah so support and confidence and that will be for item said that is oke so what will we do so it's very easy so I will write it here in this part so what we create is now being one of the Association rules for this particular frequent item set so we'll consider a table having four different columns that is we have Association rule and which rule is there then we have the support here then we have the confidence and finally confidence representing percentage confidence in percentage so we will have this kind of four columns and so we will just fill this it's very easy so basically in order to determine the Association rule we just have to so it's a general formula like if you have three different items in this item set so the Association will go see the probability that n factorial so it will be three factorial so it will be six so you will have six six different items that this Association rules for that and so to create it's very easy like you have you have to write o and K gives E or implies key so it's like for example let's take the case of iPhone and case can also buy a tempered glass or oh in to e gives K like a tempered glass or iPhone can also purchase or customer details and so you can write it in a number of possible ways and so we write in this way that is o and K gives e and so this is one double then we have oh and E gives K this is another rule or you can have K and E gives o so this this got completed now you have to just reverse it on the opposite manner like he gives o and K then like it's K a gives oh and E and finally you have po gives K and E so this is how we construct this as creation table now what we do is we write the support so support is free for all these things it's very easy like in this step six we have seen it's three four all this now we have to determine the confidence so confidence like how to determine the confidence of what value we have to put in there so it's like confidence is just integer or a floating point value but then to in order to determine that the confidence the formula is given in this way like confidence is equal to the support that is base one this column divided by the number of patterns occurring number of occurrence of patterns patterns means this pattern whichever is there on the left hand side of this particular Association rule so what we'll do we just check for these all things from this and we'll write it and the confidence so it will be supported by the number of occurrence of the patterns so for the first case it will be 3 divided by okay so you just have to consider oh and cape so when K comes in this okay okay and okay so it will be 3 divided by 3 and so it's 1 so it's like 3 divided by 3 gives you 1 like oh and E oh and E oh and E and O and E so it's like again 3 divided by 3 that's 1 so it's like 3 divided by 3 divided in all cases support is 3 and so we just have to since ok and EK and e ka ones 2 3 & 4 so it will be full so it's like in this 0.75 and this it's like 3 by 5 is there and point 6 and this is 3 by 4 that is 0.75 and we have to convert it into confidence X very easy you just have to multiply these values with hundred so it's like morning two hundred hundred this will be hundred point seven five seventy-five percent days this is 75 and this is 60% vision this is 75 so next what we have even is that is a minimum confidence like minimum support we have 60 percentage so we converted into that is the support cone so here what we'll do is just rule with this that's like this is used with frequent items and and this is used with association rule so with 80% we will just prune all those values which fall below 80% which like will rule this this this and this and so we'll have these two that is we have this association rule so our final step in this is you have the rules comparing this minimum confidence that is we have the 80% minimum confidence and so we have this association rules which support and we have this confidence and this column that is in percentage and so we'll create this table having only these two associations will status one which satisfies 100% so it will be O and K gives you a and O and E gives you K support is free and this is 100% and so this is how you solve a frequent item set meaning for a priori algorithm and also you will list down the Association rules and finally you have to write the statement for this like hence the final Association rules we have hence final Association rules are we have o and K gives E and O and E gives K and so it is used for market based analysis like I have said in the start of this video like this kind of Association rules are mainly used in like recommendation systems in like customer buying frequent kind of atoms like if a customer is buying bread then it's like he'll buy butter or jam or kind of milk kind of items so it's like it's related so you have this association rules in put it into that machines so this also use in machine learning so algorithm also learns like this kind of associations are there so has to it has to give the user this kind of relevant items so well that's all about regarding solving a problem based on a priori algorithm in determining so hope you guys enjoy educating watching this video please do like share comment and most importantly subscribe to my channel thank you very much for watching this video 
5TDbJJ91BzA,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-01-25T23:30:36Z,Data Mining (Spring 2016) Lecture 4,https://i.ytimg.com/vi/5TDbJJ91BzA/hqdefault.jpg,UofU Data Science,PT1H17M19S,false,348,0,0,0,0,"actually that's what you should be doing Oh as a journal Oh creator sanyo everyone's let's get started so uh so just to start I just want to remind everyone that the homework is due on Wednesday um just from I knew that give turning through canvas one person has from campus I still need to fix all do it as soon as i can after class if not let me know and it's doing to 45 so you have 15 minutes to get to class after afterwards right no so if you have your question there some the office our times are here Michael moved his office our locations is now on the third floor this is where a bunch of other tas our files i think still upstairs but it's right up the stairs close to where the other offices are i think okay so any question about that there's been some good questions on the discussion group we train and quickly yeah something a last question on the very last part question for ya former specific number of her son value is the how many calls doesn't take as a fun yes so I'm looking for something like a formula now you know so so if you're used to like as an occupation that's probably like so but I'm not looking for specific values you know a good way to figure out a formula if you're not trying to do it is to do it for a range of specific values and try and find the pattern and then often and fitness it should be clearly you should should be able to feel is going on yeah but if you do have a formula for Jim units that the number of runs per and that's good enough for did you want yeah yeah if you have a formula that's great I'm great so um let's see so the other thing I want to talk before we get on with the lecture is the proposal the proposal the project proposal is due a week from today some people have posted things on the discussion group that's great to trying to find partners I just wanted to talk a little bit more this for a few minutes about what the proposal is about again just to kind of help facilitate some questions Campbell questions you might have so the this scale the project is you know does not need to be kind of huge but you should have which should be something you should have fun with hopefully and so if you look the the grading again is broken into these these are various parts and every part except for the final report you know more or less you you just need to follow the instructions ok follow instructions and you get the points the the final report that's where if you did more i'll give you more points than if you did less right other things there's kind of it's hard to judge the quantity because there's so much bearing degree of stuff but by the final report there and so you know think about there's the basics of what you need to do and those i think we'll get roughly like an a-minus raid and then the UM the if you go up above and beyond that you'll go up to the a great so if you just do what's in there you'll do fine but that's where if if you kind of do something much beyond what is required that you'll get more Andy and I still have been set the final deadline for the presentation wolf we'll figure this out at some point it could be during the time of the final exam or maybe some or for the for the poster presentation we'll figure it out later in the semester if you have concerns about those those dates let me know ahead is there a problem ok so the scale the project you know it should scale with the size of the group almost all of you if not just for all of you a couple of you semi emails today and heaven always gone back to you I will there today but almost all of you will be in groups of two or three and so the scale the three-person project should be a little bit more and a good way to think about it is you're going to you should be focusing on some technique from class that you're going to be using or you're going to be extending so the various simple and basic projects and these are not always the most exciting ones but people can take them pretty cool is to take a few techniques from class and variant something and compare them on some real data sets to experiment and had the more people on the group the more types of things the more better comparison I have to do but it could also be an extension something that you come up with on your own or something more advanced that is beyond kind of this more vanilla stuff that will walk through it much more detail but something all kind of mentioned in class but won't have time to go into the full depth of those are other kind of good things to look at the building to build a project around and so if you're still trying to come up with a topic you know I want something that you're interested in I don't want to tell you topics because then people all do the same thing you know boring and you won't get as much out of it but if you're still can't figure something out and you say you found a group you're looking for a topic a good thing is to look at some data sets here are some websites that may I created a while back I think they're all still so updated but there are lots of big big data sets here you can browse there are plenty of other places online that you can find interesting data sets and you start with those and then then do something kind of think of something cool to do with them if you're not sure what to do with the technique for us you know that's that's tricky try and remember from the first time overview something you don't have to have all the details at this point I try and do something you think would be interesting to analyze ask me questions I usually try and imagine a day if not sooner or come and meet me after class or an office hours so what I absolutely need Oh a week from today in the president also as is who's in your group kind of roughly what day did you plan to use that not a lot of detail that he'll be in the next step in the data collection report what structure do you plan to mind kind of or what technique do you plan to use kind of these are the basic idea you don't have to have a lot of details here um why do you think this will be interested right and and what will I find interesting about right if if it's you know you're welcome to do something that maybe someone's done before but try and make it so it's more interesting i'll probably give more grades on the fun reported by interesting right so you know I I'll try and judge you on the merits of everything but all equal if I find it more interesting or probably get a higher grade so so these don't need to be full in detail but just a rough idea and from experience right read through projects we've seen projects for for a few years now I'll try and give some feedback and I think this will work for I think you might run into a problem here or there or you should tweak it in some way I'll try and get some high level view to help give you feedback but probably most of the reports generally two-thirds of on just are great at this point and I'll just kind of let you keep going with it but always feel free to ask me if you want more questions or have something specific that you'd like to know about alright so ending any questions about this now okay great so so feel free to talk to me more later alright so on lecture hey okay good so today we are going to be talking about technique called mini bashing to the side came back here hit the windows button click desktop take great things this second of the pharmacy great okay great thanks okay good okay great thanks that I'm still learning how to use this this is the department I'm going to get my own hands hopefully by next weekend okay great um so but this is I think if it guarantees these things if these are the best way to write on a big screen like that so all right so some min hashing is going to be a way it's basically going to take some objects that are sets and turn them into some other objects which are vectors okay so it's going to be them and you know any time you do this transformation or not any time but often you're going to get some some air in this process but why is this going to be useful well so we're going to kind of have three steps here so the last lecture was ongoing from was on these k grams and basically this was taking these um thinking these documents and turning these into sets right so that was kind of the point of this last lecture moving to take take document and we can turn it into on a set right word it was you he was um and the set contained these key grams which were these me if it was a three grand over words it would contain all the sets of consecutive free work each of those was an object in a sense okay but we but again it won't matter so much what the sets are there just we can as as objects from some some high-tech space right so it could be we have a set to be 12 and and seven as a subset of say 1 2 up to say 100 something like that but but typically the cake grams these sets are much bigger instead of 100 this is like a million or a few hundred million of all possible sets of words of three words that work ok minimize you will be learned from sets into vectors and then the reason for this is then next lecture we'll be talking about locality sensitive hashing and and so this will be doing two things one it's it's going to take this min hashing and generalize it to other sorts of objects and other things other than sets so we can take as input but in particularly it will work with min hashing and then it will allow us to answer two questions editing and so the idea here is that you're going to have um let's say we start with some sake tax which is going to be a large number of these sets ok hand and so then the first question is we're going to have some some query object this is a query and what we want to say is is Q in the set X so we want to answer this quickly alright so the first way we can do this would be just to come check q against every object in the set every object in acts so so each x think of remember going back to hear each axle one of these documents like it's a webpage or it's an email or it's another assignment that we've seen in the past so we're checking to see you have a copy front and we want to check is your assignment matches exactly one of these other sites right so there are teachers if there are teachers in like the English department they have a history of all of these essays for for college admission essays there's a big kind of a full summer always old college admission essays and they can see if yours it exactly matches someone else's very close to it great wanna do this quickly without comparing against every single one because that's going to take a long time the other thing to think about is are any to come SE q QQ prime in in X on the same right so are there other any two objects in in this in the set X which are have the same or very close to each other right so maybe you've taken another essay another homework and you change a few words a few names and variables but otherwise they're the same is this something because some hot chick and so we wonder this quickly now in this problem is even harder this one we would need to have checked up against every cue against every document in X here we need to check all pairs so this was a size egg this took a linear time in in the naive way district n square time to check out para transit right so so so if we just have sex there's no clear way how to do this but if we convert them into these vectors then we'll be able to use this lsh technique Wednesday which will show us how to do it so basically what we'll do today is we'll talk about how to essentially think of pre-processing these sets with into these vectors and then we'll use this as a representation and then we can use it for these sorts of applications will we'll talk about wednesday okay and so we've already done some sort of modeling of some sort of approximation to get into these sets so a little bit more when we converge these vectors some error begin the process should be so bad okay so it's not going to be an exact a one-to-one transfer like we won't be able to go back this would not be possible to go back to the sets from the vendors but that's going to be fine we're just going to want to answer these sorts of operation some sort of distance operation again we're going to look at the Jaccard similarity between two sets say s1 and s2 and we're going to get something like an approximate Jaccard similarity on the corresponding vector 1 and vector to that correspond of the sets and we want these to be a roughly the same so it's going to be a randomized algorithm and they're gonna have the same expected value it turns out and and that with some turn off hopping bounds we talked about a couple weeks ago then this will show us that it's going to give us a pretty good accuracy okay so that's got the general plan today the general plan is kind of this is what we're going to cover today ok so now but how I'm going to progress and doing this is we're going to generate so if you think of a vector so we're going to go from some set into this this vector we're going to call B and say v1 v2 the K is going to be the elements and what we're going to do is we're going to talk about most of the lecture just creating just one of these elements and we're going to create all of them in a way that's Heidi independent so each of the elements are going to be created the same way independently and we'll talk about painting one and then i'll talk about how to do them quickly do we do all but they're essentially you just do the same thing for each of elements ok so I was just going to create one object of this vector and by the fact that we have kale and pretty independently if each one works right on average then if I do a lot of they're going to work even better on average and and with very close to they expect to die will show the expected value matches ok so I'll be more precise here but this is kind of a preview because sometimes kind of seeing how this all fits together will be kind of fun it will be to get lost they don't give you this is highly will be ok so again we're going to be talking about the Jaccard similarity so let's review this again quickly these cards in line between two sets again remember this is a section the size of the intersection B over any Union d so if the sets um if it's a equals 1 2 and V equals 2 3 4 then this is going to be equal to hear the intersection on top is just going to be odd to the size of that is one and the Union on the bottom is one two three four and so overall this is going to be equal to one-fourth right okay so that's the jacquard similar we talked about last week and that will be the similarity that will preserve between the sets and will preserve this in the representation of the vectors okay and so we're going to go through an example now we're in particular let me just get a new page here look at s1 s2 it's going to be just 93 s 3 2 3 4 6 and 4 it's going to be one or 6 and 0 and I'll go through an example all turn these into vectors and I'll walk through how I do it and then we'll and actually I'm going to turn these these sets into a single number okay this is going to be the element of that vector to remember up here I'm just going to do this for one element of the vector and then I'll repeat that so all I need to do is create a single number that represents each of the sets okay so so the way and do this is by doing a 80 yeah so the first thing to realize is that there's a different way to represent these sets so we talked about text and then into these commands as sets we're going to go to vectors but before that we're going to go to a matrix representation and this will be kind of sometimes convenient for how to think about how this works on why it's going to work so let's say that the domain is he 123456 and so I have different sets one okay and so now this matrix which is going to be in here is going to have four columns and six rows then drawing this just as a chart and what's going to be inside each element is going to be a one if this set contains that value and is 0 otherwise yes so this would be one way to think about it okay and so now in it in order to get kind of I want to get kind of represent this as a single number so now I think this is a major excites a chart but you've read by rows and columns into matrix and so the thing about this is that this is a bad representation compactly because it's going to be mainly zeroes right if you remember with the UM so it's called very sparse if it's mainly zero because the because most words those key grams are not Knievel's document so like the third column for the second column for the second set only in 1k gram in it meaning maybe it has a few but there are lots of possible things that could be in here so so I won't actually use this representation to store it in the algorithm but i will use it as a as a way to think about what's going on okay so now to do min passion we're going to do this in in a few again in a few steps so the step one is is is going to be to permute the rose okay so if you remember these these elements when it says one or it says to that's not really meaningful it represents some k Graham and I haven't which one right and I'm just representing this abstract would but the ordering of these like shouldn't matter they're just sets right sets don't have an ordering notion so that means that I can reorder them and I'm going to reorder them in in some way that's random ok so I'm going to randomly reorder these rows and so for instance I could do 256 143 ok and then if i look at the quantities here in this matrix s1 s2 s3 s4 now it's it's just permuting the rows of the matrix kind of squished here okay so it's going to be like this one's going to be one is in there five is in there six is not in there one is in there 0 is not and three is not this is 0 0 0 0 1 0 1 this is 1 plus 0 1 0 1 1 0 catch me if I have any mistakes here and all right so I've randomly permitted these roads ok so now I um now in order to get a value of out of each of these on step 2 i'm going to take for each set find the row that is the first one ok and i'm going to give it the corresponding value ok so what's going to happen is that s one and it's going to be a this will be some sort of hash function not a random hash function so this will be some on like locality preserving hash function so this half from m1 is going to give me a value to and this is going to be a knight one but it will so 44 m of s 2 i'm going to get the value 34 m of s 3 i'm going to get the value to again and for a half of s4 i'm going to get the values 6 okay so the two I did a random permutation s1 this is the first 1i go and see a value to press to this is the first 1i go and see you value 3 that's 3 1st one go see value to s4 first one long see value six okay so this is this is the value of this of this vector this is the value I put in the first entry of the veteran seems kind of weird right yeah ok so the next okay so and then I'll just repeat this I'll do a different random permutation for the next element of the vector and do this again with a different random permutation and I'm going to get different values here right and that's why I fill up the whole vector okay but let's before we kind of talked about that let me state a property that my estimate of the Jaccard similarity this is my first estimate with just one value I'll put a one down here because I'm only going to use one entry of the vector the first estimate between sets s s I tennis jay is going to be one if M of s equals M of SJ + 0 if they don't yourself okay so think of this M is a hashing I'm hashing onto a value okay and if they fall in the same bucket I'm going to say that they have one and if not they become Jaccard similarity of zero right so Jaccard similarity is usually between 0 and 1 fill the more similar they are the larger the value the lessons are 0 and I'm saying it's either 0 or 1 i'm being very strict here right it's either you pass or you fail right so this is a very crude estimate okay but it's going to have the property that the expected value of the Jaccard similarity the approximate one is going to equal exactly the true Jaccard similarity so let's going to have exactly the right expected value so it's either is your one but there's some probability is going to be one and all the rest of the probability is going to be 0 right so if the prompt so it turns out that the probability is going to be one is going to be equal to the true Jaccard similarity that will give me the rain expected I does that make sense might say the probability that they fall in the same bucket is equal to the Jaccard similarity then this will be true does everyone believe that is that because then to get the expected value I say the probability is 1 times 1 plus the probability at 0 x 0 probably in 0 x 0 is always 0 so it's just the probability is 1 times 1 you the one doesn't do anything right so you just need to show now that the probability that they hatch together is exactly the charts alert okay so before I talk about how to use this let me try and prove this fact okay so so is everyone so usually it's a good idea to understand the proof it won't be too hard or maybe once you see it it won't be too confusing but both but before I do it think to yourself how would you show this why would this be true why not yeah so well I could use one of these different hash from these different summers like unlike the dice similar being step if I use that one well those don't give the same value so why does this work for the Jaccard similarity and not the dice similar so something specific about that how the jacquard simply works there's a way to do a similar thing for the dice but it's not quite as as as as simple as this one and this is what commonly would but people using this set okay so let's again review the Jaccard similarity between these guys and where this is equal to the intersection between SI intersection and SJ over the union between SI union SJ and in this denominator it will be useful to write this as SI intersection intersection s j+ SI symmetric difference as a jet okay so this is equal to equal to the union right remember the symmetric difference does everything that you can accept intersection so that works okay so i'm going to show that this is equal to the probability that they hatch together okay okay so um let's let's consider for all the objects so so let's look at some let's call x x equals 2 s.i union SJ and so I'm going to break this into this this into three parts this is this is equal s by intersection s j+ SI symmetric difference of SJ and seemed to have turned off I can try to touch off again so it's going to be these two sets um but if I think of this if I draw a picture here this is si SJ and it lives in the space here right so the intersection is is this part will call this value a the symmetric difference is here will call this part b and then the everything else is is out here okay and we'll call this part c and this is not for the vents okay so so now let's think about what happens I'm going to do a random permutation of all of this domain so I so I'm going to do a random reordering of this domain and then I'm going to look at the set X and I'm going to see which object is going to come on the top so I'm going to random reordering on this domain in which i'm just going to come on the top right so I'm going to get some ordering and these cells here okay and so a lot of the cells in this ordering are going to be from the set see some of them are going to be from the so some of them are going to be from the from the symmetric difference meaning that they're in an object SI for SJ but not in the intersection right and that's the orange ones of our argued beef from the intersection okay so so now if I look at of the ones so now in order to get so if this is the ordering then in order to get these hash functions I'm going to ignore all the green ones all the green ones I'm not going to ignore because I look at the first nonzero because I looked at the first non-zero in this ordering it's the only one that matters is this red one the green ones don't matter anymore just the red and the orange ones the ones from the settee and me okay and so in this case the first one in the score during was a red one but it could have been an orange one right but this was a random process okay so if the first one in the bordering is a red one is this are they going to have a jacquard learning estimate of 0 or 1 0 and it feels an orange one of within one correct so I need to know know the probability that the first object was an orange one it was promised set a not far from the set of size eight not from the set of size D and the probability of that is going to be on the probability of em i equals am j is going to be equal to the size of a over 8 plus B right so the number of objects entering the intersection because it's a random reordering of these just the red in the orange objects and if I have to pick one of them at random to be first if I happen to pick an orange one from the intersection I get a tirsan from the intersection of the sets I get that the hash functions are the same if I think one that's not that's then it's going to be then then I'm going to get the hash functions are different so now we just realize that this this intersection here is is exactly the AAA is exactly the size of the intersection and B is exactly the size of the symmetric difference so the probability that they fly is exactly the jacquard similar and that means I get the right expected value ok so let's have a short q proof okay so I've got this very simple estimate now this very system has been for one value so now i want to every every lecture I tell you it's going to happen um okay so I instead now I want to get a better estimate of the Jaccard similarity so now I want to say that my Jaccard similarity with the vector of size K hey gonna be here between two sets a and B is going to be the sum over i equals 1 decay of one if m i pay equals M mid-40s otherwise and this is the book / k so we can take the average of these right so if i take the average of a bunch of things with the right specta value it still has the red expected value and it turns out that this is exactly the property you need to plug into the no me so you can say something like the probability that the Jaccard similarity between two sets and the minus the Jaccard similarity cific approximate one its approximate one between a and B that this is less than some quantity epsilon epsilon is some error bound between 0 or 1 so say like point point one right so think of this as 0.1 so I'd say that this is going to be less than 2 times minus 2 epsilon squared over K times the kind of the this Delta value which we knew the random variable that we're taking the average over was between 01 so this was k x 1 one squared okay and so if you said so if you want to get this to me this be as small as you get k larger you want it to basically balance this epsilon so if you set k equals to approximately 1 over 2 epsilon squared and epsilon is like point 1 so about Beyonce about 200 then you're going to get roughly a constant probability of failure so maybe so probably what you want is is probably around a thousand you're going to get a that the difference between the Testament and the true value is off by more than often is we off by no more than ten percent right so a thousand 4k is kind of a good guess or maybe more maybe 10,000 okay so you're thinking okay this I'm turning asset which in all the examples had like three or four values into it into a vector of size ten thousand right so that's effective what I'm doing in each of these things involved from using this huge sparse matrix so this seems like okay this is good could be a little bit wasteful right so where this is useful and where this was developed was for the very early days of the search engine this was like pre google and they were trying to do things where they're trying to remove these duplicate web pages and so this was the easiest way to do it so instead of doing all pairs they were instead spent time to pre-process each web to a vector of of integers of size a thousand or ten thousand and then just compared these vectors and we'll see next lecture we'll be able to sort through these very quickly okay but we're going to pre-process the sets into these vectors in the and the key grams you would get with web pages if you had a longer k grams could be much larger than that think of this was four characters you have many more than 10,000 characters on mostly yeah when you randomly permute the rose do you have to have a rule that says don't choose the same element as the first element the next time oh no no army and the fact you don't know the random permutation is chosen without knowing the actual documents that's fist so you don't know which ones are the first and if you look it back at the permutation in this example if all of my documents look like document on s2 right then I don't really have to go away isn't all these zeros before I hit the first one and since these are going to be really sparse most of the documents are going to hit something fairly different when they hit the first one so I'm not going to know which one is really the first and in effect ok the next thing I was going to talk about is this process of doing it turning into a matrix in for meeting it is really slow and expects that this is not how you want to do it I just want to describe it this way to kind of give you some intuition of why this works what's going on there's a much cleaner way of doing this that's this can be much faster to do ok so Oh before I go into that one reason why it's useful to think about this permutation is all we're doing is we're using the just the top just the first one in here but there's a lot more information if if if if to these columns have the second the second element does not match with the first and the third one that tells us something more instead just like in the first one we can look at the top several of them and this will give us a much kind of a richer hash hash function and and you can show similar approaches with with this technique as well and this has some slightly better properties a little bit more complicated but in in some case if you want to kind of supercharged an algorithm is actually looking at the top five or ten is going to work better than just the top one and I gave a reference in the note 2 up to a paper that starts to talk about these but it's it's a pretty technical paper but that might be a cool thing to explore one of one of the PhD students here kind of a momentum is this difference instead it was for his whatever application he was using it for from some big data processing on that he got a fairly sizable fruit meat using the top cake okay so the next thing to talk about is this is how to do this fast yes so how would we create these vectors in a way that's faster in fact how would you do this random permutation of the matrix what's what's the right way to do that how would you we talked about you know there's a couple things that I talked about so far then some operations ask you analyze in the here you look at it opens that location window yeah def use your finger you swipe from the left to right back through this as your finger okay great thanks we talked about couple operations one is a rampage you can generate a random bit just gets you a zero or one but what's most common what's fairly useful to use and well with mainly use this in class so so this will generate a random essentially a random um double or a random fractional value between 0 & 1 okay so internally what the computer will do is it will generate a bunch of random bits and then it will bring them together is like the binary decimals or something like that there's various other ways of doing this but so it's it's generative generating random bits underneath it but it will give us a random value between 0 to 1 and so but we need to do a permutation of the speakers how do we do a random reorder means yeah rather than actually changing the matrix couldn't you just generate random tomorrow indices like in a few random airway disease in order that don't duplicate each other than just go to those rows in the original matrix rather than actually a festival yeah so so you could generate these random row in disease now so it's an interesting idea there's there's an issue with this and that the rows are going to be really sparse so this can happen is I'm going to generate this random row and then probably it's going to be like think of this is a for gram of characters and the characters are XZ r q or XKCD rights it was supposed to be a random 7 characters so and probably nothing has X case CD in it now there's a comic but when the in the comic when they started the name of the comic he picked those random characters because he thought no one else but whatever use those okay so I'll just get back on topic thank you switch okay so so so this would probably be fairly wasteful we don't want to generate the random rose we're going to think of this from the perspective of the data every time we see something in the set we want to generate something random about that to process and ethics so kind of flip this around some okay so what we're going to think of doing is we want a hash function from this domain me make my notation work some hash function from the domain that are set is in so our set our sets are subsets of this sort our sets are subset of this domain so for every element in the set I want to hash it into its rogue ok so I wanted for every helmet I want to randomly hash in into Wyman's roads now I could have collisions of I hit that's right this could be a problem well so instead of doing that why don't instead passion to a random value between 0 1 so some some random value between 0 & 1 so well so I'm going to use this is random hash function actually okay let's that's that's there's some you're going to do this at low level details but you don't actually need to do this let me take expect ok so I'm going to go if I do it to end now there's probably going to be some collisions but as long as they're not the top think lighting that's going to be okay instead I can do something like 100 N or n log in like I don't expect like I know from the from the from the coupon collectors say if I did 20 hundred and login that I don't expect to have any collisions for instance right so I'm going to generate i'm going to create a random hash function here right ok and so then if something is the minimum value if for everything in the set I just keep keep the minimum right so now so I can think of for every entry in the matrix in the in the vector I'm going to have the the J pantry each of those is going to get its own hash function and so then the my algorithm is is going to look like this so let me write off the how rhythm the code room for all the elements in the set Castle then for all of my j equals 12 okay so this is all the entries LVS spell that right in in the vector ok so for all the entries of the vector that I'm going to say if my hash function J the Jade hash function to absol assume i pay these hash functions of X is less than V J then i'm going to set V J is equal to the hash and so i'll start by with a vector of v1 equals essentially infinity be to some very large number essentially okay so this is my vector and then if I see something that's um in this case smaller so I'll order things so if when a hatch to something smaller then that's the smallest thing I've seen so far it's hash to some value between 1 and 100 and log in right and so I have to some value and so then if it's the smallest thing I've seen so far I keep track of that right and so I can also store this is the vector value I can store you know there's some slightly different ways you can implement this you notice up here I kept the actual element of the run so this would be the valley keeping the value X instead here but I've kept the ordering in this way I've just kept the ordering if I want to keep the value X then this set the beat enter I I could read we name this variable and have this be like this uy j py j Vijay is equal to ask so i can do that instead if you want to match the notation i had earlier but these will effectively be the same thing to only be the same value if they have to go okay so this is a much clearer algorithm I can just use this hash function to just maintain the smallest thing I've hash two so far okay and so if you've noticed I did this thing where i looped over so I'm looping over the all of the hash so I'm looping over all the hash functions here instead of here I could have swapped I could have swapped these two for loops and a board of work the same right it didn't be better to swap the two for those or if they only have any questions about how this works so he let me go through a small example what's good yeah so just I'm just trying to steal understand so you recently like you take one last function utilize everything in the set and then you find out you store the minimum making a hash that's like if you want go to your next hash function and then X except I'm reason for every element I go through all the hash functions involved are leave you do the other way yeah so i can swap but but before i talk about that just are there questions about how that how this how it works maybe i'll go through a small example right so let's just look at one set s so let's look at it set here this is going to have in it the elements are going to be call them a and B and C okay and so then going a half function is going to go that I'll just define it on on these values HMA so of two hash function H of a is going to find this way let me this so H of one and each of two now a be MC and so this will be the definition of these hash functions 481 it's it's going to map a 27 and h2o map 822 HB is going to map be 23 and this one's going to back feed to 10 and this is going to nap see to 22 and h2 a map see two for one ok so now i'm going to start with my vector has values infinity infinity and i'm going to go and i'm going to process so this would be my vector here in a process a with this hash function with hash one and it's going to give me a value of seven which is less than infinity so i'm going to fill this in here so this is after I process a and and hash function to will give it a value to which is less than infinity so I'm going to get a value here ok now I'm going to process be hash function one is going to map it to three which is like seven so I'm going to replace seven with 3 i'm doing the version with the blue not this red version ok and then 4b is going to map to 10 h 2 which is larger than two so I keep to right then I'm going to do see seizing a map to 22 its larger than three so I ignore it so I keep three and NC is larger it's going to map 21 which is less than two so I'm going to keep that right so my vector the end is going to be three more this and this will be my representation of the whole set of the whole set yeah yeah and if you have multiple sets you just keep doing this for each set or placing whatever meanwhile each set is going to be a different vector so this will be the vector representation of the set pacified another set that's prime rights to going to give me a different vector so remember I'm mapping veteran sets two vectors and these vectors will work within this lsh scheme talk about on quizzing and the house values are determined way yeah so the hashcode us were determined randomly remember of these random hash functions worked and then the hash function itself is deterministic but the the mapping is ready so I think of your picking a random seed which which map these and and so instead of these values a B and C these could be strings or something that you're mapping into these integers okay all right so so so I do this for each set and if I had a bunch of sets yeah I'm going to get a bunch of vectors of each effect set and get a new vector this is to convert from a single set into a single vector presentation so okay so so why would I so now if I swap these two is two lines if I swap the two for loops right then then it's going to get the same result I'm just going to all first process this column and then I'll process this power instead of first processing this row and then this road that this one it's generally kind of its is usually better for very big sets to think of processing it even this way oh I'm so why would this be so so so typically what you do is you would think that that even though you know we need something like a thousand or ten thousand of these different hash functions those are going to stay in in memory of the computer whereas the sets or things that we're reading may be reading as they come in or we're reading them off of disk right so these are going to be bigger objects that were compressing or we're scanning a web page as well as we're going we'll talk about crawlers later later in the class and these are these are companies is a program things go over the web and read all these web pages if you do it this way only needs to read the web page once in fact if you remember one thing about this why we use this as a set the set it this would be this should give the same answer as the same thing as if the set was changed to a meet a see right if I wrote this is my second stead meaning i came across these k grams when i was reading a web page it should give the same representation as a vector right because if it sees a again it's going to go to the hash function and it's just going to be the same value so it's not going to be any different if it's the lowest value little keep it if its larger than we've seen before it won't keep right so I don't need to go and pre process things into a set as I'm for every element here as I go through this step here but if I go through this step I can do this as I'm scanning over a web page I can be sliding have a sliding window over the King Rams i'm doing and immediately throw them into it into all my ash functions and update my vector so i can do this as i'm reading over the web page as our meeting over an email to process it right so so that way you don't have to read over multiple times reach actually so so this works to create these vector representations kind of on the fly as your kind of quickly processing this this texting everything in the cane graders and the set representations can kind of fit into this pipeline to immediately calculate these vectors and this will be kind of the internal representation this is just a bunch of numbers now representing these these these webpages or emails okay um so ok so this take one more thing say just what is the runtime of this how long with with this take to draw on after this discussion so it's going to be so if I have a tree process as a set it's going to be 0 as the size of the set times K which is the number of the hash functions because for every object you know just the two for loops right so it should be very hostage in the set you throw it into one of the hashes um ok so so if i do i have used k in two ways with k grams let's say it's uh it's it's a it's a it's a g gram ecstatic if if I'm if I'm doing a geograph over a document with with N and words in there and characters how long is this going to take so I have a document let's say I'm doing it a group of g of g words and i'm doing these these G word ramps over a document with with n words it how long is it going to take to create these vectors page here right so let's say I've got a document I'm going to have n words i'm going to create she words Rams and we're not going to have a half some tools what is the runtime going to be in terms of the number of words in terms of these dots mmm ok and choose and choose jeans and choose g as k which is me roughly end to the energy that's cat does that not be pretty slow like it is if g was three vets and cute it's gonna take a long time I don't think that's going to be right times G times and today that one's and times G times K so that means that for every gram I need to look at that's also going to be too too large I'm not going to need to spend that much what kind of lightly and / g um let's gonna be too small oh okay we're getting too and this is too big and / g x k this is too small me n minus G times K or you can just ignore the Jeep that actually the g doesn't really play because if i look at a document so so so let's just write an alphabet and do it horrified so if i look at at at the three grams right so for each each each letter there's one there's one on g gram that starts at that location and then at the end this one there's there's there's nothing that starts to so it's actually being technically n minus g plus one ok but but in fact these are constants so you ignore them the big o notation would be n times K right so the the size of the g gram is not going to come into play right so this is going to be really fast to read over this well ok actually if I go back if I'm a little more careful this is actually gonna be the right answer because what's going to happen is when I push this into the hash function this hash function here is going to take longer to process something that has more characters or more words right so this hash function itself would roughly take home of G time so this would take longer to run the process if I had if I had a larger brain ok so actually I will go back and say that this this was actually the right answer so what if you get a web page or something that doesn't have any g like you're doing a 100 gram tiny little web page that has one word on in life what then you've chosen the wrong value G remember G is a model enjoys and you usually get to choose this so if you're going to be dealing with documents which are very you know it is sometimes feel empty then they're probably not very useful documents so if you ignore them that's probably okay so if you know there are lots of weird things you can do like you could combine vectors with different values G create different vector representations and then query under both of them and kind of title by the results of various ways or build up concatenate two vectors together that you build the various jeez and this sort of thing should probably work but if you're worried about having various things you know and often you'll you'll look at one type of data that will fit one category or you'll use gene to be some small some small value like a tree or Ford and if you only have one word that you know not much you can do okay so I finished a little bit early today do have any questions about how this well maybe before we leave just think about how is this locality sensitive hashing going error it's always good it before you hear a lecture before year fruitfully trying to think how would that work why you know how could you possibly do this so think about this locality sensitive hashing how would you use these vectors and in particular what these hash functions mean to quickly answer these questions if you have a set of all these documents and you have another one how would you quickly find all the all of all the nearby bones if you have a bunch of a bunch of documents how to quickly detect a few of duplicates so you can't remove one significance from the set or look for two assignments turned in from the class that basically the same same after all right so how would we use the structure didn't so everyone in here a scene about anything years as has done like a basic data structures class right we've done where you've done binary search right so if your binary search you want to find nearby things you can sort them and if you want to do this query you just use the sort of order of log the binary tree right if you want to find two similar things while I third in the sort order they would have to be right next to each other but now we don't have a single ordering so that's going to be the challenge so we're going to have to think a little bit outside the box or we can't quite use those ideas so that's it's all in there today thank you but today two three or four yes "
SbMBv8nU5pE,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-04-20T22:30:00Z,Data Mining (Spring 2016) Lecture 25,https://i.ytimg.com/vi/SbMBv8nU5pE/hqdefault.jpg,UofU Data Science,PT4M58S,false,106,0,0,0,0,it's called a wspd which is a well separated wealth separate here decomposition of a graph which is is a mouthful and a technical description is kind of a little bit painful so I have four minutes left so I'm not actually going to tell you exactly what a WS VD is because it would take longer than that but I'm going to tell you how to construct which i think is what's actually cool about this if you go back to the range or the nearest neighbor part of class we talked about these these data structures for high-dimensional nearest neighbors one of the things we talked about was this quadri and the quadtree works by you started with with with the data set and it was inside of you assumed it was inside of a a unit square could be unit knock something higher dimension and what you did is you kept dividing this up into these hierarchical cells split geometrically so there are squares at every level going down okay and you keep keep going down here and um and so forth and so what you do is you then once you have these these on and so for each cell at each level you're going to keep essentially one representative somewhat arbitrary to at the top level I'll keep these representatives and then at the next level down I'm going to keep each cell gets one representative of this one and then and then each level down if it doesn't have is representative already you keep going down each one gets one representative ok and now i'm going to basically draw edges but only where I need to ok I've only had raw edges where I need to draw edges and so basically each of these well within a hierarchy each one is going to go down to all of its children you're going to get these properties going down maybe there's a central representative like that but then I'm only going to draw other edges and so the number of edges here is still linear in the number of cells because it's got the tree structure I'm only going to draw these cross edges if I need to write so I can draw probably from each of these blue cells I can just draw to the green one I don't have to go to all the blue cells at this level because going to be green wanted and then going down the hierarchy is going to give me a good enough approximation okay and so the nice thing about this is that of the there's there's some value can work out that the theory golf this is water stood you don't need too many total edges in here you break it up into these hairs in this nice way so but you know vertex has too many has too high of a degree because it kind of it goes across and then goes down the hierarchy is needed and you can eat pretty easily again kind of just really decide whether you add these edges at different parts of the higher and so this this comes up at a lot of places this is like this quadri if you seem like needs multiple methods and scientific computing basically the same idea um there's this um this kind of what if you do like was like the like the fast Gauss transform or something uses some basic kind of decomposition like this as well this sort of trick comes up all over the place and you can also do graphs parcel patient's all right so that's all for today um really enjoyed kind of giving the lectures again so kind of I'll see you all on the enter exam on Monday and don't be your homeworks in your poster so 
m9Az2r025dA,28,Plenary Session delivered at IIIS Conference on July 2012,2012-07-31T21:21:58Z,"Dimensionalities of Knowledge Discovery using Data, Text and Web Mining (Prof. Richard Segall)",https://i.ytimg.com/vi/m9Az2r025dA/hqdefault.jpg,IIIS .Channel,PT39M39S,false,376,2,0,0,0,"sir richard Segal he is with the with Arkansas State University in the Department of computer and information technology into college of business professor Segal holds a BS and ms and mathematics an MS and operations research and statistics from Rensselaer Polytechnic Institute in Troy New York and PhD in operations research from the University of Massachusetts at Amherst he's been on the faculty at Texas Tech University the University of Louisville the University of New Hampshire and the university of massachusetts at lowell and west virginia he has published in several professional refereed journals his research interests include data mining text mining web mining database management and mathematical modeling his research has been funded by the US Air Force and by NASA by the Arkansas bio besides his Institute in the Arkansas Science and Technology Authority he's a member of the editorial board the International Journal of data mining modeling and management in the open cybernetics and systems journal and the open medical informatics journal and he has served as the local arrangements chair for the 2010 mid-south computational biology and bioinformatics Society conference professor seagulls the keynote address is dimensionalities of knowledge discovery using data text and web mining professor richard seagull I first want to thank you for the kind introduction of my talk which is entitled dimensionalities of knowledge discovery using data text and web mining whoops I don't how do i go back here okay first of all I want to bring the audience up to level of what data mining text mining and web mining is and then show some applications of research that I've been conducting for the last several years okay what is data mining data mining is knowledge discovery in data or databases and its extraction of interesting information it could be previously known or potentially useful information that from data and large databases alternatively names afford data mining on knowledge discovery in databases kdd knowledge extraction data patent analysis data archaeology data dredging information harvesting vigeous business intelligence etc whoops I'm going wrong way here typical example and data mining is using customer purchasing patterns to predict what they products they purchase and where they're placed in the store bread and milk are obviously placed at distant locations in the store but things that associated with each other could be located close like flashlights and batteries and a classic example is a bear in the diaper example where they can move the display of the beer next to the diapers on Thursday or Friday nights when the husband's go out to purchase beers because the things that are visual and related related application is automatic detection of fraud such as in credit card usage analysts look across huge numbers of credit card records to find deviations for normal spending patterns another example is the use of a credit card to buy a small amount of gasoline followed by overseas plane flight the claim is that the first purchase ten test the car to make sure it's active here we have a picture of data mining and knowledge discovering patents we start on the bottom left you see databases then we have to clean the data make sure it's complete and then integrate the data and so then we have a data warehouse and then we have some tasks relevant data and data mining is the core of knowledge and discovery process and from that we extracted Atta fine to find patterns in the data then in the upper top we have knowledge discovery text mining is discovery of computer and new previously unknown information by automatically extracting information from didn't different read written resources and a key element is linking together extraction information to form new facts or hypotheses and or more conventional means of informations also known as text categorization text clustering concept entity extraction production of green in latex omni's an example is there were some written documents that were published in in the 1800s or so and they didn't know who the author of it was but they had another document like the Bill of Rights and they tried to find the frequency of the terms within the original document which they knew the author of and if they could find the same frequency of terms in the document that was unknown who the author is they could make a good conjectures who wrote the other documents web mining is the integration of information gathered by traditional data mining methodologies and techniques with information gathered over the world wide web and it's used to understand customer behavior evaluate effectiveness a particular website health quantify the success with a marketing campaign web mining lets you look for patents in the data through three different methods content mining structure mining and usage mining content mining is used to examine data collected by search engines and web spiders structure mining is the examined data related to the structure of particular website and usage mining is used to examine data relevant to particular users browser as well as data gathered by forms user may have submitted through the web transactions and I have an example later on about hotel customer comments now I'd like to talk about the background that I've done in data mining and I'll talk about the work that I've done in text mining and web mining in 2011 I co-edited a book and I have a copy right here I co-edited this book visual analytics and interactive technologies data text and web mining applications it was published by IGI global in 2011 362 pages I co-authored it with my colleague Quincy Zhang in my department at Arkansas State University and make I was actually his wife who's at the University wisconsin-superior and this is the novelty this book is headed we talked about large volumes of data and complex problems inspired research and computing and data text and web mining and however data analyzing data is not sufficient it has to be presented visually with analytical capabilities it's a comprehensive reference and concepts algorithms theories applications software and visualization of data mining text mining web mining and computing supercomputing this publication provides a coherent set of related works on state of the art of theory and applications of mining making it a useful resource for researchers practitioners professionals and intellectuals and technical and non-technical fields and some other work that I've done with dr. Zhang we published a journal article called review of data text and web mining software that appeared in 2010 in cyber nets which is in the international journals cybernetic systems in management science and we also have a chapter in a book this book is book published by springer-verlag entitled convert commercial data mining software by dr. Zhang and myself it's in the book data mining and knowledge discovery handbook edited by Odin Mayman and layer rokosz and I said to Springer völlig here I have a chart of different data mining software and we looked at gene site poly analysts SAS enterprise miner and no works predicts and I'll show some output of each of these in my presentation today you see that gene site is kind of narrow minded its statistical analysis and cluster analysis and neural works predictor is also very specialized statistical analysis neural networks and self-organizing maps poly analyst has also decision trees regression analysis cluster analysis and Association analysis and so does SAS enterprise miner text analysis is in poly analyst but it's in a separate package for SAS text miner here we have some text mining software that we looked at we looked at compare sweet SAS text miner text analyst visual text mega puter poly analyst and word stat and we looked at data preparation data analysis and resulting reports and computer Swede was only had two categories that we had SAS text miner was most comprehensive of that as well as visual text and mega puter poly analysts also pedal most of the features then we looked at web mining software we mega puter poly analyst also was available capable for doing web mining looked at SPSS Clementine click tracks in ql2 and looked at data preparation data analysts and click tracks in clementine had most of the features tailed ql to at least and meg apologize who came in second there my title of my talk with today was dimensionality of data mining text mining web mine and the novelty the research that I've done is that I took data mining to a new dimension and I did data mining of microarray databases which means we're looking at data at the genomic or DNA level and it said not the number for let me just give you a picture of genomic DNA and as you see this immense you know strand of DNA has an immense amount of data and in biology they take spots from gene DNA and they put it onto a glass plate and each of these dots has an immense amount of concentration of data and so on the right hand side here here whoops go back with my spot here there we go on the right hand side we have two different conditions attested condition in a normal condition and we can apply those conditions and generate different spots on a glass plate so essentially we have a glass plate of micro Roy databases and there are different colors because of the immense amount of different characteristics for each piece whoops normal way here okay so I presented a presentation at other places i'm just saying that i'm going to talk about data mining a microarray databases for bioinformatics using for selected software and my pictures on the top are because those are the types of databases i used i use abalone fish data I used forest cover data I use human lung data I use mushroom data so that's why I have those different icons that are on there okay let's talk about my career databases according to Hardiman in 2003 the use of microarray databases have revolutionized the way in biomedical research has been conducted that high-density arrays of specified DNA sequences can be fabricated onto a single glass slide or chip okay what is a microarray is a huge collection of spots that contain massive amounts of compressed data molecular Rays have been used by researchers and life sciences for genetics because DNA contains so much information on a micro scale each spot of a microarray this could contain a unique DNA sequence as I said before okay here we have a picture of a machine of a microarray data capture and did this these expression levels on what can be extracted can generate several thousands of genes at once and so here's a picture of you know the mechanisms that that's used to generate their microwave databases on a glass plate which for further testing and my data mining as well okay one of the distinctions of the the data mining i did is i said settled my talk is dimensionalities i'm doing the complete reverse dimensionalities that's typically used in a clinical study in a clinical study you could have tens or hundreds of variables and you could possibly have a thousand or up to a million cases but in a typical genomic case that dimensionalities of the reverse in the sense you could have 10,000 or 100,000 variables and only ten you know tens or hundreds worth of data so the dimensionality is very intense ok so in 2004 I studied in azmat database which stands for osmotic stress microwave information and I i took rep representative data for that I think if corn data would be represented for this this was funded to me by the Arkansas Biosciences Institute whose headquarters are on my campus even though it serves the whole state the software I use I use SAS state enterprise miner and the plant data that i use i used approximately 100 microarray experiments before at the University Arizona as part of a National Science Foundation funded project named the fundamental genomics of plant stress and representative of that could be used for biotech applications such as manufacturer plant made pharmaceuticals or genetically modified foods i selected corn because it's one of the most research products in the food and in feed system in as generic genetic as well as our dynamic properties of world document Kona safe stable information a medium for genetic expression has been shown to accumulate high levels of mana Claude antibiotics which are proteins not achieved in other plants and the UH Smith database is available for public access on the web and it contains about 20,000 experimental stress tolerances or EST used to produce these arrays and they considered as a component of the data warehouse capable of being subjected to data-mine in the planes on that database of rice barley corn ice plant and derivatives the azmat database the micro database includes four thousand EST s for corn ice plant rice 2004 pioli 9000 for a ribbit asst and EST against experimental stress tolerances to environmental factors such as wind rain drought I'm talking about here that could the amount of salt on the atmosphere and so forth and the data mining that I performed in 2003's for the representative factor of salinity for the selected plant ingredient chord is representative other plant biotech databases that could use a biotech manufacturer by a pseudo calls using clamps okay here i have a cluster proximities of of corn micro databases that taken to logarithm a2 and you see it's very intense and in 2005 I looked at mushroom data abalone fish data differences between these are that the dimensionality is a mushroom data was discrete data and the abalone fish data was numerical data and I use software of SAS enterprise miner neural wear and gene site the title of my paper that i published with dr. Zhang I'm lead author was data visualization and data mining a continuous numerical and discrete nominal value of microarray databases or bioinformatics and we received one of the three highly commended paper awards for the year two thousand six from emerald journals for this paper okay here we have a carrot some of the characteristics of the mushroom data and the data include you know information about the cap to kill the stalk the Veil the ring etc and here's something some of the research that I did I took the results of the prediction phase of neural network training using neural where predictors applied to a bonefish dater I looked at the results of classification phase of neural network training using neural work predictors applied to a below and fish data I also plotted the target versus the predicted values using neural word predict using a target value of the number of rings of the abalone fish data okay here I have a picture of the predicted versus actual values of the Avalon fish data and so one is pink and one is blue but the point of the matter is that they overlap very well the only difference is that some of the some of the spikes are higher than the others but generally the point of the matter is that the data mining that I did for this data was that it was pretty close and again look at the intensity of the data then I used gene site software for the variables of length and height for cluster analysis and i did a scatter plot of the abalone fish data okay here's the window of gene site software that they use for the cluster analysis for the variables of length and height which i've highlighted on the left-hand side there and here's a scatter plot as i indicated and that was the abalone fish dater and as you can see the regression is pretty strong and pretty linear then I did k-means clustering using the euclidean distance metrics that are selected variables of LinkedIn height for the abalone fish date or generally generated by the gene site software I did hierarchical clustering using the committee and Destin distance metric has generated by Jane site and I did principal component analysis in three dimensions including five clusters for the abalone fish data generated by gene site to the next jizz are respectively k-means clustering oracle clustering in the principal component analysis here we have the k-means clustering and the first column is length the green column is the height those we just have two variables but you can see the intensity of the data there but it even became more intense when I did the hierarchal clustering just look on the left-hand side the article clustering how intense those hierarchal links are because looking at DNA level it's very invention is very intense and I have three variables here of where there's three columns first is length second is diameter and the third is height and here we have the principal component analysis on axis number one I looked at five different variables there and they're color-coded respectively then in 2006 i looked at forest cover data in human lung data and i can one of the differences is the dimensionality forest cover data i use 60 3377 rose and the human kong lung cancer are used about one-fourth of that 12,600 rose and for forest cover i use 54 attributes and for human long i use 186 attributes which is above loop about three and a half times the number of attributes ease force cover data the forest cover data was available from university california irvine machine learning laboratory the human lung cancer was available on the web by the Broad Institute which is an institute in collaboration with MIT Harvard and affiliated hospitals in the Whitehead Institute in the software that I selected to look at this data was SAS enterprise miner neural were predict mega puter poly analyst and bio discovery gene site here I have some of the major functions for the selected software SAS enterprise miner did all of them decision trees regression neural networks cluster analysis and Association and so did Polly analyst neural where is strictly neural network stuff and gene site is specifically cluster analysis they're very specialized but it give a lot of information here I have a description of the forest cover data on and I also want to point out that i use qualitative and quantitative variables as indicated in the data type column the second column here I have a window of SAS enterprise miner that set up the workspace before us cover data set here's some of the decision trees later on I'll show you some of the intensity of the decision trees here i have a self-organized map for them normalized means of clusters for the forest cover data and the clusters are on the right hand side here i have a hierarchal clustering of global variation with the Lydian distance using gene site that's a forest cover again look at the intensity of the art of hierarchies on the left hand side there and I have one to several columns they are different for different variables i have about seven or eight variables on the bottom there here we have k-means clustering of the global variations for the Pearson correlation using gene site for the forest cover data and the columns again represent different variables here we have scatter plots of the actual forest cover data and you can see the intensity of the data there as well here I have self konos maps for the square deke Lydian metrics using gene site and again look at the amount of information that I'm getting here for the forest cover data these are four different different clusters there's about twenty five clusters right there okay here we have box plots for the first thirteen gene types using on gene types using gene site and this is for the human lung data this is human lung data look at the difference of dimensionality to this data and you know I have maybe ten variables on there but look how much more intense the human lung debt data is versus the forest cover and then intensity of this this is human lung these are 25 clusters again but again this is even more intense than the forest cover data this was self-organized maps using the kitty and death distance here we have time series plots for human lung cancer using gene site here we have forest cover data this is a link diagram for forty soil tops oil types using a poly analyst for the forest cover data and here we have percentile distributions of individual gene dimensions of human lung project using poly analysts and this is human lung again so the conclusions of that part of the research is that the study of the contrasts of the dimensionalities of data at the micro-ray level the forest cover versus human lung I take credit for use lying software never intended to be used the microwave level except for gene site and the selection of the fourth software for that application is unique in the comparisons that I was able to make data mining of micro databases is an entirely new area it's only existed about ten years in an immense amount of publications about biomedical research now being directed in that area on the Broad Institute we expect this area of data mining my career databases to become an influential factor in weight data mining can be formed or the databases for these dimensionalities on 2009 here at WMC I I presented a paper with my graduate student Ryan Pierce of advanced data mining of leukemia cell microarrays and leukemia is a cancer of the blood or bone marrow that's characterized by abnormal proliferation of blood cells using white cells using micro sites micro databases used for leukemia where hl-60 jacquard and b4 u937 and the sauce the data was again the Cancer Program website of the Broad Institute which is a collaboration of MIT Harvard and medical communities hl-60 is human proto lipstick leukemia cells that has been used for laboratory research and how certain kinds of blood cells were formed and that's from cited from Wikipedia 2008 micro databases is used to describe a repository containing microarray gene expressions that's also cited from Wikipedia these the trot cells from wikipedia says that sell their cells used to study acute leukemia expressions of various receptors ceptable to a viral entity particular HIV turcotte cells is cited by wikipedia by the primary uses however determine the mechanism to differential accessibility of cancer to drugs and radiation self-organized maps are a noble tool to provide of mapping from the input space to clusters that are organized into a grid that is usually two-dimensional and they differ from k-means clustering because k-means clustering cases are grouped together based on their including and distance from each other in the input space self-organizing maps tries to find clusters such that the two clusters are close together in the grid space have seeds that are close together in the input space and according to turonian are all from the Federation of European biochemical societies for the analysis of gene expressions data using self-organized maps self-organized maps is an unsupervised neural network learning are with limits to successfully use the analysis of an organization of large data files now here i have a list of some of the related research by myself this slide shows for things that I solo authored data mining and micro databases a biotechnology it's a chapter in a book encyclopedia of data warehousing in mining data mining a micro databases for biotechnology I gave it seminar at the by rnr bio for max world at Arkansas Biosciences it's to lecture series data mining a micro databases for analysis of environmental factors on plants using cluster analysis and prediction analysis and I did something on environmental facts of corn and Mays here I have something by co-authored with my graduate student Bryan Pierce in dr. Zhang advanced data mining on Khemia cells my data mining micro debut of human lung cancer applications of neural networks and genetic algorithms for bioinformatics knowledge discovery and data visualization data mining micro databases to bio technologies here I have a picture of some of the normalized mean plots using self-organized maps for leukemia hl-60 the normalized means are on the right hand side the blue and it's sort of like in three different ranges they're here I took I compared to all of them which the blue versus the purple pacifically number 22 so there was some disparity there here I took all of them the blue ones the purple ones are a one number one and the white ones are six and the light blue is number 17 in some disparity they're here this is for clusters 16 and 17 here I have some time series box whisker plots and I looked at zero hours four hours 65 hours and 24 hours to see how hl-60 varied over time here I have the Buster proximities from traditional clustering for hl-60 ricotta nb4 and u937 here i have a self-organized masterclasses 1 16 19 and 24 verses all the normalized means and you see the sort of like a nonlinear relationship for the number 24 which is the purple ones but all the other ones were on the on the zero mark then the conclusions of using self-organized maps is over clustering is that the data mining and interpreting patterns of gene expressions perform for the hl-60 leukemia data set up turn obtained by the Broad Institute so how most notably the similarities in the scale dimensions of the cluster proximities for traditional clustering compared to that of self-organized maps as well as greater detail this is certain by the number of clusters creating utilized self-organized maps crystallized the disparity and the results the two methods and that paper that are published I talked about advanced data mining techniques for leukemia cells this was article with published in the Journal systemics cybernetics and informatics in 2009 we looked at leukemia data specifically clusters 31 and 35 all of them are blue and the purple ones are number 17 so you see some spirity there okay here I'm talking about the decision trees as I said earlier look at the intensity of the dimensionalities of here I mean I can't even read the numbers just looking at the slide here because there's so many branches in this decision tree but there could be read if I you know blew them up but I'm just saying pointing pointing the matter is that the data mining for the microarray databases created a very intense type of situation here also is exemplified by decision trees here I have some clustering results using self-organizing maps for hl-60 Charcot and be four and u937 leukemia data here I have that for those four again in the extremities and cluster cells and the picture on the right shows some disparity again for the purple one which is number 17 I believe but most of them were pretty much normalized on 0 there as advanced data mining conclusions so that applying data mining l6t data by itself led to visualization plots that had dispersed normalization means such as shown by figures 3 and 5 who has compared to that the aggregation of those for leukemia data similarly figure 44 hl-60 by itself uses yielded for only four clusters after applying the data mining while figure 9 for the aggregated data you need to double the number of clusters ok now I'm talking about my research that I've done in text mining I have shenmue which is a member of the university Arkansas at Little Rock and dr. Belford is also the universe Arkansas at Little Rock have collaborated with me and we've written a paper called linkage we're working on research right now linkage discovery within some voting symposium proceedings and it was published in the technical proceedings with a six by ona no toques conference in Little Rock in november of last fall 2011 and that uses latent semantic analysis or LSA and expectation maximization to discover connections between papers within a symposium proceeding then linked papers along a variety of things Shen Lu and myself she's lead author have just had a paper accepted in the international journal of information decision sciences will be published in 2013 and the type titled the paper is linkage to medical records in bioinformatics data we provided an algorithm called entity resolution for the fuji center model to improve the results of some semantic analysis with identification of similar records here we have some output of SAS text miner for linkage and frequency of terms and you see on the bottom right there some links in there they're different widths the wider the width means the higher the frequency of the frequency of that term whatever it is and so the ones the top a very frequent the ones on the bottom of our second frequent and the third branch are less frequent of those three I did some research with hotel customer comments more on the web and we extracted breakfast and we looked at their written comments breakfast slow I can read all of these but they're written comments on the right hand side there and we extracted we looked at the comments that we got for the keyword that we extracted for breakfast that was inputted and here we have text again here we have microwave databases again but we can have text descriptions for each of the microarray databases because it's so intense data we can have so much text but we want to extract just some of the keywords we can do text mining to get the the appropriate cells that I want to query databases that were interested in and we also can went with once we extracted the terms that were interested in then we can do histogram plots to exemplify the frequencies for each of those terms that we detects mindful here we have a picture of a concept link using sacs text miner and the in the center the word is statistical and on the around that are terms that are related to the term statistical and also the number of branches of those but we can also rotate this thing so if we rotate this around we get get more dimensionalities of the data so another on the left-hand side there you see some additional terms there so we went we found more granularity in the data to get more terms text terms that were related to that particular term here we have clustering the results of text miner and again we're looking at a hotel customer written comments such as front desk very friendly and we also looked at the age and gender of the of the respondents here we have linked diagrams again and we can connect the relationship between each of these in other words we have air conditioning by a double our there the red dots the red dots are related opening windows change room cleaner dining etc and the turquoise ones noise and traffic yellow tea coffee doc etc so you can link the terms together color code them as far as how they related to each other here we have a dendogram and concept map using word stat which shows another technique for aggregating the data and showing the relationship between different concepts in text ok also todos can talk about my research in web mining mega puter poly endless can be used for both text mining and web mining so here's the workspace again for mega puta poly analyst and again we used the same data so here is the link diagram again we also use for web mining we took histogram frequency plots of age groups and gender for the respondents and but we also looked at my university which is Arkansas State University we looked at the website of our university as far as the amount of information that they had stored on there and so any University said isn't very intense now in putting things on their web if it be for public the dissemination offer internal administration use this was for internal administration use and then we were able to extract from that the undergraduate admission data because a lot of the applications can also be done online so we're using web mining to extract the emission undergraduate admission data for my university Arkansas State University then we could you know we could mine further and further and so forth if we wanted to here we hate we did keyword extraction report we looked at faculty for Arkansas State University using poly analyst because there's faculty data information about that and again here's keyword extraction for the hostel hotel customer did online surveys web mining can be used for the for breakfast and their responses there to SPSS Clementine is a workspace for spss Clementine which we use for our web mining and we looked at this is again for the advanced visit segments of data and here we have a table of the web extracted with 250 1998 records with seven different fields here we have decision rules for determining clusters of web data so in other words in web mining we can define the rules whatever way we want so we can refine the web mining to our unique specifications and here we have some decision tree results in there nodes using SPSS comment on I've also done some work with Shen Lu and this is of 2011 last year a statistical quality control of microarray gene expression data the novelty of this is that we used a technique of statistical quality control in their traditional methods which have never been used for microarray gene expression data which is using things at a very different dimensionality of what they were tended for typically classical control charts have never been used for microarray gene expression data and so we use this to find out within the data with you know where the data within the central tendency mean or you know how far were they deviant and so forth and we had range charts for the random data for with full change of 1.5 after sampling selection featuring selection we generated 18 information products we use data mining models to evaluate the quality of information products we use the k-nearest neighbor k-nn random forest self-organized maps and multipass lvq and the one with the largest precision a recall is is in good quality the key characteristics how you know how do we how do we define measurements for an array containing an array elements we assume our goal is to compare a query in reference sample which we call our Angie the measurements were a variable data and attribute data the variable data of microwave gene expressions were the ratio of the ice gene of the array which can be written as TI r sub I over G sub I most often tlie expression data represented by a logarithm base 2 of the expression and this has the advantage that it produces a continuous spectrum of values so the attribute data the micro gene expression were precision and recall which sort of the same thing is specificity which these are biology terms for the similar concepts all right so here we have nine different charts here for the total data we're using quality control charts and we use different p values and different fold change values so you see that some were very close the full change of 1.5 in the upper right was very close between the two columns while a p-value point 05 with there was a big difference between on the left-hand side for data one through eight full change of 2.0 and p value of point 01 on the bottom middle was very close and then we looked at the balance data the total data was balanced and unbalanced data but then we looked at balance date pacifically and we saw that full change of 1.5 and p value of point 05 and the right-hand side was very close and then we if we'd use point 01 then they were not so close so we were able to the novelty of the research is that we were using statistical quality control techniques for micro databases and able determine which fold changes and p-values for the best for our data performance according to the experimental results we can see that the data set the total fold changed set equal to 20 and p value point 01 both its precision recall or greater than the average of stable this research by Lou and Siegel used the total data quality process improved the quality of microwave gene expression data to find key characteristics and measurements analyzed data quality with Western Electric rules improve data quality with data mining tools and so this concludes my presentation and I hope I'm able to give you an overview of the dimensionalities of knowledge discovery using data mining text mining and web mining I've done a lot of research in this area and as i said i have booked those published in 2011 so maybe you probably can get a good price on it in mazon calm and other places are selling it weight less than what it is and so i leave the floor open for any questions so you may have "
iZcx6wnScJU,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-03T23:42:21Z,Data Mining (Spring 2016) Lecture 7,https://i.ytimg.com/vi/iZcx6wnScJU/hqdefault.jpg,UofU Data Science,PT1H23M51S,false,182,0,0,0,0,"like I've tried a couple for Android nation of that experience does more difficult work injury I chose the driver to give you guys the schema CSKA processed immediately goes to our school yes I trust your word processor video so like two things one is shipping is Alec neurologist hasn't been released by NDSU so its current that is that actually doesn't suit seals knowledge to negotiations which is because the right your total cigarettes generalities or just the difference is that these issues by what does offer a personal stuff adjustable should probably know I did that case the rules I did actually this is due to exercise that I decided to see yes yeah but like so like it's when I didn't have to sound and among charm but really tender kiss kinda but like somewhere else yes by itself sure I mean so let's get started um I'm so welcome back to class um so I see I've got some good questions on the first come on our idea which is argument posted so if you've any more questions please post them to you compulsion to canvas see the other thing came up is the notes on them on the web page I update those every year and I've updated the first half or so of the notes and you can tell because well for one thing there's a jump in the numbering I've reorder things a little bit in that number at the top but also there's a date she should find a date that the Bible says 2016 if it's not that that date yet then then i'll probably update those before i give the lecture topic and the same goes for the homework someone somehow found version of a double burger 2013 if you answer those questions you you'll probably answer them incorrectly this I changed them a little bit since that comes so just something to be on you know look out for okay so we dey is the last lecture on is the last lecture of condiment on dealing with distances and similarity but as well these things will be used in the in the clustering section and also in the regression section as well so we'll keep keep using these concepts so we'll start fostering an extra gun one day okay so back to the problem at hand here the idea is we're going to have some some large data set p where say the size of p so it's a set and i can take this set notation for that as the cardinality the set Celeste's some this is um this is something like 1,000,000 and then the kind of to encourage questions are which hairs p and p prime in the set p are close so you can define close based on some notion of distance and some threshold which are specified and so give it and give it a query point Q which is from the same universe SP find the closest point p @ p to q so we phrase the second one a little bit differently before we want to say is there something that's the same as a query q in the second is there something the same or something close to the same one way of doing that is having a distance and finding the closest thing and if that's close enough and you say that that these are the same so one way of thinking about this is saying that let's say p let's say this star is going to be our mid P&P of some distance that we're going to use between P and Q and let me and so we can will often write this as feed with this subscript that's an uppercase P more clear in the LA tech query q so this is so who's seen this art mid notation before I just not seen this every year few people haven't seen it so the minimum this is essentially I'm to the mid of a element so of any element in the set P you want to minimize this function which is the distance and the aardmen is the argument that minimizes it so it's the object Pete the point p which minimizes business right so that's what call that peaceful that's the nearest neighbor of Q in the city so if we want to find the skier state okay oh I'm going to get back to what happens in low dimensions but before I puts part of it we talked about with little County sense of passion and with and also part of the lecture today will be with with high dimensional data and we talked about this these ideas of so high dimensional Euclidean how Euclidean it where does high dimensional Euclidean game come from we talked about these warnings about you know using Euclidean distance or most distances with the different coordinates of something out if if the units are different this can cause all sorts of strange problems so insular so maybe the intuitive way to think about this high dimensional data is you would either data set each data points a customer and each dimension is an attribute of home but these may not match height and weight they own the same units you can't really it's hard to define the right way to find a disability so so where do you get this high dimensional data that makes sense think of it in in the euclidean distance so i just described a few ways where this comes from and then i will go through one in in a little bit more detail or a few detail so one is in what is in time series and so I'll just I mean there's much deeper bonding that goes into this but the idea is if you have something like a like the like the price of a stock or the or like the temperature measured and every say every certain time point maybe it's the closing of a day or it's the high where is the temperature at noon and every day or every hour you're going to get a sequence of these of these measurements so supposed to say that the temperature today every hour was something like you know maybe the very early this morning at 1am and then 2 a.m. and 3am the temperature was 15 degrees Fahrenheit 15 16 18 and says if you know and so on so you get these different readings of the temperature ok and so then if we have a string what you might want to do up until right now I guess my let's say it's 35 degrees right now and you may decide to take a sequence of these save link on 10 and this describes like 8 a-8 n-dimensional point all right so now like you can't now it gives you the temperature over a power over over a span of like 10 hours and the way you'd want to use this would be in various models remaining weight these we can in different ways based on so how is correlated but these all the same units and you occasion what to look up say sequences of these stock prices or these temperatures in some and some sequence there's there's various ways you wanted I'm happening kind of get into them but this might be a way might model some of the days that some of you have talked about to the project so this is just but again they all the same units which avoided that problem which boy the following had before so there's also okay so that's all I want to say about time series in in machine learning there's this idea of there's this idea of using kernels to use this kernel trick where instead of say building a linear classifier you would build a nonlinear classifier and if you do these radial basis kernels these actually look like like like it looks looks like a Gaussian kernel and one of the issues is that you can you can do all the same thing like funny classifiers or or doing regression on this data by just using the inner products which is essentially with what the what the colonel is here and so if you haven't sales before I am NOT gonna get enough detail so you can follow up with you steam user Colonel tricks before so between two points this is replaces the circle replaces the dot product you'd use in euclidean space and so you can do everything by just using this kernel on but then to start with and data points he need to create an n-by-n matrix of all these inner products to do a lot of the work and this into space or cost cost and division so there's a way to approximately lift this into a Euclidean space that's pretty high dimension that M depends on some accuracy parameter so if you want accuracy that some apps you know some added to their these values these enterprise value vary between 0 & 1 if you on air between epsilon 8.01 you need roughly 1 over epsilon squared and features and so you lift them to this high dimensional space and so if you hundreds of millions of data points which happens in various ways a lot of these internet companies have data sets like this then you can lift it to maybe a ten thousand dimensional space and now creating the inner product using these vectors is a dot product again and the natural distance associated with these is the inning distance this Maps without any air into something called a reproducing kernel Hilbert space but that's the technically infinite dimensional and you can't really work directly with it other than as a functional space but this gives you an actual feting and then you use you putting distance it's really high dimension ok so those are a couple of other examples so if you see those before maybe those aren't useful to explore more we'll talk a little bit more about how to get this data from images and there are two ways to think about treating images as high dimensional vectors where you want to use the euclidean distance the NAU again Euclidean mean we talked about that's the l2 distance we talked about if last lecture the first way so that I mean the most important thing is to first realize and you know this used to be much much more obvious but if you if if you're talking about image it's actually made up of these little pixels so I don't know like when the internet first came about and you load it up images you could see all the pixels right because it took a long time now they're so high resolution you like this may not be something you notice any more but images they're pixels that means it's a bunch of these little squares and I can see nicely and on the other notes that all posts on the web page you can see a grid hungry they're bringing good thing of each of these as pixels let me try and draw a few of these down the corner and so each of these this is a pixel and each of these pixels has a value if it's a black and white image which is a lot easier to think about it's just a great value between 0 or 1 and usually actually not between 01 it's usually between 0 and 291 and 256 usually usually that's enough if you look at like the he and cut my watch I think I would have 32 values for color but that's so pretty good you know even 256 is usually plenty of different colors and so often you're going to get not just one great skill value but actually one that you can rent rename and a blue value which makes up a color so you're going to get three numbers each of these but for now we'll just talk about just the great skeletor's okay so now let's say that image actually consists hears about everything i can draw of someone's face let me try and draw these with with the pixels so this is going to look kind of weird see these pixels are going to all be colored red and i'm going to leave my eyes and they're okay right so just I want you to keep in mind that this is these are these are actually pixels here right so that looks like that like a face and then there they've got a neck here they've got a body you know this is very very horrible part right so these pixels are all filled in right so that's a picture about other person right so each pixel is lack of this or if it's the person and it's white if it's the background right okay so and maybe it's not black or white it's on values in between there but this now you all these pixels and so you can convert this into into a vector in there have two common weds the simplest way is just to think about if this is a if the number of pixels is 105 100 so that means I go 100 here and 100 here then I'm going to turn this into a 10,000 indeed legal 10,000 10,000 on dimensional vector so if you remember what vectors the order of the coordinates matters so I can just arbitrarily basically what you what you do is you kind of go through an order these come first and then these you know so you come come back here and then you cut across and you just string them out you vectorize this this this square right and so you just ring them out into 100 at a time a hundred time you give this these 10,000 dimensions okay and so this is you get these ten thousand dimensional data points and so there's been a few projects in the class where people at some pictures like this one of the limitations of this is basically if you're trying to analyze objects and often people do this with faces pictures of people their data sets out there where the they've been three process so the face is always in the same spot in the picture they basically made it so the whole thing not doesn't have this beautifully drawn body here but it just has the face in the picture and so the nose and the eyes are all essentially in the same spot so they've kind of regular ested and then you can try to analysis on faces after they've been somewhat aligned already in these pixels right so they may even be 100 500 pixels is probably plenty for analyzing faces possibly at least there's some days before you can start to do interesting things with already because the variation in phases is much larger than what you get to go small pixels but you if you take a picture with the high-powered cameras be much more than 105 number of fixes here so you get much larger ok so this only works this only works if pre aligned ok so so so this was kind of computer vision in the 1980s and 90s actually often often worked this way they looked they kind of this up those thought be pre line or they tried to do processing on things kind of thinking of it in this way there was a shift in the 90s with an algorithm called sift that what it did is it tried to pick out interesting pixels and it used the structure more of the pub that this pixels next to this pixel and these are the princes here and what did try to do is to find certain pixels which are on corners right so these are if I look at a box around these guys are are inside these are inside and these and these other ones here are outside here and keep trying to tack this this is in something interesting is going on here because it's at the corner with the sky would shoulders are really fighting and so these these corner things or other sort of things to have very sharp gradients tended to be the interesting parts of pictures so what people do is they take these pictures and scan for things that have these interesting features and then throw these into a database so every picture generated a bunch of features that it through into a database so each picture was many data points and so that if you wanted to find a similar picture you generate all of its data points and look for similar data points in this in the database and of many of these of these data points aligned or to another picture it probably had a similar object it's lots of similar features to you if they're scrambled so for instance if one picture is is directly the other one is a selfie through the mirror so everything is reversed it should still work right so there is pictures of cars they so certain things you goddamn if I really want your vision one is people now working at the another is cars before great at identifying cars and cars that various features like around the wheels are very distinctive and so far so it's tended to find these these these features and so let me just give a brief overview of how these cities features work the idea is if I look at and a single pixel here in the middle then i can i can kind of label the pixels around it one two three four five six seven eight right so there eat two pixels around this one here in the middle and so so she stiffed stands for shift invariant feature transformer but really it's trying to it's trying to find these features which are going to be just said this these features should be in variant 22 on to scale and rotation and shift so so she this shift was made it into the acronym of the scale and rotation didn't but the ship is kind of a big thing right before if I just use the vector the director vector representation I if I shift the image off x 1 pixel it completely changes right but if this should be invariant the feature will look the same even if I shift it I take away this strip of pixels that add the money and here the feature locally will look the same ok so it's using so if i take some pixel in the middle here i look at all these eight neighboring pixels and so I first thing I do is I find the one that has the largest that that has are just gradient so the difference in the value is between these two are so largest let's just talk about the grayscale value so I can just compare the differences between them there's a number between 1 256 the largest difference between the two nuns and then kind of i'm going to re label these so in i think in clockwise order starting from the first started for the month lauren discrete this kind of gives me an orientation of the pixel okay and so this gives me eight eight coordinates by doing this and I've somehow rotated them and that's posted make it invariant to rotation I like eight possible rotation as it turns out that this works of course not so to do this and so this deals with shift in rotation because I do this for every pixel this takes you our boat ation to do what scale instead of looking at just one set of neighbors like this around a pixel I do this at it does something I won't get into and I want to scrap this in full details kappa kind of complicated but it repeats this for it creates a similar thing here at four different scales at the fort neighbor so it'll do it not just at this picks off but had four neighbors and the notion of neighbors is at four different scale so it's here it's also so I instead of making a 1 x 1 pixel to its neighbor I'll look at a set of four pixels and look at all their neighbors of size 4 and I take kind of an average of these values so it does this at four different scales so I'm going to get four times for different scales just going to be 16 kind of scale components and for each one of these scales i'm going to use eight coordinates okay it's kind of strange construct but what you end up getting is 8 times 6 128 128 dimensions you'll end up joining all these together and you get 120 dimensions for each of the each of the feature pixels and then in the and these are all scalar values and so the right way people think of the distance between them is it is is using the Euclidean distance and so this is generally what's using computers and and so so there was this algorithm was this has an interesting story there's a up so there's this researcher david lowe and he tried to publish this this technique over the course of like five or eight years and could get it published at all finally published it in some kind of a lower level journal and since then it's been cited like over 10,000 times or maybe more than that on some huge number where it's nothing like more half of all the citations at that journal for some period of ten years keeping this one paper this something pretty crazy and if this was used and basically most the late 90s and early 2000s a lot of computer vision was based on either these techniques or there's some variants of these that back to a different high dimensional space there are some other ways of creating these features but these sip features work or pretty well to get this high dimensional representation of key points within these images and then you use these to find and the key thing is then to find the nearest match to these so you want to a nearest-neighbor find all the similar objects one way is to use the locality sensitive hashing that we talked and this actually works fairly well for this but there are other techniques that also work well that are kind of based on lower dimensional constructs and so the rest of lecture I'm going to be talking about these and stay so locati sense of hashing meant for arbitrarily high dimensions it didn't care how high the dimensions these will things that will work for medium to fairly large but not too large size dimensions and there are ways to get them to work at 128 dimensions and all kind of I'll kind of build up to these the rest of the function okay so who'd heard of sift features before okay every year is a little bit less than us if I think but these were like all the rage in computer vision now this deep learning techniques are are taking are taking over a kind of replacing these if you think of if you know about deep learning have the first level of the deep your own death is finding types of features and those features are kind of in use in place of mesit features and they tend to work better but not you know not too different than if you had just had the first layer just see it with the SIP features of the image so this kind of figured out manually well ahead of what deep learning figured out how to do kind of someone automatically okay so let's talk about this so let's let's talk again about the nearest neighbor problem okay so let's start with the problem just review let's start with data in and just one dimension so we'll start with a point set which is is a is a subset of r 1 so again this is notation um this is in the well so this is just reals okay so um so if you have data set and this could be large we won't find the nearest neighbor to a query how would you now with pre-processing pre-process and a set so you can quickly find the nearest neighbor we talked about this briefly was kind of a conventional way to do this yeah so so you build a balanced binary tree on top of here so you build a balanced binary tree on top of here and you kind of store these also you can store them in a list so or is some sort of linked list on the bottom so now what you do in the query is you say you keep um and each know you store the kind of one of the elements which is splitting it and you say okay so this is the split element here and so my my query point is on this see that's going to be your left side of the split so I go down to this tree now this sub trees I go this direction in this direction and I can find one of the points which is either just a lot of it or just to the right of it it was just the left I can also use the linked list also check here and I know one of these two points must be the nearest eighth so you can do this in log n time and the space complexity of this is linear so that make dollars between those two blue dots I mean so what if it was equal distance which which one is the nearest neighbor or is it kind of arbitrary yeah oh I'm so that points arbitrary and we're going to make it even more our trained your son oh yeah so it's it could it could be either one of those we know must be one of those two or they're equal distance okay so if you instead have data sets in in two dimensions so now let's draw your great so how do I find the nearest neighbor now well before I talk about that let me say let's look at this actually the structure of these mirrors neighbors first there's what i can do is i can actually you know if i have a query point i know this point is the closest that has to be the nearest neighbor but i can i can say something more interesting and we kind of draw i can divide up the plane so that so into these regions they do this right there looks something like this to these regions so if I anywhere in this region this is the nearest neighbor but anyone in this region this is the nearest and this is called the the Voronoi diagram okay and so if I'm asking nearest neighbor question I'm really asking about this diet I need to be able to figure out which cell of this diagram I'm in that's that's precisely tells me which is the nearest a turn okay and so these lines are bisectors between these points you kind of draw them out and that's kind of when you draw it and so if the points are are not if they're in kind of if they have continuous coordinates and they're not in some strange position then in the plane these all meet at at at there are three lines of me to pet me plus and these lines go down to infinity and so forth okay so it seems like if i can just construct this this diagram um then i can answer near saber corey's pretty efficiently it turns out in 2d that you can construct this diagram and then so in our to this the size of this diagram is this is all been ok this the size is linear in the number of points and by the size i mean i'm going to write down all these intersection points and the lines that go between I kind of describe all the line segments that describe this in linear space and then you can build kind of a query structure on top of it that queries in a bowl of log n time and in the construction in 0 n log n time so it's as fast as sorting which is as fast as building this this up the binary so in 2d you can actually do these queries in in logarithmic time which was the same as in a 1d using this by using this for a dieter if you take the computational geometry class we will probably learn how to actually be okay um okay so what happens what happens now 4p in r3 so I drew this in 2d one because it's a lot harder to draw on higher dimensions but also because in three dimensions let me just write this general ID in D dimensions the size in general for D dimensions the size is now unfortunately both n to the D over it's the it's the ceiling of d over to both d over 2 so so 44 D equals 2 we r equals to this is one for the equals three this goes to here since me n squared size starts to become big four for higher dimensions like four in six dimensions are seven of instances into the fourth the size of this diagram and so the do nearest neighbor queries you're essentially working on this diagram which itself is really complex you're essentially and because you could ask a query which is really close to one of these points you kind of have to resolve these issues and so if you're going to pre-process it so you can answer these things officially you're essentially worrying about this you're since you worried about this warning diet and so to answer exact queries high dimensions is not really going to be feasible it either require a ton of space or a lot of time or or both there's a ways you can use a lot of space and get the query time efficient or you can use less less space and get the query time to be slower but it's hard to get or there's ways you can trade off in between these it's hard to get both yeah there's the query there's also scale with the amount of dimensions yeah so the their various ways to trade off the size of the query time so one way is to not compute the full structure kind of to build a first divide up the points into kind of local parts and then build that structure of each parts and that because you can trade off or actually what you do is you or you another way it is the sample drink a subset of the points and then build this structure on the subset and then recursively do that up the pieces and their various ways to trade off the size in the query time but one of them has to be so if you want to do it exactly and you care about worst-case runtime and how are the complexity of this we're in kind of in trouble in higher dimensions so what so I'll talk about well so so what instead what people consider our DS of approximate nearest neighbor queries so instead of saying I want to find the exact nearest neighbor let's let's loosen this a little bit so let's say again that p star equals R min of P p of right and so and so i want to in n just to simplify notation say this answer so my goal now is to find a p-hat such that the distance from P T app to p is less than 1 plus epsilon of the distance from east are cute and so now this this epsilon is going to be some air parameter state between zero point so often we make the small thing of this at staying 0.1 crazy Valley bachelor so I can be off by ten percent of distance so if they distance is small I'm going to find something else as a small distance right so now if I look at this picture here so if I'm close to the boundary and I don't know which of these which of the two points is closer I could think of drawing a ball around the distance so this is Q and this is P star then I can expand the radius of this ball right extra 1 plus epsilon factors or ten percent and now it's okay and after I do this either of these points which are in this expanded fall our ok to return I have more options you think well that will not seem like that would have been the best choice but actually the one of top also would have been reason okay and so so so this hopefully should seem like a fairly reasonable thing to do we care about is or something close enough and so we really often care about this this distance and Leah so if we have ten percent error in the distance that's probably enough to say is there something close enough we want to find something close you know usually we if we kept something not quite exactly the closest thing we're close to that that that's going to be helpful it's another good example which is close to our query so another reason is that remember these distances that we chose we're using the euclidean distance here but this choice maybe was also arbitrary with we changed what distance we're using maybe p hat was closer than P star I'm not sure right the choice that the distance was also a little bit of arbitrary so you shouldn't care so much about getting the exact minimum as opposed to something that's close to the exact and should your general work just as good so this is a hopefully this seems like a fairly reasonable approximation to this and so if you allow for approximate nearest neighbor queries then you can then stuff becomes a lot easier to not it's still hard in higher dimensions but becomes more possible to still use these techniques okay so so so this was small dimensions let's talk about now these these medium dimensions and so for medium this is kind of a term I I've made up I feel this is really stay d is in say three to twelve me that excessive medium to as low a lot of my stuff works sometimes some of the low dimensional stuff will work in three dimensions you can kind of get those structures above actually there are ways to actually get stuff kind of like this these query times to work fairly well in 3d actually but forty this everything starts to break down so you can like do these approximate born a diagrams and and those in three dimensions are still still very reasonable mean you can do the exact for them on 3d but in general three and higher dimensions you want to use something else ok so the right way to think about this is not so much like the morn a diagram but like this binary tree version we want to do something like a binary tree in higher dimensions and i'll draw example to get into d because it's art drawing high dimensions how would you do a binary tree in higher dimensions or something like a binary what's the right generalization to two dimensions on the here at KD tree yeah great so one option is a KD tree and so the idea is that again it's going to be binary and you're going to split so let's say that my data looks like like this and so you can't just just divided by x coordinates it's not going to work but starting by dividing it by x coordinates is going to work okay so i'm going to divide it up into cells starting at say this will be like the median point in the x coordinates so think of sorting them all this way and then this is the fifth so I could have took the form of the fifth doesn't matter so much let me add some more data points this will look a little bit nicer ok so now so I can split here so this would be the top down to my tree if I the query I will say in by the left or the right side of this line and then I want to go down a level and then I want to again split on the median and so on this side i'll split on this point and and maybe over here i will split on this pump so there so the second level down i'll split on the y-coordinates instead of the x-coordinate and it has me a different split these side so then I'll recursively do this then in the next level again I will pick one of these these these data points and split here or I'll split here or and so again it's different in each of the cells I'll keep splitting until there's one work maybe actually a constant number of points in my beliefs usually with these structures if you're really making the large you want to stop with theirs when there's a constant number say like 20 or 50 and do a brute force search of that instead of doing all the sensor business so now I'm so the k'da tree will look like this I can replicate this binary tree structure oh ok so now let's go through an example with a query point let's say that the query is here how would I find the nearest neighbor of this well let's start and go on this the right side of this line and then the bottom side of the red one and then the web side of this one and I say okay this might be the nearest neighbor but it might not be right if I draw the circle around here with this radius oh no it's intersecting these these uh 0 second these other sounds that let me pretend it went a little bit larger sounds a little bit larger ok so is intersecting these other cells here so what I need to do now well I go back off the tree I'm at this node of the tree I go up I checked this out which is so I can parallel to this I can also draw like it like a tree where this is the top split and then I have to y coordinate splitters and then back to x coordinate splitters and let's say that my query point ended up down in here so I first have to check its Dave yourself which is over here I check this one I find the closest point in here it turns out this was a little bit further away so then I say okay that's this is still my ball I go back up the tree to here and now I made this so I see that this split point is actually closer so I've shrunk I've shrunk my radiance a fair bet turns out this is the nearest neighbor crying to the picture but I still I still need to check the two children of this guy you can check with children going down here and so I go down and checking their empty but I still intersect it across this line so I have to go back and check the split and indeed my circle crossed here so I go back down here and have to check like one of these things okay so you have to go and walk around this tree and you keep pruning off parts of the tree I did not need to go into for instance this whole sub tree because this circle crossref here you can do that with some geometric check to the essentially the rectangle form right here okay so this this gives me the exact nearest neighbor but it might take a long time if I'm going up and down these trees there's some kind of formal pounds you can prove on how many times of the doubt but it grows with the with the dimension and it gets closer to two it gets closer to being linear to get the higher dimensions okay so if i do that instead this approximate nearest neighbor then what's happening is that i only care if I'm within a 1 minus epsilon of the query point I found so if I started with this point then I'm okay if this is not the absolute years point if there's a point that lies in between the board circle and the grey circle that's okay I can skip that point it's closer but not that close enough i care about it and then then I did need to search this subtree I pruned off this extra subtree here and then when I found this point I've again wrong with a circle a little bit and soon xo now in terms I didn't need to check this sub tree because the name class Sarah I didn't need to cross over this arm old half of the tree turns out shrinking these circles a little bit ends up helping a lot it it it allows you to prove much more the tree much more efficiently so this is close to what is used in practice using what's on so there's this package that you can do for these these these on for doing these approximate nearest neighbors that works fairly well in 3 2 12 dimensions it depends on kind of the ambient dimension of your data if it's actually very full dimensional and being not mailing it up like eight something like that I've heard people use it up to unchanged up to 20 or 30 that's in some cases and so this package is called the a and n package I think if you search for a NN in Google they'll be well on the top page this is a nearest-neighbor package that's pretty efficient okay so this is just the kind of give a more complete story here these are this is how they do it in practice what's actually used to so well then there's an there's another thing used in practice in in databases often called in our tree so there's these notions of these on these e treats you keep takin the 6000 whole database class hopefully talked about the d trees already and these are these one dimensional structures that that use that the data is stored when it's on disk it's really big you bring it into memory in a whole block at a time and so instead of building a binary tree here it says each of the leaf nodes is a huge block of like like several hundred or a thousand or thousands of data points and then it has it that way split so it's a really short tree with really wide splits and so each item of the tree is a whole block that maybe has a search tree inside of it so you can split more easily those are kind of their ways to generalize those higher dimensions one common was an artery and what it does is it thinks that the data let's see if I think from this example it's it doesn't try and force these splits the same way before it's things okay at some level the data is all inside of a rectangle the r is for rectangle here and then it doesn't try and split these down the middle because it would be kind of more lopsided instead it it's let's try and fit each the data set recursively into rectangles as small as possible rectangles tend to work easier for computation because our axis aligned and it's easier to kind of work with an x and y-coordinates independently and so it does this recursively it has a tree so the top level is it's going to be this big green rectangle and then they're going to be these three red rectangles of the next level down and then you could recursively split those and each of these may be a whole block of data and when you're having a wider split this tends to work better especially if data is clumped in very surprised so this archery is also very popular in some areas what's usually used to prove things formally about it is is a a quadtree actually just to mention this quickly take compressed quadtree technically and so it or this could be an octree or two to the D tree this one instead says first some so data set it says i don't care what the cichlid is here i'm going to first say everything is in a cube and then i'm going to pick the geometric center of the cube and split this to to the d ways it's a cells and then recursively do this until the cells are empty and so so I I don't need to do these cells here because they're empty princess and so this is a geometric split which means that the the ratios are better than the KD tree in some cases it can these things can get really skinny because you're splitting on the median point and that we ventures that the level of the treatments as log in but it may get skinny in the worst case and practice it generally works fine the quadtree ensures that all the boxes are round so you get nicer geometric properties and but in shirt in order to keep the level from getting too too close what can happen is you can get some points very far away and then two points really close and you would need to keep splitting these these down you know I keep going recursively down I don't separate these points what you can do instead is say that the compression is that there's some cell down here and I make this a direct child of just note if there's nothing in between and then this kind of deals with some these issues and if you want to prove any bounds you can you can do something like this you can build this in something like i'll probably get this way n log n plus 1 over apps to the D time and the query time to something like log n plus 1 over epsilon sandy so if epsilon is large then maybe this is andy is not so big this is maybe not so bad but these only work reasonably one two or three dimensions and practice okay so this is this works for this works for medium dimensions so I said like D equals 3 to 12 and usually the KD tree is likely to do it with these approximate nearest and recruits all right so what why do these breakdown in high dimensions so psyche greater than 20 so ness this is going to be kind of or maybe we're than 12 greater than 20 definitely bad stuff starts starts after what what's why is this why are these not going to work so well so who's heard of the so-called curse of dimensionality yeah so kind of when you're dealing with high dimensional data often you can't use your intuition from low dimensional geometry and often the hours that are associated with those don't work either so so so kind of the main problem here is that usually these structures are based at some level they're based on like these boxes or these squares because you you want to split orthogonal e because then you can use the coordinates and it's pretty easy to do but this distance but we look at the distance from something it's inherently using the pit in distance it's a circle so if you so those don't match and now intuity this isn't so bad there's some corner spots I've missed but they're not off by that much even if you make your epsilon large enough then these are roughly the same okay but in in high dimensions in high dimensions proportionately these corners get bigger and bigger and it becomes that the circle you look at the biggest circle or the biggest ball that fits inside of a a box the vines of those are drastically different as so let's kind of see the so so let's consider I think the easiest the so that they're kind of a nice way to look at this is so to look at call this radius 1 this is going to be a a ball in saying rd with radius 1 and what is the volume of being one do you know what the volume of a high-dimensional ball is well it's it's not something you can actually write down in in closed form it's PI over DS over two times over the gamma function of D over 2 plus 1 the gamma function is this weird function that is it's defined well for four I think for odd numbers but you can you can write it down approximately another so d over 2 it's so for its it's basically d over to the factorial of you over to ever knows what the web factorials right it's it's 1 x 2 x 3 x 4 x 5 36 up to d over 2 and so which of these is bigger hi to deal with you or be over two factorial factorial is much bigger right they both have D over 2 terms the first one here is smaller but after they get bigger than pie all of these are big right so it's closer to its closer to D to the power D or team right which is bigger than a constant 3.14 you too d over 2 all right so this is bigger which but for all the illustration all I want to say is is this is less than 1 excellence the volume of ball of radius 1 when you get to D is large this is going to be less than value 1 okay now let's look at the volume of the cube so in indy dimensions with radius 1 right so this is equal to minus 1 to 12 the deep right so it's this range twice 11 to the D so it's actually a 2 by 2 cube right if this is radius 1 then again to buy too few to fit in here what is the volume of this cube 2 to the D right so this grows really quickly with exponentially quick this goes really hot this one is less than 1 and shrinking this one is growing exponentially so the volume of the cube is contained it is much larger than the volume of the ball and so essentially these days structures you're trying to approximate this distance function with these cubes or cubes that I've kind of tried to combine together with the KD tree or these rectangles either xq to them right the quadtree at least has the right proportions the KD tree could get even skewed so these are worse approximation because it'd be often sub direction more so this is really why this grace I mean this is one way to see why it breaks down there there are other ways to see if the interaction with the Voronoi diagram is another one when you get all these bad situations happen okay so so what can we do in high dimensions are we with hopeless and as I mentioned there's this aann library which is for free nice to use and this can work for d it may be to up to you know up to maybe 20 you can get this to work in some cases on so and then there's also this locality sensitive hashing thing if I saying it works by to 220 basically I mean that it's working better than the check-in call all distances so you can always spend linear time with just storing the days and check all the distances that always works it works it can improve on that up to maybe 20 minutes and the lsh will work well if n is really big if n is is is is very big so that the dimensions don't matter but in order for the constants to kick in for a lysates you really need a big talking like maybe like you know like 100,000 and and that and it will work better than LSH something's up to like around again again around 12 or 20 dimensions depending but either of these is what people typically use these are kind of the lsh has been used a lot but people realize that you can do something based off of the kind of some modifications mods to the arms up to the KT treaty so do something like the KD tree but modified a little bit and you can get things up to up to a few hundred and so that in fact when I talked about these sift features in 125 they all have used these modified versions of the kini tree and often these will outperform Alice age by on by a bit not these are my Alice H actually works really well on those but you can do better with these modifications so how does this how would this work why I mean after all I told you about the board a diagram and these and the circle within the box kind of issues how could how could this impossible work why would you be able to do searching officially you're going to so one kind of phenomena that this this leads to will we'll see this in in a we'll see this in in another lecture is that in high dimensions most of the distances look more and more similar to each other a lot of distances look especially if you do random points inside of our in the inside of a ball I'm side of a cube most of the distances are going to be very close to each other so if you use an approximation it's going to be it's it's going to be easier to find some point which is close but these are kind of random points that all the distances look roughly close if you have structured data you're doing this because you think there's interesting structure you think this distance mean something so so often data is in in in is going to be off today is going to be kind of inherently lower dimensional it's not it's going to have deep coordinates but it's going to maybe this means it kind of lives close to a lower dimensional subspace or if you look in a local neighborhood all the nearby points Lyle lower dimensional subspace there are various ways to to kind of to their very supposed to capture this in in one way is is called the his idea this was called a bounded on the founded w dimension and there the idea is if you have the data set and you draw any fall around the data set um if you draw any balls centered around the points and then you try and cover all of the this is any radius r you try and cover all the points with balls of radius they should be a bit bigger balls of radius R over to these balls are raised over two and then take the log of that number this that gives you an approximation of what the ambient dimension looks like so if in in in two dimensions you can cover or in D dimensions including space if they're full dimension you can always use roughly two to the D balls and cover a ball of twice the radius so if your data happens to have this property everywhere you say the doubling dimension is now there are other ways of defining this notion of the ambient dimension data but this is have a common and then if you get the smaller and smaller radii i eventually there's only there may only be one point in there so you can cover by one so it's the maximum of the log of the number of points the number of balls in and so this is kind of one notion and so the idea is kind of you know in one needs kind of for 2 t's had to illustrate this picture if it's inherently more dimensional then it kind of lies on a line right so I should be able to do something then like these lower dimensional structures and one day I can build a binary tree in low dimensions I can do something like a KD tree where I split along one of the coordinate axes okay but what I want to do instead of using the coordinate axis which is easy to do implementation wise I want to pick a different way of splitting the data okay so the idea is so I'm going to build something just like a KD tree but smarter splits so so if my data kind of like kind of like this then so there's some data here and some here then really I really want my split of my KD tree to kind of divide these into cars this is really what if what I would like to do and I don't need it to be access aligned in this case maybe access line would have been fine right but but I can i can pick it to be arbitrarily i would surely wrote it rotated and so you know in general higher dimensions is a half space you can define it locally with just just the normal vector and so forth so so how would I pick these half these splits better how would I better split these these data sets instead of just alternating through the coordinates and picking the medium by groups oh is that what you said yeah so so one way they're kind of to a few common ways to do this one way is to cluster the data and so we'll use a version use something like k-means or k center cluster which we'll talk about next week and just do this into two clusters so think of these points are here and these are here and then make sure you divide the two clusters or something or maybe the clusters come with the center points here and then you make the half space which is orthogonal to the line between the sentries so at every level just pick two clusters and then split these clusters the the other way is to do something like some and we'll see some in later lectures you would you would do some random rotations um and so do a few and then pick the best so do several random rotations of the space use the coordinate axis split and see how well the split works is this an even split are the points other are there you know what is the house cute or the points on either side have you kind of what you want is that if you look at the furthest these points are away from each other or how well they're in close their ball you want that to shrink for the very rapidly so so you can kind of check that so you can try a few random splits and then evaluate how well they work and choose the best one and this ends further Wilton something I haven't seen published and I think this would work similarly is just pick two random points draw a line between them and take the midpoint so random points this you may have to do a few trials of this but one of these you give you a pretty good split the idea is to again to adapt onto the data so even if your data lies locally looks kind of somewhat flatter than full dimensional it's going to give you a better split than just using something arbitrary so if you can somehow use the data to determine the split it's going to work much better and the key thing you want is to keep the aspect ratio keep the datasets round and to keep about the same number of points on on either side and if you've both of these properties then it's going to work a lot like doing the kg in in small dimensions which is which works pretty well and so this small change on to this is Katie tree and then using the approximate nearest neighbor allows you to scale this 22 to hundreds of dimensions as long as your data as well be great if the data if you generated data so it was random data inside the cube every data point at a random independent coordinate it's probably it's not going to help them right but if you did is structured you think it comes from something like these sift features or like these images of useful things or something else that has some sort of structure to it then then this probably will help you can get hundreds of dimensions all right yeah so anything else I wanted any questions 1980s i want to add you got a couple minutes left so so here's one kind of thing to think about i talked about high dimensional data but having a lower mental structure why would data have like does this make sense that it would have a lower dimensional structure do you believe that you know if you keep around the talk to the people like me you'll probably hear people say that why is that the case oh is it kinda like if you look at regression it's almost like your line is like yeah so obviously we'll talk about regression in a couple weeks and that's trying to fit a load of internal structure to high dimensional date but why would it have that potential structure why would you I was just askin analysis yes so again that's trying to find it now even random data you can see that in front if it's gender randomly some way it's still going to have nice structure because you can't make it to undhan without enough data points but but why if you had structure to it why would it be yeah good good right because you know often this data is generated under some process right and there's only a few things that actually are governing what's going on so you know if you think so so often really big data you start to see heavier tails I will talk about this where you get more diverse things but they may exist with the with the image of megan and so there's this idea that these images images of more for pixels there weren't too many different things going so if i took a bunch of pictures of the same person but i have the person change their head by just rotating a left and right slowly they're only rotating in what dimension right I can they're more and more dimensions or more pixels I get but there's only one face and it's only rotating under under one dimension that one parameter describe everything there was about that about all that data so it should live roughly in one dimensional space it's really complex really high dimensions but really there's one dimensional data going up and similarly with people there's so they're only so many if you look at the basis only so many parts of your genes which influence how your face looks and then you can choose to grow your hair wear glasses and stuff like that but there are only so many traits that influence it maybe it's it's not one but maybe it's 20 or 30 which is much fewer than than than like the tens of thousands you have for all the pixels in the image right so there's often some very low dimensional thing going on generating the data and that's why you should hopefully be able to find that and use that to search for ok cool so this is the end of the similarity stuff but we'll be using this stuff in the next three sections on clustering and then "
gOsxj0j_3gg,27,"2015 Network Analysis Short Course
- Systems Biology Analysis Methods for Genomic Data

Speaker: Giovanni Coppola, UCLA

The goal of the network analysis workshop is to familiarize researchers with network methods and software for integrating genomic data sets with complex phenotype data. Students will learn how to integrate disparate data sets (genetic variation, gene expression, epigenetic, protein interaction networks, complex phenotypes, gene ontology information) and use networks for identifying disease genes, pathways and key regulators.",2015-12-19T02:11:19Z,Data Repositories and Web Tools for Data Mining,https://i.ytimg.com/vi/gOsxj0j_3gg/hqdefault.jpg,Computing Technologies Research Lab Streaming,PT1H9M23S,false,376,4,0,0,0,"thank you thanks see you thanks for having me and is this working is this fine so I'd like today to go over a few of the tools we are using every day for data mining as you know as you see I enlisted the help of two very young miners I have at home but they are really good at mining it they helped me illustrating this talk so all the studies you have been discussing over the past days the ones that are using a mixed strategy strategies are aimed at identifying the things that are of potential interest and especially biomedical medicine biomedical research those things then are obtained by mining and by studying the datasets that are generated by high-throughput studies and then with the goal of identifying a single gene or pathway or subcellular comparing in much more time-consuming and expensive studies which may include the generation of a mouse model or drug screens and it can take a long time so the goal of these tools is to try to make sense of the larger than multi-dimensional data in a way that points us to a more tractable target which is it could be again one gene or us or one pathway and this is a boat at the genomic level so gene expression studies methylation and things that are related to the genome expression in general and at the genetic level so things that are related to to sequence the DNA sequence and in many cases even after all of this we still go back to a high-throughput screen for example once we identify a gene or a target of interest and or if it is for example a protein when I want to screen this protein with high-throughput compounds and often and often the cycle starts all over again so the sections I'd like to go over today are one related to general repositories that we use to both deposit our data and retrieve data sets browser both at the gene expression and the sequencing level tools for gene annotation and for pathway analysis and tools to handle gene lists and I will go over tools that are either present in general domain and other favorite and popular among users and tools have we've been generating here at UCLA so the first is repositories and the most basic repository is a repository of raw data so data that is being generated by in experimental setting and and deposit in shared databases there is assuming that the business one is called G or gene expression omnibus and it's maintained by the NH and it has now thousands of both microarray and NS sequencing data also methylation and other types of genomic data and the second one is maintained by ensemble in Europe and it's called array Express they both have a similar interface and they are used to retrieve or annotate existing datasets many journal I would say most of the higher level journals require deposit of raw raw data in one of these databases and I really encourage this as a common and there's a good practice because this allows pure computational biologists to be able to download them and reanalyze them often answering a different question or avoiding to repeat experiments at AI or again are very expensive so the first repositories are for gene expansion for methylation data the second tool I use we all use is called the UCSC genome browser and in the like for gene expression data bases there is two major repositories one is curated by USC UC Santa Cruz and it's called UCSC genome browser and the second is curated in England and it's the ensemble database so one thing I'd like to I don't want to have like I wasn't I wasn't planning on having a pure hands-on sections but sessions but I we didn't want to go over some of these tools and and show you how they work so if I can get out here this so I can click here so this is a starting interface of the UCSC genome browser as you see it's a complex interface as a multiple sections but the one that has all the information for about genomes is here and you can choose the genome there is a number of million and non-mammalian genomes now all available in database and they have different levels of curation some of them are at the very basic level not even genes annotated but not in most cases especially higher mammalian organisms it's very well-decorated so let's say I'm interested in a particular gene or microtubule associated protein tau you can select it here and get to the corresponding genomic region where the gene is a question yes so you can see that so there's a panel where it shows all the tracks and then there is a large set of possible tracks that are currently all hidden so they are not shown in the interface so you can sure you can decide which tracks you want to visualize for example if I'm interested in the expression of my gene of interest I can turn on the corresponding expression tracks there is many of them here I think the one that you usually use is called the gnf database but I can't see it from here but one of this many expression repositories or genetic variation for example if I'm interested in the Commons it's in a particular gene I can turn this on and show them there is also a table browser so an interface allows you to download this data and analyze that analyze it with R or other commercial software ok so this is a an example of a genome browser the ensembl database is similar to the UCSC genome browser it does essentially the same thing and it is maintained in the UK we suggest a matter of preference and we usually use the UCSC genome browser one thing I didn't mention both of them allow that but we have caused to upload your own tracks your own data so if you had for example gene expression experiment especially for RNA sequencing you have multiple tags you would like to visualize them on the genome there is a API user interface that allows you to upload and visualize your own data on the same intern in the same environment and also to share it with the collaborators so you will have a link that allows you to shade but again the ensembl databases does the it allows the same job they're equivalent especially in biomedical research we are often interested in phenotyping so in things that are not only a jus expression or sequencing this is easier not only in animal models but also for example from patients or from multiple conditions and the the most popular repository for genetic and phenotypic information is called DB gap and it's also maintained by the NIH contrary to the others I mentioned this is not a completely open resource it will need some type of application sometimes even IRB approval to obtain information that's because both the phenotypic data and genetic data are considered to be sensitive and therefore the curators are our control access to this data additional repositories which are increasingly used especially for sequencing studies are at the DNA level so sequencing databases I mean I'm sure you've discussed this with about Jason Ernst and and people interested in genetic control of both gene expression and epigenetic regulation and so in many cases people have a kinetic variant that they have as a starting point and they are interested in first knowing whether that genetic parent is equally abundant in the for example Caucasian population or other ethnicity it means this seconds they are interested in knowing whether that genetic variant is causing an amino acid change if that amino acid changes deleterious and so on so there is many an increasing number of tools to do that and the very first one there was set up is a database for browsing the 1,000 genomes data so the 1,000 genomes project is aimed at obtaining the general the genome sequence of 1,000 individuals across multiple ethnicities and the results from of this study are available on this website a second similar project that is also only related to the only limited to the coding portion of the genome and that's called exome and the exome variant server is a server collecting information from about 6,500 exons sequenced in multiple individuals mostly with the commentary either normal with no medical problems or with common traits like cardiovascular disease diabetes things that are studied in the general population and the third and the most recent and the most comprehensive is the exact database which has been developed at the Broad Institute this database includes it includes the previous ones but now it includes many other datasets for a total of about 65,000 genome exomes so by creating this database you can have instantly information from about 65,000 exomes and I'll just show you how easy that is so they have a again bigger they have a search box and they also have examples and one example is this is for example this gene you click on it and you get first a check showing the distribution of the coverage so these are each one of them is an exome and you just see the distribution of college how well the gene is called across across its axons and then you see all the variants have been identified across 65,000 exomes in in this gene you have a sense of whether they are non-coding like this one or coding whether this coding variants is predicted to be not not important for example for synonymous variation or importance like for me cents or deletions and so forth so you also get a sense of the frequency across multiple well frequency over all here but then if you click on the specific variant you also see a frequency across ethnicities so the RS number is if there is one is here for example here it is and the entire data set is available for download so you can I think there is a section up here of downloads you can download the entire repository and actually we also have code to include that into our annotation and as I mentioned before it's often the case that you have a Valentine you know that changes amino acids but you don't know exactly whether that change is predicted to be deleterious or not and there is a ton of tools that try to answer this question and one of them one of the most popular is called polyphen and polyphen is a tool that integrates into information from evolutionary conservation from a number of algorithms for prediction and tells you it gives you an estimate of the predicted impact on protein function if you have a specific amino acid change and a more advanced tool which is being installed in many institutions including at UCLA is a server for what to facilitate advanced a data analysis both at a sequencing level and the prediction level this server is called galaxy and it is I would say even though it is a user interface it's pretty advanced in terms of what it can do it's essentially a web rapid for code that it is being developed underneath so we will allow you to essentially run code without the actual command-line interface okay any questions about the database part the repositories so I'll move down to a gene annotation the gene annotation part so once we have our gene list we'd like to know more about the genes that are in our differential expression analysis and they're possibly pathways and and I'll go over the tools that are available to do that the first and I would say probably the most comprehensive repository of the gene information centric information is called the gene cards is it has been recently the website has been recently changed and it's brand-new and so once you have a gene of interest you can go to this website put genius search box if I can't find my mouse here so again Tao just because and this website is a portal aggregating multiple sources of information so it will start with synonyms of the tower name which often very helpful especially when you do when you try to compare across species often gene names change across species and except for this is a human data but will have multiple names also from animal genes it will give you a summary about the function the genetic location where this again human where with the chromosome the potential alternative splicing isoforms the domain the function as it no as it is known plus links to the multiple repositories cellular location and one thing that's a proofing protein interaction and this is something that we will go over in a few minutes potential interactors splicing isoforms decide the possible spriting eyes forms as predicted by databases all expression across multiple tissues all the way to reagents you can you know if there is an anti ball you can bite you can there is a link to a company that c then makes it for plasmids or vectors or so this is i would say it's a good first stop for for a gene that we don't know we don't know found the function so once we know a gene name we are interested in understanding more about the gene function and obviously this is not a trivial and easy task especially when done on a genome-wide scale so a first attempt and probably the oldest attempts at categorizing genes based on their function is the gene ontology effort gene ontology was developed more than 10 years ago and it's still probably the most common way to go over gene expression data one thing that's often asked by people going over gene expression reports is what are the gene ontology categories and the structure is pretty simple there is three main categories one is called a cellular component that's referring to the position in the cell of a specific gene the second is molecular function and is related to the function that it is known for example for an enzyme what's the molecular function what's the protease it's going to cut amino acids and so forth and the third is the biological process whether there is a specific pathway within the cell where the and this gene is involved with and then I can refer you to people describing both the resource and also giving tips for maximal and and use of gene ontology yes so I I probably we can go over at least mean especially in the david software that I will mention now they have a hierarchy within a specific category for example within cellular components there would be there are usually five levels so there is the first level that's the least specific and the fifth level which is the most specific and I would probably we usually when we output results from JUnit organizers we take the top three levels third fourth and fifth because the 50 is often to specific and exactly in the causes exactly what you were described in if you it will change significance and the number of categories that are statistically significant if you choose four over five and so forth and the child parents it's more for visualization and I'll show you an example of that so David this Atul maintained at the instance NIH and it's one of the first tools using so the gene ontology notation is something that's independent and it's been curated by a gene ontology consortium and then multiple websites including these and others use that annotation to provide it to users so the input to this website is a gene list you need to choose a background so Julis you want to compare it to and then you will categorize those genes based on a gene ontology and we'd also will also tell you whether a gene ontology category is over represented by running basic statistical testing webcast alt is a similar tool and this is what you probably were referring to because this is the type of output you get you get very complete but sometimes also overwhelming result a set of categories and you see desired for example biological process and molecular functions are one component is not shown and and it goes in the in order of details so this would be more or less level one and this is the most detailed subcategory and the ones that are usually they are statistically over-represented so you would you see more than you would expect by chance in your gene lists and so I think this is a visually Pleasant and helpful way to show data and this is on the website this is an interactive so you can click on this box and get the actual number of genes that are in each of them the ones on the top our parents or the ones on the bottom so they are they include the ones downstream yes yes I know so it's a fear it it's a pretty popular sport within the genomics community to hit on gene ontology says it's up to date and it's not a good way to I as I mentioned this is a start it's a way to end yes some of the websites don't update it regularly but as I mentioned the gene ontology consortium continues to annotate it but not all the websites refresh their databases regularly which is the best well we usually use one gene ontology tool and and I'll mention so I'm going over three or four of them and we will tell you which one we use later and we also use a pathway analysis tool so we have one for each category and the one we use I'll mention in a moment is called enricher we still use data too so for for publication purposes most people use a commercial software like for example ingenuity or they just make tables so a table from David will be limited in our pipeline to the top three levels and if they are clear sometimes they are clearly redundant so they are you know the names are almost the same and I think it's totally fine to just meet them for cried and just show the ones that are not overlapping and another thing that's important is we also put the gene names so it is clear to the reader whether it is the same genes that are there in the categories over and over and get different names or two totally different gene names one matter that is B so two methods are mentioned now one is called gorilla and the next one is called GSE a try to so domain the methods I mentioned earlier use a subset of the genes the genes that according to our statistical threshold are interesting those may be network modules or may be differential expression analysis results and so forth there are other tools that try to take the entire set of genes because it's always sometimes difficult to come up with a threshold in terms of statistical confidence and so this tool tries to take into account the entire set of genes ranks based on a metric there could be statistical significant or intra modular connectivity and and and they will give you a p-value based on the entire set and that's why I'm mentioning it because it's the algorithm is a slightly different from most gene ontology algorithms and the second one is GSE a developed at the broad this is interesting for two reasons the first is because it takes is try to takes into account the entire set of genes and the second is because it is based on a set of curated gene sets that have been developed not just on gene ontology but also on gene sets based on pathway analysis manual creation and it is similar to the reach your function which I think you guys have discussed to being developed by Peter and Steve yeah you will but the concept is similar so try to instead of using one annotation to use a number of gene lists and then searly try to try to overlap them with your list of interests and try to annotate them this way so this is what GSA does with their own set of gene lists I we use the intermodal connectivity the skilled we decay within so the intermodal cognitively scaled between 0 & 1 2 ranked trains kme we use within the gene yes we usually don't so we usually don't have a multiple our input is usually just a journalist we don't input specific samples is that what you're asked this might be possible here but we usually don't use this because we assume that we start with this gene set that's already been statistically significant or validators or so and so this is one of the outputs from GAC a this is all the genes that are in your data set and these are ranked based on how much they are enriched for a specific pathway so this is you will get one cot of these for each of the pathway so there's hundreds of gene sets within the GCSE a algorithm there is also an R package that you can use to run it and to the extent the ranked genes are skewed in one direction or the other this part will be significantly over-represented and as you can see this is the entire set of genes and this is the entire settle and this every time he hits your pathway it will be flagged here and the statistical distribution will tell you whether overall that pathway is enriched in your in your gene list the most youth pathway analysis tool is a commercial one so you have to pay for it and it's called ingenuity and the reason it's not an open-source is because it's manually curated and so there is a lot of work that takes into get then it's needed to create all of these genes based on the literature it is relatively well annotated and also regularly updated they now then our they are now a suite of tools now not just for gene expression but also for analysis of sequencing data you do need to be it is a commercial so we usually usually the best model I think it is an institutional license so the license the entire institution they pay a single license and then all the users within that institution can have it I think so single license is five thousand dollars a year ten thousand dollars for five and the institutional is to 2020 or four or thirty thousand dollars and I have two screenshots of the output from this type of analysis so you output you input your you feel it your gene list and you get a number of measures some of them are very similar to gene ontology in terms of the way the data is treated and some are a little more advanced where you have an existing network and then on your on that network your gene list is overlaid for example within this network only this genes here are red so I would say there are over over Express or upregulated in your gene of interest and the fact that they are there is more than you would expect by chance based on the sides of the pathway and this is why it is presented as a statistically significant result and this is you I'm sure you have seen in publications this type of spider plots showing genes some of them have color meaning that they are from your differential express gene list and so they are either upregulated or downregulated some of them are don't have a color meaning that they are in the network as it has been annotated by ingenuity however they are not in your differentially expressed especially in gentiles and if you click on the edges you will have information about why they have been connected if the evidence is coming from the literature is coming from prediction algorithms and so forth and this type of analysis is often helpful because as you know in many cases results changes in transcription factor activity for example are not related to expression so if a gene a transcription factor is activated it will be activated to post translational modifications like phosphorylation and so forth and and that will change a number of genes downstream so when you look at your differential expression analysis you will see the downstream effects but you don't see the transcription factor itself changed and and so if you have a pathway or some type of map of these interactions you can go back and identify that transcription factor even though it is not changing in your gene list and this is the tool you so this is what we are using most recently it's called enricher it is a repository adding including multiple sources of an efficient actually I can show you I think they have an example set so so they have a gene ontology analysis they have a transcription factor analysis epigenomics road map I think JSON earth must have discussed that encode data it is visually so this is something I can go straight into a paper it's a a curated visualization they also look do do you also look for over a presentation of transcription factor as I mentioned earlier it is often an analysis it's done downstream by looking at genes as opposed to the actual transcription factor itself expression across multiple tissues is in multiple databases one of them one of the first developed is called biogps and it's collecting information from a number of tissues collected with microarrays in in the past 10 years if you go to gene cards Dinkins we link to this so if your gene has data across multiple tissues tissues will be it will show in there so you don't need to go to the biogps website literature one thing that's often of interest is whether our gene of interest is has been reported in the literature and to what extent it has been associated with our trait of interest for example our disease or our condition and so there is a number of websites trying to do that they annotates the literature they identify synonyms which is often a problem for in in data mining and and also will try to estimates the degree of significance over over-representation of a specific relationship between a G and a trait so there is these are just three there's many for example I can show you what the output of one of them is it's called the chili bot and it's a relatively old website but still there they have a system saved results here they you can just look at one of them so if you enter a gene list it will first look for the terms the number of PubMed records it will look for synonyms you can change the actual code you can look sue me you can change the actual query code for example sometimes it makes mistakes in identifying the synonyms maybe I can click on one of them and show you so these are the PubMed abstracts and you will see that some some have the exact gene name and some in some others for example here the software identified this as corresponding to this and it will put it here not so many words do this work manually but I've we find this type of tools helpful at least as a starting point as a screening so you see in many cases it will replace what's in the abstracts with what you're looking for just a gene list you can put actually let me show you you can put three things either a single gene list to two different gene lists or a gene list and a trait a keyword for example cancer your gene list and then some traits of interest a lot of our work is new science and so we also use a number of tools specifically focused on either brain or neuronal tissues so the Allen Brain atlas is probably the largest repository of gene expression and recently also RNA sequencing and proteomics data in brain both in human samples in mice and also in non-human primates so do you have a very good browser they also have a java application that you can install on your computer and go through specific sections of the brain or specific in situ hybridisation so that was at the brain level there is also datasets looking at specific cell types and this is the most comprehensive data set looking word being generated at Stanford and looking at specific neuronal and glial types so there is a neuronal astral subsets associate a society or good end reside progenitors newly formed mature or good answer sites and so forth they have a very helpful just I just told you I'm sure you guys have are familiar with RNA sequencing but in just you remind you that this D irradiated that in the that currently has the most comprises most of the repositories is has probes on specific sections of a gene whereas RNA sequencing which is currently almost it is replacing microwaves it will sequence the entire entire transcript so databases like the Stanford database I mentioned will give you information expressing fpkm which is a measure of RNA sequencing abundance and for example for the tau gene you will have a higher expression neuron and lower in the other cell types link dude it's database there is also splicing database so as I mentioned a sequencing will give you information from multiple for multiple exons so this is the gene these are the exons and you see this is across multiple cell types and in some cases you can see that there is a switch in the eyes form for a specific cell type so this specific cell type only as this exon here which is an alternative isoform of this particular gene so example like this are do the bases like these are for cases where you you can actually get information about alternative splicing from a browser Co expose DB is a Japanese web site so it looks for Co expression so survive I will discuss the annotation pathway analysis and then cellular brain localization now I mentioned a couple of core expression tools one of them is so you you have discussed network analysis as a way to look for expressed genes these are much more primitive methods just looking for things that are correlated in a standard linear fashion but still potentially helpful for especially if you look in large databases and not time consuming and essentially free so this is one of them Co expressly B and I'm sure you've also discussed the gene Network a tool we like because it's trying to dissect genetic influences on gene expression in recombinant mouse strains you you're familiar with Israel in two more tools one developed by looking at protein protein interaction and it's called string so if you have a gene a differentially expressed gene list you might have an over-representation of proteins they interact with each other because there may be part of the same complex or the same a protein complex and so so this is the type of question that this database is try to answer you feed this is the second kind of warm after geology there is a protein versus RNA you know it's everybody's like there's a so let me say first of all the correlation between gene expression and protein levels is not as high as we would expect however there is two levels of noise one is coming from the gene expression data the other much larger is coming from proteomics data so I would say that the the low degree of correlation is at least to some extent a technical problem first assumption the second assumption is that when we look at gene expression we assume we accept the idea that that is reflecting protein levels because that's what the proteins do things in the cells so we are looking at gene expression analysis as a proxy for protein levels and yes this is an assumption and as all experimental assumptions may have some limitations so not all of the differential expression genes not all the genes we see differential Express are necessarily also different at the protein level it is okay and not only did you specially people do that also for juwes so june wide Association studies results they have a list of low-side that are significantly associated with your straight so not even the expression level at the genetic level and then they look and see whether there's a enrichment of genes that are supposedly interacting based on this database this is also termed the problem with this which you haven't mentioned but which is that the fact that many of these interactions are potentially false positives they are results of the two East's hybrid screens or highly it's highly artificial conditions and many of the interactions that are in here are not experimentally validated they are just predicted so that's an additional value know provided that you are aware of the limitations all the world the limitations we have described yes I would as a reviewer will have a problem somebody said I see this differential expansion list this is also I see that also in a protein protein to actionable so then I conclude that it is true I think that's a little and the last story and this is a game for biomedical research is called dapple and it's a similar idea so instead of giving you over representation of in protein-protein interaction it will give you it will take as an input genes that are resulting from genome-wide Association studies so a set of states from a genome-wide Association study and then it will look for protein-protein interaction Co expression and a number of additional repositories and it will tell you whether you see more than you would expect by chance your own gene list the last set of tools are tools that we have developed here at UCLA the first has been developed in dan gatherings lab by BJ chandran is a postdoctoral fellow we made a web interfaces it's at this website you can sign up for an account and and analyze your own data so the essential analysis is done by this tool is the following so you have a set of genes that are in your differential expression generates you this tool will take all the gene names will go to a genome browser and retrieve the promoter regions of all these genes which you can set I think it can be one Megabass upstream or half a mega base then we'll take all those regions and we look for known transcription factor binding sites at the sequence level the first thing this is something that many other also other databases do and the second thing it does and third are the the the second it will look for things that are not currently known to be transcription factors so we'll just look for sequence motifs that are significantly over-represented within this set of promoter regions that's the second and the third they will link it to literature resource so Vijay put together a database a little literature database similar to the chili bot tool I mentioned earlier but in Dessau you will look for co-occurrence of a transcription factor of your gene lists and in the literature database and I think it's not been not published yet but it's I think I'm the review as a tool it's a commercial database yes so this we had a license first we bought a license for transfect and I think Vijay used the most recent version to generate his prediction analysis and then that is within this database in terms of the actual matrix that is publicly publicly available we had to go back to because there is release server for example the matrix from five years ago it becomes public and the most recent only you have to pay so we went back to the most recent public free version there is a public version but it's just older than the current one so to answer your question the most updated was used to generate the tool but the ones that's online that is online is the last the first possible publicly available not a fair question it is about what is the largest super large number of websites provide this service so how do we orient ourselves and I our approach is usually to try a few maybe a couple the added value here is just the fact that they will look for it individually these things are done but not as far as I know not together that's the it will allow I will avoid to run to and you were not only one and the second thing is also looks for literature or expression I think he just added also my Kearney so it's trying to be more comprehensive but yes it's not necessarily it is putting together pieces that are already available algorithms that are already it is so it was done for a study where for peripheral degeneration so neuronal regeneration so it is I would say it was generated on your own I did it but I don't think it's only for neuronal datasets it doesn't say here so I think I think it doesn't have a limitation in terms of cell type the second tool we developed here again in dengue Sheen's lab is neuronal specific and it is a way to predict neuronal developmental stage using gene expression so Jason Stein postdoctoral fellow in the engagement group took collected microarray data from a number of cell cultures from neuro blood cells that are developing in vitro and compared this gene expression profiles to developing brain so data that coming from the airlin brain atlas and then asked the question to what extent can I predict the age of a brain sample based on gene expression data in vitro and this is the point of this paper which was published in neuron earlier this year it's called the algorithm is called context and they will take as an input a gene expression data set and they will tell you what is the developmental age this is gene expression data set is which what is the best match in terms of developmental age so this is only restricted to neuronal cultures and developmental brain data sets the last one is a tool called Network browser we are making available so we have generated Network parts for a number of papers including this one published a couple of years ago Intel and but we are making available on our github page it's a code repository also the code to generate this type of plots and I thought this would be interesting of interest to you guys since you have been learning about network methods once you do this type of analysis a it's usually helpful to show it on a web browser and I can show you this tool from the paper so these are the so you know there is modules there's multiple modules in each of the network analysis and and so you can get information about a specific module by clicking on it and you can control the number of edges you are showing you can get information about a specific gene that is in the nectar what's the connection if you right click on it it will link to gene cards the tool I showed you earlier and other the brain span which is a brain Atlas database are made and if you right-click on an edge it will we look for PubMed results from I I don't I'm not accurate enough from here for PubMed results Azhar including both genes that are connected by that edge this is public this website this is an if you could do you see it it's in my slides the link is there but this is still about the the code we will put it on github later but this particular analysis is already there it's using our using our code you will be able to do that absolutely yes yes using no we have to edit the codes that we will put on our github page and we make it so two things one if you want to do it now you can just email us and we will do it it takes really an hour so we can and we will make a webpage for your data but in two weeks we will have a web page with the tutorial the the main problem with making a cold public is to rise a documentation right so that people can use it that's what it's not been done yet yes I think Neal who did this analysis provides provided the connections and I think it's based on correlation between module ID and jeans I think here based on topological overlap yeah this is a on the web side it's essentially taking those two names and building this this URL and then you click on it and you get so it could be that there is no hits it's just a building in factories it is three jsut right here I see a technical it is like the exact it is so however this Anton cytoscape are more powerful in control as you can see this thing as a mind of its own is like decides how to to show to visualize the genes and we can control that whereas with with Vasant and cytoscape you have more and one other thing I want to show is that you can also add metadata to specific genes so you can either have information about a module or completely separate tools and this is and this is usually a good companion for papers so when you present publication okay so in the last section of the talk I wanted to go over tools to handle gene lists not individual genes now it's addressed by the entire set of genes and these are tools that have all be developed here at UCLA the first one is called repair and it is a gene expression repository similar to geo however what we have what we have here is not raw data but it's data it's been already analyzed so we don't have individual samples but we have ratios across average of samples we have a demo of a tool called HD HD which I will present in a minute which has also access to repair this is a set this is an account but you can apply for an account here there is not member register you can register for an account and we will give you access to the public version of repair the first screen in this database is a set of studies so you can see the experimental name when it was uploaded or was the user what is a platform whether it is iron a sequencing or microarrays the species and and more methods so that the matter than the actual specific platform and actually I can just show you live how it works it's my mouse so this is a suite of tools and repair is one of them and one thing that yes I will so this is one account it's my account as a number of datasets but not all that is if we can control access to specific users so every user can only see their own their own data and I'll I can take one of the public repositories for example this one public databases and I can get information and also edit information about a specific specific gene expression set and so there's two ways to credit this database one is by if you are interested in specific chains so for example I'm interested in the expression of a specific gene in the in a specific data set so I select the data set and then I look for a specific gene so for example gap for the tree you just press Enter or since inside zoomed in there is a there's a button down here there's a button down here says go so we will first tell you what are the probes that are part of the that are on that particular array platform and they will then give you the expression difference of that those specific probes in your dataset so this is not very different from looking it up in Excel however it is first less error-prone because you might know that Excel you know it's easy to mess up and change the color order and everything and the second is that we linked to our databases for example this is that bombards database the Stanford cell type-specific database we linked it to here we also linked to two other resources like the gene card tool the rad database the Allen Brain atlas virus database and so forth so it's a relatively easier to browse data and it is also as a link to the tool themselves if you click on this line it will tell it will go specifically to that database and show you the expression of that particular gene so that's the first way you can query so for individual genes the second way is to look for specific gene lists across studies so my goal is not to look for a specific gene but to look for gene lists for so let me go back to the the same study DRG I'll just take one random example you can choose your comparison of interests and you can change the statistical Thresher you can see you can be more or less permissive in order to get your results number of genes so for example here we get 600 probes if I change the threshold to something that's more permissive then number hopefully will increase increases because I'm less stringent and so one one way to deal with this once you have your the criteria you like you can actually show those genes and and the interface is similar to the one I showed you earlier there is a color coding based on pole change an absolute expression level and annotation from other databases you can export this to excel or download or run and rich mint analysis using the enricher function which I'll mention in a moment that's one way and the other way is to compare to other studies and here I think it's where this tools tool is more helpful than than Excel because you can add additional studies to the one you just mention selected so you can find another study which could be a different contrast from your own study or a completely different study that has been imported in this database from the literature so I can look for another DRG study Oxana crash it could be a completely different species as long as there's there is some overlap in terms of gene symbols and and so you have a number of probes here I can again relax the criteria and look for so and I can add a third one too but what I can show you here is that once you have to differentially expressed gene lists from two different studies you can then ask to compare them and you can and you you have here the output of you know a Venn diagram you have things that are shared between the two datasets things that are only the first manat in the second and things and vice-versa and so that's an app for I think fast way to compare data sets this specific data set I I really randomly chose it but it seems like the overlap here it seems in the opposite direction so that they are the same genes but they are changing in opposite direction and you can also decide how to group them these are currently staged by default or groups by gene name but you can if I can do that from here you can drag and drop the header to the top of the and group them by probe name or by contrast and so forth if you are interested in proteomics this is not it's not ready yet but we are also developing a tool to visualize proteomics data this is a still in beta and but the idea is to show on a browser the as you know programming math spectator's single peptides and so you have multiple peptides from for each protein and so we developed a way to show a protein structure and then to highlight peptides as they have been identified by your mass spec on the protein itself and this is helpful for mining proteomics data which we don't work a lot on but we will work more in the future yes - excellent yes so so for my credit probes then how do I go back images for micro probes you can get this information from the the protein the gene lists itself so let me show you one gene list so if you click on the array the probe itself you have the sequence did the actual probe sequence there was on the array and if you click on it you will go to the genome browser and show you exactly where that sequence is mapping but this is a mockery so it will tell you this is this is the gene and there's there will be somewhere I think it's here I can't see from here but there is usually attract it shows you your sequence here it is it says your sequence from blat search that's where the sequence is mapping and this is in the on a human genome browser that's for my curries you can do that also for I&A sequencing and that is a different we also have a browser for RNA sequencing data and let me see it will take me a while to get to an RSA and a sequencing but that will show you the entire set of reads that are mapping to a specific gene I think one of the hopes okay I think it will take me a while I'll show you but we also have a browser that 4-iron a sequencing data so we went to the to the sub list and the query and the sad results and I mentioned how to compare multiple data sets the hdhd resources the tool we I mentioned earlier it is focused on specific disease Huntington's disease and it is a repository for gene expression using the repair tool I mentioned and also additional tools that we have developed and one of them is this gene set enrichment analysis which uses the Unreal development kit available for the HD community first but soon there will be an open tool for annotation of WG CNA data as well and the output is similar to a gene ontology analysis however the actual gene sets are from a number of network analysis modules curated by Peter and you can include your own gene sets if you have other analysis if you have done in the past you can you can include your own gene sets and look for overrepresentation of them your gene expression analysis so in conclusion I hope I clearly explained a number of ways we can use existing resources and develop the new resources to make sense of large-scale genomics data and I hope this will as you see here the time to generate data is large for this type of studies so I hope that this pipeline will save you some time and this is are the people that I've that have created the tools I've shown you and in my entire group is interested in this type of development thank you for your attention "
Ypn1XKjRuwc,27,"Data mining is the process of sorting through large amounts of data and picking out relevant information. It is usually used by business intelligence organizations, and financial analysts, but is increasingly being used in the sciences to extract information from the enormous data sets generated by modern experimental and observational methods. It has been described as ""the nontrivial extraction of implicit, previously unknown, and potentially useful information from data"" and ""the science of extracting useful information from large data sets or databases."" Data mining in relation to enterprise resource planning is the statistical and logical analysis of large sets of transaction data, looking for patterns that can aid decision-making.

This video course teaches how to do post processing in Data Mining. The course has been taught by Rajesh Natarajan, Hexaware Technologies

For more videos on this course visit:
http://www.siliconindia.com/university/index.php",2008-07-04T10:32:24Z,Post Processing in Data Mining,https://i.ytimg.com/vi/Ypn1XKjRuwc/hqdefault.jpg,vimaliswamy,PT7M26S,false,1574,0,1,0,0,"now association rules has its actually origin in 1993 whether one famous work by agarwal molinsky and they and Swami in this was a paper published in ACM sigmod conference before that because my before this was the start of Association the start of research in association with 1993 they published a paper on mining huge dodge databases sales databases and finding out patterns single now association rules basically are they bring out here in relationships hidden in the sense that they're not interested but they are all co-occurrence placed for example if you look at look at your say grocery shows or a retail store and see how people are purchasing items together so so you can buy bread and I also buying better together so that is a co-occurrence so can you come up with some interesting co-occurrences that is what association rules bring out now the basic structure is very simple a implies B where a and 3 are both set up items when the important thing to notice that there is no common element between anp so if you are saying bread and butter implies say jam is nothing common between bread butter ice on the decision side and comma and butter on the you know the consequence side there's nothing common between them is the items are not one item checkers magazine cannot occur in a comma and there are mainly characterized by two measures which are called support and confuse the support is basically the strength of the rule in the database so it basically is the probability that if you look at our own like bread and butter the probability that both Britain whatever leave at least if you're seeing it now why is it important because if the if the strength is very less say about 1% not quite one percent and it doesn't make sense for me to actually look at however if the strength is enough if it is high then it means that there is something that is this patternists am not strong and this no people have been purchasing the same kind of people are in purchasing the exhibit in the same behavior Prime anytime I being so can I do something about it way so that is not support takes you under I have confidence tells you is basically is that if a person purchases bread how likely is the person how lightly with what assurance can say that a person will also very likely to purchase but it so given that decision how likely i will the consequent is also true so that's what is said here it is support confidence of eighty percent means that if a person has purchased bread and i am sure that eighty percent of the times is he really purchase but of us so well great inverter go together pretty possible I just briefly touch upon how you rip this association rules without going to reduce of them basically is a two-step process the first step is to find out all frequent patterns frequent item patterns like all item sets which are frequent that is my support is much more than the minimum support so that is the first step so that may be the case that may be bread and butter or some other set together a purchase for more than say ten percent of the transactions some other set of items say look at pencil + erasers I am say some other registration area tambien purchase for more than say five personal projects so these are frequent itemsets once you get a frequent itemset does anyone have the combination you put two in artist in one hour consequent see this applies a certain confidence criteria once you get that then you have the rules so these two steps takes place through now various there are various methods button that most famous being the a priori algorithm and which basically used a very simple good but a key point here is this is a massive search through the database but in searching each and every transaction you're not going to do any estimation so it is a complete such and the parameters of the ruler exact so if you are saying that if it is it is eighty percent confidence eighty percent means that it is there is eighty percent confidence interval it is not an estimation of any salt another important thing is that if you have two or three algorithms which will give you our secession rules all the algorithms will give you the same results same set of rules so you cannot evaluate and I'll go to them based on what result it gives electrification rules or any other kind of news Baron you get a different foundation to be you have if you have a que if you are clustering k-means and it might give a different cluster than other and you evaluate this Muhammad well it was it performs on feature later but in this case all algorithms will give you the same set of rules so how do we aware it it's basically our demands on input output memory and computation so much in some cases it will take a single pass through the database to some algorithms will take single pass I might take multiple process somewhat is more memory and so the last point is very important which is rather entire problem starts is basically the it's a very high degree of automation basically you give only two things the support threshold and the confidence station and then the algorithm search through the database give you the rules because of that you have very little user interaction user is not able to gain the algorithm is various stages and that leads to many problems a rule immensity problem which can be split up into two parts which are interconnected which is a rule quality problem and the real quantity problem now the role quality quantity problem is basically that very large number of guru sucks so this was the first study by Sergio bring Google in money and others sanford olicity and in 97 when they looked at on census database via census database and they got about twenty three thousand seven and 12 groups of course if you change the parameters of the rule you will get different sets of those so you're in the same parameters they got some 20,000 rentals obviously I don't think anybody will sit and wait through 20 3212 rules to find out which is the one which is more normal which is the one which you were actionable there across the pond again it's a smaller problem when you look at telephone company fault management basically they are looking at which are the kind of telephone for peripheral faults which occur in a network within a given span of time say suppose first before trackers and within five seconds or 10 seconds any user demand defined limits you have three four faults coming so that the entire group of faults can be categorized us say which is this is a problem in one area does this fault this what a occur with talk called see again again which means that these two areas might be related to it so those kinds of knowledge you get from so they followed 1004 introduces "
5O5JOCXcRmo,24,In this lecture we will cover the basic concepts behind machine learning and data mining with emphasis on supervised learning and classification. Without going into details about classification we will talk about how to evaluate classification performance.,2014-01-22T23:48:45Z,UVic MIR Course Data Mining I,https://i.ytimg.com/vi/5O5JOCXcRmo/hqdefault.jpg,George Tzanetakis,PT48M26S,false,N/A,11,0,0,0,okay so welcome to the music information retrieval course at the University of Victoria so again I'll do my say hi to anyone who is watching this armor so far no one is watching live but people will watch it later on video so the topic of today's lecture is going to be data mining so in some ways what we have done so far is we did a crash course in digital signal processing just the bare essentials to get us going with spectrograms and with being able to compute audio features and now we're going to move into another part which is kind of foundations which is data mining so you know you can take a whole course on data mining and machine learning so of course we will only cover the basics but that will give you enough of material combined with a digital signal processing to move into building actual music information retrieval systems so at this point I will try to switch to screencasting so I will do this screen share and hopefully it's now showing the slides so if at any point that's like disappear if anyone is watching here let me know so that we don't have what happened last time so I'm going to talk about data mining and there's going to be two parts to the lecture today an introduction and then somewhat unusually what I will do is I will talk about evaluation so usually when you learn about classification you are taught the basic idea then you're you go through a zoo of different classifiers in the end you learn how to evaluate what are the procedures for evaluating it in my opinion the procedures for evaluating are more important because you can easily find implementations of classifiers and that's why I start backwards so we'll pretend it there's this magic box that does the classification and we'll just see how we can evaluate it so in a nutshell I love this quote this is one of my favorite quotes it says essentially all models are wrong but some are useful and it's a quote by a guy named George box who actually passed away relatively recently last year so he got interested in performing experiment in statistics after performing experiments where they were trying poison gas to small animals to see sort of how many would die with certain exposures and so forth and it's a very important quote that you know there's no correct model we are always approximating in some ways but some models can be really useful so that's the idea behind data mining and machine learning is we'll build some models they will not be perfect but they will get quite a bit of mileage out of them so a little bit about terminology the same kind of ideas have different names depending on which community works on them so when computer science theory people do this they call it machine learning at some point database people got wind of that and decided well we can use that with data big data and started calling data mining and if you talk to a statistician they would just say well they're just doing statistics and they are calling it something different so it really is statistics under the hood and it gets the fancy name of artificial intelligence if you are in applied computer science but then they're lying ideas behind these categories are the same and there's significant of although each term has its own flavor in one thing that I think is really important to understand that it's something that even I've been working with machine learning techniques for close to 10 12 years now I still haven't completely got in my head around this is that it's a different way of programming a computer really so we know how to program a computer by giving these precise stepwise instructions when we write in a programming language but we can also have a computer perform some task and in many cases tasks that we could never write an actual computer program that would solve that task by these methods of data mining and machine learning so we will be looking through that so it looks like we have two views maybe they are all in this room but if you are viewing this from a way and if you could mute your microphone that will avoid some problems with the hangouts looks like everyone is muted so it's fine okay so the problem setting so what is the idea of learning what how can we say a system learns so a computer program this is a definition by Tom Mitchell who has written a really nice book on machine learning a computer program learns from experience e with respects to tasks t measured by a performance measure P means that the performance of these tasks as measured by B improves with the experience so this might sound a little bit either too obvious or too confusing so I'll try to give you a specific example a chess program learns from playing chess with human players so you have it be online in it will play games and the measure of its performance is how many games that wins if it is a learning program as it plays more games it will start winning more games so the key issue is the improvement is not the absolute performance so you might have a chess program that's great that beats a world champion but if its behavior doesn't change with experience it's not learning you might have a very lousy chess program but if it's a if it's performance improves with time then it is a learning algorithm okay so so some definitions so classification or supervised learning is the machine learning task of predicting or inferring the class of an object typically that the object is represented as a vector of numbers by analyzing training data consisting of these feature vectors which represent the objects and associated labels so again this might seem a little bit unclear but here is an example so I just want to check the slides look okay in the Hangout okay so this is a problem so suppose I give you a database that contains the height and weight of a thousand people as well as a binary attribute that tells me whether a person is a basketball player or not a professional basketball player or not since we are in Canada you can substitute basketball with hockey that makes more sense so you're giving some training data and then I give you the height and weight over hundred new people and you are asked to predict whether they are professional basketball players or not and that's called the testing set so for that said we don't know what they are and the metric of performance is how many correct predictions we will do ok so now I'm going to give you a specific example so this gentleman here can't even remember what his real name is but he's called the rock his nickname is the rock he's a wrestling guy he has played in some movies and he's a pretty big guy so the question is if you weigh as much as this guy and if your height is as much as this height is as much as this guy are your professional basketball player or not so how many of you think he would be classified as a professional basketball player by height and weight okay maybe five how many of you think you would be classified as a normal human being okay so now I'll just show you a picture this is a the rock between Charles Barkley and Shaquille O'Neal who are professional basketball players and as you can see she looks like a tiny  so he you know even this really big guy is small compared to basketball players anyway this is just to lighten up the lecture so here's how this would look from a machine learning perspective you would have some training data here on the left so this is your training set and as you can see I have height in centimeters weight in kilograms and a binary attribute whether you are a professional basketball player or not and so a feature matrix is a 2d matrix where each row so each row which we call an instance or a sample corresponds to one particular person so this row right here could be the row for the rock or the row for one of you so it gives you a height and a weight and then you can also think of the columns as attributes so height is an attribute Wake is an attribute and depending on the problem we'll have a lot of features so these are the numbers that characterize a particular object and then we have an associated class label the zero so this says that this particular person has a height of 195 a wake of a hundred and ten and is of type 0 so finally you have a column that first ones to the attribute so this is the definition of a training set and a testing set and essentially the name of the game is how can we use this part as input the left matrix to build some kind of program that will take us in put one of those rows and predict with reasonable accuracy what a class will be for these question marks okay all right so let's move on so let's formalize this a little bit with mathematical notation so you have a set of training vectors X I they live in a d-dimensional space of real numbers the I is the index in terms of the instances and n is the number of instances and these the dimensionality of the feature vector so d in the case of the basketball example would be too because we have only two attributes and n would be a thousand because we have a thousand examples in our training set and the associated ground truth classification labels can be written as integers why I with a subscript I that connects the two ok so so a classification algorithm typically supports two operations so train the train operation takes as input the label training set and outputs a model and a model is some kind of representation of a computer program that is a representation of the classifier for that particular problem so the model is a model for a particular problem so it would be some procedure for determining whether someone is a professional basketball player or not and then the other mode of the classifier is called prediction and that one takes us input a trained model and an unlabeled testing set so that was the matrix on the right with the question marks and produces a predicted label test set which is a 1 V vector of labels so after classification the question marks here would be replaced with predicted labels okay some of them would be correct some of them would be incorrect and a good classifier would have a lot of correct predictions a perfect classifier would classify everything correctly okay so at this point are there any questions all right I will quickly check here so we have two online participants if they want to ask any so anyone who is online if you have any questions feel free to put them in the group chat and I'll sort of check with the corner of my eye so okay so now let's continue with evaluation and this is really important to understand the valuation ok so the evaluator so suppose I have my classifier and I predict my labels I sort of predict who is going to be a basketball player and who is not I need to somehow know what the correct answer is so in fact even for the testing matrix I do know the lid have the labels in order to evaluate the performance but I don't use that information in training the classifier so I have two labeled set one label set is used for training another labeled set is used for testing does that make sense ok so so the evaluation consists of taking the predicted enables the ground truth labels so two vectors comparing them somehow calculating some metrics and telling us something about it so the classifier is really treated as a black box so one example that I like to give is suppose you are a company and you give a contract to two companies to build a classification system they don't tell you how they do it they just give you a program you run it you see how it does and you pick one versus the other you don't know how they achieved at us they might had a human to do it or they might had a machine all you care is how well they do it ok so an example the classification accuracy can be computed as the percentage of labels that were correctly predicted so the ones were the ground truth and the prediction label match so you just count how many times you have both labels be the same okay so in order to evaluate classification ground truth is needed not only for the training bad for the testing so usually you have some amount of data that is labeled which comes from the problem set usually labeling is labor intensive so you just have some collection of data with labeled so suppose I'm a company I have a database of Records I have the labels but I want to make a program that can predict new records so I need to have some kind of scheme for splitting those that label data into two parts one part that i will use for training in one part that i will use for testing in the company scenario i might give the training data to the contractors and keep the testing data myself and then when they give me their programs i will run it on the taste testing data that i have kept and see how well it does okay so this is the scenario that i mentioned in this slide you contact two companies and they don't want to tell you how you do it they just provide your black box but the point is that you can run that black box on any data that you want so you don't have to run it on the data that you gave them you can run it on data that you have kept secret to see how well they do so so how do you choose which company to to use okay so obviously you could say okay how well did you do on the data that I gave you one company might say I think we are doing ninety five percent classification accuracy the other might say I think we are doing eighty percent classification accuracy you might be tempted to say okay I'll pick the one with the highest accuracy but then you say well let me check on some data that they have not seen you check that data and you find out that the company that said that is doing ninety percent is performing at sixty percent and the company that said 80 is performing at 82 in that case you will probably pick the eighty percent company because their estimate was reliable okay so this is kind of the game you have to play with machine learning is you have to think about that scenario because we're not interested in the performance on data you have the classifier has seen we're interested in the performance of the classifier and data that it has not seen so just to make this a little bit more concrete if you have some training data let's say you have an estimated of the accuracy that the contracted companies somehow do so they take the training data somehow they estimate how well the algorithm will perform that's the ninety percent and eighty percent then I have the company that's contracting has some data that it's keeping on its own to keep them honest so let's call that GA for generalized accuracy so that's the accuracy estimated on new data so the real challenge is how can I estimate EA so that it's as close as possible to GA so as a company I want to have a good handle of how well will my classifier do on unknown data but I don't have access to that unknown data so it's what is a scheme I can use to sort of simulate that scenario okay so with me so far any questions alright so so this concept in machine learning is called generalization so generalization is the ability of an algorithm to perform well on new examples of data that are not part of the data that it was trained on okay so estimating the generalization performance is not trivial but it's at the heart or a good classification system and the reason why it's not trivial is because you have this problem of overfitting so overfitting is when your model starts trying to fit the data too much and starts modeling noise and errors in your data so if you have a very complex model it starts to capture your data really well but in a way that doesn't generalize very well so when overfitting the estimated performance of a machine learning algorithm can be misleading and far from the true generalization performance so a simple example would be a memorizing classifier so suppose you if you find an entry that's in your training set you classify it with the label that it is at the training set that classifier would work perfectly well for your training data it just matches everything to its own self but if you give it anything different than what it has seen it has no way of determining what to do okay so that's the concept of generalization and overfitting are very important so there's different schemes for splitting the training and testing set so now we are at the perspective of a company that has received this data and tries to build a classifier and it needs to somehow get a good estimate of the generalization accuracy so to do that because the results can be misleading as a result of overfitting if you train the data and test a train the classifier and tested on the same data that's a methodological mistake the company is not if you want to test it on data you have not seen so basically what you want to do is have some scheme for splitting your data okay so we'll look at different schemes so one simple scheme would be to split it as a percentage so you say okay i will use 75% of my data for training i'll keep 15 % of my data for testing so there's a trade-off there if you have more training data these models are statistical so typically the more data you have the better they work however if you have more testing data then you get more reliable results in terms of classification accuracy for example if I only predicted one basketball player well maybe i predicted it correctly or incorrectly it will just give me either a hundred percent accuracy or fifty percent accuracy that's not particularly informative okay so so somehow we have to deal with that trade-off but you can't kind of have your cake and eat it too because you're not you don't have to do this only one time so one other idea is you can do multiple splits so I can choose to pick some part of my data for training in another part for testing and estimate my accuracy and then do that again and estimate my accuracy and do that multiple times with different partitions and somehow combine the results to produce a final result okay so just a side note this is more of an implementation thing but sometimes it's useful to keep that in mind because sometimes you do that type of data shuffling on your own frequently if you have a feature matrix you need to shuffle the entries so you just need to reorder them randomly so sometimes this happens when you do multiple iterations of splitting into training and testing some programming environments provide you with random permutations but in some cases you need to implement them so it's relatively simple to implement a random permutation but not completely trivial so if you want to look it up look for new some shuffles and things like that now because feature matrix is typically our matrices are typically very large when you do a permutation you use what are called external all permutation vectors so the feature vector is left unchanged in memory and all you do is you access the matrix through and min decks that is / muted so i'll show you a picture so here for example you have your NBA players the original order would be 1 2 3 all the way to 99 if i had a hundred vectors a permutation would be rearranging these numbers to let's say you know these are random numbers but there is only 170 in this column there's only 18 it's just a shuffling okay so you can't do a shuffling by simply picking up a random number in putting it because that would result in duplicates some make sense so now if I want to access the permutation I just go to the 17th century and read it and then I go to the atria entry and read it so i might have multiple of those permutations so this is just a side implementation detail okay so so now comes the one of the most basic validation schemes and one of the most important to understand and that's called the k fold cross-validation so in k fold cross-validation you start by doing a shuffling of your data so first your data gets shuffled around so that it's kind of random then you partition the data into k folks so you make K subsets of your data so it would be if you had a hundred maybe you do 10 10 10 10 10 or if you have a thousand you do if we do 10 fold you would have set that has a hundred asset that has a hundred it set that has a hundred okay then what you do is you use one fold for testing and the remaining k minus 11 folds for training so if i had a thousand samples in each fold was a hundred i use 900 so nine of those faults for training in one of those faults for testing and then I change which one I use for testing so I use another one for testing and the other nine for training so because i have k folds i repeat that k times and i get predictions for each fault so the total number of predictions will be the number of instances because if i have in one fault i will do a hundred in another i will do a hundred in another i will do a hundred predictions by the end i will have done a thousand predictions so in terms of number of predictions you do the same number of predictions as you would if you use your training set for testing and then you can just calculate the classification matrix you need based on ground truth and prediction so there is a little bit of a trade-off if you choose K to be large then you will have large training sets because you don't have many faults so if I choose K to be too I would have basically two cases 500 samples in 500 samples so my classifier would get kind of bigger but the so you get lower by us but at the same time I would have higher variance because the training sets all end up being similar so there's a little bit of a trade-off there but sort of stand our numbers one number that comes up all over the place is tenfold sometimes you see fivefold sometimes you see threefold but those are not notice that there's only one initial shuffle a fixed number of iterations one for each fold in no estimate of the variance in your estimation like you just get a single classification accuracy when the number of faults equals the number of instances then then you have a case where you use all your training samples except one for training and you test only with one and that's what's called leave-one-out evaluation leave-one-out evaluation is quite accurate because you use so much training but it's also very costly if you have very big data set so it's only used when you have a really small data set ok so it's k fold cross-validation clear are there any questions okay so then I will move on into a different kind of procedure which is called the bootstrap estimate so unlike k fold cross-validation the bootstrap what you do is you pick random samples from your instances so instead so for example in k fault what i would do is if i had to do it in this class i would great you let's say in three groups and then i would say you group is you are going to be testing those other two groups are going to be trained and then we're going to switch around but everyone would stay in their groups instead I could say okay I'll pick 10 people randomly and they will be my training and then I do something then I pick I tell them sit down again and I take 10 people randomly so the same person might be picked again for training whereas in in k fold cross-validation you never pick the same portray the same instance again for testing you only test once so it's a little bit like sampling with replacement or sampling without replacement so the same sample might be selected multiple times in some samples might not be selected ever so i might do bootstrapping in this class and you might never make it to training but we sum up you can show that if you have n instances approximately sixty percent of them will be selected and if you randomly take numbers and approximately three thirty six point eight percent of them will not be selected so what you do is those will be distinct instances and this is because the same instance might appear more than once so what you do is you randomly pick n samples and then you use that for training and the remaining instances are used for testing okay so you can repeat this process as many times as desired and that gives you a handle to the variance of your estimation so if I do that let's say a hundred times and every time my classification I Tracy comes out as eighty percent that's a pretty good results if sometimes it's 90 sometimes it's 50 sometimes it's 40 even if the average might be eighty percent that will be pretty terrible because it means that depending on what data i use my classifier is all over the place so the variance of the classification is important so does everyone understand the difference between bootstrapping where you randomly pick samples and then you might pick the same case again and folding where you just partition and then shuffle the partitions now it's very common that you people confuse these so sometimes people are doing bootstrapping and calling it cross validation or doing cross validation and calling it bootstrapping so this is at least to my knowledge this is the more correct terminology now there's a lot of variance on these classification schemes so there is repeated cross-validation so what you can do is you can shuffle your data perform k fold cross-validation estimate your performance metrics and then we do it again with a different shuffle and every time you do it you get an estimate so then you have a sequence of estimates and you can compute their variance so that's similar to bootstrapping you can do multiple rounds and that's called repeated cross-validation and another variant is stratification so suppose you have a problem where the class sizes are very unequal so for example let's say you're trying to build a classifier between male female students based on their records in terms of height and weight and I don't know whatever else other attributes you have if you are in engineering there's a small percentage of female students so what would happen is that if I did something like 10-fold cross-validation it might be the case that my training set might have no example of a female student or my testing set might have no example just randomly because there's so much of an imbalance in the size of the each class so one thing you can do instead is do stratified cross-validation so what that means is that you do the folding in every class separately and then you merge the classes so if I had let's say a thousand male students and 100 female students instead of mixing them up and doing the partitioning i would say here is a fold over a hundred male here is a fold of ten feet now I have a fold of 110 that's guaranteed to have a hundred male and 10 female so that is stratification and you can do stratified cross validation or stratified core bootstrapping basically you just do it separately for each class and then join the results and then run your evaluation matrix ok so this ensures that the number of instances for each class in each fold is approximately the same as the distribution of classes in the training set so I know this is it sounds a little bit tedious and boring but it's it's important to kind of get a hold of this because it really will help you understand how to interpret classification results are there any questions alright moving on so so there's other potential variants and these can be actually quite important for music so sometimes the folds that you do might make sense if you have some additional information so if you have multiple collection experiments or time information is included you might want to do it that way so if you're doing like music tracks you might decide instead of doing random folds to do folds where one fold is all the tracks in one here another fold is all the tracks released in another year and the year information is a side channel that you use to do your partition so in this case the analyst provides an extra vector of integers specifying the fold index for each instance so that's a case another common scenario in music is what is called artist filtering which means that you don't want pieces of music by the same artist to show both in training and testing because the idea is that if they come from the same artist maybe they are recorded the same way maybe they are very similar and by splitting them into training and testing you make the results a little bit better because those are easier than not so what you can do is simply ensure that an artist gets either exclusively allocated to training or exclusively allocated to testing so that's called an artist filter subsampling if there are many instances so if you use big data sets then the training time for a classifier can become prohibitive so what you can do in that case is use a random sample of smaller size for training and testing and that works pretty well and if you have an iterative algorithm so in iterative algorithm does multiple passes over your data instead of doing a picking let's say let's say I was doing the algorithm on all of you instead of picking ten percent of you and running the algorithm on that ten percent again and again and again as it iterates what i can do is every duration i can choose a different subset so it's a little bit like trying to do a little bit with some data then using a little bit of other data and it turns out that doing that scheme in some cases can be just as good as running it over the entire data and be much more efficient so there are some efficient algorithms for training support vector machines for example that are based on this idea so now we go to measuring classification performance so once we have done these experiments we have basically a vector of predicted answers and a vector of ground truth answers so for each instance we will have one label for the ground truth and one predicted label by the classifier so all classification performance metrics are based on this information for some the only thing that matters is whether the prediction is correct or not some others also matters what the class is and what the type of error is so we'll talk about that so the output of these measures can be a single number a matrix or a plot either character rising the entire experiment or a specific class so I do have a live question that will be interesting so how come on our parallel training algorithms addressing the matter of prohibitive training time yes there especially these days when people are really dealing with huge data parallel implementations of training are quite common but even with a non parallel implementation we can still process quite large amounts of data with the right algorithm with today's systems ok so hopefully that answers that question and there we go back to this so classification accuracy is an easy one to understand so it's the number and you can also calculate it for each class separately so i could say i have an eighty percent chance of predicting correctly if you're a basketball player or not or i could say that ninety percent of basketball players were correctly predicted as basketball players and eighty-five percent of no one basketball players were correctly predicted as that a more informative structure is something called the confusion matrix which is a c by c where c is the number of classes matrix m where each element shows the percentage of indus instances with ground truth label I that were predicted with label J so this will become clear with an example but the confusion matrix reveals information about how classification in this classification are distributed among different classes so here's an example that's actually from music information retrieval application this is a classic problem of automatic music genre classification so the overall accuracy is around eighty percent and then this acronym stands for the following genres we have blues classical country disco hip-hop jazz metal pop rock and reggae so if you look at the particular row if which is the easiest general to classify just looking at this matrix classical because it's the diagonal in its ninety-four percent so what this means is that 94 of classical instances are correctly classified as classical music one instance of classical music was misclassified as country three instances of classical music were misclassified as jazz and two instances of classical music where misclassified as rock music okay and if you look at this matrix it kind of makes sense the hardest genre to classify is rock music and that's because rock music tends to have a lot of different styles in it and also the errors sort of make sense so for example the largest number of misclassifications for rock music happens with country music and country music in some cases is similar to rock music or at least much more similar than something like hip-hop so as you can see there is no miss classification with hip hop in this case so the point is that the confusion matrix can give you a little bit more rich semantic information especially if you have some prior knowledge of what these classes represent and what these classes are so does the confusion matrix make sense any questions all right obviously this can be expressed as as a direct number of instances or you can convert it to percentages if you want so that it's sort of more easy to interpret so if you look at a single class there are four possible outcome combinations of ground truth and prediction so if you use terminology from binary testing we have true positives that means the ground truth and the prediction are both the class that you are examining so if I have then you have false positives where the ground truth is not the class but the prediction is the class false negatives and true negatives where both are not the case so if you think of them as ones and zeros you got 00 01 10 and 11 so if you look at even if you look at the multi-class problem you can still treat each class separately as a binary problem to compute these metrics and based on this terminology we can define these metrics of precision and recall this come from the information retrieval community search engines that kind of stuff and there's a trade-off between precision and recall in it's easy to come up with a scheme that maximizes one or the other so for example if you have a classifier that only predicts two instances correctly so out of a thousand instances just predicts two instances correctly it has a precision of one because it only makes two predictions in both of those predictions are correct so it's like someone who if if they make a prediction it is correct but the problem is that they miss a lot of other things that could be predicted in the refere to be precise recall can go the other way so if a classifier predicts everything as heavy metal it will obviously predict all the heavy metal pieces as heavy metal but also it will predict everything else as heavy metal so it will have perfect recall but it would have terrible precision so these two measures kind of fight each other and you can combine them in something called the F measure which is defined like that and it's basically a combination of precision and recall so higher f-measure means that more instances of that particular class are classified correctly and low f-measure means that they are not so well and accuracy can also be defined in terms of the true positive through negative and so forth okay so I'm not going to talk about it today but notice that there are scenarios where errors matter and they can be given different weights so an example is medical diagnosis so if or when you have when you're someone is pregnant with a child they have a test that tests for certain genetic abnormalities now that test is designed in such a way that if there are genetic abnormalities it will come out positive so it might be the case that you are told that Oh something is dangerous and your baby might still be okay but then you do further testing and and both would be what they don't want but so sometimes it will have false positives but that's okay what you don't want to have as false negatives you don't want to have a case where the baby would be born with genetic abnormalities and the test would miss it so tests like for HIV or four pregnancies all that stuff they try errors in one direction are much worse than errors in another direction so what you want to avoid is someone who has cancer going miss detected which means that occasionally people will be told that they have cancer but they don't but that's a price you have to pay with me so classification errors for certain problems can be more nuanced so I think that's it pretty much in the next data mining lecture which we will do tomorrow we will start looking at some specific classifiers so that concludes the lecture and I will stop the broadcast so bye everyone remotely okay bye say bye to everyone excellent all right 
RXKQYo09j6c,27,"Please join as a member in my channel to get additional benefits like materials in Data Science, live streaming for Members and many more 
https://www.youtube.com/channel/UCNU_lfiiWBdtULKOw6X0Dig/join

Please do subscribe my other channel too
https://www.youtube.com/channel/UCjWY5hREA6FFYrthD0rZNIw

Connect with me here:
Twitter: https://twitter.com/Krishnaik06
Facebook: https://www.facebook.com/krishnaik06
instagram: https://www.instagram.com/krishnaik06",2020-05-28T10:05:57Z,Data Science Vs Data Mining,https://i.ytimg.com/vi/RXKQYo09j6c/hqdefault.jpg,Krish Naik,PT11M,false,7935,320,9,0,33,[Music] hello all my name is Krishna and welcome to my youtube channel so guys today in this particular video we will be discussing the basic difference between data mining and data sides now understand one thing guys most of the concepts that are used in data mining is already overlap with data science so people usually find it much more confusing in order to understand the basic difference between data mining and data science so we will try to clarify that particular doubt over here and trust me this was one of the most requested video by many of our subscribers so let's go ahead and try to understand what is the basic difference between data mining versus redesigns so to begin with guys will try to understand what is data mining so data mining is the process of discovering patterns in large structured data sets involving methods at the intersection of machine learning statistics and database system the data that we usually consider are structured data and are usually used in size so if we if if in short if I try to explain what exactly is data mining first of all we need to begin with the data warehouse in data warehouse we have huge amount of data collected from multiple sources and then if you want to find out patterns in that pool of data we basically use data mining and what are kinds of patterns we can use and this pattern you know you will also be seeing in machine learning algorithms because all the algorithms that we use like supervised or unsupervised machine learning algorithm they try to find out patterns in the data itself so we as we go ahead will try to understand more things with respect to this so what is data science I hope everybody's pretty much familiar with data science so let me just describe with a few liner over here so data science is a field of study which includes big data and I takes data mining predictive modeling data visualization mathematics and statistics it uses techniques and theories drawn from many fields within the context of maths statistics could be the science in function science nowadays if you see the diagram on the left hand side you can see that there is also something called a statistics and data mining over here okay and then we also have research we have domain expertise we have analytics systems we have programming skills and algorithms and these all are converging to words and the main converging point is towards data science so data science in short uses all the techniques that are involved along with big data domain expertise statistics and data mining are also pretty much important so most of the machine learning algorithms that we basically apply like classification and regression algorithms in all statistics and data mining itself as you can also see that statistics data mining and record installs are converging at some specific point so let's go ahead with the next slide we will try to understand the basic difference between data mining and data science based on these all points that I have noted on the right hand side one is definition why is it used what is the final goal what kind of data is used process in data mining and data science and conclusion in when we'll go to the process in data mining and data science you'll try to understand that how similar the process is with respect to the kind of data science projects that we do so let's go ahead and try to understand the definition so data mining is the process of discovering patterns in large data sets involved methods involving methods at the intersection of machine learning statistics and the data system so here you can see that based on this particular diagram in in this specific statistics and data mining it is all about discovering patterns you know in the larger data set data mining is used in data science it's clearly mentioned in the second point that yes data mining is a part of data science let us see some of the example I didn't find trends and patterns from past data okay you have a historical data in your data warehouse and you're trying to find out patterns and that pattern may also be helpful for you to predicting the future patterns and that is what your machine learning algorithm does not saying that data mining and machine learning algorithm are one in the same some of the functionalities do converge so let's go ahead why is it used okay so the main goal of a data mining is to meet the efficient use of historical structured data to identify bad trends and patterns so that the future trend or pattern can be identified it is an important step in knowledge discovery process you should understand this terminology guys knowledge discovery process the end goal of a data mining is basically to understand this knowledge discovery process you know let's let's see an example suppose an example is that I need to identify the pattern of the sales of a sweet shop right in festivals so usually if you know about the sweet shops that are open market during the festival season they sell a lot and suppose we want to predict you want to identify patterns suppose in this Diwali from the past eight years whenever the festivals are there what kind of patterns were the sales basically with respect to the sweets that was sold in those shops so we can identify those pattern whereas data science derives insight from both the structure and unstructured data it is very much common it is very much important to see this definition guys okay from both structured and unstructured data over here I told you only structured data right the main goal of the data science is to come up with insights that will help an organization to take proper decision so the final goal okay when when I am considering data science is actually to make a proper decision it is it is a decision making process altogether the best example of a use case what we can see is that what is the procurement of raw materials required by logistic company in the third quarter so suppose you are working in a company where Lipton tea is actually sold and suppose you want a raw material of tea crops and that time your logistic team should be able to procure enough amount of tea crops so that you know the production process continues on right if you run out of the raw materials and it is very difficult for you right so here you are able to make a decision right based on the insights that I derive from both the structure and structured data definitely data mining will be used within this okay so in short I can also consider that data mining is a subset of data science itself now let us go ahead and try to see the next slide the next point that about data mining is that data mining only deals with modeling finding patterns or predicting outcomes while data cleanup is also part of the process and data mining does not deal with the processing of massive unstructured data sets it does not deal with right we already have a pre-processing steps in data science right so data science is a roughly combination of math statistics computer science that deals with ETL of structure and structural data modelling and resonating it to get insights so this is pretty much it important to understand the basic difference over here it does not deal with the processing of massive unstructured data sets here we are since we are using ETL you know so because of that we are actually using structure and structure datasets modeling and presenting it to get insights so definitely this modeling part probably it may be because of data mining it may be because of machine learning algorithms that were using right so let's go ahead and try to see what are the process involved in data mining and data science so in data mining we start with data integration so we have a data warehouse where multiple data sources are combined then we do data cleaning data selection data transformation we know this right data cleaning handling missing values handling bad data right data selection is that how much data we require what are the kind of data will be requiring for doing the data mining data transformation yeah then we have data mining extracting useful data from the existing data from the pool of data we're trying to find out the useful data and then we are trying to find out the patterns okay finally you can see that there is a knowledge discovery right knowledge discovery and representation is given to the user in the form of graphs and tables and some visualization diagram this is pretty much important now if I go to data science you have data extraction and data pre-processing creating models optimizing model deployment of model and creating visualize reports one thing you can note guys this data cleaning data selection data transformation is a part of data pre-processing and you can consider that when we are creating models right we are basically using machine learning algorithms and machine learning algorithms you have both supervised unsupervised machine learning algorithms right you you try to solve classification regression problem statements by using different different algorithms like logistic regression and decision tree regression or logistic this is me tree classifier random forest regression random forest classifier and many more right so here in this particular step you know and you can see that this data cleaning data selection data transformation is getting captured in data pre-processing so this is overlapping right so these are all techniques along with data mining and pattern evaluation so if I consider about data mining and pattern evaluation it may be getting used in the machine learning algorithms right so machine learning algorithms can you can see that data mining is a part of that you know so all these trays two three four five six steps is basically combining two and three in data science so internally this is getting reused over here so if I specifically want to say but in data science you understand that we start with a data extraction process right and over here we will be having big data we'll be having different types of data stays good so over here we'll be having different types of databases both sequel and all sequel databases you know it may be having big data and many more things so after you do this you then optimize your model then you deploy a model and finally create a visualization reports so definitely we can see that most of the steps that are involved in data mining is already present in data science so nowadays people are just focusing more on data science because data mining previously when there was a large amount of data warehouse where organization were collecting a huge amount of data at that time data mining was pretty much famous but now with the help of data science since most of the concepts that I involved in data mining is already integrated in most of the steps in data science so data science is becoming much more popular right so let's go to the next slide the final conclusion we can see is that data mining is an activity which is a part of broader knowledge discovery in databases process while data science is a field of study just like mathematics or applied mathematics or computer science most of the process such as statistical analysis pattern recognition intersect with data science techniques you can see this diagram on the right hand side so finally we can consider that data mining is a subset of data science data mining is definitely getting used in machine learning algorithms you know where we're actually trying to find out the patterns but yes there is a minor difference so I hope you like this particular video guys please do subscribe the channel share with all your friends whoever require this kind of help and yes we are pretty much here to 200k subscribers please do subscribe this channel share with all your friends I'll see y'all in the next video have a great day thank you won it well bye 
ow7cvZOzp6w,28,"Download high quality version: http://bit.ly/vDnQu4
Description: http://events.ccc.de/congress/2011/Fahrplan/events/4652.en.html

Yuval Adam: Data Mining the Israeli Census
Insights into a publicly available registry

The entire Israeli civil registry database has been leaked to the internet several times over the past decade. In this talk, we examine interesting data that can be mined and extracted from such database. Additionally, we will review the implications of such data being publicly available in light of the upcoming biometric database.

The Israeli census database has been freely available on the Internet since 2001. The database has been illegally leaked due to incompetent data security policies in the Ministry of Interior of Israel, which is responsible for the management of the Israeli census.

The data available includes all personal data of every Israeli citizen: name, ID number, date and location of birth, address, phone number and marital status, as well as linkage to parents and spouses.

In this talk we discuss various statistics, trends and anomalies that such data provides us with insight to. Personal details will obviously be left out of the talk, though it is important to note that any person who wishes to retrieve such details can easily do so.

We will end the talk with a discussion about upcoming and relevant privacy issues in light of Israel's soon-to-be biometric database.",2011-12-28T23:48:49Z,28c3: Data Mining the Israeli Census,https://i.ytimg.com/vi/ow7cvZOzp6w/hqdefault.jpg,28c3,PT26M3S,false,3052,17,2,0,0,"please welcome very warmly mr. Adam he'll tell you about the data mining in the last election in Israel Thanks thank you okay good afternoon everyone my name is valera and as the title suggests I came here from to this I came here from from Israel to talk about the Israeli population census about what it is about what we can do with it and eventually why it matters if you want to follow the slides online there are online right now on this URL it's a bitly 28 c 3 d mi c so that's for the introduction so back in 2001 we started surfacing on various file-sharing networks on the internet what looks like standard Microsoft Access program even though it's right-to-left visits in Hebrew actually has beneath it very interesting database and it's one that has actually never been seen before and the database is actually the official Israeli government database that holds the entire all the personal details of every single Israeli citizens either alive or that has the season has within Israel in the past so all the details of every as well as citizen are in this database and can be looked up through that horrible UI this data is the data that has been collected in the Israeli census ever since the Declaration of Independence of Israel back in 1948 so dad has been collected ever since and the leak like I mentioned started approximately around 2001 it has actually leaked several times and the last leak happened in 2006 so so what does this data have so the data schema once we take a look at the database underneath we see basically most of the most of the data that you would expect the government to hold other citizens so we have a unique identification number which is given to any person the day he is born or if he immigrated Israel if he's an adult so unique identification number and then we have obviously the name of the person the date that he was born again with with the country that he was born in the gender status that can be single married divorced or deceased the current address and phone number and then we have interesting foreign keys actually point to this person's parents his father okay go Thanks so foreign keys that point to the person's parents to his father in to his mother and if that person is married also foreign key to his spouse there are also some other fields and metadata which I will talk about not too interesting the entire database holds approximately 9.2 million records so as of 2006 that that's equivalent to roughly 7 million citizens in Israel and then 2 and a half or two point two which are deceased or whatever so so that's data schema so when again when this thing came out in 2001 I was 17 years old and I didn't really know what to do with this other than look up famous people that I wanted to find their phone number and that's all I knew what to do it so that was back to then and and fast forward to today and kind of interested in what we can learn from this data looking at the big picture mining this data is not only a technical challenge but to me I think it's also important to understand where this leaves us as a society now that all this data is out there in the open so so the first thing here we go first thing that I that I asked myself was was how easy is it to find someone in this database obviously people identify each other by by their names and out by unique IDs we're not just numbers were people with names so the first thing I wanted to find out is given a name how easy is it to find a specific person and it turns out that it's pretty easy this is the uniqueness distribution function basically telling us that given a single a pair of a name and the surname there is a 50% chance of that name being unique so 50% of the names in Israel are unique to that specific person and obviously that function goes up so we have a 60% chance of a name being shared by two people at most then we have a 70% chance for finding at most four people with that name and then it goes up to 100% on the graph 100% we reach at the most common name in Israel which is shared by I think more than 2,000 people so so that's what the the uniqueness distribution looks like now obviously like I said people share names and we can always look up people by other innate by other fields so if we for example take a person's name and a surname and then looking up against his city we have 87 percent chance of finding one single record that goes up to almost 100% one to look up a person by his name and his date of birth and then you can always you know look up people by their name and filter by various criteria you know against whatever you know that the person that you're looking for matches so finding someone in this database is not a tough task at all the second thing that I noticed is that if you remember from the data schema we actually have the ID the unique identification number for every person and then we have foreign keys like I said to to the person's father to his mother and to his spouse if he is he or she is married so in this example we have the person 1 2 3 4 with his ID we know these parents are 1 2 & 3 4 and you know very easily we can see that the father 1 2 has again parents 1 2 & 3 4 is another 34 they're made to each other pretty easy stuff foreign keys so when I saw this I was thinking the logical thing to do in this case was take this thing and throw it into a graph and see what happens and see if I can try to match and not all the population but most of the population throw it into a graph and see if I can find connections between each other and the easiest way to see how this works is to follow an example so let's take for example a subject that was born in 1985 and let's assume that for simplicity that generations are roughly 25 years apart so for this person we we have as data and you know who his father and his mother are and they were born in say 1960 and then for those people again we have just following you the the route for the father or the mother is the same thing so for the father we know that we know who his parents are and they were born roughly around 1935 so this is what we have so far the interesting thing is that once we get to the people that were born roughly around these years there is no more data as to who their parents are and that is from two reasons one is that Israel exists as a country only from 1948 and back then the population of Israel was no more than 300 thousand people so either their parents never were Israeli citizens they still lived abroad so that is one option and the second option is that those the the generation above them might actually have have lived in Israel the problem is that the data isn't consistent and unfortunately for for people roughly around those years say up to you about 1950 55 60 we don't always have parental records and the data isn't always consistent so going above that generation is a little bit difficult but what we can't say is that for this person if if we know who is grandfather than his grandmother is that stands easily to his uncle and to his cousin so so given this person for most people we are able to go up all the way to the father and the grandfather and from that and from that generation to span out to uncles and cousins so essentially giving us the opportunity to map out families spending all the way to uncles and cousins okay so what does this graph look like so like I said we have approximately nine million nodes and using this data only only the the strict foreign keys that we use we get approximately forty four hundred and twenty thousand connected components which if you divided by the nodes it's an average of families of 20 people so essentially using this data only we can we can build a graph of families of up to 20 people and then span relationships from there it's interesting to note that the graph connectivity can be much stronger if you use other metadata and other heuristics which I won't go into now because you don't have time but but this connectivity can definitely be approved can be improved and then you can all even spent relationships further apart so that's what building graph now I mentioned it very shortly at the beginning the data has been leaked over over a period of almost 10 years so the the first leak that we know of dates back to 1998 the diversion of the data dates back to 1998 and then the data has actually least several times up until 2006 so usually when weeks happen they are recognized and plugged immediately not in Israel in Israel the leaks happen again and again and again and again and again over 10 years which is sad so this actually puts us in a unique situation because we actually have the opportunity to analyze the data as it changes over time over a period of 10 years which is a lot of time and a lot of data and so the question is what can we learn if we take two versions of this data and diff them again one against another an old version against the new version and what we would find out so differing basically gives us three types of results the first result would be new records now these are kind of trivial cases new records can be one of two things either children that were born sometime between the old version of the data and the new version so say a child that was born in 2005 would not exist in 2001 obviously we would exist in 2006 we have another case which is people that have immigrated to Israel and we can know this from the date of the birth so say someone that was born in 1966 you would expect him to be in the old version but he's not and he is in the new one so we can verify that by looking at the country the country of birth of this person and some other metadata and we can conclude that this person has in fact immigrated to Israel at a certain period between these two years so new records are pretty easy then we have updates now updates again fall into two categories one is as standard updates on the data of a person so people can change their name people can change their their addresses their phone numbers things like that so data that can change we would expect it to be different between the old version and the new version and then we have people that have passed away sometime between these two years so a person that was existed in 2001 sometime in that between those years passed away is now marked as deceased he is not deleted from the database so people if urine database are in there for life or for death or for whatever so so again those are pretty easy which brings us to the last and most interesting case reductions people that exist in the older versions say in 2001 and do not exist in 2006 if you're like me you're going what the yeah sorry yeah that they found they found the category of the status change yeah and questions for the save the questions for later I'll save some time for that so so again redactions we have no idea now honestly your guess is as good as mine I have no idea what these things are okay I really can't say anything more than that because I honestly don't know now this is under the assumption that the data has leaked from the same source across the entire period so obviously there can be problems if if you take data that has not there's not consistent with the older versions for various reasons so assuming that the data has actually leaked from the same source and is consistent these are some sort of reductions that means someone sometime decided that this person existed back then and does not exist anymore again your guess is as good as mine and data reductions are interesting in another context also really into Israel and that is that Israel has a law that requires every map vendor to pass satellite imagery that it wants to publish through the government essentially giving the government a chance to censor whatever the government thinks is should should be censored giving us an interesting interesting case that Google for example is not required by Israeli law to to do whatever while Israeli map site is so basically giving us a map of the same area with one one place that isn't censor than the other one very neatly photoshopped so so this is interesting because if someone wants to redact a piece of information if he can count on that data being the single source of data then you're fine as long as the Photoshop guy did a good jobs and you then you managed to hide whatever it is that you wanted to hide but the moment that you have another version of the data 2 2 2 diff against that's where the problems start essentially making the redaction not only useless but but even harming the efforts of hiding whatever is that you want to hide because now you can say hey wait there's something here that I don't know what it is from from the from this zoom but if i zoom in I might be able to and you know and they're gonna stay away so I want to zoom in and see what someone's trying to hide for me so so this is a very interesting dilemma so what is the problem with all this so sensitive and private data has been leaked and social engineering has obviously become much more easier and we know for a fact that in the past several years this data has been used for various identity theft scams and other scams mostly related to money but this data actually has been out for 10 years and how do we adapt to to the situation because what's done is done we can't take this data back and it's not going to change and the problem is the problem is that is the future is that we haven't actually learned anything or learned much from from this case and how we adapt it to to new laws earlier this year the Israeli Parliament passed the biometric data law essentially it's a law that allows the Israeli government to to regulate the creation of smart ID cards so essentially ID cards that enable biometric data collection for the for the purpose of making authentication much stronger than what is today with with the existing ID cards and mitigating the problem with fake double ideas Israel has a problem that a lot of its IDs are actually fake and people have double identities for for now the reason of you would think but but for your reasons of usually for criminal activity so that's a problem that the Israeli government would want to would want to take away there for the past is flawed the problem with this law which is generally a good idea seeing that the world is going to use much stronger identification means biometric passports etc so so the the direction is definitely good the problems in the details as always the system should work something like this the government issues new smart identification cards that have the ability to save data on them and the moment that a new citizen receives identification card the government takes two of his fingerprints hashed them and throw them on the card so essentially the card now has a hashed romantic data of this person which is fine and the next time the person wants to go to the bank for example he would show them a car they say okay this is you that's your fingerprints fingerprints hashed them match it against the card the person is who he says he is and this is fine again this is going with most of the standards of how the world is going with biometric passports we also know that all the as we've seen all the identities are saved in a central database which again is something that is pretty normal you would expect the government a modern Western government to to have some sort of of records of who the people are that live in the country so again this is fine as long as this thing doesn't leak as it has now the problem starts with what this law also lets the government do and this law essentially gives the government the opportunity to put all the biometric data unhatched and unfiltered in this database and they're saying that loud and clear so the government essentially wants to start building a database that now has not only our personal records but also our biometric data and saved in a very non secure way and this is a problem now we all know that once the database database like this exists everyone wants their hands on them so so from from starting off from a project of a single government office now all of a sudden lots of people want their their their hands on this database and everyone everyone need needs access to this database from the police to whatever and again this is a problem the Israeli public has expressed a lot of concern over this law unfortunately parliament has not actually addressed any of the of the concern raised from the public so much that in fact professor Adi Shamir which if anyone knows is the s in RSA clearly a guy we should not listen to also the recipient of the 2002 cheering award and he actually reviewed the law and all of its technical specifications and actually gave some feedback as to what can be improved so that the law still maintains the purpose required by the government also maintains the privacy of the citizen is much more strongly and even that feedback has actually been been denied so that says something about how much do you process went into this law I'm going to cut it a little bit short so I can leave some time for questions so I'm going to end on this note we've seen that all of the data of a zero citizens is out in the public and we've seen that for that for a period of ten years nothing has been done to to stop that leak so in that situation I would be asking would it be wise of us to start collecting more data by your metric data in the database that we know is not sufficient enough to hold this and I think society should much more closely monitor the data collection policies that the government maintains on them thank you very much we do have an additional five minutes for questions so please put your hand up if you have any before so then yeah or just a quick question how many deletions did you have in the database it's hard for me to say I can't really say because I honestly I don't really know it's it's a little bit of a problem I have no debt more and more debt on that so sorry okay any more questions right there in the back I can t see things there's a microphone and you counted settles and so forth of the names and what is the most common name in Israel the most common name Wow I think David Cohen or something or Moshe oh Anderson any Jewish name you can think of it's probably the most anything okay do we have any question from the IRC wait I'll get you a microphone there's one question from krokodil Ariane it's how can we get a copy of this database Wow okay I'll say that it's not it's not that easy but it's definitely not impossible like I said this data is out in the open it's harder to get it today than what it used to be several years ago but if someone is inclined to he can definitely find this data so yes it is possible okay we've got another question from the audience yeah the record deletion one guess would be that it might be people who just went out from Israel and gave up the citizenship do you know about the process what they do with data that's an interesting angle and it is possible I it's kind of again it's kind of hard to say because the the data is not always consistent I'll just say that I know of cases of again giving up your citizenship is not something that you would usually do but there are people that do not live in Israel anymore that still exist in the database I can tell you that I don't know if what the exact citizenship status but that is definitely an option yes how do you know or ensure their on authenticity of the data and is there any possibility or means to find whether there were intentional poisoning of the data for various purposes hiding or I know okay so we do know first of all we know that the government acknowledged this week and it has acknowledged it for the fur all the period that this data has been leaking so that the data definitely came from a government source and we also know that from the various metadata that we see on the database in regards to to poisoning the data that is definitely possible again this data is leaking all around the internet so it's definitely possible for someone to find it corrupt it in some way and and and throw it away I'd say that it's not entirely that's not something that you would commonly see just for example all the ID numbers have check digit at the end of them and from a simple check you can see that all of them are indeed consistent so that's just one field and you can always verify then against other fields but I again it is definitely possible I have no data confirming or okay we have two more questions from the audience hi the Israeli government has been relatively reluctant to publish numbers regarding the number of Israelis who are actually leaving the country so moving away does the database actually have any information about this how are you know how are people actually moving away and left the country marked in the database yeah well again people leaving the country aren't really recorded I mean that's not a field that the government wouldn't would hold as to where this person lives right now so that's not something that is recorded in this database there is a various metadata you can try to guess from as to is this person let's say active as he somewhere around that's not something that I researched entirely but it does exist so and again that's as long as people haven't given up their citizenship which is like I said I'm not sure how it is exactly processing the database so there's no real data on that okay one last question did you always simplify database scheme or is it really not possible to track data like devotion or adoption or something like this okay so it's it's definitely possible not directly from from the fields that we've seen but using various heuristics such as looking at several children of a person and seeing that some of them have different mothers than you can assume you can conclude that this person has any divorced and has children from from other from other parents so yeah pretty much a so you can conclude it by looking at the data from a higher level but not from the specific fields there's a lot they can implement you know from looking at various fields and making different conclusions that fir from the data okay thanks everyone thank you very much "
KvJQWyh1BEw,22,This video is recorded to meet the requirements of the Data Mining course (WQD7005),2019-09-28T07:36:40Z,Data mining assignment part 1,https://i.ytimg.com/vi/KvJQWyh1BEw/hqdefault.jpg,Alireza Ghanbarzadeh,PT6M44S,false,77,2,0,0,0,hello professor my name is ali riza ambassador on the students at the master of data science at the university of malaya I present this assignment for the course wqd zero zero five data mining session 2019 and six two thousand twelve two thousand twenty in sinister wife so what is kdd process here I will give an overview of kdd or knowledge discovery and database knowledge discovery in database refers to the overall process of discovering useful knowledge from data so we have some big database for example with data Mart for a big retailer store we want to discover knowledge from this data our goal is not just to use hill the state and that select some rows of the data to see what they already have we want to get actionable insight we want to predict we want to prescribe we want to get knowledge from the data so YK t values kt k today enables us to extract knowledge from data in the context of large databases as you can see there are six steps of selection processing transformation data mining and interpretation and then the end knowledge not for pharma step and the one result so the difference with the difference with kdd and normally you'll discover is that you use data mining techniques like classification clustering and stuff to get knowledge to interpret data evaluate data i get knowledge about data the final user i'm in the end user of kdd our CEO and decision makers and company while the users of the normal square query SQL query are operational staff this assignment consists of six miles stone age' steps follows the gating process systems so this is slide that I present today covers the first step the selections that Collin slide percent of data selection a stand or process or domain is financial domain data would be gathered from credible sources using the proper techniques so the data acquisition or data selection state is the process where the data that is relevant to the analysis is decided and reaching from the data source for example the domain that you are going on that we're going to analyze you is financial data I want to go on I'm going to retrieve historical stock prices from sources like investing calm Yahoo Finance or other credible financial data providers to analyze the data mining them and to predict future prices so how we actually gather data funky stick with these websites we gather data from this website using web scraping or web crawling techniques the descripton or web crawler against the automatic gathering of data from the Internet's I use it I do it using program program language part and treatment and libraries that I'm going to use a URL leap recreate a JSON I mean request is a subset of your living that method in the URL object in JSON you're early your honor it's a package embedded in Python I mean if you want to use URL leap you don't need to install it it's already embedded in Python and you can import it and use it it's a package that collects several modules for working with URLs we are going to work with your we've got requests that open and read the URL the JSON library is used to collect data in the format of the JSON which is a lightweight eight integer and format inspired by JavaScript let's go to the courts okay as you can see you are a lip is imported here and from your elector request I'm poor Rico's and you are open in order to open the URL and read it in JSON so this is the URL that the data inside it and you can see it's a long URL you are at the trick with that URL open we call the URL open of the request and the URL open and the input the URL object a fit design we define here as the input to this function and assign it to the results variable because the data are needed to be decoded to the utf-8 it is brilliant encoded in utf-8 want to decode it for further use I define a variable ism if you're like decoded data and really using a REIT method and decoded using the decode matter and assigned to decoded data at a later step using the JSON that notes function to really code at that and assigned into JSON on the line data so when your weed's data from such a structure such a data structure that is an ID and a key error might happen the key might be invalid so you need to use error handling mechanism here we wrote a try then this data which is a variable data later I'm going to restore it in the list and although the data section of our JSON data because all the data that we get from here was a long list of data so just in the data section that it was open close high low and several other items so I assigned this data section of the JSON data which is a dictionary a list of dictionaries to do list it and call it here based on the wrong data and you can see the results here the date is a time series data so we have the date of timestamp here overpriced clothes price low higher the volume that they start traded and other related data okay that's it it's the final step of the selection assignment thank you very much 
X1AmZKVf23Y,27,This video gives an overview of several methods and models for performing data mining.,2020-03-18T16:35:19Z,Advanced Database - 027 - Data Mining - Methods and Models,https://i.ytimg.com/vi/X1AmZKVf23Y/hqdefault.jpg,Brian B,PT26M27S,false,95,N/A,N/A,0,N/A,in this video I'd like to talk about a little more specific topics regarding data mining and in the last video that I produced we talked about an overview of data mining and the purpose of data mining and I've talked about some of the applications in industrial settings or educational government setting so the purpose of data mining in this particular case is prediction classification identification and optimization so I'd like to go over several of the options available for producing some of these items the first is prediction discovering knowledge in data the the first way to do that is Association rules looking at data to determine Association rules between different items so X if you have X or you purchase X then it implies that Y will also exist with a certain level of confidence so X implies Y and X notice is an item but it's within a set so a set of items can also predict another set of items and the classic example of this is Market Basket data so bread may eggs for example imply that you purchase butter and also in some cases diapers have actually implied that you would purchase beer alongside diapers so again both sides of the rule that you come up with the Association rule may be a list of things more than simply one thing on each side so a couple of things to note here support and confidence support is the percentage of data in the sent that contains all the items included in both the left and the right sides of the Association and I'll show you an example of this in just a moment and the confidence is just the percent of the time then it proves to be true so let's look at a more concrete example we have five shopping carts here and notice that cart one has items a B and C cart two has a C and B cart 3 has BC in the car for has a D and D E and cart five has B C and D so if we start looking at our association rules notice that item a appears in three out of our five cards but our support is a and B appear in two of our five cards so let's let's notice a a implies D or associates with D that appears in cart two and it appears in cart for that's a support of two out of five so two carts out of five cards total have an A and D together and the confidence is that when a appears in a cart which a appears in a cart three times two of those times it appears alongside D so our confidence is our confidence is two out of three the second rule that we see is C implies a so notice that we have CN a and one two cards so two out of five is our support our confidence is two out of four it's two out of four because C appears in four cards but only two of those cards have a associated so confidence is how many times happens a implies C which is the opposite of C implies a we notice that the support is the same but the confidence is different and the reason for that is look at a a appears in three cards instead of four and then finally we we see a combination of two things on the left so B and C appear in a card that appears in cart 1 it appears in card 3 it appears in cart 5 but it only appears with the one of those times so the support is only one out of 5 the confidence is one out of three because we have a situation where one time out of three times we have a C and I'm sorry BC and D together so this is the one and then the three times that B and C appear are here here and here so that's one out of three as far as confidence so that's examples of Association rules and we can determine from this what items will likely be purchased at a given point in time this is called Market Basket analysis a second example is when we start to look at classification rules so classification is placing instances of data into the correct bucket or the correct category the category that they belong to so normally when we have classification rules we have to split up the data that we have into a training set and then a testing set the purpose of the training set is we have a set of known items that have known cat so whatever algorithm we use has to learn that information so that it knows in the future when things look similar to that training data it can place the new data that it's not see correct bucket now something that you should note this is not the same as categorization because categorization is when you do not know what the categories are the categories actually emerge from the data rather than classification which has a predefined set of categories and pre labeled data some other knowledge that we can discover during data mining is looking for sequential patterns for example predicting that when a customer buys something in one transaction they will follow up with the purchase of something else in another transaction market basket analysis can help with that but market basket analysis is normally only used for predicting what items appear together or appear to be associated together time series patterns is where you look for a sequence of events that are all of the same type so it's a sequence of events so this one leads to the next one which leads to the next one which leads to the next one time series can be used to discover sequences and patterns for example the longest period when sales figures increase each month you can see some of the trends that are associated with them part of the process of data mining is developing some understanding of the business itself as I mentioned in the previous video you must understand the environment that you're working in before you can even start doing data mining so you have to understand the business requirements and objectives that you're trying to meet then you have to understand what the data looks like what it means how you how you can identify different parts of the data and then what each of those parts means whether it's raw data or whether it's aggregated you need to understand all of this then you need to actually prepare it if there are outliers you may need to remove this because outliers can have a serious implication on the data itself the next item is modeling choosing how to perform data mining based on a particular model we just talked about two models broadly discovering Association rules and also discovering classification rules but the model is a particular algorithm or set of algorithms that you use to perform the data mining itself and it really depends on what outcome you desire for which item you decide to choose then you have to test that the model is actually a valid model if it predicts only two percent of the time then you do not want probably to use that you need to ensure that your data mining model is effective and valid after you do that then you want to put it into use and deploy it so next we'll go through some of the data mining models that exist and just to give you an idea of what is out there and what is possible for you to use the first of those is regression so regression is a statistical method where you predict the value of an attribute which is the dependent variable given a set of other attributes which are the independent variables and if yours Mystikal person that probably sounds familiar to you however if you're not in statistics then what this really means is that a set of variables are predicting the outcome so you can use items like linear regression where you have a an actual linear function and you can draw a straight line through the data to determine how far away things are between between different items you can also use nonlinear regression and draw things like a parabola or other curves that fit the observed values and it turns out that you can fit things pretty closely depending on the order of the function that you are using when you're doing regression so regression is one technique and statistical packages like R and Python and SPSS and SAS and MATLAB they all have the ability to do regression on datasets as long as you convert the data into a format that those applications can understand one of the next models that that we can look at is decision trees for classifying different items we know what buckets things need to be placed in but we need we need to train a particular algorithm algorithm to recognize that one of the algorithms that can be used for this is the decision tree learning algorithm and this develops an actual tree by examining data in the training set to figure out how the attributes and values are related to the outcomes so the nodes of the tree represent the partitioning attributes and each of those partitioning attributes allow the training instances to be separated into disjoint classes and I'll show you an example of a decision tree in just a moment partitioning conditions are usually shown on the branches so I'll show you that as well the tree can then be used after it's developed to classify new cases that you haven't seen previously so in this example we have a high school average so let's assume that you have an average score that's a percentage score now notice at the bottom you're trying to classify whether the student will do poorly there will be an average student there'll be a good student or an excellent student and if we follow the left hand side of the decision tree we noticed that if their high school average is less than 70 and they have less than 1000 on the ACP or SAT they will they will be a poor student likely if their SAT score is 1000 to 1400 they will be an average student and if it's greater than 1400 they will be a good student following along the middle path of the decision tree if their high school average is between 70 and 90 and their SAT score is less than 1000 they will be an average student at least from this prediction if their SAT score is between a thousand and 1400 they will be a good student and if it's greater than 14 under they will be an excellent student on the right side of the tree if their high school average is greater than 90 their SAT score really leads to them being a good student or an excellent student so if it's under 1400 they will be a good student but if they are above 1400 they will be an excellent student and in a future video I will show you how to develop a decision tree for a particular set of data another popular and very common data mining model is linear neural networks I'm sorry nonlinear neural networks so neural networks if you if you take for example linear regression that draws a straight line and I mentioned that you can have nonlinear regression as well where you're drawing multiple lines through the or multiple shapes of a line to try to fit the data what a neural network does is it attempts to draw a very very specific line through the data to classify each and every observation that it finds it turns out that these can be very accurate and that that can be both good and bad and I'll talk about that in just a moment but a neural network receives a training set that provides facts about the input value so raw facts for input into the neural network and then using that training sample it adapts each of the nodes of their neural network as it learns additional information in multi-layer neural networks you get a situation where you have hidden layers and those hidden layers are developed by the system as it continues to examine individual data items that are fed into it and it uses a generalized regression technique I'll show you kind of a the idea there in just a moment the system then refines the hidden layers until it has learned to predict correctly at a certain percentage of the time then you use your data set to provide test cases and then evaluating in a single layer perceptron network it's pretty simple on the left side you have a set of inputs there's always a weight zero and then in an input of one on weight zero but then the others have their own weights associated with them so these are these are input values to the neural network and then it takes the net input function and sums this times this Plus this times this Plus this times this Plus this times this so that's the net input function then using an activation function at a certain threshold it says okay this item is this type it if that is correct during training then it produces an output if it's incorrect however then you have to go back and update the weights to make it more and more correct in a multi-layer perceptron network or a feed-forward neural network you have each of these are inputs to an input layer and then you have hidden layers and then an output layer when it trains this it becomes kind of a nebulous situation you don't understand how the training is working but you should be confident in that it is modeling the data correctly and you are you become confident in that by using your testing set and if you can't draw a clear line between the data that means the data is not linearly separable and that indicates that you have to use a multi-layer neural network one of the problems with a neural network is that you can over fit it overfitting is the idea that the prediction function is so accurate that it also models incorrect points just noise in the data that's why I'm making sure that your data is clean when you're training is very very important and it also means that the prediction function will will perform poorly figuring out how this works and explaining how this works sometimes becomes difficult but as long as you understand what is happening then you should be okay so that's a basic overview of neural networks and what they're used for another thing that we can look at is clustering where we have classification models such as decision trees and neural networks clustering is a categorization model and remember the difference is that classification models allow you to put things into a bucket that it's known but a categorization model you do not know the label to put on a particular bucket you just know that things look alike so it's methods clustering is is a set of methods that are used to place clay cases into clusters or groups and then using a training set you identify a set of clusters that that the database rows can be grouped into usually what is used to determine similarity is a distance function where things are if things are actually close together in distance if you plot them on a plane then they are considered to be related so in the same cluster and here's an example just of some groupings of data you notice that cluster yellow cluster red and cluster blue are all related there so the yellow clusters are together the blue clusters are close together and the red clusters are close together so each of these boxes represents a data point that that matches other data points or is similar to other data points within that cluster if we're talking about optimization one of the ways of doing optimization is through genetic algorithms genetic algorithms are things that come from a biological perspective looking at actual genes and how those genes replicate so you start with randomly generated States called the population and each one of those items within that population is an individual that usually is represented by a string and it has a finite alphabet so you know exactly what each of the items means sometimes it's just a simple binary alphabet a 1 or a 0 in the net 1 or 0 represents whether a condition is turned on or turned off but you might have to have more complex individuals and again you start with random this this part of genetic algorithms is really important figuring out what your data needs to look like in order to make this happen and that's not always easy so you rate each of the individuals after you create them based on their fitness so you take that representation of each individual and then you use that representation to calculate a value for fitness it should return a higher value if it's better and a lower value if it's worse so that's what how you come up with the initial population in the initial rating the second step or the next step I guess is to generate successor states through a reproduction process you choose to parent states at random and then select normally a crossover point or crossover points and these are random positions within that string that determines what what material comes from each parent then you may you may subject that successor state to a random mutation with a small independent probability so that means that some successors that you create will mutate and some will not you add the new successor to a new population a new generation and you determine the fitness for each of those items and then you just continue repeating this process and you repeat and repeat and repeat until you find an individual that is fit enough and usually it's the algorithm has converged to a particular value without any variation over a number of generations so what this is doing is trying to find the best answer for a given problem it's the optimization algorithm that is often used in data mining so I know I know without a clear example of these items that it's a little more difficult to understand but I do intend to give you some examples in the next couple of videos to show you at least how decision trees and how market basket analysis works and I may give you also an example of a genetic algorithm to show you a little more clearly how these things work 
pewqekYaCgI,22,"What Motivated Data Mining?
What Is Data Mining?
TOPICS covered using Knowledge Discovery from Data
(KDD Process)",2020-05-22T08:43:55Z,Unit-1 Topic:What Motivated Data Mining?,https://i.ytimg.com/vi/pewqekYaCgI/hqdefault.jpg,ANTHARAJU'S CSE ONLINE TUTOR,PT17M7S,false,1064,19,0,0,1,[Music] hello Newman welcome to my Channel today I am going to give a lecture on the introduction portion of unit number one so these are the topics I am going to cover in this unit what motivated data mining what is data mining what kind of data can be mined what kind of pattern can be mined classification of data mining systems data mining task primitives integration of data mining system major issues in data mining so these are the topics I am going to cover in the unit number one now so in this lecture so I am going to focus on two topics starting topics what motivated data mining what is data money these are the two topics I am going to focus in this lecture so first of all so what motivated data mining we will see that that means so here we have to focus so what are the things which motivates to do the data mining so that that gives the origin point of view where we have to focus okay so first of all before going to start which things motivate the data mining we have to see this evaluation of database technology so here data warehousing it's it's not a one time or technology is suddenly developed or suddenly evolved not like that it's a evolution that means from the starting of the origin is data collection and data is creation this is the starting of the data warehousing era so whenever the data has started then that they that is the origin point of view of the data warehouse well that is a requirement for data warehousing that has started he in the 1960 so initially we who have started this data collection and the Euler database creation the 1960s select later so they used to use this DBMS database management system like a query languages we have invented so next the technology era started and advancement of DBMS okay so initially in the 1960 they used to store or store the data in the form of files ok later they came across the DBMS and next advanced DBMS so we can consider data warehouse and data mining is one of the advanced model of a DBMS from the 1980s to present we are talking about all this terminologies like advanced data models in the data warehouse and Ola people so we will talk about Ola P in the coming classes it's not issue ok this is evolution this diagram says so why the data warehouse what is the starting point of the data warehouse so whenever in the 1960 the attack election has started then that is that that will be the starting point of the era now why data mining so initially people used to store in the fire data in the files that's it they don't know what the use of that it's ok fine now I'm going to discuss why the data mining now see this you can find all these things you you people know very well about all this things www World Wide Web Amazon Flipkart sense source YouTube medical related data etcetera all these things are what all these are the resources so for data production so where the data is continuously producing by these things like internet online shoppings o as well as the sensors continuously predict the data and since the data and next thing is can like that all these things are data resources where the data is going to be generated okay now what are the use of the generated data they have to store that data okay of course that is the thing you have to do they have to store the data next what is the use of that data we are going to see in the next coming slide so these are the examples of data generated from the resources so first thing is sales transaction stock our trading records product description sales promotions company profiles customer feedback and so on every click what you are doing giving on the screen and as well as on the internet so that will be its generate data example see this a Walmart is one one of the example so in the wall mask Wal Mart it's a it's a big company so under this company the people will do the transaction like they will buy the product the what happens whenever they pride buy the product they e they make the transaction right so similarly so how the particular transaction is going to be useful to the Walmart so that gives a basic requirement for the data warehouse and data mine so here so these are the examples so remote sensing scientific experiments engineering observation all these are the examples where the data is generated yes so this is the worth of data mining whenever the huge data is generated from the last resources or last the last slide I told you that multiple resources are available these resources will generate tremendous so data continuously or discreetly okay so now what is the use of the data the data is used now he is there any use of the data yes obviously there that here the use of the data is to apply the knowledge knowledge that particular data is the main target in the data mining concept that this particular third gives the birth of the data mining yes the powerful and versatile tool are badly needed to automatically uncover the valuable information from the tremendous amount of data to transform such data into organized knowledge so this is this statement yes whenever the data is huge and asla people he people are very curious to know what the data is talking about so that that particular thought gives the data mining subject birth okay so that is the motivation for data mining as I told you that whenever the data is include equal continuously generated so that motivates the data mining so in this present trend 2020 you can see any everywhere the data is going to be generated okay in every field you can say the computer gives a very empowering all important aspects from every field there the one more thing you have to remember now second concept what is data mining now we will enter it to the actual thing related to the data mining what is data mining so this is the brief definitions or some keywords which is related to data mining which is a meaning of the data mining so extraction of interesting patterns or knowledge from the huge amount of data that is called data mining extraction of knowledge we have to obtain knowledge from the use amount of data so that this knowledge is very helpful to to take the business decisions so this is the basic line story line of the data mining concept and next alternative names of the data mining is not discovery in database kdd knowledge extraction pattern analysis data archeology data breathing information harvesting business intelligence support there are a lot of odd number of names are there but out of all these things the data mining is a buzzword it's a it's a familiar word which is well known by everyone so of course you can you can call this this data mining here with this new alternative names also now I'll give you a simple picture about mining so have you heard about the gold mining gold mining or the major objective in the gold mining is people has to dig the rock dig the rock or land what it may be then they how to find the gold then we will generally call that the gold mining but we won't call that as a rock mining okay so people think the rock as well as they will find the goal for for for for finding the go for ending the goal they will dig the rock so we are calling gold mining here but what about data mining so we are digging data to obtain knowledge see the similarity here gold mining in data mining gold mining is the outcome is gold here but in the data mining outcome is data no it's a knowledge okay that is the reason you have to remember or you have to understand this concept from the depth okay in a in a broad way here see this so here digging the data and find the knowledge but not knowledge mining it's not knowledge mining you cannot we are not calling knowledge mining of course it's did the same thing so we are going to bind the knowledge okay by digging the data so here this is the thing we are going to say but we are not calling that as a knowledge mining okay they subject the reason here is the knowledge mining is a lengthy and as well as people it's little bit difficult to remember knowledge mining of heard that knowledge may be a different varieties see that is the region so they are they ignore this knowledge mining and replace the town with data mining so what so here we are digging the data and finding the knowledge so if you compare or relate these things to the mining concept okay as the gold mind says so we are going to mine the rock and as well as obtain the gold so here the data mining says have mined the data and obtained the knowledge very simple so here this is the situation of the world so because the data is used so people are very busy busy with doing their transactions obviously the transactional resultant gives the data but the data is not in an organized way and without knowledge so this is the situation of the present world world is richer data but poor at knowledge now what we are helping with the world okay with respect this context yes we are going to organize or searching knowledge in the data and finally we are going to apply the knowledge from the data and the data is in the organized way whenever the data in organized way then only we can obtain the knowledge so so that is the thing we are going to do with the data mining with the data I am going to we are going to mind the day knowledge from the particular data now how that is possible so this is the first step in the data mining process so first thing is kdd process knowledge kayford knowledge D for discovery another D for data knowledge discovery from data this is a first step we have to remember in this particular course this particular diagram says it's a diagram from the famous textbook so this particular diagram says the evolution here so the sequence of steps so first of all our data the generated data is available in the databases so consider any Flipkart Amazon or YouTube or it may be there the company having their own databases now what what we are trying to do so we are going to mind that particular those particular databases so but can we directly mine on the database no that is not possible if you do that particular thing if you directly mine the data or the databases what had happened it will disturb the regular transactions that is a reason we are going to invent the data warehouse that means the third the the location the pole the particular repository which is out of the institution that is the thing that is the organization out of the organization so we have to create one a storage location okay now in the first step see this so databases are their flat files and databases so we are going to perform cleaning and integration steps so we'll see in the next coming thread in the next lectures what actually cleaning and integration remember that is the process so after this process so we have to bring those particular outcomes to the data warehouse from the data warehouse this is the valid data a data warehouse is the second level from the data warehouse we have to do the selection and transformation that means pick some piece of piece of data where we need to do the mining on the data later next do the mining here on the particular piece of data mined the data right and next update the pattern the pattern gives the knowledge so know finally the raw data is going to be there at the data base and the outcome is to the user here the user and this is the data analyst or the company guy who are to know about data so this is the thing this is the process of knowledge discovery to apply the knowledge from the raw data so these are the steps I summarized total seven self data cleaning to remove noise and inconsistent data next data integration where multiple data sources may be combined in one data selection where the relevant data to analysis tasks as they three from the database fourth one data transformation conversion from one to another next fifth to one data mining so this is the heart of this fifth step actually the data mining comes under to the part of the cell this particular process but not to the complete so here next pattern evolution and finally knowledge representation so here so these are the seven steps which comes under kdd process so incoming the lecture so we'll take one by one one by one these topics like to understand better this kdd process why because this is very important concept okay yes it's fine so we have completed two topics so one is a introduction to data warehouse and next thing is kdd process so we'll cover the remaining topics in the next lecture within a short period so meanwhile subscribe to like that video share it to your friends and so that this this particular tribe will be helpful to the required people who are to learn the data warehouse concept thank you everyone thank you for your patience in listening thank you so much 
gOtWrNzMVDY,28,"For more information visit: http://bit.ly/28C3_information
To download the video visit: http://bit.ly/28C3_videos
Playlist 28C3: http://bit.ly/28C3_playlist

Speaker: Yuval Adam

Insights into a publicly available registry

The entire Israeli civil registry database has been leaked to the internet several times over the past decade. In this talk, we examine interesting data that can be mined and extracted from such database. Additionally, we will review the implications of such data being publicly available in light of the upcoming biometric database.

The Israeli census database has been freely available on the Internet since 2001. The database has been illegally leaked due to incompetent data security policies in the Ministry of Interior of Israel, which is responsible for the management of the Israeli census.

The data available includes all personal data of every Israeli citizen: name, ID number, date and location of birth, address, phone number and marital status, as well as linkage to parents and spouses.

In this talk we discuss various statistics, trends and anomalies that such data provides us with insight to. Personal details will obviously be left out of the talk, though it is important to note that any person who wishes to retrieve such details can easily do so.

We will end the talk with a discussion about upcoming and relevant privacy issues in light of Israel's soon-to-be biometric database.",2012-01-18T22:08:45Z,28C3: Data Mining the Israeli Census (en),https://i.ytimg.com/vi/gOtWrNzMVDY/hqdefault.jpg,Christiaan008,PT26M3S,false,263,6,0,0,0,"please welcome very warmly he'll tell you about the data mining in the last election in Israel Thanks thank you okay good afternoon everyone my name is you've all Adam and as the title suggests i came here from to this i came here from from Israel to talk about the Israeli population census about what it is about what we can do with it and eventually why it matters if you want to follow the slides online they're online right now on this URL it's a bit ly 2083 dmic so that's for the introduction so back in 2001 started surfacing on various file sharing networks on the internet what looks like a standard Microsoft Access program even though it's right to left visits in Hebrew actually has benefit very interesting database and it's one that has actually never been seen before and the database is actually the official Israeli government database that holds the entire all personal details of every single Israeli citizens either alive or that has the season has lived in Israel in the past so all the details of every years well as citizen are in this database and can be looked up through that horrible you I this data is the data that has been collected in the Israeli census ever since the Declaration of Independence of Israel back in nineteen forty eight so dad's been collected ever since and the leak like I mentioned started approximately around 2001 it is actually leaked several times and the last week happened in 2006 so so what is this data have so the data schema once we take a look at the data base underneath we see basically most of the most of the data that you would expect the government to hold of its citizens so we have a unique identification number which is given to any person the day he is born or if you immigrates Israel if he's an adult so unique identification number and then we have obviously the name of the person the date that he was born again with a with the country that he was born in the gender status that can be single married divorced or deceased the current address and phone number and then we have interest in foreign keys actually point to this person's parents his father oh ok cool thanks so foreign keys that point to the person's parents to his father into his mother and if that person is married also foreign key to his house there are also some other fields and metadata which I will talk about not too interesting the entire database holds approximately nine point two million records so as of 2006 that that's equivalent to roughly seven million citizens in Israel and then two and a half or 2.2 which are deceased or whatever so so that's data schema so when again when this thing came out in 2001 I was 17 years old and I didn't really know what to do this utter then look up famous people that I wanted to find her phone number that's all I knew what to do it so that was back to then and and fast forward to today and kind of interested in what we can learn from this data looking at the big picture mining this data is not only a technical challenge but to me I think it's also important to understand where this leaves us as a society now that all this data is out there in the open so so the first thing that there you go the first thing that I that I asked myself was was how easy is it to find someone in this database obviously people identify each other by by their names and out by unique IDs we're not just numbers were people with names so the first thing I wanted to find out is given a name how easy is it to find a specific person and it turns out that it's pretty easy this is the the uniqueness distribution function basically telling us that given us a single a pair of a name in the surname there is a fifty percent chance of that name being unique so fifty percent of the names in Israel are unique to that specific person and obviously that function goes up so we have a sixty percent chance of an A being shared by two people at most then we have a seventy percent chance for finding at most four people with that name and then it goes up to one hundred percent on the graph one hundred percent we reach at the most common name in Israel which is shared by I think more than 2,000 people so so that's what the the UNIX its uniqueness distribution looks like now obviously like I said people share names and we can always look up people by other innate by other fields so if we for example take a person's name and a surname and then look him up against his City we have eighty-seven percent chance of finding one single record that goes up to almost a hundred percent when to look up a person by his name and his date of birth and then you can always you know look up people by their name and filter by various criteria you know against whatever you know that the person that you're looking for matches so finding someone in this database is not a tough task at all the second thing that I noticed is that if you remember from the data schema we actually have the idea the unique identification number four if I for every person and then we have foreign keys like I said to the person's father two his mother and to his spouse if he is he or she is married so in this example we have the person when two or three four with his ID you know these parents are one two and three four and then you know very easily we can see that the father 12 has again parents one two and three fours mode 34 they are married to each other pretty easy stuff foreign keys so when I saw this I was thinking the logical thing to do in this case was take this thing and throw it into a graph and see what happens and see if I can try to match not all the population but most of the population throw it into a graph and see if i can find connections between each other and the easiest way to see how this works is to follow an example so let's take for example a subject that was born in 1985 and let's assume that for simplicity that generations are roughly 25 years apart so for this person we we have as data and you know who his father and his mother are and they were born in say 1960 and then for those people again we have just following the route for the father or the mother is the same thing so for the father we know that we know who his parents are and they were born roughly around 1935 so this is what we have so far the interesting thing is that once we get to the people that were born roughly around these years there is no more data as to where their parents are and that is from two reasons one is that Israel exists as a country only from 1948 and back then the population of israel was no more than 300,000 people so either their parents never were Israeli citizens they still lived abroad so that is one option and the second option is that those the generation above them might actually have have lived in Israel the problem is that the data isn't consistent and unfortunately for four people roughly around those years say up to you about nineteen fifty fifty-five sixty we don't always have parental records and the data is always consistent so going above that generation is a little bit difficult but what we can say is that for this person if if we know who is grandfather than his grandmother is that spans easily to his uncle to his cousin so so given this person for most people we are able to go up all the way to the father and the grandfather and from that and from that generation to span out to uncles and cousins so essentially giving us the opportunity to map out families spending all the way to uncles and cousins okay so what does this graph look like so like I said we have approximately nine million nodes and using this data only only the strict foreign keys that we use we get approximately forty four hundred and twenty thousand connected components which if you divided by the nodes it's an average of families of 20 people so essentially using this data only we can we can build a graph of families of up to 20 people and then span relationships from there it's interesting to note that the graph connectivity can be much stronger if you use other metadata and other heuristics which I won't go into now because you don't have time but but this connectivity can definitely be a proof can be improved and then you can all you can spend for a relationship sfer the part so that's about building ref now I mentioned it very shortly at the beginning the data has been leaked over over a period of almost 10 years so the the first leak that we know of dates back to nineteen ninety-eight that the version of the data dates back to nineteen ninety eight and then the the data has actually leaked several times up until 2006 so usually when weeks happen they are recognized and plugged immediately not in Israel in Israel the leaks happen again and again and again and again and again over 10 years which is kind of sad so this actually puts us in a unique situation because we actually have the opportunity to analyze the data as it changes over time over a period of 10 years which is a lot of time and allows data and so the question is asked is is what can we learn if we take two versions of this data and dip them again one against another an old version against a new virgin and what we would find out so difficult us three types of results the first result would be new records now these are kind of trivial cases new records can be one of two things either children that were born sometime between the old version of the data and the new version so say a trout that was born in 2005 would not exist in 2001 obviously we would exist in 2006 we have another case which is people that have immigrated to Israel and we can know this from the date of the birth so say someone that was born in 1966 you would expect him to be in the old version but he's not and he is in the new one so we can verify that by looking at the country the country of birth of this person and some other metadata and we can conclude that this person has in fact immigrated to Israel at a certain period between these two years so new records are pretty easy then we have updates now updates again found to two categories one is is standard updates on the data of a person so people can change their name people can change their their addresses their phone numbers things like that so data that can change we would expect it to be different between the old version and the new version and then we have people that have passed away sometime between these two years so a person that was existed in 2001 sometime in that between those years passed away is now marked as deceased he is not deleted from the database so people if urine that I base around there for life or for death or for whatever so so again those are pretty easy which brings us to the last and most interesting case reductions people that exist in the older versions st. 2001 and do not exist in 2006 if you're like me you're going what the  yeah sorry yeah they fall they fall in the category of the status change yeah and questions for the save the questions for later I'll save some time for that so so again redactions we have no idea now honestly your guess is as good as mine I have no idea what these things are okay I I really can't say anything more than that because I honestly don't know now this is under the assumption that the data has leaked from the same source across the entire period so obviously there can be problems if if you take data that has not there's not consistent with the older versions for for various reasons so assuming that the data has actually leaked from the same source and is consistent these are some sort of redactions that means someone sometime decided that this person existed back then and does not exist anymore again your guess is as good as mine and data reductions are interesting in another context also relating to Israel and that is that Israel has a law that requires every map vendor to pass satellite imagery that it wants to publish through the government essentially giving the government a chance to censor whatever the government thinks is should should be censored giving us an interesting interesting case that Google for example is not required by Israeli law to to do whatever while Israeli map site is so basically giving us a map of the same area with one one place that isn't sensor than the other one very neatly photoshopped so so this is interesting because if someone wants to redact a piece of information if he if he can count on that data being the single source of data then you're fine as long as the photoshop guy did a good job they need then you manage to hide whatever it is that you wanted to hide but the moment that you have another version of the data two to two diff against that's where the problems start essentially making the redaction not only useless but but even harming the efforts of hiding whatever it is that you want to hide because now you can say hey wait there's something here that I don't know what it is from from this from this room but if i zoom in I might be able to and you know they're going to say way so I want to zoom in and see what someone's trying to hide for me so so this is very interesting dilemma so what is the problem with all this so sensitive and private data has been leaked and social engineering has obviously become much more easier and we know for a fact that in the past several years this data has been used for various identity theft scans and other scam was mostly related to money but this data actually has been out for ten years and how do we adapt to to this situation because what's done is done we can't take this data back and it's not going to change and the problem is the problem is that is the future is that we haven't actually learned anything or learned much from from this case and how we adapt it to two new laws earlier this year the israeli parliament passed the biometric data law essentially it's a law that allows the israeli government to to regulate the creation of smart ID cards so essentially ID cards that enable biometric data collection for the for the purpose of making authentication much stronger than what is today with with the existing ID cards and mitigating the problem with fake double ideas Israel has a problem that a lot of its IDs are actually fake and people have double identities for for other words of you would think but but for your reasons of usually for criminal activity so that's a problem that the Israeli government would want to would want to take away their for the past is flawed the problem with this law which is generally a good idea seeing that the world is going to use much stronger identification means biometric passports etc so so the the direction is definitely good the problems in the details as always the system should work something like this the government issues new smart identification cards that have the ability to save data on them and the moment that a new citizen receives identification card the government takes two of his fingerprints hash them and throw them on the card so essentially the car now has a hash dramatic data of this person which is fine and the next time the person wants to go to the bank for example he would show the car they say okay this is you let's your fingerprints fingerprints hash them match it against the card the person is who he says he is and this is fine again this is going with most of the standards of how the world is going with biometric passports we also know that all the has we've seen all the identities are saved in a central database which again is something that is pretty normal you would expect a government a modern Western Government to to have some sort of of records of who the people are that live in the country so again this is fine as long as this thing doesn't leak as it has now the problem starts with what this law also lets the government do and this law essentially gives the government the opportunity to put all the biometric data unhatched and unfiltered in this database and they're saying that loud and clear so the government essentially wants to start building a database that now has not only our personal records but also our biometric data and saved in a very non secure way and this is a problem now we all know that once the database database like this exists everyone wants their hands on them so so from from starting off from a project of a single government office now all of a sudden lots of people want their there hands on this database and everyone everyone needs access to this database from the police to whatever and again this is a problem the Israeli public has expressed a lot of concern over this law unfortunately parliament has not actually addressed any of the of the concern raised from the public so much that in fact professor edition mu which if anyone knows is the s in RSA clearly a guy we should not listen to also the recipient of the 2002 cheering award and he actually reviewed the law and all of its technical specifications and actually gave some feedback as to what can be improved so that the law still maintains the purpose required by the government will also maintains the privacy of the citizens much more strongly and even that feedback has actually been been denied so that says something about how much do process went into this law I'm going to cut it a little bit short so I can leave some time for questions so I'm going to end on this note we've seen that all of the data of Israeli citizens is out in the public and we've seen that further for a period of ten years nothing has been done to to stop that leak so in that situation I would be asking would it be wise of us to start collecting more data biometric data in the database that we know is not sufficient enough to hold this and I think society should much more closely monitor the data collection policies that the government maintains on them thank you very much Oh we do have an additional five minutes for questions please put your hand up if you have any we have options before over there yeah or just a quick question how many deletions did you have in the database ah it's hard for me to say I can't really say because I honestly I don't really know it's it's a little bit of a problem i have no doubt mom or dad on that so sorry okay any more questions right there in the back I can t see things use a microphone and you counted set apples and the softer names and what is the most common name in Israel the most common name Wow i think david cohen or something or moshiach Oh Anderson any like Jewish name you can think of it's probably the most okay do we have any question from the IRC wait I'll get you a microphone there's one question from krokodil aryan it's how can we get a copy of this database Wow okay I'll say that it's not it's not that easy but it's definitely not impossible like I said this data is out in the open it's harder to get it today than what I used to be several years ago but if someone is inclined to he can definitely find this data so yes it is possible okay we've got another question from the audience yeah the record deletion one guess would be that it might be people who just went out from Israel and gave up the citizenship do you know about the process what they do with their data that's an interesting angle and it is possible I it's kind of again it's kind of hard to say because the the data is not always consistent I'll just say that I know of cases of again giving up your citizenship is not something that you usually do but there are people that do not live in Israel anymore that still exist in the database I can tell you that I don't know if what the exact citizenship status but that is definitely an option yes how do you know or ensure there on authenticity of the data and is there any possibility or means to find whether they were intentional poisoning of a data for various purposes hiding or I know okay so we do know first of all we know that the government acknowledges this week and it has acknowledged it for the first at it has been leaking so that the data definitely came from a government source and we also know that from the very meta data that we see on the database in regards to to poisoning the data that is definitely possible again this data is leaking all around the internet so it's definitely possible for someone to find it corrupt it in some way and and and throw it away I'd say that it's not entirely that's not something that you would commonly see just for example all the ID numbers have check digit at the end of them and from a simple check you can see that all of them are indeed consistent so that's just one field and you can always verify then against other fields but I again it is definitely possible I have no data confirming or okay we have two more questions from the audience hi the Israeli government has been relatively reluctant to publish numbers regarding the number of Israelis who are actually leaving the country so moving away does the database actually have any information about this how are you know how are people actually moved away and left the country marked in the database yeah well again people leaving the country aren't really recorded I mean that's not a field that the governor what would hold as to where this person lives right now so that's not something that is recorded in this database there is various metadata you can try to guess from s2 is this person let's say active is he somewhere around that's not something that I researched entirely but it does exist so and again that's as long as people haven't given up their citizenship which like I said I'm not sure how it is exactly process in the database so there's no real data on that sum it up okay one last question is the rtt show with simplified database scheme or is it really not possible to activate a like devotion or adoption or something like this okay so it's it's definitely possible not directly from from the fields that we've seen by using various heuristics such as looking at several children of a person and seeing that some of them have different mothers then you can assume you can conclude that this person has any divorced and has children from from other from other parents so yeah pretty much it so you can conclude it by looking at the data from a higher level but not from the specific fields so yeah there's a lot they can you can learn just from you know from looking at various fields and making different conclusions that fur from the data so okay thanks everyone thank you very much enjoy "
vGyGnHT7NjU,22,,2019-12-07T22:08:25Z,Database development lecture 28 data mining,https://i.ytimg.com/vi/vGyGnHT7NjU/hqdefault.jpg,Teng Dequn,PT5M58S,false,1,0,0,0,0,welcome to database to a lecture and he did a warehousing or if he did Hermione matter from Ola P to the Hawaiian in first like we were analyzed the his oil issue which is an analogical photos it so what is the corresponding concept is OLTP which is online transaction processing potential are going to talk about the Himani which can be a sink as extended full of AP so final factors that have the most influence over sell the product rather than trying out a period like you know from cells from select models and price for natural products with obstacle stacks to buy models so the data binding refers to the discoverer of fun even own a patent and knowledge of the data and nearly fifty percent of people who try to apply the whole of the master by Joe must hard because with choosing re together and this individual patterns of the bank transaction indicates as they're running out for terrorist cell to come up many areas of major science and mathematics as well machine learning and the statistics and determined here we are going to eating system of basic methods so determining the real-life examples just like up for the supermarket's take a managed to Senator of motion sell out of food type a third before some of the families have announced that they are pregnant so and maybe the gmail can use stem detection method through to to get some features handy with the rich who know whether it is all stem oh no so there's an ethical concept that whether to cheap and used you know purchasing real close to see whether whether they are pregnant or knowledge as an ethical issue but we do not have about them here and the marijuana exact market check house so imagine the market and we are going to see the purchase ID item bought and she's elusive English paper so she is used for decision-making which are which item enough reasonable together can influence the item placement decision process and so first can we identified with some relationship just like pipetters and appears the dieters in Chinese called Tiago and so we also see like the streaming service like some here some films and here's all customers for them and has some possible function that which we were frequently like the singing film and we can be used the fourth government in the film and they can be described as a set of the item I'll and stratified from set of the basket be for each basket BJ's a suitable i i'm the for example in this case is a purchase idea judge and as important as this one two three four four parties under theses item and so in this case is love the bucket have our index of 1 to 4 and the item has all the things all other things are taking place here so it's a mathematical representation for this one is two eye problems and so another example is that this case is duties under our items and these are baskets and freakin item mining so we shall add math using a bad together so for today we are going to introduce a concept of self support which means that the support of sap Sanji of is equal to the frequency of which the items in G occurs together with the basket in B which is just equal to the number of pockets pockets in d containing all Adam energy divided by the number of buckets in V so for example what he belong to find the support of milk and the tools we just want find all the MU and to serve milk and juice one milk juice twice and there is milk juice in the following so the sport of milk and to say 2/4 is 0.5 and what you want to find the total brat and use we just found I was in grad so this is Brad and the statistics is 1 and no Brad pollution no truthful Brad and Brad but no Jews so this is 1/4 so this is the truth or something and the frequent item here that were given a set of admire a set of our can be so we are going to set a threshold for the support for example let's see this holidays open file and the body future support is with Hawaii hope to the open fires and African - and was a lot less times and he's not frequent so so let's say milk and cheese are frequent and the president was an all sequence and it acts here is let's see what if we have funds P I and B are so in this case is bi and bf is just like this one this one and this one this one this one this one this one this one and all others are not happy and be as occurring together so it's a 4 divided by 81 evaluate to so it is frequently since 4 divided by 8 is greater than 3 divided by T and we can also find other fifteen divided Adams actually I bought one two and so this is a taste for thank you for comment and subscription 
O9QnC5WJJ90,28,".
.
.
.",2016-04-13T05:06:58Z,Lecture 20 —  Frequent Itemsets | Mining of Massive Datasets | Stanford University,https://i.ytimg.com/vi/O9QnC5WJJ90/hqdefault.jpg,Artificial Intelligence - All in One,PT29M51S,false,23481,136,11,0,3,the study of how we find frequent itemsets was one of the earliest directions taken by data mining researchers and the our priori algorithm is probably the most cited work in the data mining field the motivation for this study was to determine unusual sets of items of people purchased together in supermarkets a no discussion of finding frequent itemsets would be complete without mentioning the possibly apocryphal story of beer and diapers and will tell that one soon enough but in a rough outline we're going to start with a model of data called the Market Basket model where data consists of sets of items and sets our code baskets for a reason we'll discuss and a set of items is called frequent if it appears in some large number of baskets we'll talk about Association rules which are statements that when a certain set of items are found in a basket than it is unusually likely that another item will also be found there and finally we'll give the operatory algorithm for finding frequent itemsets this algorithm while it has been improved upon over the years is the fundamental starting point for all of these improvements the market basket model is essentially a representation for many many relationship between two concepts which we call items and baskets on one hand there's a large set of items an example is all the different items Walmart cells of which there are hundreds of thousands this was the original purpose of the model analyzing the things people bought at a store however the model has many other applications if a few of which we'll talk about shortly and then on the other hand there's a large set of baskets each basket contains a small set of items the original picture was that a market basket was a shopping cart a physical that is not electronic and customers would wheel their market basket up to the checkout the cash register would group all the items in the basket together in one bill and that bill would be stored in the stores computer system thus by minding the contents of the various market baskets the store could learn what people bought together a hamburger and ketchup for example that in turn with the store figure out some ploys to increase sales think about what a store might do to exploit this information we're going to return to the subject shortly the most useful and also most basic question to ask about data in the form of market baskets is to find those sets of items that appear in some minimum number of baskets to find the support for an item set to be the number of baskets of which that item set is a subset we can give the support either as an absolute number or as a percentage of all the baskets data mining problem called frequent itemsets involves a number or percentage s called the support threshold any set of items with support at least as is called a frequent item set okay here's a very simple example of data in the market basket model okay there are five items milk Coke Pepsi beer and juice the support threshold will be three that's an absolute number not a percentage of the baskets here are the baskets we're using M for milk C for Coke and you can probably figure out the rest okay what are the frequent itemsets well almost all the sequencing Alton's are frequent each of the items except Pepsi appears in at least three baskets for example beer appears in b1 b3 b5 b7 b6 + b8 okay Pepsi itself is not frequent because it appears only in b2 and b5 there are also some frequent double Tain's for example milk and beer appear together in the four baskets shown and beer and coke also appear in four baskets and finally coke and jolla and juice appear together in three baskets but no other double Tain's are frequent for example milk and juice appear together in b2 and b6 but in no other baskets okay also there are no sets of three or more items that are frequent so we're done we found all the frequent itemsets one might have included the empty set that is surely a subset of all eight baskets but the fact is uninteresting and we usually ignore the empty set as I mentioned the the original application for this sort of analysis was looking at things people bought together in a store in this case the items really are the items one might buy and the baskets are sets of items bought together by one purchaser okay there is a story that the first interesting discovery of a frequent itemsets was that diapers and beer were frequently bought together and once you think about it it makes sense if you're buying diapers you probably have a baby at home if you have a baby at home you probably aren't going out to a barn to drink so you bring the beer home I have heard several people claim that they themselves discovered this so I suspect that really nobody did and it's just an illustration of something that you wouldn't think of without a way to analyze massive amounts of data but which proves to be true and explainable rationally once you see it okay the first thing marketers did with information like this was to reorganize their shelves they would put the diapers in beer near each other but put another item that made sense between them like chips but there is a more subtle way to exploit the data mining discovery suppose we run a sale on diapers but we raise the price of beer people will come in to buy the cheap diapers and while they're there in the store they're likely to pick up some beer not noticing the price is too high or even if they do figuring it doesn't make sense to drive to another store just to buy beer everybody wins the customer doesn't lose money and the store gets more customers and on average receives the same amount from each customer only the competitors of the store who don't have their own data mining experts lose incidentally a subtle point here is that there is causality operating and it's very hard to deduce causality from data that is if you don't think about the causes underlying the data you might imagine that you could just as well run a sale on beer and raise the price of diapers but people who come in for the sale on beer are not going to buy diapers if they don't have a baby I just want to point out that these techniques are appropriate mainly for brick-and-mortar stores a brick-and-mortar store needs to know that lots of people buy diapers and beer or else they're wasting time and money optimizing the sale of something that is rarely bought anyway that viewpoint matches well with the idea that we're looking for high frequency rather than correlation between rarely purchased sets of items online stores on the other hand do not need to rely on high frequency since they can tailor this store differently for each customer thus entirely different forms of analysis or leader needed for online stores and we'll cover that as a separate topic here's another problem that uses the same data model with an entirely different interpretation suppose our data is a collection of documents think of the sentences that appear in one or more documents as a basket the items are the documents themselves and the basket corresponding to a sentence contains all the documents in which that sentence appears now what does it mean if a set of items appears together in many baskets the item set is a set of documents and these documents or items appear in a lot of baskets together that means there is a collection of documents in which a lot of the same sentences appear documents that look like that may well be an instance of plagiarism it is interesting to note that in this case the items the documents are not in the baskets which are sentences and in fact it's the other way around but as we mentioned items and baskets form of many-to-many relationship and you can always view such a relationship from either side however when it comes to the algorithms we'll discuss there is an asymmetry we assume that baskets contain small numbers of items while items can be in very very large number of baskets the algorithms would take too much time if baskets contain large numbers of items because the work done processing each basket is normally quadratic in the number of items it contains here's another application involving documents let the baskets now be the documents and let the items be words we can think of a basket a document that is as the set of words it contains but remember what we just said we have to be careful that the average basket doesn't contain too many items if documents are books for example they would contain thousands of different words but if documents or tweets or emails we're okay because these are typically short in the case of tweets there necessarily short we can cut down on the number of words in a document by ignoring stop words as well the common these are the common that usually don't carry any significant meaning now what does it mean if a set of items is frequent that means we have a set of words that appear together in a large number of documents by the way that's another reason to get rid of all the words that are not pretty rare or will just discover the words like the and and appear together in many documents that's big deal right on the other hand if rare words are relatively frequently found in the same documents then there might be some connection between the words I'm supposing so that Brad and Angelina might be two such words or is that old news probably is anyway just to remind you of the scale of the sort of problem we're thinking about if we're dealing with real market baskets a big store like Walmart will sell a hundred thousand different items and stores billions of baskets in its database for analysis on the other hand the web has billions of different words most of them by the way of misspellings and many billions of pages often the problem of finding frequent itemsets is characterized as the problem of discovering association rules these are rules that say if a basket contains some collection of items then it is also likely to contain another particular item the notation for Association rules that we use is shown here informally if we assert an association rule that says I won through I K in plies J we mean that if a basket contains all of I 1 through I K and it is likely to contain J as well the degree to which this event is likely is called the confidence of the rule it's the fraction of the baskets containing I 1 through I K at also contain J for example here are the eight baskets we saw earlier a possible Association rule is this milk and beer imply coke let's focus on the four baskets that have both milk and beer but we see that b1 and b6 do have coke while B 3 and B 5 do not thus two out of the four baskets with milk and beer do have coke and the confidence of this rule is 50% one reasonable thing to do with market basket data is to find all Association rules that have a minimum support s and a minimum confidence C for some values of s and C that you decide on before you run the algorithm the support of an association rule is the support of the side to the left of the arrow that is it is the number of baskets containing all the items on the left the hard part of finding Association rules is really finding the frequent itemsets if an association rule like this has support s then the set on the left will be frequent with support s that is the set by 1 through I K but if the countenance of the rule is also high that is the confidence C is close to 1 then the set of items with J the item on the right thrown in will have support CS now if C is large then CS will be close to s say perhaps 1/2 of s ok and here's a recipe for finding all the association rules with support as in confidence C start by finding all the item sets with support at least C s also find the item sets with support at least s that will be a subset of the first set of item sets let's focus on an item set in the first collection that is one with support at least C s suppose it has K plus 1 items as members there are k plus 1 subsets of size K each form by removing one of the items I've abused the notation a bit by singling out one of them is J the item to be removed leaving I 1 through I K but in fact we have to do this k plus 1 times 1 for each of the items having removed J look at the support of the remaining set I 1 through I K if it is at least as we might have an association rule with the set o that's it on the left and the k plus first item J on the right suppose the set without J has support s1 and with J the support goes down to s2 then the confidence of the rule is the ratio s 2 over s 1 because that is the fraction of the baskets with I 1 through I K that also contain J if that confidence is at least C then we have an acceptable Association rule we're going to let our finding frequent itemsets in a setting where the basket data is kept in a flat file not any sort of database system as I try to argue on the previous slides it is finding frequent itemsets that is the hard part of finding Association rules so even if our goal is to get Association rules and in many cases we really want only the free itemsets anyway not the association rules what we're going to talk from this point only about the problem of identifying the frequent itemsets we assume the data is so big that it has to be stored on disk since reading data from disk often takes more time than what you do with the data once it is in main memory our primary goal will be to minimize the number of times each disk block has to be read into main memory we're also going to assume the data is stored basket by basket rather than by item or in any stranger way and we're going to have to find for each basket all its subsets of a particular size we can do that in main memory once the basket itself itself is there we can use K nested loops to generate all subsets of size K since we assume baskets are small and often K will be only one or two anyway we're not going to worry about the cost of doing this generation however you should be alert to the possibility that if you're asked to generate all subsets of size a hundred thousand from a basket with a million items you just couldn't do it okay so here's a picture of what we imagine the file looks like items have been coded as integers so the file is a sequence of integers we need to do a way to indicate where one basket ends and the next begins so we might use an integer like minus 1 which we suppose can't represent an item as the separator for baskets as we mentioned we can focus on the number of times the disk block is moved between disc and main memory turns out that the algorithms we will study each operate in passes during a pass the entire file is read block by block in order a surrogate for the the cost of the algorithm is thus the number of passes the number of disk i/os is is that number number of passes times the number of blocks that the file of the basket occupies another non-obvious point is that for the algorithms we consider the limiting factor is usually how much main memory is available so for example it is common to have a pass where the file of baskets is read and as we read the baskets we count all the pairs of items contained within that basket we need to have a place in main memory to count each pair so that means at least a few bytes prepare if we have a hundred thousand items then there are five billion pairs at four bytes per count that's 20 gigs of main memory okay we can do that by a little extra for a machine or use several processors but if we have a million items that's a half a trillion pairs and we can't really manage all the counts in main memory and if it isn't obvious using disks to store the counts will not work the counts after we have to increment or essentially random so if even half the counts need to be on disk at any time we have a 50% chance of needing to disk i/os with every increment I'm going to concentrate on how you find frequent pairs of items often finding frequent items that is sets of size wonders or Singleton's is not too hard because there aren't so many items that we can't count them all in main memory as we make a pass through the baskets but is also common for the number of pairs to be too large to count them all in main memory you might think that there are even more triples of items than there are pairs you'd be right however the algorithms will cover exploit the fact that once you have the frequent pairs you can eliminate most of the triples and not count them at all one might ask why there shouldn't be lots and lots of frequent triples the reason is that if we're going to bother to do a frequent itemsets analysis we don't want so many answers that we can't even think about them all as a result it is normal to set the support threshold high enough that it is hard for a large item set to be sufficiently frequent as a result most of the sets that we meet that will meet the thresholds will be Singleton's or double - the bottom line is that we're going to concentrate on algorithms for finding pairs the extension to larger item sets will be given once and can be used with any of the algorithms we discuss let's start by talking about what we might call the naive algorithm we want to read the baskets in some number passes so why not use just one pass and count all the pair's in main memory we mentioned this briefly but just to make sure we understand what happens when we process a basket we use a double loop to generate all the pairs of items in the basket and for each pair we add one to its count this algorithm actually works provided there is enough space and maintenance of items the number of bytes we need is roughly the square of the number of items in our data that is the number of pairs of items is the number of items choose to or approximately half the square of the number of items and if we can count each pair in two bytes which is possible if the threshold is no more then two to the 16th then the number of bytes we need is exactly the square of the number of items if we need four byte integers to count then we need twice that Square and just to recall the typical number of items if you're Walmart the number of items is about a hundred thousand so you might be okay but if you're dealing with items as webpages then you're definitely not okay before we proceed I need to talk a little more detail about how you organize main memory to do the counting of pairs there are actually two approaches and which is better depends on whether it is likely or unlikely that two given items ever appear together in a basket one approach is to maintain a triangular matrix I'll talk about this on the next slide but the idea is to maintain a two-dimensional array where a of I and J is only there if I is strictly less than J the second approach is to keep records with three components IJ and C meaning that the count for the set of items I and J is currently C you organize this collection of records by indexing on I and J so given a pair IJ you can quickly find its record and increment its count or just read its count if that's what you want the triangular matrix approach requires four bytes per pair of items I'm going to assume from here on that integers require four bytes even though if as we just mentioned it is okay to keep them small then fewer bytes could be okay it is even possible in some circumstances that you need more than four bytes but I'm not going to worry about that case either on the other hand if we keep a table of triples then we need 12 bytes prepare for each four IJ and C but the advantage is that a pair only needs a record if it appears in at least one basket as a result the space requirement for the tabular approach is 12 bytes per existing pair but not possible pair here's a picture of the difference for the triangular matrix you need four bytes per unit area of the triangle for the tabular method you need 12 bytes times the fraction of the area that represents pairs actually present so if more than one third of possible pairs are present in at least one basket you prefer the triangular matrix if you expect fewer than a third of the possible bill pairs to be present in the data then you should go for the tabular approach now let's look at how we construct the triangular matrix given the given that this structure is not exactly built into most programming languages first of all we'll assume the items are represented by consecutive integers starting at one if items in your data are represented by their names or by integers that are not consecutive then you need to build a table to translate from an item's name the data to its integer a hash table whose key is the original name of the item in the data we'll do this just fine okay we're counting sets of two items so we can think of each set as an ordered list of length two that is will count all I and J such that I is less than J I want to use an order for the pairs that look like this if there are n items in total then first come the N minus 1 pairs whose small a member is one that's these these pairs are in order of their larger member then come the N minus 2 pairs whose smaller member is 2 again these are ordered by the larger member then the N minus 3 pairs with 3 is the smaller and so on what we really have is a 1 dimensional array and we need a function that takes I and J where I is less than J and turns it into the position in the array belonging to this pair here's the magic formula I'll let you figure out why it works but for example if N equals 10 let's look at the pair 3 5 I claim it is at position 2 that's I minus 1 I is 3 of course times 10 that's n minus I over 2 that's three halves plus 5 that's J remember J is 5 and then minus 3 which is I you work that out it's 19 ok that makes sense because there are 9 pairs ahead of the payer 3 5 that have a 1 as the lowest the lowest member of the pair there are another 8 pairs that have 2 as the lowest and then there's one more pair 3 4 that comes ahead of 3 5 the total number of pairs that are represented as n choose two or about N squared over two and we use four bytes per pair so the number of bytes needed is about two N squared if we use a table of existing pairs then we need space 12 P for the triples where P is the number of pairs that occur in the data as we mentioned this amount of space is less than that of the triangular matrix as long as P is at most one-third of the possible pairs or about a P less than N squared over 6 however that for this method we also need an index of the pairs of integers so that we can quickly find the record for that pair this structure also requires space depending on how we implement it for example we might implement a hash table in which case we need pointers to link a list for the for each bucket so let's say here's the hash table here's a pointer to the first element of an i J and a C and then a pointer to the next element and so on that would add another 4 bytes prepare on in particular a not counted in the 12 P is all the space for the bucket headers that's probably not too much but then another integer at least for each of the links that is going to lead to a cost of about 16 P rather than the the 12 P and and in addition again this is the the cost for the bucket headers which is probably negligible 
YzQWRoJz7zU,27,"Data stored in the database. A database is also called a database management system or DBMS. ...
Data warehouse. ...
Transactional data. ...
Other types of data. ...
Association. ...
Clustering. ...
Classification. ...
Prediction.",2020-10-29T23:50:07Z,Types of Data: Introduction to Data Mining,https://i.ytimg.com/vi/YzQWRoJz7zU/hqdefault.jpg,Charles Edeki - Math Computer Science Programming,PT33M42S,false,91,7,0,0,1,again welcome to introduction to data mining its 632 and in this lectures we're going to cover different types of data again this lecture is in our textbook section one so our main objective again is to go through what is a data and also talk about different types of data that we can mine or can be used in determining so first we start with uh what is a data here we say data is a collection of objects and they are attributes or we can also use the term they are characteristics so example of a data can be students data and then a student to be the data object and the student last name first name address grade etc will be their attributes or the characteristics so again a data is a collection of data objects and they are attributes so a data set can often be viewed as a collection of again data objects and we use other names for data object as a record point vector pattern events and event is used mostly in statistics case sample observation also used in statistics and also entity and it is using database so an attribute is a property or as we said the characteristics of an object so example is we have a student a student will be the data objects and then the attributes or the characteristics of a student again can be the last name etc here we have example of a attribute can be the eye color of a person or a patient's temperature attribute also can be called as a variable or a field characteristics or features also a collection of attributes will normally describe an object for us so if i have an attribute of a last name first name the grid and this can describe the student objects also object is known as record as we said earlier the point case sample entity or instance again depends on the type of subject you are taking for example in database we may call it record in statistics we may call it sample or observation also database we may call it entity so it depends so what is an attribute values here we see attribute values are the numbers or symbols also it can be a test that assigned to an attribute so a student's last name will be the attribute value which in this case it will be a test such as james manuel or student's age 27 this time it will be a value so attribute values can be a test it can be a symbol or it can be values that we normally assigned to and attributes also the distinction between attributes and attribute values and so the attribute will be the name the characteristics name and so example of an attribute would be last name then attribute value will be james so james is actually the data object so here we say same attribute can be mapped to different attribute values so example can be a height can be measured in feet or a height can be measured in meters also different attributes can be mapped to the same set of values so we have the attribute value for id and h they are all integers they are all numbers but their properties may be different so for example we know students id we cannot find the average value or the sum of student id even student id will be a categorical data we can't do arithmetic operations on it but with student age it's a it's a numerical value a quantitative we can find the mean media addition subtraction in arithmetic operation on it so the properties of of an attribute values can be different as we said so id has no limit but age may have a limit we know a person cannot can never be thousand years old or 500 years there's a limit but id we can choose any number there is no limit so next we talk about different types of attributes and this is also covered in statistics there are at least four different types of attributes we have the nominal ordinance interval and also we have the ratio so nominals can be values that we cannot do an arithmetic operation zone we can never do an arithmetic operation so example will be id numbers or my eye color or my zip code i cannot hide or find the mean of a zip code so nominal data only arithmetic operation we can perform on nominal data will be the mode or counting then we have the ordinary ordinary means against something that is in order so ordinary will be example rankings example the nba league new york knicks is first brooklyn nets second so we put them in ranking all grading system students a store got grade a student b got grade c etc so ordinal values are normally values that we can put in order so that means in this ordinary operation arithmetic operation that we can perform in ordinary can be everything that we can perform in nominal which in this case is counting on mood but including less than and greater than or less than equal greater than equal so with ordinary we can perform a relational operation greater than or less than and equal the next one is interval and the diff interval can do almost all the arithmetic operations but with interval the value zero means something again the value zero means something so a very good example would be the temperature the weather temperature if it's zero that means it's very very cold but in ratio which is the last one if it's zero it means there's nothing so a very good example would be again my bank account in my bank account if the balance is zero it means i don't have anything in the bank so zero means something so with interval we can do all the operations for nominal and ordinary also including addition subtraction then with ratio we can do all the arithmetic operation for nominal arduino and interval including multiplication division so dismissing ratios we can do counting or we can find greater than or less than or equal we can also use the addition subtraction and we can also use the multiplication division so those are the four different types of attributes that we are going to encounter in our data analysis or data mining course so these are the operations we as we are saying earlier so the type of an attribute depends on which of the following properties it possess so with the nominal is only the thickness equal not equal that's it with the ordinary including greater than or less than then with the interval we including plus or minus the multiplication multiplication section will be our ratio so with ratio we can do this thickness we can do ratio we can do this thickness order addition and multiplication so that's what we said here norma nominal attributes only the stiffness ordinal attribute is this this thickness and order interval attribute is the sickness order and addition then ratio all the four properties so we have some table here with some example and the attribute types is on the left side we have nominal ordinary interval ratio then we have the description of each then we have an example then what kind of operations we can perform there so when we look at the nominal we can see that with a nominal we say the values of a nominal attribute are just different names and nominal attributes provide only enough information to distinguish one object from another so whether it's equal or not equal so very good example would be the eye color female or male employee id or zip code you can see that all these attributes we cannot do much arithmetic operations on like addition subtraction multiplication division but we can count or we can say what is equal not equal so example of operation we can do here is the mode counting we can also find the entropy then also the contingency we can also find a correlation and we can also do the charge square test you know a child square test use a qualitative or categorical data and then nominal data is a qualitative so as we mentioned earlier there are two types of categories of data either we have the categorical or qualitative which is the nominal but quantitative or numerical values in this case it can be both interval and ratio and possible arduino also so with the ordinary the values of an ordinal attribute provide enough information to order the objects so whether greater than or less than to put them in order an example with the hardness of minerals can be good better best design order or grades can be a b c and d or street numbers can go in order 100 120 150 street numbers so anything that we can put in order we take the ordinary type of values then the next one is the interval so with interval we can perform uh again we can almost perform everything including everything of nominal ordinance then including addition and also uh subtraction addition and subtraction and the only thing we know we mean difference between interval ratio is that with interval zero means something so candidate temperatures what are in sections of fairy height with interval since we can do addition subtraction means we can find the mean standard deviation or we can do a pse correlation t test and also f test as i'll put this is testing then the last one will do almost every operation every arithmetic operation so whatever nominal ordinary interval can do ratio also can do then including multiplication division so example with ratio also can be temperature and kevin or monetary quant quantities like my checking balance account or the price of a goods because of the counts age weight length etc so with that also we can do geometric mean harmonic mean percent variation we can also do hypothesis testing on ratio values so these are the attribute level again nominal ordinal interval ratio here we want to talk about transformation so with nominal we can do any permutation of values now if our employee id numbers were reassigned will it make any difference that would be a comment with odinna again everything will be an order so an order preserving change of values medium large extra large with interval our main concern is that zero means something so uh temperature again is a good example then with ratio can be anything length can be measured in any measurement either in feet or in meters so these are the main two types of we saw that those four are the levels of measurement nominal ordinary and we also have the interval and then the ratio these are the level of measurement but actually the attribute values are breaking or attribute tags break into two the discrete attribute and also we have the continuous attribute so with the discrete attribute it has a finite or countable infinity set of values so it has only infinite or countable infinite set of values so example of a discrete attribute can be the zip code count or set of words in the collection of documents so which means nominee is a discrete attribute and we say yes often represent as intelligent and variables also binary attributes are a special case of discrete attributes then we have the continuous attribute containers are able to have a real numbers as attribute values so example can be temperature height and weight so this means the interval ratio can be a continuous attribute and we say practically real values can only be measured and represented using infinite number of digits so continuous attributes are typically again represent as a floating point values so these are the type of sets data set we have so for example we have record graph and ordered and then let's keep going but before we talk about types of data set we should also know that the general characteristics of data set will be the dimensionality the dimensionality is there of a data set is the number of variables that the objects in the data set process so if the object process two attributes then we have two dimensions three attribute and this is called a dimensionality and also in data preparation lectures we're going to discuss about dimensionality case which means if i have a data with so much attributes it will take long for the guardian to process everything which takes longer time and sometimes the algorithm being confused with so many attributes so we have some method that we can do to reduce the dimension of the attributes reducing dimension of attribute means again we want to reduce the normal variables that data objects has so we consider data with a small number of dimensions tend to be qualitatively different than the moderate or high dimensionality data also the difficulties associated with analyzing high dimensionality data are sometimes referred to as the case of uh case of dimensionality so again we have a techniques in data pre-processing that make it possible to reduce again the number of attributes also one of the characteristics of a data set would be spacity city and another one is the resolution so for some data sets such as those with asymmetric figures most attributes of an object have values of zero and many in many cases and fewer than one percent of the entire entries are non-zero so in the practical sense we see a spastitis spa city is an advantage because usually only the non-zero values need to be stored and manipulated this results in significant savings with respect to computation time and also the storage that's the computational complexity concept and also ratio resolution is one of the characteristics of data set so it's frequently possible to obtain data at different level of resolution and often the properties of the data are different at different resolutions so those are again the three major characteristics of data set the dimensionality of the data set spacity and also the resolution so we have different types of data again we have a record in record we can have a data matrix uh more or less like a tables which consists of rows and column or regular document document data or transaction data so much data money work assuming that the data set is a collection of records or data objects and also each of which consists of a fixed set of data fields the attributes and so for example of a database relational database we can store values in the more or less like a table the rows is the whole set of data object which is the record and each field is called the attributes like last name first name etc so we may have a records can be as we said earlier i can be a data matrix or document data or also transactional data so a transaction data is a special type of record data where each record or the transaction involve a set of items so example would be a grocery store i bought few items let's say pinot gara bread and brick again this is a transaction data set and normally this type of data set we call it the market basket data and when we cover the association algorithm we're going to discuss it by the market basket data in our previous lectures we said the association is a data manual guarantee that make it possible to find two or more variables if they are relate associated they vary so our case with the market basket data i want to know what percentage of customers that buy bread they also buy break or buy bread they also buy meat or something [Music] so we also have a graph data which is like the world wide web a good example and also the molecular structural chemical and data set so a graph can sometimes be a convenient and the most powerful representation for data especially when we are representing data in hierarchical form so we will consider the two specific cases again the graph will capture relationship among data objects and also the data object themselves can represent as a graph and normally the relationship among the objects frequently will convey important information in graph in such cases we can say that the data is often represented as a graph and the data object are mapped to nodes of a graph while the relationship among the objects are captured by links or sometimes we use the term the edge a graph normally consists of a node and a line or a link between two nodes or more which is also called an edge also we have an ordered data so all that data can be sequential data concept so here we have a spatial data temporary data sequential data and genetic sequential data so a special data normally is a data that you represent in space location like a gps data set to be a very good example and also a sequential data also can be called temporal data that can be thought of as extension of record data and the temporal data means that data is dealing with time then smallest i've got time series data for example the stock market data set is time series because the stock price changes with respect to time also transaction data retail transaction data can be a temporary data because things are bought we recorded items that are bought with the time the time that they were they were bought another we have the sequential data will be a data that is in sequence so a very good example would be again the molecular or dna dataset yeah we say genetic sequences we know that genetic sequence is agtc so we may have any other agtc gtc so that's in sequential order so sequence data consists of data set that as a sequence of individual entities such as a sequence of words or letters in the terms of genetic sequence data is in the letters the order of letters ajtc that's a very good example of a gene this is sequential data also term series data is a data that record values with respect to time so most of the time the x axis will be returned special data as we said relate to the location so here we say some objects have a spatial attribute such as position or area as well as other types of uh location so spatial data can be a gps as we said so dimensionality we talk about the cost of dimensionality and when a data set have a very high dimension which is the attributes are so many then we say there's a case of dimensionality the reason why we say case of dimensionality is that if you have a data with a very high dimension and what happened is that the order reading will take so long or even sometimes your gardening will confuse and to go through other organi and the attributes to make a decision so dimensionality we always want to have a normal moderate attribute and also we talk about sparsity only presence counts here we said spasticity deal with space location so for some data sets such as those with asymmetric features and we say some attribute of an object can have a values of zero in many cases less than one percent while the rest will be non-zero values so sparsity is an advantage because usually only the nonzero values need to be stored and manipulated also we talked about resolution the patterns will depend on the scale of resolution so this is an example of a record data data that consists of collection record each which consists of this set of attributes so here we have a record with 10 data objects so it should be a task id i one two three out of ten there are types ids so this is a recorded by tax payers and we have ten taxpayers we have attribute whether they get refund yes or no we also have the marital status status if they are married single or divorce or windower then we have our their testable income how much they make for the year and then we have this would be a training data set so we have the class level which is to cheat whether yes or no so our main goal of this data is to build a model that will be able to predict or to tell us whether a taxpayer cheat on tasks yes or no so the first four attributes are the attribute of the uh the taxpayer the last also is the attribute of the task fair but this is a very special attribute with something we call the target variable or the class loop this is what we want to predict we want to create a model that can be able to predict this result for us then we talk about the data matrix will be in the form of a table rows and columns as you can see example here so here we see if a data object have the same face set of numeric attributes then the data object can be thought of as a point in multi-dimensional space where each dimension represents again a distinct attribute remember we're talking about dimensionality in a case of dimensions always dimension represents attributes then a document data here we say each document becomes a term vector button is one record so h10 is a component or attribute of a vector and the value of each component is the number of times the corresponding term occurs in the document so here we have three documents and we want to see how many times this voice okay in each document transaction data we say as a very good example would be uh basket market and data set here we can see grocery store in supermarket the first customer by brad kochmick second customer beer and bread third customer buy beer coke diaper and rick fourth customer by beer bread diaper break and the last customer by cook that pembroke so we can use the association redeem example be the apron or gary theme to find a relationship between all these transactions what is the chance that if a customer buy bread you also buy coke based on these data set we can find what we call the support and the confidence to predict and find the association we also have the graph data again graph data suite we explained earlier time a very good example would be the web website data with the html code etc then we also have chemical data example here is the benzene molecular c6h6 this also can be mined and other data other data is almost same as sequence data so this is a sequence of transaction we see that if a customer by a b also by dc or bbc so again we can analyze this data set to find the relationship between the items or events that take place then we have the genomic sequence data this is the htc's form of genes or dna we can also analyze sequences that are used in determining we also have a space we know spatial is the space temporaries the time so we might have a data that is both in space and time so example will be this data is about temperature of different locations so the template change with time so that's why it's temporal and also the data is from one specific space area or location so this data cell because species special data so that'll be the conclusion for our lecture on different types of data sets so so here we discuss about what is a data set and we know data set consists of uh attributes so we discuss about what is an attribute the characteristics of the data and also we talk about different types of data sets so again wish everybody the best next lectures we're going to discuss about how we prepare for data before mining and also data quality so again we share the body the best thank you 
8kok-2p72cs,22,Reference Book ---  Data Mining:  Concepts and Techniques(Second Edition)- Jiawei Han & Micheline Kamber,2020-05-25T19:30:39Z,Data Repository for Data Mining : OBJECT RELATIONAL DATABASE,https://i.ytimg.com/vi/8kok-2p72cs/hqdefault.jpg,Ganesh P,PT21M14S,false,11,0,0,0,0,next we are going to study advanced database systems first advanced database system is object relational so how can we construct object relational database object relational databases are constructed based on object relational data object relational data model is used to construct object relational the object relational databases are based on object relational the object relational data model extends the relational model by providing a rich data type for Hanley complex objects as well as objective formulation most sophisticated database applications need to handle complex objects and object relational databases are becoming increasingly over in industry most sophisticated database applications need to handle complex object and structures so of central asia databases are becoming increasingly the in the industry an object relational data model immigrants the essential concepts of object-oriented were each entity is considered as an ostrich so object-oriented relational data inherits the essential concepts of object oriented databases this object and class concepts so in object relational data model each entity is considered as an object as we are considering in object when we consider the old electronics example some objects are individual employees customers in this object relational data model the data and the code relating to an object data and code relating to an object are encapsulated in a so the data and code of an object's data and code of an object are encapsulated into a single so object or object relational data model in interest the essential concepts of object oriented databases each entity is considered as an object and the data as well as the pole of an object are encapsulated into a single now in object relational databases some objects have same properties so we can combine them into a which is known as process the objects with similar properties are grouped in process in object oriented relational databases basic purities object a group of objects with the similar properties are grouped into now consider a single object for each object until business an object for each object it has associated with some meshes some some measures or some attributes something some some attributes or some measures are associated with each other so each object has associated with it a set of variables set of messages set of methods set of variables describe the objects objects are described by a set of fields this corresponds to attributes in the NDP relation and relational models in ER model or relational models we have attributes the equivalent of attributes is the set of variables in object relational data maps so we have set of variables second set of messages the set of messages associated with the object can use to communicate with other roads for build the rest of the data lists then an object want to communicate with other object or a must of the databases it uses messages instead of message messages associated with object and use to communicate with other objects or with the rest of the databases third thing is set of methods each method holds some code each method hold some ho this code is used to implement the so message is used to communicate the TV objects or literally in their database the messages are implemented using a set of members so it set of methods holds or improves when a message is received then a message is receiving upon receiving a message a method is involved and dark method returns a value so one object want to communicate with that object or with other objects some methods are used then a message is received upon receiving a message a method is called this method returns a value in exam the method for the message get photo of my get photo of employee get auto of employee is a message then for this message a method is called this message Mildred sorry this method will retrieve and return a photo of the human embody so each object asked associated with it the following three days set of variables describe objects similar to attributes in PR or relational second one set of messages used to communicate with other objects or with the rest of the database systems and third thing is set of methods each method holds the code to implement it now objects that share a common set of properties some objects share common sector properties they can be moved into objects each object is considered as an instance of object each of the days and uses of its class the object classes can be organized into class hierarchies or sub class setup so that in each class represents properties data woman who objects object classes can be organized into hierarchies class or sub class hierarchies so each class represents the properties that are common to objects in us consider one exa we have a embroil [Music] okay so this class has some attributes and some methods this employee class contain the variables like name address and consider another class salesperson salesperson is another class which is a subclass of temporal sub class so he sales persons object assumed just once more as a salesperson object would inherit all of the variables present in the superclass of then we declare when we have an object of salesperson say s1 it integrates all the variables in the employee class in addition this salesperson has its own attributes in addition it has all of the variables that pertain specifically to being a example omission in employee there are three attributes name address voltage in sales person assume there is one more attribute attribute comes when we declare when we use a salesperson object s 1 then it has all the variables of both employee ancestors then even is a object of employee then it has only the three attributes so the class inheritance feature benefits information share now how data mining is performed in object relational systems for data mining in object relational system there exists different techniques or some techniques are needed to handle several days or data mining in object relational systems techniques need to develop for handling some features that is to handle complex object structures complex data types class and subclass hierarchies property inheritance and methods and efficiencies so we need to develop already there exists techniques for handling complex object structures and complex data types and class subclass hierarchies property inheritance and methods and procedures so this is a brief introduction to object relational object relational databases are constructed on of jetty relational data model it has a rich data type for handling focus objects and object orientation what is the importance of this object relational temari's majority of the database applications need to handle complex objects and structures so object ratio data bases are becoming increasingly popular in industry and then this uses object-oriented dr. basis illiterates that is the this object relational database data model illiterate e essential concepts of object oriented business that is NDT he's come to the lesson then the things associated with object set of variables set of messages and set off then inheritance property we discussed in the property that is how hierarchy hierarchies are formed and angled type of databases the class Littleton's feature release the information share then when we apply this object relational systems for data mining we need to develop for handling some features like complex object structures converse data types cross and cross iraqis property the retains and 
Q6kCzmBaivs,27,Data Mining 03 Unit 1 Notes   Part 1,2017-08-30T12:32:26Z,Data Mining 03 Unit 1 Notes   Part 1,https://i.ytimg.com/vi/Q6kCzmBaivs/hqdefault.jpg,GaryBoetticher,PT14M44S,false,262,4,0,0,0,"hi I'm dr. Gary Bennett sir and in this video we're going to go over the unit 1 data mining notes and the title is what is data mining and I've set up these notes more or less using the same format and what you see in the beginning of the notes is or are the course or the goals for that particular unit so we'll talk about what is data mining this is going to lay the foundation for the rest of the course and we'll talk about what's called the data mining process and having a well understood well-defined process is critical to effective data mining so make sure as you go through the notes to read the goals secondly you'll see in each unit of notes there are objectives things like at the end of the unit you'll be able to explain what is a quant what's the difference between order and chaos give some real-world examples of data mining what's different in data mining data querying identify different perspectives regarding data mining what is the in order of the steps for the data mining process and the significance each phase plays in performing data mining how we might do some data pre-processing or pre-processing cleaning of data what are different types of models what are different categories for hypothesis identify or name 5 out of 10 data mining mistakes and what are some ethical technical issues related to data mining and then you'll see any terms here and the idea is as you go through the notes you come back to these objectives make sure you can answer each of these particular questions ok so I put a lot of time and defining these questions as we go forward here's an outline of what we're going to be doing well let's get right into it why study data mining now I know for a fact there are many universities that you could attend and many course options and I'm really appreciative that you decide to enroll in this course so let me say some things that maybe get you motivated to work even harder in this class first of all here's a 2017 salary distribution for data scientists and so we see the average salary is ninety one thousand high ends about 130 low end sixty two thousand and of course it depends where you are city wise how much experience the company and job but then we see the salary sixty to 130 for any bonus profit share of exist total pay so 61 to 149 I think that's a quite attractive salary now Wall Street hires people that are called quants and they get anywhere maybe ninety to one hundred and fifty-seven thousand what is a quant they're basically they collect data from a client company analyze information and advise decision makers on financial risk and investments typically a quant holds a master's degree now you might be a quant might also be a person who develops strategies from making buying and selling decisions in the stock market in 2012 Rutgers Masters in quantitative finance was getting 110 to 20 to 120 per year and here are some salaries for quants I had from 2010 so they can be 640 thousand a year four hundred something thousand four hundred something thousand Blackstone Group is paying 398 thousand well that's quite a lot of money but let me let me put a perspective on it if you're mad managing a or if a manager of a billion-dollar hedge fund hires you and if you could save the company 1% which is 10 million dollars well they'll be glad to pay you you know five hundred thousand of the 10 million so it does sound like a lot of money but you know you're dealing with astronomical numbers there well in figure one two we see the example of chaos on the left kind of universe and it might have felt like that way with the hurricane Harvey coming through and on the right is the UHCL marching band you probably didn't know we had a marching band and you might think I photoshopped it and of course I did we don't have a marching band but it looks cool and so the right photo shows order and the left show two photo shows chaos and the question is what's the difference between order and chaos are they related do they have anything to do with data mining and the question is will is the universe deterministic has everything been determined or is it more or less kind of a confused chaotic state and I don't have it my perception is that the universe is probabilistic that you know if you're driving from point A to point B I might not be able to predict what lane you're in but like more or less could say okay you're going to take i-45 and Bay Area Boulevard lissa to come from the airport to a UHCL and what we're trying to do is data miners is to recognize any patterns in terms of the chaos and make order out of those patterns and discover those patterns as we proceed so we want to kind of move the chaos over to the order son here's some examples one of Amazon real-world examples you can read those I won't go into detail so we have this Amazon knows what you want before you buy it Facebook has some voting there article now here's an interesting one you've probably had this happen you go to Amazon you look up a book like on data mining and then what happens they say things like if you buy that book instead of fifty three dollars if you buy that in the second book to give you a price break of you know hundred and something dollars they're also customers who bought this book and they give a list of other books that the customer bought and this is an example of data mining where the Amazon goes back over their history and they see where a person is bought book a and B and so when a future customer comes along and request book a they show book B this is called a predictive cross selling and it's also known as market basket analysis so they do data mining to try to put things out there to you know make you purchase more than you were originally intended and sometimes what happens you know if you see a book so that's an interesting book you come across something else that you hadn't thought about well what exactly is data mining before we talk about that let me talk about data mining versus database queries a database query extracts out like a subset like a certain number of rows or certain number of columns like a project as columns and it can do aggregates like average sum count data mining does not part is not part of the original data set it's based on personal financial information is person X a high or low risk for a loan how many hurricanes kind of ironic will hit Euston this year very ironic house out there a database query provides provide precise results data mining might produce fuzzy results what are the chances that a person will live to be 90 for example and database queries will define sequel in data mining a lot of tools and techniques and depending how you use the tools and techniques you can come up with a variety of answers so here's an example of a database query select star from students where name equals Smith select star from students where credit's number credits enrolled is a greater than equal to nine or tuition balance is zero well different people have different perspectives of data mining and what I've done here this is the dining philosophers problem if you've had operating systems I'm sure you've seen the problem there and so we have five philosophers sitting around and eating spaghetti because they're all vegetarians and one philosopher my philosopher might say well data mining works with data therefore it must be viewed as a data DBMS perspective now another one would say well wait a second data mining is lots of Statistics therefore it must be viewed from a statistics perspective third philosopher might say well we're really concerned about performance so I want to focus on hardware and parallel algorithms I think that's the most important and then this philosopher says needs lots of AI machine learning since its tries to find patterns well then the question which perspective is correct and there's no real correct or incorrect answer it's a matter that you can assume one of those different perspectives will be focused more focus more on the AI machine learning perspective in this class but we'll also look at statistics here in it so what is data mining well the definition we have it's an automated process for extracting non-trivial novel useful patterns that's important word from a data repository with the goal of helping a knowledge worker better understand his or her environment one more time data mining is an automated process of extracting non-trivial novel useful patterns from a data repository with the goal of helping knowledge worker better understand his or her environment now a couple things here it doesn't have to be automated might be manual that's okay we want to come up with novel patterns something that is beneficial and that we hadn't seen before we also want it to be a useful pattern there was an example they had artificial intelligence software and it was examining accidents that happen around in the home and so most accidents in the home happened on the first you know flight of stairs on the first stair going up the flight of stairs and the last stair before you hit the landing on the second floor well the artificial intelligence program said well that's easy to solve just take out the first step in the last step and you solve the problem so that might be a reason there might be a solution but it's really not a you full solution and sometimes some of the tools we use they produce a very a model that's very human readable and human understandable other times it produces something that we don't understand the model so it would be nice if the knowledge worker can understand it but not on a requirement an example might be is you have a for example a neural network which can predict the stock market and make you millions of dollars and the idea is that you might not understand how it does it but it just kind of works just like when I get on a plane or if you get on a plane I don't know all the mechanics of how it works all I know is it gets me from Euston to New York or Euston to San Francisco whatever the case might be okay you can read through that well here's something suppose I cleaned I'm a stocked predicting program that can predict tomorrow's final price within 1 cent with 99.3% accurate is this useful and what might that program be worth would it be worth $1000 $10,000 well for you I'll sell to you for $5.00 okay because you're such a great person now the only thing I forgot to tell you is it when I run the program it takes two days to run the program so it is accurate but the results when you get it back are not valuable as you can see here's an example of a neural network solution really messy looking and here's one of a genetic algorithm this one is not human understandable not even human readable this one is human readable but not human understandable so sometimes they produce solutions that don't make much sense and they may be effective may not be effective so we want to be aware of those situations okay I'm going to stop and go to the create a new video starting with the data mining process that will stop right here "
tvW-b_dNclU,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-17T23:24:04Z,Data Mining (Spring 2016) Lecture 10,https://i.ytimg.com/vi/tvW-b_dNclU/hqdefault.jpg,UofU Data Science,PT1H11M4S,false,173,0,0,0,0,so let's get started so let's start by looking at the schedule the class or thing is coming up hopefully you all got the homework turn it over any second to the next homework will be posted later today on clustering so you should be able to start the start within that right away I haven't do the Wednesday read for the midterm offensive will be topics about clustering on midterms I figured you'd want to do it do it go for the menu so but you should look at that a post announcement canvas to be similar in scale to the first it works the other thing to look at is is the data collection importance due on one so what all of you got to feedback on your projects we hadn't made the group's yet so some of the comments on the went to one person in the group so talk to everyone else in your group most everyone had good projects we were on the right track I wanted to talk a little bit about what goes into the data collection report just due next Monday since this is always I mean it can be confusing what what a master at this point and so first understand the mean you know the main mechanism of this the goal of having to try the Santa so that you're still working towards the project even if you haven't they witnessed all of the techniques you need in class yet so I want you to try and so not just to download and data set let's start thinking about how you would represent this this data set so a lot of what we talked about in this class is you take some data whether it's a document or something and you've converted into some other format and once it's in that format then you can run an algorithm we're doing Alice's on it and there's some modeling involved here and I want you to kind of start thinking about that process this day come home so not just downloading and say I've download on my harddrive but you need to kind of you may need to to clean the data a little bit to filter out elements of it that you don't want so certain aspects like that so let's go through these points can people reduce may be making figure so they kind of the three things that you need to are the five things that are required that's beautiful so head of them these are I mean these questions are not meant to be hard if you if you kind of do something reasonable you'll get all the points that the first thing again is I want you to at most one page a few of you turned in project proposals that were too long and i need to put pagelets otherwise people just keep making them longer and longer and longer so I'm doing this for your own good most one page description for this a little bit more than the project proposal and and don't feel the need to make the font smaller force stuff like that but you don't need to write package so so talk about how you obtain the day and how how large it is and these are mainly because you know based on what your proposal is about you may need a certain quantity to see a certain certain phenomena you're trying to find we can give guidance and say this is is not going to be enough data for you want to do or maybe it's so much data that you're just going to spend all the time kind of wrangling the data and not be able to do something interesting so I'll tell you actually you want to throw a most idea it just because it's going to get in the way you kind of exploring what we wanted and so it depends on the you know I'm on your on your scenario so you can do something like what is the gigabytes but something more useful is like the number of data points for the or the number of attributes alert app or something else like that that'll allow us to give more kind of specific useful feedback and then the third one is about format are you storing it for for some of you who have like taken database class or two you may say i'm going to load it into a certain database and that's that's okay what i really want you to talk about is kind of the abstract abstract data to hear right I want you to say I'm going to represent this data as a matrix as a point set and underlying distance maybe the Euclidean distance or veggie time distance and you can change these things on that you could say I'm not sure what dis excuse we're going to explore that sort of things but up but I want you to think about the format the data not just is in a database but how are you what abstract data type are you putting it into so that you can then process it with our local we'll talk about class and so part of this so the fourth one is like how did you process it to make it easier so I'm enema so you should have done some processing of the data at this point so I'm asking how did you do it right so it's just as a way of checking that you've actually actually done something and you don't need to be too detailed here I just find out roughly what your how you did it you throw away certain like if you have Twitter data there's lots of metadata you probably don't need you can throw that out okay so then the fifth one is is kind of always trips people up is how would you simulate some data that's similar to the data that you're working so I think all of you had projects where you had your plane of work with the specific a specific type of data or a a specific data set I'm asking if you didn't have access to this day set or let's say the data set was not can be large enough to really see how well the phenomenon you want to find with the daylight alderman then how would you simulate data so it looked bigger so you could see something about what happens when you scale the day and so just saying if you're represent your data is a matrix just putting random elements and the matrix is probably it might work okay but it you might want to put more structure and so will for matrices we'll talk about sqd pc and that may be helpful later but you know and you know if you don't know how to answer this one yet some of this we haven't covered in class you're dealing with graph data this is still have a challenging way and you want to generate a large graph that looks like it's a it's a social network for instance well you know people have studied large random graphs that you could generate in mathematics for a long time but typically they assume each edge was equally likely to to exist in the graph and social networks are very different than this so you want to simulate it so there are different types of know different sorts of communities in the grass and so you know you may not be able to completely answer this question but try and think about it what you would do and then you it's good to keep this in mind as you're doing it allows you to understand various aspects kind of how what you're doing it might hear after the aspects of it okay is that clay any questions on David clutch report how this will work okay so i do monday shouldn't should you take too much effort to write up the report at this point but we need to start collecting your date a lot of you already kind of downloaded data is you just need to kind of look at some basic processing all right let's let's get into the lecture so today is we will talk about we meet talking about spectral clustering which is the third general type of clustering that will be discussing this class in it just like the hierarchal an assignment base we're very different from each other this one so I could be even more different than the other other two it's all right so so spectral clustering instead of instead of working on on point sets we have a distance the input here is is the infant visit is he is going to be a graph and so just so how many of you I mean you've taken like 2100 s or you work with grass before is not a new new concept anyone's matter of a graph so let's just draw an example I'll use an example graph that that's okay we'll use this okay so then so the elements of the graph are going to be the things they want to cluster so these are like the data points but instead of having like Euclidean structure where you have are these edges in between these notes okay and so this is your info will look something like this probably go you're doing this probably it's not going to be so small so one strange thing about this is a lot of examples you see off it will will look like these small grass and drawn cheese paper and those of you lot easier than you're actually working with much larger drafts then you won't feel to use all the same intuition but will develop a mathematical framework ritual which will generalize where you don't need to see what the pictures okay so now to find a clustering of graphs the spectral clustering is going to be it's going to be top-down so what this means is that it's going to find a a cut in the graph so it's going to so we're going to take this graph is going to be often we write this be as V is a set of vertices and ease a set of edges here and what you want to do it is divided is the these these vertices into two sets s and the set my distance so everything in the set s these are both in the subsets of diversity subset of all these are subsets of vertices everybody into two groups so as we'll see a good clustering of this graph is going to probably look like these could be a subset so then pass be the set a b c d right and then everything in the in the everything else so we'll write this as the the set- SVU or comics say s prime then as a crime maybe everything else does it do gh answer so you interviewed you just need to divide the graph of the two parts and then if if your goal is not to find two clusters but more than then you just apply the same algorithm on socom so recursively and i'll need on the on the suburbs right so the sub graph of s right is so i can have a graph that's induced by s so this is going to be the set s and then all the edges that are in between two two vertices in its right so this the set s is as above and the set of vertices is going to be or the the home so sorry the set of edges is going to be something like there's an edge a B and so notice these are I'm not going to do talk about graphs that have a direction between the edges right now so that an edge is just going to be a pair which is on order so i canna use these squiggly brackets so as a bee it has a see it has um a d handed has B D and C d right so it's all the edges that are in between vertices in the sub graph it is not into advantage of C and C is one of the end points in the subcraft the other ones not so I don't need to look at this so then once I have this this plus i have this subset of the vertices then sorry that notation es5 these are the edges that's why I was saying horses so these are the heavens defined by this subset okay so so so once I have this subset of the vertices that I can I can look at just those engines inside there and then I can recursively apply a technique to try and find clusters within this set EBC key set s here yeah so this is the general framework so what we need to be able to do is to find this good way of dividing the graph into two parts after we do that we can keep recursively doing this until we're work we're happy or we reach some number of clusters or wearisome some freshmen in a similar way similar aspects we have a hierarchical clustering which was bottom homes okay so so then do this so then recurse great so this is a general framework so the key thing now is fighting a good  a good way to split the vertices um okay so what what would we want in a good cut what makes the one I've drawn here good what's what was good about that so I'd like up one edge good so so two properties that we're going to want a good fuss so one is that that will say that the so we'd say that the cut between s and s prime is is going to be small and this is exactly going to be so just a number of edges so it's the number of edges where one vertex is in one side of the vertex is in the other side right so this this is only edge of the top so this is the only edge that's covered so we want this to be small okay so good um so is is this it is this gonna be enough get the cardinality of the first use about the same both yeah okay good right so so we also want something where the graph is balanced the cut is balanced on either side we look to be about the same size otherwise I could also do a cut here and this one has the same number of edges which are cut but you seem like just chopping off your each vertex seems much less useful than dividing by this on the c/e edge because i'm really i'm not going to do much of my recursion I basically gave most of the same graph I started with and I just and the other scientists has one vertex it's very boring and so in very large graphs this this would happen a lot threw me a lot of these social networks and stuff these dangling vertices had very few edges and you'll tend to just cut these off if you if you just try to minimize the cups see want some you want some way to balance so the two sides of it it's a what's typically you is um is it was called the volume of one of these of these subsets so the volume now you want this to be big and so this is even number of the induced agents well I'll be a little bit different than the induced n is the number of edges it's not just the number of vertices the quantity we really care about to be the edges it turns out so the number of edges with at least one vertex in s right so so in the example we have up here this cut for s it's going to have these five edges I've talked about here right the between the ones inside of it but also this MCE i think i'm going to count that one as well so i'm going to count that siiii so in this case the volume is going to be six and then what would the volume V 4 s Prime everything outside of this house oh yeah that would be five right there for internal ones and then the CIA and you know the cross ok so now so it's okay so we want that the cut too small and the volume is big how would we you know having two different individual kind of properties by themselves it's not it's kind of hard to work with so we want to combine these together in some way and a very popular way and they're they're actually a few ways of doing this it's called me it is called the normalized cotton and so this is called the end cut of say s s Prime this is going to be cut of s as private volume of s but this by itself isn't good because then I can just make the volume bakelite having s I can have the same cut and like an epic cut that goes to H here but instead of having each in the set s you know HIV s prime right so I could have a volume very big for s so I needed to Devon the big bold on s and s prime so what you write is cut of s s prime over the vine that's fun okay so I need the volume to be big on both sides so let's go and look at these examples up here so we'll call this tea and then the other party Prime and then we'll call this one s this cup so let's calculate the normalized cut score for both of these for both of these uh these of the subjective right so what is the normalized cut score for s and s prime remember it's going to be one over 16 yeah right so they cut size is one so it's gonna be firm s and s prime P 1 over 6 plus 1 over 5 and this turns out to be something like 0.36 7 and so let's do this instead for the normalized contact with the end cuts of T and T Prime and so this again that size of the kata is going to be 1 by the one over here one what's this I haven't escaped that yet okay so so what is the size the cup of tea so now it's symmetric whether it was t or t prime the cut of t is going to be 1 i'm sorry the volume of t is going to be one there's like one edge it goes from h2g and the settee and how many edges on the other side will one two three four five six seven eight nine ten so it's going to be one in ten now and so this is gonna be one plus and so this is going to be was this is 1.1 so for the cut T now just come off the single vertex this is how much larger this was 1.1 this is 0.36 seven right so so I so I've got a much smaller normalized cut score for the one that divides me and it turns out this normalized cut works pretty well there other variants but I think from what we've from what I've read about this seeing is that the normalized cut ISM is a pretty good one to use in practice and so in the talk about two two ways to find good cuts and and one of them and one will be optimized for the normalized cut one of the optimizer a different cost function forget baby that's the other one maybe something like the product of the volumes on the bottom something like that and if that one works okay but I guess slightly Buswell other versions of this might be slightly mathematically nicer in some context for proving results in theory but this one works fairly well practice was normalized cut-ins a good thing to try and try to minimize okay so so how would we find a good cut or if this seems like a hard problem with with with the hierarchical cluster we could kind of merge things from the bottom of the sea kind of simple and straightforward with the silent basin can pick centers but now it's have to think about these these centers in a way that's that's uh that's useful for instance using like the the distance to walk on the ground from one of these vertices like in this picture if i picked like B and F right those those might work okay if I said the distance from either of these is the number of hops but in much larger grass this becomes doesn't work nearly as well there's these things called like these on this pump so like this there's a sink all the small world phenomena where supposedly in these large social networks that the number of hops to any person or an average person is supposed to be small something on the order like originally there was this experiment and I think by Milton in the 1950s where he he tried to connect people like in the mail this is very pre-internet right and they had so what they what they said is he gave a much people on a task of trying to get a letter to someone in some he gave someone else's random headrests and he said mail to someone who you think know someone who's going to be closer great so mail to someone who you think know someone who may know someone who get mail out there so people would send it to someone who lived in that state that they knew they had works or they knew it someone moved in in the right City or something like that and ours it turned out that all the ones that were successful which I think was only about twenty percent it took about I think it always included in him like a most in him and most so seven hops it took only seven cents of the letter so there's this notion of the the seven degrees of separation and so that but people have been saying with the internet this should become much smaller and so Facebook recently released some study which seems kind of somewhat controversial kind of partially just for the headlines that said the average distance between any two people based on who's friends with who in the facebook graph is like three and a half so for any person in the Facebook Graph you can probably find a path that's three or four people in order to pump so in order to kind of have friends of friends of friends of friends site does connect sins of the people right so these notions of kind of distance based on the graph hops it's kind of make up work very well because a lot of people are going to be about the same distance there could be tides like half the graph maybe three hops away right so if you can say to well then that's very strict so these are kind of you can't use these same sort of intuition anymore on graphs things break it okay so now that I've told you these things that don't work very well what what other ways could we possibly do like what's another way to deal with crafts so what we're going to do is we're going to switch our abstract types and so this is going to be very cumbersome to draw so i'm going to use slides again and hopefully this do this okay so we're going to look at this graph again here and the idea is we're going to represent it as a matrix okay so we have this graph same graph and hear all the hedges and list it out so you can draw or you can write analysis of edges or you can write as this matrix you can think of it as kind of like a chart or something where you have each node is a letter or it next it to the into the the rover the column and there's a one if there's neck and it's 0 if there's not such as something you can write this as the matrix like in Matt if you use MATLAB will do some experiments with matrices later in the class the clustering or work we won't need to use the the matrix part of it so we use this representation as a matrix now what's before we talk about how to take advantage of this what's kind of silly about this representation yeah it'd be super spars meaning that they're gonna be a lot of of these these entries which are zeros look fierce I look here I think so so if i look at these this row this was with the this this note eh you can see that almost all of it is zeros except it was 11 right and and for much larger grass this is going to be you know even more true you're going to get very very sparse snakes or something like less than one percent of the data is going to be be nonzero there could be one 101 percentage of subjects now it depends on who you talk to some people will say well the when these graphs grow various ways you can give these sparse matrices these these graphs that the you know what is the rate at which the number of the average number of engines rows per column and so some say well it's going to be something close to constant that used to be the prevailing thought or maybe it was something like it was log in if there are any people then you have about log n hat edges on average but they're also size that says it's like a psycho polynomial so the number of engines is going to be roughly V to be 1 plus C for some constant and so in this case the maximum number would be C would be one it means that it's completely connected but you get numbers where C is going to be something like 0.2 are up to 0.5 so it's controlled upon overly large but if you look at like Paula wheels like what like taking something to thee to the power 1 point2 it's still much smaller than the whole graph if it's very big that could still be less than 1% of entries are given once so even though you're used to thinking in like an average class that log in is so much smaller than a polynomial well you know are these scales polynomial still very very small ok so we're going to use this graph this matrix representation and there'll be ways we can deal with this varsity implicitly they like in matlab there's a way you can store this in a sparse format and then it basically has this vertex edge representation that it stores internally but you can do operations on matrices you'd normally want to do in it and it's optimized to only kind of look at the entries which are non zero so there there there are quite a few mechanisms to deal with this varsity and we're going to kind of ignore those for this function but if you really want to scale this off then that's I do okay so what we're going to put it to is not just look at okay so we're not going to want to look at just this this matrix here where this is just what's called the the adjacency matrix so i'll write this as a this is the same matrix we had before in the same representation that's adjacency matrix for every row this corresponds to G Rho G and say column e right there's a one-in-four anytime we also want it look it was um what's called the diagonal matrix and so this one only has entries on on the on the diagonal going down the middle here right so the only has entries here so this is forcing it of course that has to be very sparse if they're n data points are at most n entries in it and these these are these on this diagonal or or this this often called the degree matrix it says for each entry this is birth so this is a a right so I can I have a self edge maybe in the JPC matrix you could you know you could say something like there's a self illusion but in general what Scalia's is the degree of every vertex so it's how many edges does that vertex have so a has edges with b c and d so its values 3b as I just with just ad so the value is 2 and so on ok so again very simple you can represent a matrix but you can think of it much you could probably want to internally represent a different format and so then what we want to construct is some matrix called the laplacian matrix okay there are a couple of versions this is the underlies will talk about the normalized as well a little bit the laplacian matrix is just d minus eight right so it's basically I'd start by filled in the diagram of this with with every with all the entries of the diagonal matrix of the degree matrix and then I filled in the adjacency matrix by just subtracting by just putting a negative sign on each of these so this is the laplacian streams so it's a little you know i personally go have a great intuition of what this is well yeah it's actually did there's some cool intuition of what this is as meaning here one way you could think about this is that even think of the D matrix as the amount of flow like you think of there's water flowing around this system and there's these amount of flow water flowing into into the vertex it has so many other vertices and so each of them is sending some water or something into it and then the a matrix is saying where's that water going there's some sort of some sort of equilibrium here we'll talk more about various sources equilibrium with Markov chains later in class but in this case you see that the sum of every one of these columns is going to be exactly 20 right the degree is going to be exactly the distro of minus ones and you could model this differently you could say instead that there are two ages from A to B and then you have a minus two you could do that and then the degree would also increase by good you can mile things slightly differently as well ok sleep so now we have this matrix and now if you kind of look at this went squint devil of it before we actually say how this is going to work because I've represented the vertices in in alphabetical order turned out to be in the right order you're going to kind of see these these big blocks of zeros or the melee zeroes you tonight so I want to say look there's not much stuff being connected in here and in there so I kind of want to break it kind of across this way this kind of makes sense and so the ordering of the vertices didn't really matter so let's say I can be ordered them inside so I somehow one to find a way to order the vertices in this case may be alphabetical is a good way to do it but by the head of time that's not something we know so I'll reorder the vertices so I can cut them somehow so on one side is is one set once as the other set ok so this is kind of an intuition again this seems like a hard comment or Obama I could keep checking all these reorder rings and so forth um but it turns out this matrix representation to be very useful because we can do we can calculate what's called the eigen values and eigen vectors of this matrix all right so so so so who is who's worked with eigenvalues eigenvectors before and for who is this unfamiliar concept ok so it's well briefly just briefly mentioned as you should have taken these in a linear algebra class if you have a matrix M then in can I get vector of this matrix is is a V such that so if your exists a scalar so this is a is a matrix and this is a scalar so if i multiply the matrix by this vector I'm essentially getting the vector back up to some scale which should seem this so this matrix needs to be a square matrix right that's so you know for the store so I can map the same vector this means there's and so the eigen is the german term means something like self not sure if her different translations that's like you're getting back the vector when you multiply it by the matrix so this is that what a high in vector is and turns out that the number of distinct these are very special vectors and the number of the stink bugs is up to the rank of the matrix is the same rain and so I've listed them here and so this value lambda is the eigenvalue that and so for this enormous laplacian and this is on the graph here I've written out all the eigen vectors as columns so this is the second eigenvector and we'll call this v2 and this is the second eigenvalue cos lambda 2 and so this the first one is going to be for the unnormalized pasha the first ones can be really boring it's always going to be 1 over square root for each entry and the eigenvalue is going to be 0 and this this one we just ignore and there with it and so if you want to actually calculate this for the purpose of this class it's just going to give us you can just you can call in in matlab so for a matrix L you call this function e I G and returns we can return a matrix V where the columns are shown here and in the end this and the eigenvalues now because if it's a vector or fits to stay I think that's going to be a diagonal matrix with the eigenvalues video so you can just call us at matlab this is a very well optimized thing so we just need to call us and we get back this matrix ok we're going to get back this game design factors these eigenvalues ok and so now can turn out that then it's going to you should give them them to you in sorted order so the smaller item values comparisons right so 0 1 you ignore the second one is going to be interesting this the second one is called the problems of the Fiedler vector of a graph and so this is this is a really kind of it comes up in many contexts really want to do this hey ok so this beaver vector look at this this this has encoded in it everything like it needs no to do the split it turns out what if you look at the numbers in here what how do they correspond to always get the data into s and s prime and we decided was the best the best health yeah so the the ones which are negative our tests and the ones which are positive aren't as prime right hands and so actually the sign of all these if you x by x minus one it doesn't really change much information so so the fact that one's thing in the others positive doesn't mean much but the negative ones are one side of the cut and the positive ones are on the other in fact it's kind of more interested in that if I just look at so I'm I'm just looking at V V 2 at the three-year so the first two eigenvectors it's going to happen is that if I just look at V to NIH wall and use those as the heck's poignant and if i look at e3 they use those as the whiteboard it but just look at me to just going to export it then they were minus signs mayest points for 24 b minus point 364 ad minus point to foresee right and then plus one point our point of seven and so forth for e.g nfh that tells me like how to draw them on the line right this first eigenvector says what is basically the best way to draw these these vertices on a single mind so then if i want to find a cut I just can look at them in this order and so often you can just say I'm going to split on zero so I want to split here on you know it might be useful to split here on zero in some cases what's going to happen is there's gonna be a big gap and it's going to be not exactly at zero you may get something where you draw them and you get so this is the zero point you get a bunch of things that look like this and then you can other ones out here and so then you want to split here where this was zero so so what you want to do is take this one dimension and then there's a lot of in this one dimension and then and then look for good splits here there's a big gap and if you're too lazy to do that you can just split up sir okay that's usually a good first thing but sometimes you want to actually split up something other than search okay and then if I look at this the third eigenvector to then i can draw based on that and then I get a two-dimensional picture right I'm going to get this this two-dimensional picture and so this allows me to do more after that right so let's let's say instead of rerunning this this eigen vector decomposition because it's it's you know it's it's it takes a lot longer than then leading just the data to compute it so maybe I want to reuse some of that so I can do something like well I'm going to look at it in the ones at each of the quadrants so these are the same quadrant these are going to be break into into four clusters this one I can look at it this flips based on on the first two eigenvectors based on the the positive minus the positive negative or I can look for other ways to do it so if I was forced to go into four clusters this is a seemed like a pretty reasonable thing to do one on one of H off by itself if I had to split this side C is kind of connected to e in some way so it's much less kind of tied to that other other part of the sub graph that's okay so let's let's keep looking at this there's some other cool properties involved here so why is what happened with a and B here let's look at at these deaths and dr right on top of each other what's going on is this just kind of a weird weird phenomena that happened on this data set is their kind of why would they go right a top each other is it because they share all their neighbors that they their neighbors with each other oh yes oh yes oh they have the same seven neighbors except except for each other right so if you look from a crab perspective they basically look exactly the same right there's no difference between handy from the perspective of the graph so I haven't really these eigen vectors have not really been able to distinguish them but that makes sense all information about the graph doesn't say anything different about them so so so why should it give it give them value which are different I look back at all of these these uh these eigen values so there's a nds for the first and the fourth roam here they eventually become different here in the last one and this is kind of essentially arbitrary one got negative 1 over square root to the other guy positive normal screen Oh so yes it so these really didn't get separated um what what else happened in this drawing here so one way I drew this that this is not the only way to draw it here you might also instead do the coordinate of the assistant so these coordinates did not use the eigen valuable it just use the eigenvector but these are I think value kind of should tell me how important each of these vectors these eigenvectors are the smaller the vent right the intermission i want to say so so so a better way doing this likely to have the coordinates of a so i went to an embedding of a into into our to a better way of doing this maybe so this is v1 of a times 1 over R this is V 2 lambda 2 as an x coordinate and b 33 of a of 1 over 93 is the white boards so so basically if the eigen value is really small I want to d'accord it to be bigger and this will stretch out the ground I don't think I have picture so I so I don't think I'm a picture of this one but but if you're going to call them this tells you a better way of drawing to kind of it says that what happen is x axis would be much more spread that the widest because that the first eigenvector tells you much or the second eigenvector tells you much more than the third row and this is roughly the direct way to do by multiplying the court is by 1 over the height ok so but this isn't weird embedded into into into the two dimensions here or I could do I could use I in vectors 2 3 and 4 go into into three dimensions should give you another idea about maybe a different way to do clustering islands what else does the structure tell me distance the distance between the point yeah so now this distance makes sense this actually using that you could in distance between the points if you think a reasonable thing to do so often what people will do is they will invent it into two or three dimensions and then the one is the version of the key means clustering comment after they've invented it so if they want safe you want five clusters it's not as obvious how to do it maybe this is splitting it two quadrants kind of is not the best ways you could embed it into two or three or four dimensions k-means clustering doesn't really poised on it doesn't really care and then you can run mode zagreb on top of this and do you provide some clusters that way as well so now once they do this in getting now I can use these as assignment based on rumors right so this gives me a way of kind of taking the graph and putting it into a low-dimensional into a low dimensional Euclidean space ok yeah I've got some picture in the notes where I do that based on where i use the eigen values okay so this this unknown Wisel washing is its kind of it looks it looks a little bit nicer it has the property that each of these vectors is it is going to be a is is going to be a unit vector and kind of a kind of looks nice it's easy it's kind of easier you draw pictures it kind of makes more sense but when you want to minimize the normalized cut turns out you want to use the little washing matrix instead uses defined slightly different so this is the condom this is the normalizer wash or lock would you call just laplacian matrix the in the literature you'll see various variants of how these are called whether the posture enormous collage and so instead of using the green matrix it uses the degree matrix to the power minus one-half and so basically if you have the matrix which is a diagonal and you take it to some power that's equivalent to taking each entry on the diagonals that copper so I went from so so free to the minus one-half is going to be 0.5 77 that's right and tues and minus one-half is is 1 over square root 2 is 0.707 so that's what its interests and so then the laplacian matrix is as written like this or the normalization is the identity minus d divided about half a times keith of nights when this is equivalent also raining d minus the unnormalized a passion to the needs of the bus you can also write it this way and then indeed if you multiply the d mites what have you can kind of remember this one was equal to d minus a that's if you do these deeds of the minus one-half times d times d minus one-half that becomes the identity matrix ok so they're just like nom matrix algebra that works out the same and then you get this this other term right so this is me the regular laplacian matrix the normalized one and you know the entrance look similar to before but they don't have quite the same nice you know doesn't the same property where the sum of these columns adds up to as a heads up 20 for every call you don't have that property what you did kind of do is you took what this effectively does is it tooks this entry which before was a it was a minus 1 and it basically is dividing through by the square root of the degree of me as the degree minus one divided by square root of the degree of being done to the degree of deep alright so i looked at the degree of the two vertices that its corresponding to so this was this has to do with this edge between the let's see here that's what I'm looking at and B has degree 2 and D has has a has has a degree 3 so I'm normalizing by square root of the product feelings turns out so you kind of want to depend on both of the vertices so I'm kind of normalizing them by the degree of both vertex is what is kind of going up and it turns out that the if you want to minimize the normalized cut then using this laplacian and then again taking the eigenvectors and eigenvalues this one is more preserving the right property we're not going to kind of go wyd excuse but so this normalized a flash it corresponds to normal and it's not that much more difficult to work with you've just you've scaled each of these these these things with majid c matrix by the by the squares of the top of their degrees so it's still just as far as before and everything like that right so now I want to take the eigenvectors of this matrix elements of l0 and we're going to get this instead here and it's going to look fairly similar the first one you still want to ignore this one you kind of say just ignore this off and then the second one right here where this is lambda 2 v2 is is really what's telling you the best information of how to split okay and so if I draw a picture of this and yeah so in this case I have scaled by one over the skill by 1 over lambda in each of the coordinates so if you notice the ones here the the y-axis which used v3 got / 0.72 so it's going to be so that it's going to completely they're going to kind of be close to between minus 1 and 1 well in that range with these r / 0.125 which is basically x hit right let the values to maximum value i got was like 23 438 so i got up here this is like the minus for something that's five on its course okay so now if I look at this version I can see that this first item for the second eigenvector is really separated these these are four things from these boys so now this is s right so yes I flipped the signs that signs really don't matter to the chick don't at all I put the signs on them that doesn't matter but now I see much more operation in between tweet assets so it's much should be much when this did not do quite as much pulling each away from Erik Jones it's kind of said well it's pretty far away from everything but it has like one edge so I don't want to really push that too far away it's gonna you really don't want to cut this one off because its volume if I have said just here it is it's not going to be as large so that normalize cut score cutting this is back against okay so i'm not going to pull this one so far away from the data as i did with the unnormalized approach so if i look back at the hunter lines of caution here right so a chapel really far away forever beans but I didn't really it's not a great thing to cut it doesn't tell me much to come to chop off each that's when you have a lot more versions this is one one of the skin work back I you're going to get use this will clash of interests system okay yep why is the distance from E to C so much longer than yeah they put that one edge but there's more to the cut going on right he is connected to G and H which are not these are not connected to anything on the unset s before right where where C and D are connected and c is connected to a and D and these are connected to each other there's much stronger complot going on here and this song captures all those properties let's say really these are you know these are really tightly about a D and B are really close to each other because there is so it's a click they're all infected together and the only other thing they're connected to a C which both main defect right and c is pulled away because it's connected to e but not so much it's doing its really concentrating really putting in a similar geometric location things that are very welcome you know it's the intuition i can give you there's there's some there may be a different mathematical way to giving intuition explain but this seems like actually want you don't is you really don't want to separate a B&B from each other in different classrooms there there's connected as they can right so these are really close together so if you did k-means clustering on top of this these are really going to be one cluster in a really tight connected where's you know here this seemed like a good customer it's not as tightly connected EG fh there's there's a click there again but what they connect to is different this one connects to h which is pulling the f1 off in this direction and he is connected to see which is one that one off so there's kind of not quite as type of structure yeah so we're gonna like look it's like widest shakes out yeah so you know the so we'll talk about like the SPD and eigen ket vectors a little bit later when we get to some parts in better in the regression part of class you get some more intuition there but you know going beyond what I've told you in that this eigen these eigenvectors the second i think are in particular is the best way of drawing a straight line it's kind of what I'm whatever's as much as else by the same in this lecture kind of diving to why it is requires much more machinery that we don't have so the PCA SVD lecture will will look more into kind of what's going on very populated matrix decomposition okay so another important thing to mention is that I started with my input here being being a chef I said that my input was something like this this graph here I have these mercies and edges in this game you know this gave rise to a matrix right where I have this this adjacency in this degree matrix if I'm look at this adjacency matrix it's these are typically between music valley between zero and one third 0 for 1 right but what else did we look at the head values between 0 for 1 so so we talked about these when we talked about similarities right so if i define us similarity like the Jaccard similarity between objects paid and B this is going to map to a value between 0 and right so if instead of saying it had to be 0 or 1 I can say well most let's map it to a value between 0 like like the Jaccard similarity to these objects if I do this and then I can still drink the adjacency matrix is exactly to find a way and in the degree or this degree matrix of the diet I can just sum up the columns I could sum up a column and that becomes what the value is here on the diagonal I can use this instead this is typically sometimes called if i define it this way this is affinity matrix or if i use what's called if i define a similarity as a as a curl then then this guy becomes um what was all the grand matrix so so colonel um the what should I say both so good right thing about kernels so let's sync the data points if they line in in Euclidean space i can define the Gaussian kernel if these are vectors you can do something like eats the minus a minus B okay so can do something like eat to the minus k must be squared and so this defines gives you a value between 0 1 and this is a common friend spatial Colonel there's a way to do a polynomial kernel instead and for certain class of these kernels this this this grant matrix tells you this similarities the same way and you can use this instead of instead of using this as a graph and then you can use spectral clustering to based on just just case with a similarity score so that's how many people take it the machine learning class did you guys do some sub techniques based on curls so you talked about something so you can replace some like a like a regular dot product with the colonel a lot of the techniques use their go through and so this is similar cute where i can replace an inner product with a similarity notion of the colonel and i can put this into a matrix and I can do spectral clustering based on palm and this gives me a nonlinear view of the data once I only care about things which are so for instance with the Gaussian kernel if this is in one dimension it looks like like this where a is here then the value of the kernel for a value B is here so it drops off after a certain value close very close to zero work as close it has a larger and you can adjust this based on some some Android print for instance and that tells you how it is how spread out this Carlos and so this this is a nonlinear way of saying the things which are close to each other I really want to make sure they're not in the same the same cluster things that are further away well those those items cares so much about their their far they're not similar so if it's far it's twice as far the value the similarity is still roughly zero and so sometimes when people do with to make this faster they'll do this and then we round down some of the entries 20 and you get again get a fairly sparse matrix that's it so this using these notions of similarity or kernels you can extend this spectral clustering to use these these odd these right ok that was all I had planned to cover today any any other questions about this this spectral clustering or or other clustering in general there are types of custom we didn't cover in class maybe you said oh why not what happened to this type how does that fit in ok so the clustering homework will be posted online hopefully this afternoon or this evening and then next Monday we'll start on the streaming part of class with a story a week on this and then we'll we'll do more stuff with with regression inside the regression we'll see some more streaming techniques this is a way for doing data mining where your keep getting more data all the time and you want to maintain some sort of structure on the data and so it's useful for very large scale kids and the sorts of things will be able to work whether to be much simpler than before we're not good at their ways to do clustering in this but then a quite satisfying as things are just keeping simple simple statistics on the day now goes again will be very very cute okay great song on i'll post the cluster in houma to 
vrX7cM1FC_A,27,"Peter Leonard and Lindsay King of Yale University discuss reasons for current interest in TDM, what makes a good project, and implications for libraries. They also demonstrate Yale’s Robots Reading Vogue platform, showing projects based on the ProQuest database.",2015-08-05T14:52:35Z,Text and Data Mining in the Humanities and Social Sciences—Strategies and Tools,https://i.ytimg.com/vi/vrX7cM1FC_A/hqdefault.jpg,CRLdotEDU,PT1H20M54S,false,2389,13,1,0,1,"good afternoon welcome to the Centre for research libraries webinar on text and data mining in the humanities and Social Sciences strategies and tools I'm Bernie Riley and I'll be moderating today's program we have a couple of housekeeping details we since we had about 250 registrants for this webinar we are we have muted all your phones so that we don't have too much feedback and distortion during the course the webinar you should feel free though to post questions to us through the chat box that those questions would be exposed only to the presenters and to us here at crl rather than to everyone on the who's it to in attendance there will be we'll address these questions during the question and answer periods during the fourth of course of the webinar we are recording the and archiving the presentations today and those will be available to you after the fact on the CRL website we see or else interest in text and data mining is it comes from a stems from a number of a number of things number one cero has always been a organization committed to preservation and accessibility of primary source collections the we've supported since founding in 1949 cero has supported original research in humanities and social sciences in more recent years we have begun the negotiate the negotiation of licensing terms for the acquisition or the subscription of our members to large databases including the ProQuest some of the ProQuest databases like the one that's going to be featured in the presentation today we have also cyril also provides resources for licensing to help inform member institution decisions about investment in digital resources and digital large databases those those resources are in the form of the each is adhirata platform which is a platform for reviews and valuation of major databases and also the the live license model license and live license software which was recently updated by a know person and a team of licensing specialists in under the auspices of crl when with with help from the with support from the Andrew W mellon Foundation so CR elves interest in text and data mining again arises from its interested primary source collections its activity and licensing of databases and its its work in the providing resources for for local licensing and subscription to the large databases we've had a number of webinars on text and data mining in the past crl has and that I guess falls on to the rubrics of rubric of support for licensing an investment in electronic databases to early text and data mining for libraries webinars we had were in july 17 19 2013 and February 19 2014 those were organized by a no person and crl the first one had representatives from Elsevier SDM crossref and that the presentations from that and the audio from that are archived on the web under crl events and the february 19 2014 webinar included Robert Scott from Columbia University we've more recently gotten into the planning and presenting of webinars on a big big data we had two webinars on mining big economic data one in October 2014 which feature the Robin view and crisp Pierce from The Economist Intelligence Unit and a webinar on February 18th 2015 featuring Bob berry Bordelon from Princeton University on economic and financial data so there's a fair amount of information on those archived website or those archived webinars that that can give you background for TDM today is a different matter today we have two presenters our focus is going to be on strategies and tools for text and data mining in the humanities and Social Sciences we have two presenters who are going to bare the shoulder the burden of doing this presenting Lindsey King from Yale University Lindsey is the interim assistant director for public services and the Robert B Haas family Arts Library at Yale she was responsible for collection development reference instruction and outreach supporting students and faculty in a number of fields including studio art history of art and architecture theater studies and dance before coming to Yale she worked in the art collection at Northwestern University library and a museum education at the Art Institute of Chicago Peter Leonard many of you probably heard Peters speak already on text of data mining in various venues he is librarian for digital humanities search and director of the D H lab at Yale University library where he helps scholar to entry humanistic questions with quantitative and algorithmic methods prior to coming to Yale he had responsibility for humanities research computing at the University of Chicago he previously served as post doctoral researcher at UCLA on a Google digital humanities grant to text mine literature in the Google Books corpus though both of our presenters were formerly from Chicago and it seemed to indicate a brain drain moving from Chicago to the East Coast but we won't worry too much about that today we never begrudge the I believe anything so I will now turn the presentation over to Peter and Enzi thanks very much Bernie hi everyone and thanks for joining us this afternoon just to give you a sense of what we're going to cover today you've got an overview in front of you so we're going to begin with an overview of text and data mining as a technique and a trend to talk about the history of text and data mining as the research approach and then the trends that are converging on our current moment why it's having such a big impact right now and then we'll move on to making sense of the data giving you some suggestions for what makes a good TDM project that's meaningful for your users in particular and we'll talk about what this means for library how do we obtain data for TDM projects and how do we provide researchers with access to that data so then we will take a pause for questions and so you can enter those into the chat and Bernie will moderate that part of the presentation and then we will then we'll move to a demonstration so after we talk about some concerns and concepts on the history we'll demonstrate a project that puts all these ideas into practice which we call robust reading folk okay so we can go to the next slide I will hand things over to Peter thanks very much Lindsay and thanks to everybody who's attending today thanks also to Virginia and Bernie and the rest of the team at the TRL so what we're going to do today is we're going to talk a little bit start out the webinar we're talking about text and data mining is a trend trying to understand why all of us 2015 are in this webinar why this is a subject of increased interest especially in the social sciences especially in the humanities we're going to do that we're going to talk about first some of the historic predecessor possible current allies of TDM we're going to talk about some of the reasons for the increased interest in the last couple years especially in the Academy in the academic library world and so finally in the first section we're going to talk about two major approaches two big ways of speaking about text and data mining into social sciences and humanities so with that we'll move on to the next line great so I mean of course always important to remember that everything that we do today it has its connections in the past especially in the Western Addition it's impossible to think about text mining without mentioning the monastic tradition of biblical concordance as early as 12:30 people amongst in monasteries were writing down the number of times the word has occurred in really important religious texts of the Bible and other texts circulated in the Western Christian tradition and that Christian monastic tradition of creating these textual concordance 'as we can think of that is mainly one of the first attempts in western intellectual history to do text mining although it was done by hand without the aid of computation and that monastic tradition I think has a connection to one of the sort of people who were thought of as founders of modern digital humanities a name that may be familiar to some of you that Jesuit priest Father roberto busa who after the second world war in the 1950s and 60s began to input the works of Thomas Aquinas into early IBM mainframes he did some of that work at Yale in fact so thinking about that that continuity going back to full counting words by hand to doing the work with mainframes after the Second World War we could also think about other times other disciplines that have undergone transitions that have been mediated by technology one of my favorite examples is the shift in astronomy with the invention of radio telescopes in the 1930s so we think about the discipline of astronomy this was a field that for hundreds and hundreds of years pursued more and more precisely polished pieces of glass to let people make more and more precise observations with their eyes about the heavens and all celestial body but with the introduction of radio telescopes it'll play near by Germans in the 1930s suddenly astronomers found themselves with a signal processing problem because you can't easily look at the results of a radio telescope you can hook the radio telescope to a speaker now do you'll get static and get white noise suddenly you have the signal processing problem of figuring out what's the frequency of the data I should be looking at this is another example in the past of a different disciplines outside social sciences and humanities but it had to come to terms with new equipment new techniques to make sense of a lot of data and then finally moving to the present day in 2015 I always say to folks in the humanities here at Yale but it's important we always try to think about other people on campus who are doing allied work now there's some obvious there's some obvious departments doing really important data mining work you could find people interested in this in applied math and statistics in computer science in other fields but the other thing I think it's important for us not to forget is that many people in linguistics departments have been doing text mining for decades especially in sub fields such as corpus linguistics or computational linguistics so it is important to keep in mind both the sort of history of other fields counting data counting words throughout time as well as who might be on some of our campuses who might be able to help us out if people in the humanities in the Social Sciences pursue texts as data mining with new bigger so that we'll move to the next line and this is where we move away from the Harrison from history and start talking about why are we all on this webinar right now what makes 2015 this moment where folks in the library world or folks in the humanities folks those responses are increasingly interested in text and data mining I'd like to talk about three main points here the first reason for the current interest is of course the sheer volume of digital source material it's now available online I'm sure there's everybody on this webinar who's heard of Google Books and many of us probably have heard of hobby trust the organization that manages the materials scanned out of research libraries that went into Google Books many as I've heard of JSTOR and we can also think of routes like Earth Store and other commercial vendors that manage discipline-specific digital material but the second point is our access to this sheer amount of digital materials is increasingly being opened up for programmatic access through application programming interfaces or api's which is just a fancy way of saying that they're increasingly sophisticated ways of constructing queries to get specific bits of the data that's in places like hoti truster in jstor specifically for the purposes of text and data mining so many us have heard of haughty trust probably fewer less may have heard of HDR C or the Hadi Trust Research Center which is working within the money trust to enable programmatic access to data in Google Books corpus there's a similar group within JSTOR the JSTOR DFR program stands for data for research so using JSTOR DFR I would be able to get maybe term frequency counts for all Journal articles about German studies in the 1970s and of course there are an endless number of vendor-specific text mining API Elsevier has been in the news in the last year or so about them ramping up a TDM API for their particular proprietary collection information the final point I want to mention after the sheer amount of additional material and after the more sophisticated ways of programmatically getting out that digital material is the increase in the sophistication of the techniques we have in order to make sense of this large amount of material and I think the first point in this section would be to say that what many people may remember from the 1990s as being termed artificial intelligence well the term is shifted towards machine learning some of the goals are the same so the algorithms have changed but there's incredible sophistication in the machine learning world of being able to make sense of data that isn't very well catalogs that we might call unlabeled text or unlabeled data and some of this algorithmic sophistication is being made possible by the computational power that now have available to us that maybe in the night is only available to users of supercomputers nowadays with the Moore's Law of a doubling of the amount of transistors you can fit on a CPU every 18 months with a relatively low Ram prices were able to tackle really complex sophisticated problems that would have taken a lot of expensive equipment even 15 or 20 years ago a lot of this is connected with sort of real-world implementation of Bayesian statistics Bayesian inference the ability to sort of choose through full data sets rather than doing sampling which we were forced to do in the past so those are some of the reasons I think that the the current moment represents a sort of turning point for interest in text and data mining amongst humanities and social science researchers and with that we'll move on to the next slide which is the final section of this first introductory part where I want to talk about two really high-level like 10,000 foot level approaches of thinking about text and data mining this is my own this is my own organizational schema these are not scientific terms but I like to think about text and data mining is falling into two categories one of these is looking for something you think is there and the second one is letting data organize itself now the first of these is actually pretty congruent with standard practice to at least on the humanities I'm a literature person myself um the notion of looking for something you think of it is there is very similar to reading Hamlet and looking for a moments where the main character is unsure about what to do we think we know the text so we're looking for words like honor or certainty or things like that but the second of these letting data organize itself it seemed pretty heretical to disciplines like the humanities or literature where we don't really believe that data has agency and yet what we're going to show today in both the theoretical level as well as the real-world level I'm going to try to show some examples of how looking for sort of implicit structure in large data sets can sometimes be a useful tool in making sentiment Meishan so in the example of looking for something you think is there let's move on to the next slide many of you will recognize the Google Books Engram tool which was released around 2010 as a joint venture by Google and by the Harvard Cultural Observatory in fact there's arguably no better poster child for the digital humanities in the public imagination and the Google Books and Graham tool so it was covered in the New York Times a lot of people instantly grok what's going on when you show them that some of the same researchers who helped Google build the Google Books and Graham tool later built a different tool called bookworm and the best way to think about bookworm is that it's like a bring your own books version of Google Books so you bring the books or the texts that are important to you you bring your own data and use the three open-source tools what you should look for patterns in words that you think will be meaningful we're going to show you some examples of how we put this to work on Vogue later on let's proceed to the next slide and this is exerted a slide that shows a couple different aspects of a technique that I think can fairly be filed under the letting data organize itself rubric and this particular technique which is one of many I could have selected is called topic modeling the topic modeling is attracted a lot of attention in the humanities recently the two images I put on this slide are a neh National Endowment for the Humanities workshop on topic modeling for the Humanities that's on the left and then hiding behind that is of a special issue of the journal poetic called topic models in the cultural sciences which came out a couple years ago which covers I think both the humanities and the social sciences so topic modeling is a really powerful technique for making sense of unlabeled texts of books that you have in a chance to read yet it's very mathematically complex and it's beyond scope for me to try to explain it right here but I think we'll see some examples later on going through vogue it make it clear that it has have some applicability when you have a corpus of text that you'd like to look for latent structure within in other words that you'd like to let have the text to organize themselves for you the next slide will show you one example of math Ronna though corpus but I'm going to let Lindsay talk a little bit more about topic modeling in vogue and what it allowed us to expose in that corpus later on in the webinar so with that I'm going to switch over to Lindsay who's going to talk to us about making sense of data okay so we'll move on oh here we are on the next slide so the next thing we're going to talk about is how to get into the excessive data how do you approach a TVN project what makes a good project and how do you kind of assess what you have available what you might do with that so to move on to the next slide on a practical level what makes a good TDM project and how do you determine where to start so what you might do is start looking around at your local data sets things that you have in your library it might be for example a campus newspaper or in archival collection there's something that you have physically in your library you maybe even if you're lucky you have the copyright to that material and it's locally meaningful so for example Museo news for us we are we're beginning to do some TV and work on that big extent because it has already been digitized and we do have to copyright and the other thing that's good about having a locally meaningful data set is that you have the local experts who can interpret that data so you have lots of buy-in because it's important to the people you have around and you also have people who know a lot about that set of data though some examples of that might be within an archival collection you could have letters and postcards that could turn into a correspondence network so that's a really interesting TDM project you also have Texas newspapers as I mentioned so that could become an Engram data set because it is very well marked with time so you could look at changes in your campus culture over time but the newspaper searching for different words and with the strategy of looking for things that you think might be in there and then another option is to purchase relevant data that is important to your specific researchers so your students and faculty so and we'll talk about this a little bit more later you do need to focus your resources you can't possibly buy every available bit of data we have to think about what's relevant to the researchers on campus and this is an example of just-in-time acquisition rather than just in case so you can really focus what you have okay so I will hand it back over here we'll go to the next slide Thanks so in this section we're going to talk a little bit about what information you might have either and you already have in your libraries or in your various institutions or materiality you might be considering doing your own digitization of or possibly licensing or purchase and data from a vendor one of the things I want to make clear on this slide is at least in my own experience also there are so many techniques out there that you shouldn't despair if you only have a partial aspect of your data there are lots of text and data mining approaches out there that will work just fine on metadata alone on the other hand there's some approaches that work really well with raw data and don't care about metadata don't care about cataloging information or subject headings or anything like that of course many of the text and data mining approaches operate at the intersection of both data and metadata and one point we're going to make here is just also that more is always better so a text and data mining on three poems written by a poet who then died it's not a good idea but text and data mining written on 3,000 poems over the course of a decade might wield some really interesting solace so what we're going to do first of all is talk a little bit about what approaches are perfectly capable of working with metadata alone so this might be a case where you have a collection of information that hasn't been digitized the full text that haven't been digitized but you have bibliographic records there you have archival records you all sorts of information they is about the data itself and one example of this is actually the Republic of Letters project which has been undertaken at many different campuses there's part son done at Stanford part spent in other places that really was a course in epistolary Network a correspondence network in which much of the research was done purely on who sent what letters to whom at what date and from what city to to what other city it's not necessary analyze the content of the letters as long as you have who sent which letter to whom at scale you can make some observations about the characteristics of that network through time and space even if you don't have these handwritten letters transcribed another example of a project that works only on metadata alone will find on the next slide if we can go to the next slide and this is not really text and data mining it's more of a dashboard a way of exploring metadata that I built around 3000 photographs taken in California in the 1930s as part of the Farm Security Administration what you're looking at here is you know screenshot of an interactive website that is not really about the pixels of the photographs what it's about is only the metadata so you're seeing a map of California at the county level a chloroplast visualization that shows where the pictures were taken you're seeing a timeline of the top right about when they were taken who took them and what they were about on the website this lets you sort of choose each facet and make selections in each of these and sort of play around with the data to see where the correspondences are so that's something we did without having to do something really complex like signal processing and analyzing actual photos is purely an explanation of the metadata itself if you go into the next slide we can talk about there are many projects that work with poorly cataloged collections or collections in which you don't really have very good metadata so one example I've already mentioned here is topic modeling which is an algorithmic approach that relies on term co-occurrence to surface latent discourses or latent topics within unlabeled text and maybe within the library well we would say with an uncut alog text the text about which we don't know very much the great technique for projects that you have that involved material that we might describe as great unread not the Canon but the other 99.5% of literature that isn't very well studied another example which there's some interest in nowadays with more and more big textual collections is a technique that comes to us from computational genomics health sequence alignment there were some amazing work on this done at University of Chicago a couple years ago on the French textual database basically it allowed them to discover quotation citation plagiarism and illusion in 19th century French novels and that's the technique that I won't go into in this webinar but it's a great example of not needing to know who the authors were or how important the books were to be able to find overlapping patterns of textual reuse if we move on to the next slide I want to talk very quickly about some of the fine print some of the real technical details that I spend a lot of my time worrying about when I get my hands dirty on a text and data mining project this may not be very secondes but it's actually very important for getting good results intact to data mining so the question I always try to ask folks who are working in the lab is who have interest in a text a data mining project is how machine actionable are your texts and there's a spectrum here so we have a wonderful Rare Book and Manuscript library at Yale a lot of what they hold are manuscripts they're handwritten it's very difficult to do TDM on handwritten documents they would need to be transcribed in most cases if you have a typewritten or a printed archive then you're pretty much ready to go modern OCR is extraordinarily good at creating really good quality text out of typewritten or printed books and the other thing that sort of come up recently is I've been thinking about if we have material that was as yard that is to say transform the optical character recognition into text but it was done a long time ago I'm actually considering redoing some of that using modern algorithms irrespective of whether we did it internally at Yale the ar-15 years ago or whether it came with some a vendor and the reasons for that are a couple we go to the next slide modern NCR actually uses language specific heuristics so for example to try some rec if you tell if this text is in German it's going to figure out that a Q and a Z are statistically unlikely to follow each other in German so it's probably a mistake they'll try again to get the word right another example that pops up a lot when we work with newspapers is when especially when words they're in release of textures and release in columns reconnecting words that were split at the column break can be very important to get better quality results and in books if you have you know a running header or a footer that includes like the name of a chapter that can sometimes skew your results in some text and data mining approaches you can imagine with a tool like the Engram tool if the title of the book is on every single page of the book that's going to skew the results if you're trying to look at the frequency of a particular word over time now these are showstoppers but I think it's always good to think about how Machine actionable are your text how cual how high-quality were the prophecies that were used to transform the text in the machine actionable objects I should also mention here that there's really important work that's have done in the last five or six years not only unfrocked or or black-letter texts most often used in German but also on the 18th century OCR project by Laura Mandel has resulted I works will soon result in even higher quality texts from the 18th century so going back to some of those early texts and rerunning with your modern FCR because sometimes really pay a lot of dividends if we move on to the next slide we're going to jump to part three and in this section Lindsay on both are going to try to tease out what some of the things we've said above or before in this webinar might possibly mean for libraries so we've talked about what type of data you might want to use is it good is it a local interest or is that a specific research or interest that you're trying to meet and then of course they'll never tably come a point where all of us who are trying to set up this access your researchers come to terms with either a vendor or another obstacle that we have to deal with in order to pull off a TDM project now I would say the landscape of data access especially in the vendor space right now is a spectrum at one end and we certainly have had projects that end at this point is we just can't get the data from the vendor it's possible that the vendor has no ability to give us the data doesn't have a legal right doesn't have the technical ability that still happens in 2015 as always very frustrating there's one level above that in terms of access which is that we're starting to see some really basic TDM tools integrate themselves in the vendor platform so when I say a hosted platform I mean the interface the scale or ProQuest or Etica puts on top of their data so if any of you have access to the National Geographic archive that has like a very basic Ngram tool built into that and I'm really always really excited to see those tools be part of the basic browsing experience because it means that it's just it's there it's for free and they maintain it and they debug it the problem is of course that you don't really have control if you say I want to be able to treat uppercase words separately the lowercase words it's rare that you're able to go in and make those types of detail changes one level beyond a tool that's part of the vendor platform is of course an application programming interface or an API where you can ask questions of a large corpus you could say give me all books in Google books that were written in the 1890s in Italian and from a particular city so that's an example of being able to download specific information from either JSTOR or hoti trust or other vendor api's in the future and that's that's pretty much the best you can get except for the last item of this bullet point list which is full access to the raw data this can either be done by downloading the information I want to make it clear that this is not this great thing this is like actually getting the information from the vendor over the Internet in a kind of packaged form or in some cases we get hard drives through the mail from various vendors which contain local copies essentially replicas of the information that's already available online as part of the vendors package there was a great presentation given at CNI in 22 by um Molly Herndon I think Molly Susan Molly tamarkan and another fellow from Dukes but when we do it all these hard drives and they were one of the first people sort of talked about what is it like to use these than their hard drives to text a data mining well you're going to see later on is a project that was dependent on the full data of the full raw data if I should dependent on six terabytes of information from Pogue there are different levels of potential and effort in each of these and there's kind of it's a sort of linearly scales where the more you can do with something by having the raw data it's also gonna take a lot more programming effort so that's that's one thing to keep in mind and without all switch over to Lindsey okay we can go to the next slide now expanding a little bit on what participants saying about what we do with full data and what that means I wanted to go into a little more detail about the way that we thought started on the vogue project so initially Yael did purchase a professional access license for Vogue for the ProQuest Vogue archive and in this case that was the key she was getting the data for text and data mining so what that means is that we are licensed so that we would have this in perpetuity and typically that also means that we can get one of those local copies on hard drive so in this case it was many hard drives um six terabytes of data as Peter mentioned but it changed on us having that perpetual access relation so I think that's an important concept for us to mention at the outset for this particular project because having that professional access license really bolsters our arguments for why we should have potato-y we're entitled to have the data and that is in our licensing agreements with them so of course that is not always affordable it's not always available not every product is available with the professional access license and they are typically in the five figures when they are available so we're not imagining that this is possible for everyone but that is the way that we have come to that having the full data for folks so what we advise people is to think about text and data mining at the points of purchase and when you are working at those licensing agreements so that you can ensure that you have the access to the data if that's what you and your researchers want so there are some different models that it's a little bit early days for all of this so right now often there are additional fees for text and data mining for getting the raw data files that's not always the case but in some cases that the information is very valuable or very protected by in multiple layers of licensing and copyright then you will be asked to pay an additional fee to get those hard drives of data or to get those official downloads and that's not really scalable of course across libraries worth of data we have so many databases we have so many different products so of course even just the work that it takes to work out all those licenses it's not really scalable and our big advice would definitely need to involve your library copywriting copyright and licensing specialists whoever they may be they will be your friends in these efforts because often it is specified in your license but you have to ask for it and so you can definitely look into that more with your specific cases that you have in mind but those are a big point of the site we would also say um securing and storage and preservation of local hard drives copies it's very important often that is part of your licensing agreement that you are limiting the access to the data that you do get so you get the full data but you're not putting it on an unpredicted server maybe you have agreements that researchers will sign before they get access to the data so you have to really spell out who can access this data what they're going to do with it so that you can keep in control with that and not run afoul of your licensing agreements the other thing about secure storage and preservation is that many of us have existing internal preservation systems but that's not really what we're talking about here it's not designed to allow you access protect data mining it's not even really a local preservation copy it's designed for catastrophic events when you would really need to rebuild your catalogs but it's not the same thing here so the getting the data if you have an e-journal preservation system it's not the same thing so that's an important point as well so with that I will ask for the next slide and we can switch to Q&A mode so Bernie is going to moderate and see that some questions that we can take thanks Lindsay can i mentor chance great yeah we do this is Bernie Riley and Cyril again we have questions coming from a number of sources number one the chat and so if you have questions do you want to ask if you're participating in this webinar you can write them into the chat now we have a few there we also have some questions or some topics that people asked us to address when they register for this webinar and we have a lot of those if we don't get to your question don't despair we we use the questions that you asked to shape the content of future crl webinars and this as I said earlier on this is the fifth webinar we've done on text and data mining certainly the ones be further ones that will happen so we come to the questions someone asked about the JSTOR service that you lived in and Peter mentioned is that something that's currently available as when it comes under the Jase or subscription or is that a special fee for service service yeah the JSTOR DFR or data for research is actually been around for a couple years and I think it's actually a really innovative way of solving some of the problems that we've identified in this webinar specifically access to in copyright material what these sort DFR does what data for research does is it allows selection of articles from the current set of journal to JSTOR many of which are in copyright their post 1923 Journal articles you can go in and you can actually use J stories own subject having affordances to be able to say I'm only interested in German Studies or classics whatever you want to do and what I want to do is I'm going to pull the what are called term frequency counts so these are actually sort of like dehydrated articles instead of the articles being in the word order that you would expect if you are reading them what they are is their term frequency count but how many times does the working saw occur how many times was the word Berlin occur and so by giving the data to you in that format it's not readable by a human but it is readable by an algorithm in fact it's readable by technique such as topic modeling so what's great about that is folks have gone in you know didn't memo went in and looked at classics journals from JSTOR over 100 years Allen Rudel went in and looked at German Studies journals looking at how those how discourse in those disciplines changed and they did those projects within copyright and out of copyright materials through JSTOR DFR because the articles they were given were not human readable but they were machine readable and so anybody can do this I don't even think you necessarily have to be a member of the JSTOR consortium don't quote me on that but it's very open you can go into google JSTOR DFR you'll see the ability to download these these extracts of term frequency counts from JSTOR good so that's kind of a limited k2 bill capability but it is available to people that have the corpus exactly yeah another question this is kind of our crl heart which what what's the status of OCR for non-english language text and and particularly text with non-latin on Latin script right well that's a great question and I don't work primarily in non-western scripts my feeling is that the kind of fun there's two products that are pretty good depending on what how you fall on the open sorta slide so a B which is a B D YY is a Russian company it's a company that makes commercial software but in the a be fine reader product which is not more than like 100 or 150 dollars with educational discount is actually really quite good at most languages and then there's another product originally from Pamela Packard and now sort of controlled by Google which is the tesseract OCR engine that's free open-source OCR engine I've run both it depends on whether you want kind of a command line experience or a kind of Windows point-and-click experience but Barry's question about the the non-western stuff that's really crucial and I'm going to punt on that because I haven't done enough in non-western languages to be able to say how good that stuff is I think one thing you could say was that it does sometimes come down to print quality so even in really boring non exotic languages like French if we're working with teeny newsprint on microfilm sometimes even though French is a very well understood problem French is very you need OCR but it sure gets hard when the text is really cruddy and it's from a 1940s newspapers on microfilm and we're trying to scan it so I think that it's kind of a dual access problem of how good you know one access is how good is the heuristics for you know various languages and the other accesses how good is the image quality that you're able to fit into it great thanks Peter so I assume non-western scripts referred to not didn't refer to treatments for Bollywood movies that's I think an important and growing area of research on those campuses right we have you know it's interesting also with the issue of non-western stuff so in some cases you know ale we happen to have preserved a lot of English language missionary material in mainland China there are also collections that vendors have such as Times of India which of course printed in English so one way it's not a substitute for working in the languages of those regions but sometimes as an experiment you could work with some english-language sources which cover that area even if even if you find that you know OCR of CJK characters is too difficult to proceed with great there's there's a fair amount of interest being expressed in the open access corpora of texts that are subject to data mining or text and data mining I know there's a fair amount of an old fair amount of mining of WikiLeaks and Wikipedia next someone asked about chronicling America is that subject that's a new that's so people know that chronicling America is the national newspaper digitization project sponsored by the Library of Congress which does basically 323 English language only but I mean within those confines is an amazing project and one of the things that makes them amazing is as a great web frontend we things like positional OCR search hit highlighting as well as I believe in API that is to say an application programming interface to be able to ask questions of that corpus I haven't programmed against it but I know there's at least one graduate student at Yale who has so um I think this is another case where if you look for the chronicling America API you're going to have a much easier time making that happen than you possibly was by looking at a commercial project which product or somebody's also mentioned the questions which is ProQuest historic newspapers I think that came up a little earlier in the chat transcript so the ProQuest historical newspapers project is an amazing product I think that I don't want to speak for ProQuest I think the the trend I've seen from ProQuest is to be more interested in TDM they've hired a person recently in the Ann Arbor who has extensive experience with text and data mining and we've been able to work with him to get access what I would say about that is staying with all vendors it's going to come down to making sure that libraries are able to get access to that material without paying really exorbitant rates for products they already licensed making sure we can get that raw data without being sort of double or triple charged the related question is about the commercial texts they text out there the in the big commercial news text databases like LexisNexis effective and Reuters yeah you know um I've had I've had less experience working with those I have worked with some of the material that goes into LexisNexis you know one of the interesting things about TDM is trying to tease out the relationship between the venue know the company that owns the copyright and the multiple companies that have created the digital project to sell it so oftentimes as newspapers like everybody ever tried to work on the New York Times it's completely Byzantine who has what vendors are selling subscription access access directly to the New York Times very complicated I have to preserve a purchase of a case by case basis in terms of decoding who has the right to give this and to us we I think are trying to get a copy of one of those big newspapers for a specific project and if we're successful then I may have some actual expertise in that area afterwards but right now it's time just as confused as all the rest of everyone on here my take is that in the in those big commercial databases like the fact IVA and LexisNexis not some there they are building powerful tools to mind those that but they're slanted towards commercial or towards use by people in the in the finance in the fields of writings and economics of those and we've we have had conversations with a number of faculty that have worked directly with some of those big vendors on getting data and getting rights to privileges to to mine from directly from the corpus yeah it seems what seems to be happening is that though they're more willing though the company companies that control is dead this tax data's are more willing to work with faculty that are in the business schools than they are to work with faculty that are in the humanities and Social Sciences so it's kind of a unlevel playing field seems to me yeah oftentimes law schools and business schools are latent hidden sites of tedium that never go through the library so business schools will sign agreements with commercial companies get access to the data it's a funded research project at $200,000 moving around libraries don't know about it the challenge for libraries at least I feel the challenge for my position is can we be nimble enough and quick enough to support researchers in business schools or law schools people who move very quickly who have access to funds who have to put a sort of value on efficiency and conducting projects not in the humanities time scale but in the kind of law or business time scale that's a challenge that I feel because I feel like you know the work that I can do with the vendor is ultimately effective but sometimes slow because sometimes it's a long conversation to find somebody in the company has ever heard of PDM or who in the scare of tedium so maybe there's some potential for collective dealings with the vendors on behalf the humanities and social science research communities you know I think some thinking some good good well thank you though sir those were a number of the questions not all the questions as I said don't be discouraged if we didn't get to your question we will all be grist for the mill but we want to move to the next stage in the in the presentation which is the demonstration of the Evoque database indeed the work that got Lindsey and Peter been doing it at Yale great there we go okay so I'll start by talking a little bit about the project in general and then we'll switch to the live demo um so to start out with when we say that we have full access access to the data we've been talking about that provoked specifically that means that we have access to both text and full page images that it does not be an API but this is us having the files for all of the text and all of those full page images and that's four hundred forty thousand plus pages of back to 1892 it's six terabytes of data that's mostly images that takes up that much space and so it's a complete archive of the publication history info there are no missing issues the ProQuest and conde nast archivists and people developing that project worked really hard to make it a complete archive it's very well marked up so that allows us to computationally use the metadata for research purposes and that means we're pulling out labeled elements like covers or author names or using word counts um it's a really great example of what you can do we have a full full data and so as I mentioned before that came from our professional access license to the ProQuest folk archives and that's enabling a lot of experiments and that we've been running with us on this project that we're calling robust reading Vogue's and we have as I mentioned really good metadata markup from the vendor so it's important for us to say we did not digitize this project we've got that would take a really long time so we we've got not only all of the text on the page of each add for example but also the date the page number the product that's advertised whether it's part of a multi page spread so that's it's really very robust and it's allowing us to do a lot of interesting things so for example the fashion sheets are taped tagged with the photographer designer art director almost anything you could think of just search about that image so it's really a high level of detail even more than some of their other products and another thing that they've done is to join the text of the articles together so often in vogue and you've got the title page and then you have the beginning of the article and then continued on page 400 and you just get all the way to the back of the magazine so as you can imagine semantically that's really important to make that text together so that's giving us all kinds of things that we can do with the full text of the articles as well and I should pause here and say that this is like the opposite of an open data set there are multiple levels of copyright and licensing restrictions on it so it's not just for quests but also conde nast is the original creator of the content so we've got our professional asset access license whisper quest then we've got the conde nast copyrights and all the advertisers copyrights individual authors so it it's quite protected as that goes so that's why we talk a lot about limiting access to the data and their intention is really to maintain vogue as an entity but not to have individual pieces of the archives out of context so that's really the value that Conde Nast has and progress has said that they won't hang on to in terms of dealing with the data alright so here we're looking at the website for robots reading vogue so this is where we're collecting all these different experiments that we're conducting on this data so one of the things that's been great about folk is that is such an accessible set of data that everyone's making sort of relate to it so we can talk about all these different approaches for text and data mining with something that people at least have some level of familiarity with and maybe not as much as they would think so some things that come out of the process and it's been really interesting to work with this set of data so one of the things that I'm going to talk about first is Peter mentioned earlier as a bookworm and Graham's search which is the open source version of the Google Books and grand search so if we go to this is one of the experiments that we have on the main page but we're thinking when we start to do a search and then grep search on the Vote corpus we think about things that we know about folk and so here's one of the searches that I touch down in the past and that gives us an interesting pattern so we're thinking about things that we might want to search and of course when you have this kind of interface you can think of all kinds of different things that you might want to search for but if we think of a things that we know about so business fashion magazine its marketed to women what might leave search so it shows corset fertile and bra search across all texts and we're thinking about tracing patterns and searching across the publication history in a way that's slightly different than in the native ProQuest interface we can do a keyword search that's definitely possible but in this case we can actually track the usage of these terms over time you can see that I got these different terms in and if we click on an individual point I'm actually getting a list of the results and that and I will talk a little bit more about that in a few minutes but it's really important to us to be able to connect to the original the underlining articles and text there so that we can do some more exploration from this kind of search but you can see that and we've got the word girdle which is for a little bit ahead of course--it and of course it takes this long slow decline in the early 20th century and got the word bras so this is just something that I started came up with off the top of my head and you can imagine all kinds of other interesting searches that you might do so moving on to something that we might not think about searching in what we think of as a fashion magazine I wanted to see some words that might talk more about women's lives across from back to 1892 to mm up to the present and what might tell us about how things have changed over time so that that's amazing that we can chart very clearly with this kind of experiment and so I type in the words career education family in college and this is across all of the text that includes advertising articles various genres that have been invoked over the years and I came up with this surprising result so this is not what I would have expected it to find and I find this huge peak for the word college in 1944 so I can say to myself what this is not what I predicted and so what what can I do with that so given we've talked about how good metadata is they were just using one of those facets we can actually limit that so I do another I did another search just on the word college to look at that peak and I can actually choose in the bookworm tool just to look at the genre of advertisements but this is all the text that's labeled as an advertisement and how often is the word college used and then I can also split that out with the article text and what I'm finding is that the peak for college is actually 1944 but this is advertisements and an article that sort of that same flat line that I'm seeing with all the other terms that I had you so lets me drill down into what is going on with this word and then I can look at it even further in more details I click on 1944 at that peak then I start to see of course this is all advertising including assess the case so if I click into one of these advertisements and I can see what it is what's going on so here is the this is now we're in the ProQuest interface so that's an important point we have now the bookworm tool is just seeking you into the pro question effect not retreating something else and so what we have here is an advertisement but his four sweaters so what I actually discovered with this strange graph result is that the word college it used a lot in advertisements for things that someone might need to buy if she were going to college so this is really interesting to me and I definitely want to look at it more but its advertisements for things like pearl necklaces I mean there are actual ads for colleges but more often it's saying like this one it says and that's the first step she knows that time you start the business of being a freshman just to have this kind of sweater it should keep adding these famous Scottish sweaters to mark her college years so this is somewhat surprising somewhat not but very interesting that we can actually see that in the bookworm search and then it leads us in choose close reading and what it what is the hinds of those results so I think what it really serves is do you point out areas that would be injured to look at you're sitting through this huge amount of data trying to think of interesting questions to ask and maybe you already have some in mind but what we've found consistently is that the data will surprise us with interesting places to look so we can then dig in a little deeper and be surprised with what we find so this I should just pass her moment here to say I told you that that we went into the program interface so this is just because we have a subscription to the ProQuest vogue archive so if you were looking at our bookworm search and you were clicking through you would get to this point of seeing what's behind it but you would not be able to actually click into the content unless your institution subscribes to ProQuest so that's one way that we're providing the tool without recreating access to the underlying data and that's really important for our licensing agreements and sort of differentiating between what the library is doing and what ProQuest is doing we're not duplicating efforts there okay then moving on to topic modeling that's the next of our experiments here on robust reading folk and this is the other technique that Peters mentioned a few times so we're using we're using type of modeling which is the statistical algorithm to expose things that we might not think to search for so it's sorting all the text of Vogue into these different topics which we can we might not even think to search for those in a keyword search or Annette and gram search so what we're looking at here we have said to the topic modeling algorithm we're using mallet here that we want 20 different topics and these topics that are in quotation marks on each bar here we have applied so we've applied after the fact that there are 20 topics that are groups of words and then we have sort of looked at them and figured out what that is so just to explain what's going on here with the interface and we've got these groups of words that commonly appear single words and then double words are by Grim's right side and so scrolling through looking at these different topics we can say well I'm not that surprised that there is advice of etiquette in vogue and we can also look and see when in vogue history those topics were the most popular so we can look at that over time also so we're looking through way of decorating and gift when this is not surprising there's some great food writing in vogue society of course and then we get into one more surprising things there's a politics topic on many different words for dress making and then this is the one that really surprised us so there is a topic that we're calling health groups of words about bodies and disease or fitness that kind of thing that kind of thing and you can see that there's a huge increase in that the saturation of that topic in the 1970s and 80s under the editorship of grace meribella so what we have here across in the different bars are the the editorship of these different women who have been the editor-in-chief of folk and so under grace Mirabella you can see that this topic became really popular and if you look over to the right you can see some of the titles that are under this topic and so if we think about vogue we would not expect there to be such such preponderance of health topics but we know or I know from reading what grace Mirabella had to say about her vision for the magazine is that she wanted to change the magazine and to make it more relevant to modern women and she was really concerned about not only the clothes that they were wearing but the health of the body underneath the clothes and she happened to be married to a heart surgeon as well so she got in trouble for trying to eliminate cigarette advertising from Vogue but so this was really interesting to me to be able to see using this topic modeling algorithm that she did make that change and then is actually visible so we can say that she effectively changed the magazine and then Anna Wintour took over after her and was maybe less interested in those articles about health so that's a really dramatic example but it was also an example of like contact the contents of the archive surprising house and another topic that I'm really interested in was actually the top one so I'm in art librarian of course I'm interested in art but it's surprising to see how much coverage of arts and museums there has been invoked over the years and so we've got a pretty consistent coverage over the years and we can look into but individual articles and who is writing them and there's a lot of interesting stuff going on there so again that was a bit of a surprise but it's nice to be surprised by the data and trying new things to look into if we go back to the home page one more thing I want to show you is that we are using the data that we got from ProQuest to actually facilitate some student work so we've had some students in computer science we're really interested in what they could do with this data and we love that because they can come up with new things that we wouldn't have conceived of so we're always thinking of new experiments but they came up with even more so we have two students one is in applied math and statistics and she was applying a lot of computer science and programming methods but she's also interested in gender studies issues so her name is Kristina Wong she's doing this analysis of faces and body language using face recognition software facial detection and facial recognition to look at the images in vogue and try to make some conclusions about what was going on and with the way that women were portrayed so that was really interesting because that was completely different from the other kinds of experiments that we'd run on the data and we also had Clyde Edwards who's looking at using pattern detection algorithms to look at the amount of space on the covers of that was given to text versus image and how that changed over time and so that it was just really cool to see students that access data and do different things so it's not a protective machine in the computer science department but they did all kinds of really interesting stuff with it okay so now we can go back to the slides okay great so thanks for that demo Lindsey and now we're just going to wrap up with this concluding slide on we sort of wanted to say that what we began the conversation with which was this notion of there's a large amount of material out there in JSTOR and Google Books Google Books managed by Holly Trust in these commercial vendors like Elsevier ProQuest like we've seen today we've also got new ways of accessing it whether that's the raw data or through api's and then more importantly we have these new algorithmic ways of processing it so not just counting words that we think of ourselves but relying on data to organize itself through some of the latent patterns that are hiding inside the state data as we found out in the book project ourselves securing access to this data and figuring out all the complicated levels of licensing can be really hard but we think that when it works it does yield really powerful results for scholars and it really means that are the folks on campus here at our institution are able to engage with additional materials in ways that they couldn't in the past you know this is probably true in many institutions but an increasing amount of our non rare books pendant Yale is going to electronic resources so librarians being able to help out with getting access to and sort of getting researchers hands on this electronic data I think is going to be more and more important over time we really believe in trying to secure access to data it's important to researchers on campus when we can and we also think there's a value in finding local projects like Aslan cement and campus newspapers we hope to do a big project on the Yale Daily News in the next year and finally this notion of in all of the confusing world of TDM of all of our efforts to understand the field of machine learning or to think about ways of trying to find signal within what looks like so much noise we think it can be helpful to kind of divide our efforts into building better and better ways of looking for things that might be there based on our presuppositions as well as exploring ways of letting the data organize itself and with that I think we'll go to the final slide and open it up for questions and comments thanks Peter thanks Lizzie Lindsey the we do have a bunch of questions and comments have come in one is that the ProQuest Vogue content the text that you got was to enable you to do the kind of things you did with it the kind of analysis that you did with it it seems like it had to be pretty richly marked up and it wasn't just this simple ASCII that usually get in some of the news databases like Factiva and them because we need to distinguish things like advertising from the articles that kind of money so that's running how does that compare with the text that's available like safe removal books or happy trusts yeah that's a great question um one of the things one of the ways that metadata and raw data came together in our vogue project was the notion of we ran the topic modeling not only the journal article and only on the articles in vogue we use the explicit metadata that we were very lucky to have from the vendor about was something a fashion shoot or an article or an ad in order to only put the the journalism the kind of articles in vogue their topic modeling now you don't have that luxury when you don't have the data that's been expertly marked up by a team and Google Books is a great example saying with you know Project Gutenberg a lot of sources out there are never going to have this kind of you know let's just use the word TEI for lack of a better term in this kind of Mantic marked up about what we're looking at I think we would have been able to do a pretty good interesting topic modeling projects if we didn't have that markup of what is an article and what isn't has but we would have ended up with a lot more jump topics and we would have had to do more work to pull out the advertisement in some previous work I did on the Google book stuff I actually did talk modeling on Google books in a particular Northern European language and it's it's true that when you start looking at the stuff that's in Google Books it's a wild and wooly world out there I mean we had um you know Danish the Latin dictionaries printed and rocked or there's like lists of people's telephone numbers from 1920 not everything in there is fiction right this is the downside of the great unread is that when you move outside the Canon there's just no predicting what's going to be in your data set on one interesting project that could rely on article segmentation but didn't rely on human judgments about what is what is the project by Rob Nelson at Richmond called mining The Dispatch this is a Confederate newspaper in Richmond Virginia from 1860 to 1865 if I'm remembering that project correctly Rob had human segments the articles so you know every article was separated from every other one but he didn't make any human judgments about what were these articles about and instead use topic modeling to sort of categorize them and it's not what's similar way that we did with some robot through oak so he had to create that dataset himself and he had to make the decision about am I going to just put it all in his raw data or am I going to at least turn each article into a unit or am I going to go all the way and try to make human decisions about this is an advertisement and this is an article and I think that's another interest in foil to look at in combination with the Vogue project is to look at mining The Dispatch from the University of Richmond for sort of how they figured out what the right answer was in that context great do is the ProQuest historical newspapers as richly marked up and segmented as as vogue do you have a sense of that that's a good question we'll know soon because we're trying to get some of that information my feeling is they may have from what I've seen I think there is somebody on this call who knows more than me but I think they may have article segmentation but not you know judgments about is this an in common you know it's historic about the economy or about science or something like that you know they do have article segmentation depression was that the soap is really such a popular product that the content is just interested so many people that they really went above and beyond on that one so even as a comparison Women's Wear Daily which they came up with after Brooke and they're doing Harper's Bazaar next it's not the same level because vogue just lends itself so much so if we look at women's wear daily the metadata is still great but it's not a labor of love yeah it's interesting we're of the complexity an electric property issues with content yeah it's an interesting you know we we sort of got lucky in this case that that the archive was you know this particular magazine was kind of an edge case in a lot of different areas if this has been a magazine that when it went public for five years went on a business in the 20s we wouldn't have been so worried I mean we wouldn't have worried as much about a copyright and licensing right but cutting I still made a lot of money from folks so this was like you know this was kind of like jumping into the deep end in terms of trying to figure out licensing but I think you're right that you know there because it's a valuable product they put an amazing amount of effort into it the real question you can ask would be is there any way that we could use these sort of techniques like topic modeling to provide you user interface affordances to this massive archive 2,798 issues for 2,000 pages is there any way that that work could come back into the ProQuest interface maybe one day ProQuest would allow you to navigate Vogue not just by issue a number and looking for words but rather by the kind of somatic streams that's up to ProQuest which runs their platform and they have very smart people in there who are pushing that platform forward but I think it's a question that we're going to investigate on campus some of some of our own internal archives so we have papers in Henry Kissinger we have a college new sticker that goes back to 1878 we have a lot of material that we'd like to see if we can provide not just standard next page previous page buttons but also some TDM that turns into a user interface TDM that allows you to surf through the information by using some of this machine learning so that you can get more out of this these sort of overwhelming our well if you can TDM the Kissinger papers there's a lot of good out there that will thank you for that training you can imagine all the things that go into that project I can't imagine okay there was a practical question which is that there's yells license for the ProQuest both allow visitors visiting researchers on campus that Yale to see the results I assume to see the results all the way into the full text from that's it that's a great question um I think it might come down to the peculiarities of you know do our public access of the computers or does our public access Wi-Fi allow folks who are physically present in the library to we're going to stack size in some way to view the material and I think it does now I'm not you know it would be a high burden for every to travel to New Haven just to be able to click through we've tested this in other universities that subscribe to the ProQuest vote are both archives so the links will all work if your institution subscribes that and we think especially a pretty clean answer um we were actually a ProQuest physically and demoing this and it turns out the ProQuest guest Wi-Fi network doesn't have any subscription rights so the links fail that just proves that it worked right that they hadn't allocated that IP range that that set of licensing material um so you know the vogue archive is you know a product with a price tag associated with it so I totally understand if libraries make the decisions they you want to subscribe to that but either physically coming to New Haven or being on a campus that does subscribe in this archive all the links should work okay a technical question recommendations for software for abuse and analysis of student text and I assume that means electronic dissertations and theses but yeah um it depends what you're trying to answer it depends what your research question is um there are if you have sufficient material if you and you have like seven student papers that probably have a good match but if you have 700 student papers or something of that scale the freely downloadable tool mallet from the University of Massachusetts Amherst is a tool that will let you do topic modeling on any collection of text mallet is by no means a friendly tool to use but there are actually some great tutorials out there if you google for things like topic modeling mallet Scott Weingarten a couple others have written really great walkthroughs about how to get mallet installed on either a Mac or a PC it is a command line experience there are some tools people have written to rack the mallet code around other software some people it depends on your campus some campuses have great support for our or other statistical packages sort of huge data analysis environments that let you load in plugins and do things in tools like our or a rapid miner or other software other folks who are comfortable with programming want to tackle things themselves famous Lee Ted Underwood in the English department at urbana-champaign sort of implemented a topic modeling algorithm himself not all English professors have been coding the skills to be able to do that but there are some people who can do this from scratch other folks will use mallet or tool similar to mallet to do that the bookworm tool is really available it is a set of Python scripts and Mike insertion scripts doesn't require anything more than kind of an extra Mac Mini you might have or a virtual machine that you set up in order to ingest that text I would say probably the vast majority of the labor is and data cleaning that they have prepped getting out getting access to the data but those are some tools that just hit me off the top of my head there's of course the great Boyan tools written by Jeff Rockwell Stefan Sinclair from Canada they just pushed out a release in the last year of pushing Boyan which is sort of web-based text mining even further so we're clearly in this mode and in text and data mining where there's no one tool like Photoshop so for those governments have been doing digital photography the good news in that Photoshop is they can do anything you need to do a picture it's just expensive and complicated and hard to use well we're not really there in text and data mining there's no tool like Photoshop that can do everything but it's just hard to use I don't know if we'll ever get there it's interesting to compare it with other areas like geographic information systems where esri arcmap tool does everything you would ever need to do to a map it's just expensive and hard to use I don't know if we'll ever get there I don't know if we'll ever have a Photoshop of text and data mining I don't know if that's desirable I don't know but it'll be interesting to see how that evolves over the next couple years as more and more universities build digital humanities lab and we get more and more interest in text to data mining in the humanities Social Sciences and say good note to end on Peter thank you you the if we didn't get to your questions please do know that we will take them into account when we develop future CRO webinars the quite a matter topic that came up a number of times today a number and it was a number of the questions that people submitted was the matter of Perpetual access and how that might enable be a way to to kind of adorn into text mining and data mining in a large corpus of licensed materials we will we are developing a webinar on perpetual access and adults and Overson is the process of designing that at this point for CRL the and our senior consultant and licensing and electronic resources so that will be announced at some point I want to thank Peter and Lindsey today for their work through a lot of work went into producing the webinar today not just - not just in the last couple hours but in the last several weeks and Peter and Lizzy have been terrific on that we're - thank you also for your participants all for your comments and your questions that help shape the content the webinar as well crl does depend on the experience and expertise of people in our community to inform the prudent and wise and informed investment in electronic resources so we will let virginia care close out the webinar thanks Bernie this is Virginia Kerr have communications and development at CRL and a special thanks to Peter and Lindsay for an excellent presentation today and thanks to all of you for joining us today like to remind you of a future webinar coming up very soon - by crl on CR ELLs own programs in licensing in our acquisitions programs that will be on August 19th and the registration form is available for that we greatly value your feedback and we encourage you to fill out the brief online survey which will appear at the conclusion of this webinar as you log off your feedback is very important important as Bernie indicated in our programming and planning for future webinars a recording of this presentation will be posted on CR ELLs YouTube channel soon we hope to also document many of the questions and comments especially if some of those were not addressed directly in the audio of this presentation we'll send out a notice to all registrants from the webinar for letting you know when the recording is available you can sign up for more updates on CR ELLs collection services and upcoming events through our online you can get information on them through our online newsletter crl connect so be sure that you are signed up to subscribe to crl connect and you can also find us on Facebook and Twitter we look forward to hearing from you thank you very much "
fcyPCOmXpdM,27,"This video contains the description about another example problem on Apriori algorithm for finding frequent itemsets in data mining.

#APRIORIalgorithm #Apriorialgorithmwithexample #Apriorialgorithmindatamining",2020-05-03T02:16:13Z,Apriori Algorithm in Data Mining | APRIORI ALGORITHM | ALGORITHM WITH ANOTHER EXAMPLE |,https://i.ytimg.com/vi/fcyPCOmXpdM/hqdefault.jpg,DIVVELA SRINIVASA RAO,PT13M28S,false,747,11,0,0,5,hi friends today I am giving a lecture on example problem on a priori algorithm in the previous video I am discussing one example problem on a priori algorithm in that video I am discussing another example problem on a prairie algorithm mainly a prairie algorithm is used for finding out the frequent itemsets from the event transactional database okay so now this is the given transactional database it contains two columns first column is the transaction ID and second column represents the item IDs okay in that transaction ID the transaction numbers are they from 100 200 300 400 hundred to four hundred and in the item IDs what are the items that are purchased by a particular customer in a particular transaction so here item item ID 1 3 4 or change in transaction ID hundred in transaction ID 200 2 3 5 items for children in transaction ID 300 1 2 3 5 items are purchased in transaction ID 405 items are changed so this is the given transactional database D and minimum support count is equal to 2 so these are the inputs for the given problem that is example problem Apriori algorithm first scan the database can the transaction database for support count of each and every item so here there are five items on day 1 2 3 4 5 so these are the 5 items all day so how many times each item is there in the given transactional database so here so candidate item set one that means the item set contains a single item how many times that can be there in the transactional database so that is represented by c1 in that see one item set and support count okay so first of one how many times one is there in the transactional database one to two times okay next how many times item two is there in the transactional database one two three three times next how many times item three is there in the transactional database one two three three times next how many times items for is there in the transactional database only one time okay next how many times item five is that in the transactional database one two three so three times okay so there are five items are they so each and every item how many times is present in the transactional database okay once we are meeting see one now compare the candidate support count with the minimum support point okay if the candidate support count is less than minimum support count we have to prove that candidate item check if the candidate item items that item support count is greater than greater than or equal to 2 that can be exist in the Stephen okay so here first item item set one support counties to to yield greater than or equal to two so s true it is exist in the L next item set to support count is 3 3 greater than or equal to 2 condition proof so it is the exist in the transactional database next items are 3 support counties the 3 greater than or equal to 2 so as condition proof now it is exist in one item for support count is 1 1 is compared with 2 1 less than 2 so it can be pruned from c1 so that is not there in l1 next item 5 support count is 3 3 is compared with 2 2 so 3 is greater than or equal to 2 now it can be exist in l1 so in the l1 whose items that support count is greater than or equal to 2 that item sets are only present in l1 the remaining item sets are pruned from l1 o next one once we are getting l1 generate c2 from l1 situ means candidate item set to that means each item set contains two items okay so here one is combined with 2 we are getting 1 comma 2 1 is combined with 2 3 we are getting 1 comma 3 1 is combined with 5 1 comma Phi 2 is not combined with 1 okay so is combined with the 3 we are getting item state 2 comma 3 2 is combined with 5 we are meeting item set 2 comma 5 so 3 is combined with the five we are getting item set 3 comma 5 so how many times it is exist in the transactional database that means we have to find out the support count of each and every item set next first one 1 comma 2 how many times 100 two items or four children together 1 comma 2 it is not there is present one time so here once - amar - is only one time it is present ok next one 1 comma 3 how many times one and three items are children together in that transaction database 1 comma 3 1 time 1 comma 3 - thanks so here is is - thanks next 1 comma point how many times 1 and 5 items in a transaction database only one time yes next 2 comma 3 how many times items 2 & 3 or 4 children together in the transaction database one time - 10 - tens next - I'm fine how many times to section beta base - thanks three times one time two times three times next how many times items 1 2 so 1 2 after getting c2 these are the support comes of each and every item set next once we are getting the item set support count in free to compare the idol support count with minimum support on so it is less than 2 it can be proved from the egg to next it is less than 2 it can be proved from the to next the remaining are so 1 gamma 3 is existing 2 comma 3 is existing 2 gamma 5 is existing 3 gamma 5 is existing in L 2 what are the items X are present the item sets whose support count is greater than or equal to the minimum support on that are present in L 2 1 comma 3 comma 3 comma 5 comma 5 items that supports forms or greater than sorry we're too so that they exist in next one let's generate c3l okay so generates degree means the items the candidate item set contains three items that can be generated from hey so here by clubbing these two am test one item is combined compulsory per combining okay 1 comma 3 2 comma 3 here 3 is combined between them so by clubbing these two items X we are getting 1 2 3 next 1 comma 3 2 comma 1 comma 3 3 comma 5 by clubbing these 2 3 is common between them by clapping we are getting 1 comma 3 comma 5 next 2 comma 3 to come of it among them item 2 is combined item 2 is common by combining them we are getting 2 3 5 next remaining there are no other items X okay now once we are getting candidate item 63 find out the support count of each and every item set from the transactional database we have 1 2 3 so 1 2 3 1 2 3 1 2 3 only one time if they're in transaction ID 300 so it's important if one next one 1 3 5 how many times it can be present in the transactional database 1 3 5 only one time it is present next - how many times - one time - then so two things okay once we are meeting the candidate item set c3 paper so that it can be compared with the minimum support on whose candidate item set is support greater than or equal to 2 that can be exist in l3 so this items that support counties 1 it is less than 2 then it can be proved from l3 next one three five is a support found is one so it's important is the less than or equal to the minimum support um so then it can be pruned equal to the minimum support um so it can be exist in l3 now contains only a single item so that is two it satisfies the minimum support condition now so from l3 so we are generating c4 but there is no candidate item set we have we have there is no candidate item set we are getting from LD so therefore C 4 and L 4 is equal to 5 so whenever C comma K C suffix K and L sub its pay is equal to PI Apriori algorithm is eliminated then the item sets in the else of XK minus 1 or frequent itemsets here C suffix for and else of its four or five that is null set there are no item sets are there in c4 and l4 so hence the item sets present in the else of XK minus 1 that is the l3 are frequent itemsets so in the l3 only one frequent item set is that so that is called as frequent itemsets therefore the frequent item set is equal to set up 2 comma 3 comma 5 so this is the description about example problem on a priori so thank you thank you for watching this video if you like this video please subscribe my channel me so the better for yourself if you like this video please share this video we were friends and classmates thank you 
yn89cspIUF8,27,"THIS IS THE FIRST LECTURE ON THE TUTORIAL SERIES ""DATA MINING BASICS "".
THIS LECTURE ILLUSTRATES DATA-MINING AS A KDD PROCESS.
DO SUBSCRIBE THIS CHANNEL AND PRESS THE BELL ICON TO HAVE NOTIFICATION ABOUT THE UPCOMING LECTURES. SHARE WITH YOUR FRIENDS AND THOSE WHO WANT TO LEARN THE DATA MINING TECHNIQUES FROM VERY BEGINNING.
ONE QUESTION IS ASKED AT THE END OF THE VIDEO, COMMENT YOUR ANSWER.",2019-10-24T16:03:57Z,DATA MINING AS A KDD PROCESS|DATA MINING BASICS BY DR  DIBYA JYOTI BORA|LECTURE#1,https://i.ytimg.com/vi/yn89cspIUF8/hqdefault.jpg,Dr. Dibya Jyoti Bora,PT12M30S,false,88,6,0,0,1,hello guys I will come into the determining basics a new course on data mining and in this course I'll try to illustrate all the important technical term analysis related to data mining and about different data mining techniques to sir frequently use and is a very important ok so this course will be very important for you as theoretical knowledge about this data mining techniques is very important to understand or to implement those different techniques so first of all in this tutorial series I'll try to cover all those dirty little backgrounds and after that I'll coming coming to the practical implementation of those techniques from fighter ok myself dr. Dubois Devorah and this is the lecture number 1 and today our topic is data mining as a knowledge discovery process that is data mining as a KVD process ok now what do you mean by data by d-c data mining in general terms means mining or digging deep into the vector whose's in different forms to gain patterns and to gain knowledge on that pattern so what more important in this definition is digging deep into data as you know in our common day-to-day work we use mining to explore minerals from heart of course from deep of the art is it it so those minerals are ready important for us and those are not really accessible without we go into the defaulter art so similarly in a big amount of data some important bedrooms are hiding there ok so we need to find those patterns and thereby extract them and convert them in them into knowledge and you know knowledge means what import applications of this pattern isn't it so that's why the definition is like that the DA mining in general terms means mining or leading depleted octopus's in different forms to gain patterns and again no less on that pattern now in this process of data mining three important steps of involve roadless begin cream put in steps are in both what are they first you know large datasets are sorted you have to sort the large that does it then we need to identify the patterns that is patterns are identified can relationships are established apart from the dialysis and soul the better or problems means after identifying the patterns you need to perform more clear observation on those patterns and thereby establish relations who shall help us to solve some particular problem clear now if we talk about data mining as a knowledge discovery process up till now you Gordon that we are performing these data mining to gain knowledge from a big amount of data okay so that's why we have the term data mining as in knowledge discovery process but in this process several important techniques are involved or I can say several important steps are involved who said very important and the sequence is also important sequence means one after another you have to perform those steps so first coming to the first stage that is data cleaning this is something also known as data cleansing okay so in the taglines mean what happens the noise and inconsistent data is remove as I have already said depth here we are talking about a large data sets so in those data set may not be all those data are important for us so we have to remove those input one important or I can say that inconsistent data also the noise present in the data noise means what those data points pushing no way relevant to our data of interest okay so the technology means noise an inconsistent data removal safe here so after data cleaning we come to the same data integration okay so integration in division means combining okay so here in this step multiple data sources are combined as you know we will have those different data points or different data from different sources like your normal our DBMS like your action sheet or some hard copies based that analysis processes hard copy - that are connected okay so from different sources you are having those data collection isn't it so in data integrations you need to combines those multiple data sources clear multiple data sources are combined into a tight division then next step is data selection and in this day data relevant to the analysis cancer retreat from the data base you know from multiple data sources we have command those different data now all those that are not relevant for us so in the collection we are going to select only this in data they are important or I can say they're relieved him to our current analysis thoughts here then data transformation the transformation means for here that eyes transform or consolidate into forms appropriate for mining but performing Samadhi or aggregation operations well this may not be very clear to you so I'm giving you an example what happens suppose you have collected data for wait data okay so with maybe in kilogram or wait maybe in pounds but you know that if we have collected this data and if we were going to perform some analysis then what will happen those data are mixer of to the village if we apply the technique of the kernel as a spoon in those data then obviously our result will not be accurate because you your kill your mother is different add your founded recipient so we need to convert them into one particular form or one particularly need like KZ or kilogram or like Bom that's only one so in that way only it will be appropriate for the other analysis that that needs to be perform on it clear so that's why the in data transformation step data is transformed or consolidate into the pumps they appropriate for our mining techniques clear now this step after the data transformation step means after we convert our data that is being collected from multiple choices and that is being selected or I can say only the relevant data or LLC stocks are selected and then we can spawn it into one particularly then only we have data mining paths and in this step intelligence methods are applied in order to extract the data patterns so in data mining we did have different intelligent methods well what what are those intelligent methods will cover in the coming classes so you don't need to worry about that but currently just remember that I am using the cub intelligence why intelligent because of course they will act intelligently to find out the hidden patterns from those data we have collected okay so in the data mining state intelligence methods are applied in order to extract data patterns then as we have the patterns with us the next step is the pattern values enslave and here the patterns this data Porter patterns are evaluated then in all this presentation as soon as you evaluate this data patterns means then you have to now represent those patterns into some meaningful form or into some useful form then only they will turn into what knowledge what exactly we seek or so knowledge is represented yeah Bethany palace instead and this is the elastic where we have our goal that is we have transform those elevate them data collected from different sources into some meaningful patterns and also useful and this is exactly our knowledge so you have seen that in the data mining as a knowledge discovered process we have several steps like data cleaning that are integration that are selection data transformation data mining pattern evaluation then knowledge representations well this will be represented throw a diagram you can take a screenshot of this this so you would see that this database the steps are represented like this cleaning and integration selection and transformers and then data mining that patterns evolution of the patterns and ultimately we have the knowledge so different sources like databases bases flat files from those we have collected the data from integration and then we read the data then selection of the relevant data take place and then we convert into one particular form so that this different data mining techniques can be applied on them and after applying those intelligent data mining techniques will have records and then we'll evaluate that pattern with based on some matrix that where that is patterns are useful for us or not so where are these other patterns that we are looking for and after this patterns being evaluated then we will try to represent it in some meaningful forms and they are nothing but our not less what this diagram is clear to you so this is what data mining has a kdd process knowledge discovery in database do remember that ad this stands for knowledge discovery in database okay so we have covered the definition of data mining and important process involved in the data mining and most importantly data mining as a knowledge discovery in the toughest process where we have seen the different steps like data cleaning and the Thank You Priscilla the selection data transformation data mining eternal fellowship knowledge representation and this diagram this is clearly presenting all the steps so I hope these topics are very clear to you now so let's days for equation so to this question is the term k TD stands for I am repeating again the town KD T stands for what you need to comment your answer so guys so if you like this lecture then do like it and also subscribe to this channel if you have not already and share with your friends so that they will also get benefitted from this so we'll meet in the next lecture till then take it away 
ZisqOr8GP9k,27,"This video describes about data mining from the scratch, kinds of data to be used for mining system.",2020-08-09T15:37:31Z,Data -Information-KDD-Data Mining,https://i.ytimg.com/vi/ZisqOr8GP9k/hqdefault.jpg,Florence Programmingz,PT15M30S,false,174,N/A,N/A,0,4,[Laughter] [Music] hi friends welcome to florence program in this video i am going to explain you about data mining what is data mining or the functionality of data mining and what type of data can be mined so these are the things we are going to see in this video so just we can start from what is data see every day we are using or we are generating huge volume of data so data is nothing but a fact or observations or measurement about some objects suppose we are going for shopping and we are buying some product so the name of the product is a data the quantity what we have purchased that is a data and the price for the unique product that is also a data so like that every day we are generating a huge amount of data in our life next what is information see data is different from information see information is process the data see as i said we have a data like product name and the quantity then price by using all these three data can we get some information yes the product names product name and the quantity 1 and the price is 100 rupees so we can get information that the product rate is 100 rupees so this is the informations that informations we can pass to our friends see i got i went to shopping yesterday just the product cost is this much so this is the information so the processed data is information it is useful for our life for analysis for taking decision for communication so information is processed data yes now data is as i said we are creating we are generating huge volume of data every day in our life so we can say we are living in the age of data then what is the use of the data and how we are generating data some of the examples i have given here see online shopping applications like amazon snapdeal flipkart walmart they are handling millions of transaction per week at thousands of branches so for every seconds people are generating number of transactions they are doing number of online shopping so here huge volume of data is getting transferred over internet next you create a telecommunication applications so in telecommunication applications people we all keep on talking with our friends and relatives so in the network it carried tens of beta bytes of data transfer every seconds then the another applications we can take in a health industry health care sector they are generating huge volume of data like medical medical data patients records patients diagnosis record patients monitoring record like that they are generating different types of data like text data image data video data like the different types of data they are generating a medical sector when you come to web search engine see everything we are searching in the internet so whatever the doubt the people we are having just to be a text in the google so the google is process tens of beta bits of data daily and coming to social media people nowadays they are spending most of their time in social media like facebook whatsapp twitter instagrams so here also it is processing different types of data like text feature video like that it is processing huge volume of data every seconds so around us data is moving through different media so we have a huge volume of data with us what's the use so we need powerful tool to uncover valuable information from this huge volume of data so such tool is nothing but data mining system so data mining system helps us to process a huge volume of data and it will give some information for us so data mining is extraction of interesting patterns or knowledge from huge volume of data because we are moving towards information age we are in data is from data is we we are moving towards information age or knowledge age through data mining system so data mining will extract information from the huge volume of data from that information we are getting knowledge we are getting intelligence for our enterprises so data mining we can say knowledge discovery from data so some other names we can use as knowledge extraction pattern analysis data or ecology data driving and information harvesting like that any names we can give related to data mining so data mining is a powerful tool which helps to get knowledge from huge volume of data so nowadays the data is very very important everywhere if you are going for any shopping people are asking you to fill your name your mobile number your database because other data is very much important for many enterprises for marketing so data is important source for us for generating information and getting knowledge okay yes next we can see what are the steps involved in knowledge discovery process or data mining process so in other word for data mining is knowledge discovery process so steps in the process of knowledge discovery there are several steps or involved here the first one is data cleaning data cleaning is nothing but some missing data or some errorless data will be cleaned or will be handled and which make the data to be a clear without any error and without any missing values so that is called data clearing and the next two processes data integration so data integration means as everybody knows data virus means that data from heterogeneous sources will be integrated together so suppose i am going for some analysis about some data means i will collect data from different sources so i have to integrate all the data together forever mining process so that step is data integration next date of selection see the data integrate integration after integration we will have large volume of data but all the data will not be relevant for our analysis so whatever the data is really relevant or really interested for mining that data we have to select for our mining process so that is the third step in knowledge discovery process the next one is data transformation data transformation is nothing but since we are collecting data from different sources so every sources will follow their own format once you integrate all the data together we have to use some uniform format for our mining process so we are transforming the data to the unifor see suppose some numerical value you can see the value may be from 23 some places it may be from 100 or some places it may be from thousand that is two digit three digit four digit like that but it is very difficult for our process so what we can do we can transform all the data from zero to one so normalization we can convert the data to a particular range 0 to 1 or 1 to 10 so we can normalize all the data here the next step we can apply our mining algorithm here we can apply our mining analysis or functionality on the data so up to the fourth step the data gets ready for our mining process so next we can apply our mining functionality on the data so as a result of mining we will get patterns we will get number of patterns but all the patterns will not be relevant or we will not be interested pattern because patterns must be interested for our analysis so we have to evaluate the obtained pattern from the data mining process then that pattern can be represent or it will reflect the knowledge so that knowledge can be represented in some form either it may be in the tabular form or it may be in the line graph or bar graph or pie chart as we lie as we require we can represent the knowledge in any format so these are the seven steps involves in knowledge discovery process this is all about data mining now we can see we are saying data data data so what type of data we can use for our mining purpose so the next this is the pictorial representation of the steps what we discussed in the previous slides so the pattern evaluation then at last we are getting knowledge the knowledge can be represented in any forms next we can see what kind of data can be used for our data mining process any data we can use some of the data i have listed here not restricted up to this there is no limit we can use any type of data as per our requirements the very basic forms of data for mining applications i have given here the first one is database our normal database either oracle or mysql or sql any database we can use so the database will be the structure like row and column and every column will be treated as attributes so if that attributes we can use for our mining process and the next one is data barrows data so we can use data from data of arrows so when you come to data of arrows the structure of data warehouse is cube structure multi-dimensional data model it is cube structure so that we can use so olap we can apply here that is online analytical processing we can do on data warehouse model and the next one is transactional data so all transaction based data that is sequence of transaction will be stored in the transactional data so suppose in a bank transaction and they want to make analysis about whether we can approve loan for a particular customer or not so what they will do they will analyze all the transfers the previous three months transaction of a particular customer then they will take a decision whether to approve or not so for that they may use they can use transactional data are all our online purchase online shopping we are doing particular transactions money transaction we are doing so such data will be in transactional database and the next one is object oriented relational databases so relational database with object oriented concept so these will support all object oriented features like class object inheritance polymorphism all these features will be supported here so that database we can use then temporal databases temporal databases is nothing but time related data then sequence databases all sequential events in material of the time all a sequential event will be recorded in a database so that we can use for our mining process then time series databases so time series databases if you are seeing see suppose stock market data or inventory data these are all some it is based on time so such database also we can use for our mining uh process and the next one is spatial databases spatial databases some geographical data map we can use then satellite images we can use so that data can be used for our mining purpose next to texture databases texture databases text will have the unstructured format of semi-structured format text mining we can mine the patterns from the textures so text to mining we can this is a very separate research area then multimedia databases is streaming data multimedia data is the images or animation data so that is streaming if you want to generate some patterns from streaming data we can use the multimedia database then web database web database suppose you have a website and you want to know which page is visited oftenly by the users web users so we can do that analysis using web databases so these are some kind of data we can use for our mining purpose so hope you you understood about data mining process and what are different data we can apply for our data mining process thank you you 
6KksleTYiQU,27,"This short course will help you to understand some data mining techniques for knowledge discovery and knowledge presentation. At the end of the short course you should be able to use the skills for knowledge discovery and future prediction from a suitable dataset of your interest.

This course is coordinated by Associate Professor Zahid Islam and Dr Michael Bewong. The final webinar is presented by Dr Michael Bewong.

You can also find more videos here from Associate Professor Zahid Islam  - https://www.youtube.com/channel/UCrcz90CHdK6GrtnMvk_vVXA/videos?view_as=subscriber 

This short course is also a taster of the Graduate Certificate in Applied Data Science course at CSU - https://study.csu.edu.au/courses/technology-computing-maths/graduate-certificate-applied-data-science",2019-11-27T05:09:28Z,Free Short Course: Knowledge Discovery and Data Mining - Webinar 4,https://i.ytimg.com/vi/6KksleTYiQU/hqdefault.jpg,ITMastersCSU,PT1H15M42S,false,250,3,0,0,0,"hello people will start filing in just as your all file in you may have noticed we're missing the dulcet tones of guy coward who's normally your MC you've got me tonight Chantal Hale I'm with IT masters and I help to deliver our masters courses welcome to the fourth and final webinar for the knowledge discovery and data mining short course and tonight as we had last week we have Michael B wall with us who will be delivering this last webinar for us we encourage the asking of questions and chat during the webinar so there's two methods you can do that it's obviously the zoom webinar chat and that's so that you can chat with your fellow attendees so remember to change your to field through all panelists and attendees so that you can really interact with the other students and then of course there's the Q&A section and that's where you put any questions that you want Michael to answer later on or when he gets to the end of that area and we'll make sure that we have a few designated times in which to answer questions in the Q&A section also tonight we've got Jason Howard from CSU he will talk a little bit about the actually the graduate certificate that this is based on at the end of the course so if you want to if you have any questions about that or if you want to know a little bit more stick around for that okay Michael I might hand over to you and we can start on this final webinar thank you very much Center I hope you can all hear me all right all right so welcome again to the lecture series today we will be talking about decision support systems just to give a bit of context of what we're going to be doing today I'll just recap some of the things that we've done over the last four weeks so in session one we started off can you all hear me all right just checking Sancho can can you hear me yeah I can hear you fine excellent thank you so just to recap the stuff that we did in session one we pretty much looked at the introduction to data mining so the emphasis there was really what data mining is and we saw that it is really using a sheet of tools to discover knowledge from data to solve real world relevant problems so three main things that we saw from there the word to the hell and in the Y so the what involves the tools that we use the how involves how we combine these tools to discover knowledge and then the wire is why we actually combine the tools in the way that we do and why the results are presented the way that they are so as I mentioned last week una Y is really the most important part of data mining in session 1 as well we looked at the introduction to the data mining life cycle basically looking at data collection all the way to knowledge reintegration insertion 2 we focused on decision tree classifiers so decision tree classifiers is one of the mini machine learning tools that exist in decision tree classifiers there are two main things that we needed to understand there so the first thing is how to build decision trees and generally their algorithm for building decision trees are referred to as decision tree induction algorithms the second thing is how we use decision trees to make predictions in session three we last week we focused on knowledge discovery all right so knowledge discovery focuses on how we make sense of models and what relevant information we can extract from the model so pretty much talking about interpretability there we looked at three main things so the purpose which was really to define an objective for the data mining tax knowing the tools to use and then discovering the knowledge and the second thing we looked at we looked at there was the techniques that were used so we saw that broadly we can classify the techniques into supervised and unsupervised learning algorithms generally when we say supervised it means that the data center were using are labeled and when we say unsupervised the data set that we are using is not labeled and then finally we looked at interestingness measures so we realize that we can discover a lot of rules with no leak in the form of rules or patterns however of all these that we discover which ones are most interesting so there we looked at interestingly some of the measures that we looked at work support confidence conciseness actionability and so on and so forth and then we look at how all of this worked out in a case study so having recapped all of that this week we are looking at decision support systems so decision support systems allows us to actually incorporates all the knowledge that we have discovered into a functioning system so the aim of this lecture really may not necessarily be to show you how to build a decision support system from scratch but it's to enable us understand what it is and what is involved in it so the skills required to build a decision support system is out of the scope of this subject and however there are other subjects within that broad set and subsequently the Masters and that will give us the tools that allows us to build right away from simple dashboards all the way to full functioning systems so the reason we are looking at this topic today is that as a data miner once you build your models you need to present the information in some sense all right so presenting the information in some sense often would end up to be used for decision-making and that is why we need to understand decision support systems so that for example once you have your prototype and then you have a software engineer who is actually converting that prototype into an application or software you as a data miner you have an idea of what goes into the building of that software all right so going on to the outline for today's lecture so there are three main things I'd like to cover in the introduction I like to look at the types of decisions that we make the types of decisions types of decision support systems and from there we look at the architecture of decision support systems in looking at the architecture we look at a case study and I reflect the components of the architecture within the case study and then finally we look at the requirements of a decision support system at this point is there any question at all on what we have done previously and what we are going to do there's nothing in the QA I'm not sure just typing them in but I think we can go on excellent sounds good sounds good all right so um first of all we may say that a decision support system is any system that aids uses in decision making process and when we talk about a system we are really talking about a group of interrelated software that is working towards an objective okay so this is a relator software could be user interface it could be database management system or it could be actually the data mining models that we have developed so generally when we talk of decision support systems we're really trying to support decisions decision makers rather than replacing dummy now when you go on from decision support systems where we do not have a human in the loop then we are talking about autonomous systems alright so no more systems are really systems that do not have a human within the loop beyond that we can talk about technological singularity where computer systems are self sustainable they become cures they are self-aware self improved and actually that is where we become extinct but X like saying we are far from that right now um so decision support systems can really help us make decisions it could be simple decisions as to for example what we should watch two decisions that has an impact like pretty cool members should be used for a particular flight okay so in technical terms when we talk about the first problem about what movie you should watch we generally refer to it as a recommendation problem in data mining some of the solutions to this generally referred to as recommendation systems and they rely on two main characters two main types so the first type is always what we call collaborative filtering so collaborative filtering simply means that we use the users previous history as well as other users that are similar to this particular user so for example John Smith is for the five years old he plays soccer he likes to drink beer on Saturdays and watch military-style movies all right if there's another individual let's call that individual Peter Smith Peter Smith is also 46 years old he likes to play soccer and he also likes to watch movies so the question would be what types of movies would be just knit like two words because Peter Smith and Jones might have similar characteristics on all other attributes we may use John Smith's movie types to recommend a movie for Peter Smith so in this sense we call it collaborative filtering because we're using knowledge from personas that are similar to the person that we want to recommend to the second one is content-based filtering where we actually use tags within the content to actually recommend movies so so at this point what I'm talking about are the tools that we are using the data mining tools that we are using the aim of this topic that we are discussing is how those tools are used to support decision so in this case a decision like you want watch movie when we talk about more serious cases like crew members we generally refer to that problem as a scheduling problem now this belongs to the broad group of problems which we call optimization problems which has various constraints and generally we try to optimize and objectives in this case objective might be the cost alright so in that sense let us look at the types of decisions that we tend to make or these systems will tend to make so two types so the first one is a simple view where we want to make a choice among an alternative so whether I mean you know have council not the second type is a complete view where we want to look at the choice options so in the complete view we are looking at how each of their attribute affect the outcome so for example in last week I introduced this data set to year so this data set is about individuals and the other diagnosis and in this case the diagnosis being cancer or not cancer now when we talk about simple view then the decision could simply be well John Smith to get cancer if he smokes tobacco that's a yes or no so a choice among these options however a complete view would be which combinations of attributes would affect his outcome so in this case if we are looking at the outcome being the diagnosis cancer then we can say well one choice option would be 50% reduction to work or 50% reduction in alcohol 10% increase in exercise or results in 70% reduction outcome now another option may also similarly results in 70 percent reduction in outcome so at the end of the day we have options and the user or the decision-maker can choose the option that best suits so in this case John Smith might say well I'm not a fan of exercise but so I'll go with the second option because it has the same outcome of the first so these are the types of decisions that generally we require decisions of assistance to help us to make then we may look at how the tiles of the decision problems are structured so generally we have two main types distracted and unstructured and of course we can talk about the semi structured with lies in the spectrum between structured and unstructured so let me talk about structured problems we are generally talking about problems that are well known problems whose solutions are defined so we can talk about manufacturing process for example the making of alloys we can say well if you have a limini on the ad scandium turret you get a metal that is before aircraft add magnesium chromium you get a metal that's before ship magnesium a silicon you get a material that is good for bicycles and so on and so forth so these will never change all right so these are fixed we can also talk about a production line of beverages right so in that sense you know say that if the bottle is between 70 and 90 percent full we accept it as good quality if it is above or below any of these thresholds we rejected so this also does not change and this is we say that the problem is structured and the solution is well defined generally we want to talk about decision support systems we are talking about unstructured problems and semi structured problems so in fracture problems generally the problems are not known so for example what is the likelihood of someone buying a bicycle giving his profile okay so this problem is not necessarily structured and to solve these we may use tools like data mining tools to train models and use those models to help us to make a decision on this in such problems we generally try to approximate a structured version of the problem and then use a solution for the structured version of the problem as a solution for the instructor so that is where it is a decision support system because the human in the loop takes care of that instructor nature within the approximation all right no and so in that sense we may say well what are the types of decision support systems that solves these types of problems now I must note that the primary difference between decision support systems and other information systems such as database management systems is the presence of a model to support a solution so when we talk about the types of decision support system we may talk about two main types the model oriented decision support systems or the data oriented decision support systems in the model oriented decision support systems generally we are looking for we assume that the model exists prior to the decision making activity so generally these models will rely on simulation tools to search through a set of feasible solutions to get you on such a say optimal solution now so for example if we say what are the options of making bicycles from 6 kilograms of aluminum sheet well there could be two options two bicycles in three hours because the machine has to cut carefully or one bicycle in 30 minutes because the machine does not care hard to cut their material so in that sense the the decision maker can take this information and then make a decision in this sense we are not relying on data to train a model but in that oriented decision support systems we generally rely on data to train models and then use those models to make our predictions so for example if a customer buys a bicycle what else would he or she buy in that sense we need to use historical data and then train a model to determine the association between bicycles and other items now we may also look at other types of classifications like document driving decisions of persistence which focuses on unstructured data particularly text data or knowledge driven which focuses on rules obtained from expect so for example if you have a diagnosis application meant for a rural area the decisions that are made might have been sorted from a medical expert that says well if your skin is pale and your blood short then you're probably suffering from fever or something right so in that sense we said is knowledge driven because the system is built on rules and the third other classification is communication given where it really focuses on collaborative settings so in this regard I have a couple of colleagues who work in the augmented reality area where one of the research problems they try to solve this if you have two people geographically separated and say for example one is on some mechanic and the other is not and there's a problem on a vehicle then is there a way that the person who is not the mechanic can actually make decisions through the help of the other person by augmenting their reality in this case the vehicle so that the mechanic can actually place tags and make an point so that the that the layperson can actually fix the problem so these kind of decisions of persistence we say they are communication driven because they are using communications to aid in the decision-making now in this subject we really focus on that oriented because we are looking at how we use data mining techniques to discover knowledge from data and subsequently use that knowledge to make a decision so at this point I'll probably pause and ask if there are any questions at all on what we looked at in terms of the types of decision styles of decision problems and then the types of decisions of our systems so we've got two right now just a quick one asking what ml means which was on the page before oh sorry but so that means machine learning machine learning great yeah then do unstructured problems become structured problems given enough data in history No so an unstructured problem means that there is uncertainty that we cannot model so generally when we train computers to make decisions computers can only make decisions if there's structure so for example if we talk about the decisions of sorry decision tree classifier to make predictions as to someone will have cancer or not what we are essentially doing is that we are taking that complex problem about someone having cancer and simplifying it into a decision tree and saying well if you satisfy this criteria then you are probably not going to get cancer so that feeling itself is a structure and it is structured and it is solving an unstructured problem and so that is why you probably not get 100% are crazy on your tools or machine learning tools all the time so hopefully that answers that we just got one other question any two different times for bike manufacturers and it was eventing in two hours two over three hours that was just an example so I was just trying to illustrate the fact that if you have a machine making tool and then that machine is to make a bicycle from a single piece of aluminium then of course you don't care how you cut the pieces of aluminium to make the bicycle but if you need to make two bicycles then you probably need to cut it and carefully so that can optimize the space on the aluminum sheet and so I was just using that to illustrate the fact that for a different outcome you require a different length of time so here my emphasis was on time and cost so if you're a decision maker of a factory that makes bicycle then well you need to decide whether you're going to emphasize on time or on cost so in this case I was just trying to use that illustrates that point so I don't make bicycles I just hopefully that answers that question all right okay so we'll carry on so now we'll look at the architecture of decision support systems now what I'll do in looking at the architecture of decision support systems is that I'll present a case study and then we'll look at the components that make up the architecture and then link those components back to this case that and see how it reflects in reality so um this case study this was a project I was done by xayide in 2016 so it was pretty much a proof-of-concept decision support system that was aimed at predicting things like volume inflow evaporation and release rates from them so obviously we know that this is an important problem especially in this drought so being able to predict these variables of course can allow farmers to make preparations that can allow a lot of policymakers to put the right policy in place and make timely interventions so I cannot overemphasize the importance of such a problem and such a solution and so to make such a decision support system the general architecture has four main components we may talk about the dialogue primitives which is nothing is just a fancy name for a user interface we may talk about the database management we may talk about the model management and then we may talk about the solution base management now when we talk about a user interface the user interface actually is the main in point of contact between the decision-maker and the whole system so arguably it might be the most important part of the system so because of that we need to ensure that it satisfies certain characteristics so for example it needs to satisfy support for model choice so the user needs to be able to to decide which model he or she is going to use for solving the problem of course you can define default models but ultimately it is useful to allow the user to be able to decide which models to use for a particular problem because they use user may use background knowledge may use experience may use some knowledge that is not available to the decision support system so for example if you have a data set and that data sets let's say the class variable is numerical then you may the decision support system may use regression analysis because it sees that data type to be in America however to the user that might be considered categorical or the user might want to do categorical analysis on the class variable so in that case the user needs to be able to modify the model that is used secondly we need to be able to support the reasoning so if we say for example these three products in the shop are always going to be purchased together and therefore when we stock items we need to stop them always together we need to provide justification for that and that justification could be for some of the interestingness measures that we saw last week so it could be things like a support or the confidence now just drawing from my experience on some of the projects that were done in the past I worked on a project where we tried to predict civil unrest events okay and there were two one major aim was to ensure that our crisis of our predation we're good so we wanted to say okay there's a 90% chance that there will be a swan rest event maybe in Sydney for example but that information is is okay well ninety percent chance there's going to be a protest in Sydney where is it gonna be what is it gonna be about alright so what we needed to do in the decision support system was that we needed to provide context to the predictions that we're making to support the predictions that we're making so for example to say there's a 90% chance that there's going to be protests in Sydney because of climate change rallies okay or because Mariano police is giving a speech or because there are four tiers and no rallies so this gives better context to the decision maker to decide whether or not they are going to deploy police to keep peace and that can even tell the type of intervention - for the decision-maker - to make so it's really important to provide support for the choices that the decision support systems are on it now generally when you build a model a data mining model you generally would be doing predictions but you can also build a tower my name all those subs provide context to the predictions that you are making the third point is that you need to be able to provide support for choice and optimization of the variables so what this means is that we need to be able to allow the user to tune the parameters of the variables and last week somebody actually a question about what is the best confidence value use now if you're a decision maker that is where you this is where you can vary those parameters like the threshold for the confidence lolis so for example you might say well your default value might be 50% and as a decision maker you might say well I want to look at the rules or the knowledge that is discovered when the confidence is 90% and act on those rules it only and so you need to have the ability to be able to modify those decision variables and finally we can talk about a graphical interface at a graphical interface obviously it's very important because that gives a holistic overview of the solution and the problem in such a way that is not too cumbersome for the decision maker so graphical interface is always very useful in decision support systems now reflecting these characteristics of a user interface in the case study we may look at the water DM decision support system so this was developed as I mentioned by Zoe in 2016 and now this is a clean interface that allows you to do some visualization it allows you to determine what models you use for the knowledge discovery I think the prototype allowed for decision trees but obviously when you develop such tools you have multiple models that a user can choose from that allows you to make predictions of which is what we obviously want to make to track the prediction are crazy and so on and so forth so generally when as we I mentioned really the user interface you want to be able to allow the user to actually interact with the underlying models in such a way that is flexible so then we can also talk about after looking at the user interface we may talk about the database obviously we need data to build the models and therefore the database is an intrinsic part of the decision support systems sometimes the the user might just want to create artistics about the data so so it is important to have the database to be able to interact back and forth with the user interface we can also talk about the model base or the model based on model management that is where all our models are stored and money so if the user wants to make a prediction that prediction will actually access the right model that is relevant to the problem if the model does not exist then the should be a system that triggers and new models with train and stored to be part of the model base and we may also want to talk about a solution base where we store all previously previous predictions because sometimes you may want to actually make a prediction on an item that already has a solution so rather than actually playing a new model or making a police on a new model you may just call upon the solution or you may also want to track how your predictions are behaving and so all of these put together make the decision support system now remember a system is nothing but a group of interrelated software working together to achieve a common objective so in this case decision support now so as I've mentioned we look at how all these components are reflected in the case study for what a DM so for what a DM so for what a DM we see that the database is represented I started from the Bureau of natural metrology a New South Wales water wetter zone and so on and so forth now on the local server side there's a scheduler here we call it a cron job so that is nothing but a scheduler that actually collects data from these data sources periodically and then that's splits the data into what I relates to R or water or relates to weather and then here we do the pre-processing so this is one of the topics that we cover in the subjects as well pre-processing so when you have data it might be that that's in the sense that there might be redundancies there might be missing values there might be duplications and so on and so forth so here we do the preprocessor to clean up the data so all of this phones part of the database and then once we have to clean that we might train our models so the models that we talked about particle we talked about decision tree classifiers so we might invoke the the models to be trained on the data that we have so here we can refer to this as the model pace now note that the user is interacting with all of this true the user interface which is nothing but a PHP website okay and then obviously we have a database that stores the previous predictions and so on and so forth and we may say this is the solution base so here at least we see how the various components of an archetypical decision support system is reflected in this case study of what the Year all right now so now we'll probably move on to the requirements of a decision support system now generally when we talk about the requirements we talk about two major things we talk about simplicity and consistency now one we say simplicity we are saying that we want to organize the information in such a way that the human being can understand or the decision-maker can understand or want to take into consideration the cognitive capabilities of the decision-maker so well fun fact normal human beings can handle I think for concurrent things at once so I mean I can barely manage two concurrent things so I'm not about super smart people and talking about normal people so if you bad a decision-maker with a lot of information it's probably not gonna make the decision-making process any better so we are not saying that you simplify the problem we are saying that you simplify the presentation of the problem so that the underlying complexities are still there so in this way if the user wants to get more information about a particular component the user can get it so for example when I talk about simplicity this is what I mean so these are two interfaces for investment one is clean and you are able to dig deeper if you want and the other presents all the information in one go so if you probably design this sort of user interface it might be the last user interface even for any company so you might want to take this into consideration the second important thing is consistency so when we talk about consistency we may talk about two main things so internal consistency and then the needs outcome consistency when we talk about internal consistency we're talking about data representation how we handle data and so on and so forth so supposing you have models that are present that are predicting probabilities now Miss Ida you decide to present probabilities for all the models to be in percentages or to be in fractional values so if one model presents probability as 90% or in a percentage then you do not want another model to present a problem eating in fractionally so that is one thing you need to consider when we talk about consistency we're also talking about data consistency so in that one data set you don't want to use last name and then another does its user name so that does not give internal consistency now when we talk about needs outcome consistency then we are saying that we want your outcome to what the user is expecting so if the user is expecting a yes or no answer then you don't want to design a decision support system that gives probabilities because that might not be useful to that user now again reflecting on some of the work that I've done in the past still relating to the civil unrest predictions so when we started a project we use various metrics and then came up with one metric that we call the risk score now house risco was understandable to us because that really presented or reflected the risk of an event it wasn't entirely understandable to the clients and for the client simply wanted categorizations okay characterization like low medium and high risk that's all the client wanted so in that sense we had to convert these risk scores into those categorizations and to do that we needed to convert them into probabilities first and converting probabilities into categorizations is not that straightforward well we might say well anything above 70 is high anything between 50 and 70 is medium and anything below 50 is low but generally those problems are not that easy to solve in that sense in this case not only did we have to consider the probability of an event or caring we had to consider the scale of the event okay so for example if you predict that is going to be 90% chance that there will be civil unrest and only 1% ends up well it's not word for the decision that artists and police are going check that person right but if you can't combine the probability that it's gonna be 90 the sense that this is going to care and you're gonna have about a thousand people sign up then that is something that we need to consider so we needed to consider all of this information to classify or to categorize events as low medium or high risk and if you've done some data mining in before you know that this is not a very easy tax to achieve in that sense and then similarly on another project we actually that was for us to predict exploits of vulnerability so we started out by predicting yes or no so basically what I have an opening to be exploited or not but a client wanted probabilities so since two different clients they both wanted different things and so we had to design the system such that it conforms to what the client wants it so when I talk about needs often consistency it can really be summarized by this alright so if a client wants this don't present the client for this that's pretty much what it is it might make sense to you but in the end of the day it was a decision-maker who is actually going to make those decisions so um I think that brings us to the end of the three main things that I wanted to talk about in decision support systems so basically we looked at the price of decisions types of decision problems types of decision support systems we won't talk about the types of decisions we look a simple view or complete view I wanna talk about the decision problems we are looking at structured or unstructured problems I will talk about the support systems and we are looking mainly at data driven of model driven there are other ones like communication driving knowledge driving and so on and so forth then obviously we looked at the architecture of decision support system which has four main components so there we can talk about the user interface which is arguably the most important part of the decisions of our system we can talk about the model base the data base and in the solution base and of course when we talk about requirements we talked about the fact that the system my be simple it doesn't mean that they are the lining models no mostly simple it just means that we present a problem the solution in a simple way it must also be consistent that means that what's the user or the decision-maker expects conforms to what we actually provide us a decision support so I'll just summarize this topic by drawing my own experience now again talking about the predicting of civil unrest so one we're working on this project there are two main things that we were taxed to do so to ensure that the accuracy of our predictions were high and also to ensure that we provided justification or support for whatever predictions that were doing so as that of my - we predominantly focused on these two things so one of the meetings we went to we presented our results and I said oh well we really have high precision recall and f1 score and we're getting something like zero point eight zero point seven five now in that presentation the researchers that were sitting there were quite intrigued and they were like oh how did you do this and we're going on and rambling on about the techniques that were using by the analysts that were to use the tool we're not really fascinated by it alright so in a subsequent meeting what I did was that I converted all of that that were saying into a prototype the prototype was nothing but an interactive graph that had spikes so the spikes indicated the probabilities of an event occurring and when you click on a spike that gives you some supporting information about what the event is about when it is gone okay that sort of information in the subsequent presentation everyone was much more appreciative of the information that we are presented again the same models the same results but just presented differently so if there's anything for you to take home today it is that decision support systems are really important and it is the end game of the data mining process so once you do all your data mining build these beautiful models that have high accuracies it is only useful if we can embed them into a decision support system to help make decisions so at this point I'll ask if there are any questions at all about the lecture that we are giving No okay so we've had a few questions and some of them probably less relevant than others but we'll go through some we had somebody asking about what a DM is news in New South Wales wondering who what was in playing Queensland a few years ago when those severe flooding and Hamilton had some feedback on that I just thought I would highlight that verbally check out the QA for that information about I think about the discussion about decision support systems could this be used for matching missing persons and deaths of unknown people maybe see our story had this last night yeah no absolutely so in this case it will depend on what data would be available to us it could be data from traffic cameras it could be data from schools or that sort of thing but ultimately anything that we can any data that is relevant to their problem can ultimately be used in a decision support system so it would depend on the specifics of the prom but yeah most most certainly actually that reminds me of a project a colleague has been working on and that on entity resolution so that research is trying to identify whether any two profiles are the same in it in a database so sometimes you might have an individual in one database calling himself William and then in another database calling himself bill all right and so then the question is well how do we know that these two people are the same so there are some complex techniques in solving that sort of problem like using graph dependencies and so on and so forth so most certainly for the missing persons case I would assume that we can model it as an entity resolution problem and I use some of these techniques to resolve but they are very complex techniques to use what I was muted nor does I still don't understand the difference between a data miner and a data scientist all the steps I mentioned so far are done by data scientists okay to be honest the term data miner and other scientists cinoman synonymous nowadays all right so this as far as I'm concerned there's no real difference so some people might call themselves data miner some people might call themselves data scientist the skill sets that you require to do both essentially the same alright so I think it's a it's a choice personal choice for people really and then we've just got one last question so far you supervised learning another way of suggesting that a particular DSS has no programmed model related to the problem but the machine is allowed to do some clustering etc to come to a conclusion yeah that's correct that's that's actually well put so when we talk about our supervised learning we do not have labels and so the decision support system may try to discover some patterns now these patterns could be clusters it could be Association rules it could be anomalies and so on and so forth so that's exactly on point yeah that's correct so just just wrapping up then I'll say that over the last four weeks we started by looking at introduction to data mining then of course we went on to look at decision tree classifier and how we can use that to how we can build this increase and how can use that to make predictions and then we went on to look at knowledge discovery from decision tree classifier basically the interestingness of knowledge that we can discover and then finally we have looked at decision support systems now in the subject we cover a few more topics like crop data detection we look at privacy-preserving data mining clustering time series data mining and many more now all these are tools or techniques that are relevant to solving various types of problems so a tool like decision tree classifier classifier may not be good for solving an unstructured sorry an unsupervised learning problem in that case you might want to use clustering so I entreat you all to enroll in the graduate certificates if you can and at this point are probably handover back to Chantal and Jason to give you a bit more details about how you can go into the gosset and hopefully in the future in the master's program thank you so much for that it was a really great lecture I was obviously a little lost at times but I hope oh good everyone else could follow along so as Michael said we this is part of the subject that's part of the graduate certificate in applied guided science and so we're going to hand over to Jason house from Sears who will talk to you a little bit about that course itself Thank You Chantel and welcome everyone my name is Jason health and here to talk to you about the graduate certificate in Applied data science thanks to Michael for a great presentation look I've just copied into the chat window the the URL perhaps don't go to it right now but the URL of information about the full graduate certificate for those of you who are interested in moving on from this short course into a more formal program so let me tell you what that's about and also a little bit about Charles Sturt University who you would be studying with so not sure what you guys know about Charles Sturt it is predominantly originally based University so you can see there on the screen some of the photos from each of our different campuses so we have campuses in large regional centers such as Aubrey wagga wagga Port Macquarie Bathurst but the thing many people don't know is that CSU most of our students in fact our online students so CSU has around about 40,000 students 25,000 of those study online so one of the largest universities in Australia for online learning and what that means is you don't have to attend campus at all you can you can do your degree wholly and solely online without ever setting foot on a campus and for many people who are working who have families obviously that's an attractive proposition so moving to the next slide I just wanted to let you know that we are a leader in online education and the kinds of presentations that you've seen by by Michael by zahid in this short course these are standard features of CSU online programs okay so for those interested in doing what we call a graduate certificate and I'll explain that a little more in a moment effectively though it's four subjects four subjects to get a postgraduate award we have this graduate certificate in applied data science kicking off for the first time next year 2020 so you would be the first lot of entrants into the program and the first lot of graduates and and being an early adopter in areas that are very employable is often a really good strategy so let me just briefly tell you about the program first of all it has what you can see at the top there one core subject and a core subject is a subject that every student of the course has to do so that one here is foundations of big data analytics and that covers areas such as introducing you to the practical concepts behind analyzing big data extracting value from masses of data so you be looking at techniques such as data mining machine learning and AI now that subject is is core the other three subjects you get to choose depending on where your interests lies so you get to choose those from the list of restricted electives oh yeah look Robins asked a question there about programming principles it is about a programming language it's based on Python so it's up to you whether you do that subject or programming whether you have that programming knowledge or not but you know programming as a skill is useful in many different areas including areas of data analysis that that it would be a good choice so that's based on Python just going through a couple of these data mining and visualization for business intelligence you're looking at basically the application of visual datasets so how to how to present views of complex data trends and patterns so it's all about presenting those views to other people and also helping to interpret those to other users so often data you know needs that visual representation to make sense database systems is all about more traditional database designs so you're looking at you know your traditional SQL type queries building tables all of the aspects around data integrity within garter based systems programming principles we've discussed Internet of Things is fairly self-explanatory there's a couple of statistical subjects where for those wanting to go into that side of of data science are available to you so we have scientific data analysis and that one is what you looking at things such as correlation there is an introduction to are someone asked about that in the chat box there is an introduction to our programming in that subject data classification and descriptive statistics you're looking at probability all those fun things so that is an extremely useful subject the other statistical subject there is experimental design and analysis and that's really all about being able to have the knowledge to design experiments but also to perform the analysis using the appropriate statistical protocols on those experiments so you're looking at things so which is linear regression multiple comparison methods analysis of variance and covariance even an introduction to survey methods so being able to to get the data but also to be able to create experiments on a small amount of data and to validate those the last subject there that we haven't mentioned ITC 573 data and knowledge engineering is all about interpreting data using an ensemble of trees and clustering also looking at cleaning data so all the pre-processing tasks that you would need to perform before you know going on to do some real heavy number crunching also looking at the preservation of privacy there in that subject power bi I look of it I'd have to check on that I don't I don't believe so I don't believe so so look that's an overview if you go to the link that I've pasted into the chat window you can actually click on the subject numbers and I'll show you that in a minute towards the end of the presentation but that's an overview it's only for subjects not not six okay you're gonna paste again okay there we go Chantell if we could just move the slides along and I'm sorry for not showing my face tonight I'm not I'm not shy but I'm just traveling tonight so it's the internet is a bit dodgy so I don't want to use the video so in terms of hello we'll take we'll get to that in a moment in terms of the environment where you do your subjects this is a screen dump of what's called a learning management system so all of our every subject that you enroll in all of the four subjects in the grad cert you would get access to this learning management environment where you obviously meet the lecturer you go through each of your topics one by one there's your assessments in there that you have to complete you're able to check to other students you're able to obviously get not just written information about the topics but videos podcasts it depends on on how the lecturer designs their particular site it's your one-stop-shop you can also submit your assessments through each of these portals so everything can be transacted online okay so thanks for that someone's corrected me there and pasted it to all the attendees so what are the entry requirements just quickly bachelor's degree or higher you're pretty much straight in but if you don't have a bachelor's degree but you do have industry experience there's a possible we can possibly allow you in as well it's a bit hard perhaps to to verbalize exactly the industry experience that will get you in but certainly if you've been working as a programmer in network engineering in security even in those different areas as a professional for three or more years it's it's very possible that we will allow you into the graduate certificate we need to look at you know the position you're holding you know we don't particularly want very junior level positions but if you've been working in the industry three or four years it's certainly worth putting in an admission even if you don't have a bachelor's degree one of the good things about this as well is that it's a pathway whether or not you want to do that into a master's program now that could be with CSU it could be with some other institution but if you have a four subject graduate certificate typically you will get straight into a master's and you will get depending on the program of course all of the subjects is credit that would certainly be the case at CSU where you would be eligible for credit and entry into our masters of information technology based on what you've done in the grad cert ah yes Robin that would be that would be sufficient most certainly upon me now a lot of people don't necessarily know what a graduate certificate is where it sits in the overall qualification hierarchy at the bottom and when I say at the bottom these are university level Awards rights her still pretty high at the bottom you have your undergraduate degree so your bachelor programs then you move up to your graduate certificates and graduate diplomas their postgraduate programs they sit just below the masters so they're higher warts they're higher than bachelors they're only short and they do allow you as I said to go in to mark certain masters programs and gain credit at the very top of the tree typically more for researchers or people wanting to work in academic positions are your doctrines but I guess if you're doing research in industry as well doctorates can be useful and once again you know starting out with a great cert it may seem a long way away but there is certainly a pathway to a doctorate from a grad search through masters so that's the qualifications and where they sit so moving along time commitment how long in terms of how long does it take you to complete but also how much effort do you need our subjects are designed for you to do well if you engage with them around about ten hours a week ten to twelve hours a week that will vary depending on the subject in any exposure you have how new you are to study but on average around about ten hours a week per subject now a subject a session a session lasts around about three months so it would be ten hours per week over that 12-week period is what you're looking at for one subject although you can enroll in two so you could do you know 20 hours a week and finish half your graduate certificate in around about that three-month time period so overall the graduate certificate will take you if you do it at two subjects at a time and that's that's fine two subjects part time is is a recommended load you would finish in six months so you can have variable study loads you can just do one you can do two you can do more if you're feeling particularly courageous you can take losses of absence so maybe you're starting in one session and just do a subject and then you feel next session look I've got really busy at work I need to have a break you can just put things on hold and come to it when you're ready so there is that inbuilt flexibility so just moving along now to credit some people will ask about about credit credit may be available for any previous postgraduate study that you've done but it needs to match obviously it needs to match sufficiently the curriculum that we're dealing with so if you've done for example a master's in statistics then it's very possible you could get a credit for one of those statistical subjects that I showed you industry certification not so much in this course we you know if you did have an industry certification in in some kind of data mining area we could look at that for a possible credit but you know it would be on an as-needed basis we would have to look at that to see whether you would qualify we don't give credit for work experience work experience allows you for example to get into the program if you're working in the industry but there's no credit available for that credit is assessed as part of the application process so one when you apply if you apply you would upload any documents around that and we would look at those as part of the application process on both whether you're admitted and also whether you could receive any credit so just moving on to cost important consideration for subjects you can do the maths yourself for subjects in the degree for Australian and New Zealand residents three thousand three hundred and fifty per subject for international students three thousand five hundred and fifty per subject there's also a small student services and amenities fee I don't have the exact number they're about thirty dollars a subject you can certainly look at talking to your accountant about about perhaps are claiming this as a professional development expense if it's if it's related to your work in some way there's also a fee help a government fee help programs where you can defer those costs so there are different payment options I won't go through them here if you look at the website that I've posted the link into the chat window you can find out some more information around that hex sorry is is different that's around a Commonwealth supported courses fee I mean fee help is is help Sophie help is what we're talking about here not hex in terms of applying we have a number of intakes next year 2020 is when we kick off so there are a session starting in March May July September November so lots of different options for you to start you can apply now for the March intake and really it it's just up to you as to as to win best suits you if you're choosing to start when when that start time would be it cost nothing to apply now I've got the URL there says u DD u dot au forward slash apply I'll need to check that whether that still current we can check that in a moment let me just have a look there and see if that still works the other one that I've got and I'm gonna paste it into the chat window so all panelists and attendees won't forget that one study dot CSU dot edu dot are you are all subjects available through all intakes no they're not so depending on on when you start yeah you'll have different different subject choices Stannis now your core subject ITC five seven five from memory that should be available into all the all the sessions you'll certainly have a choice available so you won't be constrained for choice but not necessarily everything runs in every session so let's just move along Chantell look if you do want any more information and I'll actually start sharing my screen in a second just take you through the the very start of the application process you can contact the course director his name is Sebby riemann so SI riemann at CSU dot edu dot au you can also contact me Jason how earth Jay how are that CSU dot edu that are you always happy to talk to prospective students so if if it's something you want to have a further discussion about just please please get in touch always happy to chat and yeah I mean give it some consideration I mean obviously study is a big commitment and it's something that you don't do lightly but you know it is a new award for us in a university that specializes in online education with a great reputation in in fact the leader in in these types of courses in Australia in postgraduate IT programs so let me just try and share my screen Doug no you don't need to take RT C five seven five first it's not necessarily a a prerequisite can you guys see what I'm sharing now Chantel can you just give me a verbal in I can see what you're sharing yes straight okay cuz I just I'm on a laptop and I just so if you go to study dot CSU dot edu dot au they start your application it's just a matter of answering a few questions there's an apply Now button if on the applaud we might be losing audio a bit unfortunately with the addition of the video house everyone else's audio okay I can maybe just take you through with a few clicks rather than doing much talking so I click on as a domestic student it will ask me for the level of study and I would say postgraduate and then it would ask me if I want to do teaching for some reason I really don't know why we say no and then it says ok go to the online application sticklers clicking on that and then you need to either register or login if you haven't registered before I'm going to just log in here and it will give you a few instructions around starting an online application but we can just go down here to the start button and say domestic and we would want postgraduate by coursework graduate certificate that's what we're after domestic student and we just you can always come back to it later and it's got all my details there including my date of birth which I'm not so willing to share because you'll know how old I am but after that you know you go through your sections of Education which course any documents to upload and so on so I'm just gonna stop sharing there now oh do I Jason sound look at him NBN customer sorry about that thank you for that and hopefully the zoom recording should come through pretty smoothly on that one i if I was a guy he would have come up with some clever joke about sharing your data and data mining and bla bla bla but I couldn't come up with it quick enough so we're rapidly hearing the end of of tonight's webinar which means the end of the short course the exam should hopefully be up tomorrow I'll send you an email once it's up so that you can start filling it out and and getting certificates thanks again to Z hidden Michael for each doing two weeks of a really engaging short course with a lot of really interesting things there and for Jason for coming through and and helping us out here and obviously if a guy for doing a much better job of this emceeing thing than I can manage if you have any last-minute questions I think now is the time for them but we should be wrapping up right about now I always think I should have some background music while people type in questions conceivably but we're actually we haven't lost too much time at the end of this 15 minutes over for the last one when we're going through a new course is it's pretty exciting and again any questions can go on the forums Jason's emails or see if I can share those on the screen again oops find me zoom again so I can share my screen the jason's email there hopefully that's what you're getting and yeah like i said any questions post them on the forum and i hope you've enjoyed this and learnt a lot thanks Chantal thanks everyone appreciate your time as well and a big thanks to Michael and zahid from asset Charles Sturt as well great job and thanks to all the participants to hopefully Michael will be seeing some of you in his subjects when their next run absolutely I look forward to meeting a lot of the participants yeah she'll be exciting and as we said before this is just four weeks of an entire subject that would have many more weeks and as far as I know Michael goes over time a lot so it would definitely be cover a lot more material thanks again and I hope to see you all next time we have a short course biology "
_h7Nr6--gUQ,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-04-20T22:18:07Z,Data Mining (Spring 2016) Lecture 25,https://i.ytimg.com/vi/_h7Nr6--gUQ/hqdefault.jpg,UofU Data Science,PT1H3M42S,false,114,0,0,0,0,"you okay so this is the last lecture in here of the semester there's no class next week monday which is is going to be the interim test ghostly the last lecture let me get the lights alright so so at this point we've given feedback to to all I believe all the posters so that's it so if you haven't seen that impact in canvas you should you should go and look at it if you you know for some fraction people we just said approve or there's pretty minor comments and when you're done with those just send those two to Chris Coleman via the instructions in the PDF of the project so you put data mining posters and all caps in the title I don't find them all if if you got more if you either got more expensive feedback or you kind of were said you know there's this is pretty far from what we are expecting um you know please take some effort to like change up the poster and then feel free to send just me the email another copy the closer i can give you either tell you it's good or saying actually there's a little bit just in case so but I know some of you already did that that's great and I think most the posters now are going to be in great shape what's the deliverable for final summary this is like a resubmission of this it's I don't think there's anything on canvas so the final the final grade you there's I get 15 points out of 100 project points will be assigned for me at the poster day and doing doing that so the points will come from what you have to when I go run to the posters at some point I need to have you explain to me something and it can't be mean not to its kind of its again one of these parts where if you kind of if you do all the requirements you're going to get all the funds right but so you have to give some explanation some interactive explanation of your poster you have to you also have to write summaries of other people's posters to be there and so those 15 points will be given on the on the poster day so you should have a poster day right so someone from your group either should send it to Chris Coleman to make sure it gets printed and if if that you know so bad you need to do that in early get all the points for if now you can print out your own some people have kind of gone and said well I'd like it bigger or like a printed bigger or I'd like to keep making modifications past that and I've gone and printed in kinkos or something I'll cost you like 50 60 bucks or something I don't know if the dentist but if you want to do that you can go do that if you want to just print out some slides I think one group has two versions one looks life or without but in the past people put up some slides they don't look as good you can do that in your own though as long as there's something there on the poster day you're going to get those those 22 points and you can get more than that again they'll be voting on everyone's favorite coaster so you know you can you can earn forget what how many bonus place you can get in the past has been like at least five bonus points for getting the favorite coaster this can be longer great so put some effort into those look forward to it so any questions on a match or the end of end of end of the semester up question poster question not sure about the only genius yeah my concern there was on the new project it says like content to the forking to RC on your poster and some of those can't have that on there and Leslie right to be 18 doc for not putting a problem he had his but we were unsure what yeah so I mean Evans son hey I'm so high I list on so so it's it's it's fine to its find out a little bit of text air just like full sentences and like paragraphs or not not directly to do it you can also have it so you'd have something to point to as you're describing it so as I go around and look at the posters i am going to want to like hear these things from you in the in the two to three minutes that then you're going to have to explain it and so i'm not going to read any texts i'm not going to have time to do that but you're going to want to guide me through the poster ezer explain that and then that was a corresponding part should be on your poster so um yeah so so likely if you have you know if it's just there's a big block of text you might get docked some points for them right there if it's a little bit of text on glass of wine so I'm just trying to tell you what will be most effective during the business issues all right so um ok cool and then this will the last homework don't forget about that but I think it should be similar in difficulty to the frequent items home and time all right oh and don't don't forget to do your your class surveys especially if you're going to give me good scores on those if you're not going to be good scores you can forget that's that's that's fine ok so but I you know I think there's lots of really great stuff that we covered in the class 4a long material lots of great resources so great oh ok so the last topic today will be graphed sparse ification ok um and so this is this is another topic in like understanding doing mining and stuff on on graphs um the kind again we think we have some grass we have some input graph which is the vertices and edges and the the goal of this is to say well these graphs are can be very big these graphs can be a pain to deal with because of their size and what we'd like is is a smaller G prime that basically so there are a couple of couple of perspectives on us why is this important well so there have been this view that the that the number of edges in these graphs like as I was mentioning on Monday that there are these models of graphs that did they have these purely random edges where each edge was either it was their equally likely in this random graph bottle and these were not quite the right model there is more of a mouse based on preferential attachment and other properties that are more realistic and so another thought of these grasses as they grew that you probably have some property that there was some constant where the number of edges would be roughly some maybe this was 20 times this number of vertices so they go on Twitter who follows if you follow someone are they falling back either way this gives an edge and maybe the average on average someone has 20 followers and so then it's constantly be 20 so for ebony 20 inches or maybe this would be head because follower follow back they they give one edge and so I mean average 10 followers so it feels 20 20 followers on average then I guess those coupled with maybe this will attend but there's think of sub concept how many friends here on facebook that's covered larger you probably an average number of friends maybe closer to some like you know how many links are there on a webpage right you know maybe that's closer to 20 on average right so one question is what is kind of these rate constants how we measure this / graph well it's not hard to estimate this value or to count this value if you have the whole graph you could just keep you keep a counter but a better question is is this the right model um there's some people who still think this is the right way to think about it but another view is that the number of edges is the number of vertices to some power so 1 plus beta and then data say is usually in some range 0.1 to 0.5 okay so there's some it's it's the number of vertices every vertex at least has one edge right then did some power right so it's it's not you know assuming that you don't have multiple edges doing this this is the same thing on then you know this power is at most two right you know there's servoz right well let's let's look at some numbers here let's say the number of vertices is 1 million okay let's say the number of vertices is 1 million what is number of vertices to the power of 1.1 this isn't this must be huge right some polynomial of a million it's supposed to be unbearably large number okay any guesses quick guesses top your head shop enough what 22 million 44 million 1.2 mm okay you know initially if I think I would have thought it would be larger but you know it's only this is about four million okay so if it's not that much larger this is actually a concept of four right I said the constants probably maybe 20 or 100 and a 1.1 a million to one point one is only four million right if I do 10 milli I think the constants like five it's like  from 10 million goes to 59 right so this isn't so bad if you go to 0.5 now starts going fairly it goes more I written this down I forgot what is uh um yeah what is 1 million to the power of 1.5 what is the to the five billion right yeah so this is 1 billion so now this is bigger right the differ between a million and a billion terms of storage that's a factor a thousand difference right that's that's kind of that's something else is going unused right so it's somewhere in between maybe of time of constant for diff constant a thousand in this space we sit between 10 and 20 this seems reasonable ok so so but from a modeling perspective so these numbers are about the same right so these pic if you just look at one data set you actually don't know is it is it this model a constant types of reverses or a power of the number vertices but what difference does it make in terms of model like what does that tell you about these social networks so kind of this model again says as they grow on an average each node has more edges all right so as they're growing each edge gets more and more edges right so as Facebook is much likely than you actually delete someone for your friends loss this kid does have you know so you intended for you meet more people you add more people write this seems this section seems much more reasonable and if you think that's quite sure on the web also what they just end up more links are our screens are bigger at least a have more resolution in them so you can fit more on the screen it is intend to have like these headers on the top that have lots of links in the head these counters links right so and it's home I'm not sure that's turn up just probably true I don't have any data to back that up that's my guess so so I think this model makes makes more sense so um if you're saying this model then model that the number of edges is growing with the number vertices then you know you need to start out as this number you not explored quite as much most of them has been to fix V and then reduce the number of edges so reduce the headset so go down to have the same vertices in the graph so I have the same say number of people but you want to reduce the number of edges and so if this saying that edges are growing faster than ever vertices to reduce the edges back down to something that is going to look closer closer to this it's going to end up even go ok ed so there's been kind of sick so there's been definitely some work at this area trying to do this and you know if you talk to various various people have been developing these methods they're kind of saying you know these are we've kind of started from what can actually be done we're ahead of the pace of the size of grass but eventually he's going to become very practical and useful we're going to talk about two versions of this right now one is is going to be general and mostly theory and if you talk to people and do this I say soon this is going to be useful and some of the other stuff like with thee with the matrix sketching or the zooming algorithms on the data sets we ran in class those days sets weren't big enough for those useful but those are now useful they'd say something similar is going to happen here is the grass grow bigger I have some doubts but it it it may happen I mean but right now Facebook is already has billion people on it and I don't think they actually use these techniques so the other view of it is going to be is more restricted we'll talk about planar graphs or they know exactly if we planar but usually so we'll think about particular something that's useful in or at least low dimensional and a good example will be like road networks and so these are going to be more practical some of the tools we'll talk about I'll talk about what is known about them and then I'll say what you actually do in a practice to get them to work okay but this is kind of fits in with some of the other things in class and so this may be something they kind of keep in mind is possible going forward okay there's there's another part of this story that i should mention before i go on um there's that there are remember i talked about that there was like this MapReduce and there was a Duke and so these were kind of these general purpose tools and then there were these specialized tools that were these these descendants of them that work for a particular type of data one of the most common things within these specialized one the big class and things were things for graphs and there are these packages which are not exactly in this framework but there's something called GraphLab there's like giraffe I think it's how it's spelled but this one I think is directly in the dupe infrastructure this to it there's there's some version of I think there's some version of spark that works on graphs in somebody's so they're so specialized some of these things and these are things that are very scale scalable and that they can work in parallel very very often and so there's been some of this stuff fits into dealing with when you have huge data you may need to use this very parallel machinery there's been kind of a another some pushback that's saying that if you're careful how you store your graph you can do you convert almost any graph that may be aside from the facebook graph any graph you can almost publicly get you can do this just in the in the memory of one machine so most of the graph mining ore processing if you're careful that you represent it very well so this means that you need to be very careful how you store all the engines right if you have a vertex you want to say store the edges you don't want to store it as a major crew that's gonna be very sparse but you want to just maybe not store that it is directly but some compact representation of it you can get these to fit on one machine and then you can run these algorithms by few iterations or something so think of doing page rank and you're doing this by just doing this power method you never actually densify the matrix you always keep around something sparse and so there's some there's for some of the stuff you can do you actually don't need this very parallel system but these require less careful implementations and so a lot of people still use these these packages for dealing with large graphs but they may not give even that much of the speed up even though you may have 10 or 100 times more machines okay so I just want to kind of give some context of this before I dive into this into some of the actual techniques so I think these techniques there they're out there they're in discussion when you talk about large graphs this is often what people talk about so I think you should know about it but I think you should also be aware that they may not the other things i think we tom class have all been kind of their used clearly use some place and so some of the stuff i'll talk about first it will map to similar things in the rest of class but i don't know if they're actually kind of used in the gin study with graphs for English but ok so that's the caveat on this ok um ok so now ignoring this is tell you that you can not worry about it too much but just just to give you the general picture because there's some really cool map that goes on here the idea is now i want to go from a a graph and where i'm going to think that the number of edges is going to be something like the number of vertices to say 1.5 and i'm going to reduce this to a graph g prime which is going to have the same vertices in another edge set trying where e prime is going to be a subset of the edges and i might change some waiting on these edges and i'll describe that but in particular you're going to have e prime is going to be some constant times the number of vertices times blog space equals x so some constant no reverse these times log of the number of vertices ok so this blog is going to replace this vertices to the 0.5 ok so just give you a picture what's log base 2 of of 1 million what's what's going to be log of 1 log base 2 of 1 million it's going to be about 20 okay so and usually these logs going to be log base tues maybe it's a log base E it's a natural log it's going to be without the same was that just puts a suit that goes down to like 13 or 14 okay so so we're still getting about a constant when T and then there's this other constant here which is going to pend on how accurate your answers this constant is going to be and actually this log I'm actually not so worried about but this constant is going to be 1 over epsilon squared to get epsilon air so we're going to get some you get ten percent error here you're going to get a factor of 100 in front of you so a lot of the literature says let's not worry about this constant but actually for a lot of the things that you want to do my actually need this this one about some square okay so now now that you must be very excited about these techniques let me just tell you so that there's some kind of cool thinking that that goes into here and it will you'll you'll see some cool structurally grand off in the process okay so the idea is that the the basic technique is is we call edge sample and so this will work within kind of within a pretty good bound of the best you can do on there some much more complicated algorithms that you can run but this edge sampling is work pretty well basically you're um so you're going to UM sample each edge will call this e IJ this is the edge between the VI + VJ in to be prime rights you keep it with with some probability P IJ so you do reach these independent so you're going to just sample each edge independently of each of the other edges do not get the same probability you're not going to sample them Heidi the biggest an example each one independently with some probability and so there are certain age of which are going to be more important that you want to sample more with higher probability ok and so this is where you kind of get to see how this interacts with the structure of the graph and so the pub is going to be equal to the minimum you each edge i'm going to sample of this probability in and so basically on each vertex i'm going to keep around t edges and t is one of epsilon squared log of the number of vertices will turn out to be the right number and we'll see where that comes from and so it means that the the edges which are connected to vertices that have very high degrees so they have a lot of edges you sample those less freedom whereas vertices that don't have a lot of edges you want to sample those much more likely in fact if vertex is less than T edges then you want to you want to sample with probably one which means you always keep it right so let's say T is going to be like five right so I want on average every vertex to have five edges if it vertex is fewer than five edges and I always i'm going to keep all of its edges okay and the reason for this is if it becomes disconnected um then it will really may really change the structure of the grounds right so if if it has small number of edges these edges are going to be very very perfect okay um and so kind of the size bound is you're going to be able to say that the expected number of edges but expected the expected number of edges so the expected value is going to be T is going to be bounded and T times number of vertices yes eeprom okay so then if I settee this way I get the size mountain that that I said okay right so I no longer am growing polynomial in the size of a number of engines okay it says to see this is actually pretty easy to see I basically assign every edge to the vertex with the smaller degree okay and so then the probability that is that it's capped okay is okay and then for every vertex I look at which of those edges are kept and the probability is kept is going to be t / it's it's the Greek red so the so it's so they expected number for each vertex is then going to be at most say it's degrees times T over its degree it's it's like for each X I sign it most this many engines to the vertex V I and then I keep it with probability that's because I signed it to the one with the smaller degree and so it's unless it's one and then that's the case where I have less than T vertices otherwise I keep into this probability and so this is equal to T right so the expected probability the expected number of edges per vertex that that side to it is gonna be in Mesquite ok so I'm getting the right expected number of okay and I'm keeping the four vertices that have small number of edges and keeping almost all there's or almost all bears with lots of edges those are the ones where i can throw away so you can think of these debts parts of these graphs I can and it's easier to throw away these these these vertices okay but if I do this I'm going to I'm going to lose some information right so let's say I have some graph that looks like there's some some pretty sparse version of the graph over here and then i connect it to this very dense graph here alright there's some very dense brother graph and so this part is the one where i can i can i can throw stuff away but if i throw stuff away then i can it means that it no longer is going to look as dense I've kind of lost this information so i need to do something a little bit more i need to set the weights of these edges so so basically what i'm going to do is I'm going to set the weight of an edge I keep equals 1 over P IJ if kept um so and it's 0 otherwise okay so if I keep it and I keep it with probability 1 so initially all ages essentially one if I keep with probability 1 this is still one if I keep with probably one half then I gets way too I keep with probably one-tenth it gets way 10 right so it's part of this graph that's very well connected and I throw away a lot of edges I'm giving that way back unto those edges and so this means that essentially the total weight of the graph is going to an expectation stay the same so I'm not changing the total weight of all badges and so then ok so now if I find reduce the graphs this way what properties in my teeth right what what are the properties I'm keeping its going to have some property that this is going to preserve the cuts Emma graph okay so now if I to try notation before in Ginji all the the edges have weight w/e so for each each edge in the graph it has a wave w/e and each edge in a prime has weight w/e crime okay so and these these are the new weights here so these are by default always going to be one but in general you could have you could start with different weights and then you could factor and then this changes a little bit where the degree is going to be the sum of the weights instead of just the sum of counts of one right and all of this is going to go through the same way before so you can think of these as always neat one okay so now the property I want to have is that I will consider all remember from spectral clustering lecture is that a cut is going to be two subsets of the vertices where s union t is equal the full vertex set and s intersec t is an empty set so this is a cut and what I want is for all cuts when I have the property that I want to look at all the edges in the cut I think this is going to be epsilon ok so I want that the weight of all the edges that I cross in the cottage would be about the same so this is what this epsilon is getting it and so if you are going to reduce the edges you basically need to get this property you basically need to have tea at at this level so you need to have T is going to need to be one over epsilon squared log of the in order to get this property so if you want to preserve these cups remember these cuts are basically what you're looking for in things like in the spectral clustering and in in detecting these communities switcher if you're doing either of these operations then you basically if you you can't really reduce the edges that much in order to get a good approximation to these cut properties here now this holes from all cuts if you just cared about eating one cups you can probably do that you may not know what that cut is you may not know if that cut is ahead of time so if you do this with you randomly choose them in this process then with some probability this property is going to hold and so essentially you can believe you could still run spectral clustering or find these communities in the graph but you have to there's this there's an upper bound I think there's essentially they think there might be so with random sampling it's not going to work but there is I think there's a technique that you can get rid of this log log V here but you still need this one over epsilon square trip and I think this is really the killer really remember this may be about 20 this may be if it's ten percent which is probably not enough you need this would be a hundred if it's one percent then this may be 10,000 right so you know this is the term which is kind of specular so a better way to think about this is that if you have some abstract graph and you want to reduce it and then run your spectral clustering or you'll find you communities on it you're probably none good luck it's probably not going to work out so you in general it's gonna be tough to do this unless you start with a really really dense graphs and then it's become somewhat interest okay great so this this should look somewhat like this this growth sampling technique for matrix tension and there that dignity because actually becomes it is actually fairly useful in the matrix sketching set you can implement it very fast and at a first approximation it does fairly well and people actually use that approach this it's basically you need a really know large number of these these edges to really make this happen probably in practice this is probably an over estimate of what it's actually going to be you could probably get away with like maybe T equals 100 and probably work pretty well but your graph probably only has something like a t equal to the average number of edges per vertex is probably only going to be like 100 anyone's so it probably isn't telling you that much okay I just want to before I can move on to the the kind of more restrictive and practical method there's there's a this part of the problem is it's pretty well understood from what what can be done perspective as it because I understand it there's there's a more general version that's preserving the laplacian of the ground so remember the laplacian of a graph you can think of it as this degree matrix minus the adjacency matrix so this one is it stuff on the degrees are on here and the rest of 0 and then the adjacency matrix is so edge e IJ is is one if if there's a connection otherwise it's zero if there's no edge right we looked and this is the thing I guess this was the unnormalized laplacian matrix and so more generally what you'd like to do is to preserve the laplacian g- the laplacian of G prime you like this to be something like less than absalon type of beef so this property says have not only do you preserve all cuts but you can you can get this there's a slightly stronger bound you can write for its 1 minus epsilon so you can actually get these relative our balance here on these on these elections of these matrix matrices and so if you can think of this just X as being some way of kind of interacting with the graph save the value of something and if the X if this is if this is all zeros and ones then this are you coming to a cut so one is on one side zeros on the other side but this is kind of saying you can you can have these more gentle ways of interacting with it and for certain sort of analysis this is what you actually want to preserve and this is a little bit harder to do there's a way of sampling this there's you can you can still sample the edges but then the the probability you sample each edge is is going to be proportional is going to be some constant times the effective resistance so the of an edge so if you remember from your from your physics or your maybe intro electron gineering class you could look at a network and say there's some resistance on a bunch of of these these different edges and some some system and so there's less resistance here because where there's because you have because there are multiple ways you can cut across this right whereas if you cut this to completely disconnects the group right so that the effective resistance does a better job than just looking at the degrees of the individual notes so the degrees of something you can easily calculate really quickly from each vertex but this is a little bit harder to calculate the effective resistance it basically boils down to either solving a system of equations or you can actually I think it's into being similar to your some kind of relate like the doing like the haggin analysis of the system as well so it turns out this is going to be roughly as a harder to calculate these values but if you sample according to this again with with T equals 1 over epsilon squared log V then you're going to get this stronger valid here okay so i just want to mention that that you can you can get these sorts of bounds as well and so this is this is so kind of like an active area if we're still trying to understand if if we can do this efficiently yeah so and there are people would think this actually this uh I guess sampling I'm not sure if you know the poem is calculate the effective resistance is actually pretty slow so people want to do this as fast as before and there's been work and trying to kind of speed up the runtime and doing this but again the bound is going to look something like this on the sides so I don't know if this will offer become a very useful thing but people work in this area firmly believe that okay um ok so that's why we'll stay for the general brass versification it seems like something that would be really useful to do it but I have I think if you really are going to lose too much information in many cases so what can we do that's better let's assume instead let's instead soon okay any questions on on the general graph specification okay so instead we're going to assume that these vertices have um um live in a metric space and so then each edge each edge which is there's some weight of this edge which corresponds comets usually maybe is is equal to the distance in this metric space between v1 v2 and so in particular they'll be useful to say that these vertices live and say are too often so they live in two-dimensional space that's at least useful to think about it this way you could you can often think about like points on on a road and so like if you're in in Utah there's salt lake city there's there's Provo you know this is this is at 15 and then you go up here to Park City and so forth and off to wendover in vegas and so forth right you have these no drug networks and they kind of approximate this this this this opinion distance so it's a pretty good approximation to what's going on here these things will often some of these will work generated metric space and if you have a road network and you have the travel time distance on these roads this actually um this actually creates off is a metric space in between the vertices you can measure the distance between two things as the shortest travel time as long as you don't worry about the cemetery of the travel time if you make left turns it takes longer than right terms and so forth right but if you do you ignore that maybe it's just the the total distance divided by the speed limit and then this gives a symmetric ok and so then we have a metric space on here and now we want to try and spar safai this graph and now there's a lot more hope for what we can do right and this road network like what I've drawn here is actually a pretty good sparse ification of what's what's going on right this is maybe 215 what is this this is this is ed going through here and this kind of tells you how you get around about there's other stuff going on here but for in a pretty good approximation this is this is what it looks like right and so this is often so well talk about is kind of the the backbone of a lot of how these these are routing software action works so there's a lot more details that gone into but on the first level when they first created these these global kind of you know when you type into new google maps or or whatever you ask for driving directions it tries to look at this map and it really quickly needs to find the shortest path it can't run the full Dykstra on the scrap right so what it does is it starts with the scaffolding and it says ok if you want to go from vegas to Park City you would go like this right and then it might say well there might be a shorter route someplace else in there but usually this is pretty close right or you might start not exactly in Vegas but you go on to the highway and then come off of park city someplace like that so what it does is it maps to some note on kind of this support structure that's been sparse ified and it finds the shortest path here and then often it refines it refines it a little bit afterwards so it starts here then there may be several levels of sparsity that more more depth and it only needs to search locally so I'm going people taking like the AI class okay so good so there's like there's these search techniques and those like a star search right so its star search is when you want to search this large kind of space of things and if you have helped if you have a lower bound or on the optimal you say I have one path and I know this will take was as a maybe six hours right probably try it faster if you're if you want to but for maybe six or seven hours I know no path is going to take longer than that so I don't need to look expand my search space any further than it if I start I don't need to start looking off of here because I know that the distance from here park city plus this is not this is not if I go further out here it's only going to take longer because I already have a lower bound and this is not going to be a short cupcake right if I took if I took 70 over and then up something like that I'm this is going to take longer and I can really quickly proved out any passwords okay so you start with the scarification and then you build up but okay um right soon so this is one of the kind of applications of this and so now the goal is going to be a little bit different we're not going to try and preserving these cuts anymore or these laplacian switch is some generalization of the cuts instead what we want to do is to is to have so to define a metric on the graph so you want to go from again one raft down to a smaller number of graph and and again these are going to have the same vertices but the small number of edges um you know in this case you made up throwing away some some on some vertices as well as scaffolding and then having a way to connect to them but they're all implicitly there and you actually know maybe you'll see exactly how this this might this might work we have this hierarchy here and you want to give this so that you can define on metric on this graph distance so you have some distance on the graph between two vertex one and vertex two and this is essentially the shortest the it's the length of on the shortest path from v1 to v2 on G okay so its side ones see so I so I get one is that G prime is going to now satisfy that one is less than so it's often written again with the T parameter it's called a this is going to be called a tea spanner so this distance is always going to be smaller I'm going to spar safai the graph so if i take out an edge i would have used i can i'm always going to this distance will always have the shortcuts in it that I might miss I might miss some shortcuts on but I'm going to so this is always going to be larger but it's not going to be too much larger right and so you might have safe t equals to one right so it's always within a factor 2 or T equals like point one so it's within ten percent of the cost right and so these are the things that you're going to strive for so this so a lot of literature when you get bounds like this as a 1 plus epsilon and this literature it's 1 plus T that's just so if you see it don't get confused okay so when you're doing this you want to have this property and you want to minimize a few number of things you want to have small number of edges you want to have small total weight of edges so before when the grass parts application we actually kept the total weight the same now we can get down to a smaller total weight we can just throw manages because we're not worried about cuts inning and then you want a you want these small max degree when you don't want is you can actually sometimes you'll actually change engines or you'll edges or you'll start with graphs that have a lot of edges in them and I'll explain why that would make sense and you want the maximum degree to be small so on a road network this is this is probably not going to be a problem because you know you he you usually most most intersections have three or four degree you see some with five but it's it's rare to have larger than this but you want the small the overall degree to be small so one thing says this will often start instead of a road network and is the base because that you may just think of you have a bunch of points on the plane and you and you have all possible edges right so so if instead I was planning some I said you know there was there something horribly wrong with all of the roads of us like there was some weird ass and bringing all the concrete and asphalt disintegrated I can redo all the road network now now I just have these locations where people live and they need to go and I need to build all new roads I get the plans in a scratch what would I do over and you know if you look around the internet you can probably find some people have fantasies about this and actually we plan this or they're trying to you see these things where people are playing like a rail network through the US which mean like a high-speed rail network probably never going to happen like the full of us because so much more spread out of madness in Europe and so forth but people have dreams or these are people who live on the east coast who have never been out to Utah let's say yeah let's just put a high-speed rail I can't be that far I think we're probably the last places to be I one of the biggest cities to not be on a high-speed rail I guess you could connect this to Vegas or something but i guess vegas and LA might get a high-speed rail soon that's one of the next next places I with some Chinese investors there's a new big Chinese casino they might put a direct route then but ok so maybe there's local but anyway so you have these new data points and you initially have all of these edges every engines available right and so that means that each vertex initially has if they're n vertices than those n edges and you don't want this this makes planning really much more challenging it's kind of a pain to deal with you get these these large edges is essentially what you're getting rid of a new draft specification these are the problems in things like counting triangles these vertices with a lot of edges on them are pain so we don't want we don't want two big macs degree okay good so now this this problem this problem is going to be much more reasonable to do something like this oh just before I talk about however there's one more modeling issue which is kind of important um it's a road network a planar graph is it planar graph a good bottle for a road now and it turns out that you know for a first cut approximation get like ninety percent good player graph works works pretty well when you start getting lower lower level on there it starts to break down these overpasses our problem you can you can get one way but you can't and these things are crossing over each other you can't you you can't take the intersection even though crosses right and so and another issue is like these are these roads which are which are one way these one-way roads and cities and you know for instead of cities where they have like parallel one-way streets I know like salt lake has five and six hundred bits hopefully it's not that confusing but some cities like a lot of their roads are one way because their streets aren't is wide and so that creates some problems it's worse when you have like these freeways that have feeder roads on them but the feeders go only one Ark or one way and one side of the road and one way or the other thing that gets is confusing and so there's this a lot of effort now there's all this cool data from from open street maps where people I is going to look at this data it's open street maps data so this is kind of like a community crowdsourcing way of mapping all of the roads it's a goose but they're they're owned by Google right open street maps is an open source of effort to do this and it I think it was it's now pretty good at the US it used to be much better in Europe but people would go and they would just bite with uh with the GPS tracker on their bike and then upload it and so we get all these traces of where people went on these roads and then you try to figure out where are the roads from all these traces and you really if you start just doing a our graph it doesn't do a good job you need to know we decide the road there on and all these other issues so it's it's going to be a pretty good approximation but there's some modeling issues with so kind of keep that in mind if you get it deeper into this okay but for now we're going to ignore those issues we're going to assume that so let's now assume that V are our datos are no "
WwqTdww1D80,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-08T23:39:20Z,Data Mining (Spring 2016) Lecture 8,https://i.ytimg.com/vi/WwqTdww1D80/hqdefault.jpg,UofU Data Science,PT1H24M33S,false,169,0,0,0,0,bye-bye make it look like a pics of life yes but I was that yeah Renaissance Italy that was one way to need this common function out of a like they kind of like Serge go for something yeah so you're announcing it started at the resurrection but thank you make it will actually I see if I wanted to repeat that there's other stuff sure i mean thursday night going to be probably a wife Ferdinand II of drops every other week so she accepted justification or not over from Saturday looks like we congratulate that our universe day you'll see if Johnny onto of issues it comes your bike so it sounds what is the log function like where did that try to make sure for sure you know I for sending the same yeah I can make sure right and I I've always thought that probably too bad restoration was not even actually collected a huge Gator SETI but my questions like you find the address right you have to pre-process it considerations were there to make it faster we actually like download big files or something yeah it says 30 squishy get started even I thinking fish hopefully is coming on soon so starches some administrative things so let's see so the homework one grades were possible I have no change I have not had a chance to look over them to carefully yes when I will kind of do that soon and make sure everything's fair there's at least one question that I think everyone got wrong and according to the TA and I said you're being too harsh so either has or you will be giving some points back that I think not only lost like one point on that one it was like four be thinking of so long but I didn't look over everything yet maybe now's good so I see in the next homework is not due for so I think the student Nicks is that sound right the next one is due februari 17 so that's going to be next week wednesday so why'd you start working on that that's great it's kind of building on these idea of hash functions but you're doing this with actual text with stuff in my book in the last section of class we are starting now today and in the next three lectures will be on on clustering just something okay so we're starting now electric clustering and so these so this is this is kind of a very traditional topic topic vessel that's associated with data mining but it's also something that's very vague so I can take your class if you take flash and algorithms it's all about these very extremely well-defined problems that you're given a well-defined problem or I'm going to solve it quickly closely it's kinda like the hops it's a very it's it's it is it's as a topic is is very is very is is Billa 5 my colleagues so so it's a pretty dull defined top so that means every variation of clustering is solving essentially a slightly different problem and and there are many different sorts of problems you could want to define and so a lot of the challenges in dealing with clustering is choosing what problems set up to run so i'll try and in this section of class will try and uncover like broad categories of these of all of these techniques but there there there but there's many we are Camille to cover just because there are so many salty burns so let's start so saying in particular let's start with so obvious let's start with kind of a formulation of what would be a hard clustering there's something called a soft clustering which is even more but you're starting with some so you're so you're given a set which is the input a set X which is going to lie meaning in in some sort of metric space and you have associated with this some some distance function which goes let's just say generally between two points in the symmetric space okay so this could be very generally any distance function so that's why we talked about distant functions later or or while we spent a whole lecture on distance functions it can really be anything and for this also that is um that this is gonna be a metric there's some for clustering formulations where it's for things to work its kind of required for to be metric for others you can kind of have loosen this all I'll talk about this as it comes up so then a clustering is going to have three general properties to it so c1 so what you're okay so so let's see so a so you're going to have something called a a cluster which is going to be a set se SI which is a subset of X so there's there's also this term this term clustering is used in in other areas so like in in some areas of statistical spatial analysis that clustering will be defined not as this necessarily well I'm getting a little bit of myself so a cluster is one subset of data and then a a cluster ring is is going to be well we'll call this skripsi of X is going to be a s1 s2 up to sk so it's going to be a set a set of these subsets in the hard clustering will have three three properties now so c1 is that each SI is a subset of X ok so we said it's a subset of the data c2 is that each pair s I intersect SJ is going to be the is the empty set so they're not going to any interceptions so this is a hard clustering so it's going to things aren't overlapping their urgent or either allowed to flap we're not going to if it's if this is not true otherwise and this is known as a soft clustering and so that c3 is that the union of all I one okay these sis is going to equal our set ok so I'm so in other words you can think of this as a partition of the data so your partitioning the data up into these parts so this is what we're talking about clustering this talking about in other areas a clustering maybe you have a big data set you want to find some particular subset which is tightly grouped together in some way and you just just define that subset the rest you don't care that's that's a useful objective in some places here we're talking about partitioning the data in the past so think you could be like a common place where this comes up in like these these large Internet companies is this that access all of your customers and the clusters are different types of your customers and you want to say customers and set s1 behave in a certain way I want to target them with a certain time in advertising I want to give those customers some ads or maybe customers in type 2 are the ones who who spend a lot of money but don't make money for the company these are like big spenders but are very savvy so they only by when there's a sale or something like that turns up this is have very useful that is not easy to do for most companies in each challenge is different there's different aspects and really what differs here is a choice of the distance function confusing how do you properly defined the distance you know among these customers there's also a question of which general approach to use to find this clustering and which which which objective used to say how good it is impossible right this does not say how good is the cluster what I could have done is I could have made as to threats came empty and then s 1 equal to the cluster acts that's possibly could have done it that's a spices formulation I haven't told you anything about the objective of Puppets right objective is saying how good is the cluster right so I need it disaster these properties but then I want to score it right so I'm going to score some classrooms better than other clusterings and then I'm going to try and find clusterings which have a high score maybe a low a low score depending on if I want to have a higher right okay so before I say anything specific let's let's try and give some general principles about what is a good clustering or was it bad news right so start by drawing have a picture and then we can comment various ways on picture here so let's say my data look kind of like like like this okay so what would be a good cluster what is what would make sense to be the cluster here from the cuts good space we circle the three groups of dots that are close to one another so right so these are kind of close to each other these are gonna close and those are polite yeah so let's let's go do that right so I have these maybe one cluster another so this would be asked 1 s 2 s 3 yes so this guy seems reasonable right okay so who's seemed clustering my lab does this do to anyone okay is this new to anyone this idea of us weakness okay good so you know this is kind of a very common picture or sword thing you'll see this this if it looks it looks like this it's not really that right it's you know pretty much anything we described will give you the the same Rizzo love this day you want you're going to want these three clusters assuming that you're told thank you one of three clusters what if someone said I wanted four clusters he said no no here three oh no no I want four clusters what should I do you're working in a company your boss says I need four groups because I can put them in a box the squared and divide them that next way I want four groups now oh you know also now what do you could create an empty group yes that's that's that's a good thing to do say new customers you know make something up you know that's that's probably probably a good strategy but but if it comes back and says no no that's something that you know that one doesn't count I just split one of these groups yeah so you can split one of these groups well maybe you say let's split this group here right maybe say okay let's break this one hop into I don't know maybe something like this right sure you know that seems she dreams football then you can make this s2 and s4 ok but this now it's not so clear what is the right thing to do why did I split this group instead of this group over here so you know this this will depend on on the formulation and what result you get as an independent also if your data looks like this you don't need to run an algorithm just draw a picture and circle it on the piece of paper okay the power of these algorithms is that they're going to basically just take in this data set in this distance function arbitrary distance function maybe as long as of symmetric and then they're going to run so you know so they can deal with things where you get these a lot of attributes something more you're measuring things more than just two dimensional like you're cleaning distance playing you could have a lot more Fussell's you don't need to feel to see them understand what's happened but will draw pictures a lots and 2d because we can draw them in 2d and illustrate something so i'll try and point out like there's there's this very famous picture that shows up and lots of papers where someone created this this this data set that looked like that look like this and then this paper claimed and hundred of papal hundred other papers claim that the clustering should look like that's too it's it this makes more sense and maybe they're a little bit further apart I'm not sure if I drew just right but this picture itself seems like look completely ridiculous when does your data given look like this right this this is there I don't know what you're trying to illustrate it but this seemed like a very strange strange thing to happen your data is never going to look like this your data is going to be probably high dimensional you're going to just have access to it through a distance function and that's how are you going to do you're not going to see something that looks like this on the plane you know that if it really looked like that on the plane then take out of your marker and draw a picture yourself so okay so going back to this distance function here what yeah so if you look back at this first example here look at this example why was this first clustering let me get rid of this for the purple one why then this makes sense as the customer why why was this what told you that this was guilt what are the basic things you are after the distance between the points what what about the discipline what is good about this clustering respect to the distance there closer distance yeah so those points the points inside of the cluster are close good so me so there's one as the points it's inside of the cluster these distances are close so this is often referred to or sometimes refer to as the width is the sum function of the distances in the cluster this is in the cluster and this is you want you want this to be small okay good so this is one thing you're looking for um what else would you look for yeah this is me cluster yeah this is kind of my left this description day it's something about the distances in the cluster and you want that spot there's something else yet this small number of clusters yeah so if you had some choice if you were choosing k may want the number of clusters we often refer to as K the variable if you were choosing k probably air on the smaller side you if you can describe it just as well as the smaller k that's probably better typically the larger the K the objective function is going to go down usually and this is going to make it challenging and choosing what the right Kay's on and we'll we'll come back to this sets as that's the mother jones there's let's assume you're given the value k i was given three and now i chose the e street clusters what is the Y within the cluster of the width to be small there's something else let's go ahead then because you want to maximize the distance between clusters yeah so there's there's another thing going on here that the distance in between to the clusters this distance often called the split so this is the distance between Plus bears you want this be large ok so you want so you want the width which is the inch inside clustered distance to be small the clusters to be well grouped and the split which is the distance between clusters how far they are hard to be large so you want that to be distinguishable that there are two different clusters which is not kind of some confusion and then within the cluster you want to attend in groups these are generally the two properties that you want though split to be large and this width to be stalled and there are many different ways of measuring these these days and and sometimes you you can write an objective with only one of these two but if K is fixed maybe only need to minimize the width and the split will take care of itself or maybe you just care about the split and and the width you're not so concerned about if K is this cave if K is good but there as you started to say like there was some representative point and you're talking about the relationship to that to the final width versus they needed some or the all these businesses there are lots of ways to kind of define what these needs and so well will kind of get into some common cases but even trying to enumerate them at this point is just going to be kind of kind of kind pointless really you want to use one that works about with the algorithm the general algorithmic technique hear that your your algorithm is using more so then whichever one maybe there's something about modeling at the best okay so so the what we'll talk about today please do here so we'll talk about so the in general we're going to have three types of clustering that we'll talk about in this part of the class the the one today is going to be higher arc of La Mora to hire alarm to the clustering this is probably maybe one of the first things you would think about how to do this hour maybe not explosive things but it's kind of the simplest and most general and a lot of things fall in this category the second one is going to be so of assignment based clustering and then the third is is going to be spectral clustering so this one so this the spectral clustering this spectral refers to the eigen structure of a matrix so if you we're going us for this we're going to need to start talking about matrices more carefully and the eigenvalues and the singular values of them and we'll come back to that much more later in classified let me talk about this now what you do is you take your data your distance matrix or your distances of the points you formally that is a graph which you then represents a matrix and then the matrix kind of tells you how to cluster the data using the eigenvalues kind of cool but it's going to be very different than we will talk about today and Wednesday on Wednesday we'll talk about these assignment based clusterings this includes the the k-means formulation which probably many of you who is king clustering who's not heard of k-means clustering okay so k-means clustering is often the first thing that people think of when they think of clustering if they they have been exposed to it and it will be included here I deal with this sort of clustering is that each cluster gives one representative point and that's enough to define the customer you map every data point onto the closest representative fault so you assign it to the center and that's called the center that representative from so you sign every point to a center to the closest one and that too the clusters okay and so k-means is an example of this or the we're this where you you use the essentially the l2 version of this there's a very cool algorithm called Lloyd's argument which makes it very easy documents but there are other variants of this which which are sometimes even more useful or more stable on various so we'll talk about this on Wednesday today we'll talk about those higher echelons of clustering and so this is overly complex word up so if laboratory means that it's your kind of gluing things together your kind of starting from the bottom up and your kind of building all the clusters making them bigger and bigger together in contrast the spectral clustering of the example of something top down where you start everything is in one cluster and you keep chopping it up in pieces ok so so this one is going to be top-down where you're kind of carving it up and this and the one we'll talk about today is it is going to be bottom-up where we start with a bunch of really tiny clusters and kind of start gluing them together ok so and then the silent based clustering is going to be kind of even it's going to be something where it's based on these centers and you do it really different ok so this this kind of these three classes which will probably cover a ninety percent of each class roughly covers probably ninety percent of all types of clustering but there's other types that I that I won't get to there's things like mean shift which is doing some some weird thing and there's this density-based custer where you try and find these uh where the regions of high density anything in those gets them the same cluster we are going to talk too much about this because you can go on and on there's a class that's that Suresh Becca that some raining and occasionally offers I think of the offer next fall the whole semesters I'm custard so you could have a whole class on Linda's right stuff well we're going to do basically three lectures and then we'll come back to something related to it base line graphs that's kind of different from both these as well based on something called modularity in one of the last lectures it's okay so this hierarchal agglomerative clustering so of any any questions any any questions on this before you go look what could you explain the differences between salt fostering and hard book yeah okay so hard clustering restricts that any pair of clusters say s1 and s2 here they cannot share any of the points each point is assigned to exactly one customer as so you can't work that statement requires the third point as well but each point is it at most one cluster is what the property tues where a soft clustering does not require that let's say that there was some some data points some other data point right here that's head sits in between s1 and s2 and maybe both these are really well defined in their desert then there's this one other point sitting out here I don't know which cluster to assign it to I could force it in one or the other but maybe it's better to say I'm not sure it's fifty percent one fifty percent the other and there there are some formulations which easily kind of to this or it could be that there are three really well-defined clusters and then there you know with a thousand points on them and then there are ten other points which are outliers that are kind of off do I really need to assign those two clusters not sure maybe I assign them partially a halt clusters because I had no idea you could you can do very simply so soft clustering is when you're not enforcing up folks is it a Muslim cluster usually it's given a fractional assignment across multiple clusters okay so hierarchal agglomerative clustering oh one more general thing I want to say about all this um you know the other thing about clustering about being this very kind of this this very this objective which is is is kind of very nebulous and I haven't you know the fine and precise yet other than the split with is that if data is really well plus turrible that means it's really comes from different groups it's usually any algorithm you run will work it will give you about the same result if it's not possible meaning that there aren't well defined groups are you there trying to find K or you're trying to find four clusters in their heart for clusters here there are three most our rooms are not going to give back right or maybe they give some different carbon share results so if it is well costabile most things will work if it's not wrong possible then it's it's not going to make too much difference the main difference is in some subtlety s and the distance functions or the sort of objectives that you have sometimes you want to favor certain situations like this picture here this two moons kind of like half moons so these two moons picture you want to favor something that's kind of looks like this or some other objective and then things can make a little bit of difference or you care about things that are easy to implement or work fast for your talent so you need to choose an algorithm which is very efficient and so those are really going to be the main things look for some subtleties and the objective functions and the efficiency of the algorithms or maybe the maybe some extra structure that you get out of the cluster so that those are kind of going to be the things you look for ok um ok so the hierarchical um collaborative clustering so I'm just going to write down the that the general algorithm and then we can discuss it it's uh it's very very very simple um so each so first lines each data point X I is is a starts out as a separate cluster pestle each data point is its own cluster to start ok and then tongue while two clusters are are are close enough then we're going to do find the closest two clusters s I and SJ and we're going to merge SI SJ into a single cluster will call that's that's not a problem okay so this is inside of the one okay and this is the this is the whole algorithm right so while there are clusters which are close enough then you're going to find the closest two of them and merge them together so you're going to kind of find clusters that are close as long as you can do that you keep merging them until you're satisfied with each side all right so just to draw kind of a simple picture here okay so let's say this is our our data set and so initially there there 5.0 initiative there each in our own cluster and then I'm going to say well points 1 and 2 are close enough so let's make this into its into a new cluster then 4 plus 5 or close enough and these are the closest to remaining this is going to be a new cluster and then maybe s 1 Prime and s 3 and data point 3 down here these are close enough so i will now merge these together let's call this as three prime and then I can decide whether I stop maybe s3 and s4 Prime maybe these are closed up maybe I merge them together right so one thing you can see out of this is that if I I write my data points like this then i can think of kind of building a tree out of these these these data points right so I can build this hierarchical tree of howard cluster them together and this kind of tells me and then this close enough version maybe I decided I want it to stop when when I three clusters so then I essentially tied I chopped it off here and so now I have these three different clusters I chopped it off there but you know so so this this notion of close enough sometimes you can wait until the end and then go back and do that so you know you don't have to decide that right away you can just run it until it's all a cluster and then go back and decide where was closer but it gives you this this nice structure here in that not only to have these clusters but you have hierarchies of clusters within the cluster size all right um but hopefully you realize there's this you know if I told you to go and implement this algorithm aside from hopefully what comment you could read my handwriting but it's written better in the notes please it's not too bad what what what else would you stop and say wait wait wait wait you haven't told me enough how do I do that right what what's missing that you don't that I haven't told you that what is they hear definite your  crooks the definition about clothes the definition of clothes yeah so one thing is closest so I need to find the closest two clusters there's someplace else that I mentioned a little bit so you can find how closer two clusters yeah another words was close enough you so I these two big things right this is close enough watch it will come back to this there's some interesting discussion there the closest so I've defied I've remember my input included a distance function between points but now i need to say between clusters I've got s3 private US war crime what's the distance between the mark is s let's say cluster s 3 and s 1 Prime and s 4 prime which is a closer to how do I decide which one is closer to right so what what's the clothes um you know between clusters right so what is closed me when I'm talking about between clusters so if I just it feels just points this is you know we've discussed this their various distance functions here but if I talk about between clusters I haven't we haven't talked about that so if there are sets right the clusters are sets right so we talked about the Jaccard distance between sets is is that can work here why is the Jaccard distance after work yeah all the sets are disjoint then they're all going to be this is one so the Jaccard distance doesn't help okay but we've got another distance between the points inside the clusters we need to somehow use that what should we okay so there there are lots there are lots of right answers here so what is what do you think is is a good right answer for distance between clusters okay the distance between the average so the average of the clusters themselves yeah so the okay I'll put this in a larger category so create a center of each each cluster and then take distance between the between the Centers right so how do i create the center of a cluster see the average right so so one would be one notion of the of the center with the average and if I'm in in in Euclidean space then I can always create um let's say the Euclidean mean of the data right so if my data if I'm using the Euclidean distance the mean is is well depart right but let's say that the data points now are going to be sets like these are these these these bags of words or these are these are these these sets of K grams that I have in a document how do i create the average of these sets of K grams what does the average need no that's well if you know I'm going to answer this because that's it's a really good question the average probably is i can work in that case for things that aren't if you're not using if you're not using the euclidean distance or things that operate nicely on various manifolds then usually the the mean is not well defined you can't just take a mean of data points if they're if the data points themselves are says so if you can't take the average or the mean then what should you use the mode but what if each other yeah you could use something like the the mode which is the data point that occurs with the most frequency but maybe they all occur once right yeah could you get the point that's most similar to all the other ones yeah good good so a so the average is the point minimizes the sum of the squared distances we'll talk much more about this on Wednesday but so pick the the point s in s I which minimizes say the sum of colors see so s in s I say the distance from sea to maybe square that is going to look like the meat and so you can do this just by checking all the points sometimes you you don't want to check all the points because that's going to take a long time so you'll sample some points and then whichever one from the sample and you're going to use and you can use us as the representative or their other kind of ways you can not be precise about this so you can pick one of the data points which acts like the meat and this is this is a pretty good choice in decimal we'll talk more about wednesday maybe you don't want to use the sum of the squared distances maybe just minimize the sum of the distances doing that and maybe you the need the average is the one that minimizes the sum of the squared distances as well we'll say in various things this is more susceptible to help my arms if a point was in the cluster that really shouldn't have been that's going to pull the meat over much more than if I just look at the sum of the distances so maybe I don't maybe I don't want to square them this is be a little bit more stable with it can be harder to work with this we'll see okay and then if all this fails if if you don't want to spend the time to do all this all the points at the same weight so that mean the mode is that can help you can't compute the average picking the point that minimizes something is too slow you can just pick it in and our literary point so random pick the first one that you can assess just pick one of the data once again if the sets are well cluster Bowl the clusters are tightly grouped and separated this will probably work as well as any other point if you're if you're lucky if you're unlucky maybe it causes a problem so maybe you check a few data points then pick the one that minimizes the sum of objective and this one work a little bit better but even just some arbitrary point name okay so this is definitely this is definitely a good option to pick a center and the whole all the class of algorithms will talk about Wednesday will be based on some having a center representative of the cluster what we didn't want to Center what if there are certain issues with all of these we may have had to resort to just an arbitrary points which seems kind of Alex's you know seems kind of arbitrary right what if we didn't want to define a center what other way can be defined distance or closeness between clusters okay good good so the distance to the on the sausage should say the distance between the closest point so the state distance between s1 and s2 is going to equal to the min of lowercase s 1 and s 1 and s 2 s 2 that distance is one that's too so I take the two clusters and so if we scroll up to an our example here I took the two clusters here and it took the closest two points this one and that one there that defines the distance what sort you know I could have done similarly the distance between the the farthest points right i could taken the farthest point between the two clusters right this way also makes sense what what what difference between these objectives what difference is this going to have what sort of effect this is going to have on the sort of buses I it's gonna that where the clusters form yeah so what can you tell me about where the clusters will form how would that make a difference would an example of how this could change the result let's go back here if I had a big white board right now then this is all the outfit but we are so what what difference with this hat so this date is set it's probably going to be the same alright this one is well costabile really doesn't matter what i do if i look at this is kind of crazy to who data set you're going to give different answers if i use the closest distance versus the furthest distance what would the difference be what how would the objectives change so one way of thinking about it is if I was doing the closest distance I kind of drawing kind of a graph here and as long as there's some short edge I can draw between these then I did I can connect them the ability to connect something depends on the the closest edge i can draw to make it into a graph which is which is all connected but if i'm using the furthest distance it's getting sure that everything is is closed that means every point is close to every you know if I if the clusters are closed and I have to move on so every point is close to every other point so probably what i will do is I'll end up getting so that the closest this is well I think it's something like this like this famous two moons example the furthest distance well we'll probably end up with something like like this instead they should overlap maybe maybe kind of like this they might think that the kind of the shapes to find clusters don't even need to be convex it's going to keep them very much more tightly grouped so this is the purple is based on the furthest distance and the green is on the closest business so there's this this branch of machine learning and data mining called that's called manifold learning that was really hot maybe 10 or 15 years ago and the idea was that really high dimensional data is not really the high dimensional it lies in somehow lower dimensional they model this as a manifold manifold does this math project that locally looks like it's euclidean distance that means locally it's it's it's uh it's and so in this case locally low-dimensional you could eat business so in this case you could think of there's kind of like this one dimensional manifold that describes each cluster and that's all you need to make these kids you just need them to be close to something else and so this objective kind of worked with this manifold learning kind of a kind of thought process so manifold learning so this was cool it kind of a lot of people to do some stuff there are lots even more stupider no sense like like there's this famous there's the Sun this thing called the Swiss roll so who's court officer of the Swiss roll data set okay is this crazy day said where someone generated where data attention lies on a spiral and they said okay you can learn the spiral and this has been used hundreds and hundreds of times data does not look like this but people could find this manifold structure that lied in there and there's bunch of data points on here what happened was this was cool and if you kind of made up babli like this you could get to work really well then people discover that there was noise and data and this literally worked so well if you had noisy data mostly this kind of noisy and so you know there's a lot of the ideas from this kind of permeated in tongues of areas but manifold learning itself is is often not used there are some cases where you can you know data must lie in a certain way because it's been normalized like cosine distance normal as all that is with light out of sphere and that's actually a manifold and this kind of worked but you have to have some very strong assumptions that are actually course about the data if there are no easy then all this kind of rates often breaks down under some small noise conditions they can get to work but you know so anyway so if you wanted the closest vistas kind of follow that bottle and the furthest distance really restricted things are compact it's kind of this change in the distance function has an effect here so in these two ways to find the distance between clusters there's kind of one other there's various other things you can do i want to mention one other kind of general we defined the distance between clusters and that is basically you you look at some objective of the join cluster and you want that to be small okay so that you want so so it's it's kind of something like the the density that so the distance is defined as the density of s1 union has to exit so for instance what would an idea of the density me would say the radius of minimum in closing fall so so that if the radius of the minimum closing ball is small then it's than the distances so you're not looking at actually the distance between the clusters themselves you're looking some objective on the union of the data sets so things are easier to to join if the cluster stays compact in sorry right for me it's the it's the average distance either either to the center or among the points after after after joining so so you you joined them and then you look at the average distance to the center that's your distance between the clusters or you join them in you look at the average pairwise distance that's your distance between clusters so if you think you have two different joints whichever one gives you is the resulting puffs which is more past those are closer so your goal is essentially keeping this width notion small as you're growing these things and we want to create the cluster which has the smaller result with right so this relates to this notion of the width between clusters so this is another way of thinking about this as well okay so there are lots of ways to think of defining the distance between these clusters within this algorithm and in each of them in the same framework each link is going to give a different results for certain types of constraints but in general things are well clustered they're probably gonna work about the same and this was just this notion of the closest locals could find the closest two clusters if these all these objectives are in a wall things to do and you can see hundreds of papers on different ways of defining this notion of distance between clusters maybe that's now the papers phrase they say here's the whole Smith way to cluster and basically comes down to a different distance option okay so now the other other question that we didn't answer yet is when are these clusters going to be are they are they close enough oh wait but before I do that there's there's one there's there's an issue with step three which are the closest to clusters and comes up a lot in this sword formulation and it's often on spoke about let's unspoken you know most people just just ignore this and that's if you have ties so that's so if you have if you have a tie two clusters are exactly the same distance to each other seems do we really worried about these things well you do if you're starting from clusters of size one they'll often be defined of a discrete nature I'm drawing them again in this in Euclidean two-dimensional space and we often in Euclidean two-dimensional space you assume they have continuous values and these ties don't often happen but when you have discrete data when you're talking about something like the Jaccard distance between sets ties come up it's it's not that uncommon to have ties and one kind of particular nasty issue about these hierarchical monitor clustering is the choice of how you break a tie can drastically affect this resulting hierarchy if I had chosen to put three with five before four with five it may have really differently the hierarchy me turned out very differently so um pump so what should we do with ties alright this is one thing I don't have a good answer for so this is its kind of sensitive to this you could try breaking them randomly and try rerunning it again and see if it changes a lot it may not be sensitive right if the data is a little cluster whole it probably will not change things Ramage if data is not very well cluster abell it may have big changes so if your data is all pretty much spread out it all looks pretty pretty similar in in all in kind of all of its uh that's some kind of evenly spaced inside of a box or something this could give you all sort of different of different results so if that's the case that maybe say this data is not very well clustering these small arbitrary decisions that made a big difference meaning this was a poor choice in the Opera where this was a poor choice to try and cluster this data some of you will do projects where your your project will be to try and cluster some data set and this has happened to past semesters and it's usually at least one group comes back and said I tried really hard to cluster this data set and we couldn't write we couldn't find any reasonable clusters this this happens to a lot of data it should not you should not necessarily try and force clusters on there that's they're not necessarily sometimes you need to part of one quick thing about clustering is that it's not just the modeling thing it can be used inside an algorithm sometimes you want to do some sort of some sort of divide and conquer on your data and so it makes sense the group's facially or similar points together and processing them and there doesn't so matter maybe there are multiple equally good clusters with very different structure but they don't matter for your algorithm as long as they both give a pretty good kind of spatial decomposition and in that case that's okay that if there isn't any good clustering but they still I'll give good spatial decompositions they allow you to do the divide conquer fight right if you're doing if you're in quicksort at your data and it doesn't really cluster well what quicksort still works because you're able to divide the calm side high school and some high dimensional stuff like these range searching or these nearest neighbors to achieve data structures it didn't matter that was clustering my stove on a partition today so that's little boy that was possible Mary all right how was I was going on too long I think right all right okay so let's get back to this last last point so what is closed what does it mean to be to be close enough so how would I decide to stop it's what is the right way let's go spend a little bit less time on this question when would I stop merging the other clusters reach a certain number yeah you reach k clusters yeah you're often told I want K clusters or you guessed some value K and when you get to K clusters you stop if you do this one asked you why you choose it okay but maybe someone had a reason like I want I've got square i want to buy in the four parts of the four clusters for talking to the you know venture capital you know but yeah you've reached king yeah oh do something like the speed like this here the develops all the data but take some fraction of the radius or something to use that as a yea ok so the rain so much like the radius of of all clusters reaches a a a maximum right so each if I merged any two clusters they would get too big right merging would get too big in other things merging would get to now I haven't specified what two beers but maybe this is some quantity that you can you can understand if you put instead of just some number K I understand what four clusters means I understand two big means that they're they're further apart than then some quantity that makes sense in your distance metric that's something that maybe makes more sense instead of I don't know so you might be able to put some some number on it radius of all clusters rage maxima if we have a data is very far away for another four other do this I think it's going to another yes so some you're saying is some data it's very far away from other data points then we may not have a good grasp on what this means is that you're saying or their priests asking like outliers are going to cause a problem if I theta is very far away from others I do not begin to into some classes I so the rate of that class I we're never to come the count become yeah so I mean so the question is the radius will never get too big yes so what the the comment was suggest this versus essentially some distance which describes the total radius of all points I put one ball that describe the toll rates of all points let's say I wanted ten percent of that means right so that way if I were to include it so it's all one cluster I know I which day yeah but there are there are some issues sometimes it doesn't make sense how to choose discretion and you need to be careful I heard it's a good thing to watch it yeah yeah size of the split between the clusters define specific yes so this was seem to be more about the width some notion of the width got too big this is the the split got two small clusters got too close and subsets they're going to kind of eat closer to each other if they're too close you would merge them but it would there's some notion of split which are defined over the quantities and differently is that you could set something up doing it increases the average distance but then you never get number the first one yeah often the average distance to the center things like that are always going to go up as you busy Roach yeah a lot of these things yeah so we'll draw a picture is a second but let's say the you can have like the number of clusters you have some quantity or like the inverse of the quantity and they're pretty much always going to kind of look like this so as the number of clusters gets smaller yes some point is going to go up or it could be the inverse of whatever quantity it is so it's something like the average distance so if this was say the average distance to the center as you get some small fusion or clusters this is going to go remember to your better one yeah so they're going to stay here so it's it's hard to look at this for most quantities so this is some you know pause so some objective and so one other one is you had some some density and you can you reach you read some threshold base of the density so this other type of distance criteria  and again this is going to happen again this could be a density thing the density gets smaller and smaller to keep merging typical there may be a few little bumps in here but we're but in general this is gonna do fine density is the number of points within the cluster or whenever you find us never points in and give you space you know you could do something like the ratio of the number of points to the volume of the smallest of all the container points there there there various ways you could do this they're very notion of density yeah a lot are going to have this Oracle objective there's one objective that we'll talk about towards the very end of the semester call called the modularity and this is often not used in this partition idea string but in finding these kind of anomalous regions which are really well concentrated and thus modularity concept is not going to look like this it's going to have an actual minimum at some point it'll kind of tell you where to stop and there are a few objectives you can sometimes get like this but they're usually there is usually hidden or or not so agent so parameters Tom so all of these essentially have some parameter that tells you either I was too wide I I got two small there's some density threshold I reach some value k there's some threshold that I'm somehow telling you me to stop there's there's not some magical way to do it modularity kind of gets around this although if you squint at in some ways you can kind of see that they did make some choice which seemed very reasonable inside of it but if you would you could have maybe put a different number other than one in some place and it would still make sense and so it looked its there's a parameter that was one and so you don't see it because it's a coefficient on a lot but one is the one that makes sense and a lot of people uses modularity and buying these communities and girls but so now if you look at this chart so it kind of seems like there's some region right here which seems good if I get a smaller than this it kind of shoots up this is this is not uncommon and this bob is is is often called the elbow we try and write better so it's like the elbow of my arm right so kind of this way right it starts shooting up right here so maybe I want to pick this number of clusters if you plot some objective function it often looked like this and you want to pick some point around here that's often this kind of seems like area in half at hawk and weird but this is often not a bad thing to do if you can plot this and with the heart you kind of are always maintaining this value so this is kind of a often a useful reasonable way to do this ok so again I don't good answer this is again a modeling choice it's not it's not like in the algorithms class where you're given a very precise thing here you're making all these decisions and they all affect the other again this is useful because these decisions often for data where you would expect a good clustering these decisions usually don't make that much of a difference but they can in some cases make some game small small differences any any questions on this on what this closely between the objective so some objectives would be so like we talked about the like this with other the merging would get them to be too big right so and so in this case as you go the number so if you look it could it could be a variety of things it could be like the average distance from the center of the cluster is you've got smaller number of clusters the average distance is going to go up when you have n data points and then cluster is the subjective is 0 yeah so each of these somehow gives you some sort of objective we want to look at the density of the clusters maybe that's the ratio of the number of points to the average distance to a point or the furthest distance from the center or maybe that square or something depending on what dimension you're in right but there's some notion of a very solution of density you could and then this notion of split you want the split to be small maybe this is one over the split and that's that's going to work better you somehow if you want to drop in traits like this is your number of clusters get smaller you what the objective to go to go so you're balancing the number of clusters kind of the tighter the model the simpler the model year produced a 0 vs the how poorly it it models the data write your ideas as the objective goes up the model that it is is representing the data works you know less about the data from all right okay so one last thing to talk about for this lecture is the is the efficiency of these apples how long does this run so the runtime has as a as a day on function of n where this is equal to the size of the of this size of the input would be all right so rental is a function of n how long does it take to run these these algorithms ok so first thing let's assume we merge to a small number of clusters right so at least we start large but we eventually want this to be small so let's say for instance that you know let's just run this all the way to the end and figure out when is enough late right so so how many merchants so how many merges we have how many times we perform this merge operation so okay so here's a okay it would be like roughly log on or no it's a log in or it again so it's in LA I mean you have to go through each you have to see each data point response move yeah so I'm not talking about the time yeah I mean that's the ears to your starting to get the right direction so this is how they ask a different question you've seen the answer this how many if you have a basketball tournament with 64 teams and it's single elimination how many games with it 63 games each team loses once except for the team that wins right so if you have in teams you basically play Oh event games every time you merge a cluster your number of clusters reduces by one there's kind of you've got rid of one cluster so you're going to need Oh n merchants now you might build optimize this you somehow say they're a whole bunch of things that are close and I can merge all of them in one step but they can be tricky sometimes because you need to find it an independent set of things to merge you don't want to merge one cluster into two other clusters at the same steps need to be a little bit careful that you can't do this together so you have to do all the emergence and so well let's let's just kind of look at the naive virgin where each round you just merged two clusters together so they're all that rounds of this of this wild like we go back to this however another go back to this algorithm here right go back to this algorithm this while loop basically has opened steps mobile van steps here through this wild ok so now merging them let's stop to worry about version that we need to find the closest two clusters how do we find the closest two clusters and how long does that take right this emerging it turns out and they will do it as fast as find the closest two clusters so how how do we find the closest two clusters how fast how long does it yeah so so let's take one where we do something simple that we can do fast yes so let's pick an arbitrary point right so that takes each closest computation takes constant time right we have some center or we have some arbitrary point turns out if you want maintain the averaging emergency clustered you can update that constant time right you can you just average you a weighted average of the two centers you can do that cause sometimes well ok so it takes cost let's say it takes constant time to measure the distance between two clusters so how long does this take stuff the closest to this n squared Cartesian yeah so I mean if you're not doing anything clever here it stinks there's all things squared of these these comparisons right you're you're every there so at the very beginning so for the first half the Verge's you have at least and over two clusters so you have to do and over to choose to is still low event script and this end so for the first half of the words is you have of asymptotically large number to check so if this takes constant time to tell the distance it could take more it took it could take n square time to tell the distance but it turns out those then all the customs would be small if you're at that stuff typically so let's say it's if it takes constant time for each of these this is over n cubed because each round you need to spend that much time now if you're clever the thing is only two or only two clusters are destroyed and one is new one each round so most of the distances you're competing staying the same in between the rounds there are n squared distances but only event of them are changing so if you're careful with a priority queue where you can maintain a priority queue of the distances you can update this in log in time and then you only need to remove out your olvin ones that are with clusters that no longer exists and added for many more each of those takes n log n time for all those operations and then you take the top of the priority queue you can turn this into into boa and log assuming that you can calculate the distance in constant I'm if that takes longer than this going to take if it takes but if it takes longer you probably can fold it back into the old n squared without increasing that because you only need to update go of anything so if it takes linear time your distance then that's good that's okay that is still a little bit cute now if it's the all Paris distance you want to calculate distance between all pairs of points to all the pears on the other cluster this one might take long but in general it's either holes n cubed or or n in this case it would be a low of N squared log N and it turns out this is a big number this takes too long this is usually going to be slow when you run it on a lot of data you this is going to take a long time so if you use this for your project every year someone comes back and says we tried to find higher group clustering on it and it took to look this these areas are not efficient there's some ways you can somehow playing with a number of merges trying merge some things together this world usually leads to problems if you try and be too clever but if you try and do too many wants to try and curve two things together when you end up making more arbitrary decisions which affects the modeling so this is a general fairly slow the ones we'll look at in the next the next two lectures either are directly fastest or the center base or can be made faster using some advanced techniques specialized alright so that's it for today we'll talk about the assignment based clustering win 
05uWXF6GwtM,27,"Data Warehousing is the storage of big data. Data mining is the analysis of the collected data in order to find trends in the collected data. 
Filmed in Tanzania
Copyright Mark Wolters 2019
Topic 14: Supply Chain Management
#marketing #dataanalysis #data

YouTube Advice Playlist: 
https://www.youtube.com/playlist?list=PLuAz-nxZVHKCgTMK9qJVoRnnx5D1EdmUj 

Marketing Training Playlist: 
https://www.youtube.com/playlist?list=PLuAz-nxZVHKA3XLEqGREc66eVBRho7wQF

I hope that these marketing & business videos can help you learn more about business and help you succeed in class or in the business world. And yes, this is the same Mark Wolters from the YouTube travel channel Wolters World. 

Find More Marketing Videos & Course Information at http://www.woltersworld.com/courses 
or on our future site http://www.professorwolters.com

Follow us on Facebook: http://www.facebook.com/professorwolters 

Subscribe to Our Other YouTube Chanel ""Wolters World Travel & Culture""
http://www.youtube.com/woltersworld",2019-11-18T13:00:00Z,Data Warehousing & Data Mining Explained,https://i.ytimg.com/vi/05uWXF6GwtM/hqdefault.jpg,Professor Wolters,PT3M21S,false,2736,95,1,0,6,hey the filler marketers professor Walters here and today we're here in the Serengeti National Park in Tanzania and today we're gonna talk about is data warehousing and data mining and data warehousing is just the collection of all this data that companies do collect on their clients on their you know the clients their customers their suppliers their competitors it's all the information we're collecting for example when we came into the Serengeti National Park they copied our passports they took information where we're from and all these kind of things so they can kind of collect that information I think it's the collection the information the collection of this big data is what's called data warehousing ok and so we have all this data the thing is we have a list at it but what are we gonna do with it actually use that data to figure out the trends in that data that's what's called data mining so we kind of have this big chunk of data imagine a big square full of informations Pulitzer full of data in there we've got to think of us what are the trends what does it to really tell us and so you drill down your data mine into it defined what are some of the trends what are some of the things we should be looking into so for example here the Serengeti National Park will track hey where are tourists coming from what are the ages of the tourists so they can see is ok we see that we've had a big increase in US travelers coming to the Serengeti so maybe we should advertise even more in the US because it's really resonating with them so you'll see things like that a thing in your own life if you go to the grocery store you have their like Max Card or they're super like saver card or something like that whenever you shop you swipe your car for those discounts but they also track what you've been buying your groceries your toiletries all these kind of things and they can actually figure out what should we recommend to them that's why I know when our kids were little we'd go you know buy diapers so we'd have six-month diapers then nine-month diapers and then we'd notice the coupons they'd send us oh well you know it's like we know you're not gonna use the coupons right away so with your next set of diapers is the 12 month diapers because they kind of used all that information drill down and said hey these people have kids so we could try to sell them the the bigger size diaper or we might learn that hey they want to their they have these little kids maybe we let that we send them a flyer that says hey don't forget we have our special toy sale in December so come and get your toys here at Meijer here at Target or something like that and the thing is for companies you can really learn a lot but the thing is it can be very dangerous as well because think about it all that information we collect if someone else gets it off we lose it I mean that's where privacy concerns come in because every time you download an app and say I accept their terms and conditions they might be able to take your pictures your phone information your addresses your emails what do they use with that what do they do with those things well that's one of the big issues with data warehouses we got to keep those things private so you spent a lot of money to protect your data warehouse to protect that big data because the thing is that big debt is worth a lot of money because if you can figure out those trends you can figure out what's next what's the next things my customers want to buy and so I can develop that product and sell it to them before they even realize it's there okay so I hope this helps you kind of understand the difference between data warehousing that's the collection of the big data and data mining which is actually you mine down and find trends and the real information what do these numbers actually mean within that data anyway all that helps you out if you want to learn more go ahead and subscribe on our YouTube channel professor Walters if you want to learn more about the Serengeti check out our other channel Walters world and we do all kinds honest travel videos to help people out travel better just like we help students study for exams here we help travelers study for their future trips anyway we shall the best buy from the Serengeti 
JT-MhvsrOfc,27,"#DataMining | What is Data Mining? What are the applications of Data Mining?  In this course, you will learn the basic concepts and fundamentals of Data Mining and more.

About the Speaker: Raghu Raman A V
Raghu is a Big Data and AWS expert with over a decade of training and consulting experience in AWS, Apache Hadoop Ecosystem including Apache Spark.

He has worked with global customers like IBM, Capgemini, HCL, Wipro to name a few as well as Bay Area startups in the US.

#BigData #DataMining #GreatLakes #GreatLearning

About Great Learning:
- Great Learning is an online and hybrid learning company that offers high-quality, impactful, and industry-relevant programs to working professionals like you. These programs help you master data-driven decision-making regardless of the sector or function you work in and accelerate your career in high growth areas like Data Science, Big Data Analytics, Machine Learning, Artificial Intelligence & more.

- Watch the video to know ''Why is there so much hype around 'Artificial Intelligence'?'' https://www.youtube.com/watch?v=VcxpBYAAnGM

- What is Machine Learning & its Applications? https://www.youtube.com/watch?v=NsoHx0AJs-U

- Do you know what the three pillars of Data Science? Here explaining all about the pillars of Data Science: https://www.youtube.com/watch?v=xtI2Qa4v670

- Want to know more about the careers in Data Science & Engineering? Watch this video: https://www.youtube.com/watch?v=0Ue_plL55jU

- For more interesting tutorials, don't forget to Subscribe our channel: https://www.youtube.com/user/beaconelearning?sub_confirmation=1

- Learn More at: https://www.greatlearning.in/

For more updates on courses and tips follow us on:

- Google Plus: https://plus.google.com/u/0/108438615307549697541
- Facebook: https://www.facebook.com/GreatLearningOfficial/
- LinkedIn: https://www.linkedin.com/company/great-learning/

- Follow our Blog: https://www.greatlearning.in/blog/?utm_source=Youtube
Great Learning has collaborated with the University of Texas at Austin for the PG Program in Artificial Intelligence and Machine Learning and with UT Austin McCombs School of Business for the PG Program in Analytics and Business Intelligence.",2019-02-13T12:30:00Z,Data Mining | Tutorial for Beginners [Part 9] | NoSQL  Database | Great Learning,https://i.ytimg.com/vi/JT-MhvsrOfc/hqdefault.jpg,Great Learning,PT22M2S,false,756,13,0,0,0,[Music] the idea is to talk about big data right so I must also tell you at least a bit about something called now sequel right maybe not in depth that like many people asked questions also so normally you are DBMS systems and all are built on the idea of sequel you know what it is I don't have to teach right now now sequel databases are the databases we use in the world of big data so the simple definition is that this is a database which you use in Big Data why because Hadoop and all are not databases haven't and all our analytical platforms so what if you have a requirement where you need a real time response for huge amount of data in our DBMS so what is the primary purpose of an RDD ms client interaction somebody is interacting they should get output very fast right what is the same thing you want to achieve with big data then you will be using something called no sequel databases now this is a very huge topic and I may not be doing justice by covering this in half an hour or something so somebody see that mention so it's actually a very pretty huge topic again there are too many things inside this but just to give you some basic pointers whenever somebody's are now sequel there are four categories and that is something very important so you have something called a key value store or key value based and then there is something called document based and there is something called columnar and then there is something called graph days so failure major categories are there in no sequel so first question you need to ask if somebody is saying that okay I am using no sequel database which category and they must be able to tell you one of this insurance'll actually but how many normal sequel vendors you know like normal a DBMS how many vendors you know so let's say Microsoft is there right who else is there like that how many will be there what do you think because of another reason because now sequel is not a mandatory concept it's not like everybody should learn if your company is using it in done and in many companies when I have gone they are using no sequel databases and the developers who are working on them do you never have a complete idea and that's perfectly fine nobody is asking you to sit on one to complete no sequel alright so maybe companies use no sequel they know how to ingest the data or to store the data how to get the data and some very basic idea how to connect it and or that's enough so it's not like a DBMS where you learn theory and sit and understand internal of ayodhya DMS where the queries firing that's not required okay and it is not possible also be humanly I don't think it is possible to learn all this okay commonly when people discuss theory they will say there are four categories what is the basic difference in sequel and now sequel let me ask you this in sequel let's say you are working for our production company okay so you always do this right you will have a product table you will create something called an and just giving an example okay then there will be a customer table right then there will be a orders table correct so define your schema in the our to be a misread you always do what normalization or denormalization normalization yeah depends but normally normally you normalize it by and the basic idea is that you represent your data in multiple tables and whenever somebody want to get some answer you write a join query this has been the foundation for every a DBMS in now sequel there is nothing called a join you cannot find the word join first of all and then follow this concept that is the starting point of discussion now this concepts itself we can discuss for a day right so basic idea is that you don't represent the data like this why because if you are having small amount of data this is nice but what if each table is one terabyte then every time you have to do a join query every time these three theories have to be joined so that is not possible for me so then I come to a world of big data my data is big my table survey I want to eliminate join operations so what we do now seeker is that we create a single table we don't even call it a table something we create a single thing where all these three things are present completely normalization there is nothing called normalization this is the basic principle in this no sequel spirit which also process a lot of questions I know how on earth are we talking about a more but understand so in normal a DBMS of course first thing you are using a language called sequel here there is nothing called sequel so there itself the all the concepts that we undergone any concepts you learn is gone because no sequel and so another interesting point which people never understand is that if you are firing a sequel query pool is actually doing the job is the sequel engine meaning I am writing a join query and the intention of the join query may be produced and records or something like now who is actually doing the hard work is a sequel engine and that is why it is still in an or sequel database the no sequel database will never do a hard job meaning you will have an application have you ever thought about this you open Amazon the page right you have an Amazon Web page then you click on a product the product page loads so fast that have you ever thought millions of customers are accessing Amazon and millions are not clicking on different different product page we did ever happen to you that takes five minutes you are the product page now and look at the amount of information you have in a product page Amazon the image reviews rating if I were represented as a Terry minimum 500 columns will be there correct how do you scroll through a product page and see that amount of information it has so what Amazon does is that Amazon uses something called DynamoDB a Nizam's no sequel database is called DynamoDB the AODA is a homegrown database it's Amazon's own product dynamodb comes in this part it's a key value store dynamodb comes in this part now just to give you an idea what key value stores this they store the data in the database as a key and value hmm so a neuron has let's say a table like structure well there is a key and a value the key is the product ID the value is all the info meaning if you're not clicking on the product page product link is the key value now when you click it is firing query on DynamoDB okay and the beauty is it is just bumping all this data okay and what to make sense from the data the application layer has to decide not the database forgetting the difference in our DBMS the database to decide what should be shown here now sequel database is just like staring and producing the result they don't do any analytics meaning in this ID the value it will have a lot of things that are maybe the images of a location two images or URLs and everything but which is a petitioner application the application has to be powerful to display that since Pete and I we can share if not here here also speech would be there faster lookup and that's okay because otherwise I shall have created some tables for my is in the product and then a join operation every time somebody's clicking that's impossible I am only talking about key value stores okay that's better is also dissimilar but this is the idea so did not go here and in fact it is more slightly select queries so in it's like scanning the data so you should be easily able to find that I can get the data what would be that our application has to be site not like sequel in seeker and make great then order then join them so nothing I'll do whatever I want we will do this once you get the data that is an application layer the application layer may have anything it depends technologies or anything else for very powerful applications also there is no specific type of applications you can run any type of application you can learn like from any programming framework you can connect to no sequel right this so Java based applications and Python based application a tear-off application framework has to be very powerful to serve the need immediately right and another thing is that this may not be reliable when it comes to reliability are now sequel databases are not hundred percentage reliable you cannot say there are certain Nano sequel databases which are reliable which you present so reliability are not reliable in the sense abilities actually built using something called a base so like I said in acid here there is something called base basically are available soft state eventual consistency I think they just made it up if it's the so acid that the opposite of acid is what base then they figure out their what each letter stands for alright so so this basically available soft state eventual consistency basically but that is the principle which guides now sequel databases when I say not reliable these now sequel databases are usually installed in a tester we're gonna set Hadoop like 50 machines 30 machines and out so when some of the machines crashes when you service be available or not so that they can sacrifice so that is some now sequel databases in which you may get a connection find out that the query may not run but they may have sacrifice that for speed really faster right on the other hand which are very reliable would probably the parish may be a bit slow right so you have an option so these four categories right so you have options like whether that has to be reliable or unreliable you want speed or you want consistency etc in simple ways their own force equal standards at all you don't have acid properties at all okay so there is a classic use case of sequel and no sequel the classic use cases that you are storing unstructured data first of all so for example if you are having dynamodb this value can be anything I don't care it's not a table right so the value can be anything so I can store images or links or anything I want because what to do with the value my application will decide on our DBMS that is not possible I need my strict schema again the schemas very loose in no sequel many no sequel databases do not support the idea of a schema they don't need because it's already dumping that they can getting the data fast right less things will be handled and also is this sorry Willa without company right so one of the big data project so Allah uses no sequel database then you say no sequel database called mango DB MongoDB false in document category what is a speciality of MongoDB when we do these stores the data as JSON document that is why it is called document based no sequel so in MongoDB if you search for your data you can see that everything that key value key value key whether it's like a JSON document is called a document based our DP so I am giving you a real use case right so what is where is all as requirement if somebody is using their app you start the app the app starts there only requirement is that them should see the homepage of the app and how many cars are available so if you're opening the app and at that point in time itself map is not opening it is getting instead you will hate Ola if you're able to open the app see the cab and if there are no tabs on the cab says I'll take half an hour you are ok with it you don't mind even if you call a cab she cancels in the middle you are ok because you are stuck and I you will either use oil everywhere but you man the situation with an app itself is not opening so what all of this MongoDB is used to manage the connections initial connections meaning data how many drivers are there how many tabs are there with their location store in MongoDB and all they use are gate also so when you're opening the app where connection is actually hitting MongoDB and since MongoDB is a distributed database it can accept 1 billion connections simultaneously no problem and the queries are faster so you need open the app zebra cab and you're happy but then you actually book the cab and then you pay money and all MongoDB Staton picture sorry BMS because you cannot say the money is halfway reliability so mine will be be slinging the best shot in a wire situation maybe you will not see any cable so possible at megget stock that's okay because i won so there my you are requirement business requirement alert I won a lot of customers to be served at the same time back even if 1 million customer open apt connection should be established and they should be able to serve then you're opening the app itself query you are searching for cap so that queries handled and MongoDB so since it is a distributed database it is really fast so the queries will go he will see that I have never seen any time an overhead step you can see the cars and on initially later it's a different story right so they're they don't really care about reliability and or all the Care is a user should be able to use it the same story applies to IRCTC a FCTC was on traditional platform now they are either on MongoDB of Cassandra you can see the difference if you log in only different only reason is so I am being searching connection searching everything is nice and I search there is like 20 tickets in Odyssey then I will to transaction I booked I mean dating this hundred there is no what is there is no relation between them because my initial sales and connection and everything is handled I know sequel right and the type of now sequel IRS AP uses this sacrifice consistency that is why if you know you is a hundred seats available I see 26 available that's okay consistency is at revised what connection speed is there so end of in a transaction happens it is a DBMS it can also under is there even if you see like this flying how many delay how many cancel except like cetera imagine now I can't create a table no doubt about it main airport that's fine the draft database will be a better solution for me I can have all these are just as airports and the routes and then I can add attributes to the routes I can say in this through this many flights are flying and why this flight got canceled so your entire data becomes represented in a graph format and then you can write graph Greece not sequel queries run on graph and it is much faster so in certain use cases people prefer your graph databases to speed up Peter uses graph database you have this person then he'll follow someone and you someone see Peter he repeated so this connection is actually graph then packed with or is actually on draft database so if you represent it like that the queries can be faster because normally if I create a table the table will be like 1 million drop a million or three million the ask and reuse it right so the relations can reuse it and the queries are much faster they so they use their own format of queries under it's not sequel and then the after database not classic example is neurosurge a you know further the fourth is actually the first company to come up with where no sequel solution they were the first company to start this thing back in 2004 or something the point is you cannot practically replace a DBMS with this you may be able to do it but with very careful observation but because these two guys have nothing in common that is but they have invented a new category of now sequel this is similar to sequel now that is also possible smaller like like Roseanne table and acid properties and all but apart from that majority of them are like have you heard about this caching you you can you cache the data right ladies ladies ladies ladies is actually no sequel database but I think it works on the sequel lob platform right like you can read this there is a vendor called radius this has a high speed caching platform it's used for caching the queries but that is considered to be a no sequel database because it can be installed in a cluster and all but this also understands for normal sequel analogies and I think so there are some platforms like radius which are also very much like sequel vendors all in the no sequel category that actually means that you are using the data a big data and the tools were using select Peru or high horsepower the what these tools are specifically designed profit-taking so and you say normal analytics you can use into doesn't really matter but when you step into analytics we are either using highly responsive specific big difficult for an artist and offer the amount of data is also groups that has to be there you have to have enjoyment of 
91lfMXGfDKM,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-10T23:43:16Z,Data Mining (Spring 2016) Lecture 9,https://i.ytimg.com/vi/91lfMXGfDKM/hqdefault.jpg,UofU Data Science,PT1H23M34S,false,233,0,0,0,0,and in fact they did a poor job scheduling this is a slot when they have these talks that cage I asked them are required to come to so I'll need to move my office hours next week wednesday and i know this is the day the homework is due so so I wanted to kind of discuss it as a group probably all of you are the ones coming to class they're more on top of things but you also get the benefit of taking part of this this discussion so the options would be where I can move them too would be from there usually 10 to 11 on Wednesday I can move them to nine to ten o'clock for Wednesday also buy my lunch hour new one is free the people have preferences if my preference would be the nine to ten o'clock so I'll do that unless i hear otherwise in general is this is this is a bad time please please let me know I'm being too I I mean to shift this a few more time in the semester hopefully not other times right on the data homes I know that's the you have the most questions also realize we haven't gone back to you on the project on project proposals yet I think most of you had questions you've actually been able to talk to the person which is great that works a lot better we'll try and get those back and then next week i will discuss a bit more on the next step which is done for the data collection form just what should you be doing for that and again it should not be take take too much time but itch it will take will take some time again like the project proposal you know that was there just so you're straight to think about these topics so as you see the techniques in class you can you can kind of have in mind oh yeah that would be a good technique to the project network or thinking about with the data collection report for some people they're going to be able to download data from that's nicely format and there's not all that much to do for other people this may take a long time and maybe it takes a surprising amount of time and you may not get everything done by reports but at least you'll get started on it's better doing it now than at the last min has a new award gets to the car stuff anyway i'll talk more with that next week and finally next week monday there's no class it's like I think it's presidents day right so no class expired okay so let's let's get into it so the topic today is going to be it is going to be a silent paste cluster and so the kind of the kind of the on the thief um so the canonical example of this is is that k-means key means clustering problem so who's heard of k-means clustering before it was not heard of k-means clustering so this is a guava very one of those common famous problems in data- we'll talk about this prevent other variants and and how they all interact with each other ok so again for these problems were given a set of data points so this is a set of of data points and so often we can think of events two-dimensional points in two planes we can draw pictures and we're also going to have a distance that is is the fine in between these in between these data points and so in particular for the k-means clustering problem we're going to require we're going to specify specifically that x is going to be a subset of our d so it's going to be these each of the points can be a vector in indy dimensional space and the distance between two points a and b is going to be exactly the those exactly the hell to distance the straight line distance between ok so for King means that's going to be the assumption for other variants we'll talk about this this assumption years is not going to eat it so we can go back to this this general assumption okay and so that now the the goal in this setting is to find a set of K a set of case centers and these case centers are going to describe everything along with the D as my ex's input everything we need to know about the clustering the what differs in these in the different formulations other than k-means is going to be the objective that you want to solve for okay so with the with the Evans we talked about on on Monday these hierarchical agglomerative clustering we didn't really steak with the true objective ones we didn't say we would like to find the solution which optimizes this problem we just kind of said here's a procedure of how you blew these needs question together and then we kind of talked about which one's work for which situations with for this seventh base it's much more clearly defined so so so in particular for the key means so um ok so in general what you want to do is to minimize a cost function on the Centers that you found so a set of K centers because we had set C and in that and the data function acts and the distance here is just going to be the day when sex and that distance yours just going to be implicitly okay and so what's common to all these is that we're going to use both we can kind of specify this a bit more we're going to use this function which is is the nearest neighbor phone so this is going to be the r min of an element see I in the set C of the of the distance between X and C I okay so that we have this assignment and we're going to say that we have the set of centers but we also have the the we're going to have K clusters and these are the s-1 through s K and so s I is going to be the subset of X such that fee of C of X is equal to C okay so this is so we're going to find the closer is based on these centers using this nearest neighbor function okay so this was we talked about nearest neighbors before in existence this is the nearest neighbor in the set C of centers to some data point X it returns these centres see I which is closest to X and then the clusters are defined as the of all this data points such that their nearest neighbor is equal to Center see I of the customers okay and then these cost functions this cost is n is going to end up somehow being I think you know some function of some some cost cat function on CI si ok so I'm going to somehow have have some other cost function to find just between a cluster Center and it's set of points and i'm going to buy these together T they're going to be a sum or a max or something like that so have to be a sum or max and this cost with some well measure this is costing us so we'll go up ability for examples really quickly but before I do that let me just draw a picture of how this clustering you can make this work so our data points might be like this okay and then we could pick our plus our centers so this will be C 1 and so this the point in black here are x is going to be C 2 c3 they might be data points they might not be data points and so then to do the assignment we can draw the the same thing we saw in the near see upon this does board oi diagram in between the cluster centers okay so now the points in this cell all the flash points in here their closest Center is this one all the black points in here their closest center is this one of the black points in here closest centers this one here yes we have this board I dying again underlying what's what's going on and so and then the cost function will somehow define some he's so so we're going to somehow say that this is now clustered to this is cluster one and this one is a cluster 3 and so the cost function will define some cost of each of these and either add them up or take the maximum so so then it will apply a cost cash on each of these and then combine them together that'll be your total cross so now for the key means formulation we're going to say that cost of CX is going to be this sum over X and X the distance from X to c of c of x squared some of the units are strange so we'll take the square root here and that's but it's minimized the same way so it doesn't matter so much but it's looking at for every data point imagine to the closest center and it and it squares that distance and you want to minimize that so this is the k-means clustering problem define the set of k centers and you're given k is input typically somehow and and then you're trying to find a set c such that when you map everything to his closest center and you some the squared distances of those right so in this picture here I'm summing up all of these distances and if those distances squared I some all these up and I do it for every month right so for every point I map it to this close to Center and I square its its distance and so this the sum of all these squared length is the k-means cost um before we talk about how to approach t mean sir there are some other useful variants and and you can you know I tried to write it like you can write it in this way as well where you can instead and in some cases this will be useful this is equal to the sum over see I in C of the sum over x SI Oh right so if i write it this way it's still to sum all the distances of these points distance is it once and because i sum over SI back to the centers they don't need to have this key function kind of hack related what's up this is the same thing okay so the other cop other popular versions of this are the case center problem and this one the and we'll call this cost to this is going to be costs infinity and this is going to be equal to the max of the distance between X ok so in you can square it if you want to but it again it doesn't make a difference there's really no need to squirt that's just the maximum distance I want to so instead in this picture here I'm going to look at all the edge lengths and whichever edge length is the longest that determines my cost so I want every data point to be close to a center where's the k-means is kind of averaging over them averaging over there square plates say why are using the squared well there's the kate median is is going to be the sum of X and X of the distance X PSD and this one's natural so the comedian is where you don't square this business here the k-means is usually much more popular than the comedian and we'll discuss why that is even though it may seem strange that it squaring but turns out that's to be really helpful then there's there's another version that you'll the this is called the King me do it NE d i od'd case you can't read that it's the same as it's the same cost function as above so so again uses cost one but it requires that the center set is a subset of the data points so it fixes that the center's are subset videos games and so we'll we'll see why you'd want to do that as well it turns out that generally this where now the k-means is there only for computational reasons it makes the computation easier there's there some other reasons that if you're if you're modeling data you assume it follows some distribution that may be this is a little bit better but this one is without story is more is this one's going to be more robust the k-means also the main algorithm which is called Lloyd's out that people typically use requires requires this this property I mentioned that the distance is the straight line distance is including distance so that however it requires it if you drop that requirement then you often are stuck with a situation where you only know how to compute centers which are of this subset of the data and in that case you probably want to use this formulation instead because it's it's more robust and then they're equally challenging to do so that's why there's not kind of a cost to but the sum of the squared distance is where you will you make this requirement you you could do that also but it's it's usually if you're going to make this requirement you might as well use costs months okay so so first I haven't told you anything about algorithms in how you would do this K means is is a problem description is not the algorithm the algorithm is we'll we'll talk about okay so they need questions on on the clustering formulation this became meteoric let me try and rewrite this is that more clearance that's a ciggy I wish I could write like a typewriter okay all right and this this structure this morning I grabbed tell you how you decompose the points this is that's gonna say fix across these variants all you need is the center's to tell you how the clusters are forming and so so the only thing differing is the cost function what you're trying to optimize and so this is useful in a few ways the center's give you some notion of a model there's an underlying thing you think the data points are close to the center that represents all the data points in the set that in the cluster right so all the data points in cluster SI are represented by cluster Center cm so you kind of are really kind of summarizing the data this way that the hierarchical clustering it wasn't so clearly doing it now if the hierarchical clustering used the center as part of its approach then yeah you get the same property but this is inherent part of these things okay so so so who is seeing the comment algorithm for a k-means clustering or call Lloyds are so so for those of those of you haven't seen this how would you how would you try and solve this k means find cluster centers to minimize this can be impossible what's what's the idea so it's kind of going to be for the actually let's let's come back to the King needs let's come back to my needs are let's start with one that's even simpler let's sort of think with the case center problem so this is often called the Gonzalez cover ok so the case century memory is is minimizing the longest edge that the Mexican accent cost so so let's start with this one how are you how would you solve how would you try and find a set of centers that minimize the farthest edge what's an idea it's going to be really soon okay how would I give some scaffolding here so i'll just write out the algorithm the first step is we're going to choose the first center see one from X have arms which we're going to choose this one arbitrarily so choose some element of the set X and so as we do this we're going to maintain a set of centers and and so at first we're going to say this set this is an uppercase e is just contains one element just the first Center later whoops love c2 so later we'll have c2 equals equals c1 c2 and so forth and c is going to be c1 c2 of this see I perfect okay so so i start i have one center so far ok so now how I choose the next center the set for this from yeah good that's the thread right so that's that's basically the bargain we just have to repeat this so let me put this inside the for loop so i equals to 2 k to set C i equals to the r max of all the data points of our distance from X to is to his closest center of the center's we found so far okay so in the first case this is this is this is the set I mean blow this up a little bit this is fee of see this the set C and I minus 1 right so this is the set of centers c1 c2 up to see I'm right so at this point where where i is equal to 2 so this the first round of this is just see wat which is a single-center so it's just the point for this from it and so but after we found the first two centers we picked the point that's furthest from either of those rights look so the thing about the case center objective is its cost is determined by a single point there's one point which is the furthest away from one of its centers and so we picked that point because if we pick any other points it might not decrease the overall cost if we pick that one it has to decrease the overall cost because that one now is the distant zero so now the next largest distance is either the same or smaller the next largest distance is usually a nearby point to that one as well fredson oh so this is this is the harwitum you just run for K steps and in each step you find the center farthest away from you find a data point farthest away for all the existing centers right so let's let's run through a small example algorithm of this here okay so i'll start by picking say this point this will be C 1 I picked this arbitrary ok so now I need to pick the furthest point away from this what do you think it's maybe this one down here right then I fix c2 now it's a little trickier but what I want to do is I want to draw this war annoyed diagram again so I draw this morning dagger which now just splits the data points by a hyperplane and I happy to go through this point to let's say it's if it's on that side ok so in general they're probably not going to go through the battery within it if you can assign it cited to their point if it's right on the line that means it's the same distance even point so it doesn't matter what side of you ok and now we need to find the furthest distance from one of these points from one of the data points one of the center's and it's all these points and these are close to on the centers so I'm going to tend to pick one of these much over here and I think the first one is this one that goes right through the line so narrow this one will be C 3 and then i can draw the board i doubt your witness yes so then this is the 49 ER after three steps I just keep picking up for this might work and this looks like it did a pretty good job there were 10 points here here and here and found centers in each set I did I did really well so yeah okay is something that we determined yeah cases were given on the input if not one thing you can do is you can run this out for a larger k and then and look at the cost and if and it kind of there's this this elbow technique where this is the value K and this is the cost function and the cost should kind of come down like this and so probably thinking something around here the cost is not going to get too much lower if I run it again I'm just going to pick this point and the furthest distance is going to go down not by too much we're picking this one went down by a lot right so you're going to kind of drop off through the quickly and they're kind of level out so so you could run longer and then kind of go back and say that was the right otherwise it's just give it a ok so this is the album it's it's very simple you can you can do it in the runtime you can what's how long does it take to run ok yes you can do this in can if you're if you're if you're smart about it right so each the slow part here is so I mean you have Cade time to go through the step step to write those the factor K there and then you have to take this arc max right so you're looking at the maximum distance here so but you don't want to the thing is this our back includes this nearest neighbor cost of SATA code so you don't want to recompute all these nearest neighbors every time most of them stay the same right each of these data points has the same year savers before and so actually their cost is the same I don't want to have to recompute its cost I only want to update the cost of these data points which are closer and so if I know the old assignment if I keep track of and where they didn't points are assigned when I have a new center I just need to scan them after I must choose into Center and you just scan them again and update their cost right and then this will allow me to quickly pick the quickly freaking excellent that's you can do this in in km time I have K squared ended in my doubts but think that's things that they only need to do with k tons yeah you should only need to K times at top you have to do two scans of the data for every round of K but each of those dancing hope it is that people roughly see how that works I've written down some pseudocode in more detail the notes online so if you want to employ you can implement k times it so it's pretty quick I mean this is compared to these uh these hierarchical clustering algorithms that if we're really careful in some cases you get them to the N squared log in now it's basically linear time k top 10 cuz k is usually much smaller than that's it so this is a much faster faster out okay book what is what strange biasness is this guy what is the effect of running well okay look let me ask let me ask a different question um I said my goal was to minimize the longest badge this saga was not guaranteed to minimize long as such I kind of I just ran it really and then I I said okay that's good enough I think if you had a bunch of data that was in a circular area you don't know picking you might start out at the center of that circle and then you just start picking all the points are on the edge of the circle or something and alternating yeah you would you really right well the pic so take turns out think I mean when you have is the right intuition so let's say that so let's think of a data set for a lil draw a circle first and then I have all these data points inside here including points on the edge of the circle and this is some in the middle and so let's say I first picked a point here in the middle then i'm going to pick a point on the edge of the circle and then something far away and they've into the circle I'm going to keep picking these ones here eventually it's going to be instead of picking this point it's going to be further to pick you know some point here that will be further eventually you'll start pushing back inside again I mean if your data looks like this I don't know why you're having so many clusters in the first clips again if it's well toss durable this will probably pick a center in each of the clusters but yeah so what property though is that initially and this is true even if you do have clusters let's look at another example here let's say that my my data points are kind of in these circles what I'm going to do the first one may be arbitrary then the next ones are going to be kind of on the boundaries of these circles I would rather have these move into the middle here it's going to make its they represent the data better if they're kind of central to their cluster this case centering algorithm is not going to pick something central to it is just going to the Gonzales I was disconnected points kind of the battery you know but you know that's not so bad as long as you divide them into the right sets that's probably okay if you want the model to be better then there are other harms you could run maybe as a post processing step or instead of us and the k-means out is going to be one of those are the Lloyds r4k means well we'll talk about that next and so in fact you should be able to see that if I move that the center to the middle here I should get a smaller for this distance right if I considered this point over here it's closer to this X than it was it's closer to this accident opposed to this other data point so I'm actually making my objective smaller bites entering these so what this algorithm says is that it's this algorithm has the proper that it's going to be a two approximation so that means it's going to find a set of centers such that the cost infinity of the center's it finds is going to be less than or equal to two times so where this is um so if if if sea star for the optimal centers that I guarantee the center's I find by this algorithm are going to be within a factor to a cost of the best possible so this is not too hard to show I might ask you to show this on the homework this will this will be true as long as the distance function is a metric this as much as a metric than this is going to be true so I might ask is probably like a long discussion I'm sorry but that's it's not too hard to show it's just a few line proof so it guarantees to get close to the optimal cost so how about an algorithm that does better can we do do better than a two approximation how would we do better think maybe doing this step where we centered them after after we found the clusters it turns out that it's empty hard to do better than a two approximation in general for our fur inside any metric space so NP hard to get a 2 minus epsilon approximation so if you from any epsilon greater than zero so to get a 1.99 approximation that's NP hard trip now if you are doing this they're if they're couple caveats myths if your data is in is in two dimensions and you're using euclidean distance you can get like a of 1.87 approximation in that case something like that using a much more complex arm okay that's that's not really do feel I would steal it probably Eustace difference between a 1.87 as if it's and below that again as ideal but if your data is its well cluster abell then this probably works much better than into a proxy so it's a it's it's just that there are some special cases where in jail you can't you can't do better than their two approximation so you might as well use this algorithm if this is your objective and maybe a little optimization again very simple algorithm very scalable the only downside is that it's very it's requires the previous round on the form to complete before you do the next one so if you want to do this in parallel or something like that then it becomes it's not clear how to do but otherwise it's very efficient if your data if you're going to allow k rounds in case okay actually it's pretty pretty cool okay let's okay so let's talk now about lloyds out for k means and so this is a tribute to Lloyd in he apparently came up with this in 1957 but he didn't publish it until nineteen eighty-two apparently he running down to place it was never officially published and finally someone said you should you should go and officially publish this but it took what's that like 25 years so it's a long time again a very very simple acronym oh um is it's going to be a little more it's not going to be purely greedy like the case in arbutus chickens all sorry let me write this down and then we'll talk about it so instead of in in this this arm instead of choosing one center at time I'm going to kind of try and choose all case centers at it but I know I'm not going to do it right the first time so then I'm going to try and fix it and then keep fixing it so I'm going to choose hey points see it was a subset of X and so so so maybe this can be arbitrary so arbitrarily choose K points and this is going to be my initial set of centers again I'm not no longer building up these centers I'm always going to have in fact okay centers and so now I'm going to repeat the following process so there will be two steps inside of here for all X and pecks define the right so this is going to be to assign or reassigned to the closest center ok so in the first step I just had the center's and I sign everything to the closest I haven't haven't haven't done and then in this step 44 I in the set k so for each cluster right center i'm going to set i'm going to reassign the center to be the average of si of this SI and remember this is the set and so again I'm going to average based on the set assigned to the center this is all of all the data points where their closest point is the same Center okay and then I'll run this say until saying is um it's it's converged and I'll talk a little bit with that means but it's at some point I'll stop either when things stopped changing or maybe else stop before that if things are changing but very school okay so in order to compute the ad- center I need to know this the center assignment I need to know this set of all the data points of its center right and then give it a set of centers I'm going to recompute this defines the set of centers I can recompute the assignment of the points and each time the center is an average right it's going to it's going to give me an average so it's going to always going to be in the middle of all all the points of my days right so I'm not going to get that example I got there where the center is on the mound every time I run this step I'm going to remove this is data point on to them to the center okay so let's let's uh let's let's look at some of these steps in a little bit more or here what do I mean let's look at this step average that's I right so this is if I might take this set so I ok I want to read a few to center here I have a set of data points so now I figure I have this set and I know and I actually don't care where the old center was I want to create the new center here i'm going to create the new CI from the sesame but the average well the nice thing about the average is that i can do it one coordinate a time so i can basically look at the average x coordinate and also look at the average white coordinates and that are maybe no overall average right so is so this is basically one over the size of the set times all the data points in here and I look at X was a choice so cfx suppose we see see lower case X all s is this I I'm going to look at the export rooms right I can I can take the this set look at this guy's x coordinates and I average to export it that gives me the x coordinate of the center and I and I do the same thing with the white boards to do these independent guests one of the nice things about the average is 1 these things that's I can do this coordinate independence if I rotated the accord with Frankie this is actually you know give the same answer so that's again that's one of the nice things about you working in the game in the euclidean space the other thing about this new center I is if I do take this average I cut with the average of all the other points it turns out that this is equivalent to choosing see I equals the arm min of some point in our d that's minimizing the the sum of the squared distances okay so the average turns up minimizes the sum of the squared distances to all the other points not the sum of the distances that turns out that's called the elbow medium the sum the average which is very easy to compute right I just need the average of the x-coordinates average of the white coordinates that's give me exactly the men of the minimizing point of the sum of the squared distances which was my objective function that's why I wanted this is my job and I wanted this as my and what this is my objective function for cami's this has had been a point of confusion that's the difference of some pretty funny points in history where there's at some point there was a I think the US government asked has someone to track the point in the US that minimize the sum of all the distances to all of the all the residents in the US based on the census and so they computed the average which was not minimizing the sum of the distances it was analyzing the sum of the squared distances and so if you know the you know how the population of us change over time it's actually having these outliers that right there's there's some people all the way up in Alaska and it started on the East Coast and people moved out all the way to California or Utah on the way out there and and so there was a fairly big difference between the sum of the squared distances and the sum of the distances and so they had this wrong are like hundreds of miles I don't think it mattered that much but I think the I think they still track the was called the l1 medium and I think that's now in the in the it's like in somewhere around somewhere out st. Louis but it's been moving further south reason you can see some charts at this online or sometimes will track the average just because they're easier to eat okay so so this average step is really easy to compute it gives something which is really in the center of the data set every step in the center of the data is trying to approximate and so this the saga this is is very simple very easy every step is not hard to do the two a step is to find the nearest neighbor we already talked about the event and most days have people do this exactly just by checking because the set of centers is often small if your sender centers is big then you can use one these data structure we for you have to rebuild these data structures so it's not clear you're going to get that much of an advantage doing that but you could have some cases okay so let's talk about the convergence if is this our you're going to converge to the right answer is this kind of is this or is this going to converge is this like I'm doing kind of two things I'm reassigning Andrea setting centers and then I'm reassigning the the points to each Center like sometimes when you run these iterative algorithms there could be a situation where you get into an infinite loop where you assign the central over here and then and then you think of you have three centers and they keep kind of shifting a rotating the assignment kind of moves with them right this is this out of going to kind of um is this going to get into an Internet loop is this going to convert to the right answer how many steps does it take what's what's going on what do people think so also what is it so it's a convergence so how do you know what to bring okay okay well that's the right answer good and you you maybe maybe they're great intuition or inferred from the multiple questions I was asking but yes oh so it converges that means it's not going to get second influ otherwise we wouldn't be talking about but may not find the true minimum right again like the key Center problem it's it's it may not actually find the ultimate optimal solution right it made active the radio in fact it may get stuck in some situations where neither of these steps changes the result but there's another solution which is better let me try and draw an example here so you could get in a situation let's say you have some data here some data over here and then some data or here and you're given three centers and if I choose one Center here once under here and once that are here what's going to happen is my lord I diagram is going to look like this and then it's going to look kind of like this this is my morning diagram and so each of those red centers is in it it is not going to change it is the average of all of its data points or maybe I can move this one down here it doesn't have to be one of the data place and with that one down there these are all the average of all their data points and each data point is in the right corner but clearly there's you know if you look at this there's there's another solution which is going to do do a lot better right the blue one is going to do a lot better cost but the red one is a local minimum so that how it was not going to find a blue one like this so if you get the center stuck in the wrong place you might get stuck okay so may that lease the global maximum what you should probably consider doing is trying to try with a different sort of initial centers and we'll talk about that in a second and and then and then rerun it again and probably eventually will do all find the right solution if it gets close to the right solution it will converge to it but if it gets stuck someplace like this red situation then it might not be able to get out version well it converges to a local minima meaning it stops Yeah right so the question is how do you know that it's going to stop and so the way to think about this is to go through the cost function right remember our cost function remember we have the cost of X seed is equal to the sum over X and X of square right so this was our cost function and a purpose to see that both of these two steps they only decrease this cost function okay if I'm assigning a difference if I'm assigning a point to a closer center I can look at that individual point this term in the sum decreases right so so that the step 2a has to decrease this cost function because I found a better nearest neighbor posterior ok and the averaging step is exactly this art min it's the it's the returns the center which minimizes the sum of the squared distances to all this all the points in the set and if remember I rewrote I set up here that you can rewrite the k-means cost the sum of overall all the center's times the cost in that Center in that cluster right so that those are equivalent I've just broken up the summit of parts and so I've minimized each of these inner terms every time I did that centering stuff I'm fine if I return the minimal Center for that the cost Musco it must not increase it right if it's the same Center than the cost hasn't you know that then the cost has a has a decrease but has it increased either right so that means every step they or me here I am decreasing this cost function medicine what that means is that I can't possibly have and any step does the same set of centers or if I do that I know I the same centers have been changed or kind of the center's is this these can be continuous they really could be anywhere in space but the set SI these clusters this is a is a combinatorial set there's a finite number of possible ways of breaking of the day of partitioning the day it's it's a lot it's like was it it's it's n it's a it's k choose and different ways of bringing out the data no it's like a shoes and if it's k to the power n ways of a partition the day indicate busters but that's finite so I know it will eventually stop you know they're there are so why can you see there are K to the N so why can't there be catered impossible clusters well the way to think about that is you have you an N so there are enemies of these data points in each of these you can think of storing and this is actually a good way to implement the algorithm you have an array and the array goes from 1 to N and in each element of the rate you store value between 1 and K great this is either 1 2 3 up to 50 k so in this array that's from each data point mapping to a cluster it's a value 1 through K and so if I can choose K possible locations here k there and so forth each of them has k independence locations it can be our assignments that can be so there are tatum and possible clusters so it's it's finite it will eventually every time i do a new assignment of clusters I need it different to lay here and every step along that assignment the cost decreases so it eventually is converging the cost is going down and certainly so you know that's so there are a couple ways to deal with this problem of and one point hasn't converged you could say that those do arrays that you have of the center assignments has has not changed in between between the steps but you put what kind of something that's happened to you start getting very small movement where if you're for instance if you're in a situation like this like this bad case but you have two centers and there's a big there's a big cluster right here you may kind of move around very slowly with the very subtle rearrangements and if you're in more than two dimensions then this kind of can sort of have ants and so you don't want to so see me don't want to run to a completely stops and also that's a little bit trickier to to check you have to kind of us you store your old array and then check if it's exactly the same as before maybe you can just have to keep a bit of whether any of the center's was reassigned but the other way to do is just keep track of the overall cost and if overall cost has changed by small enough amount then then you can sell so that's more common so that so if costs change is smaller than some so it's smaller than some threshold so set some threshold for the difference between the cost values or maybe some relative changes change less than one percent say okay that's probably good or maybe you just say run for Jamie iteration usually 20 iterations is more than enough on most reasonable data sets okay so assuming you started with a good set of centers right let's go up and look at this top line here as well assuming answer with a good set of sinners it's a pretty easy algorithm it's guaranteed to converge to a local minimum but if this first set of centers is fairly reasonable men dish that this work saying it gives centers very representative of the data inside their cluster as well so that's why this is such a popular are very easy to use generally works well if the data is well cluster of all this should generally work work about if there are nice well-defined histories um okay so but there are examples like like these this this is kind of this red example is pretty this bad right this is not what you want and so if we're in in two dimensions we can like plot this data and we can look at them and say yeah these red centers are not only one but if you're tuning you can plot it you can also just circle the right centers that we don't write so so this is useful in higher dimensional data where you can't really see what's going on so much you may not know that this has happened if something bad like the red centers on this happened how would you prevent this from happening random starting centers and remember Yenisei compare the results not really sure how you figure out which one's better yeah so so how to choose the columns to the initial C centers so so one choice is random right so the chosen at random and then you run it and you can compute the cost really easily the cost is kind of almost naturally computed as you're as you're doing it so then you can compare the cost with one cost is much lower than the others then you take that solution right so compute C centers run it for 10 20 steps and then try it again and keep whichever one has a lowest cost so Keith cost solution after and so important to do this after running voids are so don't keep it don't think case centers and then choose one of them based on its lowest cost use it run Lloyd's on for a few steps and then pick the local calls that are the question so random it's a pretty good idea but if I have this situation and say I've about a third of the points in each cluster then you know a certain it's it's it's not too likely to pick two points in the same one right so if I think the first one over here the second one over here then about a third of the time the third one will be over here and so you know if you have more clusters if instead of three clusters you have like 20 clusters k is 20 and they're all kind of in a situation like this symmetrically then the chance that you get one point in each cluster might actually be might actually be pretty low this goes back to the coupon collector car right you can expect them to take about a walk a samples until you pick one point each cluster well so if we know that about computers then why don't we pick k log k sentence and then we can cluster with kale oo k instead of k and then read group together the center's so so so run lloyds with k log k centers and then and then and then try to regroup these and then you can use like hierarchical cluster you only have Kayla key data points not too many you can then kind of merge these together until you get can regroup with hierarchical of llamaron festival ok what's the wrong time to water flowers ah it's so it's a number of steps you've run so the number of times for the for loop which may be 10 and each step take some takes end it depends on this nearest neighbor problem ready need to be assigned to the nearest neighbor so the averaging stem takes linear time even across k clusters you divide up the cost the assignment to the nearest neighbor again you may need to use an advanced near sabor de structure two runs all right reserved darling k centers you can just check each one against the case so this so this step in most homo k times and full of it you can check their only case fairly small compared to him so you probably ok running king of it and then number of loops is probably custom so the disappearance of youth in care centers is on perimeter of clusters yeah so why name is character first round good good yes that this was the other another answer I was looking for you Gonzales he's used to Cazalas case center harbor red suit so this is another good idea using it's all skate center in one force that you are not too far from any points right so so you know get a people actually it will never wind up with with this solution here it won't wind up with this because I mean you could have picked a data point here but either problem either of these points is further than then then any point here is to this one right if not then it probably doesn't matter so much if it's not that big a difference then needs to kind of look like 1 plus ratings right so so use if we use the case enter our rhythm we're going to avoid this situation so it turns out that the case center is it is a good idea but there are some tricky situations where that one doesn't even always thought you can still get into some of problem cases okay so to get around this there is another algorithm called k means plus plus and so this is going to work that this is going to work similar to the palace algorithm but it's more tailor-made to the case to the k-means cost function okay so so let me write this down shooters see one in X and and so this is the first one is is arbitrary and again i'm going to maintain this set of centers so this is the for the first I centers in the set and so then for two then I'm going to again you a for loop k and now I'm going to since I flew so I'm going to choose see I from X with with probability proportional two and so instead of choosing the one that's the furthest value here I'm going to choose one proportional to this distance so the ones that are further i'm more likely going to choose okay so instead of biasing towards really far away points I'm going to choose points that more likely over there the problem of case center is that if there's one outlier data point I'm going to put a cluster Center I'm always going to choose that now it turns out for a lot of these formulations you sometimes that is the right thing to do sometimes it's not the right thing to do and there's kind of some banner case where it's far away enough to be an outlier but not far away enough that you really need to produce the same and and so this will kind of it will sometimes find out better points but it will more likely pick some point around where there's a lot of other data plans and exactly proportional to the cost function you're tryna venomous instead here and so you you still may need to run this a few times randomly this is a randomized algorithm I'm choosing with probably proportional to the distance so so so I may need to run this a few times because sometimes it will still pick that outlier which is bad but it will only pick it very likely if it really needs to be a center by itself okay so kind of we start a bit late and there were two more things I wanted to talk about so I'm going to talk briefly about and they're both of them are brief so well talk briefly about one and then I'll come back there should be something here I'm going to choose this proportional to this distance how do I do that how do i right now are able to do that i'll come back and tell you in a second so if you need to go at four 20s enough to watch the video otherwise i'll spend other five minutes doing that let me say something about the cave median and came me do it cost versions here first this for a minute as i just started talking about canned beans is susceptible to these outliers it's more likely if there's not a point to say I really need to put a center there even though there's nothing else around it came comedian because it's only the sum of the distances out of the squared distances which makes that I or even more important it's more likely to pick a center within the actual is so this is more robust to these allergies they became medium for much the other thing about this lloyds algorithm is that there's this nice way I can take the average which minimizes the cost the sum of the square cost to all the points in in that in the inside the center's there's not a similar closed form the cane mean there's there's something called the l1 median or the geometric median that is the point that minimizes the sum of the distances but there's not a closed-form friends there is an iterative step you can do instead so inside of the iteration of assigning reassign of the center's you can to find the new center for us for a cluster you can iterate towards that and it's a pretty easy update stack and so you run that for a few durations and that always decreases the cost and then you go back and we assign points the center's if you repeat and so you can fit another step inside of here but both of those things both of those iterates of things that are guaranteed to converge in that way they need essentially the data to be in to be using the Euclidean business some needs you can adapt to some other manifold structure but in general you need the you kidding this is if you don't if you're using like the Jaccard distance instead because you're talking the distance over sets then there's not a notion of what is the average of assets of sets I don't know what that averages anymore so then instead of that you can choose the center as one of this one of the data points and so he discusses a little bit the higher up goal clustering and that this is called the key meteor so often you use this if you you use this instead of the comedian akeem because it's more robust and so use this if your distances is not the naturally as good in business then you probably want to use evolution of this and if you look at matlab i think it has a key needs built-in thing and it came meteoroid i don't think it has a king eating because you usually want to use this one or that one just for your speech okay so that's there's some other cool stuff about this I won't get into let me you may want to know how to implement this on your homework so so maybe i'll tell you how to do that so so first so I'll finish this just in a few minutes and then you can ask me more about a lingering maybe another class at five time all discuss this again this is a really cool thing that comes up over and over again so start by saying what's the sum over these all of these all these distances right so they can compute a sum and let's call this a let's call this m and this one is called MX right so this is so just this distance for the X des plaines MX right so if I kept that this sum then the probability of choosing paths should be equal to MX over the summer right so now instead of choosing proportional to this distance I can choose with this probability that I captured okay and so this is the value between 0 and 1 right and that some of these have to add up to 1 because I knew the sum of all the MX is ended up to him and are divided to buy big heavy good so so the next kind of thing to realize is that what i can do is i can take this this probability space and i'm going to think of carving it up into chunks so this is this is mx1 this is xtube is in this chunk so say this was like point zero seven this is points 09 this is M x3 up to xn so this chunk right I've divided up this probability space and excellent now I can I have this this this built-in thing where I can choose a random number you between 0 and 1 and this random number is going to index each one of these values so I divided up the interval between 0 1 with these probabilities and I can generate a random you and wherever that lands in here the bucket that it falls into is the element I choose okay so if you've gone you divide it off this and you can do this by just um so this is threshold right here this threshold which is the end of MX three this is equal to the sum x equals one two three of em X i right I just added up all the probabilities up to this point all right so that tells me where that endpoint is and so i can calculate if i calculate each of these probabilities and then i can calculate the endpoints this is basically something like the cumulative density function all over there's not believe in the ordering is arbitrary right so then if I want to do this I can I can mark these boundaries and then if I pick this you I know it falls in between these two end points i've calculated that it goes there so one way is just a scan across just kind of there some ordering then I keep checking is you greater than this one is a greater than this one when it's greater than it then just double check it's less than the next one then if it's no longer greater than this one then I know I've hit the right the right font I can also do something faster if I have this this thing with these with these boundaries here but I can build a packet that can build a binary tree on on top of these right I can build a binary tree on top of these values and so then if I have some value you that's that's coming in I can walk down the tree the bucket so then this only takes log n time to 5n and data books so so this is a fascinating okay so you know this this optimization you know for really big in it actually makes makes a big difference if you mail a pic love random things although this is going to change every step when you're running this key music + + algorithm and and so you can just rebuild this into a scan because you just need to use it worse there's another cool trick called called the aliasing trick which uses hashing which allows you to do this in constant time that if you come and ask me after class about that this one more thing about this k-means must close algorithm it's kind of there's some sort of guarantees about that say that if you run this randomly a few times and either it's going to give you some guarantee of how good you approximate King means the camions objective by running Lloyd's out of them afterwards either there's some some guarantee for any data which is like Gauguin guarantee or a constant factor guarantee like you got for the case center algorithm assuming the data is not too too crazy but it can guarantee that you don't do things bad and it actually works better this is this hour the k-means plus plus how it was maybe ten years old now and it's kind of its kind of the accepted way to pre-process lloyd under ok so that was for assignment based clustering we'll talk about spectral clustering next week wednesday it's gonna be kind of a completely different way of refreshments great except cushion the number you have to come in just one around if all the things were equally likely and that's going to be possessed me the case right but you're going to do it proportional to this squared distance from the center so some of them you want to be core language if I go back up to something that looks like this say i pick one of these records and this risk night then all of these are closer they're scared distances further assistance let's say I just pick one red marks here all of the points over here are going to have a very small square distance all of these are really very large so very light they're going to pick one of these as next week either here or there you know they're offering the anonymous induces the large 
QVdwJdI1eUs,27,,2018-04-01T17:12:47Z,Lecture: Association-Mining (DATA 630),https://i.ytimg.com/vi/QVdwJdI1eUs/hqdefault.jpg,Firdu Bati,PT42M40S,false,800,3,1,0,N/A,"hello my name is foti Botti and this is a lecture on Association mining for this a data 630 and this lecture we cover about some of the basics of physician mining such as logic and the principle behind extracting rules how do we formally represent knowledge innovation mining and we cover about some of the commonly used formal or quantitative metrics to evaluate individual rules extracted from transaction databases and frequent itemsets those are the metric used to measure the validity and relevance of individual efficient words in we covered a very commonly used simple position mining algorithm called a priori algorithm and we go over an example application or physician mining and are in subsequent modules of this course we will study two broad categories of machine learning algorithms namely supervised learning and unsupervised learning in supervised learning you have a training sample where you have annotated targets or dependent variables and from those of the region you apply some kind of logic or algorithm to fit a model or a function to estimate future classification of the target variable in the context of classification in the custom in the case of regression application where you target variable is quantitative value then that is for estimating or predicting the quantity value of the target variable that is a supervised learning paradigm in unsupervised learning you don't have a target you don't have a level or annotated dependent variable so the exercise in unsupervised learning is logically create group mix of the observation or the instances in your data set typically the Komen metric used for creating that grouping is some kind of similarity matrix yeah and clustering is a primary example of that kind of suburban unsupervised learning here in a solution mining which is also a type of unsupervised learning the objective is to find objects or attributes or variables that happen together with relatively high frequency now that frequency what is that what is considered frequent is formally evaluated using a quantity metric that we will discuss in this in this lecture support and confidence are the two main quantitative metrics to evaluate the frequency of items as well as well as a confidence of the rule extracted from those item same and then we will cover those in subsequent slides so what are some application of our station mining now assertion we can apply station mining in different domains in different contexts but one domain that popularized the use of efficient mining particular is a priori algorithm he is a retail industry so that's an application wherein for instance in retail store you go for shopping you purchase different items and that makes one transaction in the items are collectively called your basket it does a one transaction in your retail store collectively from different transaction potentially can extract useful rules but identify first a frequent item say things that are purchased requests together and an extracting useful rules to say maximize profit in for instance one strategy that you see an example application in a widely you know mentioned application is by putting different items that are purchased together in you know same area same location you're maximizing the purchase all of those item so that's one application and a very common application of missing a priority a similar application is for instant search items what are terms that are frequently used in certain charities so potentially we can identify those journeys that are used together for its Sam a OES of searches or some type of three okay you can also apply efficient mining in other domains such as engineering application or any safety domain for instance one area that I have used efficient mining is for extracting factories that lead to an in desirable event or accident in air traffic operation so when you fly it when in a particular flight goes wrong usually there are factors that contribute to the outcome of that event so by applying decision mining you can potentially discover rules useful lures that by identifying a factors that are responsible for that kind or a similar kind of event or undesirable okay okay what how do we formally represent knowledge in ization mining in that they the goal or the objective in decision mining is to leverage a relation between different items okay and you die to identify useful patterns called rules okay for example if you have a database with items such as ABCDE F typically what is involved in extracting useful rules is first you identify or discover the commonly up occurring frequent nonsense okay in this example for instance if we determine a CD to be frequent items again that is formally quantified we use it through the use of the threshold value for the support matrix which we will cover in an upcoming slide now you can potentially extract the rules such as ad leading to an outcome in C from this item set okay so the exercise in position mining is first you identify frequent itemsets and from those item sets you can't particularly extract useful rules that such by some level of confidence okay formally a one important thing that to note here in the difference between the model we have in efficient mining and other supervised learning another supervisor on the context you have a training set you apply some kind of learning rule or algorithm and you fit a model so you have a single model that you can employ for future classification or regression application you don't have a single model what you have is a collection of rules that will collectively make your model okay so you're extracting rules this is not a prediction application okay formally this is how we capture the rules in if-then statements all right so if items in st. X happens and it will have some level of confidence to satisfy the rule as being potentially useful leading to an outcome in Y that is an out gain in item say that in said Y is a rule the the part that captures the if part of this statement is Co formerly called anticipate in the items in what are called Quinn sequent or the dent part of the rule so next a simple example is if you identify the rule to be useful or interesting satisfying both the support in the confidence level such as these what this means when we interpreting and this this rule is if a customer buys milk you would have some level of confidence that that customer would also appreciate is a break it's the same goes for diaper and beer again you can apply these in any domain where you have objects or variables that can be combined together in some pressure evaluation metrics okay so we talked about I don't find frequent items in from the side and say extractive roots but how do we formally qualify those items aids in confidence and the extractor rules so one of the the widely is not commonly used for a prior algorithm is support support is the person who turn of transactions in the data set so you you put some some threshold value for the support and things that are considered frequent our major again is that threshold value okay so informally this is how often the items appeared together in proportion to all the transactions for you to is can the entire transaction and then identify those frequent item state based on a support threshold okay so it is a probability value is a probability of the items occurring given that transaction in the transaction database in the way you taught me that is formally support of an item say is you count the number of transaction containing those item sets items that are being considered and divided by the total transaction in there in the database okay typically because the application of Association mining is in large databases where you probably have a sparse data right so unless you put the support value low you want identify potentially interesting rules so in application typically you use a support of a small value support and if you think about it the most interesting rules don't happen as a frequent in a frequent fashion because if they do you will probably have a knowledge already don't need any sophisticated rules to know that are useful okay interesting patterns are something that don't happen quite often but when they do happen they use really interesting and they're not obvious okay so support is usually safe and is a corresponding metric that goes with support and a priori application is confident again the first step is you discover the frequent itemsets using a support threshold once you discover the support the frequent item sale then you extract rules from the identifies are already identified frequent itemsets okay so confidence is a percentage of the total number of transaction in the data and the data set in relation to the rules to the frequent item say not to the transaction because you know in the process of extracting the rules you don't scan the entire transaction you go over each frequent items hit and extract the rules and the rules are determined useful or not based on the confidence so informally that is how often the items in the consequent appear whenever the items in X or in the antecedent part of the transaction occur if informally this is how you calculate that the confidence of the rule is a conditional probability which means a probability of items in the consequent given the items observer team in the antecedent part or it can alternatively calculate that as a support of the rule over the support of the the item since in the and visited and unlike and support the confidence usually say to a higher value like 80% 90% okay because you want to have a level of confidence in the rules that an extracted from the item set the frequent itemsets okay in a third that means the third metric that is widely used together with support and confidence in a priori algorithm is cordless it is a type of mitri that combines support and confidence and this is another way of safeguarding from rules that are extracted by a random noise or chance so it compares the frequency of an observed pattern with how often you would you would say that just by sheer chance II mean it is formally correctly the lift of the rule is the confidence of the rule of the support of the items in there and the consequent so typically you want a large values for lift because the closer you are the least east to a body one is likely it is higher likely that that rule was extracted by yeah just random random noise or chance higher the lift is the higher the life here is actually useful or real rule okay also sometimes you use leverage that really uses a real of rate but there are some cases where you have both high support and high confidence with a corresponding a low lift he sometimes those which could be kid to be interesting so you look at the leverage of those rules to further investigate them and the way you calculate an average it's a Loveridge of the rule is that the support the difference between the support of the rule and the product of the Questacon support the constant incident in the support of the consequence okay so how do you go about extracting the rules the strategy in general involves two steps the first one is you find all the items that occur more often that means those items are satisfied the support threshold and then once you do that you go to the next step of extracting the rules but during the second step that means doing a rule extraction you don't have to visit the entire transaction in the database you just go over the items of that were identified as frequent based on the support threshold and then you taste each rule each potential rule again this is a confidence threshold can keep those as interesting or potentially useful oops and we in subsequent slide we go over this two steps so for identifying all the items its frequent items entities you have to travel through all possible combinations in the transaction database and that is a company finality very expensive it is exponential in the number of items so for K items for instance you have 2 to the K minus 1 items for instance if you have 10 items you know transaction database possibly you can you have to visit 1023 combination different possible combination of items it right as you go you know as you go to 20 for instance you get into a million so you can see how quickly the surge of space grows exponentially so there is this property called anti monotone property which was pioneered by the a priori algorithm is saying that if an item said is infrequent if you determined that a given items it not satisfying the support threshold then all of it is super states are also infrequent that is if you have for instance in your transactional database ABCDE as items and if you determine that a be not satisfying the support threshold in other words if it is infrequent then whole item set containing a B that means all in the superset would also be infrequent in you so you leverage this property to cut or to team takes palatial searcher space and this property is called anti monotone property in applying this strategy for cutting that the search space is called support based polling okay visually it is easier to understand they say if you have five items ABCD in your transaction database so in a bottom-up sir if you start with an empty set and then you go to the next items that means containing all items containing one element ABCD in a individual right in this case all satisfying the threshold value the support your body and then you move on to the next level of items hit in in here a B two items 8a b AC in in so forth so if you determine that a B does not satisfy the threshold value or fun to be infrequent then all of it is substance that is in every set that containing a B will also be infrequent so there is no reason for you to strike that space you can completely ignore it in prune kept the search space by applying this property so applying this property you can significantly reduce the surges faced in this is called on-time monotone property so this is the first strategy that means you apply this story in searching the items different items and appearances so the next step is once you start to identify all the frequent items say to the transaction then you extract rules from those items it's okay that means for each frequent item set you track all possible rules that's find the confidence threshold so you'll have threshold for the confidence in how you have the separate result for the support as we discussed in the previous slides so the way you do it is by partitioning the items it into two non-empty subsets remember each rule will have two components is the if part in the then part okay so that means each possible item said each frequent item said will have to be split into two okay in computing the confidence does not require additional scan this is another important item is because you are working against the item that are frequent there is no reason to try to traverse through the entire transaction in the database because you already by the second step you have already identified or the frequent I don't see okay so until there is two to the K minus two possible way of splitting a given item say okay so for instance if ABC is your frequent items saved to the two to the third minus 2 6 capitation of Candida rule right so that means if you don't apply in an optimization search optimization you have to visit all this space and test again it's a confidence to shop that means a to BC B to AC C to a B so every possible split into two components as you have to fit in the way you calculate the confidence for each room is by dividing the support of the I frequent item set over the support of the antecedent in general if the item said over already I mean you go over the items that are frequent so if the items is infrequent any subset of is frequent right so that that properly holds okay then initially to the strategy that's applied in here the way the rules are extracted is first you identify all the rules that have only one element in the Quinn sequin part and then subsequent items that subsequently rules are extracted from a rule that is our this is strategy plan now the entire ottom property that we love right for identifying frequent items say to cut the number of searches of the search space does not in general apply for the confidence part that the rural generation part but it applies partially to individual rules individual items in rather so that means if if X leading to the difference of Y on X no splitting the two does not satisfy the confidence to a salt then a subset of the the antecedent part leading to a corresponding Questacon also does not satisfy it's easier to explain that in example first and if a CD leading to B does not meet a confident threshold one of the rule that we extracted from a CD B item said which is this one a CD on the antecedent part in beyond the consequent part if you find that not satisfying the conference and treasure then all the rules that contain a subset of the antecedent will also not satisfy the confidence to show anything containing these accepted a c BD a C 2 corresponding BD ad - BC and so on and so forth all those are a subset on anticipate we are not satisfied you are also applying partially the anti-monitor property on individual individual subsidy again is easier to explain than visual graphs here so ABCD is items that they are considering to extract rules from so if you happen to find BCD leading to air for instance he and not satisfying the confidence threshold that is if it is a low confidence rule that means a new ruled that are extracted from this same with a subset on the antecedent side like CD CD is a subset of B CD b DS or a subset of CD all of those leading to resulting in corresponding Qin second items will also be low confidence room so that means you can KITT you can cut the dissipate the search space applying this logic or this strategy on each individual items okay so this is an example of doing a station mining in our by the way I will not provide details about some of the function in that rules package in the aerospace package for each module we'll have tutorials provided by the THL the details of the programming aspect of each module each topic that we cover in each module will be covered in the in the practice exercise in by the tutorial videos so this is primarily an overview of how we do station minding an R and for this particular dataset it's a books data ok so first thing in any data science effort or in any data mining project is understanding the objective of understanding the problem domain ok so you have to have the data you know understand the nature of the data in this case we are trying to understand the books purchasing patterns of customers of our customers our own transactions and based on their purchasing pattern or rating pattern we want to identify those customers but they should need to do some kind of promotion to those customers based on their interest ok this is just one application so the transaction will be the customers in the books that the rate of purchase will be the okay children swagat for the the the relevant packages for aerospike rather a priori algorithms or a rules package and it was this package one is for actually extracting the rules the aerospace package for visualization sometimes you need to visualize to have a better understanding of the rules generator okay here is a few cases of the data as you can see here with a few instances of the data the user ID they are waiting for any given book this is a title of the book for for the purpose of extracting rules in this particular example we are not going to be using the rating okay again not unique to Association mining in any project data mining project most likely you will - you have to do some kind of data preparation is unlikely will you find that data will be clean most real application you would have to clean the data so for instance in this application you have to remove any unique fields if you have some and also because a priori algorithm cannot handle quantitative values you have to do some kind of discretization as we cover some of this correction technique that we covered in module in March I want you can apply for this case okay so first thing you load the data here I'm using the tree transaction function which is a way to if the data doesn't count in transaction format you can you can use this function if already is in transaction format the data has been already formatted then you can just use one of the functions as simply reading function like the rate CSV file function so we reading the data here and we loading in a single format of a single what that means is each items will be on its own row individual so now the option easy to load this is not basket so that individual will have all the items set in a basket so this is a tab separated file to the separation which have and we're interesting to learn only the user ID in the title multiple and we're removing duplicates here just think here that some customers purchased you know different version of the same book so you don't consider that most important difference okay here is the summaries of the loaded data as you can see there are nine 92108 transaction that's how the rows to customers in there are two hundred twenty thousand and four hundred forty-seven ions both okay you're a few cases of the book titles like five cases in the corresponding customer ID okay okay so here I apply the size function which is part of the arils package to calculate the size of the basket and here is the summary as you can see more than 50% the cases have only customers are only purchase one item so the basket size is just one basket sizes for each customer and the number of books they have purchased okay there's some outliers as you can see you know some cases where you know paint house and lid 150 items another function is item frequency which counts the books in here normalizing by the number of the basket size here and I'm shorting it so this is just a basic kind of exploring the data so I listed the books and I sorted them according to the number of frequency their frequency and as you can see the highest frequent items is this right here is called wild animus and it happened 2500 in two times okay so we slide we have seen that the transaction has a total of more than 90,000 transactions and here even said the most frequent item is only 2500 which is if you just normalize it by the number of transaction less than 3% okay tells you the level of the sparsity in the database and you would elaborate that information before you fit a function this is part of the data preparation that you have to do so this is just me in the same information visualized again a large number of cases are small counts you know most are aggregated in this area so as you go further in the number of countries a number of kids are very small okay again the same information is splitting in visualization and here as you can see 50% of the cases right here are lasered one by definition when you are doing a session mining it's a pattern idle ticketing process right if a customer purchased only one book there is no really in pattern that we can learn from this customer so you can simply remove the number of cases and do some optimization pre optimization for the searches space now in all the discussion that I mentioned that there are some optimization in the search strategy through pruning like you know support based pruning but the reality is a priori algorithm still makes multiple trips or meticulous counts of the transaction data bestows it to the extent you can cutting the number of transaction that are not relevant for the rules is getting optimized destroyed further so means here I removed all the cases where the number of transaction or the number of items is the lesson one I'm keeping only greater than one that cut in to provide more than 15% it wasn't I need something I need more than 19,000 transaction now potentially I am looking rules only through the 40,000 and 123 of cases or transactions so these are the kind of trendy I prefer preparation depends on the data that you will be working with but you have to always look at the data first in to the extent you can clean the data or optimize it so that further analysis can be handled in an efficient manner okay here is the priory rules so this part of the arrows pocket this is the main function that fades about extract rules so I am passing to this function the transaction database data set and I'm passing to parameters of these are the core parameter that we discussed earlier the support and the confidence here I'm saying again we discussed these typically you put low value for the support here I'm using point two percent and for the confidence 75 percent okay now that returns the rule so if it goes through the transaction and apply the the logic that we discussed earlier in extracting the rules that satisfy this condition so it returns one not 191 rules here and this is a distribution mean it shows here that the language of each is palatable of the rules so Reutlingen is basically the count of items it was in the left hand outside of the rule in the right hand side some of the items in each row so here is the distribution 11 of the rules have only two items that means one on the left and one on the right in hundred of the cases of three items 66 row of the rules for items in 1400 supply by default a primal algorithms in our limits to five length of five row length of five which you can change but you know right now the best part of the reason why you don't see more than five it was limited limited to five so here is most most words contain three items as you can see there are hundred rules with rulings of rulings of three and here is summary the summary of for the support the confidence in the lift on aggregate for all the 192 rules his support and the means the first quartile quartile the median the mean in the third quartile for the 191 groups so this is an aggregate law okay it's just summary of the rule and here I'm using another function to inspect the rules generated yeah so I am picking the head which is five or something and five all sorted by confidence and I'm inspecting them and their display right here so for instance right here the antecedent of the the left side of the rule is you know this is the books these are the items in the transaction so for to score high five seven are to for the doll so these four items on the antecedent side are leading to the three to get diddly on the quints upon set so that means customer who purchased dis books also purchase these books in that size with a nine eight point eight confidence in the number of kids that satisfy this conditions are point two percent which is the support in the lived as we discussed earlier which combines support of the confidence it's a high number rather high number so this is more or less a real rule which was not generated and if there were random noise as he goes you know we displaying five rules the next one is the hypotenuse book seen just from looking at it it just kind of makes sense that people would buy books in series and to purchase the next sequel so you know Holly Potter books all these customer purchase days also buy this book which is only a sequel and then again the corresponding support level and confidence in the list of odd or displayed again this is this isn't some kind of inspection you can you can do once you pick structural it is in general its iterative process it's not one time to make ego about because based on some domain only the rules that are extracted sometimes are obvious in you know when I use them they're not interesting because there's existing knowledge about the iterative process and you inspect rules she modify some of the parameters that you passed for which Routledge attraction function and so forth okay I only used two options earlier when I called the function a priori function right but there are many other fun options that you can pass to the function called one of them is appearance appearance is the way you control the number of items in the left-hand side in the right hand side okay I have used it but this is it's a commonly used option in that function another one is an interest major again we only covered lift support and confidence when we discussed about the formal major quantity metric to evaluate different rules but there are other depending on the data set depending on the application you might want to consider to look at other metrics to evaluate rules another important item element of rule extraction is removing redundant rules redundant rules are rules that are a specific version of a given rule that did not provide any additional knowledge so an example is if you have an item a resulting in items C on the consequent side and if there is an a specific form of add rule that quack that contains a and B and antecedent section resulting in the same C with the same level of confidence that rule is a specific version of the more general rule so you're not learning any anymore unless the confidence is higher and their specific ate so in fact there is a function or a option in R to show you the benefit or the additional information again of that specific rule if there is no additional benefit or information gained then you can safely ignore that the way there are two ways removing these I think the latest version of our eros package in R has a built-in function to remove the redundant another way of doing that is using matrix manipulation are using like diagonal matrix which is covered in the tutorial practice and most likely the TAS will go over this as well okay but it's important to know about winning redundant rules visualization allows a critical component sometimes you know visualization helps a lot to identify group if there is any grouping and any patterns or any correlation that it can't be seen visually so the air air rules this package provides some functions and they can do a different kind of graphs or visualization scatter plot pile coordinate group mosaic and matrix so again the practice tutorial will go over how to generate interpret these plots other algorithm for oxygen mining in in this lecture I went over only priority algorithm but this is not the only algorithm although it is the simplest and the commonly used one the optimization the search optimization we discussed such as the support base pruning even with that the a priori algorithm does multiple scan of the transaction database so when you have large databases likely that the performance of a primal algorithm is not optimal so so sometimes you might want to consider other other Association mining algorithms such as the equivalent class transformation regret pattern dynamic programming I haven't used any of these for production use beyond in reading and experimenting with them for just my own understanding but I've just been good to know that they exist on their different application or a different way of doing a station mining as far as the resources go there are a lot on the way was you know and we provided quite a bit of resources in the class in the module page and there are also numerous tutorials on YouTube that you can find that goes as I go on and provide details about how you we do associated mining a Priory and others but I think these the the the ones we covered here provide the basics and the foundation of a parabola algorithm you may have some questions which we will address in class and thanks for listening "
KNV8U2vGz48,27,"This course aims to introduce advanced database concepts such as data warehousing, data mining techniques, clustering, classifications and its real time applications. 
SlideTalk video created by SlideTalk at http://slidetalk.net, the online solution to convert powerpoint to video with automatic voice over.",2017-05-25T05:24:05Z,Data Warehousing and Data Mining,https://i.ytimg.com/vi/KNV8U2vGz48/hqdefault.jpg,SlideTalk,PT9M48S,false,11265,55,4,0,2,data warehousing what is a data warehouse single complete and consistent store of data obtained from a variety of different sources what is data warehousing a process of transforming data into information definition a data warehouse is a subject oriented integrated time variant and non-volatile collection of data in support of management's decision making process WH in moon subject oriented focusing on the modeling and analysis of data for decision-makers not on daily operations or transaction processing integrated integrate data from multiple data sources time variant all data in the data warehouse is identified with a particular time period non-volatile data is stable in a data warehouse 7y is separate data warehouse high performance for both systems DBMS tuned for all access methods indexing concurrency control recovery warehouse tuned for a lab complex a lab queries multi-dimensional view consolidation different functions and different data missing data data consolidation data quality data warehouse a multi-tiered architecture bottom tier warehouse database server back-end tools and utilities are used to feed data into bottom layer from other databases contains metadata repository data are extracted using application program interfaces known as gateways for example on ole and JDBC middle tier laughs server implemented using one relational olap ral up to multi-dimensional lap my lap top tier front-end client layer which contain query and reporting tools analysis tools and/or data mining tools extraction transformation and loading at all data extraction get data from multiple heterogeneous and external sources data cleaning detect errors in the data and rectify them when possible data transformation convert data from legacy or host format to warehouse format load sort summarize consolidate compute views check integrity and build indices and partitions refresh propagate the updates from the data sources to the warehouse metadata repository metadata is the data defining warehouse objects stores description of the structure of the data warehouse scheme' view dimensions player marquees derived David if data Mart locations and contents operational metadata data lineage history of migrated data and transformation paths currency of data active archive or purged monitoring information warehouse usage statistics error reports audit trails the algorithms used for summarization the mapping from operational environment to the data warehouse data related to system performance warehouse schima view and drive data definitions business data business terms and definitions ownership of data charging policies a data warehouse is based on a multi-dimensional data model which views data in the form of a data cube defined by dimension in fact a data cube allows data to be modeled and viewed in multiple dimensions example sales with dimension time branch item and location multi-dimensional data model dimension tables such as item item underscore name brand type or time day week month quarter year fact table contains numeric measures such as dollars underscore sold and keys to each of the related dimension tables and debase cube based cuboid topmost 0d cuboid holds the highest level of summarization called the apex cuboid letters of keyboards forms a data cube cube a lattice of cuboids conceptual modeling of data warehouses star scheme' a fact table in the middle connected to a set of dimension tables one large central table fact table to a set of dimension tables snowflake schima a refinement of star scheme' where some dimensional hierarchy is normalized into a set of smaller dimension tables forming a shape similar to snowflake fact constellations multiple fact tables share dimension tables viewed as a collection of stars therefore called galaxy shallow or fat constellation example of star Shamu sales fact table time underscore key item underscore key branch underscore key location underscore key units underscore sold dollars underscore sold eighth underscore sales measures example of snowflakes mo sales fact table time underscore key item underscore key branch underscore key location underscore key units underscore sold dollars underscore sold eighth underscore sales measures example of phat constellation sales fact table time underscore key item underscore key branch underscore key location underscore key units underscore sold dollars underscore sold eighth underscore sales measures shipping fact table time underscore key item underscore key shipper underscore key from underscore location to underscore location dollars underscore cost units underscore shipped 20 a concept hierarchy dimension location all Europe North underscore America Mexico Canada Spain Germany Vancouver it wind L Chan all region office country Toronto Frankfurt City multidimensional data sales volume as a function of product month and region product region month dimensions product location time hierarchical summarization pass industry region Year category country hoarder product city month week of this day a sample data cube total annual sales of TVs cuboids corresponding to the cube all product date country product date product country date country product date country 0 d apex to Boyd 1 D to Boyd's to DQ Boyd's 3d base cuboid [Music] typical app operations roll up drill up summarize data by climbing up hierarchy or by dimension reduction drill down roll down reverse of roll up from higher level summary to lower level summary or detailed data or introducing new dimensions slice and dice project and select slice selection of one dimension resulting sub q for example time equals q 1 dice sub cube selecting two or more dimensions for example location equals Toronto or Vancouver time equals q1 or q2 pivot rotate reorient the cube visualization 3d to series of 2d planes other operations drill across involving across more than one fact table drill through through the bottom level of the cube to its back-end relational tables using SQL typical app operations thank you 
CdEOGKci3mc,22,"In this video, we provide a brief introduction to data mining.",2021-04-05T17:18:18Z,Data Mining Part 1,https://i.ytimg.com/vi/CdEOGKci3mc/hqdefault.jpg,IFM Lab,PT4M42S,false,43,0,0,0,2,[Music] okay hello everyone welcome back to a new week so in this video we'll move on new section data mining for you and this is also the last section we have for this course in this semester in this step we plan to provide with a brief introduction about data binding database systems for you and if you want to know more information about data mining you can undertake the development coursing state what we cover here will be a brief introduction about how to use data we have in database systems for mining some knowledge all right so i mean this is a new section before we start lectures i have a reminder according to the schedule in this week we have today nice we only have two weeks left for this course in this semester for the lecturing and we plan to finish the data mining section properly in this week with two classes if we cannot finish in this week then we can have some courses some lectures for the data mining section in the next week on monday as well i've gone through a schedule of trading alliance in this week the first one is a paper six review report and second one the homework two and both of them will be due by friday and also remind the full project the final report will be due by next friday this will be the final paper for your project and you should cover all the information about project in a paper format and we also have one final exam and we plan to have a final recovery we plan to have a final review for you in the next week on the wednesday class and we can discuss about how the formats of the final exam for you and how to do the exam in sushi master okay so this is schedule for this week as well coming weeks so please recall according to schedules and the day nine we show here they're all fixed and firm the extension will be granted for any reasons you may provide so please do this works as long as possible so you can make sure you can finish all of them before their knives alright so let's come back to the slides so we can see i mean this week we're planning to talk about data mining what is data mining i'm not sure if this is a new term for you or not probably some of you have taken the data mining course already we know we have lots of data in database systems however i mean we have so much data in the real world but we don't have enough knowledge actually suicide data we have in the real world grows a lot in recent years from terabytes to petabytes and we have different tools as well technologies to conduct data we have which can be very automatic and we have automatic data conduction tools and we also have the database systems we have the web and we also have a society based on computer science right and we're getting lots of data from the real world and the data connect can be from different sources for example can have data from business based on the web based on e-commerce based on transactions stock markets essential okay i'll connect data from the science i mean by some like scientific discoveries we can connect the data from by the remote sensing as well as bioinformatics scientific simulations for example of course we can look at data from the society as well as everyone right we can get the data from news from digital cameras and from youtube as well to facebook right so the database can be abundant however we don't have enough knowledge we can extract from data which will be the main focus of data mining actually and so the target for data mining is to discover or extract some knowledge from the data what is knowledge so from the according to this page data mining can be formally defined as extraction of interesting this interesting covers several perspectives it should be non-trivial implicit as well the previous ongoing and potentially useful patterns or knowledge for the huge amount of data we can have in database systems and we also have different names when we are referring to data mining actually in the real world as wedding research we can call it as knowledge discovery in database systems and it is also called kdd right i mean knowledge discovery from database the first chart will compose the kdds world yeah i'm not sure if you know the conference for the sika kdd which is all the top tier conference in data mining in san diego besides kdd working out on them and then mine as knowledge instruction data pattern analyze as well the main other names you can see from this page so they are all referred to data mining actually then you need to be very careful if every single nature of the data will be data mining or not we have some other exceptions right and some simple search and cure processing we cannot see them as data mining actually as well for some expert systems based on deductive rules we cannot say they are but there are mining needs right so we have some exception in this page 
8YV-EIgIOvo,27,"#DataMining | What is Data Mining? What are the applications of Data Mining?  In this course, you will learn the basic concepts and fundamentals of Data Mining and more.

About the Speaker: Raghu Raman A V
Raghu is a Big Data and AWS expert with over a decade of training and consulting experience in AWS, Apache Hadoop Ecosystem including Apache Spark.

He has worked with global customers like IBM, Capgemini, HCL, Wipro to name a few as well as Bay Area startups in the US.

#BigData #DataMining #GreatLakes #GreatLearning

About Great Learning:
- Great Learning is an online and hybrid learning company that offers high-quality, impactful, and industry-relevant programs to working professionals like you. These programs help you master data-driven decision-making regardless of the sector or function you work in and accelerate your career in high growth areas like Data Science, Big Data Analytics, Machine Learning, Artificial Intelligence & more.

- Watch the video to know ''Why is there so much hype around 'Artificial Intelligence'?'' https://www.youtube.com/watch?v=VcxpBYAAnGM

- What is Machine Learning & its Applications? https://www.youtube.com/watch?v=NsoHx0AJs-U

- Do you know what the three pillars of Data Science? Here explaining all about the pillars of Data Science: https://www.youtube.com/watch?v=xtI2Qa4v670

- Want to know more about the careers in Data Science & Engineering? Watch this video: https://www.youtube.com/watch?v=0Ue_plL55jU

- For more interesting tutorials, don't forget to Subscribe our channel: https://www.youtube.com/user/beaconelearning?sub_confirmation=1

- Learn More at: https://www.greatlearning.in/

For more updates on courses and tips follow us on:

- Google Plus: https://plus.google.com/u/0/108438615307549697541
- Facebook: https://www.facebook.com/GreatLearningOfficial/
- LinkedIn: https://www.linkedin.com/company/great-learning/

- Follow our Blog: https://www.greatlearning.in/blog/?utm_source=Youtube
Great Learning has collaborated with the University of Texas at Austin for the PG Program in Artificial Intelligence and Machine Learning and with UT Austin McCombs School of Business for the PG Program in Analytics and Business Intelligence.",2019-02-12T12:30:00Z,Data Mining | Tutorial for Beginners [Part 8] | NoSQL  Database | Great Learning,https://i.ytimg.com/vi/8YV-EIgIOvo/hqdefault.jpg,Great Learning,PT8M14S,false,804,8,0,0,1,[Music] interesting thing about this EMR cluster is that it also has a GUI where you can easily upload the data to Hadoop cluster and download the data so one thing you may be wondering is that if this cluster is here you can use form and is filed so let's I have a file just want to upload how did I do it so let's say we have a file so I'll just go to my Google Drive I should have some five so this is a typical transaction data from a retail store we have a transaction ID and there is a transaction date is a transaction date we had a customer ID an unspent category of item vivo puzzles within that he Bob jigsaw has a city of Charleston or state of South Carolina and a credit transaction so it's a typical transaction data now lets you know you're not tested you want to create the table from this file okay and then you want to probably analyze some data let me see let me show you how easy it is to do so all you need to do is if I go to UM are there is something called Q when you see a chewy just click on this U and here is basically a developer right to the cluster certain this open whew and it will ask me to create to use a comp since I am doing this for the first time so I will say my user name is Raghu and the password is some secured password is the create an account account created I land in and I will just switch the version because this is I think you four which is bad I need you three when I can do that I can go to so is already here you can see this right I can create everybody is live here so let me see browser tables i think the tables that what i will do i will go to files and this is my ID this new cell array this folder is actually my habit and if i want to update a file i can just drag and drop it here so you can see the file is getting uploaded so now this file is available in Hadoop and my intention is I want to create a table from this point then I want to query the data to be that I can simply go where there is something called tables and go to tables and there is a default database I can say this plus button create a new table and it will ask where is your data my data is in a file and there is a file will just upload already this transaction I select it and once you add the file the automatically figure out the format this is like Alex hoping right so automated so see it is saying that there is a field one flow field 3 these are all the values you are having is this correct yes it is correct so I can simply say next and it is going to automatically assign all the so I just click the submit it is saying creating table default or transactions already exists whether they exist now I can just go here and again go to my play base menu and there is a terrible transaction if I open this table there is the sample data alright and ok so now the table is created so you did not write the video statement it automatically created it internally rotate if I want to query that I can simply go to query ok and here I can say select star from 2 X NS 1 limit I don't know now the baby is let's look at the data so I want to just query something different I want to select all the data where some condition you want to be what condition will you give you cancer where the state is yeah so that is which column lost with one column hmm so here it is filled 8 right so I can say select star some transaction is one number to say where field eight field eight equal to single quotes or double quotes I think single quotes single call site California so I want to show you something if I run this query I will get the data now you can also visualize the data here I can go here okay see I can set up the type of visualization a charge this does not show anything because the query is simple but you can have all these sorts of visualizations in a data scatter map pie chart bars remember what bi plus so here by default allows you to do the same things on Big Data so it's very nice you can also download the results as a CSV as an excel file export as a PDF so this future is available on EMR also in this castor I mean I if you want to run start where to start a spark shell ok you have to say stock R but you may not be able to show you completely where Brazil take time you need more unit our knowledge but you also need to understand spark a little bit so there is something called rdd's and data frames and we need some details so to build but you have to stop a sore spot works on the command line normally you can use UI but that's not very effective there is a spark shell you have to start in that shell you have to type the commands that you were required I want to show you one more thing I told you there is a UI for our resource manager here it is you are asking how do you look at the jobs you are running right can you see here yawn yawn yawn so three queries rely on all three here can you see some of them fail so this if you look at here it is not showing that because it is an internal IP that is why it is throwing an error but it says accepted default and also you can publish it from the logs or EMR there is a problem with logs actually but in the yarn window you can see that you can run the queries and see the output here so this is the spark history server there is something called spark history server where all the so no application found if you are doing something in spark and you close that it should come here so I will not terminate the tester in Amazon I told you there is something called s3 also this is Amazon s3 this is where you can store like I said as you can see all this beta right if you open you can just upload the data like this so very easily you can upload the data and s3 is like an FTP server you just upload it and keep it there you cannot modify the data just bump it and download it that's all 
u2oSiVOQRmg,27,,2016-04-06T23:54:54Z,Basics of Data Mining,https://i.ytimg.com/vi/u2oSiVOQRmg/hqdefault.jpg,Prabhudev Konana,PT9M50S,false,88154,379,12,0,10,"in this video let's look at the business analytics methods a very simple introduction we are collecting massive amounts of data your search surfing data social interactions you have transaction level data healthcare data enormous amount of sensor data from the Internet of Things we have data that we are accumulating from business applications or data I bought what we listen to what we watch and so on then we have very unstructured data where we comment a lot on message boards or on social networks we make our recommendations on various sites all of these are part of what we call the Big Data now we are accumulating that amount of data but we don't know how to extract useful information from the data that we have we want insights and that's where the data mining is gaining a lot of attention this includes data about the financial markets and other things that have been people have been trying to extract insights for decades so there is big motivation study data mining it's a process of how do you analyze predict and discover insights from data so it's being applied to many business social and economic problems and is another whole area of this study in scientific computing scientific data it could be in healthcare genomics for example or it could be in weather patterns so there are lots of things going on on the scientific side and we won't look at that because you need to be little bit more knowledgeable in computer science methods now everything is coming together we are cheap computing power if you want even rent it from Amazon Web Services we could get the statistics statistical methods to navigate massive amounts of data with thousands of variables and often missing data then we have machine learning approaches that uses computer science and statistics together then we have all this recommendation visualization and new forms of data like Twitter or social networks everything is coming together today so we have still scratching the surface on how to extract insights from this data now if you want to be an expert in data mining obviously you have to have a big foundation in many different areas so one is of course statistical methods then of course you need to know databases and the data mining methods which we'll discuss later optimization part what the typical operations research is also part of this the topics like decision analysis by taking the insights from this you apply decision analysis then you strategic business analytics and the behavior aspects of this once you have background in the six core areas then you can apply in many different context just give you a sense of how much data we are accumulating from 2015 to 2020 we are literally increasing the data by seven fours so we are going to accumulate something like close to 35 zettabytes of data not everything is stored obviously but nevertheless is 35 zettabyte what do they look like just get you an idea one zettabyte is 1,000 exabyte and 1,000 exabyte now one exabyte is 1,000 petabytes and it goes on this is massive amounts of data so what is data mining it's a process of extracting new insights now we could extract insights that we already know about and we have been doing that for a long time so that's not really very interesting we need to be able to extract non-intuitive results for instance it is very hard to conceive why people who buy beer also buy diapers yeah it shows up of course you when you go back in analyze why it happens you realize it often happens in UK when you are in your parent you cannot go to pubs because of the pop culture what did what they do is hey they come home they need some diapers they go to your convinced your convenience store they end up buying beer so that's how you find these relationships but there is one thing about understanding this relationship in Alec's thing is what do I do with that of course you do promotions product placement you can do supply chain how much do inventory decisions you can make okay so you can do many different things from this inside on these insights so data mining is a process of identifying and discovering meaningful correlations patterns and trends using simple database queries like SQL structured query language or statistical or mathematical techniques so these are some of the examples in which you find being applied data mining these are what we call Market Basket analysis what do people buy typically together when they buy how do we promote them these are what we call the cross-selling and upselling opportunities when to discount what to discount what coupon to give to whom how do you find fraud patterns in healthcare particularly insurance or in credit card business so there are many different well-known applications and this is where the demand is today trying to have people who can understand how we can extract insights so let's there are there are numerous data mining methods but they are classified into two major ones one is the descriptive methods so you are looking at from the data are there any interpretable patterns you are not doing any significant statistical analysis by using simple method can it describe the data then you have the predictive methods where you have lots of variables you have some data can you actually make some prediction about the unknown about the future and you already done some predictive tasks like the regression in your statistics class so let's look at what we are going to do in the next few videos basically we are going to look at the three major data mining tasks that we often use one is the Association rule these are nothing but the correlation between two or more items is also called affinity rules so if I buy cheese or if I say if I buy bread I am likely to buy cheese so you can think about this as a conditional probability of C given a means we already bought a I'm likely to buy C then you have clustering where you're grouping things of similar characteristic together they may be customers with similar characteristics they may product which are similar in nature okay so you're clustering based on certain characteristics it could be one characteristic or many different characteristics but you should be able to cluster them and that's also descriptive in nature and the third one you're going to do in this class is a classification method which is predictive in nature let's give a simple example hey I want to give loan to somebody will this person return the money we lent so based on the profile of that particular customer maybe the income number of children the household size exit current debt and so on I should be able to predict what is the likelihood this person will return the loan with interest of course there are methods for data mining one is regression within regression there are numerous types of regression and I'm assuming your learned that in your statistics class we'll ignore that then coming from the computer science domain you have neural network genetic algorithms and again we go into those details we won't go into the detail so three things going to look at in this next set of videos association rule also called affinity rules clustering and classification so often you also use data warehouse the term data warehouse in data mining what it means is that how do you bring together different types of data within the organization in a consistent form aggregated so that you can extract meaningful insights from that data all the data is actually stored in a data warehouse in the past they should call it online analytical processing OLAP data these are meant only for decision making we won't go into the details of those our data warehouse but you need to understand data warehouse includes data aggregated data to make decisions "
H16hC1z_TNM,27,,2020-04-09T06:02:47Z,Data Mining (Spring 2020) - Lecture 22,https://i.ytimg.com/vi/H16hC1z_TNM/hqdefault.jpg,UofU Data Science,PT1H1M9S,false,269,2,0,0,0,"um so hi everyone so welcome four four four four take two of the of so lecture 22 on so on so Markov chains so I we we tried recording and it's somewhat earlier today during the regular time and they were a bunch of technical issues with zoom so we're we're gonna just start over again from the top so the the topic today is going going to be in Markov chains which is kind of this this like fundamental kind of kind of this fundamental concept within mathematics and so we're going to kind of describe how this gives an understanding of graphs right so it's Markov chain is is a more kind of more powerful object that has extra extra conditions and the goal today will be saying well how can we drive you know many types of markov chains from crafts and then what are the conditions under which this processing models on property of the graph and then and this is a nice way I like to interpret graphs because it ties into the linear algebraic way of way of thinking about them and so and which is you know and you can think of them as statistical and house really which ties in with a lot of the way it's other things we we talked about in this in this I'm so so so far this course the there there are other ways also of thinking about graphs this way it's particularly appealing because it mats to PageRank which will be the topic of the next lecture we'll talk about different ideas on graphs on where they so the PageRank often that's associated with the web graph that apply as many other places we'll talk about some social networks another instance of large graphs and how to think about them there are many other very common toriel different ways of developing grass as well they come up in data mining we're not going to so much cover those within within this class I'll go a little bit in the last lecture and then we will yeah so some of these ideas will carry over to other things than these main topics but this will be the main main focus for us all right so let me start by sharing my screen oh okay yes so great so yeah so um so that thing off of there okay so so the kind of I mean we'll be going through a lot of definitions and try and kind of spice it up a little bit we had talked about some life lessons you can learn from Markov chains one is and we'll see how these kind of all these how the how big things kind of fit in um so so the first one is hey it's only your composition matters going forward don't worry about the past I may get this up here so the second one is you only need to worry about one step at a time you you will you'll get there eventually or maybe it won't and then the third one is in the limit everyone has this perfect car all right so we'll kind of see these this time throughout the lecture so okay so we're talking about graphs and so we saw graphs before and this is the same example we use when we talked about spectral clustering which is the goal then was to find a way to cluster the nodes in the graph of the the the Versys of the graph and that was kind of using the the edges in order to do this and so what was what was important useful about this is that you could represent it by picture or by the vertices and edges but well as nice as this kind of graphical representation where you can say that by the the adjacency matrix and just to quickly recall this meant that if we considered an edge between C and D that I can say C is connected to D and there's a 1 here ok but this is symmetric so if I looked at D and C that is also here right so this does not have a direction on the edge in this case it's going to be the same in both directions today we'll talk about ways where you can have a direction on the edge and then you don't necessarily need to have this entry so it only go in one direction from C to D you would have a one but maybe knocked in on the other direction also note that if I take some other edge say G and H and I go from G to 2h there is a zero here because I don't have an edge here so this is not an edge so it has a zero right so this gives rise to this adjacency matrix which we see over here which is just all these ones that zeros in this this will be the starting point okay but when we deal with the with the Markov chain it's not exactly the same as just starting with the graph we're gonna have we're gonna have the same kind of node set here I'll use vertex and node kind of interchangeably in the lecture and this V will be the same ok so it's gonna have three components this is the vertex set okay it's gonna have a matrix which is is going to be down here is in the same example which is the probably transition matrix which is similar to the adjacency matrix but I've added one thing in that hive for every column I look at every column here I've normalized it so that I've taken me the adjacency matrix and this represents of node B has two edges to a and C so that's the one here and the one here but I've divided by the total number of nodes so I divide everything by 2 inside of each column ok so that's saying I'm at node B and I want to know I'm going to transition and basically is modeling if I'm at node B I'm going to take a 1/2 probability to D that's that's this one here and a 1/2 probability of walking to a so if I just have the structure of the graph that's given to me I can convert it into this probability transition matrix where I assume that add a node I'm going to if I randomly walk around this graph what I'm going to do is I'm gonna each point pick a random edge and randomly walk on one of those edges and make it uniform right so B has to as as has two edges so it's 1/2 if I look at another one like E it's got 1 2 3 edges and so a b c d e right then it has 1/3 probability of going to see it's got 1/3 probability of going to 2f and 1/3 probability of going to G right and then in the case of say you know like like H over here then there's there's only one option to go out to F and so this this isn't it's already normalized ok great so so so so so we've got this Markov chain and ok so right so so this has covered two parts this described their vertex set which we get from the graph their probability transition matrix which is also can be derived directly from the graph we also if we have different information like directedness information so it's not symmetric notice the probability transition matrix is already not symmetric because it could be normalized and in in different ways so yeah for for instance this one half and this one-third are going to be sorry not this one through this one-third here these both correspond to D and B this edge right right here it's one half from E to D but one-third from D to B so it's not symmetric already but if we had different symmetries we won in two different kind of weights and probabilities we want to encode we could easily directly encode them in the probability transition matrix so we have more power over the the kind of the information on the edges and the graphs we can we can encode okay but there's there's a third component to every one of these of these probability trends to these markov chains and that's this Q this initial State okay and so this is not inherently part of the graph okay and I have two examples of this of this initial state here one is a vector here which is 0 1 0 0 0 0 0 0 0 ok and what this says is that my initial state is always in B I'm I'm my initial States in be okay but you know or it doesn't need to have this thing where the state is only in one location the state could be like this one instead right and that what this one is saying is I'm one-tenth in an a okay that's this one I'm one I'm 3/10 in D and I'm 6/10 in this would correspond with F and F so my initial state is now just not just kind of a single location but is the probability distribution of these locations where I where I could be and I could start out this way and we'll see that in in in a number of the kind of approaches to analyzing this week we will consider we will actually be generating these probability distributions over States as soon as the kind of internal representation of what's going on here okay so these so we have this initial state and a Markov chain has this node set the probability transition matrix and then also the the the initial State okay but if we want to analyze a graph this doesn't really Nestle this initial state doesn't really have a place in the graph okay okay but let's let's kind of see what the Markov chain is doing and then we'll see how can we get rid of this initial State so we just talking about the graphs okay so let's start with an example and so I've started with the same probability transition matrix here right that we had in this example and this in this initial state which is basically in B with probability one right but after one step so I'm basically starting here and after one step half the time I go to a and half the time I go to D and so my state Q one after one step of this process and this is kind of describing a process that precedes the steps now my probability is one-half an A and one-half and deep just as this column is is telling me okay but then if I keep going in this process now I want to get to a second state instead and remember I had 1/2 in a and I had 1/2 indeed this was my state after step step cute one here and I do this again I said 1/2 well I split that into three pieces to a B and C and this other 1/2 from from a I sent to three pieces into B C and D and I add those up I get 1/6 and a there are two components there are two six and B 2 6 and C and 1 6 and D ok so I can repeat this I can also think of applying this directly the probability transition matrix twice on my initial state here that's another perfect interpretation or I can take this whole matrix here write a whole probability transition matrix and I can square it I can think of combining this together I've taken this term and I've applied this first and I get this P squared matrix which is just P squared and I can just apply that once that describes the process of doing this step twice but inside of a single and inside single matrix okay so the order of operations within matrix multiplication or vector matrix multiplication you can you can multiply things in any order sometimes it's computationally faster than one way than the other but you can combine them I can either do combine Q you know here first or I can do the P squared part first okay and I see this mass is distributed but I can only get the steps ABC and D with different probabilities I do this one more step now this kind of I started with one six two six two six one six and this spreads out even further and notice that kind of for instance what's new is this two six one third of it is going now to e and so now I have this this this factor one 9th which is winding up and E yeah so that's yeah so that's to 18 they that's what's going to 18th is 1/9 okay so it's spread out further still after three steps starting in B if we go back to original graph you see you cannot get to on F G or H yet these are still super probabilities okay so to come the big question you should be hopefully is coming to your mind as let's we can keep doing this in the can we say something about in the limit this vector qn right let's say lim as n goes to infinity what's happening right we can think of either combining this into a single p to the n like this whole huge number Tundra just applying this over and over again what's gonna happen when we keep applying this and as this seems like a nice quantity to kind of understand where is the distribution okay so the first point to make which will go and see later is it's not this is not uniform okay and then the limit of well not necessary to be uniform 1 over 1 over n everywhere okay but but the bigger issue is we cannot always define this notion of a limit so for a certain kind of processes we don't always have a limiting distribution and so in order to use this tool this idea of look what if we do this random walk and do it for a long period of time this this will as well see kind of next lecture this will say something very fundamental and important about the graph which is modeled by this Markov chain but it doesn't we can't always this limit does not always exist ok so we want to talk about oh yeah I'm so this first life lesson only your current position matter is going forward don't worry about the past we only need to worry about going forward and we will kind of you know so and that's where this Markov comes from a Markov process is something that only maintains a state and the next step only depends on that individual state great so before we talk about what's what's going on here there are there are two ways to think about these Markov chains okay so the the first way is that is that we we kind of only consider one possible so location at time okay so we're only in one state at a time we're not this is not this probability distribution error mentioned earlier but this is like this example eg where where q1 0 1 0 0 0 right so we only have this one position at time and say I'm at position B that's it okay and so this corresponds with a random walk around the graph and there's the probability transition tells us how we do this right and walk and we think about going from one step to the next I've like a kind of a discreet automaton you may have seen in a different an earlier computer science course ok and then the other one which we were kind of looking at before is we have a probability kind of distribution on space okay and say hey and so this correspondent with with the example of Q equals say 1/10 1/10 and a zero and B have three tents and c00 say 6/10 send something else right so there's a distribution of where we might be and we can kind of maintain this distribution or think of moving this just pushing this distribution through as we're hands or analyzing this okay and so a lot of places where Markov chains are taught there there are are the they're the they're actually oh yeah so so they're they're only on their they only teach kind of one view of these of these two two views of the Markov chain and they're kind of different how we lose you do to work with both these different views the first one is is comes up a lot in Bayesian analysis and came up in statistical physics and we'll see that towards the end of the lecture and the second one is more turns out being more linear algebraic and we'll will mainly focus on this and this will actually correspond with a lot of what is done with the with P drink you'll see it in the next lecture all right so okay so what we want to largely talk about is is we want to talk about this property called her got her got ik okay we say a Markov chain is ergodic if for so if let's say if there exists some time T which is you know so which is which is like the number of time steps we believed to have taken so far so that's for all and greater than T then Q of M so that's we if we maintain this distribution over the states for n steps so for all values and greater than T that if I look at all the entries in this vector it's strictly strictly greater than zero okay so after a time T every possible point node in the graph has a positive probability that I that I might be there on the random walk okay it can't be that for sometimes the future I might be there it has to be for four for every node for every time the future I have a chance to be there okay so this is what it means it means to be ergodic and if I have this ergodic property then I can okay so this definition is actually pretty nuanced and it's kind of if you look at a Markov chain you're like is is this going to be the case that's kind of hard to hard to decipher so we're going to talk about instead kind of how to define this is on this is true if if not one of so this is a Markov chain spected is going is going to be organic if not one of any one of these three conditions hold if it's if it's if if it's going go if is if it's going going if it's going to be cyclic if it's going to has pubs absorbing and ends and transient States okay and if it is disconnected okay have salt I'll define so it's easy to think of if it's organic if it does not have any of these traits that has any of these traits it's not going to be organic and so it's easier to define this and to understand it by understanding these three tricks so next I'm going to kind of define these three properties and and basically to find them through examples okay and and if it doesn't contain any of those properties then it has to be ergodic and then we can talk about the limits and this nice property and this will guide us and this will be important understanding why PageRank actually works the way it does okay okay so the first is that kind of a Markov chain it is going to be cyclic if there's kind of these classes of states where it does it kind of cycles between these different classes so if I started in one set then it will hit that in some after some fixed number of these other sets so I think this is much easier just to see by example here are three probably transition matrices which totally encode these these cyclic markov chains the first one is has two states a and B and just says that with probably one of them an A then I transition to B and if I'm in bi transition back to a so every if I started a that every one step zero then every even so if I'm at a every odd step I'm gonna be in B okay the next one has three states a B and C and it says a always transitions to c c always transitions to b and b always transitions to a okay so with probability one every every time and this is is gonna be cyclic now it has three states instead of two states so every kind of kind of thing divisible by three if I certain a will always be a and off by one a C or B okay um you know it can be more general like in this case where now I have two nodes ABCD ABCD e and F which I think are on one side of a graph and B C D and E are in this other side and this makes what's called this by bipartite graph where a has a 1/4 1/4 probability of going to each of these states okay and the same thing with F it can only transition to these days but not a and if I go back in the other direction B can only go to a or 2f see only to a or to F D only to a or death and this is messy but so it's it's only going back and forth between these two classes of states so it has you know right so a and F only go to these these these states kind of B C D and E right and so and so these are the only the are the are the only possible had transitions and so that there are only two real states that that that it can make it possibly via so so I have two classes of states can be in it alternates between these so it starts in one side then it has to move to the other side and a and back and forth right so if it has the cyclic example then the then the Markov chain is is not going going going going going going to be organic all right so okay so so the the second type of thing is going to be if there are states which are oh and so there are other things to keep in mind if if there are other things that mixes in between these states like even if I had some some edge from B this is not the right place to put an edge but like if I have a probability of going back to B that will say 0.001 and this was actually instead of 0.999 right if going to a well that's going to eventually mix together and I don't always have to go back and forth between hey I might get stuck in B yeah anything time I touch something else that recording box pops up okay great so so okay so these these are okay so I now I have these absorbing and transient states let's look at an example I have here I have a and IB and what's happening is for a half the time I'm staying I have a self-loop I stay back to state a and half the time I go to B but B always goes back but B is must be doing B always stays to itself with probability one okay so so what's happening is if I start in if I start at a this is what's called a set of transient nodes because if I start a I can't and I leave to go to B then I can't ever get back to eight so then B would be this part of an observing set because once I get to be I never get out of this set and this is an example with only two nodes but you know you can you can have a more and less let's jump straight to this more complex example here so I have a and B and a half the time goes goes to B and half the time stays to itself and B half the time goes to its goes to or this is I guess 49 out of 100 goes to itself and half and half the time those two hey but then there's this other set of let C of C D E and F and these guys kind of all mix you know to each other right they're all kind of uniform in here and B has one out of a hundred times this goes down to C so one out of a hundred times and so now I've got this bigger example where eight and B here this is transient and and c d e and f is going to be absorbing because there's no way back from the observe being set to into the into the transient set I can go from the transient set into the into the absorbing set maybe only with one out of a hundred times I'm actually in D so it's a pretty slow leak but if once it leaks out it can it can never come back so this is a weird issue with with the with the limiting case is that maybe you can have different different absorbing sets and you might get stuck into this one step that one and there are kind of these issues that that it that that you you don't necessarily live it or a and B or not don't have any massive limit everything leaks out of it so so these are also cases that we're not interested in analyzing okay and then the third class of these is these unconnected examples and this is hopefully pretty simple this one is a and B where a always goes to itself and this is B always goes to salt but you can't get in between a and B you know it can be more complicated in this case I have a and B these can go back and forth to ease of each other but then there's also this node C which is a soft loop of one so depending on where my starting state is if I start all the way and see you then I don't have in a and B if I start a and B then I end up in just in just those but never in any C or if I saw but the distribution across them I basically keep that distribution which is fixed because it's an it's not connected okay and you can think of more kind of complicated examples as well alright so so these are these three classes the the cyclic graphs the ones with absorbing and transient States and the one with which which are not connected and if you have real-world graphs these sometimes have these properties and we'll see next lecture ways that that this can be this could effectively be fixed by by kind of doing some some minor surgery on how we model these things and that helps actually making it more and more stable as well okay but but these are kind of the properties so if if it is if if the Markov chain is her cut is is going is going to be or ghatak then what that means is that we can now you know I'm not going to explain you know why we can always do this but it means we can now think of this in the limit um and so we can think of P to the N as n goes to infinity and this is going to be this this probably transition matrix P star okay what this P star is doing is it staying this is saying in one kind of step P star to you know kind of like pushes the state to the kind of the final state where it's is converged to and isn't it so so so for any starting position Q right we apply this P star matrix which is the limit of multiplying by P over and over and over again and to get this PN and this gives us the Q star okay and this Q star is now this vector so so this hue star kind of is a is a is a vector and it has some some weight for on each node okay so this will be Q star J is for the Jade for the Jake node and this tells us something about how important that then that node is going to be and what will kind of keep exploring this right but if it's organic we can then define these quantities I couldn't necessarily define them unless we knew it was it was that it was going going to be organic okay so what are some things we can okay yeah so so this is life lesson number two you you just need to worry about one step at a time you will get there eventually or you you won't right and and so you know if you want as the that's if it's if if it's not going to be ergodic and so this is idea of you just kind of keep kind of iterating this process and you you eventually will get to this this conversion to state which is which is what we're after okay all right so so what are some properties about this right okay so the first observation I want to make is that I can say if I if I take this state I've got into after a Q times P star right and this is going to be the state Q star here and I multiply it again by another matrix P so I apply this probably transition one more time just the original matrix not the P star matrix I have to get back Q star right because this is in the limit I've done this infant number of times it's gonna be stabilized there okay but what this means is I can actually place this term this P star Q with Q star and I get this expression here okay and so this means this is it's very cool it means that Q star is an eigen is an eigen vector of P okay so this is the property of an eigenvector I multiply a matrix by a vector and I get back some scale multiple of this this vector it turns out on it's always the the top eigenvector right so I'm gonna have them in a sorted order this is the one corresponding let's say the largest kind of singular value that we talked about for I talked about in like dimensional reduction okay so this is the top eigenvector the most important one is going to be the this Q star state alright so this gives us one algorithm we'll talk about algorithms to compute these kind of more formally but this gives us one way of competing in and it says it's equivalent to the top leg and vector of the original matrix P it's also the top eigenvector I could put P star there it's also the top eigenvector of p star as it turns out but we we don't want to have necessary to compute P star we can just take P take the top eigenvector that gives us Q star which is which is pretty cool okay um and it also has it has these other properties whether there's this property called that's that's called like the delicate balance and so what this is saying is that in this final configuration state where we have this probability distribution over if we took this random walk forever on this graph then the probability of being in some state I and then applying the probability to transition matrix to go from I to some state J right so I've got some state I right and I'm in here with some probability Q star high and I have some probability of transitioning to state J okay so I want to look at given that I was an I and I do this kind of one step right according to my original probably transition matrix okay so I probably be in there I take a step I'm in state J well this is going to be balanced this is going to be equal to the probability I am in state J in this distribution and I take one step from J back to I right so if you know this is true in general that all these kind of I take one step and I get the whole probability back but this is true on individual levels as well so I can I can kind of just look at two states and know that they some will have I know what the probability transitions between them are and I know that they have to balance and in some way and so I could solve kind of some kind of linear system in this way to solve it as well but this is a kind of cool property that when you're analyzing things as Texas oh yeah this is this is why this has to make sense ok so there are lots of these cool properties that that that come out of time thinking about the structure of the conversion to state of the of the Markov chain okay let me see what what else they want say yeah ok yeah so this is in the limit length lesson three everyone has perfect karma because you know you you trade off with everyone else and and that's the you know do converge to amount of karma you have you that's as much as you give you're gonna get back great ok ok so I want to talk about yeah so yeah so so I think I've said that right so let's say okay before we talk about algorithms and what are the algorithms associate with this there is a clear there's um this this limiting state this Q star I just want to highlight this is not a uniform distribution of those uniform distribution of rate nodes everything would be 1/8 right and all these values but this is the actual limit on this example here right for a yeah so for note a here this is one so this is like 3 over 20 and that's gonna be similar for kind of for a for D for for c4e and half these are nodes are kind of more central in the graph all of these have probability of being 3 over over 20 that's these values here but these nodes which are not quite as central in the in the graph kind of this state be an estate G here these are the probability 1/10 right so these are lower probably being these nodes and then the ones kind of out on the fringes H this is only one out of 20 probably being there so if I'm doing so think of this if you're doing a random walk around the graph and every time you're at a node you look at all the edges I have available and I pick one at random and do that I'm less likely to be out here fringes because if I go out there then I have to go back right away and I might not walk back there I could instead go and see the rest of the graph okay kind of so so it's not a uniform distribution here because I'm less likely to be in the edges I'm more likely to be on these more central and important nodes in the graph okay so that's kind of what what this is capturing here so right so okay so our goal kind of with this there there are other ways to analyze use Markov chains like something like the second eigenvalue tells you how fast it mixes and this is important for understand various algorithms we want to say talk about algorithms for computing hust are right so I want to compute Q star and and if you haven't noticed we've already talked about explicitly about three of these algorithms and all else let's say that the fourth one as well okay so the first hard look for computing P star is to take the eigenvalues of p right so so say the top eigen vector will call this the one of here so we compute the eigenvalue decomposition we really only need the top one so you can maybe just run some internal command inside of Python or MATLAB or something like that you get out V one now if you wanna is L to normalize this is is a unit vector turn this into a probability distribution so then to get Q star we simply say it's V 1 divided by the L 1 norm of V 1 right so if you remember L 1 norm of V is going to be the um you know sum over J from from 1 to N of the absolute value of kind of these these terms we have and we normalized by this and then it becomes a probability distribution which is what we were in for Q star before we are dealing in these unit vectors which had an l2 norm but now we need this probability distribution here okay so this is the first simple algorithm this is a very linear algebraic approach to it there are other kind of things we can use you use linear algebra ok the other way we talked about is that u star is is the limit as we do P times let me first write it this way it is P times P times P P times any initial state Q 0 here so I start with initial state Q 0 and I'm multiply these effectively n times first some sufficiently large value of n this is going to converge towards this this this Q star approach and so they're different there are two ways of splitting this the kind of the obvious way to do this is to kind of and this turns out the right way is to do think of this as a for loop for I equals 1 to let's call this M because I'm using n is that is the number of nodes I'm going to say Q I equals P times Q hi minus 1 okay so I start with Q 0 let me do this and and we'll return hue star equals you know is approximately equal to Q hem after after hence steps of this okay for some sufficiently large value of M okay so this is you want to do the vector matrix multiplication this turns out to be faster than on matrix matrix multiplication I could have done this company this this this computation first and then then these other ones right so so this is kind of the third algorithm which is closely related with which is Q star I guess as these are approximately equal to and effectively that eigen value the way eigen eyes are computed they're probably approximate as well but this is basically QM so I pre compute this QM for some sufficiently large value of M times any q0 some some initial state okay so i've pre computed p.m. and i could do this which is just the the product of I equals 1 to 2 m of this matrix P over and over and over again like there are ways you can think of decomposing this into I can say equals to say P M divided by 2 times P M / - right and then I can say P M divided by 2 is equal to P 4 times PM by 4 so I really only need to do log M of these matrix multiplications if I'm like approximately if I'm clever about it but the matrix multiplications are typically much more than the matrix vector multiplication so step two is version two is is typically faster ok so these are three algorithms that we've we've basically seen before I'm just filling them out the fourth one we've talked about which is a random so so walk okay so we want to now explicitly do this this random walk and this is one where we maintain maintain and explicit state okay so we think of Q as being zero one zero all zeroes here and this is basically it says state Q is kind of as in state B maybe we have this graph near these notes and the state is always exactly at one note and then we do this this is random walk around the graph okay so for each step each step from so this is you know so so each Q is in some in this node set and so I'm basically at some node and I move to to a neighbor on on an edge and this is kind of this is the I and I moved to the I plus one okay and then and then I just repeat this over and over again and so and so on the output I then collect the set V 1 V 2 up to V M for some large number of steps I've done here ants and so these um these B 1 through B M give gives me this this is probably like an I can apply up a probably distribution so Q star of J for instance equals the number let's let's call this set s this is the number of V in in s such that V is equal to J and I divide this by M right so I get an estimate for Q star for each value by just looking at the the number the sample estimate of the number of times I meant each of these states okay so I just physically do this random walk around here kind of mechanically walking around this graph um there's kind of some nuances in how to do this this is used a lot in kind of these Bayesian formulations where I have this posterior I'm trying to try to estimate from and and there's emissions that two states next to each other are kind of correlated with each other and there's some worry about this and so without going into too much detail so you usually do a burn in kind of period burnin period of say roughly a thousand steps and this is usual roughly was used and then collect the next roughly 500 steps okay so what can happen is if you end up accidentally starting your initial state in like some weird corner of the graph then you're going to bias towards there at the beginning and this will affect your overall probability distribution so I want to first do this burnin period where I randomly walk around the graph and hopefully at that point I'm at a sufficiently random spot basically according to the Q star distribution which turns out to be it to be okay and then I collect the next 500 states and use those for my sample estimate and this one hopefully is kind of unbiased now it's still not totally unbiased because two kind of neighboring states here in are gonna be correlate with each other and so typically you can think of this as maybe you know depending on the structure of the graph I mean couvent to maybe a thousand samples instead of 500 instead of 5,000 samples because there's some Auto correlation between them so I don't quite have all the variants guarantee I'd get with a thousand five thousand of kind of completely you know four examples but this this this typically thing works I mean in theory you're supposed to take a thousand steps take one sample take another thousand steps take a second sample but this typically takes too long this is not what's done in practice okay so this is the fourth way this this it's random walk approach so grapes so so so yeah so okay so these are the algorithms for computing Q star and you're going to be doing these in your and your homework this is what your homework will be doing they're all very very pretty simple algorithms to do the random walk gives implemented probably distribution but you've done that for Kim Yuna's plus plus and maybe something else before already so that should be to our okay so I want to conclude by quickly talking about this alternative you called this metropolis that this metropolis algorithm which kind of loose looking at this random walk approach where I'm maintaining an explicit state and this is by these five authors from 1953 a round was invented in the work and trying to create the is part of like the the Manhattan Project rows and how growth of this where they're trying to understand these physical systems that were too complex they couldn't derive what was going on and they want to understand the energy of these of these these systems and they had some energy of some of some state and this is a continuous state space that that they are working with so they had some energy of state and they wrote this as e of Q and they kind of said okay it's understood that the to understand the energy of the state the probability that a the abilities say that a particle is in as at Q is should be roughly pro-poor tional to some weight Q which is going to be e to the negative energy or something like this they have some formulation that looks like this where they could they can say I know some weights which is proportional to the probability I can evaluate at any given state but I can't actually I don't know what the kind of normalized this so I don't know the actual probability I just effectively have this likelihood how do we get a samples from this probability distribution okay and so this is what metropolis algorithm is doing when it's given these these weights and once you get a sample from this from this distribution within Bayesian analysis called Markov chain Monte Carlo approach okay so let me just quickly write down the algorithm and then will kind of explain what it's doing so it it's that now what's happening is I've got this this Markov chain and if remember this has three components this node set this probably transition matrix and this initial state Q and so I'm going to now this this node set is going to be something that's continuous instead so think of this as being a hardy or subset of of Rd okay so it's continuous state space now and I want to draw samples from it but P I don't have an explicit matrix name words it be an infinite dimensional matrix I can't write this down so this is instead going to the end and algorithm okay and this metropolis argument codes this algorithm based on the and this is kind of based on these weights W okay so I have these weights W associated with everything and they remember this portion of the probably I want to be in there okay and we'll describe all this step through this algorithm and then I have this this initial state I need to initialize that somehow remember this is now not a probability distribution but this is state v zero this is one of the possible states this is an element of Rd here that I'm working with okay so then what do we do we have some kernel function which goes from V and generates you know think of like a couch colonel and i have the state v and i want to generate some potential neighbor kind of nearby and it doesn't matter what this what this cave function is but think of you know i will select some you a propose next location somehow in the neighborhood usually you'd use a Gaussian kernel which you can generate it's easily even in high dimensions right so i can generate data from a Gaussian kernel and i make some step okay and then if the weight is larger at you that is at the previous state VI then I'm going to set you know the next state VI plus one to you right so if the weights is larger then I set this to the I VI plus 1 is here if it's not larger then this value is going to be less than or this is less than one right the weight if you overweight of VI this is now is is less than one so I still might select you as this next state but with some probability if it's not that much smaller than yeah I'll still make that stuff I'd rather be in these high weights that states these of course both low energy and they're more and more favorable so I'd rather be in these lower weight I'd rather be these large weight states this lower but not too much Louis I still might move there anyways but if I don't do this I flip a random coin and I don't move to you then I otherwise I move on the next state is isn't in the same spot I don't move again and I have to try this over again so ultimately I collect I collect the set of states here V 1 through the m4 for some large large m and then I use this as a sample from my distribution okay and so so this you can show that in the limit this converges to the right distribution there's a lot of work and it's kind of a little bit mysterious and hard to grasp win actually works how fast it converges has to do it the second eigenvalue of some infinite dimensional Markov chain and all this stuff but it does it will eventually converge as long as it's kind of connected you guarantee it's not going to be cyclic or absorbing in transient states in here because of because of this step there's always some probability as long as the weights are zero and and the neighborhood's are connected with each other okay and so there's submission that sometimes I'm in exactly the same state two times in a row but it turns out that in the limit this is okay but I need the sample to be big enough to kind of get rid of these effects so yeah so so this is all I want to say about Markov chains we'll see a very powerful way that they're used in understanding graphs for a PageRank for understanding the space of webpages the web draft and kind of other stories about how search engines work in the in the next lecture so I'm going to going to end here unless there are any questions so great so the one audience member does not have any questions so we will stop here so things thanks for joining and and trying to end entrance to end so so so so stay safe out "
Zj_csB0anJU,27,Classification of Data Mining Systems and Data Mining Task Primitives,2020-06-10T16:06:55Z,UNIT-1 TOPIC :Classification of Data Mining Systems and Data Mining Task Primitives,https://i.ytimg.com/vi/Zj_csB0anJU/hqdefault.jpg,ANTHARAJU'S CSE ONLINE TUTOR,PT13M30S,false,3579,59,6,0,4,[Music] [Applause] hello everyone welcome to my channel now so we are in the first unit and the data warehousing data mining will start a core class with the previous discussion what we have covered in the last class we have covered these two topics and before that window in my video lecture I have covered the first two topics so what motivated Delta mining what is mining what kind of data can be mined what kind of patterns can be my these are the four things we have covered in the last class so outcome of these particular four topics just I will give you the recap so what motivated data mining so we discussed the lot number of resources are available in the today's world so which motivate us to do mining the second concept so what is data mining data mining is a task where you will get the knowledge out of the data you will be how to bring the knowledge out of that particular data raw data next so what kind of data can be mined we have seen Lord Lord number of data types or forms of data where these data can be useful in the mining so if you have any queries go through the previous video free to previous lecture in my channel and the last topic what kind of pattern can be mine so these things we have covered now in the today's lecture I am going to cover these two topics so classification of data mining system and data mining task primitives so first of all I will discuss what actually classification of data mining systems so before going to do do that so first of all let me clarify why we need to do classification why we need to classify the data mining system so because the data mining system is interdisciplinary field as you know that the data is producing from multi multiple disciplines like medical field you can find data computer science field you can find it a scientific you can find the data like that banking you can find the data where the data is generated like that you can find lot number of disciplines involving in generation of the data that requires we have to mind those particular data so these particular data mining concepts or interdisciplinary fluid so that is the reason we are going to classify the data mining system so that user can understand how they can identify the need of that particular user okay so what they need actually so according to that they are going to mine okay so that is the reason we are going to classify the data mining system now data mining system can be categorized according to various criteria okay on which basis we are going to classify the data mining system according to the criteria I will I will tell you one by one so the first criteria is databases so which databases you are using depends upon the databases you are using the classification of data mining systems will change so something is like that relational data transactional data object relational data data warehouse mining system so these are the four categories of databases which will give the classification of data mining system with respect to databases similarly we can find knowledge with respect to knowledge the data mining system will differ the characterization discrimination Association correlation classification prediction outlet analysis and evaluation analysis so which gives the knowledge okay so all these are the different would equal levels of knowledge out of all these things you will get something okay so that is the reason we are going to Oh I swear the data mining system with respect to knowledge next the technique which you are using so according to the technique what you are using the data mining system differs in the above list see this autonomous systems interactive exposes system query driven system q database okay machine learning statistics visualization pattern recognition and a neural network so according to these particular techniques each and every technique having its own different data mining system okay so like that depends upon the criteria the thing will change so application so according to the application also the type of data mining system will change the classification will change see this is a financial telecommunication day and a stock market email so on etc a lot number of things are there but I have mentioned very few so like this you are going to classify the data mining system according to the oh do you call the criteria okay whenever the criteria will change that I have a data mining system will change so you can able to conclude that data mining system involve multiple disciplines okay so that is the reason we have to classify like this so this is a big classification of data mining systems see so data data binding system classified as the machine learning organization visualization algorithms high performance computing applications information retrieval data warehouse data data sustainable systems statistics okay so like this these are the basic classification of data mining system we have to understand before going to learn the data mining concepts so I have ensured very few so if you go to the category wise it will different okay so these are the things these are the data wide existence we can able to classify all right now we'll go to the next topic so data mining tasks primitives okay so before going to do that who will start what is task primitive okay the data mining query is defined in terms of data mining tasks primitives okay data mining query is something like o which is very useful to do the mining tasks okay yes like your SQL query language what you want the query language will do a structured query language say it will it will do the it will apply the query of the database so that it will give the output to the according to the user query similarly the data mining query language is defined in terms of data mining tasks primitives okay if you know very well this that avoiding task primitives then you can define it good data by the query so that it will give the it will serve the user requests okay next to what this primitive will do so these primitives allow the user to interactively communicate with that data mining system okay so because whenever the user is going to talk to the data mining system he has to know what he has to do okay so there should have some specific primitives primitives so basics are needed in order to communicate with the system okay so that is the thing these particular primitives will do okay so these are the tasks of primitives so first one task relevant data the kind of knowledge to be mined background knowledge interesting ish measure after that visualization alright so these are the five data mining tasks primitives so those are very very important to understood by the user so that if the user understood very well these particular tasks form too so that he can define a efficient data mining query language okay so depends upon that the disturb the system will give a efficient output a particular single diagram which will give all these five different types of tasks pivotal see the first one task relevant data so this particular task relevant data is defined by the database or data warehouse name see this these particular tasks relevant data comes under these particular things database tables it involves like this database table so that our cubes conditions of data by data selection relevant attributes are dimensions okay [Music] condition selection relevant attributes or dimensions data grouping criteria so these are the relevant tasks okay relevant to ask which is used to do the mining okay before going to do the data mining task we have to perform the state task relevant data next thing is knowledge to be mine so once the knowledge to our knowledge is available so which sort of representation should be their characterization discrimination Association classification and next thing is background knowledge background is knowledge is is the thing we which which gives us a reference reference to the user so which will be helpful to my efficient pattern based upon the previous experience okay and the next thing interesting this measure once the pattern should develop or obtain how quality to patterns are achieved okay produce that is going to be defined by these two categories confidence and the other support I will try to explain these things in a deeper manner in the coming lecture and finally this is a visualization all they were discovered pattern like how you can able to represent all these things using the rules tables reports charts graph and etc so right so all these things are the it defines like given the basic requirement of tasks basically it gives the basic requirement task task which is used to satisfy these particular Phi concepts like task relevant data knowledge to to type to be mind background knowledge pattern testing measure and visualization okay so these particular actions are going to be fulfill to achieve this or equal or task row record or task primitives say see this Oh these total five steps or fulfilling the task relevant data to achieve the task level data and similarly all the remaining things capitalization discrimination Association questioning so these particular things achieves knowledge type to be mined okay and finally these things concept hierarchy and as well as relationship these will be useful to achieve background knowledge simply the confident and as well as supports will achieve the powder in interesting let's measure that measure if these will help you to give the quality of the pattern and finally these tables and as well as the graphs all these things they cannot represent the visualization of discovered pattern okay so like this we can understand the data binding task primitives out of this diagram which issue which is very important to understand this data binding task primitives so this is a under the today's lecture so if you liked it share this share to your friends those who are required like and subscribe to my channel to receive more updates so your subscriptions or your likes will motivate me to do the best in the future thank you all 
CRtIjiwowoM,27,"The Data Mining Query Language (DMQL) was proposed by Han, Fu, Wang, et al. for the DBMiner data mining system. The Data Mining Query Language is actually based on the Structured Query Language (SQL). Data Mining Query Languages can be designed to support ad hoc and interactive data mining. This DMQL provides commands for specifying primitives. The DMQL can work with databases and data warehouses as well. DMQL can be used to define data mining tasks. Particularly we examine how to define data warehouses and data marts in DMQL.
............................................................................................................
#SathiSoft_Academy #Rajan_Dangi
.............................................................................................................
GO to Link(official Website)  https://dvlottery.state.gov     U.S. Department of State
Bureau of Consular Affairs
.............................................................................................................
DV-2021 Lottery Photo Requirements | DV-२०२१ भर्न  कस्तो फोटो हुनुपर्छ?
GO to Link: https://www.youtube.com/watch?v=4CpJT...
.............................................................................................................

www.sathisoft.com. is authorized to upload this video. Using this video on other channels without prior permission will be strictly prohibited. (Embedding to the websites is allowed)
.............................................................................................................
.............................................................................................................
For more updates please visit:
www.sathisoft.com

Connect with SathiSoft Pvt.Ltd via Social media ------------

Facebook
https://www.facebook.com/sathisoft/

Twitter
https://twitter.com/sathisoft/

Instagram
https://www.instagram.com/sathisoft/

Linkedin
https://www.linkedin.com/in/sathisoft/",2020-01-20T10:19:10Z,Data Mining Query Language (DMQL) || Data Warehouse || Data Mining ||CSIT,https://i.ytimg.com/vi/CRtIjiwowoM/hqdefault.jpg,SathiSoft Academy,PT26M28S,false,1521,24,5,0,2,most cars are here they are destroyed our sample data also documenting to reduce it as we thought it was a data binding for each language is Manama data mining language basically it is based on experience - especially language s cube a suitable base result is they support dr. as the interactive data and interactive data I'm interacting interactive data mining algorithm [Music] just to see just extinct via data mining finding this person texting on her phone that extended extended be and the affirmative a backhoe that cause [Music] [Music] mathematical language obviously caramelized projector lineman I've got a map ESMA yovan a few the Reverend was at you back up north on everybody you know very smartly you correctly if the bracketed zero or one operates like physically is my practically zero or [Music] you are a data mining British consumer mining primitives believe it used to there are many maybe deep sorrow [Music] this kind of knowledge to be my kind of knowledge to be mined this again background noise background on this district 8 presentation I visualize you I just discovered a pattern this your pattern you make in some second technical so money mm I really like this Cabana of them to be see people sorry data mining Pvt planning book Tosca really bit that I mean lots of data we have spotted living - those romantic oh happy dagger my Mooney Association classification clustering five people who need domain-specific knowledge disagree an honest penny something about having information like costly visualized on display goes the unique way of displaying the disparate battle you disadvantaged because really when Nicola I mean just wanna see does concern you and everybody go see the given specification for Molina be about kind of task of elemental gives man you have two tasks for the u-87 cattle money transfer you know the school I give me  there's this rumor say you would polish mind because visual ask even in Swamiji I mean we genocide visualize in technical so it's not at all when he saw that I might equally language mattered for Monica now concept I am fossa Syntex prolific data rate of the age happy large amount of dancer last one terracotta and he like it data might good reading articles or any type of any economic wouldn't say that that was to live in sir wouldn't a data let the use of a minute ago so this rather Miami my interview mighty got lucky I picked up issues not so anonymous you construction Ottawa I mean data wait on that we are syste to that abusive anybody that dinner without super Mario's for non-member believe in stupid people using given in relevance to the man it doesn't use image do you see it sensitive front of the keyboard sensitive front of the cable given it's not active you toward is released at Utica somebody activity beside it was very consoling attributes are not a bishops delicate so money that we are super exactly tiny stanza speak you know holiday back you Cleo's curse also that like you he was very sick he was enigmatic you were when you die means nose thank you my name is Rebecca money any dinosaurs disagree about I mean activity that is your less usual activity activity thank you this we see from the data is overly fuzzy relation equation that would also be cute or keep releasing a vacuum condition for transition you're not least man extended BNF I can even if I am it is but extremely B capital bracket committee 0r1 actress maximum one other issue you waiting on Seavers you order by or other at least person order by order right or at least the same and they ordered I got any further disturbance it would be nice you see optional so thank you order this not even for a sucks even is it bother you can see capacity of those two number two you know my negative stuff Scooby regatta hey Tiffani very good bean syntax so texture kind of success for the Swiss fun it's just fine kinda thought to be mind Hundley that a minor by sequencing already using a mechanism just of this is missing data mining query language infection not already quit and this was intentional relation it's not over labels up classification classification classification data windows water and classification quality very much like any cousin Dominic waited bubbles loosely database father and his rescues from the district attorney might equally languages caliber particular certification there is no classification good at your predictive data medieval correct classification apology texts later believer in owes him money qu classification lucky classifications as any as pattern a pattern of nation better name zero offer multiple assumption Sammy yeah analyzers elements and am i selling lucky classifying attribute attribute for dimension attribute 11 my classification is undeniable and it mainly as a pattern and in services your mass destruction analyzing and the glass effect attribute the active land classification clustering activities also pretty production data prediction we gotta believe a name and affiliation music also know it's procedures for the same binding the students my classification so my prediction prediction Sony pattern and shipping zone badi-ma super pattern but a minute imagine analyse and it would act in the four diamonds of light traditionalism they added the Diamonds of an amazing predictions attribute of diamonds I regard I mention it's not I mean they have a punishing consumption st. attribute diamonds I is equal to paralyze it's not be released man hey everybody [Music] you little bio classification of addition like a kind of data to remind man classification perishing align it and descriptive tasks of overly descriptive tasks use data data binding of this one thing as well diffraction descriptive tasks use data mining of the summoning and characterization discrimination and Association tenuta infer to syntax another visual characterization and he just in addition discrimination okay price is not your medicine console by Swiffer my adoption you know the date happy than 20% introduce Rebecca Castle characterizes Lamar I mean that Christie could evilly attribute of a pattern who characteristic for other artists I mean Tetris on a class of a considerable person city characteristic characteristic does a activity pattern named Simmons Canela is directly classmate active unified or the classification one is a legally possible a smile and eyes fell upon a clown I mean some major and - nation well they need supergravity money alone there's a panic oh the aggregate aggregate down a bit how much it will be down any minute the school is part so for example count on the suction someone success I'll to say count per centimeters you say a get it down if you get down to an etiquette major a meter negative mr.panda hi mrs. Albertson discrimination discrimination [Music] discreetly single bracket that I mean it's not just give me some up pat on the back of us do my classical I do not like compete mechanism there Acosta Pat and I continue their hamburgers compete your Universal comparison some people accuse money comparison in since comparison of multiple activity versus multiple a terrific American Frances multiple apps which to comparison stop analyze say I mean computer do the classic computer model Amos is it you a massive tsunami company gotta horrible what if you target with us Titleist by comparison so Commission money dissertation target condition type classification versus target plus draw type it to us the contrast class policeman and even the skinny software company also purchase contrast pass contrast us contrast class nepali a me condition instance contrast class condition destruction obviously having energy sources analyze makes multiple activity to an electronic rock valley multiple could have a unless again let's go mrs. Services Association Association Association pony basically mr. gostin hey woody I just sent it in that Billy show me even I got music opening music did not believe in the possibilities I'm so excited exercise declare Eastern religion customs mr. case mistake is resistant Bible School happy by insane Association things happening multiple attributes to this map pattern name is man it's justice inheritance O Hanuman dynamite feeling something good about yourself that my real name isn't equal to 31 syntax Ragusa concept I have a specification colitis injection concept hierarchy specification politician text a source of a name use I mean hi archiving dozen 500 I use yours parameter believe I read a proposal for attribute or that means can you mean Turkish Revenue Service missile data we have some misconception this is it attribute the old I guess David supervise the same concept higher a specification of a decent - bye-bye guitar visit this time visualize visualizes the be specification by inputting data might visualize money costly is a nice honey cause it is a display gonna induce a velocity spinnaker name on the Pliocene mission exigency that American text attached serve this with a disability nickim you he has result for imagine that imagine basic FS paradise equals results the result for my since tables reserved for max thank you the converse Trish [Music] 
nLw2PZkpA-Y,27, ,2011-02-10T01:00:13Z,Data Mining 2-9-2011,https://i.ytimg.com/vi/nLw2PZkpA-Y/hqdefault.jpg,CITRIS,PT1H56M10S,false,4609,2,2,0,0,"so I'm looking for some volunteers I thought realize that the the material online basis looks a little dense so I'm looking for some volunteers who work with me to turn it into verbiage and we can have a discussion tomorrow sometime or maybe over the phone later on hi because i think i'm going to make it a little simpler than i planned to because i think it's good for folks who have reviewed the material so maybe the end of class i look for some volunteers so what I'm doing is I'm sort of posting both the projects and I think let's see i think crowd science is okay with posting it the 70 or doesn't want me to post it i think both people a little bit more comfortable miss any email to the lists and not extending it beyond this class because a little concerned about competition all the usual for her things and actually surprised they're willing to let me share it with you people in the first instance hello because this is actually these are parts of research projects these folks want to do with us but they I said where we open it up to the students because the new folks might like to look at some real stuff so take a look i'll give you the simpler one first maybe i can switch back and forth because what I lose today what times about everybody shows a bit to 10 okay little bit by everybody get in and then look at the two projects and then budget time maybe the last 15 minutes of class to discuss projects 15-20 minutes that's why I guess I all here except those rail projects right that's why so I'm assuming and how is everybody finding the workload assignments and so on are you there is no submission but just for you to learn right are you able to crank up and start doing things a little bit takes a while take some effort so I I guess pronoun said he would help out with the are and other things right okay so part of a who's going to come a little later right he can help you a little bit we don't quite have a key I so you will have to have run over and fill a little bit of the brawl and maybe each of you could help out and I can work with each of you to each topic yeah hello I did there is so you can rally Center any comments can you hear me yeah yeah yeah oh it's back okay and you're able to see the project descriptions right the project this then you know for the course we are having research projects I able to see the descriptions Silicon Valley are you able to see it on screen yeah okay good so by the way a while we're waiting for every article in then you're reading this everyone so far easy absorption yes now don't feel shy is difficult eyes difficult getting there so trying to how many find it very slow raise hands no one's phone is slow so far okay how few more people coming in right so we'll get their feedback okay so so we're thinking of from next week having visitors come in to present each of these projects so that you can directly start interacting with the company's right so you can choose so we have maybe this one on the president next week and how to access data and so on and the next draw and the and other project which I believe this is gonna be so it sort of the products of the following right okay look at this all this whole off comments so that we can do it all together the class of the full strength right also because repeating myself Oh Oh okay so maybe you'll just get started a little bit I was there some margin okay so it's not only had an email about these projects so here the plan for today x 210 as well as late they'll come in and of it again but what we lose there are two projects and I believe crowd science calm is okay with me posting it on the website but in general I think both companies are a little bit more comfortable for competitive reasons my sending email to students passed not nestle posting it on the website and so please be careful that you know in the whole idea is to give you exposure these are both research projects and will probably have some more coming up but I've said hey you folks might like to take a crack so I'm assuming that you folks can spend between five to ten hours at least five to ten hours a week and upwards on these projects right that's reasonable amount otherwise they won't see much progress rate so anyone who's interested let me know if you're just doing a simple data then we're going to give you data of the data sets have been providing so then you can figure that out I think right so okay so this project if you look at it is primarily focused on this so here's what's going on there are companies which provide what are called media clips i think i told you what is company already media kits the analyse web arrival pattern different visitors frequency demographics and online but they also run surveys online and use that to get a better estimate of user demographics and responses on the web and then they turn this over to publishers so it's like providing a facility to publishers to tell them what is happening on their websites then these publishers can sell all this to the advertisers right you can say hey we have people who spend more money coming to my websites which are give me a higher premium right so that's the sort of a company so they were a bunch of research problems so that's one company okay and the schedule here is we've got executives cancer they start off today maybe they can come next week or the week after we go to a nice thing about a semester compared to cuatros you have a little bit more time I think right and then the the first phase is 531 may thirty first and then the first checkpoint is March seventeenth or april fifth so that we see what sort of progress you're making right now whether it's one person more we'll figure out but certainly confidentiality and das all that weekend right that's one okay then the other company is surrender now this should be relatively direct application I said relatively because nothing is as simple as it looks that's why you're taking this course this is actually you know they're looking at a lot of data on the on the web the in terms of text in other material but they actually have done some pre-processing already usually if you'd had to do it which would be a lot of work for you guys but looks as though they've done a lot of it already so now they're saying can you tell me if I look at all the stuff how is this related for example the stock market values including commodity pricing all right so the prediction we've been learning so far right it's almost a direct application right hey this is a starting point as I said nothing looks solid simple I always tell people right if you want to do research start with simple things and you keep going you blast through easy easy these and then you get stopped for a week okay then you get stuck for a monthly you know you're hitting research so on the sphere overlooking literature right so that's this problem and you can see they would like you know they say spss SAS SAS has essence all right but we have our so that's the equivalent knowledge of text analytics social media international matches will be helpful but not required but i guess some of you have the knowledge so okay that's the basic setup i think so i think from next week as a spec 70 will be here next week and the other company called science the following week now the other thing is i was talking to ray a professor Larson that we may start having some presentations I said in the course we we'll also have presentations by speakers from industry right now as a little bit of a long haul for them to come some silicon valley to hear but we I also talked to a lady who's been very good at doing healthcare analytics very articulate and she's helping Stanford with something so so so this is in the healthcare analytics fix because we said we look at advertising and Jimmy will be with us again so on yeah I guess those of you who are having the first lecture he's a little tied up with other things and then we will get probably other people then you're seeing these sort of companies then we'll get the healthcare analytics and then also there is locally in berkeley somebody in social media so and he actually ran a division of this big marketing company wpp so and he can give a nice flavor to a little bit more erratic so we are thinking thinking or for having this in the three to four are you know window so you start getting some industrial exposure but what it means is the class time available to teach you all the details is coming down so we have to figure out how much of it enough for you to learn some substance and then whether we need some makeup or you know like more a tutorial session or something like four or five versa we need a figure or some of that i think uh p okay now but before i get started then for the rest of class and we can if you look at this will put it up on the website or send you email so you can review it at your leisure and tell me which of these you're interested in and as i said i think typically accompanies probably expect five to ten hours a week at least minimum but you know the more you work especially want to get jobs in this area or or whatever right everything you know what it takes I don't have explained to you here all pretty self motivated people now today my quick questions is going to call a cover nearest neighbor classification remember we talked of prediction prediction you have lots of independent variables but the one dependent variable is a continuous variable rate and we've covered that with multiple variables and also the output crawls will be multivariate and then we are now looking at the classification which is looking at the outcomes are discrete class rank and then we have some input variables right which could be continuous example money you may your age what and then I you like to do buy this or not buy this or do you buy a PC very very laptop you might project whatever right glasses so that's what we are doing right now now I did a logistic regression but we rush through it last time so maybe we should go a little bit more peacefully give or get in a few more concepts and then get into knives into nearest neighbor classification and then my face is that ok now ninth base is a challenging so all the problem with all these things in data mining sometimes it's so easy you wonder what they were said to learn but you really want to learn it properly you wanna my god what I'm going to do right so it sort of it straddles the two right so you know I what I can do is to start doing some of the simpler things and then we figure out whether it's too easy for you and I can speed up right and then we can figure out how much of it should be in class this four or five extra session or some other session let me know because little difficult for me to gauge exactly what you can and cannot handle right now okay so now as I told you remember I always promised lunches and dinners and good things as long as you will be the right answers right so what did we discover about logistic regression last time we didn't finish up but what did we discover any anyone willing to bite the bullet and say yes very quickly everybody remembers beer preference right so by the way I think Marco had this question hello hi I had this question of well how do you actually do this right well we'll show you how to do it today okay I mean between one of the two lectures so so everybody remembers that we are that logistic regression or logistic regression is about the following for predicting whether I like light beer or dark beer yeah no you can go beyond that you can go to multiple classes and i'm going to show you that in a moment yeah as a starting point you can build out any key classes by the way any questions on hard problems ask rate because he's used with district regression so he's always good to know the ins and outs you know I often found you'll always look at some theory only when you fight it out yourself and you would actually develop new theory they really talk exactly but all the details are gone right so how are they okay so so we also said by the way we also tell you we use logistic regression rather than regular prediction because for all these reasons right all the nice model requirements of prediction don't hold when you go to logistic regression correct okay so here's where we were right very quickly now we look at this remember this that we say the probability 1 means light beer 0 means that we are a regular beer and the answer depends on your classifier right and a classified has a set of properties ax like the features of the profit by now everybody knows the term features ok and then let's we are going to multiply you have a vector of features everybody knows linear algebra so this language is fine right so I look at inner product of my coefficients w and my feature values of variables X right so that means I'm waiting each of the features by some weight vector and I then take in the plot of e to the power of minus WX and I have this transformation function right so what this is giving me is the probability of a data point which has got feature values X is telling me what is the probability that this is a light beer or y equals one correct so I have two classes y equals one is one class y equals zero as a second class I'm given data point value X somehow the other I have W right now say God gave it to me so right omniscience I attribute the Lord so you take inner product compute WX which is a scalar right in the front and take one plus one you divide one by one plus e to the power of minus WX you transformation right why is that because you said you're mapping anything from minus infinity to plus infinity to 0 to 1 so it's a probability of is it this class on that class it's not this class has got to the other class everybody with me so and if the first class is one by whatever is the other one is e to the power of minus WX by the same expression right okay now so are we done allies life happy I can give it a set of data points can go off and start figuring out whether they're light beer or regular beer I give you a formula it's like happy you believe me yes if morning will give you the odds yet what is missing we do not know w I said God gave it to us yes I mean in our dreams perhaps but in reality you've got to work hard at ourselves right soso the whole issue here is now we know this is a model it sort of makes reasonable sense but we need to get w and so clearly like everything else in life in data mining we get w from past data and we use it to predict future classes right so and remember we looked at the also called law guards right we looked at the probability of given this data point right all the values you're looking at how likely is it that it's a one or a light beer vs 0 or a regular beer and you take a log of that right and that a log of that is simply take the data point and take waited by the coefficients coming from your weight vector which we have not yet estimated that's it right and if when will you say it belongs to class 1 y equals 1 then little number especially zero right that's what it depends because if you assume symmetry in the cost then the number is zero if it's not symmetric that number may not be 0 but so it's a nice threshold rule right just compute this nice weighted value see where is greater than or smaller than some number assign it bang easy it ok now so we move enter all this right that this is a sort of take maps things from minus infinity to plus 10 one beer dollar right so and this is a little proof that if you want the expected value of the error of Miss classification right of 0 or a 1 or a 100 0 correct then you want WX to be positive right that's we went through that last time right everybody with me no problem right but this is where the fact that costs are the same was used right we assuming that l 0 1 there's a loss when a 0 is classified mistakenly other one the cost is the same as the misclassifying a1 a0 which is l 10 correct everybody be thin so far here ok so anyway so that's where we write ok now how are you going to start this problem and I'm going to give you the outline of the steps if I'm going too fast let me know because I sort of thought that you folks one with practical you don't want to be you mean you would like to know how things are done but you don't want to be unduly bogged down in algebra and you're all smart enough to go read the algebra and you have difficulties you come back and get me to explain the algebra to you is it a reasonable statement ok so so what we do is our all of you familiar with the notion of a likelihood function have you thought seen it anywhere so here's the point right let's assume I see a set of data points right so I see observations X the features and I see some result right why so the whole here is how likely am I to see these data points right so if I can find a farm this weight vector W which maximizes the probability of my finding this data right together that means I'm fitting the right function correct because I'm essentially I'm observing it I'm explaining it and I pick a function or a value which explains this but what I'm observing correct that's what a likelihood function is intuitively everybody with me so I am already observing this how do i pick those feature i mean those parameters under my control for a classifier which maximizes the likelihood of what I'm observing being true right but that's what a classified but even without a classified I just observed some data I want to find some parameters what parameters best explain the data right that means whatever I observe the probability of seeing that is maximized then that means is the maximum likelihood is really telling you ensuring that what I observe is very consistent with reality right that's what it is so here what we will do is we look at the probability of the data which is X the feature why the printer the outer X is like the input feature why is the output variable Y resulting y and we are saying can I choose a w which maximizes the probability of seeing these x and y sex right that's a simple question and now if you take the logarithm then you know there's a it's a monstrosity right everyone familiar the log is a monotonic function no problem okay so I mean why it's convenient if you have something to the product it becomes a summation right and that's why you do that okay so now let's see I'm just a sea of algebra see ya okay so how much are you folks up to some algebra I up to it yeah I seen a nodding of heads yeah the nice thing about writing verses using power points I'm forced to go slow so I can't skip steps and you can follow me that's the risk of powerpoints I think right so slow business Lee so you have the strings example I'm going to use my pointer on my screen so uh well we our goal is to get an example right because we want to show this so that you know how to solve a problem so you've got each training example X I and by the way what for the exercise let's always relate concrete things right let's see here what we had was gender marital status income in each right and then we were trying to predict light or regular everybody with me so your exercise every data point tells you male or female married or unmarried what's the incoming what's the etch rate and then the output is always the why I is is one if it's a light beer regular if they like the regular beer okay now so they're all drawn from the distribution px why they're assuming it's independent identically distributed everybody familiar with this anyone who's not yo independent identically distributed so if every data point you assume the excise are coming from x1 x2 x3 they're all coming from exactly the same distribution and similar the wise or the combination because it could be that maybe I am switching if people today are coming into the pub and they're all Irish and they suddenly becomes German then the preferences could change dramatically right or Indian Indians probably like beer right not too heavy the German let's go for the heavy beer right so you could have a transition right is confusing but you want to make certain that it's sort of the same distribution otherwise you need to use some other model ok so now so we can write this as look at it so you have many data points right we want to use all the data point not just one data point right so we say hey how likely is it I'm saying this person with all these characteristics of age and gender and income and a light and the preference of beer and the next one and the next one is next one right we are putting them all together and you want to use all the data so now we take the logarithm of the product of seeing all of it together but because these are independent they're identically distributed but they're also independent so whether were a prob abilities are independent we can take the product right so what happens to one customer is independent of what happens the other ones to take the product so we take the product of all these things and all these numbers depend on this when you think of classifying a fitting they depend on this parameter w right so we get this nice summation here right so our goal then is to maximize this expression right find a w which maximizes the expression everybody with me but then i will add all the data points together so the product here becomes a summation when i take the log right here no problem okay now we also assume that the x value whoever comes in the they you know the people they've kept their characteristics don't depend on the classifier you're using right because w is a function of classifier you are using right so they don't the value is in de pere of that so you can dump this so this expression excitable you depending on w it doesn't depend on w so you can dump that so all you care about is optimizing this summation right it's made easier I'm skipping a little too quickly or e okay yeah wide awake okay now this is where the algebra is a little tricky bit of notation by the way no mathematics is difficult the only problem mathematics is just getting familiar with symbolism and that's what is hard to keep track off I think right so here you're going to say the probability that given any data point X and our weight vector W that we need to choose if the plot if the probability y equals one that is a light beer is given by g x w write that expression and that's this logistic transformation won by this called logistic transformation right logit function 1 by 1 plus e to the power of minus WX right and we'll call this thing y hat now since the probability is any any given person which is a data point X either person drinks light beer or dark beer so the probability of y equals 0 X W has to be 1 minus GX w or 1 minus y hat right because if you're either a light beer drinker or a dark beer drinker or regulatory a beer drinker so the property of being a light beer is y hat then the other probability has 2 1 minus y hat no problem easy okay so we just say that the probability that give it a data point I the the probability of the output being one or light beer is y hat or y 1 minus y hat otherwise ok no problem ok by the way let's fix the typo ah now I know why I didn't fix it I will go back to me okay not let's not fix it ok so so it means if I have all of you are familiar with the binomial distributions multinomial distributions right but only trials anyone who does know Bernoulli trials not every does huh come Bernoulli but no ber in Oh ull I Bernoulli yeah I guess my pronunciation is probably a little different than yours ok so the problem the probability is its data point right sorry this is wrong I take that ya know that's correct so if the ice data point so y hat I say if why I is one look at this expression if the I data point was a light beer why I is one right so what's the probability of my ice point being a light beer y hat now on the other hand let's assume it ended up being a dark beer then the then the dark beer is why i equals 0 so this term doesn't look her but in this term i have 1 minus y hat to the power of 1 minus zero which is one minus y hat right so this is nice expression you just expresses what we don't know here compactly correct succinctly so our objective learning function then is I sum up all these terms right away and so the log of this is this expression probability of Y I given x IW is y hat correct so i log of Y hat but I also need to look at the but I had why I on top here so in other words i take this expression I'm summing this expression log of P by I hat but that is the logarithm of this expression so I get why I into log Y hat I plus 1 minus y I log 1 minus y hat I so let us try to understand what this mean for every data point why I is either one or zero it's one if it's a light beer if it's zero is it's a dark beer right and y hat I remember we did the error of this Y hat here was merely the probability y equals 1 given xw right that's 1 plus e to the power of minus WX everybody with me okay so this is algebra okay so I have an objective function and what is my goal law my goal is to solve for the value of W which optimizes this right because i only have one thing everything else is data X is X is data why is data so the only thing here so why I here is just data if i look at y hat that's the only unknown thing here right but what is my hat Y hat is 1 plus 1 by 1 plus e minus WX X is known to me every data point is known right so the only thing I don't know is w so my goal here is to somehow compute or estimate w and how do i estimate w that means all these this whole expression depends on w and i want to differentiate this whole expression with respect to W which is a vector and I want to find the optimal w now if this function is you all know that if you have a convex for I if I have a concave function if I set the derivative equal to 0 then by solving for it I get that that parameter value which gives me the maximum right if it's concave so it first verify its concave correct so it looks as though all of you have had a course in linear algebra and one in optimization is it a reasonable statement no so when i when i make the statement well it is concave if i differentiate it said it is zero you get the optimum you in already your head because you wanted to look nice to me or did you know it is hard okay no no but my statement that if a function is concave that you know okay it's just a word forget words this is easy okay okay so so I mean all of you know sort of a chain rule for differentiation right so Mia if you want to take it on faith or that is the expression and you can check the algebra later on so this is the gradient right you look at the function and you look at the gradient of its respect to each the parameters right and so we get an expression right here you see that so if you check all this you get the the expression that the gradient of all this is given by take every data point take the actual value that occurred whether it was actually a light beer or a dark beer and y hat I is what you predicted so you look at the difference between what it was versus what you predicted using a given value of W so in one iteration you assume a value of W if you if I assume a value of W then i can predict then I can predict what my hat is right because Y hat only requires knowing w the data point value X 0 X is known correct so I start by assuming a value of W then I can I know what the predicted value Y hat is right ma'am and I also know what why is actually from observing the data and I know what the feature values x ir right so i have a gradient value which i compute from this and if i have a gradient and i keep on updating each time i go to the optimal solution right for all of you you know if I have gradient search that's the scheme i apply right so you have a scheme where I start with the value of the coefficients being zero I start with the initial direction being zero and I keep on updating where the error is the difference between the original actual value of the variable why I which is 1 or 0 minus what I predicted and the direction I update by adding this error value and I Kipnis and then I take W and I take the original value of w/e 20 I and ETA which is some get some small coefficient i use times the direction they keep on updating each time till i go to the optimum value clear no problem ok let me tell you it was only about the second or third time in as teaching I realized that no textbook case is very clearly out this little one is very book a little piece in a book so we needed to clean it up so now you know how to write an algorithm group prediction using logistic regression right okay everybody with me no problem yeah okay no one's feeling shy okay we're a little slow but methodically oh yeah right your mark yeah yeah yeah you keep going yeah how can you guarantee you yeah that's one yeah that's is a heuristic way of doing things if you are otherwise you can have the ADA be it you know keep converging toward zero is another way that all these schemes right that's organization but so so yeah so in some sense but that's a good point so each of these things coming talk to me off I'm very good question good I like that he's wide awake okay now i believe uh so um now the question that actually asked was well do I need it can I only deal with the two classes or can I deal with more classes okay well here we show you how to deal with more classes ok let's take you know the way by the way whenever you think of generalized something even you want to generalize something what do you do you first asked the question why did it work if you know what it worked then you can see cat a fudge and make more complicated situations look sort of similar well if so great it's not either i give up or I said maybe I can do something else to make it work right so if you think of why this whole thing works let's go back and see white all words it works because if you look at this right if I look at these ratios we want the denominator cancel off so I just get this ratio of 1 and E minus WX right so you see the relative odds of this happening or that happening right so somehow my impression is if the denominator could be complicated but all the numerators are sort of looking similar right life might be happy right so in Sophie say if I've got three maybe instead of e to the power of minus WX can I get some variant of that right that's the question so let's see how we can do that so viola here's the trick instead of a fixed w one for every class if I had w1 w2 different weights right and so I think of a situation when the last the cake capital K the last I have cake classes Capitol Kate classes and you know let's think of each of the class is not the last one but think of each of the others right each of them have probabilities the same way e to the power of minus w1 KW to KW 1x eat / w 2 x and so on right so then certainly looking at these ratios you want something to look like this the law guards right so if you do that then the probability of any little K means any arbitrary class will be all those Exponential's divided by 1 plus the summation of all these probabilities right right and in the last class just like before it doesn't have an exponential but gesture one remember in previous cases one class had a 1 in the numerator and everybody else had an exponential rate well here that's you still have the base class still looking happy in one the other guy is a little bit more complicated e to the power of something probably wise everything goes through right so I'm not proving it but I'm sort of show you roughly what it looks like right credible plausible okay okay so so we did the minor example all of you are happy with this you know how to do this no problem remember it yes we have minors the number of years they were exposed to coal and then the number of severe cases and so the proportion of severe case is why you can compute right probability so can you see the features years number of you know okay so here notice that the proportion of severe cases what we're doing is why is computed by the number of severe cases where the number of miners right so we are computing the probability from the data but number of years is the only feature we are using here nothing else now our years of exposure right so we that the last column why is computed by dividing the second column by the third column correct and now when we do a function you can see the years of exposure on the y-axis which is the actual data and the weather people had a severe case of health or not which is why on the right hand side print the proportion which is the probability right of somebody following you now in the second plot on the right hand side on that real actually what happened we overlay our model which is a smoother curve right but the smooth occur is pretty much fitting the original curve right so looks as though it's a pretty good fact using logistic regression right now as I've told you before for those of you who watch BBC productions you can see coal miners exposure people dying in the mines so this is a pretty quantitative perspective on life right okay okay now from your viewpoint what would you like to use what is your secret aggression for in the internet world getting away from unhappy subjects such as coal miners what happy subjects would you like to get to if you had to now that you have a model a model is like having a hot rod right you can have the roads so what would you like to do you know how to optimize it right so let's see we've got who is the earthquake specialist here but not yet Pradeep Pradeep is earthquake right so the earthquake guys want to know so their question is if I see a building if I have an earthquake will it crash or not or what is the probability of scratch right so you can start looking at the data you have to see what is the likelihood of and there are lots of tests and so what they do is they don't want to build into crash so they actually with it they run some tests to see whether it crash or not so you could actually start looking at the data right to see at what point does it start crashing right so that's that's one interesting example any other examples you folks want to think about before I while you're thinking I'm going to give you a few mins to think about a nice example or what you may want to work on are some of the any of the problems where I in crowd science i'm going to describe a problem in a moment and tell you how you could use this prediction of class in the moment letter that so what I want you to start thinking this is supposed to be originally it was suppose with seminar class not just collected class so you're supposed to have in dusty speakers professors sometimes lecture or going to be lecturing quite a bit and then you folks are supposed to come and talk in the most engaged that's the deal right okay now the other part of it is w hat is it is it the constant or is it random variable what is w hat that's my estimate for wo right my weight vector so is it the constant or is it a random variable okay so let me tell you one thing if you in all estimations of the sort all these are random variables right we don't know what the value is you're guessing we're going to estimated based on data and then the estimate depends on the data sample so it's a random variable now if I'm given a random variable and I  it and say it's a normal random variable it must have a mean value we use that so is that enough then we estimated w hat right which is a approximation for the weight W and we can do a logistic prediction are we happy life is happy we stop there no okay yeah akshay by now knows when prefer Keller and say something am a sphere reason should be suspicious okay so what do you think we should look at actually see rule one when Professor projects something talk about what is projected trying on the money right right right so basically we have a random variable so you're using to predict something but let's assume the variances or covariance is miserable then the prediction must be miserable too why the hell would i be using it right so so your folks must think about it right even if i use an arm like proximate right so so here's the situation where i think in this particular case you have the covariance value right so the only reason I bring it up is anytime we have any prediction and all of data mining is about prediction and we are feeding these coefficients every coefficient is a random variable that means it has a distribution and we are always asking for we made some as an assumption about this random variable it's normal it's 0 it's not zero right in hypothesis testing right we are saying p value is a 0 or not right so each of these assumptions must be tested right so in this particular case you have predictor w and then you have the different values and you compute odds ratio confidence intervals all these right because once you have a random variable you can do all the strength and all of you have done a regression or somesuch course in the past right so clear no difficulties no problem yeah okay so but don't try to make me happy now because you'll be working on projects right you need to be learning right so so it's so by the way one thing I'm I'll tell you all of you because I what do you think there No so my comment to you is based on learning so in loving let's go to Professor what do you think learn professors most important characteristics should be and of course this applies to students as well any good answer dinner yeah okay tea tea tea tea tea yes that's a good answer oh yeah that's a good feature no actually you could even get a dinner because really honestly when you teach you want to understand what are you following what is the student following and not right so that's a good answer but I look for a slightly different answer but we still get dinner but i'm still looking for my dinner answer that's also good okay one more all good answers i'm still looking for one more answer okay i will tell you if i'm not willing to be stupid and say hey I'm ignorant or I don't follow something then I cannot learn right correct as a professor so when I try new areas I am stupid I don't know what the hell it is about you go la right so that's funny so even for you so the reason I make this point is if you don't follow something you should stop me and say I don't follow or can you help me understand this idea better or not right then your time is spent well- van spectra okay so i'm assuming all of this is clear to you is any if not or going too fast too slow please come back so anyway so now we'll start speeding up a little so in when we start looking at the parameters we have w if you think of wo right if i only have two variables right actually have one variable right in this example number of years exposed to coal or mine so you always have when you have this i always have an intercept w naught hat plus a value w 1 hat which is a constant in sitting in front of number of years of exposure to a coal mine right so now let us assume I increment so that's this era hat X i if i increment the the number of the the regressor or the independent variable X i buy a unit right then i get w naught hat plus w 1 hat into x i plus 1 right so i look at the difference between both these i get w 1 hat correct so the interpretation of w 1 hat or which is a long guards is the increase in the probability of success associated with a one unit change in the value of the predictor variable right I'm using an independent of predictor variable to make a prediction if I changed it by a unit how much of an improvement in egg do I get right in this odds ratio and that's what is coming from the from the question WI right correct so until you understand yeah no no no w/e onehand would be the coefficient see here we had this exponential 4.79 65 minus something something so this is w1 this thing is w 1 and this is w not the intercept is w naught and the the gain coefficient is w 1 right coming coming in ah why do you have that so you're asking why is the okay so issue is why is it that W 1 hat alone is showing up that's just awareness that's the point that's the interpretation ok so both are required for the curve but for the interpretation of that particular thing but intuitively you know what you might say is ok so maybe another way of looking at so so let's assume i divided this entire expression by this this e to the power of this this is constant right so that constant will go here it will go here and you will just have something into X here right so so those concepts are not shifting things that's why when you look at the ratios but that's a reason see that constant keeps cancelling off when you look at ratios and that's why it doesn't matter because you want x3 there right so the only way x shows up is in the constant cancels off that's what's going on ok so right so the the odds ratio of x thickness you know i plus 1 versus being I is exponential w 1 hat right so new new more core neurosis yeah I got it so y hat is given business expression if i head to the prediction rank of whether somebody is going to severely ill right after x years of exposure and so the logs ratio is 1 point 10 that means every hear of exposure the additionally the odds of contracting the this severe thing increase by ten percent right that's why you use it so clearly you can see in mortality in insurance there's a nice function to use right but a lot of things have this right if if bugs are more likely to occur in software because of increased interaction if yield deteriorates because exposure contamination many of these models right hazard models could benefit from this right okay let's see how are we doing on time okay so so by the way is this the right place for you people yeah i'm going to show a few more things and then I know so because last time I know was it mark or ever somebody said well you know you you're so quickly sort of showing as a bashed-in going on but can you sort of shows the pieces that's why I'm doing this right so I'm trying to balance it off so yeah very quickly and this is something you should do all of you are familiar with summer square is error how many people are not raise hands you don't be shy okay with it okay so if you think of it right our objective function here is maximum like maximizing this likelihood right it's not least squares error so if you have least squares error then the sum of squares is always an important measure right so because you're always asking if I build a model how good is the model how good a fit is this model based on this fitting these parameters to the actual reality that's the question are always asking right and you can never have a perfect model so you're only asked how much does it fit the so in other words if I have the data I don't go beyond this how would a model can I get and you can still run get a perfect fit right think of it if I'm using a straight line for all sorts of points I can't get a perfect fit but least squares estimate is the best fit in terms of the lis I say he'll objective functions in a draw a straight line I've got points the difference between any point and a point in a straight line is something I want to minimize that error the square of the error so I have an objective function which I am minimizing the error in some sense but in our particular case it's not least squares but it's this log likelihood raishin we are minimizing right so because of that or maximizing the log-likelihood so the equivalent of the least is estimation error or the sum of squares error is what is called deviance which has this particular formula right so you can see the number of data points and you can see the the type why I is the type of beer or whatever right or what our category are predicting so it's an expression which is a little different from the one we are used to but it tells you how good is your fit right now what is the base let's assume we didn't do any of the sophisticated stuff we decided to be naive weather is naivete stupidity now right now it is not stability so here you make the simplest possible assumption right the simplest naive assumption is I have a lot of data points and of all these points are 100 people let me figure out how many of them care for light beer maybe in Germany it's 70 or 100 buying everybody I'll declare is interested in dark beer in India Saturday are innocent light beer everybody had declared to be in distant light beer so lots of errors but I don't care that's my base ok so the you always want to compare this error that comes through with the errors which accrue when you have a knife model correct so that tells you how much this model is helping you compared to a naive model right ah let's hold off select a little bit yeah because they didn't explain what is saturated I in explaining of this so for right now let the stay and just look at this expression yeah but i'll come back here whether we are all of you familiar with the with the t-test the f-test AF distributions okay let's start everybody humbly of normal random variables okay if i square a normal random variable what is the distribution hmm chi squared okay what is the sum of chi-square distributions chi squared okay if I have a normal / chi squared is it anything okay what is the square root of a tie square distribution just by Justice square root he ought to have a normal / chi squared normal divided by square root of chi squared what is it so these are things you should become very familiar with it's a piece of cake and if you have a little difficulty i'll be happy to guide you right because you keep on encountering them right see why is that purely for reasons of ease of analytics people for continuous variables we always use normal and variance software that I'm describing and for anything discrete what models would you use binary then only multinomial so so I don't know whether you folks the familiar or you need a little primer on some of these so let me know okay okay so so anyway so in the if you think of the f-test for a linear regression model do any of you remember that f test you remember sum of squares some so do you look at some ratios of some of some some of the SSR and some of the regression divided with some of the error SSRS se ratios right so if you look at the sum squares each of them is normal and the sum of normals is normal square if you take normal square test quite some of chi squared is chi squared ratio of Christ squares okay you do get dinner yeah you do get it good I mean but at the point is it does matter where you are in this case is right but it does matter together I roughly speaking your sink that I'm leading up to right the reason is in all these i have a random variable i'm trying to figure out hey if i assume is a zero value i know the distribution has got some value which is far out very unlikely right very low value then in a hey this is not zero that means this coefficient is valid right that's all the idea right ok so the equivalent of that but that was the case of a linear regression model with least squares prediction and so on but here with the objective function we have the equivalent of that is if you look at this deviance right which we computed and you multiply that by n minus P n is the number of samples p is the number of the predictor or independent variables right so like this degrees of freedom sort of idea we have and you divide the deviance by n minus P then if that's significantly larger than 1 that means the current model is not valid right so now we are saying here is well just so what are we doing we did everything about prediction linear prediction with least squares now we are switching to logistic regression so just like in the old case we said when is the model valid when is it not model / Evan is not valid you're giving you all the tests for that right okay so let's see here so let's empty can I get you something practical we are so the question is almost it's okay study okay so my question is how pictured are you people it will take a while to get through all the ideas you have the patience yeah I see some rigorous nodding of heads how about everybody any anyone wants any other sequence of this is fine fine oh yeah okay so you know I'm also used to teaching MBAs and mes had short attention spans so they want to know how can i use it right away Engineers I think are much more shall I see patient and willing to learn concepts okay so so now we have if you have multiple coefficients Wiri wih right these are predictors right now each of them have a standard error associated with it SBJ anybody knows what a standard errors yes now also I give a dinner no t.t oh yeah yes but but yeah the you have signed a deviation but what but what is the standard error but you have it so you have estimated coefficient WI hat right so it's an estimate so you're thinking of standard deviations in connection with that estimate right and that's the standard error I'm saying Saturday VA shin is for any random variable but if I now have a prediction I'm always think of errors in the context of the of the prediction variable right and so i'm estimating this WI hat right so the standard error is associated with that prediction variable or coefficient right so now what we are looking at is is this coefficient WG had zero or not the null hypothesis is that the coefficient is zero whereas the the hypothesis we are considering the alternative s for alternative right is is this not zero right why do we care about it because you want to see number of years spent in the mines do they or do they not influence whether the disease is serious are not right if the curve here is zero number of years don't make a difference if the coefficient is nonzero the number of years make a difference right so this ratio of this even you divide this the coefficient by the standard error SBJ this is called the wall statistic and the associate p value indicates the statistical significance of the predictor excite right there's a standard sort of thing right no big nose correct everybody with thee or everybody says they're learn prediction regression right no worries yeah okay any doubts come back to another I volunteer do for people have doubts about our or about any of these I said you can help them okay now next question this is an interesting topic ok now we've got a mathematical muscles up and limber and okay another question I'm going to ask you to tell me I were two classifiers and I'm going to ask you tell me which is better right so what do classifiers do so I've got light beer drinkers I've got the dark beer drinkers classifiers looks at Ray the Ray has a sunny disposition he must be a light beer drinker but Ray goes off on vacations maybe to Bermuda and he's gotten used to this very thick heavy German beer so false right so so in other words he would predict his a light beer drinker but he's actually not right so that means I'm I'm misclassifying somebody who's a dark beer drinker a light beer drinker right so what so that's one type of Miss classification wouldn't you say ok another possibility is I look at pranava and I say look boy this is a young obviously young kid I apologize young man and say he is going to drink the thickest heaviest beer but pranava has got a very level-headed fellow and he drinks light beer right so so so what type of error is it right so it was actually he is a light beer drinking I'm calling him a dark beer drinker right there two types of errors right so we're gonna look at those know either case is bad now and I gave you an example last time in semiconductors if I say a process is in control when it's not I lose a million dollars when I say hey all you know it's out of control when actually it is in control people scurry around I lose some time but it's not a disaster it's not a million hours right so this is a symmetry right so when we look at all these classifications we need to think of this when we think of a classifier right we need to think of which what do the characteristic were classified which are important right now so you want certainly when you look at a classifier you certainly want to spell out the parameters of a classifier right in the logistic regression these coefficients w or the prediction w hat was what characterized the classifier right now you so we knew you also want the you want to be able to come to compare two classifiers so that you come up with the best classifier possible right between the two you want to choose a better one but overall among the set of classifiers you want to find the optimal one or the best one right but if you can't find the optimal if I have just given in classifier sort of some set of classifiers how do I compare each one with the other to say this is better than that it makes sense to you ok so the criteria are being reasonable accurate and cost measures right let's look at them so what do we mean by being reasonable let's assume I know the area right as a doctor I know that if I see the onset of sepsis maybe I see some conditions prevailing right like our flu and irani nose itchy what are some combination right so if now if the algorithm is not giving me that relationship will I know it's true from the domain hey there's something goofy about the algorithm right so unique so in fact yesterday sorry I visit a conventional for about 185 people yesterday who are all looking at healthcare analytics they're all kind of startups in healthcare analytics and the buzz word is predictive analytics Lee remaining prediction that's the hot buzz word so the point company was making is we had a panel of CEOs yeah so one guy says well I shows this issue of machine versus human doctors predicting so he says you know people started laughing when he said you know the machine is pretty good right i mean people are saying you know one of them said you know I never trust the machine without the doctors inputs which is true right that's reasonableness and that's what I wanted to refer to here but a site point is another guy says another CEO says you know but these doctors and let me tell you there are doctors and doctors and you go to the emergency room the probability of misdiagnosis is apparently twenty percent yes so don't go there right so he was claiming actually sometimes algorithms can be better but it doesn't that's there but you have a good expert right then you need to be able to go with expert now also in on these predictive variables actually available in future for the prediction because maybe I show dependency but I don't have that available in future right by the way I actually ran across this problem myself because we are looking at bringing GD monitoring with a colleague at UCSF now but you put probes in somebody's brain to look at the cranial pressure only when somebody has had an accident and is serious otherwise nobody the right mind would put some probe in their head right so this makes for problems because there's no benchmark so you so there's some interesting tricky issues when you do data mining now if a classifier implies a certain order of importance right amongst the variables this is a good question by the way this is a question that I got beaten up by this cat this guy from from ebay who had this operations research the MIT so our p values right but now we already said here here's a random variable we figure out whether 0 or not how far from zero using a p-value right so it looks like hey if this is far away from 0 this should be a strong variable right so hey I'm going to rank order all these variables based on the p value and is that order reasonable right now anybody gives me the correct answer and I did bring it up last time so you must know the answer at least you have very awake going but p values is correct no malice or says no okay how many people agree with Melissa okay akshay agrees Evan agrees I'm not trying now I know the last part of it is chai tante not that I'm getting it slowly I sort of think in pictures okay good okay a lot of people agree okay okay then I won't make it too much of puzzle so what will you use Ryan Carter things like we'll come back to that I mean you know the answer will come back but so this is a reasonable reasonable criteria right now this is the point will be being to look at a little bit more here the accuracy measure what do you mean by the accuracy of a classifier because for prediction we have the root my root mean square error right because least squares that rather objective okay but here the accurately measure is the idea is to compare the prediction with actual responses right for example forecast errors in time series or residuals in regression models and so on right so they're in the regression or time series we can always look at actual values predicted fitted values and so on right however when we do classification we have the equivalent of that we're not plotting them with their with time we said how much is the error right we plotted it but here we have something different because we are predicting a class and let's look at what happens this is called a classification or confusion matrix and we will see why in a moment right and this can be done either for the cleaning and or the validation set everybody is clear about what is the training set what's a validation set we have a training data and you fit your classifier and everything is on the training set including predictions but hey you already know it right you can compare the predictions you made with what actually happened but in future when you predict you don't know ahead of time right you predict and then you really see did it happen or not right that's the validation set or the test set okay so this is a core idea of issue of for the classification of confusion matrix oh oh ok you said professor needs a break maybe can go this or get a drink of water and come back positive okay so resuming okay folks back class so the classification confusion matrix you have two groups right to classes and you can generalize it but so you have c1 and c2 on the y-axis we actually have the actual class c1 and c2 right and we always say why the output is a class right c1 or c2 and the predicted class is on the along the columns or the x axis y hat equals c1 or Y hat equal C 2 right so it means you can have a situation where out of a total of a plus B plus C plus D cases a you correctly predicted that what was a class c-1 why was actually also a predicted at y hat to bc one right correct life is happy however you had be of these where they were actually see one but you predicted that with class c-2 so this is called what a false negative right because you sorry it was actually see one but he predicted see to correct so let's see what false positive is let's assume I have something which was y equals c2 is another class but I predict it to the class one is that a false positive right so the first one is false negatives the second one is false positive right and B is again a happy situation so now if I look at a classifier so I'm just here the summary right so now the way we use it in the case like the beer preference right typically we we have the cutoff for the probability value of success remember we have this probability we're predicting right is it light light beer of not and you can choose point 5 makes reasonable sense so anything is what greater than 0 point 5 it likely it's like beer and below that okay but if I do that because I'd notice that that threshold can be changed right if i change that threshold for at 5.5 here is the situations where here is a regular drinker who is predicted to be irregular those 27 data points here's a regular was predicted to be light right and when so if that's a missed classification what is that called false negative and here a light beer was actually sorry a dark be a light beer was called a dark beer right or regular beer ok false negative ok so this is the classifier right so the question of course is if these numbers keep on changing if the total data points remain the same but they're really about how do i know which is a better classifier right that's the question ok so here are some size very popular measures which also makes sense right this popular because it makes sense the overall act see is very simple right I predicted a of these correctly class one belonging to class one d of these correctly class 2 belonging to class too I predicted B and C belong to the wrong classes so the overall accuracy is look at what is correct a plus d by look at all the a plus B plus C plus D it tells me how many times I'm right right or percentage this is called the overall accuracy ok now the overall error rate is one minus the overall accuracy make sense now the base accuracy of a data set is the accuracy of the naive roll what is naive room out of anything we pick the largest number belonging to class and label everybody the same way right so if you do that the proposed but so what you say is if I took a population of hundred of these 70 were blue or 17 for light so I say the probability of anything being a light is point seven right now the base error is the 1 minus base accuracy now the last concept is very important any time I have a classifier how do I know how well is performing right any performance improvement is respect to base right so here being said if the base accuracy in the base error rate right so if you say look I use the classifier and I look at the overall error rate after i use the classifier now let me look at the base error that i started with minus the overall error because of the classifier and i divide that by the base error right you can look at it as base error by base as 1 1 minus overall error by base error right so 1 minus the error due to be classified by the original error the larger the smaller the error due to the base error right the way it results keep on getting better and better right so you get towards one hundred percent so that's the goal here correct so this is called the lift or the improvement of a classifier so a very quick point is if you know you're predicting say bankruptcy you have two classes c1 and c2 you are predicting bankruptcy so the the importance of correctly or incorrectly predicting them could be very different right because if I say hey I'm gonna invest in the bank and it's going to be bankrupt versus hey everything is hunky-dory because believe me I don't want to mess up with predicting that right by the way this is a very real thing all of you maybe your parents had money in bank accounts or something right I bill I I did look at the ftse regulation on whether our money would be protected or not during this whole thing that happened a few years ago right so that really makes a different time so so you cannot really look at the overall accuracy where you count everything equally you want to look at the false positives and false negatives with different costs and weights right okay so so are the characteristics how to evaluate classifiers coming through clearly right Marco you're fine this is all fine ok so now so if you have the acura t measures for the UNAC unequal importance of groups right how do you handle that well let's assume c1 is an important group and c2 is not so important right then in that particular case what is going to happen is that the sensitivity of the classifier is its ability to correctly detect the important group members right so a by a plus B that's all you care about here right okay because all you care about is everything that is being predicted a and C are actually sorry I take that back Ava a plus B a and B and actually they belong to class c-1 a is labeled correctly be is mislabeled right so a by a plus B now similarly if you want to look at the other population which is the class to write so d x 3 plus B is the specificity which is the sea to members right so you have this sensitivity and specificity right now finally we get into this false positives and false negatives right and remember for false negatives is B because we we we predicted that this was class too but exactly class 1 right so we lost out so this false false false negative because we just missed it out whereas in the case off so that's why we get again B by B plus D on the other hand the false positive rate of a classifier if you look at c c is the value of something which was actually too but it was predicted to be one correct so that's a false positive rate right so you could see y + b C C by C by a plus C by a plus what surplus t okay so yeah you could think of different measures you know it observe what makes sense in your in your context right ultimately it goes back to what makes economic sense or business sense now so actually talking of what makes economic sense so this is a big issue there are the statisticians there are the machine learning people and there are business people and they're all different so statistic shins always care about am i doing least squares prediction maximum likelihood error for the past fifty hundred years it's sort of the set you know now very good but may not always be useful for us any idea why ok let's look at least squares error right but at least who says these squares is a good objective function so in inventory models right if I have backlogs versus backlogs may be much more expensive than holding inventory right so you have something as symmetric so your estimation they need to be different for those cases but straight decisions don't care so in this particular case you've got actual class in the predicted class and you got the true positive the false negative the false positive and the two negative when positive is class one negative is class to sew machine learning methods usually minimize the false positives plus the fallens negatives the direct marketing guys maximize the true positives so you can see how you know what you care for is different right ok so that's important because most people I PhDs I had to keep on mentioning this to them because they're so used to literature they start writing things out or no name right on the objective function so a tell them hey think of the objective function think of the objective function it really is different and important ok now let's see what else are we do you have oh the last thing is what i said is really cost sensitive learning right that different things cost different amounts of money so in the previous things we guess at a be just number of times something was wrong or misclassified here maybe if you had company had leukemia and you misdiagnosed by golly you want to have a lawsuit right apart without losing a life well if you want to approve a mortgage guess what happened of course in a mortgage crisis of course they threw the rule book away through the classifieds away but if you didn't have a classifier and you did it wrong right that's expensive web mining little cheaper but will x click on this link was it's not promotional mailing will expire the product right because you're going to say I have a nice brochure colorful spends a dollar or ten dollars and it was how many going to check picture to the wastepaper basket with high probability of why bother to do it right so you're going to get all these right in terms of cost sensitive learning so so this is merely telling you that the traditional methods ignore costs and you can the ways of accounting for cost-sensitive learning could be to resample instances according costs right it's essentially awaiting them differently are waiting them by cost and so on right so it's actually render some icon for cost either the probability are in the weight so let's charge you need to figure out how to compute them I'm going to skip this for right now but if you have difficulties come back to me by the way there are a couple of books which are little easier than the textbooks i have given you there are tougher books and easier books there's a nice business sort of all ancient book so that's easy because they make is mental NDA so they make life easy for everybody but they don't teach the theory that's the problem but we have that so I'll skip how to construct a list list lift chart so this is important but you should be able to fix it out yourself let me skip this roc curves as well for right now and come back to the necessary ok this is an important idea whenever we do these predictions cross validation so let's assume have a lot of data this is a big issue in data mining I've got a thousand data points right now before I go to new data I want to figure out with the thousand data points I want to divide this in all datasets are typically divided two three three pieces right one is called the training set the other is called the validation set the third is called the test set test set is brand new I'll predict I don't know ahead of time what the answer is and somebody will tell me was right or wrong right okay the training set is he I took the training set I fit the model right I think of all of our data mining as I have some exes I'm predicting wise I'm fitting that function that's all they remaining so the validation set is if I out of a thousand data points i only used eight hundred to fit the parameters as try it out on the remaining 200 to see how well it's working right everybody with me now the question is if i have a thousand data points are some number 80 points should i use only one person to train in and ninety-nine percent to validate should i train on 99 percent and one person to validate 5050 what should I do everybody with me okay now these are all real very real world issues in data mining right what do i do what do you think by the way silicon lattice area of quiet all humming along nicely ah okay okay hey what would you do but by the way chuni who's at the other end he is dealt with all these issues at Cisco he is dealing with them still chewy some time will be nice to have you come out here too or maybe along with street from Cisco to talk about all the difficulties in trying to predict in use these methods which very little data right okay good so yeah that's right which falls on the my grandmother told me so and the question is why did my grandmother it could be as my grandmother's right but why right now mean so by the way as since we've done talked about central limit theorems in the past right how many data points do i need to approximate it if i wanted to use a sample right data points and i want to say hey I want to take the average expected value and use a normal approximation how many data points do I need to do that anybody knows 30 our grandmother told me ranges from six to 20 no matter how many the more messy the distribution the closer to 20 the better behaved in more uniform the distribution closer to six 266 low end if very nice uniform looking distribution six messy looking things all over the map 20 but again this comes from experience and sound right so but in statistics if you have data how do we do this validation we actually have some pretty good methodology it's not that an ad hoc and that is called cross validation so any conference you submit a paper to if you don't do cross validation goodbye okay so the idea is inside us something to Joel are you there at the other end you know that but we are working on some paper and so this whole issue of what is a reasonable measure keeps on cropping up how many data points what's a reasonable measure what's crowd it comes up every time so so so the validation set is clearly being set aside right to assess the predictive performance of the classifier now when data is scarce so you go for something called k for validation so you split the data into K roughly it equally sized parts right by the way another thing when we are talking this company crowd signs they have so much data they said hey what's the big deal we have so much meanie and we don't need to use all of it in fact we need to subsample because using all fit is ridiculous it's not meaningful that's the other issue okay but let's assume data scarce right but data is scarce not sorry andrea is yeah ok so when data is scarce the question is you split it into k equal roughly equally sized portions each one you fit a classifier right so and you can to the other so in other words you use k minus 1 classifiers to classify the data in the left out part right remaining part and then you can combine the Miss classification errors resulting from the K minus 1 classifiers right now typically you have K equals 5 10 and so on and if K is all n minus 1 right you use all the data so in other I'm using everything I only have one data point and leave out to predict right that's called leave one out everybody with me now a smaller k will give you an unbiased cross-validation estimate but with the higher variance whereas ver increase King you get the other way round right all of you know if I'm looking at average of samples if i divide by the number of samples the larger the sample less the variance right so it's a bias is the bias-variance trade-off is kicking in here everybody with me so these are all the important things for you in real world situations because people say hey how many data points is it enough for you we are being asked questions at cisco and we don't really have a clear answer they say hey Ron you know we have you are doing all this text mining you've got thousands or documents sometimes millions okay how many of these do you want labeled to give you as a good estimate I don't know well it depends on the characteristics of the data so we are telling a look we need some initial data set will get some estimates based on that and then maybe we'll get a better prediction next time right everybody with me okay wide awake not fastly okay good okay oh by the way we should be discussing projects as well right okay let's just finish this case study does it sort of that's obviously so I actually got these especially from there's a guy who worked with the mint oh the earliest examples of using logistic regression or something laughs scale the u.s. mint so I spoke to him got his papers in the convert isn't too so I sort of like taking research and converting into classroom teaching this fellow so this guy I wonder what is this one something else I've got anyway this was a spring 1996 mailing campaign have you all heard of hell HCL home equity loans take the money out look happy and then maybe are stuck with it paying the bank right let's call it h actually it's a nice thing it's called a home equity loan HCL conducted by a major bank midwest and bank mwb right now any buyer or responder is a customer who ended up taking the HCL and actually paying an interest on it that's what banks are targeting you for right I'm sorry to say that when you go out and get jobs you'll be helping banks do more of this right so target people who will become willing victims or suckers so hopefully you feel pleased about it mean well so the continuous chase of concern is the gear to date interest that the bank is expected that's what the bank cares about how much money do I make so that depends on the size and the terms of each year right so the models that were built for based on three criteria profitability one is the fifth prediction of accuracy ok now the profitability is measured in terms of the resulting total profit or return for the target mailing an audience and the average profit or return right goodness-of-fit tells you how the model is capable of discriminating between the people who respond or her own respond right and so you in other words some people say yes I want to take the loan or not they respond and the other hate they actually give me money and they're profitable right but why are the profitable because maybe there's some transactional costs and incur right so if they don't if they don't hold along long enough then I'm not very happy about it right so is a binary ratio this goodness of fit is the my head lyrics actual response rate the ratio of the number of buyers captured to the size of the audience mailed or the lift in the actual response rate by the over a random mailing right by using this there's the improvement compared to random mailing or in the continuous model by the average actual profit to return for customer right now the prediction accuracy is measured with a difference in the predicted profit or response measures versus the actual results right everybody clear ok so there's a spring 1996 mailing audience by the way this is very typical of data that the number of non buyers is much greater than the number of buyers there are very many situations in fact the we've dealt with situations in which uni very often relevant documents are a very small fraction of the total number of documents if you have million documents I care about five so how do you use this right so this is intuitive as the sample of the non bias and all the buyers in the model training also the log odds ratio should reflect this true proportion of the bio non violent so all these features need to capture now here's the data so the training later said think of it the respondus what about 200 the non-responders for 15,000 people so when you start looking into all these industry projects I like the one of the students had asked me I think is it look I can I have a lot of examples with our and all that but really what is real world is what I don't know so you're going to be running all this right what is noisy what makes sense what doesn't make sense all these issues so that so the total you have the tests you have over 100 responders out of 7,000 non-responders which is very different right and the right because the training the ratios to do towards tool 14 where the test is 127 right okay so okay how do we do it our friend logistic regression van Gogh and we change the loss function to detect the bias correctly and the profit and accuracy of the model look calculated so you can look at the media but look at all the data data right the response probabilities the cumulative audience you'll get for each of these the deciles the actual profits right so this is really giving you the whole characteristic of your predictor right so maybe you consider it study it leader and come back to me but so finally the loss function depends on which class is more important detect right as we've discussed before and the accuracy want achieve and the Prophet want to make right that's it so unfortunately I finished last lecture logistic regression I have not quite managed to get to today's naive Bayes or nearest neighbor which will have a postponed but let me start but now you have a hammer now by the way knife base is a tricky subject so we will give you some readings for next class and it's a very powerful approach especially for text and so on so we'll try to guide you that it's a little hard don't give up hope for Hart we will cover a little bit world class and figure and even the the paper we are going to post a little dense even the PowerPoint a little dense don't worry we will we'll have to do something about it now what I'd like to do is I want to go back to projects let me tell you the crowd science project and how it applies to what we learned today and whether we will be learning several more techniques for classification and one of the approaches might be how does each of these techniques do compared to others on the problem so okay can you read the sub crowd science problems so I'll define one problem they mentioned to me and they may mention more wanna take a look at it has everyone looked radiant when ready say yes put up hands so we'll start so may be very successful not quite describing the problem right they tell you why they should be excited about the company but not about the problem right so so one problem i'll tell you because I don't think is such a big deal the others i think is better they keep talking apply and so on so it's better for them to come and describe it i think so so again i want to go back to what i described at the beginning of class when these guys think of the context i keep forgetting not do not use my my tablet pc because i can draw some pictures i think that's a little bit better i mean i'm used to the document camera and multiple cameras being i can use this that okay so the problem is the following I've got a website okay I'm crowd science com I've got 6,000 sites associated with me so customers can come to users can come to any of these any given website has got different segments I'm trying to figure out what are the characteristics of people who come to me and some of them sign up and they tell me their characteristics so I already asked route science I know what are the characteristics or demographics of these people so what I'm trying to do then is to provide feedback to the publishers with whom are associated with those websites to tell you hey these people came to your website compared to others these are younger women who are interested in perfumes and shoes but hate lipstick something like that huh so and also by the way they were interested in the graphics on your page on this web page not on that webpage their interests in their text not the graphics whatever it is right so that's sort of information they provided however often it's inferred because people may not be able to give you all this information so the only way to calibrate is we actually asked the people are on your website hey how much money do you make I polite way of doing it they don't read directly with those deals goodbye but a little bit more politely and also by the way you might you be interested these days and shoes or umbrellas you know so people are finally respond because they don't bug them they stretch it out they have some algorithms for doing all this right so then question is very simple very direct application think if I have somebody coming up at the data point so they say every day I have got 100 potential victims sorry about that victims but what other word is ok but ok I can i'm going to only choose 10 now which one should I choose so if of 90 people before I ask the question if i can predict pretty accurately and i know based on similar customers that I don't need to ask them because many of the features are highly predictable right based on their features because I want ask them the question others I will so if you do this it means they can prune off and save a lot of questions are already asking and save the questions for a more safer more effective questions or use their budget of customers more effectively right everybody with me so there's a direct application of what we've learned so far and the rest of classification that will be learning can be directly applied everybody with me clear yeah okay yeah yeah yeah yeah that's the idea yeah yeah so the real question is which of the cool customers can I make a prediction but Lima you may want to ask them 20 questions so maybe 19 questions you don't have to ask only one you have asked because you can predict pretty accurately the remaining 19 so which of the questions should i ask or not ask or if a person has got many 19 out of 20 i don't i don't have asked why bother to ask this candidate ask another candidate we're all 20 maybe not quite clear right so this called infinitive data points right how do I look for an infirmity of data point right by the way this is a very interesting research problem of active learning in machine learning which comes up all the time so information read our minds okay so so so hopefully this gives you some sense of how to use classification in crowd transcom now they even come and give you other problems but yeah now whether we believe these companies to kind of set up things we're on ec2 everyone family with ec2 amazon ec2 okay you getting in radar mining right so these days everybody operate not everybody but everybody operates or a lot of people operate in the cloud so if I want something small not massive large-scale production / impaired tryouts or an idea then you just go use a service provided by somebody in the cloud so Amazon ec2 supply side so these guys are putting it up in the cloud so that easy do so that we can run our algorithms and see with the data but they want to be really careful right they want all this propriety is they want to do supervised by me this supposedly research project so so if you folks are interested that's one thing so I'm going to give you a bunch of problems and you can choose these are a couple I'm telling you will probably look at some medical problems as well and so on okay and I think Jimmy may have something on computational advertising so we'll have a range of problems the other problem is how are we doing on time not bad we might actually get to be getting discipline here right some of class is all this telecast the coordination is a little tricky but now I think everything is a little bit more autopilot mode I think so maybe all of you can take a look at this one from this is called seven deal so both these are related to social media social media analytics that's a hot space right now right so that last year the hottest companies funded were image and video advertising in the next one to three years they expected with social media and marketing analytics so these guys have a bunch of a lot of social media sites right various blogs forums social networking sites and reviews of products right so over six or nine months they've done all this work so they they are actually able to give you using the text analytics engine because it's a very painful effort to do all this the good thing or it is they've done the hard work and they've got it all nice and captured which makes life much happier for you so they are asking hey can I look at all this and see is there any relationship between the social media and can you predict the pricing of commodities based on this this is a nice prediction problem right to do with our multivariate the prediction and maybe you need to do more but we'll see but right now of course if they hadn't done all this pre-processing you would have been text mining mining because they may be some correlations and dependencies between the different blogs and forums and so on so so the issue might be these random variables are correlated right so what is the difficulty here now what have I not taught you in multivariate regression we're all going to come up and maybe we need to revisit that if you have dependencies in the random variables right oh you're a whole bunch of things the bunch of issues you need to look at right and we want to go back to look at them right when you look at the three or four things right heteroscedasticity a whole bunch of variables right we'll come back to all that so you know I'm doing the baby version of prediction because you folks all said you're familiar but at some point we start hitting hard probably come back and revisit carefully right one big issue which is still not very clear to me in a research level B just because of ignorance is which we are dealing with AOL how do i attribute when I've got ads and I'm targeting ads and I want to look at the ads and say if a look at the ad i can predict whether somebody will act or not is how do I when I knew these Vinick if we cannot directly link this particular to that particular action I just have a great numbers how do I get some measure of attribution that's a key problem in online marketing with multiple channels and I I still don't have an answer so so some interesting problems it's not like this is you know it's all done life is happy just going to crank but some interesting problems I think right so this one they're saying that you certainly need to know software business off like are things we are learning here right and essentially they want to you to be referring knowledge of text analysis social media in financial markets would be helpful but not required but statistical tools and correlation building is important so you know what's a part of them right and so both these will start in the next week or two and go through the end of may write me ya may that's right and the midpoint check in is I think roundabout March what are the dates we had March much like March seventeenth or April first right and then in medical informatics predicting which stages the disease we are looking at what sort of disease symptoms are novel could be indicative a new disease those are the problems might be there and then computation authorizing predicting ads and how well they do so that's those are sort of projects we have and there's another social media company right here in Berkeley yes they may have some other ideas so so so so that's where we are now if you want to do energy and you want to hit up some Oracle or somebody I remember talking talking a vp of oracle about data mining of energy analytics i was interested in this whole issue of pricing but then to dynamically mine and figure out how things are moving in then price ready we can look at that as well we can explore so so hopefully I is this sort of so this seems a good point to a stop and we have about 23 minutes so is this pretty much consonant with what you folks would like to try how are things going Howard backbenchers you mean a little quiet are you happy okay clear ah so so you may need a liberal well print of us here so if you want to call him and his busy in his copious spare time of five art courses and so on and other things but right after class or set up a time I think he might be able to help a little bit i think and i can be around a bit as well to help so who else is having difficulties of the mathematics raise hands coming in oh I school students okay the engineers can it's been a while it's a bit of a gap yeah yeah but you know I find the NBA's all this is typical I think right because you're off working or doing something so it needs to be fresh so typically what we have is boot camp we sort of lot of people with all the statistics and all the all the tools and pieces separately so that you get caught up but yeah okay so if you could you need help let me know and we can work out something but I primary good start I think so if we can help otherwise sir I don't mean to impose on him but he's a kindly soul so okay or you know maybe should we wait for 25 or some other time or you know so you let me know yeah and all that's why I try to give so many assignments so that you can start trying out and nothing like trying out in data mining right you go try it out but I can also go step by step one possible thing is use this room go through the slides go through each one just take a little bit more time not everybody needs to know what they don't need to get bored but we can focus on i'm happy i'm very easily how about the rest of you how are you doing I fine grind grind good I like that crime that's good word okay that's good now so it does the projects do you think buying a week or two you ready to start working on them or what do you think whether people want to spend some time in the projects one two three four okay so we'll start with the companies when they come up i think so anything else so by the way since we have exactly a minute are you have fine with the course so anything else you want me to do change it now no everybody's happy good this is what you're looking for good we want to be well trained ah the library that is the issue I don't know Alaska grade you know whether the any of these books are in a library or haider this one is it is there okay apparently it is there yeah yeah and also by the way this is a wonderful book for people have some patience analyzing multivariate data is amazingly nice book with the geometry of the whole problem and linear algebra and very practical problems also load all consistent this one I would suggest holding off this is the Bible but I would hold off till my advanced I think this as ray has pointed out Becca is for you guys Becca may be easier to I don't know so this is actually almost like teaching yourself Becca as you go along type of thing you can download from the web so that's the other thing that might be easier for you guys I think so so that's the other 1i think this is very easily available at the camera is it online available online braid you remember sure okay it is it's written and Frank written in prank call data mining practical machine learning tools and techniques tools remember to so he's got Becca the guys who use the vaca thing so they refer back to Weka the modules and so on so it's sort of nice so it's just that by founding on am i working with companies you don't use our it's sort of it's been a staple diet sort of thing you know so that's why I'm using our otherwise Becca is the standard thing I used to use with students oh okay good till next time then and by the way please send me your feedback and in requests anything you need and also your projects in your remaining it send me a short paragraph now that you be in class for a while but what are you hoping to get out of it focused manner right and how the class is helping or what I also I could do to help you get there okay by the way so you need to start signing up not only for the project which main interest were also seminar presentations on different topics so we will let's make some headway in then we'll get you signed up okay good help good yeah yeah yeahs also doing all that yeah yeah yeah aha so by the way do you know these two young people ray Donovan and there's and this is a check I'm sorry I just "
yMQf-ZRHAFQ,22,"Data Mining, Data Science, Data Analytics lecture is for IT students or IT professionals or folks interested in Data Science / Data Analytics",2020-06-30T01:16:52Z,"Data Mining, Data Science, Data Analytics Lecture",https://i.ytimg.com/vi/yMQf-ZRHAFQ/hqdefault.jpg,"Dr. Abu S Kamruzzaman, Tipu",PT1H48M18S,false,33,3,0,0,0,"just say yes like that way you said before okay great thank you so much all right I really appreciate it because as I said zoom is a little different I'm used to using blackboard collaborative altra where I could see actually what I'm doing but in this case it's like I'm in a black hole so basically if I get your input that's when I know that something is not working or is working something like that so anyway again welcome back the topic today's topic we are first I'm giving just an highlight about Who I am or who I was all this time and then hopefully you get to know about me and then based on that you can understand that like you know if this lecture is beneficial for you or I am hoping that it will be helpful so today's lecture we are covering on data mining even though as was I was saying that the mining is not a actually feel this is more like a you can say a subject okay the field would be data analytics or data science okay they're mining and people do data mining it's not so don't don't take me wrong but we will the more general term is it's actually data science and those are two different trucks so anyway my name is abu cameraman or Tipu my nickname is Tipu so and you know I do have a PhD from computer science and my background is actually deep learning which is a branch of machine learning which is a branch of artificial intelligence I'm fairly new graduate for PhD so I do have some background on this topic and I do teach on this anyway and my job is I'm a computer science or CIS faculty at CUNY and also I work in the information technology in the IT industry same in the City University of New York and for those of you who joined I mean all of you joining from Bangladesh but Rippon you know this very well this is a map of New York City Subway and this is where actually it shows all the CUNY City University of New York by the way when was saying CUNY it's the City University of New York different campuses twenty-five campuses all over the city as you can see Manhattan and then bronze new Queens Brooklyn Staten Island all those so anyway having said that so let me and also that's the URL for go to the CUNY website and this is my background okay so I started my professional career with in 2001 as you can see it's almost 20 years and I have been in the CUNY all along meaning CUNY as I said there is 25 campuses I started my career at York College as a lab supervisor I was their webmaster I also became their network administrator and then I moved to Lehman College where I was actually a web programmer anyway so and then I moved to Baruch College where I was assist that mean and also the web programmer and you know so there was a lot of learning curve as as time moved on so for most of you I know you are a fresh CSE major so it's someday you will be also becoming like what I've been doing and by anyway after that I moved to CUNY CIS which is the main branch of can like you know CUNY means where we have control all the CUNY campuses and and then I moved to a Bronx Community College by the way in CIS I was actually be a business intelligence architect and then you know as Bronx Community College right now I'm in enterprise application developer so I do have a background on dealing with Windows unique systems and also as an admin as a DD DBA as a programmer I heavily work on Java and recently my new job I am working on the.net industry so again some of this may be passwords for you what I mean the buzzwords means it might not be too much familiar with this terminology but hang in there within two three years you know you will get used to with this terminology and of course this this lecture will may not help you today but I think it will bring some kind of excitement for those of you who are bored with Ollivier CSE lectures hopefully that will bring you an encouragement look I need to focus on this and today's topic is again is the remaining but the whole reason I'm showing you I am somebody who comes from a very broad perspective meaning I don't just start started my career as a data miner or data analytics or data science as a matter of fact I'm not data science as you can see I am saying enterprise app developer but since I have been in the industry since 2001 as a matter of fact I'm also teaching since 2001 almost constantly and these are the campuses that I taught and again from Bangladesh these this is a LaGuardia Community College that's where I started my career teaching career that's what I started my university degrees at New York or in America and then you know I went to Binghamton for bachelor's master's for Brooklyn College and pays anybody this is not a CUNY campus this is a private campus that's what I did my PhD and as you can see I also know showed you the Bangladesh background where I went for a school and I moved to this country 1996 and I was born in 1976 so anyway having said that and these are the campuses Baruch College York College and in another program Baruch continuing at Pace University interviewing with University these are the campuses that I talked and I'm also a research adviser in my campus where I did my PhD so I do have PhD students that is doing PhD under me so I am hoping that this will be helpful for you so anyway but keeping that aside hopefully that you you know get to learn few things and again this is a introductory lecture and I was discussing with Cali Bandera earlier that this is a three hour lecture so my goal is to try to finish in half an hour so please don't get upset because some of the stuff again you might not understand anyway because there is a prereq I mean there is some other things that you need to understand to understand really what is data mining so it's a fifteen to twenty years career that somebody needs to really want to be a data scientist that they need to go through to understand all the concept and terminology but anyway fields are changing constantly so maybe in a few years you might be a data scientist as well very easily if you focus on those topics so these are your topics that came along data mining cyber security and out of this it seemed like this was a discussion that I had with Kelly by the way those of you don't know believe is my nephew so I was discussing with him about this and he is a CSE major in national diversity and by the way in case if you don't know National University is the national diversity combined with Open University is the third largest university system in the world so you guys should be proud that what you're doing is not something small actually so we are targeting more than six hundred thousand students okay so you're grading false light right under actually Caltech the University of California here and you know we CUNY is also not small we have we served more than half a million students so it's very close to you but you know it's still it's not as much as you guys have so anyway so then it can so next week hopefully I'll try to cover on cyber security then he came at development and hacking by the way hacking is not a job it's a kind of like a profile so anyway I just kind of took the notes what I was having discussion with him but you are more than welcome to actually send me your choices so I will send in my email address so feel free to you know email me if you have any questions or anything but again you can put it in the chat hopefully you can see my email address and but in the email please do so put that like you know Bangladesh lecture or something in the subject area so that way I know it's coming from you guys because I get lots of lots of emails so he's don't like you know I think that I'm ignoring you or I could not see you because do apply and last semester I had hundred twenty students so it is something gets very difficult to handle manage things so anyway so these are your topics initial topics that came along and I was discussing with Ghalib as well data science and cybersecurity is the most hot cake right now in the market that doesn't mean that the other jobs now are gone or doesn't have anymore anytime any job is there there will be always some maintaining going on so even you know you don't you don't have something about mainframe or COBOL stuff that we guys it upon or upon my friend knows we took those classes that industry is like you know legacy system now meaning like you know not that many jobs out there anymore but still so all of this job there market is very hot but especially cybersecurity and data mining is more popular today I keep saying the remaining is data science of the directrix so anyway and all of this had actually zip you know you would have seen if I was showing the slideshow but anyway maybe next time so now if you notice I do put my actual lecture meaning actual class where I teach at Baruch College I do teach a course called data mining for business analytics and this is actually a revised version of the first lecture that I go through with my students so this is both a undergrad bachelor's or master's class similar lecture set up so 9:20 is the lecture and by the way this is not only just computer science or CIS background they also offer this course in stat program so that should give you an understanding that only computer science or see major is not the one who is dealing with data science you know science is coming from all different backgrounds you would be seeing doctor someday doing this data science work as a matter of fact they are doing ok so a lot of times it's a team collaboration work it's not a one-person work and by the way don't think like oh maybe I'll have my team made do all the work first of all you have to get hired and prove them that you really capable of doing things that you are claiming that like you know so anyway so back to the lecture so let me just show you the syllabus little bit and this is the syllabus that I've been teaching and I will be teaching this you know next semester in fall again so this is from last year syllabus so let me just keep showing you the syllabus on a very high level I don't want to show you everything as you can see this is a long syllabus so on the reading there is books but like you know this is a book that from Stanford yeah Trevor no this is a very complicated book it's mostly focusing on stats so we covered this book and but also we talked about these are the two better books you know that kind of gives them and these are more Theory theoretical and the required programming for this course when I'm teaching is our programming sometimes I give them an option also do Python by the way Python is very hot in data mining or data science background as a matter of fact same in cybersecurity if you have to program usually people prefer using Python but again R is also statistical analysis program so the reason the highlight is are here someone who's graduating with the deny electrics track and by the way the people that's taking this course there it would be this is not a CS imager this is more like computer science or you can say computer information systems but their concentration is on data analytics so it's a data analytics background we I don't think Bangladesh has it you only all of you of you you're graduating as like it just CSC okay and I was also I looked at your at your syllabus or one or two year prior syllabus where you do have this course in your master's program the course that I'm talking about so anyway the more interesting things are and also from that third book that I was mentioning from Stanford it is a very well known book and they do have all of their lectures in YouTube and also their you know our programming assignments are assigned from that book the project I will talk about it again and then there is I have the informations how you can download the hours and stuff and by the way like I said don't like you know feel like you need to take notes or anything you know because if anyone is interested on any of this I can email you so just let me know so anyway there is a different as you can see I post the links I when I'm teaching I prefer students to grab information from the outside meaning most of the time from the web yes if I mean I will share with you later this is actually the syllabus for the course and what I wanted to put the students that you can see the goal here after they finish this course they should be able to heat the market as a data analytics person so that's why you can see there is an assignments that's been assigned there is also clear evaluation meaning each student is evaluating the other students contribution for the course or for the projects and of course there is exams as as usual there is also assignments and of course assignments so this is just to show you that how me as a person are usually these are general actually my other colleagues in the US they're teaching a course like this okay so having said that I think that's and the other thing that I wanted to show you is the day to day schedule and this is just a grading so anyway so as you can see usually week one I would be covering the lecture that I'm talking to you as well I said again your lecture is a little modified and I do talk a little bit about our programming and then most of the stuff this lecture is taken from these two books 10 and 10 and then it goes on as you can see we have 15 weeks last week is the you know that final but you know these are the different topics that we discuss based on the data mining again don't misunderstand this is not a machine learning course this is not a deep learning course this is a data mining so we are mostly playing with data here ok there is a differences as you if someone coming with the machine learning background or did a deep learning background they will understand completely what it is but just to give you in one sentence machine learning deep learning mostly deals with images and videos or you know voices here we are mostly dealing with the text or you can save and actually kind of data based concept so anyway that goes mostly on this syllabus I think I'm actually already getting out of running out of time please do let me know if anyone has to leave because I am only in the fifth lecture so anyway in then this lecture outline I had all this so I already showed you how the syllabus looks like and then at the first day or actually before we even first day and this is an outline of this lecture that I am covering today so I have instead of let me just go over what I do we already talked about the syllabus I have students actually give me a these days I do a Google survey so before even they start the class they kind of give me all this information of course their name and what are the course in your case is to CSE course they have taken their math background the statistic background if they have any ITR working background and if they have any details of how proficiency and how many classes they registered for semester you know in US students has the flexibility unless they're full-time there is a minimum and Max they can register for and everybody's goal is to they want to graduate in maybe six months of course it's a four-year degree so you know they take a lot of classes and then then later they suffer so anyway and then I asked them how many hours they want to devote for this class and then what is that expectation from me and then anyway and also I asked them if they have any tips for me this is day one okay and what's their future goal and if they're interested doing future future research this is a bit brand new field just came to the market a few years back so when I was a student this field didn't exist okay so you know that's why there are lots and lots of research going on so and this is the highlight of the whole course as you can see I am you know the introduction is in the first lecture which is not even here I am showing actually saris here so one two sometimes I cannot finish our and all that stuff in week one so we go a little further and then I also have to talk about data processing this is a very important concept to understand for a machine learning analytics or data science person to how to process their data so this doesn't even cover in this course okay this is the prereq meaning they have another course I again that I also could be teaching we call that the 30s he is 31 20 I it's called business Python for business and I think so it's it's a very you know advanced level Python course where basically I'll give you a few sentence overview what it is somebody gave you a market analysis like say you have in Dhaka you want to find out what is the best product that's been sold in different stores okay on its different supermarket so I'm not very familiar with taka so I cannot give very specific examples that good stores are good department store so I will take the US Army as an example so like say Macy's what is the hot cake that's going on all over the US in Macy's okay so to understand something like that now when you get the good data from the customer the data is in very raw format customer doesn't understand how really that data should be formatted so they give you some kind of we call it garbage okay so now you have to process that data to bring a meaningful format that could be analyzed for data mining data analytics data science whatever you talking so there is a prereq data processing that has to happen before you even try to do it data mining or data analytics or data science whatever you know which level you wanna file by the way they all come to the same channel and also for CSC students that you are joining before you can data processing you have to understand what is data so the good news is you guys have a course called data base where they teach you what is database how to create the data and stuff and again I know for report this is a piece of cake so you know you know what I'm talking about and I'm sure most of it is you may be familiar always always and by the way the person report that I keep mentioning he's a friend of mine so he's joined also from haka you know and he said maybe anyway he has 20 years of professional IT background like the way I do so you know you can and he's a rich guy sometimes maybe you can get anyway so so these are the some of the topics after we talk about data processing I just give little idea to the students about what is data processing but again this is covers in another course so we don't talk about too much about it and later towards the end I will show you what is the data analytics track that we cover in data in Borough College and then of course towards the end you will see that some things that I'll talk about the job market as well so then we directly go into the data mining or data yes these are data mining techniques you know classification and here as you can see I do talk about a little bit about the concept there is decision trees there is maybes algorithm there's K nearest neighbor there is neural networks then when we talk about the clustering we talk about key means we talk again these are buzzwords for you so don't please get confused or like you know get thinking like what am i that I don't have any clue what's going on and that's fine no you know you know you would if you were someone who is going into this field this is you will be talking about or hearing about them on a daily basis so anyway then we talked about the Association just to give you a highlight about the Association so associations means you're talking about associative it actually comes later so you like say when you go to the store like say you want to buy a pencil for example and what is comes after the pencil you want to write to a pet for example or if I check in the computer you buying a laptop you know I mean I knew I know with the these days computer comes with BGA's and all that stuff you know so after that maybe you want to buy an external mic you know CD Drive or whatever with that so this association what it does actually it kind of tries to understand the customer behavior what should be next to each other okay so as a data miner or data science or data analytics person mostly is data sign they kind of come up with the prediction algorithm analysis that okay now that you have put the laptops maybe put the cell phone next to it someone who is attracted to a laptop most likely will buy a cell phone as well so that's just to give you an highlight idea how this works okay and then of course all of these goes into the text mining or social network analysis so now of course what is this so text mining could be something like say you know now you go into like Google I mean not Google sorry the Twitter or Facebook and you're trying to understand like say you have friends leaks how many of you how many of your friends like what you putting in there I mean of course you can go them individually but like say if you're putting thousands of cost every day individually sorting them out is not very easy okay so you can have a program do all these analysis and do the rating for you okay let's say you think can think of Shakib Al Hasan for example he's we have a lot of friends like the whole Bangladesh is his fan right so now he wants to find out like you know what is his fan thinking about that he's been kicked out of cricket now right so right that he can be done or someone a data scientist can do this analysis for him and say look saqib when you come back two years or three years down the line this is what you should do and then you will get the same field as you had before you had all this problem right so that's kind of like you know a meaning you are analyzing the social network data set or it doesn't it could be also a business right even going back to me sees me sees can analyze and try to understand that hey you know what is our customer thinking about it so all those stuff so okay all right so going back down again if anybody of any of you have any question please put it in the chat so for since I mean I already I think took the half-an-hour time so I think I'm gonna how many of you are interested to go have me go over on these data mining lectures otherwise I will directly go in let me go back to the the lecture outline so so far I just gave you actually I haven't even covered this okay so later I was gonna go a deep overview of all these topics slightly what it is again this was the first lecture um yes I mean going back upon you in the chat you said political reading that yeah that's a very good example yes that's correct the Republican versus Democrat yes you know as a matter of fact Obama got elected using the data science technology in 2008 so anyway and Trump is also using that heavily as all the political figures in this country now they use this data science IV as a matter of fact New Yorker una is also heavily being used by these data scientists now so that's how they're tracking so anyway what I might do I met because this is would be a long lecture so how many people are interested have me talk about any of this because again I know you know maybe some of you are busy with you have to deal with something else is just the pond is the one who is putting up stuff in the chat line so how many of you interested me talking about this topic okay all right good all right so hopefully you have time minimum it's gonna be another half an hour if I go reading is anyone else okay well I see okay the phone is saying okay all right so I will try to be quick then so okay all right so you know you're kind of hard we talking about why data mining and repaired as well so you know there is some of that stuff like you know the data is increasing it's a huge if you go to youtube just check the data warehouse for Facebook they're adding servers every day okay even though as an end user we see that like wow Facebook imagine billions of users using this tool every day right so how much data they need to store and they don't delete anything even though you are the one who thinks that oh you posted something bad or you didn't want that post to be shown you deleted it it doesn't go away from Facebook or anything you've made a click as soon as you made the click it that's it okay and guess what it's a billion dollar industry you you and I don't pay a single penny for it why do you think that they're getting the money from right Mark Zuckerberg who is the like you know see you and the owner of this Facebook he's a billionaire right he's not only him all of his associates are be linear so where is he getting all this money from advertisements okay and there was a time I was looking at like you know and as are we actually how much they make out of you know each customer are each a Facebook user and that their most customer attention is in the US okay I think it was that in 2018 when I checked it was $79 for us and for India it was 70 or 80 cents Bangladesh was not in the list at that time so I'm sure now the new data will show Bangladesh as well so it's not that high for Bangladesh or India because I don't think that many people really in Bangladesh take that vert iseman from the Facebook and buy things but in the US it's very big so anyway that's the whole reason anything you do in facebook or any social media it doesn't go away so anyway so now and Facebook has an API which is collabing application programming interfaces you can extract their data and do analysis on that data so they let you collect the data okay so anyway and as you know there's a fight going on between Trump and Facebook because a lot of people are saying this is not ethical that Trump is bringing hatred and all this using Facebook so you know they at the end they're looking into the business so because Tom do have a lot of followers and they end up making more money using his you know personality so anyway so here petabytes terabytes hopefully you are familiar if you're not familiar with it you're familiar with gigabytes so this is much higher than that so you know we collect the data and of course make sure the data is available and image the data sources mainly you have a business in the science and of course the society basically everyone is hungry about data okay for example in Bangladesh you know Corona is a like right now is a pandemic going on even and in the US as well you know in New York we are better in better shape right now just if I go back even I was every day I was checking try to find out what's happening with the corona or when you're checking the news you actually are looking for data okay so any everyone in anyone you know if you think of yourself like say you want to go to a school today you want to find out how the like you know condition of the country like so for example right oh you want to buy something you want to find out how the market is right so if you think about in deeper terminology we all are hungry for data and at the end this is all about data okay so and that's what he says we are drowning in data but starving for knowledge now of course most of the cases the data we are seeing to in front of us these are free sauce pre-processed data okay so that means these data has been processed for us so within that humungous data now I want to find out the data that I need okay for example if I'm in the ocean I am trying to like you know grab some fish and some specific fish like say for example you know group gender for example is if I take a Bangladeshi fish or a leash right usually huge comes from mother pada right but anyway so think of emotion and there is a reason I'm giving this that example in an ocean now you are looking for some specific or like said treasure there is some hidden treasure that you want to find out now how do you know where that treasure is you might be spending your entire life 20 30 50 years trying to find out that treasure and still you might not be successful right because you don't know what this data is but you know in a data base world we try to find out technique and that's what the data mining is actually helping you these are the different techniques if their data mining or data analytics and uncle again these are different tracks and usually data science is the one who kind of the big guy who knows how to do data mining who knows how to do data analytics and all that stuff so anyway so try it basically we are trying to find out the specific knowledge that I want to acquire acquire based on that data set okay again you can take a newspaper and as an example you try to look at a news news paper you're only interested on the corona concept of the in the newspaper now you have to go through every single pages try to see where the corona is being talked about right however if they have a specific section like say page three column one only focuses on Corona and next day you go you see oh faith page three column one and there is so that's the knowledge I am trying to acquire from my in this case so you know and again when we are talking about data again if you go into the Facebook world it's Google world it's humongous okay so not everything is data mining so if I'm trying to find out someone in someone's information that's not a data mining because it's already in the directory I am NOT someone who has to do a lot of work for it and trying to find out a model a build a model or try to do something so for example I have it here what is not data mining and what is data mining okay so you know now if I'm looking for data in a specific location that could be a data mining or if I'm trying to sort out some specific documents trying to understand its outcome that could be data mining okay just querying Amazon Amazon in the Google it's not a data mining okay so there is a distinction and where does the data mining comes from again as I said these data didn't this field didn't exist back in 2011 I would say 2010 okay this is fairly new however don't think that this concept didn't exist you saw that this is coming from stacks and the Stanford that I was mentioning that book they have they have been teaching this course in 70s so imagine so don't get frustrated and I was sharing this with Ghalib that's something you learn today maybe 20 years down the line it will be useless okay I'm sure Rippon will agree with me there are certain things and that we did in college back in 97 98 we still use them okay yes most of the stuff is useless we did we are not using majority of the stuff you're gonna learn you may not use that but that might say he is agreeing with me okay so you know it's you might you know the problem is you don't know what's useful what's gonna be useless and it's same for us and again he will agree with you it's good that we have been thank you very country pond for having me I mean having a painting this because that way this guys can really understand the differences it's not me only who is talking so you know field is changing but the concept is not so basically we are read kind of revisiting the same concept in different ways okay and the field is moving based on the market demand okay Facebook came in the world and there is people okay now another person just joined so anyway well coming up thank you for joining you didn't miss much so it's an introductory lecture so anyway this is surrounded as you can see we are talking about here data mining is in the middle between stats machine learning pattern recognition and of course down there is that bit of database systems now again there is another thing also that comes this is this is the general outline for be a data miner or determining person however it's also field specific so knowing the business is very important so for example if you work for pharmaceutical pharmaceutical company you might think this is the pharmacists job right you understand what is the medicine and all that stuff but if you're someone who is data mining or data science or data analytics person for that field you need to have some level of business knowledge on that field as well okay so that's why II as I said earlier the field is moving in different directions as a me as you saw my in my like you know when I was sharing with you I'm someone who comes from the education background you talk you tell me about registration system about CUNY I can tell you in and out maybe from you guys like you know where you are in Bangladesh it seems simple straightforward but since we as the IT person we have to support the campuses or the colleges there are different ways that things happen and these are already pre structured and it's all happens online so basically we as the IT people has to build all these business logics and the better I understand the business there is business people who handles the business but the better I understand the business logics it's much easier for me to build the business I mean to build the programming logic for it so now for those of you you know CSE major that you're joining you might be thinking like where am I going to learn this business logic that means you have to do the work for that industry so it will come after you started the working it's not gonna come in a year as my as you go along you're gonna understand and to get better on it okay so hang in there maybe some of you might be already frustrated that what am i learning what am i doing but you are really in a great field because everything is becoming computerized you know so you have to take this as a challenge and most important is passion the same way you guys are crazy about video games the same way you have to be crazy with something whatever it's cybersecurity or it could be a data mining it could be game development whatever it is you have to have that thirst or a passion or have some kind of like you know motivation that I want to learn this and I want to learn because I want to do X Y Z okay and again Rippon might agree with me when we were sitting back in 97 98 in campuses in our computer science courses a lot of times we were bored we had no clue what we are doing right you know because we were totally lost like said and I didn't still remember a lot of classes I was following in sleep and I'm sure you may be going through the same problem because you've been thrown into song as he's I agree with thank you and I'm sure you all can see in the chat that he's because he's the someone I started college together here in back in 97 okay so anyway yes data science there is a big learning curve if someone you are studying as a fresh you need to actually learn from a different perspective and if I were you and if those of you who want to be a data scientist your focus should be on in a specific field and nowadays Google gives you most of the answer even for my father is a heart patient or my mother she's a diabetic patient a lot of times when doctor is saying me certain things I even see doctors googling actually for different things okay because thus operations or the procedures they used to do 10-15 years ago it's not done the same way because now the doctors has to upgrade himself or herself in front of me actually I see doctor googling all this technology so Google is a big big thing that will help you all along he's been helping us me like you know everyone so you you will have to get used to and when you any of you I mean I don't you're not my none of you my student but if any of my student is taking I have a nickname called mr. Google and the whole reason is I try to push it so much that you know sometime I don't answer questions I say go check in the Google because you know and I have instructors from directly from Google and I have seen them in the class as well they were actually googling there for information so anyway so there is nothing about memorization here or anywhere in the in our computing world it's a practice the more we practice and anon please feel free to actually you know as a report is being very active masha'Allah so for these guys as you can I think I think you can see we have six students from Bangladesh joined actually so motivate them or try to keep them an encouragement like you know hopefully that they don't they're not bored with their background so anyway going back here so we are dealing with data but when we have to understand the data we have to understand these techniques okay and your remaining is one of the subject that helps you to you know understand that so there there is the whole process as you can see this is the data and again by the way when you are seeing the Facebook you are seeing the user interface of the Facebook okay or Twitter or anything that you're saying you're seeing a user programming interface or even if your game development okay in the back behind the same that this game is being recorded every move the move that you are taking and they can analyze on this day game you know and based on that they users like you know excuse me feedback they can actually really design their next game what could be the hot cake in the market so anyway so this is all the database back-end and then as you can see the first thing is the data cleaning and as I said we cover another course for that doing the how the data cleaning has to happen so anyone someone like you in Baruch College taking my class as an undergrad or graduate they kind of come as a fresh meaning might not have any computer background someone like save you guys okay so we kind of prepared them based on background you know so anyway they have to take the courses and at the end do they become an expert no not at all but they do have knowledge okay how things can be handled the expertise comes once you start playing with that so matter thank you and I'm for like you know pitching in as you can see he just he said sure so anyway so as you more do work on the industry or as you more play with these things you don't have to work in industry you can see that you're home and start playing as a different projects as a matter of fact towards the end of this lecture I will show you certain things that will help you hopefully how you can be targeting yourself five four five years down the line when you graduate and you are ready to actually become an at least data analytics person even if it's not a data scientist okay so anyway so here you can see data cleaning and integration happens and the data warehouse by the way there's another course also that I teach at Baruch you know last semester I had sixty five sixty one students for this course this was a graduate master's course I also teach that in undergrad so that's another database course so that's for data analytics as well then you know after all this happens that selection happens then you see this past relevant what it means remember our larger I was talking about the medicine education so depending on what tasks that you are interested on that's what we mean mean by tasks related ok then now you started doing the data mine okay see and then you try to understand the pattern see this is a chart right you're trying to come some make sense an earlier recon mention about a Democratic and Republican for those of you who are familiar or not familiar it's a it's an election year for us a big thing for us because it come comes where we are I will maybe some of you as well but hopefully we'll have some really but anyway so at the end I get that the knowledge that I am interested in okay so hopefully the lecture that I was trying to explain earlier that kind of goes into it so see I'm now giving an example of web mining how does it work you have a data cleaning integration so we actually it's always kind of same the same chart that I just showed you I mean you might not be doing data warehouse but it's kind of a similar channel in general okay now it will put it into a specific field which is oil mining being talked here okay you can analyze data based on Amazon you can do it I mean Google Analytics spam mail filtering this is a cyber security field okay by the way you do also data mining on cyber security so eventually it's someone who has a data science background they can be a good cyber security expert as well our cyber security needs anyway so by the way also I'm trying to have one of my friend that or one of my colleague which is a cyber security expert maybe join next week he might be giving a short lecture for you guys to really talk about real someone real from the industry who has the background to talk about it so anyway we'll discuss next class about that so same pattern okay what you saw in the previous slide is the same cleaning the data integrating from the different sources again this is important because you know you might be collecting data from mobile I'm just giving a general example you could be derelict collecting data from website you could be collecting data from someone's laptop PC so you know or you know you may mean different sources of data coming from so that needs to be integrated very well you can work to build the warehouse and get a cube create a cube representation to build a nice chart or analytics for and that's doing this selective data core data mining and then of course do that the remaining and then presentation the result and give the result to the end-user or whoever is looking for that data okay so now business intelligence another field as I said in the CUNY CIS this is a that was my what I was doing expert expert case so similar see source coming from the different source processing exploration so behind the scene if you look at it it's the same course material they will be learning in the course now based on the business the data behavior or the pattern will be different okay hopefully I I was able to yes Ramon is just mentioned data Mart data Mart is actually a smaller branch of the data warehouse so you know it's it could be riddim art it could be a data warehouse so anyway and you can have you know data warehouse combination of multiple data Mart can be so that's again another course that we covered our complete course just on the so yes you you okay so here now as you can see the perspective from the machine learning and the statistics point of view how the data gets in into the dynamic data mining or data you know basically trying to I know in the ponies I think interested on understanding what is the machine learning so I meant to say earlier the similar concept that you might be learning in data mining most of it is being used in machine learning so it's a similar concept the datum set is different okay so see so we are doing you know integrating data normalization feature selection dimension reduction and you know so and this is a medical data example so I'm gonna go a little quick because of just again just to give you an high-level understanding of how it would be for an industry would see this and again this is the multi-dimensional view of the data mining you know that it's so it goes big and again this was the first lecture so basically I kind of give a whole overview of the whole course so for people to understand that what is the expectation you know in the class so what kind of data out there this some of it is get covered in your data base courses that you guys will be taking so like you know it's just talking about different levels of data so for those of you who are interested I'll share this light with you so hopefully you don't lose anything if you interested we can talk more detail into it so it's you know these are now we coming into that different functionality of the data mining you know the generalization here we are doing some kind of generalize the data set and again we go more deep into it as the course goes and then Association I did talk a little bit about the Association and the correlation again the whole idea behind is you're trying to see what items are frequently purchased together in your Walmart Walmart is Astoria right so you know then you know I don't like this example but anyway I just took it I should have changed it so if somebody's buying a diaper versus the beer comes next to each other so you know again there is a concept behind these things so that's what we cover and then the classification these are different kind of again algorithm or technique that we apply for data miners okay as a data miner see you have like you know see correct example classified countries based on climate classified cars based on the gas mileage all right so you can do use this in the classification these are some of the examples and the different methods that we apply decision trees nay Bayesian classification support vector machines neural networks rule based classification pattern based recognition logistic regression so again some of these gets used in machine learning okay all of this you have it in machine learning it's the same concept but the usage is little different okay and of course can it cut for detection or direct-marketing classifying the starts diseases Eve you can go on plastic cluster analysis I didn't want to go too deep oh it's right there is different kinds of data supervised learning versus unsupervised learning this is the data that we don't know actually ahead of time what kind of data we are dealing with so that's when we do cluster analysis okay so you know kind of put them in different kind of classes so you know for example take Corona is an example you you want to find out where people have been heavily affected and then based on that criteria as a matter of fact they are doing it right tracer in New York we have the tracer program they know where you know kind of this Corona has been heavily affected okay so anyway that's another method and then outlier basically sometimes you might have issues because when you are dealing with data some of these data okay thank you for sharing the ATL that's actually we covered in the okay yes Co beard 19 project yeah so see there you know you guys are lucky that you have to expertise here that they are sharing so check on them you know Chaplin he's they're sharing things from for you guys so you know ETL since you just mentioned sorry for backtracking ETL is not actually determining just just to give you a better conceptual understanding like you know that covers in actually data warehouse again I'm after the lecture hopefully you'll have a distinction understanding what is the differences between them so ETL it stands for extract transform load so that's basically when you're loading the data to a warehouse okay so that happens before the data mining even comes into play so once you load the data warehouse or data server wherever you are loading that data then you extract some data set out of that to to your data mining and data mining is mostly creating models okay and data warehouse is mostly trying to come up with analytics decisions to try to understand understand like you know behavior of the people or basically you know you are analyzing your historical data to understand but at the end kind of understanding is a similar mindset that they are trying to understand the business or they're trying to understand the for for future or what could be the future of like you know corporation or the organization so I know it's a lot of information so that's why it's some time it could be confusing where we go things goes but like you know if you are studying this more frequently you can very easily better understand what's going on so anyway going back to outlier outlier is actually something where you have somebody totally out of you know and again repair upon shared actually where the Quebec you know thanks guys you're actually helping me for next semester as well so so outlier is something like it's totally out of the blue so anyway let me give you what happens once we do a data mining project or create a model doing the data mining techniques that you just I just showed you as a very high level so at the end you come up with a result okay so now that results comes up with a percentage so and there is always there's gonna be error so based on your model maturity I call it maturity that you could say 80% correct or 90% correct now or you could say 99% correct which is very unlikely usually 80% correct or using say that 80% will fit 20% will come error that you could say more or less yes my model is kind of well will work in here and a lot of times when we look at the data we might not understand how what kind of technique we should apply you could be spending months and years trying to analyze the data and coming up with a solid model as a matter of fact the research that I did YouTube watch next video so basically what YouTube does and it's it's an actually like you know it's a data analysis project so anytime you're watching something it kind of tries to put it in a model and trying to give you what would be your next choice it's a again it's a prediction all right and I still nowadays when I'm looking into I'm doing I play with it some time because that was my PhD research project or part of it so still they have not matured that product meaning a lot of times if I'm watching on something like say Corona something else will pop up okay so it still there is a maturity that's missing on this project okay so if any of you are interested hopefully you understood what I was talking YouTube when you go to YouTube you try to watch something you do a search and then YouTube keeps the other recommendations and then those recommendations is actually based off a model that's being built okay and that's an example of it a good example of a data model as well so outlier is something that way of now of course if you keep this outlier that's gonna reduce your error right okay so now like say most of your data is in one location now this is something I'm gonna use that like you know maybe annum as an example I am doing this in New York most of your in Bangladesh actually me and annum is kind of like an outlier in this case okay we I'm in the New York City and he's in Buffalo all right so we are way far up now somebody is analyzing mine and lecture that how many people attended and they want to understand based on the demographic so me and UNAM will be an outlier hopefully that makes sense okay and a lot of times you have to get rid of this outlier because otherwise you will have wrong you know output our error rate will be increasing and similar noise is similar it's like you know so like now I'm running the lecture if any of you had your mic on and there was background noise going on so it's a similar concept okay so anyway hopefully that and it you will see this if you are doing a data mining or data science this is quite common okay and time is a very important factor and again I apologize that I thought it would be I would be done in half an hour but I we crossed more than an hour and zoom didn't kick me out because you know I recorded I was hoping that I would be done in half an hour anyway so we still on so there are lots of data analysis that happens based on the time so you know this is kind of like you know an example of giving you an a sequential pattern and our different evolution analysis so basically the example of doing a data analysis or data science using time as a factor okay this is not only for data science and sheer opponent mmm will agree with me there are lots and lots of programming that we do based on the time because there is always a factor that we have to take into consideration so at this moment if I'm looking for something where's that moment coming from and a lot of times we can do a trick we use the system time okay the computer where the see yeah the pond is agreeing with me so you take actually the system time of that moment so as a user we get fooled we say wow they did something for me instant meaning like you know and this moment whatever I did that became a reality right behind the same we are using the system time actually which is your system time so basically I'm in the u.s. or enemies in the US and you know we are saying Sunday morning or actually it's right now and you were saying Sunday night or evening right so you know it pulls you anyway so here is some examples like you know you can look in the pattern like when first they bought a digital camera then going into SD cards and blah blah blah and see some other examples as well and here see biological analysis sequence analysis this is important as a matter of fact this will become a hot cake once the quantum hits the quantum computing that's another world I did leave that little research on that as well so once the quantum computing comes you will see there is a huge difference on the computing work because we cannot really analyze our gene we cannot analyze like this corona this might have been a piece of cake if the quantum computer was a stable machine okay because computing has lots of limitation these days I mean it still is quite impossible and as I mention about Facebook see so there is a graph mining right this is again graph mining our graph database is something that's you know like for example in facebook friend suggestion okay so how do they suggest that phrase in to you this suggestion because they kind of do a graph analysis sees that you are connected like say for example me or mm or Callie or those people who know me you know our report if they're my friend in my facebook and they have friends that I'm not friend with so behind the scene they have the data mining technique that's being applied that will automatically actually like you know work on this and then give me the suggestion so anyway there is some other stuff oops sorry I see some people and I will skip the evaluation part then this is as you can see what are the disciplines that it affects a data mining again data mining and machine learning is two different field okay and of course as you can see they kind of heat in everywhere okay so you know and of course it's not these are mostly the computing field that we are talking about okay so you know this is where data mining also gets into effect and I think I can skip it why applications of data mining ok again once I share with you the slides you can look more into it and again for some of you this might some of it you might be not familiar and some of it is you might be familiar which is ok so I think I'm gonna skip few slides and directly going to the project that I hear ok so this is the project that I assigned student this is a semester long project ok so first they have to come up with you know why they are doing this project so for that of course few classes has been passed already so they kind of have an understanding a little bit of understanding what is expected so again after finishing this lecture I don't expect you any of you to complete this project because again you have to really educate yourself to understand little deeper on the theoretical component so once you have the understanding of better on the theory then of course you will be like you know in for example like what is it linear regression what is a logistic regression what is text analysis what is a Association so then you will be able to have a better understanding like you know what is the project that I'm looking for because what happens you that you the data that you will be choosing for doing this project this is where the trick is comes from that data plays a big important role because you might be doing a regression regression analysis but you did an unsupervised dataset of course that's not going to work ok because that's a linear data that's required to do that analogy so anyway so after you take the data and again I don't help them much with this they're completely on their own so again those of you who plan to do these things you would be able to do it yourself as well I already taught this course to turn so they kind of have an understanding of how what is the outcome that comes on at the end it comes out great so anyway they might be picking it did a large dataset but remember this model building model takes a while to build so that's why I take tell them to take in a small data set so first do the pre-processing how to understand the data and again they take the data from the internet okay I will show you later a few places that where you can play with the data if you want to okay after the pre-processing the data again as you can see they are reducing the data to fit for the model okay and a few other things happens the transformation can happen they can create new data set and of course eliminate unnecessary data set so for that you really need a database little bit of database background understand what things to happen and again that other course that I mentioned as well okay so and then you come up with what are the techniques that you can apply and in my course when I'm teaching this the goal I mean they have to they have to actually do minimum three and of course three from different varieties so again as you can see I'm kind of forcing them to learn things some of them don't like like it why because I'm making them do the work right but they appreciate once they go into the field and like you know say that professor you did a great job by forcing us like you know teaching us those things and again I'm not teaching them it's they're learning themselves so anyway so these are different techniques and again of course they do a presentation multiple presentations happens in the class at the beginning they're totally confused they have no idea what to do and you know so ant well then they become an expert sometime I learned things from them as well so and then of course they given a result at the end and they don't deploy it in a real world because we don't have a real life problem but this is how it would come at the end this model will be applied so again the difference between dynamicists and mining or data science I would say in their analysis you kind of take the data raw data and create a build a warehouse through an ETL and then you actually build that it like you know recall the different charts and diagrams dashboards stuff like that but in a data mining world you create a model model for it or in your data science basically we say we're machine learning it could be deep learning you know you create the model okay so this is a three to four months project they do finish on time and at the end they come up with a report actually so it's a 15 some time it could be 30 page long report they have to actually give me all the sequences that they went through okay so sorry so I think I will kind of keep most of the rest especially because I want to show you what is required for you folks in Bangladesh so but just to give you an understanding when you are applying for jobs you know you you will see if you're applying as a data science or data mining or data analytics you will see this terminology okay so at the beginning I was also confused when say you have to have it like you know knowledge about prediction methods or description methods so you have to have a some level of distinction what do they really mean okay so and of course the more you study on this like you know you can really understand any as you can see this is from 1996 so anyway so say I also highlighted remember I was just mentioning about regression logistic regression linear regressions so these are actually predictive see classification regressions these are predictive models and then clustering Association rules sequential pattern or deviation detections these are predictive models so again they're these are actual models that you know understand the theoretical component and here I'm talking about the regression this directly comes from stats or statistics all right so you know it's um as I said in statistics it's been there since 70s and I mentioned that like you know you're there the stand for they are teaching about it that course YouTube lectures so and again this is a classification and giving any high-level examples and stuff so I'm gonna go very quick on this this is actually I think leading into a decision tree model so you know what since we are already here so again this is going deep into the course this is since in the first lecture I kind of give them an under quite understanding what is happening in the width of course so these are different as you can see classification models and applications that I'm trying to show them even class angle it Alexis right you know this would be a good one for our ashram okay so in one of the Facebook post I see he doesn't believe into these data signs anyway that's his problem so there is like posturing again this is cover towards the end of the semester because this is a little more tricky and you know as I'm showing here the clustering how the clustering is shown okay so anyway so here more you know again these are real problems don't think that just just it's a part of theory it's there is people benefiting big big time out of these things so ten years ago this could have been maybe I would say a dream but these are now reality okay and like I said in you know in New York City I can say for New York City most of the campuses they are laying off professors because they don't have enough enrollment and Baruch is completely opposite you know why because they do have both cybersecurity and this data analytics track and in both of my classes I'm just praying that have less student that I had to deal with party would have done has forty forty meaning eighty students I have to deal with next semester and of course I don't like it so anyway but that's the reality because too many students weren't really able to teach much so anyway sorry it's been recorded hopefully no one from Berube's listened so it's as you can see this is the stock data that's doing a clustering analysis again this is very high level and this is about Association discovery rule about doing you know market store pattern see bread coke milk stuff that you can have and then how that rules get applied so I'm I'm trying to give you a very small high level because we spent the whole semester on these things and again the real and even I talked after the class student after and at the end of the semester I take feedbacks and they say professor when we are you are talking we are completely lost so I know some of you might be lost as well but they say we really learn it when we do it ourselves so that's there we go so I'll give you the secret as well okay so you have to actually make your hands dirty that's what I call them alright so here is different kinds of sequential patterns that's happening and again this is different anomaly meaning trying to understand this is actually helpful for proud meanings where things can go wrong and I remember I was mentioning this is also for cyber security okay so that's where you're looking for some kind of fraud detections okay as an example I could say it's a pattern actually so if I am somebody who is most of the time shopping from New York City all of a sudden I go to France or Bangladesh they will block my credit card most of the cases and that happens actually you're supposed to let them notify them ahead of time that like you know you're traveling overseas otherwise you will be blocked and as matter of fact if I am drawing a big check as I had it I had a call actually this week Monday from a bank that I had to I said more than $50,000 that I had to draw a cheque for one of the project that I'm doing so anyway so and they call you to make sure that you are the right person so meaning that's already an anomaly meaning I'm not someone who is doing a frequent subtraction on the bank of 50,000 on a daily basis our stuff so this is a machine can detect this kind of anomalies easily so anyway I will as you can see the history of data mining it starts in in 1989 so this is not very few very new industry so even though we we it got into the market a little later so so anyways sorry about the background noise my mother and father is here to watch okay so Kali you are celebrating now because of you now they're interested too so anyway so this is and as I said this is a quite new field there are lots and lots of research going on so this is where I'm talking about the research and this is recommended and of course this is not the only one you have the free world google out there you can really really find all the information so my focus for Bangladeshi folks was here actually so as you can see I brought up that where the education is coming or what we are doing here so as you can see I have the links here as well I'm showing you Baruch then NYU data science program then columbia data science program then of course tony in currently this year the best twenty data science program in the u.s. that out there and again all of this was from google I did not got something secretly okay and in the data analytics bachelor's you know what's going on and stuff so anyway I will so anyway so in the Baroque College what do we do again this is where I teach so we have C there is general I hope you can see it right can someone just say yes if you can see the web page I think you can see it okay anyway I'm gonna continue all right thanks enough so anyway there is as you can see this is the general cs line this was there before so i don't need to go over this but the one that I just highlighted was this okay where that course oops so here as you can see the first course they are taking and of course they have other courses you know it's a hundred twenty credits and as you can see we have only nine twelve credits here so see as 3120 remember I was talking about data processing that's the core their teacher they take first when they come up when if they are claiming and they're coming into the data analytics truck okay the second course they take is the database that you guys also take in your CSC for Bangladesh okay and then the third course they are taking is this one which I'm just highlighted and the fourth course that UNAM was highlighting earlier about data that data warehousing so again I talked all of these courses actually so I kind of have the idea what they teach or what they do and then of course they take another three courses as an elective so as you can see programming is their you know semantics and I mean you know all kinds of data visualization is very very important for data but this is not a core course for them but basically anytime you are playing with data visualization is very important so usually they end up taking this course more or less and as you can see there is that course as well if they want to understand more about the regressions and stuff and there is marketing course as well see so these are some of the electives that they have to take and I think I will leave for the cyber security one for the next course I mean for the next lecture but this is what we do at guru college and this is same thing that you're seeing for the undergrad grad is same girl means master's almost similar okay totally identical so I'm finding this so what is NYU does this is a completely data science and by the way this is Desna started yeah C 2020 they just started this program okay actually five years ago or even two years ago we didn't think that undergrad is capable of handling data Sciences work but industry is changing very rapidly some of you prove a strong so now they're coming up with even majors in data science for my fancy so of course this might not tell you much so forty designs these are the courses they're taking this is you know anyway you CDC the Center for data science these are the courses they're taking four and then this is from the computer science program that you know they have to take 4c like you know and then of course that is math involved so here they're showing the whole curriculum of what are the difference and again it's gonna be about 120 credits so of course you can very easily do the URL then same with the Columbia similar you will see okay so okay if I can't I will see it also showed 2019 they came up with this program okay so there is calculus there is I'm hoping that you know there is prereqs and again Columbia you know this is a I believe campus all right I'm hope that you understand what I'm trying to show you this be one of the best in the world um okay so these are the courses all right and as you can see you do have some of these courses as well so another reason I'm showing you that you are not actually too much diverted from your score CS background so different campuses are treating it differently see there is Java as well okay so you know and then of course again you can look at it this is different tracks they are doing you know teaching courses on different people so some of it is you might see it again as we talked about the cyber security or other topic so here this is an article so I mean you know you Columbia is not on the list but the sad part is is still most of them are in masters degree okay so even though they did say 20 best bachelors program it's all output is came as in most of them was master's degree so Syracuse there if you have time you can look at it what are they teaching and of course as a beginner and I mean like you know any phone will be quite familiar what may be going on with them for some of you you might not understand but it's a good resource to have like you know just to see what's going on in the world out there so anyway from that I took one of them is I think it's Webster Webster University's it was one-on-one in the rank so that came as an actually bachelor okay so see they have stat they have mathematical courses they have CIS courses you know some of you are upset maybe you're like you know I heard from college that you are learning about circuits see this is also someone I think it's a hard word course I saw but I don't know for sure but it might be also something about web okay see there they have a course we actually focus on our programming okay so these are different like you know it assigns see the remaining the same course that we are talking about is right there data warehousing there okay database is there okay and of course another course it over so of course did their you know one of the top there is reasons okay so then I wish I brought up actually it's the next one that I think so the mix here now I was just trying to show you carry us a career for example again I'm sure maybe most of you will be working from Bangladesh even if you're working from Bangladesh you might be applying for projects in the US for our sourcing and stuff and as I said earlier I don't think in Bangladesh it is a big field yet and I mean Rippon can correct me if I'm wrong but I think I is coming okay in the US it's already a hot cake as I mentioned it's someone like Baruch even like you know there is lots and lots of students are actually registering and graduating from this field so I am pretty sure that it will definitely heat in Bangladesh soon and of course as far as I know maybe of course it's a big thing in India so it's coming so what I did I just you know from the Google then I did then come on so I did see there is data scientist now if I were you I would look for this job description what is that they are actually looking for it doesn't really matter where the job is because in your case you are mostly interested to learn about job requirement and so if I were you I would be actually looking at what is actually they are interested on their skillset okay see there is different kinds of fire there is internship there is data scientist you know and there is one in here so and of course they give the different titles or different names on these jobs okay and even see it's an entry-level job basically they are looking for someone and it's from a recruiter so that's the anyway she doesn't make much difference so our junior data scientist right so again I'm just showing you a tip just to see where two things are and see the 11 data science carriers shaping our future so here what is the data science and then okay see the first title and you know it's showing you the salary these are u.s. salary so data science used to be much higher it's getting lower see machine learning engineer the machine learning scientist data architect infrastructure architect get us engineer business intelligence developer statistician like you know data analyst so these are right now the names are there as you can see the data analyst has a lower salary because they are kind of not so much proficient with the data mining or like you know machine learning and they kind of gathered the data for the data scientist okay so anyway this is hopefully gives you and of course when they see application architect this would be someone who knows how to do programming as well see enterprise so these buzzwords does mean something okay our machine learning they and sure again for some of you these are new but you know it's the pond just you know responded that you know he agrees also that it will be hitting one others as well so anyway so these are the hard things going on okay this is a very good website for data scientist cadena Nagas comm if you have not used it I highly recommend you to really look into this so this one is actually the companies out there actually focusing on data science or data analytics okay so kind of and you kind of get lots of data from here actually okay so anyway these are some of the lists and of course every time is get updated they are very good with this updating and so the list might not be made it here as well so because it's the user centric so if you're curious you can go deep into it so you know and spend some time with this and then I kind of made what are the different tools and of course every day things changes right so there are some remember I was talking about predictive descriptive right classification and prediction that we talked about see right there so anyway so and here also dimension regression Association rule classification clustering okay so like you know there is tools behind it but you are not required to learn these tools but of course you're it's very important that you understand the concept because if you don't understand where are these are you gonna run these tools okay alright so anyway so here and as time goes there will be more and more tools coming and of course people don't want to build from scratch because it's a lot of work sometime it can go years to build things so there might be over the time the same way that thing happened for data warehousing you know there are so many order cloud infrastructure there are so many tools out there that you know they will customize things for you and then you might end up using the tools so but anyway it's a fairly new field still so there is not mature tool that's out there that can really do all the work but I'm sure as time goes you know this some of these organizations will come over and you know get the customers you know like somebody like Oracle for example like you know tool based and it's it's a billion dollar industry don't think of just some some like in a small jokes you might be paid hundreds of thousands of dollars just you know build things on it based off a tool okay and this was another one 25 best data mining tools and again I just focused on data mining because I mean data science and data mining as well so you know and nowadays most of the well-known corporations they kind of have focus on this so as you can see some of these I'm sure an opponent and I'm familiar with again so these are the truths on en masse Oracle I used to this actually okay so these are different and you know it said the list will grow and come back and down its again I just used the cloud as an infrastructure as an example when cloud started it was a chaos 15-20 years ago but now it has still it's humongous there is hundreds and hundreds of databases technology cloud infrastructure out there so you know and then the last one that I had for you guys was data science jobs in Bangladesh right so this was just a search I just did as you can see this is a message is kind of do a Google PD don't do this machine that's why it's anyway so the idea was trying to start it based on the Google MIDI so I think it did came still the similar so I see some there but you know if you are more than welcome to browse about and browse in it and try to see what is the future or what is the current going on in Bangladesh all right and of course you don't have to really the world has been you know so much change that you don't have have to wait for Bangladesh even to move into you can even go into the world and apply things in the US or some other places okay so I think that's all I had from my end oh that last parts up so of course I kind of talked about a little bit I'm good so for Bangladesh folks what is that you should do first of course you have to understand what is data data basis right you have to have understand the business process in my case I said it okay I'm an expert on the education industry like you know how and when I'm saying education I don't mean teaching okay I mean the administrative part of it so I don't think in Bangladesh this is quite a big field yet or maybe in the private universities there is IT you need that handles this but this is quite normal in the u.s. every college has an IT you need where we handle things we have programmers we have database administrators we have seas that mean I mean we have a whole IT unit that handles the administration of the campus because there is lots of online stuff happens ok so anyway so then you can understand the differences between so in your case it could be in my case I said in education in your case it could be pharmaceutical company or it could be banking industry ok so you know it depends on where you are interested to put your focus on then the unter difference how do try to understand the differences between data science and data analytics hopefully you've got little idea but I don't mean by little idea you have to have a deep deep deep understanding you know what means to be a data scientist what it means to be a data analytics okay so and of course understand the data mining or data science techniques you know there are mood courses out there so what I mean by BACtrack meaning if you have to really start from scratch ABCD is 1 2 3 start from there ok and for most of you you do have to okay and then do some projects okay you know if you do projects hopefully you get a better understanding of things not just trying to learn things and you know when you do those projects I showed you the my class example you apply that different techniques or algorithms that you do and then of course you know I showed you the job search and you try to learn the tools that they require maybe something you learn today five years down the line it might not be as popular but don't think it's going away okay so you know try to pick up something good but most important part for you is actually the theoretical component of it this stuff this would take time okay and of course build your portfolio and resume so I don't think I mean oh yeah I did have that link at the very beginning like say if I show you my portfolio or resume right so as you saw I have my LinkedIn profile here I have my brew profile here right so if I can go back okay so see here again I'm also a newbie for teaching full-time so you know all this time I've been teaching part-time so I plan to go so I'm also I do also have my profile here as you can see it's a website from Peru so I kind of advertise what I've been doing and blah blah blah this is tour geared towards teaching background so this could be my portfolio for teaching alright so as you can see I'm showing my 20 years of background here right so now in your case you might say you know what I don't have anything what am I gonna do you know you it's great you had 20 years time you know you had all this and this is a comment quite simple question so now if I take you my to my resume this also I posted for you guys to show you now my resume is a little different ok I have two tracks one is the teaching track another one is the professional IT track okay so after you graduate you should have something I mean maybe as not as big as mine but you should have something alright as you can see I'm also advertising ms-office you might be thinking that's silly but you know some jobs that they are asking for this so why not just put it in there alright so yes I did remove some of the stuff that I maybe not being used for 10-15 years but if you notice a lot of jobs I applied for the whole reason I got called because they look at this and they think this guy knows something all right of course they have to test me but this Ummah is the one resume your CV whatever you call it it is an entry door for you so for example if you are practicing see right hopefully later you're gonna have this there you're practicing are they are practicing are I mean Python all right so this technical skill sooner or later should be there okay of course you don't need research unless you're going for teaching and then I'm showing my teaching background but you can skip all this see but then I do have some professional training that because of my job that allows me to do this so they pay for this so you know I show those if you do take them and certification you can show that in your resume all right you know so these are and then professional experience yes of course in your case you might not have any right now but before you graduate I would highly recommend you to do some internships okay so these are real jobs but if you go down for them down you see this was done when I was a student all right the upon you can see this was I was a library student at that time okay so this was an internship actually alright and this was an a part-time job as a student okay right so something like this before you graduate wherever it is you know but it should be something related to IT not some other like you know it whatever it helps you so I would say start your focus I mean yes today we are talking about data mining or data science next class we might be talking about cyber security and after all these that your wish list that you guys shared with me you know you should kind of make up your mind that what is that you are interested on your future career right so if I go back to your topics right these are the topics that I cover today we talked about data mining but then again you're gonna be able to see a similar pattern on these other topics that you share maybe not as big as this one but you know this was the course that I taught him that's how you're able to see everything that so but hopefully it will give you some level of understanding so you okay we want to answer your question actually I have to study myself because since I'm not from that background I might not be able to look at it but you can look at it yourself actually also and if you have anything in that I can help okay I think you look you can see what he don't so anyway and it's a kind of brand new field actually so there's the maturity still has to happen you know and anomic we have experience please sharing so anyway you know that's that's all I have from my end does anybody have any question if you have question I will you know you can unmute yourself and ask questions or you can put yourself you know putting the chat like you know so far I see only I know I went almost two hours actually actually two hours okay see enemies actually you know it's already claiming that you know it's by the way every point you don't know him he's from my carrot college from buddy he's been very helpful when he was in New York no he's in Buffalo so yes you know thank you for chipping in so um anybody have any question especially for the folks in like you know CSE majors how how about is everybody okay with this time the Sunday 9:00 p.m. the Bangladesh time you can say yes from the folks that is still here there Ghalib I hope it's okay with you and I see her hard and you know chef ID is still around I guess some people have to leave so that's fine so it's okay with you guys so you know I will schedule the session at that time and then you know I don't hear you are I hope I didn't lose you and make you fall in sleep I don't hear from anybody okay Ghalib is saying yes for you okay great thank you how about the rest is everybody okay all right good good and I see that like in your being active okay great okay all right so I'm hoping that everybody's okay so I am hoping that next class it's not going to be as long sorry I took longer than I expected okay okay all right so okay so yes I mean you know you can email us I guess you know enemy if you were interested to help him out like you know but he mailed me all the queer that everything that you are needed so just if there is anything that I can start help you with I will be more sure I'll spend my time to look at it so just email me and that everything that because I'm in business model of course it's big big but what is it and ha well I mean what kind of thing you're interested in and stuff like that I'm hoping this is for data mining or data science project so okay alright I think okay yeah that's fine yeah that big it says it's the same it's a most likely unsupervised data okay all right yes so everyone thank you again for attending this session if there is no more question I think I'm gonna end the session and stop sharing again thank you for everyone oh by the way before you leave for those of you who did not you know as you can see and I'm just shared his email please do share me your email account so I can directly send you the email right now I have a rough and Ghalib and enum and also my friend how much for for the rest please yeah I do that years okay but the rest leaves us send me your email account so I can communicate for the rest of the lectures give me your name and the email okay I do have a rough Ghalib golly why don't you send a type you're against placement so I have it or we end so I'll give a minute or two but officially we're kind of done actually so you want you can log too bad I can't see who is typing I was like this and upon yeah I have yours thank you okay let's do this if you think you don't remember or you are shy to share with everybody I will share my email account yeah thanks people so you can send an email here but just make sure you say like you know something like Bangladesh or like you know get reminding something so that way I know that is coming from you guys because it will be an unknown email account so it can go into my spam as well so for the rest otherwise I guess I'll have to communicate through golly so again is anyone typing their email before I end the session just say yes or no okay all right good Thank You golly for the input all right so again hopefully next one we will start yeah I got it Kelly thank you so next when we will start at 11:00 sharp please try to respect that time as you can see here people coming from different horizons so you know the goal is here trying to help each other and hopefully someday I might learn a few things from you and of course the teaching is my passion or talking like this man because the whole idea behind it is like I always learn things when I'm speaking to the youngsters okay so good luck everyone and "
BhKGWDmgw-0,27,"Data mining is the process of uncovering patterns and finding anomalies and relationships in large datasets that can be used to make predictions about future trends. ... It is primarily concerned with discovering patterns and anomalies within datasets, but it is not related to the extraction of the data itself.

The Primary Tasks of Data Mining
Prediction involves using some variables or fields in the database to predict unknown or future values of other variables of interest.
Description focuses on finding human-interpretable patterns describing the data.",2020-10-19T21:15:38Z,Data Mining Tasks and Applications,https://i.ytimg.com/vi/BhKGWDmgw-0/hqdefault.jpg,Charles Edeki - Math Computer Science Programming,PT45M14S,false,210,4,0,0,1,okay welcome to introduction to data mining course its 632 in this lectures we're going to discuss about data mining tasks and applications this is our unit one lecture number two so data monitors we have the prediction methods and also description methods prediction methods means we are going to use some variables to predict some future values or unknown variable example would be the stock market maybe we may know the price of a stock today the economic situation today and we can use this data to predict the future price of the stock and another example would be we have advertisement expenses we are trying to see if advertised advertisement expenses can be used to predict a revenue generation which means if we increase the amount of money we spend on advertisement would that also increase our revenue or the company's revenue so this would be a prediction method normally we use something called a regression analysis and if it's a continuum or quantitative data or continuous value we also have the description and description methods here we're going to find a human interpretable patterns that describe the data so the data money tax we have classification which is a predictive clause story is descriptive and also association rule discovery is also a descriptive then we have a sequential pattern discovery descriptive regression is predictive deviation detection is also predictive so example in classification normally our output value or data will be categorical example a patient have a disease a true or false or yes or no this would be classification but regression the stock price will be hundred dollars fifty dollars use uh again regression classification is a predictive by regression we are predicting values like a stock price or income generated whereas classification we are predicting a categorical data an example is a disease yes or no and closely descriptive the reason why because we are not predicting anything so but we want to that's why it's called cross story we want to find the similarities in the data set when we have a group of data set that i have the same attribute or the same values they are similar we group them together as one cluster if we have another data that is similar from the first crosser then we cross that and we group it as a cluster number two so similar values or data will be clustered together the similar values will be across that differently so it's descriptive association rule discovery we want to find if there's two or more variables have any relationship association for example i will go to the supermarket what is the chance that if i buy a bread i may also buy let's say pinot butter or a break or a meat what is the relationship between a customer buying a meat and also buying or cooking oil so that's what trying to find association between two or more variables sequential pattern discovery again it's almost same as association but the size different so in this case let's say we have a sequence of event we say if event a and b happen then c will surely happen or if event a and b happen then d will surely happen so it's a sequential pattern discovery which again use the same concept as association will discover in association rule discovery we are trying to find a relationship between two or more variables if i go to the supermarket and i buy a bread will i buy peanut butter or beer or egg yeah i'm but in sequential pattern now i want to find out if you buy peanut butter what would be the next thing you egg what would be the next thing so we are finding the sequential pattern discovery so sequential pattern discovery is commonly used in example the computational biology in a dna sequence we know if htc if i have hgtc it means you have a specific disease if i have att gc it means different disease so it's the sequence of the necrotized we determine the disease regression as we said is the same as causation is predictive by regression again we are predicting a value a quantitative value or continuous value deviation detection also is a predictive here we want to predict if they say for example i'm using credit card i have a credit card i live in new york most of my use my credit card regularly the system was able to build my expenses or my profile of how i use my credit card most of the time i use my credit card in new york 99 and i don't buy an item more than one thousand dollars all of a sudden i mean texas buying something about ten thousand dollars that's deviation so the system will see the deviation detection detect the deviation of my purchasing behavior and maybe to ring an alarm or they may reject the cells or they want to contact the person they'll contact me and make sure i'm the one using the credit card so deviation detection can be used again to find any deviation from a behavior even a network instruction detection system also we can use the deviation detection so these are the major data monitors and each tanks have different algorithms so we start with a classification an example so the definition said in classification we have two steps the first step we need a training set a training set means we have the data attribute which also the class level and the class is very important because that's for example we are going to use the training set to build a model then later we use what we call the test set the test set doesn't have the class level to test how accurate the accuracy or the precision of the model so what is a training set a good example will be we have a data set about patients these patients have a disease a a training set means we already know the patient's attributes and we know that they have disease a these are the patients that we diagnosed in our hospital for the past let's say five years and they all have the same type of disease we have all the possible attributes that we think it can contribute to the disease or the clinical data by the patients this is a training set why because we have the class level the class level tell us these patients all have the disease so if that's the case i'm going to build a model this model is going to learn how to detect if a patient have that specific disease a the class level then after that we are going to test this model using a patient but we are not going to tell so this time we are not using the training set we are not going to tell the model that what is the class level the class level we don't know it so now the more i have to predict if the patients have disease a or not so that's a is the definition given a collection of records which is training set each record contains a set of attributes this can be the patient temperature the blood pressure or the dna sequence whatever the attributes are for the patient an example is a patient and we can have any application let's say a customer expenses income age etc but one of the attributes is the class which is the outcome now find a model for the class attribute as a function of the values of other attributes so here we build a model with the training set now what is our goal previously on no records should be able to assign the class as accurate as possible so we may have a test data now to see if the model can assign the right class to it let's say a patient has a cancer yes or no so a test set is used to determine the accuracy of the model usually the giving data set is divided into training and test set the trainings are used to build a model and a tester used to validate it again as tango song we have a chapter for classification and we are going to use the weka pro application to try this concept we are going to have a training set we build our model then we use a test cell to test how the model is and there are so many ways to get a test set we can however divide the training set into two and use one part as a test set in weka the application will remove the values where we turn the training set to a test set that means a data with no class level or we can have what we call the cross validation with the number of faults we could say n4 it could be two four three four we may discuss all this as time goes on right now we should know the definition of classification is two-step we have a training set we use the training set to build a model then we have a test set we use the test set to see how accuracy the model is so this is an example given to us here we have a training set here a training data set the cheat is our class level so this data set is about uh taxpayers so we have a again most of the time data we always want to keep uh privacy data privacy so you can see here we are not using the taxpayers name because unless if the name would determine the class i don't think a name can determine whether a task player will cheat on their task returns so we use the task id special number assigned to each taxpayer now we have the categorical data for refund we want to know if the most likely the taxpayers will get reform they are not going to cheat or their marital status most of the time singles pay more tasks than marriage couple pay so that can have a effect on cheating or not and also taxable income is important the higher your income the more tax you pay so who knows the more money you make the chance of cheating on tasks is possible so this is our data set this data test is a training data set because we have the class level we see either the taskbar achieved or not and this is what we know so now we are going to use this training set with the learning classifier which this will be our organic let's say we are going to use a decision truth or support better machine we will discuss all this in the future we are going to use the algorithm to build the model now when we build a model finish we get our test set so you can see the test set here in the cheat section we have nothing question mark so now the model have to predict whether all these star players who choose and who didn't so again two steps we have our training set about taxpayers we use it to build a model with a classification algorithm after we build a model finish we use a test set test set mean in the class level we don't have anything we don't have no any information now the model will have to predict all these arrests and based on accuracy we may accept the model or we may modify the process we know the process of data money tax the first thing we need to select our data next we need to prepare the data data preparation the reason why we need to prepare our data most of the time and raw data always have an error if it doesn't have no error maybe we may have a missing value or if no missing value there may be duplicates so our goal is to remove these three attacks errors from the data set duplicates from the data set the missing value there are some techniques that we going to use for all these three different ways of preparing data after that we do the data transformation maybe we need to reduce the instances that's the records of the let's say we have 20 tax periods maybe we should reduce it to 10 the algorithm will work better all the attributes are so many we need to reduce it we have something called a the dimensionality calcium which means the higher the dimension of the data the more the algorithm will take longer and longer time to make a decision so if i have two attributes that seem similar i will drop one for example uh date of hired or date of birth and age they're almost the same information so i mean either take one of them out so after the data transformation that's when our data is ready for analysis then that's the next step apply the data money algorithm then we get our results we need to evaluate or interpret the results before we make a decision so next example now is classification how we can use clarification in that direct marketing now the goal here is to reduce the cost of mailing by targeting a set of consumers that we are sure they are likely to buy the new cell phone product here we want to market a cell phone product but we don't want to sell a mail to all the customers that has been making a transaction business with the company because that would cost a lot of money for the mailing so here we have to do some data money tax based on the customer's purchasing behavior or if we have the information about their income or they address the demographic area they are living we can use that to determine their income if their address is a place where we know cost of living is very high renting or mortgage is very expensive or they are from a place where mortgage or renting is very cheap so we need to know all this information for us to be able to be sure that we have a chance that this customer may buy the cell phone product so again data money classification algorithm can be used in direct marketing to reduce the cost of many by targeting a set of consumers likely to buy new products now how do we do the approach first we use the data for a similar product introduced before so we can remember the data money definition we want to find a knowledge or a pattern in a data set this data set that would be an event that took place before and it has been saved as a data so the same thing apply here we look at the customers past purchases and we see if they're pushing the customers that purchase a product that is similar to cell phone product or maybe the costs are almost the same they mean those are the customers we are going to target now we know which customer decided to buy and which decided otherwise so we may have the class level buy or don't buy decision that form the class attribute then we collect various demographic lifestyle company interaction related information about all such customers so data mining our main ingredients is data so if you want to predict it by a customer by an item or not then we need to analyze every single database customer that is related to buying or not buying an item type of business where they stay how much they aim now we can use this information as input attributes to learn a classified model example is a fraud detection like credit card i'm using credit card always all of a sudden i use it in a way that is way deviate from my normal purchase behavior so we can use data mining to build a fraud detection model system this will predict fraudulent cases in credit card transactions so what is the approach first we again this is data money classification so always we need to have a previous historical data set with a class level we know what happened so here we use credit card transaction and information on its account order as attributes when does a customer buy what does he buy i often buy some time so these are all the information that we may need or even more information then label the past transaction as a fraud or fair transactions this will be the class level so the past purchasing behavior which ones are again a fair purchase or a fraudulent purchase this forms the class attribute now we're going to apply a learning model on this class of transaction to build a model then later we can use a test set to test this model so here we say use this model to detect fraud by observing credit card transactions on an account so another application would be the attrition or charge attrition means for example if two companies let's say a banking industry we are sitting back in and uh chase bank a bank achieved and chase bank if i decided that i'm 80 years old now i want to move out of new york city to a while small village so i need to close my account this is not considered a situation because i'm not moving from one bank to another now i'm a young 35 years old man i'm still in new york city but i found out that another bank is offering a good interest or a good services than the bank that i'm i'm using so this case i decided to leave my bank and go to a new bank this is called attrition so data money have a a special program which we call the crn customer relationship management software that this software can monitor customers behavior in an organization and we try to take as much data from them from the customers and we input this data again to see how the algorithm will run on it what decision to give us so the approach to use here in to in order to avoid attrition or outright is that we use dt record of transaction with each of the past and also present customers to find the attribute then how often the customer calls where he calls what type of day it costs most is financial status marital status etc we need as much data as we want by the customer then we label the customer as a lawyer or this lawyer so our goal is to find a model for a lawyer a loyalty because i tried means i moved from one company go to my former company competitors and start business same business i was doing the previous year the previous organization i'm going to a new organization going to do the same business then we have attrition or customer contract another application is a sky survey cataloging here they go again is to predict that a class the class is an images on the sky we want to know if this image is a star or galaxy now based on the shape of the image and when we take this data using a satellite or a sensor and we analyze this data we have the training set that if this shape is so this amount or these sides of shoe represent galaxy this size of shape represent the star so we are going to be the model as a training using the training set because we know style galaxy the next we use the test set which the class label we don't have it so then it's up to the model to predict whether the image is a star or galaxy again you can see that so far we have looked through four applications and they are all in different field of studies or professional field from retail marketing to style galaxy prediction to credit card fraudulent and even to computational biology we can build a model using classification to diagnose patients a specific disease so the approach of this survey catalog is that we need to segment the image most because based on the image size we may know whether the image is a style galaxy so we have to measure the image attributes or the features here we have 40 of them the object what we mean by photo that means the features or the attributes of the image is faulty maybe the length the width the depth the side the weight with the color if it's possible color can make difference so the possible features is 40. so now we're going to model that class based on these features this is a training set we know the attributes the data we have we have the class level also so after we build a model then we're going to get the test set to test how accurate the model is so next we move to another data monitors which we call that clustering as we said earlier clustering is the descriptive classification is predictive so here the definition of a clustering is given a set of data points each having a set of attributes we want to measure the similarity of the data set when two data sets are similar we group them together as one cluster if they are not similar then we separate them to two different consoles so here we say giving a set of data points each having a set of attributes and similarity measure among them find crosstalk such that data points in one cluster are more similar to one another and data points in separate courses are less similar to one another so here we need a special guarantee that we can use to measure the similarities in the data set so the most common and easy method is the ukrainian distance the linear distance if the attributes are continuous if the attributes are values or we can have other problem specific measures and we may discuss a lot of this when we started clustering algorithm so this is an example you can see here we have three different clusters so this year is based on spain so we measure the distance you can see that the distance between each group is less api within the group than measure between two groups the distance is very long so we say the intra distance are minimus this time within the group is very small but inter crosstalk distance are maximized distance between the clusters are very large or muscle wise so let's go through some of the application clustering we start with the most common market segmentation so here we have a customers buying items different customers some buy no high any attention spend more or less money etc now we can segment this data based on any of the customer attributes if you want to uh use the customer location of the attribute then we are going to say have two or more segments based on where the customer from if it's about their income based on their income if it's about their expenses or purchasing power then fine so cross selling is to divide an item based on some similarity measurements so the goal is subdivide the market into distant subsets of customers where any subset may cons conceivable be selected as a market target to be reached with distinct marketing miss so other approaches we collect different attributes of customers based on their geographical and also the lifestyle related information then we're going to find the clusters of similar customers so if two or more customers have similar attribute values then we group them together as one across them if there's a dissimilar they improve them separately and another application is document the clustering the goal here is that we have a lot of documents we want to group the document based on the content of the document if the document have more words or spots when we group them together as a sports document they have more information on medicine we group all together so we are grouping documents based on the content of the documents they had the same content then they how they will be in one cluster different content they will be in different process so here to find group of documents that are similar to each other based on the important terms appearing in them so they approaching to identify frequently or carrying texts in each document then we form a similarity measure based on the frequencies of different terms and we can use it to again to cluster what we again information retriever can utilize the process to relate a new document or search attempt across the document so if i'm able to close studies document that this is sports this is marketing this is banking the new record i'll get across this if they fall under any of these three they need to be sent to that very location if it's marketing similar or whatever value next we go to the association we discovered so association as we said earlier is trying to find relationship between two or more variables but we have some threshold value that we need to discuss the confidence and also the support and we have a data set we want to know that if you buy an item a are you going to buy 10 b so first i need to have the attraction value for support and confidence and we may set it to let's say 0.5 percent or 0.5 value it can be any value but it shouldn't be it should be like percentage from one to hundred so now what will happen is that if my treasure value is three percent and i find for support and let's say five percent for confidence if i find it i have the data set i find the support and the confidence and they are lesser than these two values it means there's no association if they are the same or greater than this association so here we say given a set of record each of which contains some number of items from a given collection we should produce dependencies kind of association dependence through which you predict the currency of an item based on the currency of other items so example we have a data set here this is a supermarket data set we have five customers we can see the combination of items they buy in the supermarket the first customer buy bread coke and rick the second customer buy beer and bread etc now we may have a real discovery here but in order to discover this rule again it will be based on the confidence and support so we saw based on the data here when a customer buy america he also buy coke the reason why because we look at the american cook together we have one drink and cook and two and also three so customer one three and five both american coke so that means that confidence the support will be three over five three over all the total but confidence will be if we have american coke we find how many people buy break and they buy coke divided by how many people buy break so here i would say we have one right two we have four make at least which means at least you buy america more it should be on the left side value because the break will determine the coke as we are saying if you buy more you buy coke so in this case the confidence will be break and cook again is three and break the law will be four so it will be three over first 75 percent the support was better we can see that the support we have lesser value because it's three over five the whole database records and again we will go more detail an association but right now we should understand the definition of association with discovery and we want to know if we are giving two or more items by getting the first one will it determine the second one to be around so giving a set of record each of which contains some number of items from a given collection produce a dependency which would you predict or currency of an item based on the currency of other items so this has application example marketing and sales promotion the rule said when a customer buy bagels he also buy potato chips so potato strips is what they call the consequence the right side they determine what we have to do based on antecedent so the bagels will be the antecedent so consequent can be used to determine what should be done to boost its sales and acting centers can be used to see which product will be affected if the store discontinues selling bagels so if this if these bagels potato chips work then if customers stop selling bagels the sales of potato potato chips will be going down less and also we have the supermarket share management the whole goal here is like a super we want to know that if a customer buy diapers and break then the third item you buy will be bread so don't be surprised if you find suspects that nest to diet diabetes and diapers so we can use the association rule discovery in a supermarket to arrange how items are stored in the store another example is inventory management a consumer appearance repair company wants to anticipate the nature of repairs on his consumer products and keep their service very cool equipped with the right parts to reduce our normal vc to customer household so that's our goal a consumer apprentice repair company wants to anticipate the nature of repairs on his consumer products and keep their service vehicle equipped with right parts to reduce on normal vases to customers so this is discovery rule if i get a would be happy so in this case we don't know the right of repair so approaches that process the data on tools and parts requiring the previous repair at different consumer locations and this discover the code or currency pattern so want to know the patterns if a will be okay or if a would be a c okay so the next will be the definition for sequential us rules which is almost as we said earlier close to association with discovery but association means i want to transition between two variables if a will be if a would be a c but sequential means if a i want to see the order like here we say if a and b c will come then if c then b is the output so we are giving is a set of objects with each object associated with its own timeline of event now we should find a rule that predicts strong sequential dependency among the different events so here we say rules are formed by first discovering their patterns again this is a sequential dependency sequential means in order how the outcome is if it's saying this should be b then c then we know we have output of d and e association we don't care it can be a b and c let's see a relationship among the three variables sequential so it can be a b c a c b c a b for association the order there's a mara but in sequential it's very important if it's a b c then a b for c not a c b so you have to be in sequential order so example give you an telecommunication alarm logs so you can see that this is very in order we have example that inverter problem excessive line current so rectified alarm means we have fire also in the point of sale transaction computer bookstore i buy introduction to visual c book and c prime plan so there's a order that i have to let's book about a pair for dummies sequential order athletic appearance store a go to the night store a buy shoe the nest will be racket or racket ball then there's a chance that i'll buy a sports jacket it's equation further [Music] so next would be the regression regressions as we said is the same as prediction we use it to predict a value of a given continuous value variable based on the value of other variables and if we remember college algebra straight line graph by the same concept or the straight line function y equal to ms plus b m is the slope b is the y-intercept and when we study this in statistics neural networking fails also when we discuss uh neural network instead classification algorithm that also use the functions so example like predicting cells you can see that all the predictions will be values predicting sales amount of a new product based on advertised expenditure or predicting a wind velocity is a value as a function of temperature humidity and air pressure so based on air pressure which is a value humidity which is a value temperature which is a value we can predict the wind velocity or time series prediction of stock market prices as the time changes and based on economic condition we can predict the stock price the next one is the deviation or anomaly detection this is the concept of using a credit card example where we said that for example i'm in new york i use my credit card always in new york 99 percent of the chance and also my purchases with my credit card is not over one thousand all of a sudden i'm in texas or florida spending twenty thousand purchase an item in my credit card this would be a deviation from my regular purchasing behavior pattern so deviation anomaly detection can be used for credit card fraud detection or network intrusion detection also the goal of a deviation and normally detection is to detect the significant deviation from a normal behavior my normal behavior of using my credit card is just don't spend more than one thousand dollars in one purchase and always in new york city or tri-state area new york city new jersey connecticut but all of a sudden i'm in california or texas spending twenty thousand thirty thousand purchase at a time that's that will and be a detection i'm deviating from a normal purchasing again profiling now challenges that is facing data money today one is the scalability and i think scalability program is being solved now using distributed computing environment such as hadoop with map reduce scalability means we have a large scale big data so now we can use a distributed system whereby over thousands of pieces may be doing their part of the work when they finish they send it to the central comma dimensionality we have something called the dimensionality case when you have an a record or a data set with high dimension the organic tend to perform poor so we need to reduce the dimension it's a challenge also sometimes when we have a very complex data or a total genius data which is data with different dimensions different formatting we need to follow the right way to prepare this data improve the quality of the data data quality is an issue also and also data ownership in terms of privacy security protections etc so these are all challenges of data mining so again wish everybody the best this is our last lectures for week one and we call again we are covering chapter one of our course tells you which focus on what is data mining why we do data money different type of data money tax and the challenges that data money is facing today and also where we can apply data money tax at different business organization so again wish everybody the best we have assignment for week one if you have any question make sure to contact me and wish everybody the best again thank you bye 
JkwcOUyNssg,27,"You can find the entire course here: https://goo.gl/rM2W1E

You can find all the courses by Hashleen Kaur here: https://goo.gl/SPmZoX

Introduction to Data Mining| | Lesson- OLAP VS OLTP

In this lesson, Hashleen K has discussed about OLAP AND OLTP, their meaning, their differences, with examples.

Download the Unacademy Learning App from the Google Play Store here:- https://goo.gl/02OhYI 

Download the Unacademy Educator app from the Google Play Store here: https://goo.gl/H4LGHE

Visit Our Facebook Group on Engineering Curriculum here: https://goo.gl/5EqfqS",2018-02-10T05:30:00Z,Introduction to Data Mining-: Lesson- OLAP VS OLTP,https://i.ytimg.com/vi/JkwcOUyNssg/hqdefault.jpg,Lead GATE,PT8M20S,false,1045,11,0,0,1,Cupid II and Hirsch Lynne coil so today let's take an example of a huge online retail store that is Amazon and every minute data has been generated in the form of customers buying or selling products and Amazon needs to make critical decisions all based on customer activities and for this accuses various data mining techniques one of which is Ola so we're gonna be starting back in the for the lesson and also please do subscribe to our YouTube channel hi everybody welcome to lesson number 5 that is OLAP versus OLTP I am Hersh Liancourt and this is a small description here about myself so starting off with what is all up so as we studied in the previous lesson OLAP is nothing but online analytical processing and it basically helps a user to selectively extract some data from a data set and view it from various different points and OLAP helps us to process selectively business intelligence queries and basically all up it's the second stage and OLTP is the first stage but as of now recently oil OLAP has become more widely used than OLTP for any business process and OLAP basically has all the historical data stored and it allows analysis on both historical and present data and OLAP is organized hierarchically and it is stored in cubes instead of tables we don't have any relational databases here we have a cube stored here using which we model it into a multi-dimensional data piece so let's see example of sorry a differences between OLAP and OLTP so OLAP basically deals with lesser transactions and the queries are little more complex then OLTP and it is widely used in all the data mining techniques and here the data is stored and historical data is aggregated in multi-dimensional schemas whereas in oil TP so in oil in a while VP what happens is it is it stands for online an electron line transaction and here we use a certain SQL queries such as insert update delete and it is a little more faster then or OLAP but it does not acquire the amount of similar as to Ola that is your only client data is present but in OLAP also historical data is present as we we saw so moving further these are a few differences between OLAP and OLTP data jot down see all can also read that and so firstly it involves historical processing of information and here it it involves current processing of information that is day-to-day and here OLAP is used by a little high management that is by analyst management or people to make better decisions and the oil TP is used by database administrators or maybe by clerks etc and this is used to analyze the entire business this is used to run the business but not to make decisions and OLAP is used this sorry OLAP contains historical data and this current a contains current data it provides a complete summarized multi-dimensional view of theta it provides a detailed flat view of theta in form of tables yeah the database size of OLAP is 100 GB to 1tb but the database size of LTP is only 100 MB to 1gb which is very very less and here are millions of records can be accessed in minutes and here the number of Records which can be accessed is very very less so there are there are four types of OLAP servers that we have to be studying about so the first one is a relational olap the second one is a multi-dimensional OLAP thought is hybrid OLAP and the fourth one is a specialized SQL server so firstly what is a relational olap so relational olap also known as rollup is other servers which are placed between relational back-end server and client front-end tools so it is placed right in between between the back-end and frontend tool and it basically helps us to manage and store the data warehouse and or OLAP uses the relational database or tables and it basically is very important to implement the main logic and also to optimize the database back-end coming to multi-dimensional OLAP multidimensional OLAP basically is called as molap and he refuses array based multi-dimensional engines which is used to view the data in various different dimensions so molap is basically uses two levels of data storage to handle very huge or dense data sets or spa data sets so that is when multi-dimensional OLAP is used next comes hybrid OLAP so hybrid OLAP is a combination of both relational olap and multi-dimensional ola and it basically offers much better scalability than relational olap and faster computation then multi-dimensional OLAP and a hybrid OLAP server always allows us to Hue to store huge amounts of data and here all the data is stored in separate applications then talking about SQL specialized servers these basically used a certain simple SQL language status update delete insert etc simple SQL queries and they help us in in in extracting data not only from the SQL queries but all over those star and the snowflake schema button is read-only environment but this we can only like view the data or we can maybe insert some data but we can never edit the data here so let's take out your life example so as you can see this this thing here is called as an OLAP cube and talking about your life example here the problem statement is that a user wants to request that the data to be analyzed should be displayed in a spreadsheet and it should show all of her company's beachball products that has been sold in Florida in the month of July that is in July and it wants to compare the revenue figures of the same product that is the beach boy products in September that is the first thing that it wants to do it wants to take the beach ball products than the amount of piecework products sold in July and compare it with the same products sold in September in Florida but it also wants to see a comparison of all the other products sold in Florida in the same period of time so there are about three three conditions that we can see here so for this the answer of how we're going to be doing this is using an OLAP and that is for this we're going to be storing the data into a multi-dimensional database we are going to be forming a data cube and form various dimensions and then divide them into attributes according to or maybe the product or maybe the geographical geography sales region that is we're going to divide Florida into various cities and then we're going to divide it into quarters that is the time period and then try to maybe extract data from it as to these conditions and that is how these attributes can also be divided into various sub attributes so this is the end of this lesson thank you so much please do rate review and recommend our videos and you can also follow me on this link here in the learning app thank you 
GhEteXWNIXc,22,,2016-09-01T13:25:19Z,DATA MINING   4 Pattern Discovery in Data Mining   5 1  Sequential Pattern and Sequential Pattern Mi,https://i.ytimg.com/vi/GhEteXWNIXc/hqdefault.jpg,Rγσ ξηg,PT7M13S,false,16145,83,2,0,4,first we are going to discuss sequential pattern and sequential pattern mining the concept so the first thing is we should say sequential pattern mining is very useful has very broad applications one application could be in customer shopping sequences for example you get a loyalty cards for your you know shops you may want to see maybe one customer like click on the first buy a laptop then a digital camera then smartphone within six months if this forms a pattern you may be able to to try to do some kind of advertisement to other similar customers or you know serving some new incentive for this customer like a medical treatment from sequences natural disaster like earthquake happening it may have some sequences of natural and also human phenomenon science engineering a lot of things are processes they evolve along with time similarly stocks markets they if they have some kind of duration sequences web block click streams calling patterns for telephone and other things forming sequences even for software engineering the programming execution from sequence your patterns the biological sequence is very very useful to have for analysis like DNA sequences protein sequences so we'll see trying to get sequential patterns out of those very big vast applications could be very useful any important actually we can distinguish transaction databases usually may not be important to look there time effect sequence databases they have timestamp attached with it time seared databases usually the time things happened actually along the even or equivalent time intervals sometimes it's very consecutive then for sequential patterns actually there are two kinds why is gapped another is non gapped the gaped patent means you to allow to have gaps within those patterns the non gapped patterns means you were not allowed these patterns the sequence everything is important the consecutive is important if you have gap you have to treat them very seriously for example for shopping transactions probably don't care customer in the middle buying some other things so it's not important to study the gaps clickstream sometimes you may say you know some clickstream you make here but gaps some you pro do dark Arab gaps are much for par logical sequences in many cases you to care gaps so the protein sequence or DNA sequences if you insert many things in the middle of the two DNA sets sometimes you may complete change the function so let's look at the customer shopping sequence as a major example to study how to do sequential pattern mining sequential pattern mining essentially is you give me a set of sequences the album is trying to find a complete set of frequent sub sequences satisfying certain minimum support special let's look at this example we have a sequence database containing like a for customer shopping sequences ok what's the meaning of this we look at this particular sequence the sequence the parentheses means this one is within the same shopping basket then after that it get another one a B that means this a ap you know following EF but a B is getting together at the same time similarly TF getting together but following a B and then C then B okay that means each one of these you can think it's an element it may contain a set of items or your core events then this one event may follow another one the items within the event the order is not important because they are in the same shopping basket but for our convenience we can sort them alphabetically then what is subsequence actually any sub strings within this one if we can see here the subsequence you may have a gap for example you said you can have a you have BC PC actually you chop this a you can chop complete AC then you get a D you can chop 1f you can get C so this one is a subsequence of this longer sequence then sequential pattern mining the sequential pattern essentially is you if you set a support like a minimum support is 2 that means at least the two sequences contain these sub sequences you find those subsequence this is a sequential pattern for example a be getting together then see in these sequence database this is a pattern of support too so sequential pattern mining algorithm is you try to develop algorithms which are efficient scalable and these algorithms should find a complete set of frequent sub sequences we call sequential happens and also should be able to incorporate various kinds of user-defined constraints for sequential pattern mining actually our prey property the property we have used in frequent pattern mining still holds for example if we say a subsequence F sub 1 is infrequent then any obvious super sequence cannot be frequent so that's almost the same idea as our prey so based on this idea we actually can develop a lots of algorithms one we have presented a argument called GSP generous sequential pattern money develop in 1996 another one is a vertical format based mining horse paid a developing year 2000 the solenoid enter introduces pattern gross masters called prefix man developing ear Tucson and move on and then we are going to study mining close the sequential patterns called close span finally we are going to discuss constraint based the sequential pattern mining you 
VSosI2Y9eSM,22,The lecture provides a basic introduction on data mining principles. The discussion focuses on different data mining methods and key things to be aware of when exploring data mining,2020-08-17T18:18:59Z,Lecture 4 Data Mining new,https://i.ytimg.com/vi/VSosI2Y9eSM/hqdefault.jpg,Data Centric Inc.,PT35M59S,false,256,1,1,0,0,"welcome back to business analytics this session will cover data mining again this is a term that is used quite loosely and sometimes used interchangeably with business intelligence data analytics or even business analytics we will try to clarify some of the differences between some of these terms at the end of this session you will be able to describe data mining demonstrate the importance of the data mining process discuss some motivations for data mining describe the data mining environment discuss the limitations of data mining compare and contrast the data mining techniques and finally examine the ethical and privacy issues surrounding data mining some of these will be quite similar to what was discussed under business analytics and you'll soon see why as we stated in the Davenport paper the latest strategic weapon for today's businesses is analytic decision-making that is based on discovery of new knowledge through data mining we saw and discuss some of the companies which have used analytics to better understand customers in order to maximize their returns on investments while providing the best customer service but just keep in mind that though very valuable data mining is not just about understanding customers data mining is the use of tools to identify valid non trivial previously unknown interesting relationships or global patterns that exist in large databases these four characteristics are important as if we find what we already know or were finding things that are incorrect then there would be no value from investing in data mining therefore what we are looking for is things that are unknown or interesting there are two measures of interestingness one is unexpectedness something that is unknown to the user or contradicts a exist ding knowledge or expectations the second measure of interestingness is known as actionability can the user do something with what is phone to his or her advantage so if you think about an example of finding that customers buy hot dogs also buy hot dog buns would this be particularly particularly interesting probably not because we already know this but think about finding that people that by grace tropical rhythms fruit punch also buy cheapest buy banana chips maybe we didn't know this at all and therefore this information could be used to bundle items together for promotional purposes for example data mining involves capturing the data predicting using this data and acting on the outcomes of the prediction without a final action the capturing and predicting really is of no value as we look at this table it is important to point out that we have to understand the realities of data mining a lot has been promised so it must be very clear about the reality of it data mining is not a crystal ball that just makes predictions a lot of work must be put in and must follow a multi-step approach it's wrong to think you just throw some tools on a data set and get some nice results as we will see later you actually want to start with some business understanding that is what are you really trying to answer it was also thought that this was not something that was viable for businesses when actually it is applicable to almost every business as we want to demonstrate throughout this course furthermore once it was realized it was applicable to business it was then thought that it was for large organizations with extensive resources however this is no longer the case and this is something again we'll discuss further in this course finally it is also a misconception that only those with advanced degrees can do data mining there are tools to facilitate the mining however what I would warn is that the decision-makers need to understand the question being asked and can interpret results of the tools there must be some understanding of how the algorithms or techniques work to be able to understand the output of the tools there are some common mistakes in data mining that you should avoid the first is selecting the wrong problem for data mining one of the most important steps in the mining process is knowing what you are trying to answer but as importantly you must ensure that data mining is the solution for this you don't want overkill for a question a simple query could have answered the second mistake is leaving insufficient time for data preparation this effort for data preparation cannot be understated and the numbers speak for themselves some say as high as 80 to 90% of your effort will be in data preparation third believing everything you are told about the data data quality will be an issue and again this is not a issue that should be taken lightly another mistake is ignoring suspicious findings and quickly moving on mining is actually a very useful technique for finding outliers or behaviors against the norm and therefore suspicious findings could be such things as fraud and please investigate these closely another mistake is ignoring what your sponsor thinks data mining is and what it really can and cannot do be very clear in terms of expectations of data mining there is a lot of buzz about this term so sponsors need to be very clear about what they're getting finally make sure that you and your sponsor are on the same page the sponsors buy-in is essential in success in data mining there are a number of factors that have led to this push in the area of data mining one is that there's a recognition by organizations of the value of data data-driven evidence-based decision-making is becoming the commonplace in many organizations which has required and has led to a real culture change by management whereas once it was decision-making by intuition now it's become a lot more data-driven there's also more intense competition at the global scale and management has recognized that the data provides an opportunity to be one step ahead of the competitor also customers are changing constantly changing their needs and this is driven by the increased offerings by different suppliers so there's a need to use the data to understand these changes and be as proactive as possible in changing products finally does improve data capture and storage techniques which provides more opportunities through these technologies so the technology is there and there's an abundance of data so what we need to do now is harness this technology to maximize the value that the data can provide it is extremely important that data mining is not done in an ad hoc way rather it should follow a structured process a number of these processes exist and these include knowledge discovery in databases also known as kdd sample explorer modify model and assess SEMO cross industry standard process for data mining which is also known as a crisp TM the kdd process like all mining processes involves the evaluation and interpretation of the patterns and models to make the decision of what constitutes knowledge and what does not it requires a good understand of the domain that the process is being applied to and the intention is to predict future behavior of some entity within the domain and to describe interesting and patterns and trends and to model the data of the domain the kdd process consists of the following steps data warehousing which will describe further on in this course and discuss if this is the only approach that can be taken once we have the data set then we need to identify or select the data which is needed for a particular business understanding once this data is identified then it must be pre-processed such as data cleaning and dealing with issues such as missing value and inconsistent data as we mentioned before this is a very time-consuming step in the process the data isn't transformed or converted into a format that is suited for analytics and once this data is in this format then the data mining can be applied and quite often this could be considered the easiest part of the entire process but once we've applied the data mining techniques then the results have to be interpreted and as we mentioned before acted on for business decisions as we look at the top diagram showing the kdd process the first step is to really understand the domain you are playing the mining tool so that you can identify the data sources across the organization once there is a clear understanding of the data sources that exists then you can determine the goal or business question that you want to answer the particular data needed for answering this business question is then selected from the available data sets and mining is applied to get results these results then need to be interpreted and evaluated and once this has been done they are utilized for strategic decision making they game is usually to increase the profit of the organization the data mining process requires pre-processing transforming and method selection which we'll talk about as we go through this course this data is in mind and the results are presented in a visual way so that they're easier to interpret the Chris diem process is probably the most widely used data mining process it's technology independent so whichever DM tools are being used the process can still be followed and is applicable the crisp team consists of six steps business understanding data understanding data preparation modeling evaluation and deployment each step is built on the output of the previous step note also that the process does allow for going back to a previous step for example after modeling it may be useful to go back to data preparation to change the way the data has been prepared and then run the modeling again to see how the results change it is important is that the crisp Dam model does not simply say the face of the DM process but it also guides the decision-maker further by describing the tasks that should be carried out for each of those phases for example you see in data understanding it involves collecting initial data describing that data collected exploring it to get a better understanding of it and to verify the data quality a very important task in the data understanding phase the final data mining process that will be described is known as Semih and although many of the steps for this process are similar to that of the crisp DM it is being mentioned specifically because it is a data mining process that has been defined by SAS one of the leaders in data mining software solutions the simmer process consists of five steps sample X or modify model and assess when you compare these steps with those of the Christian you see that summer sample and explore steps are similar to data understanding in Christian the tasks of Salmons modify step are covered in the data preparation step of the Christian in both processes the model steps are similar the SS of sama is similar to evaluation in Christian therefore what summer does not focus on is the business understanding and deployment steps of Christian both of which are extremely important there are a large number of data mining of the chef's tools available some of the leaders include IBM and Oracle these are changing as new tools come on the market and companies buy out the tools of other companies one of the reasons that data mining is such an opportunity even for those organizations that may not have a lot of resources such as some of our SMEs is that there are no open source data mining tools such as our we will use our later on in this course data mining can be used on any data set it is often associated with data warehouses but it should be clear that you do not have to build a data warehouse to do with data mining there are some benefits of mining on a data warehouse and we'll discuss these further in this course but it is not a must and you can mine on your transactional databases or other data stores it is now also becoming more popular to conduct data mining on more advanced data sets such as Big Data spatial databases text database and multimedia databases heterogeneous and legacy databases and now even the World Wide Web the success of your data mining initiatives will depend heavily on how you compose your data mining group or Center some of the important members of this data mining group should include the leader with the influential executive who can work as a Evangelist for data mining and who can secure high-level interest in your data mining projects we saw the importance of this in the case studies that we have reviewed you also need the domain experts these are persons who understand the application domain and who can participate in conducting the business understanding phase they know the business questions that need to be answered and can interpret the results of the mining and then they know how to act on them they also have knowledge of the business rules however it is important that these opinions of experts are verified against the data as the data may be signaling a conflict in what was believed to be true and this should not be ignored the data expert these are persons who understand the structure size format and importantly the sources of the data and finally the analytical experts these are persons who understand the capabilities and limitations of data mining and other analytical techniques and who can work with the data expert to ensure that the data is in a format suited for the data mining techniques that are to be applied though there are many terms and classifications used in this space most will agree that there are three main groups of data analysis techniques descriptive predictive and prescriptive both predictive and prescriptive fall under advanced analytics we will focus on the predictive techniques and more specifically the data mining techniques as we will be covering Association analysis decision tree methods and cluster analysis just to point out though that in this figure business intelligence is considered descriptive however others also speak about business Intel being the umbrella term of all three approaches so just be very clear about the terms in any discussions that you're having then of course we have the datasets that the analysis is being done on which are large and varied we will speak more about these data sources as we go through the course just to recap we had said that data analytics is a science and art of applying statistical techniques quantitative methods and mathematical or computer-based models to large datasets using IT solutions to obtain actionable insights for making smart fact-based decisions it is a process of uncovering hidden patterns unknown correlations and trends in your data sets this diagram shows the evolution of analytics in the 70s there was a lot of talk and interest in decision support systems including expert systems in the 80s the focus was moved to enterprise and executive information systems which included relational databases these databases still exist and are an important part of your transactional data processing there was also focus on enterprise resource planning or ERP systems the 90s brought the advent of data warehouses and dashboards and scorecards and the focus was on business intelligence in the 2000s we brought the advent of more analytics such as data and text mining and cloud computing cloud computing made it possible for organizations that did not have the resources to invest in expensive servers to store data to use alternate options for storing data and performing analytics now we're looking at Big Data and the opportunities of analyzing more sophisticated data sets like social media this visual perspective shows ABARES terms which are sometimes you quite loosely and interchangeably it also shows where they fall and you'll see that some of the terms overlap like data mining is considered both statistics and business intelligence or bi while visualization is a mix of all three statistics modeling and bi the next few slides show how the categories of analytics are often grouped in this particular diagram we start closest to the axis and it shows that the first group is used answer the question of what has happened simple query languages like SQL can be used for this the next layer is the analysis layer the techniques of which are used to answer why has it happened and here online analytical processing tools or OLAP tools can be used the monitoring layer focuses on what's happening now and visualization tools such as dashboards are extremely effective for this the prediction there is a focus of this course and this looks at not just what has happened but also uses this information to predict what might happen in the future the final layer is a prescription layer and it takes a prediction further by exploring what actions should be taken based on what is likely to happen here we demonstrate various representations of the analytics categories note the bottom left shows that the output of all three can actually be visualized which makes it easier for interpretation Gardner has a fourth category which they term Diagnostics which looks at the question of why something happened not also that Gardner considers descriptive as hindsight diagnostic as insight and predictive and prescriptive as foresight as a spectrum prescriptive is considered the most difficult but it has the potential to provide the most value if you were to think about your own organization where do you think it would fall on this spectrum descriptive diagnostic predictive prescriptive or none at all for the purposes of this course will be focusing on predictive analysis answering the question of what is happening and what will happen next and why we will be looking specifically at some of the data mining techniques that can be used for this to make accurate projections of future states and conditions predictive analytics can be defined as the ability to predict the future by examining historical data detecting patterns or relationships in this data and then extrapolating these relationships forward in time some examples that have suited predictive analytics include campaign management customer acquisition budgeting and forecasting fraud detection promotions pricing and demand planning and we will be speaking about some of these applications as we go through through the course let's take a very simple example of trying to determine whether someone will be crooked or honest we could start with a data set of persons we know to be crooked and persons we know to be honest we can then look for any commonalities between those that are crooked and any commonalities with those that are honest and the differences between the two sets what do you notice between these two groups of people note that all those that were honest had a smile and round eyes while most of those that were crooked had a throne and wore glasses in this case while all those our Una's had a smile and round eyes only two of the three that were crooked had a throne and wore glasses but imagine if you had a large data set then the trends you find would be more reliable what would you do this with this information that you found when a new person comes along you may be interested in knowing if they are honest or crooked so you would check if they had a smile and if they wore sunglasses and you would use this information to determine whether they were honest or crooked one of the data mining techniques that we will discuss in detail in this course is clustering which involves finding patterns in data that can be used to classify things such as people and products into categories determined by a similarity measure for example we may want to examine whether cancer patients are clustered in any geographic area then we could look at things such as power plants in these areas to see whether this could be causing the cancer some other common applications in the area of clustering include marketing which helps marketers discover distinct groups in their customer bases and they can use this knowledge to develop targeted marketing programs in the area of insurance groups can be identified of motor insurance policyholders with a high average claim cost and these could be examined to determine the factors it's also used a lot in the medical field for example for gene expression data you can discover genes with similar functions in DNA data the outliers and clustering are also usually useful for fraud detection as these points don't belong to any particular group and they may indicate a behavior out of the norm which can be a sign of fraud another very popular predictive technique is Association rule analytics otherwise known as Market Basket analysis it involves using a rule-based machine learning method for discovering interesting relationships between variables and large data sets it is intended to identify strong rules using some measures of interestingness for example when a customer buys a particular book an online shop may suggest associated books but how do they know this it's based on the historical data that they have analyzed on people's buying patterns do you think of any company that could use Association rules in their offerings decision trees are very popular data mining techniques used in applications historical data is used to build a tree and then a case can be matched against this tree to determine the likely output for example consider customers loans in this case historical data has been used to build a tree to determine whether customer is likely to be excellent good average or bad in terms of repayment of the loan if you consider the leftmost path of the tree it indicates that if the customer has no degree and earns up $50,000 then based on historical data they are like to be bad in terms of repayment of loan should you then give a customer alone when they apply with these demographics typical statistical techniques such as regression are also classified as data mining techniques regression analysis can be used to model the relationship between one or more independent or predictor variables and one or more response or dependent variables they can be used to answer questions such as how does the change in customer service rating affect sales text mining is becoming extremely popular and it is a mining of data that has been classified as unstructured or raw text it involves the discovery of useful and previously unknown gems of information in large text collections why has text mining become so popular approximately 90% of the world's data is held in unstructured formats information intensive business processes demand that we transcend from simple document retrieval to knowledge discovery through text mining one of the most popular applications of text mining is sentiment analysis this is a process of understanding the emotional content of text think about Twitter feeds a special around your new products or services or even around the global events that are happening right now Kove 819 and the demonstrations there is a wealth of information about people's perceptions or feelings in for example Twitter feeds take a hotel looking at understanding the customers experience they find the following comment left by one of their gifts on their website I had a fantastic time on a holiday at your resort the service was excellent and friendly my family already enjoyed themselves the pool was closed which kind of sucked though would you say overall that this customer had a positive or negative experience one way to determine this would be to have a list of word considered positive such as good great fantastic excellent friendly awesome and enjoyed and a set of words considered negative which could include bad worse rubbish sucked awful terrible bogus sentiment analysis involves matching the words in the feedback with both the positive and negative lists so based on the lists the feedback had four positive words and one negative word so if we then subtract the negative response from the positive response the hotel can deduce that the overall this feedback was a positive one of course this does not mean that they should not respond to the customer about the issue with the pool text mining is not easy for a number of reasons information is in an unstructured textual form it's not written for machines but for humans to process which can be quite complex for machines you're dealing with huge collections of documents and natural language processing is very complex for computers things like sarcasm is very difficult for machines to detect ambiguity and context sensitivity also adds complexity for example when the computer reads Apple is referring to Apple the company or Apple the fruit neural networks are another example of a predictive technique we won't go into the details of neural networks in this course but it stimulates some of the learning functions of human brain it can recognize patterns and importantly it can learn you can use neural networks to forecast and make smarter business decisions it can also serve as an expert system that simulates the thinking of an expert and can offer advice some of the typical applications of neural networks include speech and voice recognition imaging and vision recognition navigations in cars face detection recognition stock market prediction among others look at IBM Watson and Sophia both examples of applications of neural networks and ask yourself these questions would you trust Watson's implementation in your organization what do you think would be the greatest obstacle in implementing this technology locally what industries in Jamaica would benefit from the implementation of a Watson do you think Sophia has a place in a business environment data mining comes with privacy and ethical considerations data collection and people raises many questions concerning privacy legality and ethics many web services require that you allow access to your information in order to use the service Google Minds email data in gmail accounts to present owners with ads how many times have you sent an email about a certain topic to a colleague and immediately you get ads about that Facebook which we have been hearing about a lot recently requires users to allow access to information from non Facebook pages their privacy policy states we may use information about you that we collect from other sources including but not limited to newspapers and internet sources such as blogs instant messaging services and other uses of Facebook to supplement your profile this allows access to your information obtained through partner sites and this is worthy of concern data mining is not without its controversies Facebook uses a beacon advertising program when you engage in a consumer activity at a Facebook partner website such as Amazon eBay or the New York Times not only will Facebook record that activity but your Facebook connections will also be informed of your purchases or actions Facebook currently offers no way to opt out of the beacon once it has been activated users can't those accounts but account data will never be deleted there's also the issue of data brokers these are entities that collect information about consumers and then sell that data to other brokers companies or individuals without the consumer even knowing watch the video and learn more about these data brokers in order to maintain the privacy and protection of individuals rights data mining professionals have an ethical and legal obligation to implement procedures to do so one way to ethically handle private data is to de-identify customer records prior to applying data mining techniques so that the data cannot be traced back to an individual in fact the identifier of the customer is not very useful for identifying patterns and trends in data sets we have seen the case of uber and the type of data that they keep about their customers they have been in the news when one of their executives was threatening to expose a customer Sara Lacey's personal data because she had been very critical of the startup do you think that this is ethical in closing it is important to note that although it is the machines and algorithms that are doing the work in the mining the people or humans are as important it is a humans that know the questions that should be asked and know what to predict and how to make sense and action the rules and visualizations that are produced from the mining humans know what is reasonable legal tasteful and therefore human decision-makers are critical to the data mining process "
wrSDo2yKs00,28,"This video is part of LearnItFirst's SQL Server 2008/R2 Analysis Services course. More information on this video and course is available here:

http://www.learnitfirst.com/Course165

In this video Scott will go over data mining, this is a big buzzword right now.  This video is a brief overview data mining, there is a chapter later in the course that will go into data mining more in depth.

Highlights from this video:

- Data mining and MDBs
-* What is data mining?
- Finding patterns
- Manual data analysis
- Find ""known unknowns""
- Key influencers

and much more...",2011-01-20T19:13:04Z,What Is Data Mining in SQL Server 2008/R2 Analysis Services?,https://i.ytimg.com/vi/wrSDo2yKs00/hqdefault.jpg,LearnItFirst.com,PT8M21S,false,4935,8,1,0,1,okay so if we should get started talking about buzzwords and we talk about things that will get your resume noticed data mining is going to be way up there right so data warehouse big buzzword right now data mining big buzzword you understand the use of the term multi-dimensional database right you're going to blow somebody's head off they're going to think you're so awesome so I am kind of kidding with that but data mining is a big big thing right now so let's kind of get an overview of what data mining is and I really mean this is like a very generic overview we have a whole chapter about this a little bit later now this particular graphic that we see here let me let me zoom into this is I think this is very important this is not a Scott Wiggum graphic this is one directly from Microsoft this is how the internal group at microsoft considers the analysis services engine to flow it has two parts one part handles multi-dimensional databases the other part is data mining now this is actually kind of lead to the ability to consider ssas not just a multi-dimensional database server but a multi-dimensional database server and a data mining server now if you want to call that marketing i get it i'm cool with that however we we need to get into chapter 11 to kind of see the actual inner workings the fundamentals of how sequel server analysis services kind of works with that but I bring this up just to show you how important data mining is it is a core part of analysis services ok now what it is is it's the practice of digging through lots of data to find patterns data mining probably understood that from the name right let's say that you have a hundred million rows and that data covers 30 years of trends maybe it is a defect reporting maybe it's seismic nation like are you going to be able to spot trends into that data over such a long period of time I mean that's a lot of data to go through right that's a hundred million rows nobody wants to have to manually go to trend analysis against that kind of information so data mining you're looking for trends in patterns that would have gone unnoticed unless you had done the data mining that's really the idea of it right so we're dealing with massive volumes of data you're not going to do data mining over 5 million rows she's just probably not enough information we need a lots of rows hey you when you've got a lot of data we need to spot the trends but because there are 100 million rows in the relational data warehouse we can't easy will easily spot all of these trends for a customer for example we might be tracking over 250 different data points gender are they married how many kids do they have income right we might have 250 different data points about that one thing called a customer lots of relationships between the entities spouses grandchildren hierarchies within the data itself you can only do so much when you just are given so many columns so many rows your manual analysis can only go so far and our usual way of getting answers is to ask questions right we write a query where you look at the result sets we run a report we look at the report which behind the scenes writes a query and returns the result set right however you have to know what the question is or else you can't ask it see that's the problem here manual digging through of this data is hmm what question do I want to ask okay what about I want to see the number of sales to people who were married to someone from another state right I mean you have to know what the question is and then you review the answer right how many tickets did our customers are clothes over the last three years data mining though doesn't require such strictness data mining is going to help you by giving you answers and questions that you should have asked it's going to go find patterns it's going to go find trends in the data and bring them back to you and you're going to say oh I would never thought to look at that so data mining is going to really broaden what you're able to get out of your data warehouse first off what we're going to work with and we deal with a cube when we work with multi-dimensional databases when we work with data mining you're going to have built your database first your multi-dimensional database then we're going to come back and work on the data mining portion of it so what we're often looking for is to find the to use the old Donald Rumsfeld quote for those of you who would know who he is you're looking for the known unknowns right what are the not-so-obvious attributes of people who are a bad credit risk maybe in our data mining algorithms we want to stop marketing to people who are a bad credit risk and we need to be able to know what the attributes are are they generally married are they generally in between a certain age group are they generally have they usually completed university right what's the common patterns what are the things that are common for people who are a bad credit risk we're looking for the known unknowns we have this information in the database we may not be able to quickly identify that what we're looking for with data mining we're looking for the key influencers this is a good term a big term here and data mining here right if we could identify what those common attributes are of people with a bad credit risk then we could stop marketing to them or we could start marketing to them if we think that our product can help them for example now data mining will allow us to do predictions okay so that's a big part of working with data mine is to do forecasting okay we can run scenarios through data mining we can change a few things tweak it up run a scenario will this improve our bottom line okay now i'm going to change this little one variable let's run a new scenario ha that's what we need to do right sales forecasting any sort of forecasting can greatly be improved by doing data mining ok now the ssas product features data mining as a core we are going to have an entire chapter on what data mining is in more detail how to do your own data mining how to do this how to integrate it with integration services we're going to play with this in depth right so this is just consider this particular video the 10 minute introduction to what data mining is you want more information about that we're going to come back in chapter 11 so really in between chapters 3 and 10 it's all about installing analysis services building your first database how to do cubes how to design your dimensions and measures how to publish this how to get it going with SharePoint or the various client tools reporting services and in chapter 11 got everything bill has come back and now talk about how data mining works ok so see you in the next video you 
7l0XoDSxnJc,28,"Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.

Video Playlist in Tamil   : http://bit.ly/34EkUep
Video Playlist in English : http://bit.ly/2OSaQrl

Email      : atozknowledge.com@gmail.com
LinkedIn :  https://www.linkedin.com/in/sbgowtham/
Instagram : https://www.instagram.com/bigdata.in/

YouTube channel link
www.youtube.com/atozknowledgevideos

Website 
http://atozknowledge.com/
Technology in Tamil  & English",2019-12-04T04:22:01Z,What is Data Mining {in தமிழ்} ?,https://i.ytimg.com/vi/7l0XoDSxnJc/hqdefault.jpg,Data Engineering,PT5M30S,false,12848,485,3,0,33,hi welcome to use that knowledge dot-com this video is in language Tamil and if you want the same video in English you can find the link in the description box of this video anything I'm a pocket or a topic in have been passing know what to state are mining so the computer it's gonna be the theoretical video so data mining of dinner so data science data engineering data analytics in the marine area terms a nickname apakah for the data mining so in the data mining are under split burning a data mining so mining 9 so now remaining now in this Oregon tone D will up with tango why you know either light or wrongly so other than the mining have been so long I Panama are they marry so wrong that the tandy yeah the my in front of are not date our okay so either technical respectively P Saldana or a daytime and I'm a greater kudu cooler than the data 11 then I'm a Pallavi she angle alone pony crock right so other Nakula paper teaching a college will be producing either not data alright so la même looks a little the data and the data 11 the humble Cucaracha fishing in Abdeen path acknowledge your data Elizabeth we get different data Leah so in an open oven or e-commerce website lab 89 occur product purchase Pantani other and the e-commerce company core data and the data which I mean done area data beer-pong if do you reckon Anna sorry even the inner product purchase paneer curry every anna-marie product Li interest come here cerebro D age and up in the age group will chrome looking in the product company kima so in order or data purchase of its your product purchases if it's a only a blue data should work wrong about Anna so you know neither coupon Donna I won't go to business i enhance penny carry the curve okay you for for example data mining low rumba data mining linaria algorithm circle were famous an algorithm of being through the birthing in a market basket analysis other of the market basket algorithm MBA algorithms alumina in the algorithm and then the video cater to the canary appearance Rica Viper occur so in Ethiopia bina general arm and then the departmental store camp owning a Medina Sela products allow and Mary packed packed Allah subhana Allah the Parliament aim pictorial serpent Isle Apollo bread do octopus ill floats upon y ni notte the crown alone the bread did occur by pure political operative kawaii perk up alaya so either were MB algorithm we do data mining the app or the other base penny in the combo shell flood karana even Alberico customer participant on the history of the path through pong and the department store car honks Serena reappear I think Bonnie said say to Drogheda mary Fallin British etc and eating a different shelf lyrica the paka paka living in know erica erica man turned wrong LeClair Erica Tony Rowe so on the Mary think Poonam so dayit have a mind bunny I'm going up and run over knowledge Datagram gaining on the ecommerce website lumbering in which Gong left Flipkart Amazon isn't learning a product participant on a key left from the product add to cart could occur to the killer come on guru or scroll Mary Vinod learn enough people who bought this also bought this Abdeen or customer who bought this also bought this product of the inner liner on Ning a mobile wanting no mobile pouch Killa cam to write a Daymare even the Ning I mean the washing machine order burning in a fridge to tvl ankhila come here so it's already customers and I'm the murderer combo lay the one wrong is wrong here accompanying the one and only interest attuned one so who data with song Inari a data Sam mine pani edit the other base when younger the product and unspin wanna if one will could image come collapse he's an angle explained one idea he did I open I an interdepartmental store okay not a department store little item so Juicery kabir recreate Gurukul mill cricket diaper recuperate there occur okay hippo in order a customer see the very ko angry customers in my opinion the combo loving here can have been passing abrade the milk bread the diaper bear egg milk a diaper beer juice and then bread milk diaper beer and then bread milk diaper juice okay he politely come on a frequency a they are being birthing in india porter comparing ham bread to milk own and then when the milk hoon diaper on so you know i think i'm on over frequency low angle kana okay so for example corner with circle Milken diaper Vandana I'm among wanna do you know now there's a racial slur case Ilona okay Allah Mary up easily end up way to be run over the AB Deena other of the : Dickey diaper whenever a pariah be wrong wrong angle so you do bit in area blocks Linda Marin area funnel is your pong a better girl a true HEPA I think I'm not in the transaction is enough so departmental owner anion upon have been birthing now the bread to milking diaper impactful fluid man so are the Copernican ahuna sales Wendy no do mo not a peer group opera Bakura say that I have milk 100 bread to Hong Kong a diaper raw milk wanna so there is a chance layer so data mining put the very close it down so initial desolate data mining of the intrabody used when you turn to the buoy naked data science data prediction on the marine utam son richard but again data mine bunny are predicting something Aleya so on the prediction when the true all right so if an armed polymer a typical waking Buddha Yayoi become a cup o Allah but that depends but on the way canal route up along a product sales adding a muhahaha YP Rico Kamiya Hawaii per card so data mining or the date of a mind penny knowledge capture wandering or data or data winning among each other live in an area determining it occurring him right so no more throat a pyramid turns opposing layer Facebook lop a potent of mile are an area information angled upon layer so are they running up under the data mining do data mining so thanks for watching a to set knowledge become in the video M look pretty soon as a scraper no longer friends and colleague of our word banana so angular channel an area videos English limb family-owned area tech videos port reality piranha sorry Marion glued up in star link and then in order linked in link on the description box lurk assign are installing patina it was acknowledged twelve turning in stuff all opening uh so that any workshops and the Maria della Pugh burning a giant penny glass thanks for watching a to acknowledge dot-com 
3OhU0ZNEniQ,28,"This playlist/video has been uploaded for Marketing purposes and contains only selective videos. 

For the entire video course and code, visit [http://bit.ly/2lVMHEP].

Market Basket Analysis is the study of relationships between various products and products that are purchased together or in a series of transactions. 
• Apply market basket analysis or association rules, to understand the relationship between various variables in large database



For the latest Big Data and Business Intelligence video tutorials, please visit
http://bit.ly/1HCjJik


Find us on Facebook -- http://www.facebook.com/Packtvideo
Follow us on Twitter - http://www.twitter.com/packtvideo",2017-03-14T08:16:13Z,R Data Mining Projects : Introduction to Market Basket Analysis | packtpub.com,https://i.ytimg.com/vi/3OhU0ZNEniQ/hqdefault.jpg,Packt Video,PT12M39S,false,293,3,1,0,N/A,hello welcome to the last set of the course Market Basket analysis with groceries data in this section you're going to learn about mba its applications prerequisites modeling technique limitations and practical projects now we move on to the first video of this section introduction to market basket analysis in this video we're going to learn about mba its application assumptions modeling techniques and imitations think about a scenario from a retailer or e-commerce store manager when it comes to recommending the right products to customers product recommendation is one important domain in data mining practice product recommendation can happen in three different ways by associating customers behavior with their purchase history by associating items that are being purchased on each visit and lastly by looking at the gross sales in each category and then using the retailers past experience in this section we'll be looking at the second method of product recommendation popularly known as market basket analysis MBA also known as association rules which is by associating items purchased at transaction level and finding out the sub segments of users having similar products and hence recommending the products what do you think is MBA limited to retail and e-commerce domain only now let's think about the problems where mba or association rules can be applied to get useful insights in the health care sector particularly medical diagnosis as an example having high blood pressure and being diabetic is a common combination so it can be concluded that there are people having high blood pressure are more likely to be diabetic and vice versa hence by analyzing their prevalence of medical conditions or diseases it can be said what other diseases they may likely get in future in retail and e-commerce promotions and campaigns can be designed once a merchandiser knows the relationship between the purchase patterns of different items MBA provides insights into the relationship between various items so that product recommendations can be designed in banking and financial services also MBA can be used taking into account any number of products a bank is offering to the customers such as insurance policies mutual funds loans and credit cards is there any association between buying insurance policies and mutual funds if yes that can be explored using market bars to get allowances before recommending a product the officer or agent should verify what all products go together so that the upsell and cross-sell of financial products can be designed in the telecommunications domain analysis of customer usage and selection of product options provide insights into effective designing of promotions and cross-selling of products in unstructured data analysis particularly in text mining what words go together in which domain and how the terminologies are related and used by different people can be extracted using association rules so what is MBA Market Basket analysis is a study of relationships between various products and products that are purchased together or in a series of transactions in standard data mining literature the task of market basket analysis is to discover actionable insights in transactional databases in order to understand MBA or association rules also called a rules it's important to understand the three concepts and their importance in deciding laws support a transaction can contain a single item or set of items consider these item set x equals bread butter jam curd the support of X implies the proportion of transactions when all four items are purchased together to the total number of transactions from a database support of x equals the number of transactions involving x divided by total number of transactions confidence confidence always refers to the confidence of a rule it can be defined as confidence of X to Y hence support X Union y implies the number of transactions in the database containing both items set X and item y confidence of x to y equals support of X Union y / support of X lift lift can be defined as the proportion of observed support to expected support list of a rule X to Y support of X Union y / support of x x the support of why an association rules stand supported in a transactional database if it satisfies the minimum support and minimum confidence criteria take an example to understand the concepts well these groceries CSV data set from the Abel's library and our sort of item set x where x equals milk banana equals proportion of transactions where milk and banana brought together 4 / 6 equals 0.67 10c supporter of the x item is set to 0.67 team-wise curd the confidence of X to Y which is milk banana too portion of support X Union y / support x3 products milk banana and curd I'll perch together two times hence the confidence of the rule equals 2 / 6 / 0.67 equals 0.5 this implies that fifty percent of the transactions containing milk and banana the roller correct confidence of a rule can be interpreted as a conditional probability of purchasing y such that the item set X has already been purchased when we create too many rules in a transactional database we need to measure to rank V rules lift is a measure to rank the rules from the preceding example we can calculate the lift for X to Y where to apply NBA to understand the relationship between various variables in large databases is required to apply market basket analysis or association rules it's the simplest possible way to understand the association so we have explained various industries where the concept of association rule can be applied the practical implementation depends on the kind of data set available and the length of data available for example if you want to study the relationship between various sensors integrated in a power station plant to understand which sensors are activated at a specific temperature level very high you may not find enough data points this is because very high temperatures is a rare event and to understand the relationship between various sensors it's important to capture large data at that temperature level next we'll learn about data requirements for MBA product recommendation rules are generated from the results of the Association role model for example what if the customer going to buy next if he or she has already added a coke a chips packet and a candle to generate rules for a product recommendation we need frequent itemsets and hence the retailer can cross sell products therefore the input data format should be transactional which in real life projects sometimes happens and sometimes does not if the data is not in transaction form than a transformation needs to be done in our data frame is a representation of mixed data can we transform the data frame to transactional data so that we can apply association rules on that transformed data set this is because the algorithm requires that the input data format be transactional let's have a look at the methods to read transactions from a data frame the a rules librarian are provides a function to read transactions from a data frame this function provides two options single and basket in single format each line represents a single item however in basket format each line represents a transaction comprising item levels and separates by a comma space or tab let's create basket format transactional data creation from a data set using this block of code and then print it we can also write data using this function in order to read the file you need to first import it using this code next we're going to read transaction using this code and inspect the TR in the data set and you can see the items in a transactional form you now let's look at a single format transactional data creation from a data frame this is a line of code for that print it and you can see the output file called single format and read it and then inspect the element using this code you can see a single format data set in this code a typical piece of transactional data is received in a tabular format just like the spreadsheet format equivalent of our data frame can be converted to a transactional data format as required by the a rules input data format you assumptions prerequisites implementation of association rules for performing some Market Basket analysis is based on some assumptions here are the assumptions assume that all data is categorical there should not be any sparse data the sparsity percentage should be minimum sparsity implies a lot of cells in a data set with no values with blank cells the number of transactions required to find an interesting relationship between various products is a function of the number of items or products covered in the database in other words if you include more products with less number of transactions you'll get higher sparsity in the data set and vice versa hi the number of items you want to include in the analysis the more number of transactions you'll need to validate the rules modeling techniques there are two algorithms that are very popular for finding frequent itemsets that exists in a retail transactional database Apriori algorithm it was developed by agrawal and shreekant 1994 it considers the Brett first and provides counts of transactions eclat algorithm this was developed by Zaki edl 1997b it considers depth first it provides intersection of transactions rather than count of transactions in this section we're using the groceries CSV data set which will be used in both the algorithms let's now look at the limitations gation rules our top choice among practitioners when it comes to understanding relationships in a large transactional database they have certain inherent limitations Association rule mining is a probabilistic approach in the sense that it computes the conditional probability of a product being purchased such that a set of other items has already been purchased / added to the cart by the customer it only estimates the likelihood of buying the product the exact accuracy of the rule may or not be true association rule is a conditional probability estimation method using simple count of items as a measure association rules are not useful practically even with a high support confidence and lift this is because most of the times-tribune rules may sound genuine there is no mechanism to filter out the trivial rules from the global set of rules but using a machine learning framework the algorithm provides a large set of rules however a few of them are important and there is no automatic method to identify the number of youthful rules splendid in this video you've learned about market basket analysis AKA mba 
2VkPhNvTjKQ,22,,2020-06-22T21:02:02Z,Data Mining and Warehousing Lecture 3,https://i.ytimg.com/vi/2VkPhNvTjKQ/hqdefault.jpg,ZDS content,PT1H23M16S,false,6,0,0,0,0,this class today's class is going to be we going to measure or not remember last week we were just trying to give an overview of what data mining warehousing would look like or what these are about but today I want to narrow down into data mining and then next week I'll narrow down to data warehousing then from there now we will go into some a bit of computations on a few algorithms to just introduce us to the concepts that we need to learn then I will move from there to see how we can apply some of those concepts in our real world so that we are able to provide perspective in how these classes relevance to us and even in our career so today literally going to look at the tasks in data mining data mining process classification of data mining systems and major issues in data mining and or major issues I would want this to be covered yes it's there in our lecture but I want us to cover it in our discussion forum so if you check the discussion forum is what are the major issues and I want you not just to focus on what we covered in the lecture but just make sure that you are able to engage in a constructive you know thinking process on how what are the major issues that we are dealing with today as far as even just big data is concerned we are having quite a lot of issues just recently issues were raised around zoom as a platform and people talking a lot about security issues that are there and how you know how data can easily be compromised data about the users can easily be compromised so what are the major issues even in your sock of inference or where you work or where you are at what are the major issues that you have been able to raise that you know in one way or the other could inform our discussion around data mining processes that we have today but also around around on the insecurities that are there in the systems that we are working with okay so now let me recall what we see remember we say that data mining is an angry non-trivial process of extraction of hidden and previously unknown and potentially useful information from large databases that's something that we were defining last week and this is the definition that was coming from Coppola a book that I said I have used before so remember what we are saying is one non-trivial process of extraction of hidden and previously unknown and potentially useful information and so that in itself speaks a lot as far as or in reference to what we desire to cover even today that what is that process how does it look like where is this useful hidden information coming from which are these large databases how does an age in a data mining eating how would it look like those are issues that we may need to even put at the back of our minds as we discussed and so like I said we just looked at various concepts here and tasks is one of them and I want to start with that and just make sure that we go together so six very very key tasks that are associated with data mining so one member if we are extracting data if we extracting we extracting that data from multiple sources and that's something that we mentioned last time that we may have a transaction or database that is currently running but also that we may have you know a data warehouse we may have a social media platform some way and we are pulling that data I remember last week we had a hit on CRM we may have a CRM that is helping us to capture data from customers and all those are multiple sources that we are having we may have even arm you know a customer care support where people do savvy after they have been supported and we have seen that in many of the platforms that we used to date and so all those multiple sources come and bring data here and you can almost guess that we may have anomalies in that data we may have the vision on how different data times need in various data sources that we have and I know we will look at the mining as well but uh and and we become the I want you to look at in the book I mentioned I have posted it on our dashboard there is a whole topic on data types which is a very interesting concept that you may work on that book is available here I posted an e copy of data mining book which you can be able to fight in our dashboard the first lecture but all I'm saying is we may want also to just look at that what are the various data types that we have and how does that even inform our data mining process having various data types within within a within a within the multiple sources where we are getting data from and so what essentially we are saying here is that [Music] that anomaly detection would be one of the tasks that we want to accomplish and this is basically identification of usual of unusual sorry data records that might be interesting or data errors that require further investigation and I remember last week I mentioned a little bit about outlier and we usually talk about outlier analysis is one of key key concepts in data mining and so we just want to know this data is coming and I gave an example last time of a student unique identifier in in in z-tech we may be calling that a registration number in another University may be an admission number in another university may be a student number and if we pull data from all those sources calling add a student number differently you may actually find that the data that comes in has some anomalies but also you know the concepts of data errors that we may experience we may have data errors data entry errors which are very common to any human being and so those are concepts all those are tasks that are very key when we are looking at ads at data mining process we need to be able to detect those anomalies that exist and I want us to as we engage and move forward remember this is not you know a monologue engagement I want you even to engage and look what are those platforms that exist online right now that are trying to be able to address the issue of anomalies you know outliers today we have platforms that are able you know re alone we were saying and those who have done research know that that we actually do arrive in any one of those outliers but today they are you know new algorithms new systems that are trying to be able to incorporate outliers without necessarily having to cause should I call it distortion or just division in how we expect the normal behavior of the data to be to look like and so that's also a very interesting area that you may want to to consider any look about what you are saying anomaly detection is very very important the other thing is an addition rule Lanning and we will look at Association rule mining after this the next topic we will be looking at Association rule mining and this is basically we are trying to such the relationships that are that exist between various variables and I must say this is a very interesting area because the more the stronger the association that exists between various variables then that in one way may inform you to make a particular decision yeah so a good example would be a supermarket might gather data on customer purchasing habits this is something we were discussing last week as well so using a decision rule learning or mining the supermarket can determine determine which products are frequently bots together and use that information for marketing purposes or even how you arrange that supermarket so a concept that we call Market Basket analysis right so the issue of okay the task of associating different variables is also a key task as far as determining process is concerned so we want to be able to get this variable get another one get another one bring them here and just try to associate them okay Joyce can you hear me if you can please type in I see you're saying you are not able to gather audio if you are hearing me now please type in so that I know okay Alice I think are okay okay yes let me just continue so what we are saying here is and please pay attention because anytime I have a question I sample I sample just to get to know that you are still here so please try and pay attention to what we are saying but I yes a position remaining is a concept that will try to address as we continue but also it's a concept that we will also seek to dive deeper look at a few algorithm you know a tree algorithm FP growth algorithm and many other that even just right from when this concept was born if I may say those are some of the algorithms that came about and so if you were to look at this from a conceptual point of view then I think it's good to dig into history before we come to where we are right now okay clustering is another thing sold and this is a task of discovering groups and structures of data that are in some way or another similar okay so we can be able to try and get through maybe is anyone here who remembers and this is these are some of the things that probably we may not want to remember but I don't need anyone of us here those who are keen on mats I'm talking to scientists so he should be keen on matza you know one who remember I know best fix you are looking at in you're looking at that in physics a lot I think physics and a bit of some topics in mathematics to be honest I can't remember quite really we remember you can refresh our might now please when you you get in here mute your mic so that you don't interrupt us so having your mic on is just like having to shout when we are in class and you are right they are the back and yeah yeah shorty he doesn't work really okay so so clustering is whereby you can be able to see a particular behavior in given data that are coming from various multiple sources and there is a particular behavior that is forming that is so should I say salte similar with another data set another data set another data set and so the form sort of a cluster that is hot area throughout and so we should be very keen this is one of the tasks that we need to you know elaborate father if we look at a few clustering algorithms in there but just how can we be able to pick data from multiple sources and once we have that data in here try as much as possible to cluster that data in you know in groups of a similar behavior that is happening in the data clusters or datasets that we have and so those clusters are what would inform us on you know how the trend looks like for example or how a particular or how a particular data is behaving and that can in one way or the other inform the next move for us so if we were I'm just trying to look for a practical example this may not be some of those examples that we like hearing especially right now we have a major major challenges that are happening in the in the ruling party and when we are having elections in this country particularly we usually have a particular behavior particular behavior that we expect cluster you know a cluster of the support that is coming for a particular party particular ID video there is a stronghold and so you'd expect a cluster around the stronghold of a particular politician so you're talking of lakeside there is a particular person would expect that you know data is going to behave in a particular way if you are talking or central is all that and I am NOT I don't know what to continue from there because I may end up losing some of us here but what I mean is just that particular trend that we expect in data is also a key task that we all want to explore and that's why I'm saying some of these are actually major topics that we would want to look at in our engagement here just major topics on how can we establish clusters how can we establish cluster sets or datasets data data sets arounds that behave in a particular way what are some others algorithms that we can use we are three tasks that we can talk about his classification so we have been able to associate we have been able to plaster then how can we classify so city this data because it's happening it's it's behaving in this particular wave and it's so similar to each and every you know each and every data around this is behaving this way so we can classify this data and say this data belongs to this particular classification and that now helps us to be able to generalize come up with a structure that we can be able to apply in the new data that comes in so whenever new data comes in then we know where to channel our data because we have already formed a classification of that particular data other concepts that will be key and introducing this because I want also for YouTube my head check on them as well so other ones would be regression so attempts to find a function which models the data with the least error okay and then so which is that function or which is that algorithm that we can be able to use to model the data up here with very minimal or let me say what we call in determining the highest confidence level yeah the highest confidence level then also summarization and this is providing more compact representation of the data set including visualization and report generation so once we are done then and this summarization is actually words now the typical layman user of the system would be interested in they are not interested on which algorithm you use how you use you know what are the mathematical functions in there they may not be interested in that at the very end they are actually interested in interested to see ok so what is the outputs what are we visualize what some of the radar lots that we have in the data that we have yeah and I give an example lasts last week of just an interesting way that you know our CEA's represented very post through just visualization of how how does it look like how would it have been if we did things in a different way this is where we expected to be but this is where we are and that helps in just knowing we're on the right foot or not and that's also very important so summarization is actually very key for a decision maker we would not be interested a lot in many other you just be interested in summarization of course they may want to dig deeper into what is a process how does the process look like and what was a boat what other processes but mostly they venture on or the focus on the output for decision making process so summarization is also a key task as far as data mining is concerned otherwise if we don't have summarization then we will have we will have done a lot of work but at the very end we will realize that data that work was not used proactively to make a decision that could edify you know a company or an organization so data mining process and so we need to just get a bit of understanding of what that process would look like and I think I'll go through piece so let me just show you this diagram first I think I should just bring it up here okay yeah so and this is just to help us see the bigger perspective of what the process would be would be because probably you might live z-tech in a few few days and you may be engaged in I see this happening a lot with only these days anyone who has participated in a research process but I've seen this with the University of Nairobi for example we have an M of research called Yunus University of Nairobi enterprises and services and we do a lot of research in fact we are one of the body that received this receives direct procuring from the government as far as research survey data collection is concerned yeah and and how would that process look like so studying the problem getting to know what exactly is a problem and for each of this semester collecting the data performing some pre-processing we'll talk processing as we continue but also estimating the model operating the model but also drawing the conclusion so let me just go through this very fast and we'll see how to end this so gathering or rather stating the problem to be able to help you formulate the hypothesis either you want to to prove hypotheses wrong or right that is roughly but you need to formulate a hypothesis get to know what exactly are you trying to prove what exactly are you trying to deduce out of the data because up until when you have that then you may not even have the understanding of what data are you looking for who are the people you want to gather data for weather elements people to engage you are the stakeholders that are interested in the data that you're looking for you may not be able to understand that very well unless you actually are able to formulate their policies but also know what the problem is so in from a business perspective this could be for example yeah we are asking ourselves hard questions on why are we seeing you know new subscriptions coming in from a particular region so we we formulate an hypothesis we state the problem and the problem may not be there is AB you know there is a negative issue the problem may be just we want to clarify what are we looking at what is is it that is pushing us to do this let me pick a few people just to confirm you here bran or Doyle are you there just type yes the prop a bell are you there just type yes wells proper you there the links are you there if you're there just type yes I'm just sampling a few please type es keep proper you there ok [Music] what Joey Joey burrito are you there okay pull a Mesa idea is to to confirm that I'm not alone yeah it's possible for you to be left with spin Sierra's eye pencil oh so it just took on oh so they the hypothesis the problem but also collecting the data so the concerns this is concerned with generation and gathering of the data in general the a new distinct possibility so fast is when the data generation process is under the control of the expert with the modeler but the other possibility is when the expert cannot influence the data generation process and so it's more of an observational approach so we can have essentially at the end of the day this may also the model that you use or the data collection process that you use may actually be just informed by what is the problem that you are trying to address in here and that's also very very important okay [Music] a probe I was still looking for you I'm glad you're here it wasn't like someone called you to awake you I hope you were like okay so let me move on pre-processing data pre-processing is an another key thing and I don't even had covered rat in this yes I'm going to just talk about that in a short while on data pre-processing because that is a key thing and that is a huge determinant as far as the results of the data mining process are concerned the results that you get they have a lot of anomalies if pre-processing of the data was not done well and so anyway I will cover that a bit but it's just things to do with outlier detection and removal or adjustment that you need to make on the outliers that exists scaling and coding you know selecting various features that you want to have are important the number four is also estimating the model so selecting and implementing an appropriate data mining technique that you can be able to use now to be able to draw conclusions from the data that you have and that's what we said in our definition that we desire at the end of the day we are able to draw conclusions from the data that we are reusing as so this process may not be as straightforward as we think it mean quite a lot of you may have to employ a lot of models here and there some of the data of course it may be some of the data may be very obvious whatever conclusions you want to truly bring that very first Paul or kids what is telling you please mute your mic and the class is complaining so it's not me alone the class is complaining please mute your mic so that they are able to hear what is happening yes otherwise they may come after you so the process is may not be very straight for and we look at some of those things for example the process of the model you know the data mining eg may improve one how do we extract the data to how do we as a hit the data 3 how do we cluster the data for how do we classify the data how what are the regression algorithms we use it may be such a complex model but it just depends on the problem the problem you are handling up there that may actually be the one that it informs they the data model or this the model that you are using in the mining process and then after that then we we talk about now you know let's draw conclusions let's let's know what are we seeing what is this informing us you know and some of those things maybe this data is telling us there is a problem yeah and I have seen I have seen some companies I know we know here and you can maybe erase examples let me just pick any one of us here do you have an example of a company that you know very well we were doing very well like 10 years ago we were they they they who is who in the market and today the in systems or in one way or the other the amok area has been taken either by disruptive technologies okay we are coming thick and fast I love a disruptive technologies or it just because they ignored you know I'm seeing those examples and you you have your reasons for that I don't want to incline myself to in one of them but it's probably also ignored the technology the advancement that are coming in and today we are no longer who is who in the market some of us here when you are growing up a company called Kodak yeah was such a huge company in this country like they were the who is who in the continuing in this country because if you needed memories caps or if you needed to keep memories then you needed to get someone to take some photos in and have them whatever yeah and so so there is quite a lot and I'm seeing so many examples we have and it's good that we have all those examples but the question here is and this is a question we may need to ask ourselves here even in the spaces where we are some of you here have companies or you are running your own startup are you analyzing the markets are you clear on how the market looks like Wow okay okay very many examples are you are you alive - the fact that technology is disrupting the markets recently I have I was having a conversation I don't know if I shared that in this class but recently I was having a conversation with some people and I was telling them what if is it true me to say that my Korea is us at stick yeah and the reason why I was telling them this is because I have in this season I have just been online and I've realized so many many many companies corporates education in relining institutions that have offered courses for free so I was wondering why do I need to be here in the first place if I mean I checked one that really amazed me was how much the best university in the world offering more than I think I saw 60 something courses that you can take you are taught by a lecturer from Harvard absolutely for free and I was amazed like so if all the offered for free do we have this learning that we are having here and will I be relevant in a few years and I'm not scared about that but I'm just asking myself how can I remain relevant in the space where I am just has a steam I serve those hard questions but yes if we are in a place where we are receiving a lot of data and we are not analyzing that data to point us to how the future looks like in this day and age where technology is the driver of the economy I can bet with you that in a company that is not keen on this it's gonna be phased out by all means even if you think you're a monopoly in any organization I was I was reading I don't know if I have that book with me here actually but I'm reading a book called zero to one yes zero to one bye bye Peter I think it's a founder of PayPal and it's a very interesting analysis these nodes around startups how startup start in the end how they grow yeah and he has been able to put the cases there it's a it's an it's an interesting book you may want to read if if you are interested in just in business but I am just asking that big question on if we are not able to draw conclusions from where we are at right now and how the future looks like my friends there are you know high possibilities that the companies we are in are not gonna be relevant in the future we are those high possibilities okay so it's okay you can continually continually what I need to do data processing techniques and so here I'm talking of what are some of those free process free process you can guess what that means that before we process data we need to do some few things here that can make sure that the data we are dealing with is the right data so good example is cleaning and cleaning of data would mean that would tell you that now Paul I me I don't know how to do to dealing let me see how to deal with it I am just trying to be very very gracious not to send you away I don't know if I can be more to what we do it Paul okay now the next time your mic is making noise I realized brand is about your journey now well the next time you me the next time your mic is on I'll make you the presenter okay better pre-processing techniques so cleaning data cleaning techniques integration remember we talked of data coming from multiple sources transformation of data production into a data that can heat in the model that we have created yeah so data cleaning it's very important let me just mention that I don't know if I have it in my notes [Music] just be clear if I have that here oh yes I mean I think I'm missing that but you can check in all the other notes but what basically I was saying as far as data cleaning is concerned you would almost guess that whichever data you get it will come with some noise we talk of noise within the data okay it will come with some noise within the data and that noise if it's not dealt with we may actually end up having negative you know wrong results all or together I give an example of you know take a case in point or who do my data and someone was asked how many years how old are you yeah and they said 30 and then when someone was entering into the system we wrote 300 yeah if we were to do the average age for the data that we collected and we have 300 within that you can be sure the average is going to be something different altogether and that can affect a lot on how the data looks like at the end of the day so very important is to note that is to note that cleaning is important to help us in removing enormous you know detecting and removing different anomalies but also now once we remove those we integrate that data by just looking for example what other data types we are using how can we integrate this data type with this one so that they are able to because they represent the same the same variable you know and just be able to bring that out all together but also transform the data okay transform the data and was to go to that what that means so we've talked about integration we are getting data sources various data sources we wrap that data and we are able to integrate it into just one those are some of the issues around integration some of the themes are dances we may have various attributes named differently and that can cause reader dances schema integration and object matching could be an issue detection and resolution of data burials and the countries that exists yeah then also data transformation whereby we are able to transform data or consolidate that data into forms appropriate for mining okay so transform is just consolidating data trying to funds that are appropriate that could involve things like smoothing the data this works to remove the noise within the data and get the data in a way that can fit the model that here talking about aggregation of data is another thing and we could have various models being used there then our transformation also would mean generalization of the data normalization we have talked about normalization in databases I believe so I don't think we can talk of data base design without normalization so I expect that you have covered that somewhere so that we are able to remove some of the reader dances that would come with that data but also attribute construction this would be we have registration number on one end and student number on one end we match those two and we call them they're the admission number for example or we call them unique identifiers and those could be some of the tasks that are input in transformation um reduction which is the techniques that can be applied to obtain a reduced representation of the data set that is much smaller in volume yet closely maintains the integrity of the original data those who have done really sucks they know even if you are doing sampling and even if you are talking of you know we talk about a sample that is representative if you know that you that if you are doing research you talk of a representative sample so you can talk of six thousand students in z-tech university and your sample population is fifty students it's not representative yeah and so even if we were to reduce data we want to make sure that the data that we have still maintains the integrity but also its representative enough to be able to help us draw the conclusions that we want to draw we are so data reduction these are things that I would want you to also look at some of the strategies you can use in data reduction this could be data cube aggregation where aggregation operations are applied to the data into the data in the construction of a data cube and that it a cube will be sort of like clustering the data absolutes of such reduction dimensionality reduction and also numerosity reduction those are some of the techniques that we can use and you can dig deeper in much also I would grow through yes I can see my hour is coming to an end so I won't go through the classification of data mining because this is quite obvious you can be able to go through this it's also available on the cross form Russian of these notes so you can be able to go through this even in your discussion and just look at various classification but also the discussion that we want to have to be is around the major issues in data mining and this I I had sampled this but I thought this is more could be more relevant if we steer what we think in our species where we are working where we have been attached before you knew in our Kenyan economy in our institution what are the major issues that we may want to consider as far as data mining is concerned that could be you concerned on how we deal with particular data and you'd want you are proposing something so very constructive and are those who are not there before I can see it today we are 66 students but those who are not there before I say that discussion forums are important for evaluation they will form part of evolution in this class and so in that on the dashboard there is at the third discussion forum which is under this lecture lecture 3 and you will be able to see the question and the question basically is for you to try and bring out and let me just read that question right now just one minute so that I get back we just read the question the question is on on post on two major issues that could have a positive or negative impact to data mining process now I want you to tie that with a real case scenario yeah if you could give us a particular real case scenario of a data mining process that can happen in a real environment and then help us to Ludmila's this so so else housing is tied there the major issues I did a positive or negative on data mining process with a real environment so you may talk on social media a platform you know you may talk of where you walk you may talk of the government of Kenya you may talk of the Minister of Health right now what are the major issues that can arise in the data mining process in that space and then posts and make sure that you also reply to at least three other post era be made by your classmates and by reply what I mean is provide constructive feedback to you of your fellow students so it's not like you know don't downplay someone's opinion to Fatali yeah don't downplay someone someone's opinion let me come again for the for the for the sake of Newton so what I'm saying is on the discussion forum for today and I whatever I said is something that is there in our okay sorry someone is just looking for me here but what I was saying is whatever I'm talking about is there in our lecture three this class let me actually lean in this to lecture three so watching over time we found the lecture three of this class whatever I'm talking about it's available there so you can be able to get it there and it's basically to this class we are looking at for the net worth of data mining so posts two major issues that could have a positive or a negative impact on data mining process and what I said is tie that whatever you think is a major issue tie that with a real tight real real environment so for example we are talking of you know which are those platforms we are talking about zoom this is the most trending one right now on some of the vulnerabilities that exist and there was an analysis that was done by Safaricom on the same which I I found interesting I thought they are considering that business in my opinion but I just you know try to get a real environment that you know an entire the issue that you are talking about solely with the issue of things to do with security of data which environment would you tie that with it may be security of data as far as Kenya National Bureau statistics is concerned or as far as the government is concerned or as far as zip Achilles is concerned what are the major issues and which environment of which industry are you tying that to so give a short explanation there of two major issues but also ensure that you you reply to what other people have posted at least three other plasmids what they have posted replied to their men and give positive let me don't say positive but constructive feedback so it's not downplaying someone but it's just trying to help them think in a different way altogether and all think in a particular way or point them to a particular truth you know about what they are talking about and that would be our discussion let me know if there is any question for today's class my one however I had intended is now over I started at 25 minutes two to three so I am here for any question if someone has a question for me to address that any question yes if you want to use your mic please unmute yourself and use the mic no problem this is open for you either to type in or use your mic no problem so job can go ahead thank you thank you thank you thank you for me there's some clarification I wanted to get on my knee come again clustering and explanation is almost the same how how how how different clustering and what I think I lost you clustering and I am losing you rogue and oh okay I'm seeing your questions or be addressing but I wanted to be clear on what job is asking come again I what I'm asking is how different is a clustering from classification because I'm not getting it quite clear from the explanation that you're given on the copy of in data mining okay okay so let's let's try and be an example let me try and put this in perspective that's a good question thank you for asking but let me try to put this into perspective it is possible to have clusters classified it is possible to have clusters classified so you this group of datasets you can have actually datasets that are classified into one yeah but it is almost impossible to have classifications that are clustered yeah so we we are talking of we can be able to classify like we can see we get data from all the students in the tech university and then we cluster that data into students which is the best example students who who have had smooth what we call this transition like they have been able to progress smoothly yeah from one semester to the other so we cluster them into one group but now if we were to classify then we will name that cluster into a particular give it a name that this or rather we may say so this is the cluster of people who have had issues with progression then we dig deeper into what is the reason so the classification will be people who have issue in progression but the the clusters will be this cluster has an issue with progression because of school fees this other one has an issue because of filling this other one so that could be the biggest difference here as far as clusters and clustering and classification is concerned so we are able to look at it from you know classification is that generic form but within classification it is possible to have various clusters that exist I I would want to know if there is making sense to you job let me know if that makes sense to you I'm seeing you're typing let's go ahead okay okay thank you let me sample other questions kindly repeat the classification of data mining what I said is there are various classifications that I have provided here and what our sitting is basically this is just more or less straightforward so it's possible to have a data mining engine based on the data based technology that you are using so if you are using a relational database for example I want to maybe sample a few here so that you understand if you are using a relational database you can develop a database a data mining engine because the database that you are using either it's a transactional database or it's a a data warehouse is relational okay and so that informs the classification you can be used you can you can be able to classify data mining engine based on machine learning yeah so if there is an element of artificial intelligence in the determining eg but you are using when growth determining agent can be classified in one particular classification yes so maybe what you can do is please go through the notes both water Harvey in the lecture in the PowerPoint presentation but also what I posted on on our on our dashboard and in case you have any question Oh be open to raise either in the discussion forum or also in the next class as we continue but that's what I meant by classification that we can classify different data mining um based on is it the technology that you are using is it machine learning is it the database that you are using is it how you want to visualize data that can also dictate the age in that you want to develop for mining of that data okay and that is also very important the other thing is yes robot I can see your question please Moline clarify the purpose of data mining probably you may not have been here last time because we talked about this and I think class we were in agreement one of the things that I said is that this class is practical yeah and one one person was very quick to ask me why are we saying this class is practical are we going to be having some practical sessions here and I say the reason why I'm saying it's practical is because all that realigning here is something so relevant in the market in the industry where we are at so Mike my answer to this question robot will be that even as we continue for example we'll talk about Association rule mining and our key key analysis would be on market basket analysis which is a real real example that is being used even in in retail shops today so we'll be able to see why data mining is important but also think about it today we are talking about Big Data yeah it's an interesting area where I'm learning just yeah just learning a lot about data mining in this system but we are talking about Big Data but wherever we are even today by you logging in here I can mine a lot of data about you yeah including the back the the background noise I hear that still desire by listening and crying this video I can about who you are yeah but I can even learn a lot about behavior of a customer from the determinate so the reason why data mining would be important is we just don't want to to get data and not make use of that data yeah but we want every time we have data we are able to transform that data into a way that can make help us to make a strategic decision for the organization so we are able to see you know the examples that we give of the many organizations that are gone down if they were very strategic in how they handle the data that exists in the market segment where they operate in I am sure they would still be up and running yeah and this is amazing read the case of Google and know why they exists why they are having 92.5 market share in such aging that's where they are global yeah they do a lot of data analysis so so that's what I can answer your question that's how I can answer your question sorry robots but this is very important even for us I mean you guys who rise up into managerial positions and you may not want to make decisions based on killing you know or you know some predictions or some assumptions but you want facts on the table before you make some informed decisions so someone Jamila is saying I don't think we have that specific PDF you are using determining concepts the one the one I'm presenting if that's what you are asking it's not yet uploaded and I'm uploading it just right away but the notes I am using here are in they are uploaded in prose form but I don't have an issue with adding even this one I will upload it just right away after this good questions right there [Music] okay the deadline for posting the forum um I usually like it if we make this immediate because they are some of the concepts you have learned here or there are some things are immensely fresh yes I can give you up to the evening of so that you can get time to post the what is the connection between data analysis and data chips so data cubes is whereby data cubes are more like data clusters actually yeah they are more like data clusters so what we mean by data or or or data analysis we can analyze data cubes if that is how I can be able to differentiate the two because data cubes are like clusters within data and data analysis is done or derived from data clusters so we can cluster the data and then analyze those clusters of a done to draw conclusions from that how can we count on how can we count on data mining issues I'm not sure I'm getting your question quite well I got that question Antoni I saw it I can see you're typing but I just try to clarify about the cut it's not yet open I am actually I may actually pull it down cuz the cut was offered through a different but linked to this platform but I want the cut that we're going to be to be squared on you know offered through this platform so maybe an MC you questions but offered entirely from this platform not a link from a different platform so I may be pulling that down or I mean of that to be an assignment at some point of landing here so don't worry about that and when the cut is ready I'll make sure our will communicate appropriately of course it's there in the course outline week six so you would expect in the next three weeks they the car to be ready and next week we'll have our assignment one okay so I'm trying to tie this so how can we counter data mining issues what just happened hello my Apophis I don't know why I left I just found myself out apologies for that but I'm gonna back what I was saying I was just trying to answer the question of Antony and asking how can we deal with the problems that are faced from hm mind okay Antony that is the same question I'm asking you in the discussion forum that you have just created so I will not answer please discuss with your colleagues with your classmates and deduce answers there are so many answers we can have to that question they the issues babe challenges of data anomalies challenges of outliers the challenges of security the challenges around visualization of data there are so many answers around that and I cannot have an answer as far as that is concerned we need you're asking is lit a submission of assignment allowed please try it at your own risk so why would you ask that in the first place are you intentionally planning to be late for ascendant I have nashua those who are here for the first class I say there is something I call late submission rule in my classes which says 10% of me with a grace period of one week when you are late for assignment submission I'll give you one week grace period after one week grace period I'll start deducting 10 percent from your performance it's day that passes so yeah you can try you can try and we see how the repercussions look like okay so in case you didn't get me right I think what I'm saying is submit your assignments promptly on time yeah because if they are not submitted yeah consequences and that's what also Hassan is trying to clarify just different in nature of assignment otherwise you may wonder where the grid went at the end of the semester and you may be shocked that I deducted the assignment marks were over so I started getting by max Ramallah Sosa's any other assignment you did just to recover my max so submit on time please and and I will not follow someone so far I haven't given any assignment I'm saying I was taught that okay let me know see I haven't given they are discussion forums are have given those discussion forums form part of assignments so it's part of what you're supposed to engage in but a formal assignment will come by next week yes and obviously so if you have not participated in discussion forums please go back there and engage engage and be able to put your comments in there yeah yes I the job just to avoid mysterious grid disappearance I submit in time yeah so that's it you're not coming after me asking what happened any other question if you don't have Christian feel free you can go to the discussion forum and Amy need to take attendance by the way now you people you are so many and I'm wondering how to know who is not here as opposed to who is here please do this we are 61 of us let me know is there any other question if there isn't I will request us to transition jacoub is that my question yes I am working from home I'm in my house right now I don't know what you need Hassan I can't do that I can't mark all presents because we are 103 and only 66 on maximum who attended here so I can't do that I'm sorry um any any other any other question any other question before however now the other things anyone with the burning question least you can unmute yourself and ask before I give guideline guidance on how I'm gonna take my attendance you so let me take my attendance in a very simple way go to the discussion forum and post your comments and as you post I'll be picking the name of those who are posting and taking my attendance as we continue so I'm seeing we are 67 how comes when I talk about attendance the numbers are rising yeah so go to the discussion forum let me type here or to the discussion forum and post you are your comment I will pick a tenders from there you guys you start showing up when attendance is mentioned if if if you have a good friend who you are giving a heads up that we have just started picking attendance please that is not a good friend anyway just go to the discussion forum posts your comments and as you post I will be picking attendance from there so I maybe every forum in a short while but anyway I can even leave it here and open a new tab for attendance as you post I will be picking your names and taking you presents thank you for your time guys this is very very refreshing I feel like learning should be just like this really enjoying teaching this time than any other time before I don't know why but that's why I am let's make it fun it's also helping us to release a lot of pressure that is coming with this season so thank you guys I have had a nice time with you guys and let's do what we need to do to learn and not to be taught but to learn from this season I mean in recent also Santini sauna let me go to the attendance now I will not focus on whatever is happening in the chat box yes YouTube YouTube be safe wherever you are [Music] yeah so I am taking attendance I can see him Farnum a son Michael to become areas they sneak in man P Ricki Lake's for alley rock and then I can see your posts all those that are mentioned I am seeing you for so keep them coming and I am teaching you attendance is marching there with this Antoni gaudí in Tunica bath yeah if yeah I can see it's a I can see if you responded to my you replied to my posts no problem I will pick that as well no problem thank you thank you for that clarity 
hMRLVFHZNsU,28,"Practical Lecture 11.1 - Using Data Mining and Recommender Techniques in Clouds 👨‍💻 - Part One

Parallel & Scalable Machine Learning & Deep Learning
16 university lectures with additional practical lectures for hands-on exercises in context
University of Iceland, School of Engineering and Natural Sciences
Faculty of Industrial Engineering, Mechanical Engineering and Computer Science

Fall 2020

Course Outline

1. Cloud Computing & Big Data Introduction
2. Machine Learning Models in Clouds
3. Apache Spark for Cloud Applications
4. Virtualization & Data Center Design
5. Map-Reduce Computing Paradigm
6. Deep Learning driven by Big Data
7. Deep Learning Applications in Clouds
8. Infrastructure-As-A-Service (IAAS)
9. Platform-As-A-Service (PAAS)
10. Software-As-A-Service (SAAS)
11. Big Data Analytics & Cloud Data Mining
12. Docker & Container Management
13. OpenStack Cloud Operating System
14. Online Social Networking & Graph Databases
15. Big Data Streaming Tools & Applications
16. Epilogue

Lecture Outline

Part One: Using Data Mining Techniques in Clouds
Using Helmholtz Data Federation (HDF) Cloud with Jupyter @ JSC
Install MLxtend Library in HDF Environment via Pip Package Installer
Using Apriori Algorithm with Retail Shopping Data Example
Using FP-Growth Algorithm with Retail Shopping Data Example
Understanding Recommender Systems & Surprise Package Example

Part Two: Using Recommender Systems in Clouds
Google Colab Cloud & Movie Rental Recommendation Example
Using Collaborative Filtering Recommender & Optimization Approach
Understanding Hand-crafted vs. Automatically Learned Embeddings
Using Matrix Factorization Approaches as Simple Embedding Model
CRISP-DM Process in Context & Understanding different Phases",2020-11-16T20:17:33Z,2020 Cloud Computing and Big Data Practical Lecture 11.1 Using Data Mining & Recommender Part1 👨‍💻,https://i.ytimg.com/vi/hMRLVFHZNsU/hqdefault.jpg,Prof Dr - Ing Morris Riedel,PT40M7S,false,46,2,0,0,0,yeah welcome everyone to cloud computing and big data today we have a practical lecture um about data mining and recommended techniques in the cloud a very interesting lecture using different types of clouds but before we go into the material of this practical lecture 11-1 let us review what we really learned in the more conceptual lecture 11 that was given actually as an invited lecturer by my phd student shadi barakat if you remember we started with association rule mining and this was a pretty clear example if you think about sales or shopping data they have transaction data some call it market basket transactions and the example that we basically had as a motivation example was really this analogy always thinking about diapers and beer always go together and it's actually not the other way around which is an important consideration in association rules they are basically always in one direction there could be the case that's actually true for both but you cannot infer from one association rule that will be the other way around as well and charlie basically lifted the idea about this this was of course of people that have babies that cannot go to pubs anymore they have to buy beer to drink beer at home so that was i think the kind of pattern that we looked at this your data mining in terms of finding a pattern in the data that exists and what we're really interested in are then these two steps really so firstly we want to know what are the frequent item sets and we learned there basically two approaches to this there was the a priori algorithm which was let's say finding all item sets that are frequent and was a little bit slow but then there was also the frequent pattern growth algorithm that is a little bit more focusing on the frequent items only and having an interesting tree structure so while you see here on the bottom left that the priori is essentially now doing this frequent item sets in a quite iterative um manner and of course you learned a little bit the last time also about the support measure that you can apply to filter those um which you really then in the end not consider anymore you will find this frequent item sets and from the practical perspective of a shop you will see that often people will say in the first or let's say the second item said that's what i already knew right so i always knew that beer and diapers go together but when you go to real shop you will notice that the human mindset usually ends with having to think around corners so already when you have introduced three item sets which are frequent uh the shop owners and also the clerks in the shop usually not really know everything anymore and the reason is of course because our human mind is limited in thinking about these directions while of course many of these association rules or more or less this frequent item sets will be known uh when the more complex it gets also the more product portfolio you have of course then the more difficult it is also for the shop owners or the clerks to know all the details about these rules and the other approach that we had was then basically the frequent pattern growth algorithm that was interesting of thinking firstly about having a support and then rearranging your item basket sets so firstly of course you count the support of each of the item sets you go through it then for each item you have a certain support so how often was it actually in the mask could basket transactions appearing so in the above example you would have for instance beer would be there three times so here in the fb growth however the situation is different different that i not anymore essentially go iteratively again and again through the different frequent items that's here i directly look on the items that are shopped i do a descending order and i think the details have been discussed by shadi already but you do a decent order a descendant order of the item sets and the support and from this go back to the market basket transactions and reorder them based on the essentially maximum support which you directly start with when you think about constructing the tree so you start with a null route you're creating the masked bar in the first market basket then depending on the descending order so to speak from the support measure and by iteratively going then through them you will see that you create a tree and this notions that you see here then over time with a1 b1 will then also add up if there's another transaction having for instance a it will add to the counting here now the other interesting thing is then in the second part of fp grows we do something kind of the conditional pattern base in the frequent pattern three that it's actually is creating here and these are these interesting linked list connections where i directly know where is exactly my node link to these different items right so this of course then also means you kind of have a linked list between the notes and with this it's also getting high performance and the performance was really nicely revealed by shadi having really a script that shows the difference between our priori and fb growth i just repeating this a little bit here also out of a paper that really clearly shows with the a priori we have particularly a problem if we get up to many transactions so if you have a couple of transactions that's rather fine but if you really have to have millions of transactions and you have heard how quickly it can go in production or in real settings with shadi last time with you know path from restores data for instance it's in the millions then you already take away the message that firstly you cannot do it serial that's crazy and secondly the a priori has certain limits so you really have to go to the fp growth algorithm however have finding a fp growth algorithm that is parallel that scales is also a question mark but we will also come back to this question today now in the other part of the course you basically have seen that you could also use deep learning in a sort of way of mining product data right when you think about we had in the shop a typical clerk is facing a customer and who is asking then essentially um i want the perfume that looks like a gold bar and i don't know the name of course everybody of node knows that is paco abana i'm sure but this one doesn't know it and it appears that this is a frequent pattern as well so many people don't know the power of him they just know how it roughly looks it's a teddy bear that you have seen or it has a strange look like a hunt grenade this is even existing as well so in a sense what you could do now of saying because we have a database which essentially does not have any information about cholera shape funny enough when you go into the production databases they don't have the word gold bar looks like a gold bar inside it has lots of information about being vegan maybe has all sorts of information about different types of you know smell ingredients so to speak so lots of interesting aspects about the data but shape is largely unknown and color as well so what you could do and what shadi has shown is you you can basically train a pre-trained network and perform transfer learning which means you have an imagenet trained one that is already recognizing a lot of shapes and instead of reusing this completely we basically shifted this fully connected classification layer here and adopted it to our needs which was essentially putting shapes to different you know products we have already in the database like a gold bar but you have also seen the cocktail shaker or the red wine country so this is an automated way think about there are lots of lots of perfumes and of course you can do this in a manual way as a human but if you have a machine automatically tagging uh you know using this mining technology with deep learning um you of course gain a lot of time and with a small helper script uh that also shady was basically put alongside the deep learning framework you also can identify quickly the majority of colors in the in the um in the image right and with this of course you answer the question um you can directly ask the db the database as a clerk a gold bar and then suddenly this paco rabanne is actually showing up so the interesting thing is by being deep learning automated we get rid and this is what you see a little bit here down there you get a little bit rid of this bias which we humans have and we discuss this in the form of this for some it's actually a vaser and you see it has quite some significance of advanta here but it's also red wine so if you're red wine drinker you definitely know that's a dick and here you use it quite often but people that don't drink red wine very much they might consider this a different thing so and with this by putting more text into this database of course then you are more let's say more objective to what this actually could be than ever human manually sitting in front of the database could ever achieve the same was true with a diamond here or in some they see it actually as a cocktail shaker or in other areas so i think there was also a kind of spotlight so and and this continues really a lot of those which i better don't show you would be amazed how perfume bottles these days look like i encourage you to go into a store and have a look we have quite some interesting examples here so in a way that was really a practical lecture it showed you that these are the things we really daily apply in research projects with shop clerks with different data sets initially with pipa performs in germany but also now we're looking at basically gaming shops we have also a wine cellar that we're working with very soon and also talia bookstore so you of course you can take now these algorithms these ideas the shopping basket analysis or the deep learning to mine product tags and apply it just to different branches and domains but i'll leave it there for you to really try it out yourself because of course as you know this is a practical lecture so it will be associated with one of our assignments which is in our perspective here now the final exam assignment of the course assignment three is all about you know trying a little bit fp growth yourself and also essentially our priori you have already jupyter notebooks uploaded so you essentially can start today but also don't forget the simon 3 is reiterating on the mnist data set thinking a bit about you know using also the google co-op with this interesting example of deep learning but let us come to the practical session today i will do some data mining techniques with you in the cloud we will have a different cloud um so that you have a very broad coverage in this course which is of course a learning goal that you're not just biased with microsoft amazon or google they're also very different clouds we have seen the ebm cloud in the last lecture from shadi has an spss modeler which is quite famous actually in the data science so many people use spss modeler but however today we will use rather the helmholtz data federation that you also will use in your assignment i will um show you a little bit the way because the assignment description is relatively you know um not so extensive because essentially you have this directly listed in this lecture here right so you always can link back to practical lecture level one and know how it's working we will look on the mlx 10 library introduced by shadi already in this particular cloud environment there's a surprise it's not installed yet and one of the learning experience of this practical lecture is really this pip tool where it is a python package installer that you incredibly use often whenever you go into different cloud vendors because usually you have that a specific package is not there that you would like to have that will be from tensorflow to keras up to this mlx 10 library for using essentially data mining techniques we have a very quick review of a priori nfp grows in the retail shopping data examples and then we start in the first part really understanding recommender systems a little bit and how you could use it let's say in a more serial manner using for instance a surprise package but then in the second part we we really come to a very interesting example that is of course again in assignment 3 using the google call up environment it's also interesting in more ways than one because we use tensorflow but not for deep learning we use it for tensor operations so multi-dimensional arrays so this is of course interesting to see that these technologies can also use for different purposes and our purpose is to create a collaborative filtering recommender and we will look a little bit on this different recommender systems and how they differentiate between content based ones for instance when we look at this in detail a key to understand really the benefits of collaborative filtering recommenders is the idea of automatically learned embeddings and we will come to this and differentiate this again to handcraft feature that you have and we make an example and the implementation of this collaborative filter we have today we will see there are many of those singular value decomposition matrix factorization is just another one so you find different ones that you can use with different optimization algorithms like std that you already know stochastic gradient descent but also with alternate least squares or other algorithms we will have a very simple embedding model at the beginning to really understand what embedding really means and to also give you some practical example what you could do in industry when using these i include some consideration of the crisp dm process in context of each of the step that was already also touched on from shadi the last lecture and now we will do and go back and forth in this again and again to show you a little bit how that reflects in practice with this we fulfill a couple of promises that came out of course some of them have been addressed in lecture 11 but some of them are now also actually addressed as part of our practical lecture so when we start and you have your assignment you will see you have to use jupiter with the so-called um julie super computing center that's one of the centers where i'm working we had this already introduced before so the idea is to connect to this and when you do this then you basically have to register and i will demonstrate this a little bit here so go out of the presentation mode and go to the jupyter jc so here you see i have already some older notebooks running a data science notebook that i just maybe stop and delete here and i have to log out to demonstrate you how it works um basically when you go to this ac web page you will see two options log login and register and for you of course you should register here and you see what you can access with it you can use different super computers like jewels juraca or the deep systems you already know the modular supercomputing environment that is also there and also you will find the hdf cloud that we're going to use today with the data science so for you please then go to register once you register you come to another tool called eudor eudor is a mechanism where a jsc so the yearly supercomputing center is actually managing all the accounts so here you see i'm already registered but please register here once you register you will see you get an email address that you have to specify and once you specify it and send confirmation mail then you have to click on the email link and then you actually registered you can log in and i don't redo this i think this is almost clear from every cloud provider today so in the end we just go there and log in and essentially this is also what you see on the slides so when we come now to the idea of using then to jupiter jc you have here the register mechanism your email and what you do is when you get this email and you log in you have essentially to join the joy aml project which is my joint artificial intelligence machine learning library but you specify this joy mill and just mention student in class and what happens is that i see your requests inside this udo when i login in order to actually grant you access to our cloud systems so once this is done and you will get a notification by the system that you actually are you know accepted for this you can start to log in so how that practically looks like you can imagine i will demonstrate this again now you go here now by now you have already registered so you can simply log in and use your udo account that you actually received once you signed in obviously you got a lot of choices here we talked in already the practical lecture zero one what a jupiter lab is so it's a large environment than just jupiter notebooks but you're now inside essentially an academic cloud based on openstack that we will also reveal soon what openstack really is and when you say you want to do a new jupiter lab you have several options which reflects the supercomputers you have like deep eureka jewels but here you see hdf the helmholtz data federation cloud the image you see it's based on virtualization like in basically amazon like in ms azure you can have a spark notebook a new spark notebook which is on version two but also typical r notebook if you like tensorflow is there of course for us it's enough for using data signs right now for the examples i want to show you today and then you basically just want to start it will take some time usually a couple of you know seconds and of course this could be also sometimes that it hangs right or that it actually doesn't work for some reason so my suggestion is maybe come either back in 20 minutes or come back basically the next day there could be just maintenance be there this is just let's say still relatively new service that we offer here in the academic environment which is of course free of cost and that makes it attractive if you consider aws and the services of course that we always discussed so being inside the the jupiter lab here and the jupiter notebooks that you can offer you have seen here different aspects first of all you can quickly go to our hpc systems as is expected right because we are hpc center but what you also see is b to drop that should ring some bells if you remember for the european open science cloud which is one of the implementation of an academic dropbox to share data with others so you see this is an interesting cloud and the most things and just talking here are of course also basically here on the slides note that of course openstack i will provide much more information on this in lecture 13 once we come to this lecture now there's a trouble if you go to this cloud and this is not really a big trouble because you will notice that wherever you go you usually have a chance to install things if you use for instance aws ami you know the image for machine learning you can use these commands and wherever you go usually if you don't have a sas setting right in ers or past settings you usually have the possibility to also install packages and this is just an example that you also have to do in your assignment um that you have to pip install this ml extend library so i will actually shortly demonstrate this so essentially before you do anything in the notebooks using yulia r python or something like this do the terminal which essentially gives you an ssh login you notice you don't even have a key to go there in i just had my username password and from here you essentially on the unix level inside the cloud see lots of interesting aspects related again to be to drop what you also see on the left hand side a bit more beautiful as a click and gui option but what we want to do is essentially just say clear and pip install ml extend and essentially this is also what you see here on the screen so i don't really do something new here you will sometimes notice that this could take time so essentially saying that the first attempts maybe to download it will fail some of them will work a little bit similar like the attempts you have in some of the assignment that in the first time it doesn't work so sometimes you just have to wait you see here for instance we apparently had a retry for some reason there was a timeout error there could be different reasons for it because we are in a very protective environment so but you see also that it should work here we have a successfully installed ml extend and this is the let's say engine behind basically we are still in the jupiter just one level below on the operating system level where we installed the package now the beautiful thing is we have it now available also in the jupiter space and what we have to do is what you know already from the other assignments um essentially upload a jupyter notebook right and this is what you have already in our case we will use here the a priori notebook that is also uploaded to canvas um i open this of course you will see a very let's say small idea of what also shady was presenting with diapers beer bread and cheese and sometimes of course nothing in the shopping basket anymore but i don't want to go too much in the functionality you want should exploit this when you do the your assignment and make some snapshots basically that i also see that it's you you can always see that it's you basically here on the uri so please if you are doing your assignments make the screenshot so that i also see that's uniquely you you notice also something we needed retail data set so this is of course something which is also already available to you and which needs to be uploaded too so essentially having this retail pop-up data set is a csv comma separated file that we also have here inside and this is of course actually using this now essentially we have a python 3 that is fine here we have the mlx 10 library so what we have to do is a little bit you know maybe clear all the output you can do this by actually going here to edit and clear all outputs you see it's still my previous run there and then you do the shift enter as you know usually from your other assignments or from your other work you read the data set we do essentially ahead here just want to know how it looks like and you can consider is just was also shady and i was in the let's say review of last lecture was alluding to that this is just essentially a typical way of having a shopit basket with bread wine eggs meat cheese and so on and then basically have a different ones line by line here we basically transform this and we'll then look a little bit on the uniqueness it is a data understanding inspection case as you also know from our other assignments that usually what you do we will do a one-hot encoding of this this is a preparation for the algorithms that you know requires in the implementation of ml extend a certain data structure we did nothing else and having this comma list now from bread comma cheese comma meat put into where it actually is is actually to one when it really appears in the shopping basket or zero if it doesn't appear so within essentially it's the same information and then when we took the a priori admittably it's a very very simple data set right so here you see um the support and for the different item sets so how often they actually appear and how many percent of the of all the shopping baskets and then we learned this is the first step you have to do for association role mining is identifying the frequent item sets but what is the second part of it is really then forming rules out of this and of course generating the strong association rules must satisfy certain constraint like a minimum support or minimum confidence things which also shady was alluding to and from this you can essentially here as an example um the bagel will often be you know then bought with braid or someone who buys bagel also buys bread to relatively high confidence here that you have um it's not soft supported so it happens not so often but still you have basically one of these association rules and now when you apply or play around with this you can have different minimum thresholds and this is of course a hyper parameter so to speak you have to work on uh the same as lift leverage which is a little bit the idea of the assignment look behind these words look a little bit what it means and make a screenshots basically related where i also see that you really executed this and you're already done with the assignment of course you're free to play around they're bigger retail data sets in the web so i'm i welcome you to download them you play around with it use it and of course we can have also office hours discussing it so this is really the a priori you have seen essentially everything but also shady was already presenting and just to go back a little bit to the slides for you as a documentation right so um essentially everything i just demonstrated is also in the slides that you can always refer to it we basically have to go to the jupiter when we are in the jupiter we extol the mlx 10 when we are there and can then actually have just a python notebook to be used then we can already use the ml extent you have seen it executing once i did this basically i have to think about that of course also the data needs to be uploaded right this is important and then you have here all the steps with a one hot encoding until you also have the configuration parameter that you really can you know fine tune and this is what how it usually likes when you do it in practice so in a way you have tons of rules which suddenly come out of there so a shopping clerk or shopping boss or whoever owns the store is not interested in that that too many of these rules so you have to pick let's say in the first discussion maybe the top 20 rules discuss it with them and then actually maybe in a field test in a flat store flagship store really try it out and monitor it over three months to show that the sales really increase if you do certain tricks with it for instance putting it on different side of the store so the longest path through the store one of the main ideas of this is to keep the customer as long as possible on the shopping area i know it sounds strange to you because we as customers we want the opposite we want to have everything maybe in one corner and go out as quickly as we can but of course chances are if you move around the store to collect a couple of other items such as sweets and stuff like that and when we now come to the fb gross algorithm as a demonstration it's really no big other surprise we load the ifp gross notebook instead of using however already a kind of retail dataset that exists i just wanted to demonstrate you a case where firstly we use mlx10 again of course but you can also create your own shopping baskets very easily using essentially pandas and data sets here you see we have your data set where we create our own shopping baskets here and you can nicely already with a transaction encoder which is already provided by mlx10 uh transform this to a data frame that is also then supported oops now i did this error here not executing this before as i said in earlier practical lectures you always have to make sure that you go from top to bottom you have seen what happens if i miss out to define the data set then i have no data set to transform or do doing transaction encoding with it so the transaction encoding you see in a way is again the zero and one here encoded as false and true so also there no big difference and basically again you see now the fb gross algorithm is perhaps coming to different items to different support and to different solutions a little bit and also in a much faster pace because we have the three algorithm i was alluding to so now if you load a bigger retail example that you find on the web for instance and loaded with fpgrows you will see considerable differences the bigger the data set is if you compare our priori with the fp gross algorithm but in a way that's not really what i what i really require in the assignment of course it would help to see that you really used it but if you want for the assignment it's enough to have it here executed and make some screenshots where i see here on the top with you you id you really executed this and it was working you may play around a little bit here with the minimum support and see that there will be several difference if you execute it depending on the idea of how often it's really then again represented in the in the shopping baskets and this really um is all i wanted to basically present in the beginning here um when i think about really having a practical demo in the second part of the lecture we will come back of course to another google cloud environment to do another demonstration but what i also found important for you is to say the following in the remaining mixture lecture parts here of the first course hours think about that data mining statistics and machine learning are really hugely overlapping everything which is data science today really don't care very much if it's statistics data mining or machine learning you would go with one operation that you just find suitable and we see this especially in the second part of the lecture we every time go back as machine learners in the toolbox of statisticians you go back as a data mining and use statistics and and so forth so essentially all of this is in this data science field really overlapping and today what's really also mainstream is the requirement for computing and one example you had already when i think about here the the kaminsk clustering which in in one ways some people would say it's a machine learning algorithm it's an unsupervised machine learning algorithm others would say clustering belongs to the world of data mining especially k-means clustering so this is really overlapping and and in the way you don't care if you have a given problem at hand you search for the algorithm you have to basically support and implement a problem now when you see another example that we will go in the second part of the lecture much more deeply into is the recommendation engine so using collaborator filtering we are google collab i was already showing you this which is quite interesting because in a way you you data mine the pattern of this viewing that is inherent in the so-called rating matrix what is a rating matrix you can quickly see that there are lots of movies that you see here on the bottom and of course you have several customers like from netflix or amazon prime and they have a legend of you know giving different ratings which gives you a customer id to a specific movie id and then performs a rating what you get out of this essentially is then a large when you basically sum up all the different ones in you know the kind of structure you see here you have all the movies around here which we call also items the movies and then also the users and you have a rating matrix and here the idea of collaborative filtering is really not to say what would other people would have looked it is rather to recommend you what other movies are out there what similar personalities than i have have looked so it's much stronger much more precise to the point and we will come back to this in the second part of the lecture so it's another form of data mining using this recommendation systems and there are actually two of them one is rather content based right where the personality is unimportant that looks it it's just basically saying uh the text in the database so cowboy movie has certain cowboy gene tech associated to it a little bit like we had also the gold bar in the perfume example it's basically a database lookup in the sense you don't really have a large machine learning system necessarily much different is the collaborative filtering based or customer based recommendation systems here you look and really identify other customers first which are very similar to yourself looking in the movies you looked before and those they looked and merge and have a similarity around this and then of course recommend those that the similar personality has looked because it has high chances that you really like it and the different algorithms for it um singular value decomposition matrix factorization is the one i want to leave here on the table for you in the second part of the lecture but let us review what it really means to do this calibrated filtering now by having some approach review here you see essentially here a sports guy who is in a way very similar even if he is just let's say a runner than someone who is you know doing a bicycle run still both having you know want to have some pizza every now and then but also some salad because of sports guys and this is very true maybe also for the bicycle guy but but they are very similar sports guys and they maybe drink some cola but the other guy didn't really drink cola yet so if you now basically say after a long ride on the bike um you really basically recommended him there's high chances because the personality is similar so this is the general idea about this the the challenge is they're very different algorithms for the rankings that you have you can imagine that the rankings you have the ratings are all differently encoded some are very sparse in movies for example chances are that you as a user didn't have seen all netflix you just have seen ten or five percent even if of all available material so essentially a phase in this matrix that i would alluding to was a ratings a very sparse matrix and you have to work with algorithms counter basically these biases and we will see how that materialized when we have the second part of the lecture today so the essentially two different techniques you would have memory waste and also model based techniques um we focus here a little bit on the model based ones which are again even neural nets that you can use but also things like single evaluate decrement decomposition or matrix factorization um this is quite nice because they work with so-called dimensionality reduction and which is good so they scale of course and actually computationally are then advantage but of course what the disadvantage is is that you can hardly explain that sometimes you have a so called embedded feature structure and we will learn in the second part of the course today what is this embedding so you learn certain hidden latent features in the data which is very hard to explain and say basically you like the caller because this was 0.57 in your case so the explainability is of course here the question but in a way industry doesn't matter because if you buy it if you buy the recommended products that's all what they want again think about that they're not really interested several times in causality they're interested just you know that it works when we think about famous data set examples is of course like and unlike famous examples are in retail that basically if you have the same opinion you really can see on the right-hand example you can really predict when the likes and dislikes of certain products are very similar and let's assume you have that not with this four products here you would have that over a span maybe of 10 000 products and you find interesting personalities that on a subset really like the same things like books for instance is a very common practice um then you you have a very nice way how to use collaborative filtering with this and of course the challenge is here the computing again you know in really production settings in industry these are billions of transactions to look through to organize and of course unfortunately also what we see in some of the practices the ratings doesn't exist but this is not always the biggest problem because in a way you can say buy or not buy is already a rating of zero and one but of course these algorithms perform best if they have a scale if they would have a notion of one to five for instance of ratings if you you know rate the product and not just yes and no to buy now this is an enormous potential for all shop owners to do cross-selling of products um and of course cloud computing really can help there a lot because it's computational very expensive if you want to try it out yourself a little bit you see here the one example of this factor matrix factorization algorithms is svd and it's very nicely supported here in the right hand side that i use really in production in some of the stores with a surprise package so if you have data sets which are not too large actually you can use that and let's say play around a little bit with this the as soon as it grows of course you have to think more about the distributed setup and this is also what we will do when we really use cloud computing in the second part also in the second part just as a refresher here's a systematic process that we will come back in the second course hours again and again we will actually go through this process to show you it has really practical significance and also see a little bit where the learning takes place and why we have to jump back often from modeling to evaluation and back to modeling then using another model or regularization and then back to evaluation so in a sense you have a really flexible move through all of these different phases here and this is really for the first part all i wanted to say today so we break here and continue in 10 minutes 
1Mq0CplM4mk,22,MSBI - SSIS - Processing Cubes And Data Mining Query Task - Part-43,2016-10-02T16:50:15Z,MSBI - SSIS - Processing Cubes And Data Mining Query Task - Part-43,https://i.ytimg.com/vi/1Mq0CplM4mk/hqdefault.jpg,Learn MSBI Beginners/Experts Level,PT9M22S,false,928,4,0,0,0,so effectively let me see if I can help out here I know this was a bit confusing we and I know this is gonna flash the yellow circles and I'm sorry about that with two things going on here okay so we have the relational data warehouse and that's inside of sequel server and then we have the cube and that is inside of analysis services and so when we deployed the cube inside of Visual Studio it created the database NS SAS created the cube inside the database and then it populated the cube oops here based on queries that it R it executed against the relational data warehouse okay so that's how the cube was populated okay so we call this a processing the cube okay so think of the cube is empty and when you process the cube it's going to select all of the rows from the relational data warehouse and perform all of the aggregations now that's the initial process later on subsequent processes may just be getting the changed rows that have changed since the last process right so let's give it something to change and this is where this script comes in so this is where the myths taught me so one SQL what we're going to do is insert an order for product key 5:59 so we're adding a product key 5:59 so that now when we come back and say show me the sales for 5:59 in the sequel server there is now an order maybe this will help you but look the data warehouse has not been updated the data warehouse still shows no orders now if we've changed this filter to be 560 take out 559 you can see that there have been 81 orders of products 560 but when we go to 559 there are no orders so the cube is now different from the actual relational data warehouse to update the queue we must process the cube processing the cube runs those queries to update the cube awesome here we go back to integration services now I know it's difficult to flip between these three environments here but so I'm back over here in integration services and let's drag now the a processing task the analysis service is processing make my connection here so it picked up the connection that I had previously set from the DDL task you could make a new one and what is it that you would like to process well I want a process and a real data warehouse you might have many different cubes I noticed we can just process these measures that's what those are down there but I'm gonna process the entire database here okay there we go now I can change the settings so if you understand all of these settings great if you don't don't worry about it right now this is just a high level overview of here's what it is here's what it does here's how to use it we're just gonna say okay and I say okay and I execute the task and right now if we take and use profiler sequel server profiler maybe we'll catch it maybe we won't and we hook this up to our missed it because you saw it turned green there but if we hooked this up to our sequel server so a new trace of the database engine and let's run this again by the way if we go back over here and refresh now you the current session is no longer valid due to structural changes in the database thank you very much awesome let's go back into the cube browser again so that I can show you this order quantity and the extended amount and again same filter we did before key is that what that is yeah and 559 was it I believe there it is you see that the data warehouse now shows that it is in sync with the relational data warehouse okay because we processed it okay now back into the profiler I'm running this against the relational database the adventureworks 2008 DW and i want to show you what happens when we process the cube so I'm going to come back into integration services and I'm going to reprocess the cube so that it picks up any new changes so we execute it take a look over here in profiler look what it's doing it's grabbing all of these queries and it's executing all of those queries against that relational data warehouse and then it's synchronizing the results that it gets with the actual cube so you can actually see exactly what it's doing and this may not be efficient this may not be what you want notice there's no where clause on any of these you might have a where clause you might have specific things that you want to get so there are more things that you can control but it does show you what's actually happening inside the sequel server when we process a cube now one final thing about this let me just show you what would happen here if we go now to the back to sequel server back in the adventureworks DW and delete that same row we just added ok so if we take a look we've now removed it from the actual data warehouse let's rebreather cube again has it changed okay so let's come back over here I hate that it's I have to redefine that each time let's see come back over here and again product key scroll scrolls almost there okay but notice that the data warehouse still shows it bonus question why does the data warehouse show that we still have that row how can we fix it we have to process the cube right we haven't processed the cube which goes and gets the changed data so we come back to integration services and we process the cube so that's our processing task there once we process it probably going to have to redo this again probably won't be able to refresh I won't can't do that so I'm just gonna have to do it again just drag the whole thing down there product key where are you right here and you can see it's now gone okay so now there are no rows so it's been processed and now it's up to date alright so we're gonna go into much more detail about the processing tasks in chapter 6 so let's now move on to the third one here the data mining ok so the data mining query tasks here we're really not going to cover too much in this course because it does require you have to have a mining model set up here the purpose here of working with the data mining query test notice that you can build the query you can have parameters and you're going to output a table so that's the purpose of this is you're going to have the output of a table that you're going to store the results of running the data mining query task data mining is very expensive on your server so to be able to store this as an output table for later processing is going to save you a lot of time this is a pretty advanced task this is one where you really need to understand what data mining models are which algorithm that you need to use there's a lot a lot more to it that we're going to then we're going to be able to cover in just an integration service as task video you 
iw1VlfkTOKw,27,"- Predictive modeling
- Database Segmentation
- Link analysis
- Deviation detection",2020-12-18T00:09:18Z,Chapter 7 - Data Mining Basics_PART2,https://i.ytimg.com/vi/iw1VlfkTOKw/hqdefault.jpg,skema lecturer,PT32M6S,false,56,11,0,0,0,[Music] good morning good day everyone welcome back to my channel this is asm654 on the dataway house we continue on the last uh video on chapter seven on the data mining okay now we on this uh second part on this video we've gotta focus on these uh mining techniques or the mining process okay remember last time i asked you to read on this book okay actually not many on this uh predictive modeling so we have to rely on the slides so as you can see this mining process uh this predative modeling is on under the mining process it's on the market analysis okay we got this technique is the cluster detection decision tree uh also the link analysis and generate genetic algorithm so some of these mining techniques we will cover in the slides later on all right go to this one predictive modeling similar to the human learning experience i use observation to form a model of the important characteristic of some phenomenon so yeah it's kind of like a model when you use a generalization of real world and ability to fit new data into general frameworks so you can analyze database to determine essential characteristics about the data set and model is developed using a supervised learning approach which has two phases which the first one is training and the second one is about the testing on a training builds a model using a large sample of historical data called a training set i can say this training more like you run [Music] dummy database content that have this uh historical previous data from your uh maybe a sale scale maybe on this whatever what whatever your company have for for the data that you store and the second process uh the second phase sorry testing involved trying out the model on new and previously unseenata to determine its accuracy and physical performance characteristics so a lot of trial and error on these ones okay on this predictive modeling application of predictive only include customer retention management uh query approval cross selling and direct marketing that's why this predictive modeling is under the as you can see on the market analysis okay on the customer credit cross selling and direct marketing okay two technique associated with predictive modeling classifications number one classification b is a value prediction uh distinguished by nature of the variable predict predicted okay statistical analysis of actual sales uh example dollar and the quantities relative to the signage variable a predictive model example okay we got this uh value content frequency depth focus so we're gonna look on the analysis of the correlational regressions experiment design uh automatization of the process and now it goes into the real-time analysis okay on the predictive modeling there are two techniques associated with the predictive modeling classification which is uh classification value prediction okay which are distinguished by the nature of variable being predicted if you gonna predict management so used to establish a specific uh predetermined class for each record in the database from a finite finite finite finite 35 net sets of possible class values two specialization of classification is three induction and neural induction okay i think this one you can see more clearly this is an example of uh classification using the three induction why they call it three because they got like a branch learning okay this one customer renting property two years is it yes or no if no yes they're gonna rent the property uh if yes is it the customer age another condition is it they're more than 45 years old so if no rent property if yes you're going to buy the property example like this one on the new only using three induction this one and neural induction we just have got two three induction and neural induction as you can see neural induction here more on the relationship on between of the uh variables this one okay examples of using your induction each process unit or the circle this one unit they call it uh um processing unit last one okay like uh use cases uh in one layer it's connected to each processing unit in the next area by weight weighted value so expressing the strength of the relationship so the network attempts to mirror the way the human brain works in recognizing pattern by automatically combining all the variables with the given data point in this way it is possible to develop non-linear predictive model that learn by studying combination of variable and how different combination of variable affect difference data set you can say this one like i think you have seen this one maybe some more or because if you read some general paper you may encounter this kind of model this one is structural structural equation modeling okay um as you can see there's some value here like this one this one i can say about the uh engagement and the job demand burn out of banyan so as you can see there's some value here 0.93 0.59 that the relationship of this value weighted to another uh like say job resources to the uh word resources is 0. 0.9 close to 1 close to 1 is meaning that it should be more related with each other on the o of this unit all right okay all right uh used to estimate on the value prediction the neural induction this one we go to another one on the value prediction used to estimate the continuous numeric value that is associated with the database record okay we're gonna compare with the database record that we have is the traditional statistical technique of linear regressions and non uh linear regression so they go for this multiple regression currently a relationship between of the units relatively easy to use and understand because on this regression all this one statistical we got some uh value measure meaning that if more than 0.5 the value meaning that a strong regression uh if below then 0.5 is low strength connection meaning that there is no connection between that two units that kind of situation okay our linear regression attempt to fit a straight line through a plot of the data such that timeline is the best representation of the average of all observation at that point in the plot because linear and some so the problem is that the technique only works well with linear data and sensitive to the presence of outliers alliance example data values which do not conform to the expected norm i think on the research you can ask the lecturers about these yes outliers on this linear data you can get more information over there so this one just more on the introductions okay although non-linear regression avoid the main problem of linear regression so still not flexible enough to handle all the possible shape of the data plot okay um yes event even that that data is not linear we can use this non-linear regression but still say here is say here that it's not enough to handle all the possible shapes meaning all for the possible coordination or interrelation between the units so the statistical measurements are fine for building linear model that describe predictable data point however most data is not linear in nature so that's the problem of our data sets or data that we information that we store in the database okay um another one under the failure radiation data many require statistical method that can accommodate non-linearity outliers and non-numeric data should be determining cover all these kind of situations so application of value prediction include credit card fraud detections or target mailing list identification okay this sum of examples are on the value prediction meaning that yes as i said on the last video if some somehow something happens to your credit card uh the bank actually can detect some of your activities uh okay activities of your purchasing uh using your credit card all right now we go to the study value variation now it's on the database segmentation this one is aims to practice partition a database into unknown number of segment or cluster of similar record so use unsupervised learning to discover homogeneous sub-population in the database to improve the accuracy of the profile [Music] less precise than other operation thus less sensitive to redundant and irrelevant features so sensitivity can reduce by ignoring a subset of the attribute that describe each instance or by assigning a waiting factor to each variable and the application of database limitation include the customer profiling okay we can we want to distinguish between the customers direct marketing we may focus on marketing for that customer that maybe have a previous history purchased with us on the last time and on cost of the cross-selling meaning we want to introduce more product or sell more product to the customer okay this one example of database limitation using the scatter plot okay they're gonna plot like this i think you notice that on the last time we have these uh trend analysis on the way card okay they have this uh plot like this okay so if okay this one agenda with the payments and also the type of the customer so look at the line here okay male or female this one is about the debit card or vista so we can see the trend okay this one no don't have like a chat roughly move to the this one database cementation associated with demographic or neural clustering technique distinguish by allowable data input method used to calculate that's the distance between record and presentation of the resulting segment for analysis you want to see differences between of the units okay we can have like this one also the database segmentation um using a visualization okay this is more advanced software that we have to get to produce this kind of visualization because normally we using a standard bar chart pie chart which is we can extract from the excel and others application but this one thing you have to get more software i think sap got this one okay link analysis study on the visualization and segmentation we got the link analysis aims to establish link association between records or set of records in the database there are three specialization association discovery sequential pattern discovery and similar time sequence discovery so all these applications include the product affinity analysis and direct marketing and the stock price movement meaning that we want to do some uh future analysis like this one stock price moment okay pretty diff something and find items that imply the presence of items in the same event and affinities between items are represented by association rule example when customer runs property for more than two years and is more than 25 year old in 40 percent of the cases customer will buy a property and association happens in 35 percent of all customers who ran the properties so this is from the previous historical data that they get so they can use this um these kinds of situation condition to offer to the other new customers in the future okay find pattern between events such that presence of one set of item is followed by another set of items in a database of event over period of a time so example used to understand long term customer buying behavior so we can predict what in the future that our customer most likely to buy or something that we can market to them so we can increase our sales and profits finally between two sets of data that are time dependent this one on the similar time sequence discovery sorry is a sequential pattern discovery this one ssh discovery okay sorry uh so they go to one by one on this uh 33 specialization okay uh last one is on this uh similar time sequence discovery find links between two sets of data that are time dependent and is based on the degree of similarity between the pattern that both time series uh demonstrate within three months of buying property new home owner will purchase goods such as cooker freezer and washing machine same as like last time on this one almost the same association okay meaning that it comes together for this uh specific customer okay it's law you can uh this one comes later on once they have buy the property okay because maybe double dude believe this uh rumagan so have to wait for another salary to come or another budget then the next month they will buy uh another goods for their new home okay okay we got the division detection relatively new operation in term of commercially available data mining tool okay often a source of true discovery because it did identify outliers which expressed deviation from some previously known expectation and norms this one i can say is another uh data mining for your model deviation detection okay uh often source of true discovery because identify alliance which as per the deviation from previously known aspiration and norm we want to describe significant changes in the data from the previous measure or nomatic value that we get from our database okay relatively new operation in terms of study can be performed using statistic visualization technique or as by a product of data mining so this one application includes fraudulation also in the use of credit cards and insurance claims and the quality control and defects of tracing okay a summary of the data driven technique we got the data visualization decision three clustering uh factor analysis neural association rule and role indication role induction okay this one based on the sakura units from the tower house i think this book can be using it here you have to check again this one we just continue to finish this slide okay data visualization is most important part on our data mining because whatever you have analysis analyze you have to produce the [Music] graph or something that can represent what the output of your analysis so a pie chart showing the cell product by region sometimes much more effective than presenting the same data in a text or tabular form so this one is like more interactive and more easy to digest picture tell more than [Music] decision three okay yes or no continuous clustering analysis study first segment yeah mostly quality clustering they're using a color to differentiate between of the units or the values all right we go to the next one of factor analysis unlike cluster analysis factor analysis built a model from data so the technified sunderland factory also called lantern variables and provides model for this factor based on the variable in the data for example software company is considering a survey to find out the nine most expressive perceived attribute of one of their product they might categorize this product of categories such as services such as service for technical support availability for training and and health system so fighter ness is used for grouping together product based on a similarity of buying patents so that when they when bundled several product s1 to sell them together at a lower price then they are at the industrial prices okay factory meaning that another factor comes to make that thing sound stronger or more highlighted okay this one is not a neural network i think this is redundant slightly which is we can have seen first one teddy goes to the back on the association rule it's a model essentially models are models that examine the extent to which value of one field depends on or uh produced by so values of another field this model are often referred to as marx get market based analysis when they are applied to retail industry to study the buying pattern of this customer so especially in the glossary and retail stores that issue their own credit cards charging against these cards given the store the chance to associate the purchase of customers with their identities which allow them to study association among the other things so they want to see the connections okay what the customer have uh what the customer or these people have interested on okay connection again rose induction is a powerful technique that involves a large number of rules using the set of if then okay some of the condition over there statement in the proceed of all possible pattern in the data set for example if the customer is a male then if he is between 30 to 40 years age and his income is less than 50 000 or more than 22 he is likely to be driving a car that was bought as new okay because you can see the trend or something that have been uh in the puzzler okay that we have recorded in the past okay summary for the theory driven technique is the correlation t tests analysis of variables linear regression logistic correlation discriminative analysis forecasting method i think this t-test is included in our then we have in the we call minority tester [Music] is also include so we try to relate this one to what to our group project on okay to be to have a better understanding on your uh lesson for this kind of this topic sorry okay on the data mining process define a problem select the data prefer the data mine the data deploy the model meaning that using several kind of study driven model technique on this uh trend up analysis uh take business actions okay once you got these uh results you got the outcome of your testing or analysis you take to the real world on the wizard action so yes are you ready for the terminal define the problem accessible data mining initiative always start with a well-defined project to ensure that the project produce incremental value include an assessment of the status quo solution and review of technology organization and business process that is what about the you want to achieve for these data mining projects okay well this one more on the summary explanation the last last line on the select data this step involves defining your data source not every data source are record in the record is required the data is usually extracted from the source system to spread server so we just select what kind of data that related to our analysis not all the data that we use okay we can do some of the dummy database so that we can test the uh the results of the technique too is it better or not to the actual later on for the actual testing no testing or the actual analysis oh yeah okay on the prepaid data this step represents up to 80 of the total project efforts that's why you have to uh get the big data on the data set so for determining the data must recite one flat table each record has many columns in addition to being the most time consuming the step is also the most critical resulting model are only as good as the data used to create them so yes your data have to be all this standardized format okay meaning that you have been you have been segmentized segmentation uh you have uh have all the same different of each categories okay so that later on is easy for you to analyze the data on a minor data typically easiest and shortest path this step involves applying statistical and ai tool artificial intelligent tool to create a mathematical model yes as you as i showed you you can jump stretch your equation is a software you just can use yes this number is come thumbs up when you get in your responding result into your uh this software okay the terminal typically occurs on the server separate from data tower house and other corporation system so we have another specification for the data mining okay deploy the model down the deployment is the process of implementing the my technical model into operational system to improve the business result okay meaning that we use the result that we got from our analysis to the actual business uh real time okay actual business world okay thank goodness i can use the diploma there to achieve improve result to their business problem identify identify at the beginning of the process what we have uh want to tackle on the projects and of course this is the step flow of this implement data mining discover the pattern relationship association get the knowledge information model validation and deployment data mining versus all lab there are two separate bits of analysis with entirely different objective not to mention two skill set and implementation method as i said to you these data mining want to discover new uh information new knowledge okay compared to all of this is existing uh information or data that we can extract from the database okay this one we can report can important young pre-formatted report distribute the whole organization or specifically define user group this type of report are traditionally scheduled for delivery at specific time of the day or accessible anytime to a web portal okay like this one it's not like ad hoc is is that more simple mostly in one two piece together okay that shows all the uh this one data visualization on pie chart graph and bar chart okay add hot curing and all up the end user defined hypothesis and determine which data is when which determining the tool identified the hypothesis and it actually tells the user where is where in the data to start exploration process rather than using sql to filter out values and methodically reduce the data into a consensus set data mining use algorithm.x positively review the relationship among data elements to determine if any patent exists so the whole purpose of data money is to yield new business information that a business person can act on okay uh i think you just read this one again this is what we have covered on the data mining tool just now this is all about the olap tool all right for the conclusion okay artificial neural networks non-linear predictive model that learn to training and resemble biological neural network in this structure decision three is about the three shaped structure that represents set of decision decision generate rules for the class future database generate algorithm optimization technique that use process such as generic combination rotation natural selection in a design base on the concept of revolution and role induction extraction of useful if they rule if then rule from the database of on this statistical significance and the last one yes it's about the determining techniques predictive modeling database segmentation link analysis deviation technique so we got all these uh things that we have covered just now all right you can try to read the books to get better understanding this one but not many are different also got decision trees okay this bought the decision three leggy okay try to read this one uh thing is you get a better understanding okay that's all for today um try to make a note for this one uh mostly for this quest question for this chapter seven is about the theoretical questionnaire okay no worries still that sort of you all do this kind of neural network when you picturing on it this one is on the level okay thanks for watching see you on the last chapter thank you goodbye 
a6-70LF8pTs,22,"Data mining, cloud computing",2020-05-05T04:02:12Z,Data mining lecture by Varsha Singh jayoti Vidyapith women's university,https://i.ytimg.com/vi/a6-70LF8pTs/hqdefault.jpg,Jayoti VIdyapeeth- Engineering,PT4M16S,false,32,3,0,0,5,hello everyone our today's topic is data mining so what is data mining data mining is basically database plus AI is equal to determining that means we have some data base and we analyze how to increase and what have to increase in data base with the help of AI is called data mining so mining means discovery and data means knowledge that means to extract relevant information from data is called data mining the first thing which is to our mind so this is the and it is and so it has that means in the like [Music] definition of mining is used now what is this tips the first the first step in determining is data selection which means relevant data is selected from various sources the second is data pre-processing data pre-processing means consistent state removal of unnecessary information that means we have collection of data and necessary information the third one is data transformation data transformation means suitable format and the fourth one is data mining data mining is done using some algorithm just to extract giving the shape of your last knowledge presentation that means data visualization we will see here is a database and internet we can you will see data warehouse and the information there are some steps involved in this architecture first is your data came from Internet and then it cleaning integrate and selection by data warehouse and database then it data mining engine it helps to you have to evaluate the pattern of data with the help of knowledge base and then the pattern evaluation after that user interface and then it gives final relevant information thank 
7YujI-iCIAQ,28,"This video lecture gives you a detailed understanding of another Advanced Data Mining task known as Sequential Patterns Mining. This lecture covers:
Basic concepts of Sequential Patterns
Sequential Pattern Mining Algorithms, including
SPADE: SPM in vertical Data Format
PrefixSpan: SPM by Pattern Growth

► More videos: https://bit.ly/2TLfkDj
► Association Rule Discovery in Data Mining: https://bit.ly/3d48NLH
► Click here to Subscribe: https://bit.ly/3eovHO3

Follow me on 
► Facebook Page: https://web.facebook.com/ashahzad/
► Facebook: https://web.facebook.com/shzy12
► Twitter: https://twitter.com/shahzadali039

#DataMining
#SequentialPatternMining
#SPADE_PrefixSpan",2020-05-11T18:54:35Z,"Association Rules in Data Mining - 12: Sequential Pattern Mining: SPADE, PrefixSpan by Shahzad Ali",https://i.ytimg.com/vi/7YujI-iCIAQ/hqdefault.jpg,Shahzad ALi,PT30M3S,false,2805,36,5,0,17,assalamu alaikum welcome to this online lecture series of data mining in this lecture i am going to introduce you another important aspect of Association rule mining which is scan shell pattern mining in this lecture I will try to discuss some basic concepts of scan shell patterns and also we will discuss some scrunches pattern mining algorithms in which we will discuss Spade which is an interesting algorithm for mining a sequential patterns which is sequential pattern mining in vertical data format another one is prefix pan which is sequential pattern mining by pattern growth and close pan which is mining closed special patterns so as in our previous lectures previous lecture we have discussed how to mine quantitative patterns and how to mine real patterns and negative patterns so now I'm going to discuss the basic concepts of scrunchy patterns and screen shell pattern mining so the first thing is we should see scrunchie pattern mining is very useful and has very broad applications one application could be in customer shopping sequences for example you get a loyalty card from your shops like actually loyalty card is a card that some shops give to their regular customers and when each time the customers buys something from the shop some electron some points electronically stored on their card and later on and these points can be exchanged for some goods or some services you may want to see maybe one customer likely going to first buy a laptop then a digital camera then smartphone within six months so if this forms a pattern you may be able to try to do some kind of advertisement to other similar customers or you know serving some new incentives for this customer similarly medical treatment also forms sequences natural dyes and disasters like earthquakes happening earthquake happening it may be have some sequences of natural and also human phenomena similarly science and engineering a lot of things are processed they evolve along with time they also forms some kind of sequences similarly stocks and markets they have some kind of duration or some kind of sequences so they also form some kind of you know sequences similarly web click streams calling patterns for telephone and other things also forms sequences even for software engineering to me the programming programming executions forms Concha patrons the biological sequences is very very useful for biological analysis like DNA sequences or protein sequences so we will see trying to get sequential patterns out of those very big very vast applications could be very useful and important actually we can distinguish transactional databases usually which is usually which may not be important to look there time effect however sequences sequence databases they have times in time stamp attached with it and time series databases usually the time the things happen happened along the even or equivalent time intervals sometimes it's very consecutive so then from and then for sequential patterns actually there are two kinds one is gapped and another one is unwrapped patrons or skinship patrons in the gap patrons gap gap patrons means you do allow to have gaps within those patrons on the other hand in a non-gaap patrons means you not you will not allowed these patterns these sequences means everything is important the consecutive nests is important so if you have gap you have to treat them very seriously so for example in shopping sequences in shopping sequences or shopping transactions probably you do not care customers in the middle buying some other things or so it's not important to study the gaps for similarly for biological sequences in many cases you do care about gaps so the like the protein sequences or DNA sequences so if you insert too many things in the middle of two DNA sets so it might be sometimes you may completely change the function function of the DNA sequence so let's look at the customers shopping sequence which is we'll consider as a major example to study to how to do sequential pattern mining sequential pattern mining essentially is if you give a set of sequences the algorithm is trying to find the a complete set of frequent sub sequences frequent sub sequences which you know satisfying certain minimum sport threshold or minimum confidence threshold so let's look at this example we have a sequence database containing four customers shopping sequences like of sequence ID of 10 20 30 and 40 and each sequence ID contains a sequence like sequence ID ten contains a ABC AC D and CF as a sequence similarly other sequence IDs contain different sequences in its you know the this sequence transaction sequence database so let's look at this particular sequence the sequence in the sequence the parentheses means this one EF like this EF means is within the same shopping basket then after that it get another one which is a B that means this a V you know of following EF but a B is getting together at the same time similarly this DF getting together but following a B then C and then B okay that means each one of these you can think it's an element or it may contain a set of items or you call them events then this one event may follow another event so the items within the event the order is not important because they are in the same shopping basket but for our convenience we can sort them alphabetically like this EF we can consider it Fe it may make snow you know confusion but for over you know convenience we sort them alphabetically this is the same if we write it BA or a B similarly if we write this F D or D F its same sequence because it in these items or these events are within the same shopping basket these are within the same shopping basket but we cannot replace there or swept these two items because they are independent they are not within the same basket okay if this is in these two items are in this in parentheses then this these their order can be swept because then they will be in the same you know transfer shopping basket but in the current scenario these are in different shopping transactions then what is subsequence actually a subsequence is actually a sub sub sequence is any substring within this one this may original sequence you probably can see here the sub sequence you may have a gap for example a you can have a a you have BC actually this BC comes when you chop this a from the original sequence this if chop this a from the real sequence then you have BC actually you chop this entire AC this entire AC then you get D and then you get C this C comes then you chop this F from this shopping basket so this this one is a subsequence of this longer sequence of this you know this longer sequence then sequential pattern mining the special pattern essentially is if you set a sport like a minimum sport of two that means at least this you know the two sequences contain these sub sequences you find those sub sequences as a sequential patterns for example if you every say like a B then C like if you find want to find this sub sequence either it is a sequential pattern or not according to this minimum sport of two then look at this sequence layer database for this sub sequence if this sub sequence occurs minimum of four two times in this sequence database then it will be a you know sequential pattern otherwise it's a not sequential pattern so let's look at this first sequence ID 10 okay this sub sequence occurs in sequence 10 so it occurs once say well a we have to check in sequence ID 20 so a B you know so it doesn't occur in sequence ID 10 sorry 20 then we have to lik look for si ID 30 so it's occur in this sequence similarly about the second sequence I did 30 so it's not in sidd 40 so that's why as you know this a B then C occurs in two sequences as in our scenario the minimum spot is 2 so this a V then C is a sequential pattern so sequential pattern mining sequential pattern mining algorithm is you try to develop algorithms which are efficient scalable and these algorithms should find a complete set of frequent sub sequences we call sequential patterns and also these sequential pattern mining algorithms should be able to incorporate various kinds of user specific or user-defined constraints for sequential pattern mining actually the a priori property the property we have used in frequent pattern mining still holds for example if we say a sub sequence s1 is infrequent if a sub sequence s1 is infrequent then any of its super sequence cannot be frequent so that's all about the same idea as we have used in a priori while we had discussed frequent pattern mining so based on this idea we have actually we actually can develop a lots of algorithms one we have vertical format based mining called Spade which was published by Zaki at machine learning in 2000 and another one is we learned that we are going to introduce is prefix pan which was published in t de PK de workshop in 2004 similarly the third one is called GSP which is generalized sequential pattern which was published by silicon and Agarwal at II D BT workshop in 1996 so another method that we are going to study is mining closed sequential patterns called floor span which was published by an at all at SDM workshop in 2003 and finally we are going to discuss constraint based scrunchie pattern mining so first I'm going to introduce you an algorithm called Spade which is sequential pattern mining based on vertical data format so you pretty still remember the vertical vertical data formation format based a frequent pattern mining algorithm called Eclat that we had discussed in the extensions of a priori algorithm for the same set of authors they actually developed an interesting algorithm for spatial pattern mining the idea is pretty simple if you take this sequence you do a little detailed study you get a sequence ID an element ID and a set of items so what you can see is for this sequence ID 1 the element ID 1 now you can see the sequence I'd even let me ok for sequence ID 1 for element ID 1 you find the item a similarly for sequence ID 1 the element ID - you get the items ABC similarly for sequence ID 1 the element ID 3 you get items a and C and so on then we can transform this into a vertical format that means we just look at it there the a occurs and where the B occurs so the a occurs you probably can see in sequence I'd even in element ID one similarly a occurs in sequence ID 1 and element ID 2 so we may write it here too similarly at a occurs in sequence ID 1 and element ID 3 you can check it from here certainly then a occurs in sequence ID 2 and element ID 1 so as you can see here similarly we have to write it for items item B and you can see B occurs in sequence ID 1 and element ID 2 so it occurs here similarly you can check for this B occurs in element sequence ID 4 and element ID 5 then we can form we can combine them into frequent sequences like a then B or you know B then a if you say a then B then you will be requiring a in front of B or you can say a is element ID is in front of Bees element ID that means for the same sequence s ID 1 we have to check where the will have to check the element IDs of e then we have to check the element IDs of B you can see from this from CA for sequence ID 1 the element ID of a is 1 and the element ID of B is 2 so this comes similarly for sequence ID - the element ID of a and element ID of B is you know for sequence ID - element ID of a is 1 and element ID of B is 3 so we can write this so similarly for sequence ID 3 we have to check the element IDs of a and B so for sequence ID 3 the element ID of a is 2 and a limb element idea of be against you know sequence ID 3 is 5 so this comes similarly we can check for be a for BA we have tried first we have tried the element IDs of B then we have tried the element IDs of a so for the same sequence ID 1 we have to check the element IDs of B then a so element ID of B is 2 so this this thing then the element idea of a which is three so this okay you can write one you can write two so this also but this is a possibility okay so same this way we can generate this and the possible sequence I'd is an element I'd IDs for a b and b a and other frequent items so for the length three what you need is you just get length too frequent ones and then you do join how do you do join before you can see these sequence ID should be the same like for if we check for B as you can see we have to generate the a be a subsequence or sequential pattern for this we have to you know look at this frequent to sub sequences so if we look at this this element ID and this element ID should be same so we have to join them so we should write them only once as if you write a B a therefore sequence ID 1 so we first write one for alum as the element ID of a then we have tried the element ID of B which is 2 in both cases so we write this 2 then write element idea of a again from the second sequence be a similarly for sequence I 2 we write 1 sorry right which one then these 3 are same so we only write it once and then element idea of a again so this thing so to that extent you actually can find all of them so that's the reason you can use a priori based principle to find all the frequent sub sequences this algorithm was developed by Saki in 2001 called Spade which is a sequential pattern discovery using equivalent class we have discussed a vertical format based scrunchy pattern mining so now we come down to see pattern growth based algorithm which is called prefix span prefect prefix pan is a growth based mining algorithm to examine the sequential pattern mining sequential pattern in more detail we need to introduce the concept of prefix and suffix prefix actually means anything in the front if it is frequently it is frequent you want to capture them as a frequent prefix like a a a or a B these are frequent or and these are prefixes so then they projection the projection of these prefixes is becomes the suffix remember if you get a a when you see is you've got a position holder for the next one which is B so the position holder is represented by underscore okay the similar thing you get a B as you can see here a be the position holder or we shift to see it is the position holder for a b and wish if we see in this sequence so that means given a sequence you will find a set of prefixes and a set of suffixes which are the projection of the prefixes or you can say prefix space projection and then for this projection what we will find is first find length 1 Spencer patterns length these are frequent patterns or sequential patterns of length 1 then we can do divide & Conquer method that means we divide the search space to mine each projected database we have a projected database be projected database F projected database and so on this mining methodology called prefix span or prefix projected sequential pattern mining so let's examine a little detail for this sequence database if we find length one sequential pattern as you can see here these are length one sequential patterns how it comes if you look at this a Romanian spot was two so this a this a comes here here here and in all the sequences though it sees this is a sequential pattern as you know this occurs four times in four sequences and it's frequency count is four similarly we have to check for B so B comes in this sequence in this sequence in this as well in this so it occurs in all the sequences similarly for C let's check for G Y G is not in the length one sequential pattern as you can see it is written here in the sequence ID 440 so if you if we if you check it in sequence ID 10 G is not present if we check it in sequence ID 20 it's also not there similarly it's also not in sequence ID 30 it's only appears in sequence ID 40 so it's sport count is 1 that's why we didn't write in sequential patterns of length 1 so these are the sequential patterns of length 1 like this we can actually get the length to sequential pattern by first doing project database or projected database then find length to sequential pattern that means if they are frequent in this projected database they will form lengths to ensure patterns and then we can do can do length to square pattern based projection find a a and a F a a and F projected database then we can we can keep this this one ongoing okay the major strengths or advantages of prefix pan is there is no candidate sub sequences to be generated and the projected database keeps shrinking as we goes on the next alterations so as a summary of this lecture in this lecture we have introduced sequential pattern mining and discussed some basic concepts of the console patterns and we also discussed some algorithms used for mining sequential patterns in which will have discussed about speed and prefix as you know spared is a vertical format based sequential pattern mining on the other hand prefix pan is a pattern growth method so at the end I have finalized some recommended readings for this lecture the interest in readers may read these articles for more details so that's it for today's lecture if you have any theories or questions you are welcome to ask you're welcome to write in comments I will answer your queries and questions thank you 
dDpGE8R0Ehs,28,"https://experfy.com ---- Clustering and Association Rule Mining are two of the most frequently used Data Mining technique for various functional needs, especially in Marketing, Merchandising, and Campaign efforts. Clustering helps find natural and inherent structures amongst the objects, where as Association Rule is a very powerful way to identify interesting relations between objects in large commercial databases. The main motivation for the course is: 

i) This course specifically touches upon the scenarios where Clustering is necessary, and which Clustering technique is appropriate for which scenario.

ii) This course also stresses on advantages as well as practical issues with different Clustering techniques 

What am I going to get from this course?
Learn clustering through examples in R – that you immediately apply in your day-to-day work
Over 20 lectures and 5-6 hours of content, plus 2 practice exercises on Clustering and Market Basket Analysis
Learn practical Hierarchical, Non-Hierarchical, Density based clustering techniques. Also Association rules and Market Basket Analysis

Related Posts: 
https://www.experfy.com/training/courses/clustering-and-association-rule-mining

Follow us on:
https://www.facebook.com/experfy
https://twitter.com/experfy
https://experfy.com",2016-09-07T13:23:45Z,Data Mining Course,https://i.ytimg.com/vi/dDpGE8R0Ehs/hqdefault.jpg,Experfy,PT3M24S,false,796,1,0,0,0,hello students welcome to the course on clustering and Association remaining this course is offered as a part of the data mining track from X / file I am your instructor my name is uneven coach I am a professional data scientist little bit about my background I have masters in statistical sciences and computing from IIT Bombay I have more than nine years of experience in analytics and data sciences so far I have worked in various industries just to name a few retail CPG telecom and travel hospitality and these are some of my interest areas okay now let's have a look at the course nap target audience for this course are beginners and aspirants who wish to change the carrier to data science seasoned professionals who wish to brush up their knowledge in clustering and also domain experts who intend to gain practical knowledge on clustering and what is the course outcome in this course will learn many of the frequently used clustering techniques and Association rule mining this course also has a lot of examples in our so that will help you to apply your clustering knowledge immediately in your day-to-day work now let's have a look at the course prerequisites for this course students are expected to know the fundamentals of mathematics and statistics and that includes knowledge of matrix algebra set theory and a bit of probability theory exposure to programming is also very very important for this course and art is the preferred language so if somebody has exposure to our programming especially dealing with data frames data types graphical plots putting that would be very very useful now let us have a look at the course agenda so the entire course is divided into five modules so in the first module we will be going through some of the basics of unsupervised learning in the second module we will be talking about k-means clustering in the third module we will be discussing hierarchical clustering in fourth module we will be discussing density based clustering and in module 5 we will be covering Association rule mining now apart from this module there are two practice exercises for the students one practice exercise on clustering and another practice exercise on market basket analysis so let's get started with the course and hope you enjoy the course thank you you 
mh_O1ndQTZk,22,"Module 1
Reference Book ---  Data Mining:  Concepts and Techniques(Second Edition)- Jiawei Han & Micheline Kamber",2020-05-26T08:44:56Z,Data Mining Functionalities:  Introduction,https://i.ytimg.com/vi/mh_O1ndQTZk/hqdefault.jpg,Ganesh P,PT14M8S,false,507,5,4,0,0,next we are going to discuss an important topic that damn I am sorry there are many Fatima in a function we're going to discuss this the following first one is concept or class in concept or class description we focuses on this characterization and discrimination so first a terminal condition week we are going to strategies concept or cross description there are two piece to study characterization and second functional V is Association mine or mining frequent Packers associations and polish we will study the mining of frequent patterns and Association analysis of assertion mining in more rural farm third data mining functional he is classification and furnish different classification methods and how prediction is and they are discussed in module three and the next dr. mine informational D is cluster analysis this is covered in module 5 and then out layer analysis - word evolution the different data mining functionalities are concept or class description second mining silicon patterns associations and correlations third one classification and prediction for one cluster analysis fifth one out layer analysis and to evolution so first an introduction to data mining functionalities or what kind of patterns and we have observed different types of databases and information repositories for which data mining can be know examine the kinds of patterns that can be knowledge the documenting functionalities are used to specify the kind of patterns to be found in data mining process in general data mining tasks by mining are classified into two categories descriptive and predict data mining tasks can be classified into two categories descriptive data may and predictive data mining tasks can be classified into two categories descriptive data mining and predictive the descriptive mining tasks characterize the general properties of the data in the descriptive mining tasks characterize the general properties of the predictive my tasks perform inference inference only on the current data in order to make predictions predictive mining tasks perform inference on the current data currents on the current data in order to make predictions so data mining tasks or functionalities can be classified into two categories descriptive data mining 30t the mining descriptive mining characterize the general properties of data in the data the predictive mark predictive data mining house perform inference on the current data in order to make delicious in some cases users may have no idea regarding what types of patterns in their data may be just in some cases users doesn't know what kind of patterns in the data are interesting or their task so we may need to search for several different kind of patterns so we need to search for different kind of projects in so the parallel search you find several different kinds of files is important in the data mining system that is it is important to have a data mining system that can buy multiple kinds of patterns to accommodate different user expectations also the doctor mining system should be able to discover patterns at various granular various can granularity or different levels of abstraction so we need a data mining system which performs finding different kinds of patterns parallely second thing discover the patterns at various granularity but is at different level of abstraction doctor mining system should also allow user to specify hints to guide or focus the search for industry packets some patterns may not hold for all of the data in another so a measure of certainty or customer tennessee's usually associated with each discovery so then we construct a data mining system it should perform militants first point it can find different kind of patterns and a parallel finding of different kind of files second thing is represent the user with different granularity or different level of abstraction third point is the - system should allow users to specify some hints to guide or focus the search for interesting so we are going to this test 6 type of atom any functional piece starting with concept or plasticity characterization and it will discuss 
SUWMO5z2Mcw,22,"By this video you can learn about what is Integration Of A Data Mining System With A Database Or Data Warehouse System with suitable exaple


#integration #integretionofdatawarehouse #sankalpinfotech


What is ETL || Extract, Transform and Load with example || ETL Data Warehouse Tutorial || Lecture -1
https://youtu.be/um_WckQkKRE

What is an ETL Tool?  || Comparison and contradiction of various ETL tools|| Lecture – 2
https://youtu.be/MtTvbAK44y8
Basic Concepts of Data Mining || Data Mining Introduction, Evolution, Need of Data Mining||Lecture 3
https://youtu.be/KdX3A-k8t_A

Data Mining Functionalities || Data Characterization & Data Discrimination || Lecture – 4
https://youtu.be/sQtEbrjUwa8

Classification of Data Mining Systems || Data Mining Classification - Basic Concepts ||Lecture – 5
https://youtu.be/b3T3kWEtWZY",2020-10-12T10:45:55Z,Integration Of A Data Mining System With A Database Or Data Warehouse System || Lecture - 6,https://i.ytimg.com/vi/SUWMO5z2Mcw/hqdefault.jpg,Sankalp InfoTech,PT32M35S,false,194,7,0,0,2,hello dear student how are you i am sanjay hadian welcome to again our video lecture series today's lecture we discuss about integration of data mining system with a database or data warehouse or many times it this question asks this way also architecture of data warehouse for data page so again in this particular lecture we specially discuss one how some different functionalities are integrated integrated with a data mining system especially database or data warehouse system so again we describe this question as architecture of our data mining system as well as integration of data mining system in uh with our database or data warehouse so first we again try to understand what is the architecture or integration of database or raw data warehouse integration means what integration is nothing but it is a one kinds of different types of module or concept connect and make a one unit or one mac one concept took my ago data base and a data warehouse different types of functionalities are used in this particular system it combine or integrate with a different ways so again how closely to module interact how in interdependence change depends on their interface complexity properly it different types of services different types of services with different modules we know this concept in detail one by one so let's now start here some basic introduction and then we focus on uh these four coupling concept or methods so data mining is described as a process of discovering or extracting interesting knowledge from large amount of data story multiple data sources such as file system database system data warehouse system etc etc okay here basic information if required by data mining data multiple [Music] this knowledge contributes a lot of benefit to business strategy scientific medical research governments and individuals everything directly yeah government concepts foreign data mining system needs to be integrated with the database or the data warehouse system what can you mention first of all you shouldn't hear away but data mining system if the data mining system is not integrated with any database or data warehouse system then there will be no sys system to communicate with each other easily the first foreign if the data mining system is not integrated with this particular concept this database of data warehouse system then we can't access it properly and we can't use our drive or communication with each other properly or easily this scheme is known as a coupling scheme or non-coupling thing coupling or known coupling scheme not coupling language but we know no known as a coupling or non-coupling scheme in this game the main focus is put on data mining design and for developing efficient and effective algorithm for mining the available datasets scheme based on this particular scheme or integration occurs to data mining design and developing a effective and efficient algorithm for mining user's particular queries and give it sorry no coupling second one is loose coupling third one is semi tight coupling and the fourth one is tight coupling which are possibilities for architecture of data mining systems foreign um any kinds of queries but it goes first to the data mining system for data mining's application in data mining application here there are many lots of tools and technology use and it proceeds the user's query and check which types of user wants to patch the record if here is available then foreign foreign the system does not utilize any functionality of our database or data warehouse for dimension puzzle if you have a data mining system they say it doesn't utilize any functionality very most a data mining system doesn't utilize any functionality of database or data warehouse is known as a known coupling no coupling it may fetch data from a particular source such as as a file system a process data using some data mining algorithm and then store the data mining foreign using some [Music] architecture does not take any advantages of database or data warehouse that is already very efficient in organizing storing accessing and retrieving the data right i think i'm not approved for you if architecture is considered as a poor architecture for data mining system however it is used for simple data processing what i mentioned was the very most important step i have now coupling that you can directly say poor architecture concept that is in data mining systems we can consider it that's why it is used or it is implement for simple data mining process next one here one drawback is there also without using your data base of data warehouse system or data mining system may spend a substantial amount of time for finding collecting clearing and transforming consuming data mining system may spend a substantial amount of time for finding collecting clearing technology data is second there are many tested scalable algorithms and data structure uh implemented in data base and data warehouse system here in no coupling [Music] in this architecture loose coupling means that a data mining system will use some facility of database or data warehouse system hatching data from a data repository managed by this system performing data mining and then storing the meaning result either in a file or in a designated plus data base or data warehouse for conventional we can consider sorry compare with no coupling concept or technique in comparison to no couplings uh technique this is better for this here foreign is better than no coupling because it can patch any portion of data stored in database or data mining by using query processing indexing and other system possibilities for the messenger then loose coupling technique is better than the no coupling technique because it is used or when you have any kinds of data from the database of data warehouse and at that time it uses some queries queries architecture a data mining system redrive data from the database or data warehouse process data using data mining algorithm and store the result in this user again previously this architecture is mainly uh for a memory based data mining system that doesn't require high scalability and high performance because of the very most important statements coupling technique architecture they say mainly basically it is used on memory based data mining system we never know drawback formula it is difficult for loose coupling to achieve high scalability and good performance with a large data set however there are a huge amount of database or large database set at that time loose coupling technique is difficulties or fetch some problems to perform a high scalability result and good performance it enhanced data mining performance again comparison to loose coupling technique we can say in this particular technique it enhanced the data mining for foreign coupling data mining architecture beside linking or data mining the system for the mission for jogging beside linking your data mining system to database of data system efficient implementation of a few essential data mining primitives can be provided in the database or data warehouse institute based on this particular technique this performs some efficiently and effective task in this particular modular concept the primitives can include salting indexing aggregation histogram analysis multi-way joint and pre-com computation of some essentially success statistical master such as some from max mean standard deviation etc etc implement in this architecture some intermediate result can be stored in database or data warehouse system for better performance and conserving students a particular architecture or a particular concept coupling technique intermediate result can be storing coupling means that a data mining system is smoothly integrated into the database or data warehouse system directly okay so easy if we are determining this system coupling the architecture of the concept smoothly smoothly means each and every full functionality used for database and data warehouse systems in that coupling data mining architecture database or data warehouse is treated as an infor masonry driver component of data mining system using degrees what's a messenger job india in that coupling data mining architecture concept database or data warehouse is treated as a information component of data mining system using integration specific and meaningful information so system is treated as a one functional component of information systems particularly are optimized based on mining query analysis data structure indexing scheme and query processing method of a database or data warehouse is the very very most important this point query analysis data structure indexing data warehouse are used to perform data mining tasks this architecture provides system scalability high performance and integration information everything directly we can say tight coupling concept or architecture which provides the system scalability high skill system scalability also we can say high performance and integration information system so integration of data mining system with the database and data warehouse system uh we especially divided in main four types uh local plane blues coupling semi tight coupling and tight coupling concept and this four concept data mining system performs its all over better tasks to the users into the end users so i hope you students you go to this concept properly and very rare thank you for watching this video if you have any kinds of question query please dear students uh give it my channels comment section as well as you can contact directly with me and solve your problem thank you for watching this video again 
LV_swzZpqXw,27,"#DataMining
#PhDLecture
#CLASSIFICATION 
#PREDICTION
#TRAINING
#LEARNINGPHASE
Data-Mining-PhD Lecture-CLASSIFICATION & PREDICTION",2021-02-02T07:01:56Z,Data-Mining-PhD Lecture-CLASSIFICATION & PREDICTION,https://i.ytimg.com/vi/LV_swzZpqXw/hqdefault.jpg,Dr. Dibya Jyoti Bora,PT18M29S,false,14,3,0,0,2,oh yes sir okay so i hope that this uh stages each of them is clear to you okay so we have seen that the data cleansing process after that we have data integration process and then we have data selection process then we have data transformation process and then we have the final process that is the data mining after that we have pattern evaluation and then we represent that knowledge each and every step is clear okay so this is the diagram now there are two forms of uh data analysis mainly one is classification another is prediction okay we always try to actually do these things means either we try to classify and or we try to predict now let's define what is classification yeah i'm taking an example that a bank loan officer wants to analyze the data in order to know which customer are risky which are safe okay so what that bank loan officer trying to do can you tell me he's just trying to classify those customers based on the history code that whether that means that loan applicant is of the type risky or safe accordingly he may sign that loan right so that is a kind of classification because we try to classify those customers then similarly a marketing manager and a campaign needs to analyze a customer who take him and profile who will buy new computer example this example will control it don't worry means suppose you have a computer show so various customers visit yourself but it doesn't imply that this of them is going to buy a computer from yourself so we need to analyze that who is going to buy or the probability of buying that computer is high okay so in that case we are trying to ready okay so classification i hope clear we try to analyze the data in order to know their classes some predefined levels a predict we try to find out some feature cases so let's uh take the example say the marketing manager needs to predict how much a given customer will spend during a sale at his company how much see this keyword is important in the case of classification we have seen that we are actually trying to determine the classes we are trying to determine the levels right but in the case of prediction of some numerical values being trying to find found out right that is how mass how much a given customer will spend during a sale at his company so we are trying to predict a numeric value please note that okay so in this case a model or a predictor will be constructed that predicts a continuous value function or ordered value now if we need to cite one algorithm for app classification then you can uh give the example of neural network right k n n then similarly suppose if you want to cite an example of prediction then that will be your regression analysis okay now next step is that how does this classification works actually there are two processes or you can say two steps first what we do we try to build that classifier or you can say your model then then only will apply your build model or classifier for the classification tax so this first place is very crucial because here we try to build a classifier based on some given data and some given knowledge about the data okay and the second step is what whenever new data appears we try to classify them on the basis of the model that we have built in the past day so let's come to the first step that is building a classifier or model this step is also known as larding step or learning phase okay because here we try to build the classifier in this state the classification algorithms build the classifier ticket we have a lot of classification algorithms as i have cited you the neural network k n okay svm so uh you may pick up one of them as per your need then you will go for building your model so in this case we try to what is that model how to classify the new data based on some given previous leveling data or leveled data right so the classifier is built from the training set made up of the tables and their associated class level what i have just said that in this state you have some knowledge about the data for which the classifier to be built right now let's mention class level uh let me give you one example see [Music] say uh you know say sharp minded suppose you are a teacher you go for taking a lecture in the new classroom okay and there that because you are new to that classroom what happens some classification regarding that student's iq level to be done so that you can maintain your pace of learning teaching accordingly okay so thereby from the past record you have found that there are two classes one is chirp one is say address and one is say dull right so from where you got this knowledge because you have some data or pass record of the students where you are going to teach so these things you can consider as levels or class levels okay so based on it what you will do you will classify the whole group of the students in the classroom into these three different groups so this is possible because you know these things you know that sharp levels iq then average level then done without that you can't do that okay so what i mean to say that classifier is built with the help of this knowledge data and that is actually what from the training set don't confuse that the data set is same okay data set is same we divide into two one is trying and that is testing we'll i'll come to that point later don't worry so the classification or classifier we build it from the training set okay now each tuple that constitutes the training set is referred to as the category or class see i have classified 23 types suppose x belongs to this serve so we can say that x is a boy of sharp iq okay then suppose y belongs to average we can say that y is a y or of average iq level and similarly z belongs to l so what it is saying that whatever the points belonging to that particular group or particular class we can consider that is of type that particular class right you know the term triple i hope you got it in your dbms right uh my voice is coming clearly right just a minute hello yes my uh hello yes so uh i hope that the concept of training is training dataset is clear that you have seen that we are actually building that classifier from the training set okay and the concept of tuple is also clear i think you can consider each data point okay you know if i am considering a table okay if i'm considering one table then what would happen this particular record we are considering as triple okay so these tuples the difference is that here they are leveled levels means you know that this belongs to either surf category say average category or dull category this information you already have okay and based on that we try to actually develop our model classifier model for the algorithm part you may choose any one of them that i have started as vm you can choose or k n you can choose or new and then n you can choose okay it doesn't matter the model will have to be first trained okay and that face is known as learning case up to this point is this clear so let's come to this diagram then what you can see from this diagram please analyze it your diagram decay data yes sir so what is this what this diagram tries to imply can you tell me let's look at this diagram very carefully and tell me if the age is low then loan is not safe and if the age like it is middle aged people and they have an income of low then we can consider it as risky and if the age is like old and the income is high or if the age is middle age and the income is high then it is safe to give a loan very good so what you have observed is from this table right okay manually you can do it but what would happen if i have suppose one lakh of such tuples now i you are considering this one two three four five tables right suppose we have one lack of such pupils so in that case we need one algorithm right first with that algorithm we will try to build our classification model and that will be done from this training data why i am citing this data as training because you see this is being tagged loan decision actually what we try to do our motive is to find out that particular sorry customer visit our bank for loan so we have to first analyze that whether that loan decision will be risky one or safe one or loss low risky one like that [Music] just so uh you can actually apply this algorithm to this training data and based on that what has happened during the learning stages or you can say the learning phase we have derived from classification rules again these rules being derived what are the rules here if a is equal to youth then loan decision is risky if income is high then loan decision is safe it is middle east and income low then loan decision is risky so this being done during the training phase okay then you got it that classifier we use it for classification then the test data is used to estimate the accuracy of the classification truth uh let me say firstly at this point first we have this training data okay training data with the help of what we have strain our model okay we have trained model and after that for the testing purpose we are using the test data okay and from this we actually doing what trying to find out the accuracy of our design model okay their scholars one thing i'll do now i will give you one paper on this okay and half an hour time i'll give you let's come to the loop okay 
pFdtfcA43VU,28,"Kevin MacDonell, Annual Giving Officer from Dalhousie University and renowned author of the CoolData blog, discusses how to incorporate data mining in your organization to create a data-analytics centered culture. He explores the uses of predictive modeling, the nature of modeling data and how it relates to your job.",2013-05-07T17:59:41Z,Data Mining,https://i.ytimg.com/vi/pFdtfcA43VU/hqdefault.jpg,The DRIVE/conference,PT1H3M50S,false,115,0,0,0,0,"kevin mcdonald is from dalhousie university and nova scotia he also is the author of the pool data blog which you can read at full day aboard you know it it's actually kind of neat the kind of stuff he writes about so he's gonna talk about some of the really fascinating things that he does and works with kind of relates to his work at Dalhousie as well as what he writes about on full data and I'll let him introduce himself little bit thank you it might be right amateurs but so far I was an conference is very professionally run I've been doing quite a few and I kudos it's really under the law I guess I work into housing on an annual giving officer and I run the phonathon program I've only been in annual giving for about a year and a half so i have actually quite a lot to what everybody i'm giving it's been a steep learning curve and i'm still on that curve but i'm getting better every day what else did I say about myself I also do data analysis for all areas of fundraising at Dalhousie so major guess Planned Giving as well as annual fund I'm in a unique situation in phonathon in that I i create predictive models and I'm also the end user so this is a this is a very good situation to be in it means that I'm really busy but at the same time it means I have control over how my moms are actually used I created models for me to giving and Planned Giving that I go back six months later and say how are you using those scores anyway and they'll say well we actually haven't done anything with them so if my model is not actually used if it's not if it doesn't leave sort of action then you could said it's a fail really so I guess certain times a year I'm 6040 doing analysis of supporting percent of the time sixty percent of the time I'm working as as a fundraiser once upon a time data was scarce it was difficult to acquire calculations were done by hand a lot of these statistical tools that we use were actually invented or created for use with manual calculation and it's amazing that we still use those tools today and they still work really well but today it's a totally different situation we have computers to do all of the the calculations for us at no time in as there ever been such a power and our desktops to do the analysis that we can do and I'd like to think of data as sort of a you know we're swimming in data we've never had so much data to work with incoming students now I know not everybody here is higher end but we're collecting more data than ever ever before and so I'd like to think of as the data is sort of a plural noun it's almost like a liquid we're sort of swimming in it rather than the old concept of data which was to think that as as a plural so these data are different but that's a grammatical thing which is there anybody here thinks it'll says data are rather than data is okay well and we can argue about that later this is def him still actually wear that anybody use man I don't know tell how the university has about a hundred thousand living alumni about 17,000 full-time students so yesterday I had a wonderful opportunity to meet with staff and various offices at the University of Washington and discovered how things are done on a totally different scale and it was quite an eye-opener I learned an awful lot and I really appreciate the opportunity to to meet new people for most of the day yesterday including visiting the phonathon room which is a real treat interesting to see how we do things that are so in a similar way on a different scale we have 30 students employed in the phonathon room and of course the University of Washington has 75 75 yeah 30 on the ship 30 on a shift yeah so it was it was wonderful as I said in my second year doing the phonathon program and I write the cool data blog I hid my spare time so cool dana wordpress org now i also have a cool data sorry wordpress com full day of work is also another way to get there there should be playing time for for questions but it just interrupt me at any point if you want to work to find a warm I want to start with a story these are actual callers from my program from last year except for that guy in the top i don't know i don't have include that guy just showed up because he was paid for two hours this is my beer money right here the others are valid values members of my team or that they were last year and we hired a new collar named jade she's not in this picture and normally in a phone a thon program what you will do is new colors while they're just getting an experienced their first calls to alumni well we might do thank you our event calling that's one way to introduce them to to get them comfortable with the automated calling system and coding and so on but when they actually start asking for donations they talk to acquisition prospects so that they'll talk to nom donors may be young alumni Colin folds where there's no risk of actually leaving money on the table so you know just get them comfortable with that so Jane was actually speaking with fellow who is an acquisition prospect as close as you can get to being a non donor and she did if she was trained to do she she left a comment for me because she encountered a difficulty with one that I was speaking with this person told her that he had a wonderful time at dalhousie his student experience was great and he would like to make a gift but she was unable to enter the dollar figure into campus call which is our automated system and I have a ten-thousand-dollar limit in campus call that's where I said it is fairly arbitrary because you never think you're actually it's never going to happen so the next morning I see in my comment report this person shooting she wasn't able to leave to get the dollar amounts though immediately I go to the database and find out who is this person to discover that he was actually a major gift prospect he had been sitting in a portfolio for a year and a half it had never been contacted the only reason he was called so early in the year was because he had a very high propensity to give score and my phone a thon model and that person actually ended up major gift to the University the the internal once once that I alerted the major guess that this is this might be an opportunity it became eternally known as the Jade project sort of a little joke but here's the facts about her donor graduated 1970s made him only a single gift twenty dollars in 1983 and but as I say he had a very high propensity to give score so we called him in September and it just so happened actually he lives on I think as anonymous donor so I can't say anything more but lives on the west coast and he's actually the CEO of a company and if that's the only way that she actually managed to it was a very long call because she went through many layers of assistance and so on before since she persisted on the call she said his next one inflating that person ended up making a gift of a million dollars to the faculty of management which also happens to be JS faculty she's a business student commerce kid so huge success story forge a huge success story for the phonathon program for Annual Giving although and we can't count that million dollars towards our total still I do have a great story to tell about to tell about Jade and I also think it's also a success story for analytics at Dalhousie I know plier million dollar gift if phonathon is not really prove that data analytics works but it is a good story so I want to focus on three main themes in my talk today first of all a particular model and what the heck isn't what do I use it for and specifically I'll focus on the phonathon program I know many how many here are not higher end or nonprofits yeah okay most of my examples obviously are going to be I'm going to loan I alumni alumni I want you to be thinking about constituent constituents in your database because data mining can be used at institutions of any size and it's depending on the robustness of their database it can be used by other prophets as well then I'm going to talk about sort of get sort of philosophical about what the nature in essence of modeling data is and how it differs from what I call everyday data the kind of data that would be used in st. reporting and three I talk about the role that you know the responsibilities that we all play all the ways data entry to database managers to i.t to the end users to protect the data and to use it wisely and responsibly data mining is a consider a general term I use it very often because it's it's it's a buzzword it's fairly well known but I think data mining is not specific enough data mining is finding patterns or detecting patterns in large databases I prefer to use the term predictive modeling which I define as the creation of a model which assigns specific scores to the constituents in your database which does which ranks them in order their propensity to engage in a specific behavior of interest and the behavior interest unusually interested in being a fundraiser is propensity to give to dalhousie but that's not the only model i make you can create new bolic models by changing the question that you're trying to answer and I'll list some of those later essentially i'm not going to get well i'm not going to get into any technical detail the predictive modeling i do give presentations that focus on specifics of predictive modeling but i just want you to understand kind of what the basics are a predictive model is essentially an equation on one side you'll have the thing that you're trying to predict your Y and on the other side you'll have all kinds of other attributes in the database that wrote that pertain to the individuals in your database so we'll have given on one side and then we'll have all these other things I actually look at about 100 or 120 different variables in our database which might relate to given so you know whether they whether we have employment data how many events then attended how many position updates that they have in the database whether we have a business phone number for them whether the business phone is the preferred phone on and on and on and on you can get very creative and digging in and finding new predictor variables a key part of predictive modeling is that we're not we're going to use this for segmentation we're going to use this for prospect identification but the key thing is we're not looking at giving history we are liberated from looking at giving history for example consider Jade's daughter he had practically no giving whatsoever and yet here he turns around and makes this gigantic gift and all along he never lost his love for Dalhousie University yet he was totally flying under the radar if we were looking at prospect identification only according to his past giving history we would never ever have reached up to that person yes prostate researchers were aware that he was wealthy individual that he had made large gifts in the past to other universities but this person live very far from dalhousie and you know where don't exactly have a lot of alumni out here and you know the chances that person would ever and touched our you know our slim this is I'm not going to read all this normally when I % like I say I usually focus on one aspect of Dana bought a date of morning and you know each of these is a 90 minute presentation or a full day workshop but I will say that the first step in the process just getting all your data together assembling it cleaning it putting it in a form that you trying to account for a missing data will take up perhaps ninety percent of the time be creating a model excuse me I had caught a cold on the airplane so folks the front row so and also come from many different sources not just the central database which would banner is what we use but also campus call I have contact data for all our alumni going back to two thousand seven as far as I know before i arrived at Dalhousie we made no use of historic contact information you know you just closed off one column project open up the new college project and that data would just sit there and would not be accessed but actually the number of times and a one has actually picked up the phone in response to a call from a student or the amount of time they spent on the phone with a student actually ended up being a significant protected predictor for my major giving model I mean who could have it's very non-intuitive who wouldn't put these two things together but in other sources that we survey data perhaps an external data as well market research data the end result is I will once I've built my model I'll just output the scores so every constituent in the database or at least mostly model on the lumps every alum will have a very granular score and that allows me to rank every single in and once you can't the lost alumni we're talking about 80,000 alumni I can rank them from most likely to give to least likely to give or depending on the model most likely to attend an event to least likely to attend an event this is not terrifically useful in annual giving because I want to be able to put people into large groups so what I do is I divide the entire population twice 4 into 10 equal groups those are called deciles so in the software i use commendable first desktop that's your lowest score possible very unlikely to get up to 10 which is the highest score nessa so this will factor into some charts that i'll show you later then the point about predictive modeling it's not about picking winners it's a vote probability it's about saying this decile i believe is the most likely to participate by me on this year or this group we should not send mail to or call because they are not going to participate how do we use the scores here's our kids give anyone who runs a phone program will tell you that it is getting harder and harder and harder to raise money on the phone because people will not pick up the phone and it seems to be getting worse and worse every single year so we need to use resources very wisely we need to call the people who are most likely to give first and persist with those segments as long as possible the unfortunate thing is that does not mean just calling donors we also need to think about the long-term health of the program we have to call nan donors so we need we need tools to allow us to decide while which nando issue recalling we just and I think most programs will just randomly call and just try to reach as many people as possible our institutions are getting too big our budgets are getting to smuggle in order us for us to continue to think that that is sustainable we need to need to rationalize it somewhat and create a is excellent for that last year about a thousand people were called 20 times or more than we never reach them so it just goes to show how hard it is to get ahold of people so last year I saw built modern dick model for propensity and give by phone of his phone specific and that was created in the summer of two thousand ten implemented it in September and what it allowed me to do was the we actually grew dollars in pledges and gifts by seven percent and I also shaved a month off the column program so we saved significantly on long-distance bills students salaries and yet grew the program so that's what we did last year so I have high hopes for this year as well this also allowed us to free up phonathon to serve other constituencies on campus for example we did then calling for alumni of the Illinois office we which I love to do I'd love to do event calling because it varies the tasks the students are being asked to do if you know you're working with your colleagues with sometimes sometimes there might be division there in a different building when we are so it's great to have that cooperation and we even reached across campus and we did some call them for recruitment of admissions we called students have been accepted to Dalhousie to invite them sessions in their cities that was a very rewarding project that we would not have been able to do that everything just continuing to do as we were before chasing people who are simply not going to give over the phone so I can't talk about data without sharing a few charts I don't know if Marilyn would improve these journeys or not I was thinking to myself during the presentation look the grid lines or now you're still there anyway i wanted to be very precise please these are dissipation rates chopped up my score so mr. percentage of alumni who were contacted who made a specified dollar pledge broken down with down by their score okay so the average pledge rate last year was about eighteen percent but the top decile this very special group had a pledge rate over thirty five percent so what what group do you think I'm going to continue call it through the rest of the year and again with Marilyn Marilyn here because normally I will take that ten room and I'll color it green to be a tannin that is very it's very significant okay Oh another thing to return know is both terms those are not past donors that's a mixture that's a 50-50 mix of acquisition in renewal so those are just there's a lot of new donors in there as well so big challenge and forth on Mandy I gave up calling a segment if you give up too soon if the same is not very well defined and you still have people who are likely to give in that second and you gave up on them you've wasted if you continue to pound away at a segment that where they're just not picking up any more you're wasting resources so here are a percentage okay let me read this I get it right number pledges that came from alumni who were called 10 or more times before they gave a pledge broken down by their score people who are low score is going to break so the tens who we called all through the year started in September and we were still calling them at the end of March if we got them on the phone by the end of March they were still pledging at a fairly decent rate not like it was back in September but there was there was a reason to keep calling them because they were more likely to pick up alright high scores are also more likely to give at higher levels the tens have a median gift of $75 and the next score level down is the median 250 this is a chart showing alumni who gave previously so these are renewal donors who actually upgraded their pledge people with high scores are also more likely to upgrade their pledge and this wasn't a model that was created to predict upgrades this is the model created just to predict propensity to give at all so that it actually seems to perform in many different functions and fulfillment rate people who are high scores high affinity for dalhousie they're also more likely to actually fulfill on their promise to make a gift that's what the scores do what's actually what's the model actually built on these are some of the top predictors sometimes with a month was very hard to actually follow but one of the variables that are surely the top predictors because they all interact with each other they're all interrelated data mining is kind of a messy business where you can't it's very hard to nail down anything about you know what are the top influencers of giving another secondary point now that occurs to me is that models are not about figuring out what causes given models are only about figuring out what is correlated with giving or associated with giving collecting those traits together and figuring out a score based on those coordinates or those associated variables yes yeah number visit what is the number of position of aids what does that mean that's the number of job tenants that we've had for them in the database it's very significant because we and this this is this goes to the core of actually what I'm talking about because in banner when we get a new job title information and the primary source of that is through phonathon because it's one of the first things that we ask where they're verifying you know where they work now or if we don't have that information we ask for it if they provided its associated with given a key point is that when we get new job title information we do not overwrite the old information we invalidate the old record and create a new one this is pretty standard data entry practice maybe not everywhere but it allows me to go into the database query and count the number of job you know employment records and having that count as it turns out in my testing that it's highly correlated with getting and it's at the top of the predictors sure ok I will be collecting the slide decks from the speakers and posting them online as well and we're also video taking the sessions as well yes number is that legacy yes I guess yeah we could call them legacy ads get cooking yeah I narrow down the number of cross references to family related so we image I have cousins so there's a child parent grandparent guardian all those the number of those you know any kind of family association with Gavin audience associated with give me your questions man good question the question is why is not having a gmail yahoo or hotmail address predictive of give and it's actually anything but one of those addresses it used to be that if we had an email address present in the database the fact that it was there was actually predictive of giving because they've actually given us that information email is different now we have that email address for practically everyone in the database the fact the matter is is that only a certain percentage of alumni are actually going to open up an email from us or or that emails may not be valid as it turns out if an email the email that they've provided to us is one of these free view counts it's probably not their primary address it's not the work address it's not their home address it's something that they just give up so that lets say doing the survey or the whatever its it may not be an address that is something that they're going to keep an eye on so I found that it didn't make it into the model but having having given us one of those types of addresses is actually negative predictor but if you get us something anything other than that it's a positive predictor yes you said yes to stage you just have blocked on my frustration now we have a contact restriction for every everything yeah do not call on wednesday night during the full moon no now we have a lot of specific types of contact restrictions and so I just go in and camp them yeah and there was one more question that I don't remember Lissa the question was was the number of contact restrictions a positive or negative predictor and I can't I honestly I can't i'm guessing it's probably a negative predictor but but to be honest i can't remember sometimes these things can be a non intuitive something that you would anticipate to be a positive predictor interacts with another variable and flips it to a negative because it's actually moderating the influence of another variable so it's just sometimes these lists are bit misleading these lists are awesome very specific to the institution you can't just go into your own database and apply those lists and expect to do prospect identification you've got to test these things for the degree of correlation with given I said I create a number of different models a new model on creating and forth on this year is actually likelihood to pick up the phone because I had data going back to two thousand seven I know exactly how many seconds every alarm in the database has spent in conversation with a student at Dalhousie and I can use that historical information to create a new model that that predicts for that specific kind of behavior picking up the phone so I do both I have a model for him to pick up the phone and our model to predict likelihood to make a gift if I cross those two together those two specific behaviors I hope that I will really enrich our ability to focus I also make models for acquisition mail so that we're we're splitting the on our mailing so that we're not shipping all kinds of money out I'm not getting a lot back and also a gift for a model for propensity to give at higher levels which is used in our beef and of leadership Annual Giving Program and I've also created event attendance elective models which are again for slimming down males for invites so now I'm going to talk about the the nature of modeling data Oscar said this in 1894 and it's truer now than it ever was it's a cube message that I want to leave you with there's no such thing as dead data there's no such thing as useless data data that we would normally regard as worthless as flawed as fragmentary actually might have value for David money and the reason is that are our ability to extract insight from that data is better now than it ever was before where does this data come from it all comes from all the normal operations of the University obviously gift processing you know everything that you can find advancement services sound from the register's office like I say some from our historical calling data almost all of the 100 particular variable settings would come from one of these sources and I've talked in a couple of slides for people I've known profits I did a little bit of work with the Toronto Symphony Orchestra just to see because I was curious I've never done any pricked modeling outside of higher end and I was just wondering can you actually do this at a non-profit and yes you can you just need to key things you have to be able to collect data that matters thats related to engagement with your cause engagement with your organization and you also need to have some kind of robust database that connects different types of data together and the toronto symphony orchestra was a great great example to work with because they have they do have this data scattered over more than one database they have a ticketing database and attracts take ticket sales and subscriptions to the concert series they also have a donation database but the key is that they can relate an individual donor to an individual serious subscriber and when i created a model for them I found that subscribing to certain concert series was predictive of giving what i did was i divided up the series according to whether they were pops concerts or on the lighter fare on more modern stuff compared to more than the classics or quote unquote serious music and the people who were devoted to serious music tended to be tended to be better donors perhaps there's an age factor in there but they didn't have birth dates like we had University data they don't have access to that information so even if it is a proxy for age it's very valuable a very valuable insight to their into their daughters and their potential donors above their ticket buyers some things that matter these things that matter making sure that your your database contains your the full range of your constituency so you're you know you the trunks of New York store has their volunteers they have their trustees their boards there are extra members are in there as well any kind of involvement or engagement any touch points with the organization is recorded and are also predicted for viewing covered that so modeling data and reporting data are essentially the same thing they look the same they smell the same but I think there's a very deep conceptual difference that we must bring when we do analytics as opposed to doing reporting and I think of the distinction as the difference between inside data and everyday data when I say everyday data that's not to diminish the importance of reporting data it's simply to reinforce that there is a big difference between these two types of data are at least in the way that we should be thinking about them and the inside data is just my turn for the modeling data everyday data is is very focused on accuracy it's focused on currency up to date pneus it's focused on facts the statistics that we would use in creating reports and so on is descriptive two sticks mid statistics and the threshold for validity is much higher with every day data than it is for inside data is if it's fragmentary if it's old if there's something missing you can't have a mailing list that hasn't been updated since the 1970s it's useless you need you need completeness or else you go you're going to throw up the data inside data is none of those things it's it's a different animal altogether it's not of its it's not about finding facts it's more about exploring for patterns and if the data is fragmentary or old that's just fine we either have techniques for dealing with it or the very absence of that data is actually predictive the absence of a business phone for an alum we don't have that information we know that alone is statistically slightly less likely to make a gift so that's like flipping and flipping the predictor on its head and also think of the number of job title updates we have been deleting that information and simply replacing it with the new information everybody would have exactly zero or one job head is in the database I'm very fortunate that I can say that they they might have 0 1 2 3 or 10 job title updates in the database every day data I'm going to get into Halloween now every day data has short-term uses with payoffs today it's valuable today insight data may not have payoffs or may have payoffs that are delayed for decades after the data is created and stored and it has to be maintained and kept for all that time and this is why I think that because the payoffs are so far in the future in some cases not always but sometimes inside data is always at risk of being lost about a year and a half ago I posed a question on the prospect DMM listserv it's hosted at MIT and it's where lots of data nerds hang out i love it and i asked i had a bee in my bonnet about you I was I was fearful that something was going to happen to the campus call server and then I was going to is the historic contact data that we had on alumni I don't know how I got this idea in my head but I'm just really paranoid because I don't manage that server isn't another building I don't even haven't even met the person who manages it and so I i went into reaction mode and so i posted on the prospect dmm lizard having have you ever heard of data that might be useful for modeling that was deliberately deleted and i was absolutely horrified by the stories that came back to me I honestly didn't think that this is something that would happen but it has and these stories I posted on my blog this lot more detail there but they follow two three main groups the first one is first of all the data was never captured it was never recognized that maybe it might be important someday it was never captured it never made it into the database the second is data that would have been useful to have no historical data overwritten with new inator this is your content to your position updates your job titles you know getting overridden delay that then the final one of the most horrible data that's actually deleted there's a database conversion happens at whole masses of your constituency that are regarded as surplus are shocked because they're taking up space so actually I just summarize some of the stories and just to give you a couple of examples and what kinds of things have happened data that never gets injured I heard from a university that did nothing to capture athletic team membership did nothing to capture other student is all myths student government all that goes made it to the database university did not have your books so you know gone very important engagement related data that that could have been used by a modeler but never will be also many universities and nonprofit organizations in general to a poor job of capturing event attendance data reunion data you know did they show up do they not data that gets overwritten what database manager deleted all the old addresses that were no longer valid at another institution whenever an alarm return to do post-grad work routine there's a second degree there is no degree was over it I'm not sure if that's even people happen and finally the deliberate destruction of data one database administrator needed to free up space so the records of deceased alumni were deleted they never existed this caused a lot of original records regarding estate gifts or downwards to be lost that's that's bad on another case his storage space was limited this seems to be a common excuse one organization did a huge purge of information the records of all non donors and parents of alumni were removed one university established in the 1930s underwent several database conversions during those conversions the following things happen those who attended but did not receive a degree were not transferred to the new database they don't count we have you know non non degree donors so people who died or had no valid address were not transferred all the female is the weird one all the female constituents middle names were overridden with their unmarried names when they married so happy Halloween so this is but this is the continuum of malfeasance I'd like to think that some of the reasons for data losses are sort of at the more benign into the scale Libby costs money to make sure maintain servers you know although the cost of keeping data for years should be cheaper now than it's ever been wouldn't you say i don't know i'm not an IT person is lack of storage space really an excuse anymore no okay good and then deliberate sometimes I don't think they'd ever actually happen but i just think i think the cumulative effect of all these losses I couldn't imagine if anything else links had actually happened a very university would a jade project ever have a chance of getting off the ground like that that potential donor would never have been contacted if we didn't know that he had student involvement sand things like that engagement related stuff we need to remember that data is not just part of the technology nobody owns the data IT does not only do it the server manager people they don't own the data the data is an institutional asset it's like the buildings it's the canvas it is a non preaching to the choir here if you guys don't you know you know this already it's an institutional asset nobody has the right I don't have to write our Daniel gaming but I don't have the right to say this is this data is now useless turn on we're going to get rid of it no it's an institutional asset the service of hard drives are replaceable but data when it's gone it's gone forever and the problem with it is that you can't you can't put a value on the data that's lost because we just don't know what we were going to be in the future so now i'm going to move in the direction of talk about the final part of what i wanna talk about kind of fostering a culture of analytics analytics is something that's really gaining a foothold at universities nonprofits are going to come on stream eventually they're going to they're going they're going to get some unis and do it in-house and I want to talk about the ability to do it in-house especially but at the very minimum we are we absolutely need to protect the data one bright moment in going through these data horror stories is that I realized that most of them happened about ten years ago 15 years ago none of them were really recent maybe maybe was too sensitive to actually complain on the list server an email to me but I like to think that we've gotten a lot better you know across institutions and recognizing that the data needs to be protected second in order to get the closer to an analyst culture we need to provide the tools we need to provide software when you can find access to the data we need to provide training on how to query the data of the database you know an annual giving probably you know we would have mostly relied on advancement services to produce reports a lot of ad hoc reports we're taking more and more of that doing it in house getting better at you know running our own data this frees up the data specialist and advancement services to do perhaps more challenging interesting valuable work such as creating sophisticated dashboards and leaving more of the ad hoc stuff to us at least if we are able better able to develop our own queries maybe they're not going to be as sophisticated as something that built for us but when we do ask for something to be built we will actually know what we're asking for communication obviously very important at Dalhousie we have summit regular meetings of all data users this ranges from in external relations the safe range is from people who actually do data entry to prospect research to people who work in angel giving anybody who touches the data from creator of the data to the end user gets together and the meetings are not always you know they're not always useful but I always come away with some tiny nugget of information that I didn't have before some idea for a new variable that I could use some new knowledge about some of skewer code in the database that gift processing uses that I have no idea what it means because I don't want to be using data in my models if I don't know what it means sometimes something will look like a wonderful predictor but it's really just a proxy for what I'm trying to predict and it employs in my model and in a way that I don't want it to happen so I have to know I have to understand for data so we have to communicate with each other part I put a lot of stress on documentation as well data work requires documentation code sets we got to know what they mean if you have built rapport we have to know how you built it it's just very important and I think we struggle with that I mean dalhousie struggles with that I think everybody struggles with that may be and I think it's important to innovate from within we're all we just we're not going to have the ability to reach out and hire statistics phd's or you know people with a lot of experience in analytics I stumbled into analytics I discovered a passion for working with data from a very unlikely source my degree is in journalism I worked as a reporter and magazine writer and editor for years and then I got him to communications at a small University then I got it to prospect research you did for five years somehow stumbled into working with data got training from Peter Wiley and John Sammis and everything just seemed to catch on fire from there I was building models for a university that had 30,000 living alumni the models were we're very elegant and they worked very well you don't need a huge institution in order to do data mining now that Dalhousie which is considerably larger than the University of where I started which is called st. Francis Xavier University and I'm having a lot of fun playing with the data there everyone in fundraising seems to be really interested in analytics it's kind of a buzz word but there's there's a number of barriers that kind of stand between getting from being fascinated with the idea to actually making it to actually building your own models and yesterday I had the opportunity to see model built for Planned Giving at the University of Washington and I was very impressed by what they were doing what they were doing in-house they get everything right and I'm really gratified to see when when that kind of innovation is actually happening with them always you know all we need to hire have anything against benders but sometimes you can develop solutions within within your own institution the barriers I think are sort of threefold you have to learn to get to learn your data in a new way think about your data in a different way you have to use the learner to use a statistical software package you have to learn data mining concepts which is not the same thing as learning about statistics data mine was a very predictive modeling is very specialized use of statistics a statistics textbook is not always very helpful in learning how to do predictive modeling there are not a lot of resources out there for learning to do predictive modeling at fundraising environment or university environment profit environment the blog is one attempt to somehow fill in some gaps but i think the individual employee is where it all begins despite all those barriers so whether whether I'm talking to people who are employers or managers or directors or whether I'm talking to would be practitioners in analytics I think about five different steps that might get individual employees closer to actually doing analytics then what analytics I have never seen it happen that it's identified as an institutional priority that trickles down to staff it's and you know this may not be a universal but other people have told me this as well it's usually individual employees often prospect researchers who are plugged into some technology they they develop an interest in working with data at sophisticated level and they end up selling it to upper management if they're successful not always successful okay so I think it can't start with individual employees so how can our organizations actually encourage this activity I always think of the famous example of google and the twenty percent times the twenty percent of their job time or work hours that are set aside for personal projects I don't think I don't think we're going to be setting aside twenty percent of our time to do our own little thing but I think there's ways that you can adapt that idea to the nonprofit world for example when I was a prospect researcher the summer times were fairly quiet the president was on vacation there weren't a lot of development calls happening I was a very reactive researcher of to helping you know long profiles person you know donor profiles or proximate profiles and in the summertime things been kind of em and of course I'd be encouraged to take modification then I think it's a bad idea to incur to to force creative employees to take vacations at certain times of you because it's in those long stretches of unstructured time that some new ideas new innovations might actually occur if you're always on a treadmill it's going to be that much harder to to do something innovative and that's when I discovered data mining got into you know reading Peter Wiley stuff online reading his book and so on my boss had no clue what I was up to now it's my job freedom is wonderful but it should be tied to business goals if you're the would-be practitioner you're going to have a lot more success if what you're proposing is actually tied to something that's in the annual plan or tied to you know maybe this will help us reach our target here's here's a new way of doing something which you know all of my organization's hopes and fears are kind of encapsulated in the strategic plan for the year annual giving as a as a plan we have with a bunch of different targets and so on you know the creative worker has to keep that plan in mind and thi-thi whatever they're doing back to that if you're an employer find interesting challenges for your creative employees if you are creative employee search for interesting challenges and actually they're obviously not hard to find there's lots of the bone there innovation is all about finding new ways to do work especially work that's a drudgery and you know trying to automate work essentially being lazy in a creative way this conference is an excellent example of reaching out to other organizations the point was made earlier that a lot of people are especially you know working in analytics I tend to work in isolation within their own the institution and that's definitely true of myself there are not many people in my department that I can turn to and discuss anything on a technical level so I obviously rely on online resources conferences are very important so if you're if you're an employee employer definitely encourage employees to attend conferences make sure that they're involved in professional organizations and you know this the University of Watchmen reaching out is this is this is the best example of reaching out across institutions the end users I started by saying that I'm the end user of my own product so that's that's an ideal said well maybe it's not perfect ideal because I don't have enough time but the fundraisers have to for example if they're the end user they have to have some confidence that your model isn't just a bunch of mumbo jumbo and it's actually something that's going to help them be successful so you need so there has to be the two-way communication happening all the time this is not just a boat by getting buy-in it's also about keeping your analytics work relevant to the organization so that you know what you're doing is answering the questions that need to be answered solving the problems that need to be solved you know for example in phonathon that the problem isn't necessarily the difficulty in getting a specified pledge from a donor the problem is identifying who's actually going to take the call as once we get them on the phone the student can then use all their skills on the call to get that pledge so what's the business question the first business questions of the part of the funnel that's being squeezed is actually identifying that's going to pick up the phone so since I'm the end user and the modeler identified the problem that I want to solve create the model to do it that's kind of a tight circle it's obviously not always that tight so I'm done the very existence and the quality of the data that we work with is our responsibility all the way from from data entry to the end user so you know if we take our responsibilities seriously we can have all kinds of examples in our alone institutions of jane projects is there any questions okay I'll start here that's like the first so uh do you work with nine hours a week I think about this Nadia how was the weekend yeah yeah and I by either book Westerners are you efficiencies I don't think so i think i need to get I obviously have to organize my time well I have a very set window drown it of course of the year where I will do most of my modeled creation I would like to stagger that and do more Magan creation more often so I will win amongst one Singh here I'd like to do more models more often that I'm constrained by you know the daily tasks involved in its money so for an organization starting out in data mining like this how do you be sure to how do you validate I was an organization validate the model so lets you create it I mean we've done some with it to record a lot of effort and similar to your situation it's kind of one person who's very interested in it and now they're in class but so when did we revisit it when we say okay this model actually protecting something or it's wrong where is everyone right the question is of and what stage of how do you validate your efforts in and predictive modeling to figure out whether what you're doing is worth all the effort worth the time and investment that's a that's a tough one I will often take a look for example the charts i showed you with phonathon and just seeing well did our donors how do they break down by score and that's I found is the most convincing evidence that I can use I'm not called on very often to produce evidence for how the models are working and other predictive models i've spoken to say that you know they don't do a lot of testing after the fact to see whether the models actually worked I always test I always see how how they performed that's how I discovered that one model I created was really doing a very good job for example this very general phone con model did a very poor job of predicting a young alum acquisition because the model was too general so this year I split the model into young alumni and all other alumni non donors so because there are two very different populations so it's a it's really a constant reevaluation that has to happen as far as return on investment goes I'm not sure how you can do that on your slide with thousands of IU's well there's a lot of different techniques the techniques ideas are fairly simply known as sophisticated as some of the things out there but I look at Pearson correlation values and I rank them according to their strength of linear correlation and then I will introduce them to a regression model one by one ranked in order according to how strongly correlated they are with the dependent variable and then I just when when our square defense to improve or I keep hitting a lot of the insignificant variables that are not adding any extra explanatory power to the model I'll stop and usually I'll stop around 20 variables 25 variables that's kind of a technical answer the question was how do you select how do you select the variables that are actually going to make it in the model because you start with potentially hundreds and yet the you know the number of variables that end up in the model is fairly small the reason why it's very small is because all these variables are actually related to each other they're not very do not fully independent of each other so the more variables you keep adding in the less explanatory power we're going to have but eventually you're just adding random noise to your model but the software takes care of so much of the decision-making about which variables dad I what software do you use I use data disk which is a fairly powerful statistics software package there are other there's no there's quite a lot of variety of their SPSS is something that is available to staff members at most universities Minitab is another one excel is not really adequate for having with large datasets I don't think I was going to mention yeah absolutely my modeling method is fairly manual there are other software packages that automate a lot of the data cleanup and assembly and decision-making for you and you can use stepwise regression which will build a model for you I choosing some curious about this one how you how you out for a result I know you mentioned you know that that the type of what they do you have a methodology for figure that out and my second question is I think in doing our modeling it really helped to know something about source of origin all user data so are there people in you partner with enjoy figure out late how good is this how comprehensive is this is there a story behind this state was required yeah data mining is a couple of questions but I'll address that one of the questions is what how the what kind of work do you have to do to ensure to learn more about the provenance of the data what's the story behind it what does it mean how has it changed over time so that you really understand what you're using data mining is partly about sitting in front of the computer and geeking out and it's also about picking up the phone and call the people and gift processing calling people who you know who manage different who own different parts of the data and talk to them well what does this code mean you know how well populated is this variable I have a perfect example and it slipped my mind but you know things change over time so you have to understand what you're looking at I don't know if I really answer to a question and I think we're actually wow it's lunchtime what can I take one more and one thing I noticed was that that worked well to identify those prospects for highs part of the leading role but included another large 50 people because our organization wanted to change and become more competitive and we found was that model identify the president of the student in the middle expose lenses now the eyes former the lowest form of the middle and if our recruiters were only you know attention to recruit those students within the middle our organization when you're going to grow so I'm just curious to know how you can balance the prescriptive versus descriptive writing yeah the question is generally if I understand it correctly that for example sometimes your model will suggest a certain course of action which you know may not be what the your actual goal is and the specific example was a student recruitment model where actually your middle achievers where identified as you're most likely to attend is definitely one of us are good students who are elected to hold for sure you're also doing part order say she wanted to change a little bit more competitive and the model was including us to those students who supporting it was fun yesterday the middle of the students were at the top here right so when you think about efforts and we're model you could be kind of secluding exactly yeah data mining does not tell you the course of action data mining informs and guides your course of action so you need to perhaps model break break up those high achievers and create a model only for them and choose the high scores from them or have a have a have a matrix where you have high achievers and their scores like this and maybe you could get focused on that way I don't know what I'm looking at the data i'm not sure what okay we're done "
ZQjUiUKD0Qo,22,,2020-07-09T05:07:32Z,Integration Of A Data Mining System With A Database Or Data Warehouse System,https://i.ytimg.com/vi/ZQjUiUKD0Qo/hqdefault.jpg,miral donda,PT9M24S,false,789,11,7,0,8,hello everyone welcome to today's lecture in borås lecture we are going to learn about the integration of determining system with the database or data warehouse system so the meaning of this topic is that how we can indicate our data mining technique for the data mining tool with our database or the data that our system so did so it may possible in some ways and that always inclusion or coupling loose loose coupling semi tight coupling and tight coupling so let's understand each terms in detail so first is a no coupling so no coupling means that determining system will not utilize any function of database or data of system so determining system fetch the data from particular sources sources like of our system or any other data source process the data using some of determining algorithms and then store the remaining results in another file so this is the simplest degree so in simple verse we can say that here we are here we are not integrating the data warehouse with the data mining system no coupling coupling is so in simple words we can say that coupling means of connecting a part or we can say that a pairing of two items yes so here no coupling means we are not a bearing or we are not connecting the data binding system with the data warehouse so what will happen in this type of determining system so in this size of system data mining system directly collecting the data from the terror sources like any file or any other data sources now the data mining system applying a particular algorithm from that file and finally the result is stored in the another file here we are not we are not integrating the database or the literal system with the data I mean with the determining system that is the first time second tear is a loose coupling so a loose coupling means that determining system will use some of the functionalities of database or data without sister lives here the determinant system will interact or connect with the Delta but also to deter this but it use only some of the functionalities of some of the functionalities of the Terrell certain database so fetching the data from data repository managed by these systems so determining system is just filter data from repository of that determining what a literal system now it performs a data mining and then stored remaining results either in the file or in a designated place in the database or the data warehouse so loose coupling is a better than no component because it can fetch any portions of data stored in the database or data warehouse by using the query processing indexing another system facilities however many loose coupling mining systems are main a memory based because my knee does not explore data structures in a query optimization method provided by database or the data were of systems so it is difficult for loose coupling to achieve a high scalability and good performance with large data set yes so we can say that lose completely Stern well the database size is very small the only functionality are used or did the only functionality determining system used of data where also the database is it fetch the data from database of the data around data warehouse so required portion will be fetched from the database what is it our house and then after they applied the data data mining algorithm from there and thence to the data on separate file Dorothy or or the designated rocket designated location determiners so that is a loose coupling loose coupling means only some of the functionalities of develops is used by the data mining system the third one is semi-tight coupling sermonette coupling means that besides liking the determining system to database or the Teterboro system efficient implementation of few essential data mining primitives which is identified by analysis of the currently encountered data mining functions and can be provided by the database or the retailer of system this primitives can include sorting indexing aggregation histogram analysis multi will join and pretty computations of some essential statistical measures such as some count lemak standard deviation so here we are connecting the data mining system with a database what effective arouse we are fetching the data from data warehouse more ever we are also do some functions of the data warehouse or some primitives like sorting indexing aggregation histogram analysis so we can all do these types of things we can do these types of things on the data of the data warehouse or the database so that is one more uses or more interaction with the data warehouse so some of the computed mining results also can be stored in the database or the litterhouse them the data that is mining result is not stored in any separate file but also that is stored on the data warehouse and this result can be also presented in the form of histogram or under form of graph so this this is a very so this is so we can say that this is an efficient way that the loose coupling the last one and the fourth one is a tight coupling type coupling means the data mining system is smoothly integrated into the database with the retire house so all the functionalities of data where also that you know by the determining system the data mining subsystem is treated as one functional component of information system determining queries and functions are optimized based on the mining queries analysis data structure indexing schemas and query processing methods of database of the data warehouse system so when this data mining system is completely integrated with database utility warehouse data mining queries and functions are optimized based on mining query analysis data mining queries and functions are optimized based on mining query seller mining very analysis data structure indexing schemas or database in whatever hours so now tight coupling means it is totally dependent on the data warehouse yes the doubt the data warehouse the data mining system will not work so first requirement is it it can it compulsory to have the database rutilated we're always connected with the data mining system then next all the functionalities like I already said it sorting histogram analysis some max Linnaeus all that functionalities also used by our today terminal system and mining algorithms are applied in the data warehouse and the result is stored on the data warehouse only and thiser is also shown in the form of histogram autographs so this is a very F Fisher this is a most efficient because since the results can be stored in the database or the data warehouse system so this will enhance the performance of the data mining system so that is about the integration of data mining system in the database or the data warehouse so simply we can say they turn the node coupling there is no rate every sort of data warehouse data is directly fetching from data sources in the loose coupling we are just we are used database just for facility submitted coupling they're just fetching the data and use with some of the data any functionality it may possible that we are collecting the data from other sources also while in the tight coupling the determining system is completely dependent on the data but also to database without the data warehouse we are not used we cannot use the data mining system it is also provides some holeshot is forwarded by traveling and the result is stored in the data warehouse only so that is all about integration of determining system the database or the data warehouse that's it thank you so much 
GCTCQEdVThk,27,"""Mining Virtual Universes  in a Relational Database""
Gerard Lemson, Max-Planck-Gesellschaft, Department of Astrophysics 
July 13, 2012 

AstroInformatics: The 2012 International Summer School on AstroComputing  
San Diego Supercomputer Center 
University of California, San Diego",2012-07-27T13:08:05Z,"Gerard Lemson ""Mining Virtual Universes  in a Relational Database""",https://i.ytimg.com/vi/GCTCQEdVThk/hqdefault.jpg,UC-HiPACC,PT1H14M,false,112,0,0,0,0,"okay so this is my second lecture out of three today we're going to talk about relational databases and storing simulations in them etc so Matt gave a nice categorization of questions how to ask questions so one of the points you made is that you want to freeze their your questions your science crisis in terms of the physics rather than directly go into technical details then you might want to formulate in terms of the data that you actually have and then translate it into tools that allow you to query the data but he said is there are simple questions that can actually be answered using available tools I don't know if I have a complete exact description this is my interpretation hard questions are those for which you may of the data but you don't have the tools and impossible questions would be those which you don't even have the data so but i want to say then is how can we use databases but i'm going to talk about is to try to use databases to make lots of questions simple and that's our goal how do we do that so YT in a sense of such an approach it it translates lots of your crashes into question to do tools to that can actually work on data that ryt lai lai that way so we try to do a database so what we do is will represent the data in a database using standard techniques then we have a tool that allows you to query the database using standard in using a standard language and that language is actually relatively straightforward so that it allows you to very quickly translate your physics questions in terms of the underlying data so one warning is that if you have a database somewhere you still don't start just asking questions of it just because you understand SQL you have to understand the data as well and here's a website created by Kevin Bundy who started clearing our database in millennium without immediately looking to all the details of the date and they came with we had result luckily there were why is enough to then contact for worker and Simon and aus and what's going on here and then they were told how they had misinterpreted some of the data and then it works better so as a reminder the data that I'm going to talk about comes from the millenniums feet of simulations it's an example it will work it works also for other databases as we've heard like for the bolshoy simulation suite so we have almost all of this information is somewhere in the database I'll mainly talk about this part today their world data particles the group's the merge the trees galaxy catalogs and also density fields so but so this is the background now this is what we want to allow you to ask questions off so first motivation so this is a slide that I got from Allah stole from our legs at some point so if you want to access data lots of things that you want to do deal with finding things in the data making subsets sometimes creating statistics on it that's cetera if you have all of this on files we can do it you have to read in all the files database is actually are much better at this for various reasons so therefore putting your data in a database is actually a useful exercise it turns out so what does a relational database offer so one of the nice things is it that actually there's a relational model the relational algebra and calculus that actually just phrased in terms of more abstract data structures tables or relations so you don't have to know actually about how your data is stored that's very nice but because data formats especially in simulations are everywhere every simulation code is its own data format now I think one of the strong points of whitey is that they will actually try to hide that and has its own io module so actually get at all kinds of different data but it's a lot of work to keep adding new new modules somewhere so a database there for us helps i mean database are not necessarily suited for all types of data but for the data that we are working with it does so so the data is described in terms of this logical structure that I'm I'll say much more about then there's a standard query language relatively standard query language that is implemented it allows you to query the data in terms of the same logical structure so then there are lots of tools that implement this no Oracle IBM has them db2 postgres Microsoft so when they have advanced optimizes that actually do this fast and I'll say a little bit more about that as well they can use parallelization Tony you don't have to worry about that and it has its own authentication mechanism as well it's actually is is useful to give users access to parts of the data for example so I think what's especially is important is that putting your data in a database actually forces you to think carefully about your data you really need to know what's in there because you need to describe it explicitly there are relations between different types of data you can make those explicit then also that query language is rather since it you don't have to worry about I oh you don't have to say I open this file browse through it etc etc in some kind of a format it's much more explicitly in terms of the objects that you that you want to work with you can ask questions in terms of galaxies halos so the this one important step to go from the physics to the data is basically supported by this query language and because of that again because the queries are quite concise and logical it's much easier to discuss them so in one of our hands-on sessions will be writing SQL queries and if you have problems it's much easier for me to actually see where something would go wrong then if I first also have to go through all the lupins and all the i/o routines and all of that stuff so it is actually the query called is rather clean so the concepts so a relational database stores data in relations and the relation is what we normally call it's normally we call it the table so it's basically a table has relations between all kinds of variables here so a table that's sorry so here a table a table has rows subdivided in columns you know what tables are so tables have names they can actually depending on the database system that you have they can actually f they can have subfolders or schemas as we call them so related data values are stored in rows so if I put an object in a table then all the properties of the objects will be in separate column in a single row generally so Rose have columns and for a given table they're all the same all the objects have the same properties column Sims themselves have names and data types and these are some of the names of the data types that people understand so instead of long long or whatever you call it big in but that's not so big of deal and many rows have an identifier so looking back at this so this is a column named why and it happens to be a real here is a column named galaxy ID which is an identifier of a galaxy this this table stores galaxies and this is a unique identifier often called the primary key so it's a key that I uniquely identifies a row in there this happens to be a big in so it's an integer star eight use that very often so here's an integer column number of particles so this is a row this is one particular galaxy identified by the number four so that is another type of column called foreign key columns halo ad descendant ID so they actually point to another table so a database in general has not just one table it has lots of tables and then very often these tables are actually related to each other now for example you might want to have a relation that I've got the friends of friends group that has galaxies inside of it so there's a relation every galaxy belongs to a friends of friends group so these kind of relationships are implemented using so-called foreign key foreign keys now the questions and this is actually a small subset of an old version of our database so every box here if we rectangle is a table everything in there is a column definition every if we roll there with a name all the arrows are these kind of pointers foreign keys from one to another so it's a galaxy that points to the halo it is in there are other kinds of galaxies the light cone galaxies that are represented in the original galaxy catalog s etc the question is why would you want to make this so complex why not just for a galaxy all the information about the halo with this belongs to so this is comes up with the issue of normalization so here we have our model our semi analytics model that you've heard about before now this the purple thing is a friends of friends group the great things are subtitles in the friends of friends group and these red ellipse azure little spirals are galaxies inside of sub halos so what you generally have is that the friends of friends group can contain one or more even zero or more sub halos and every sub halo can contain one or more sub galaxies so what I could do is I store them in one table this is my galaxy sub hello friends of friends table so the first couple of columns give a galaxy ID 112 113 154 blah blah bar with some stellar mass the B magnitude position the blueish columns give information about the sub halo it's in so everything that has the same column color here is actually the same sub halo so there are three galaxies here that all belong to sub halo with identity identifier 788 three so you see I replicate the information then I go to the friends of friends group which is this orangi reddish part and you see that here there is three galaxies that are separately in two separate sub halos all in the same friends of friends ID so this is again replicated so this is a very redundant way of storing so this takes up a lot of space now because it's these four blocks here these four sets of rows all have the same friends of friends ID so instead we normalize it this is a standard kind of pattern in database design you say ok instead of that we just have one table for galaxies one for sub halos one for friends of friends groups each of them stores a unique object you need galaxies unique sub sub titles but now we add this extra column so the galaxy has an halo ID column that is a pointer to the sub halos halo ID column so this one here is unique so for the sub halo the halo D is a unique identifier primary key lexie table the halo ID column is a foreign key column so here you see 6 6 25 points to this first row here 6625 don't know if you can read it so there are two of them here so there are two galaxies at point to the same hair go here and this one again has a foreign key column to the fourth group it belongs to so this these two sub halo six six two five and six six two six both belong to friends of friends group 1 2 3 this is the first one there ok so this is how relational databases work this is the important feature of them that you can have these relationships that you can actually start reversing and that's what SQL is all about so those are the important features right now of databases so if you want to build a database you have to start designing what kind of tables and what kind of columns do I want there so generally what we do is that every object gets its own table and objects like galaxies or halos they have certain properties like mass position luminosity each of these becomes a column and we just give everybody a unique identifier and that certain tricks actually that we use how to assign those then relations are implemented for foreign keys as I've said so you have pointers to the unique identifier column of another column so and we have some special design features so what is required for your data model but is the motivation for the data model actually we followed also in our design sorry we tried following an hour design the approach that also alex and Jim Gray pioneered but say okay you want to design the sdss database we asked 20 questions we ask the scientist what kind of questions do they think they want to ask of the database not in terms of the database but in terms of the physical properties what science questions do you want to ask and then we need to build a database that can support that so we've done something similar here so and and I think we had some of the same experience that I sent out a request for questions to collaborators and basically no answers came so then I pushed them somewhat harder and they're not only Simon came up with a list of questions and then I added some of my own and so some typical question is say return the b-band luminosity function of galaxies residing in halos of certain mass between say 10 to the 13 10 to the 14 solar masses let's see find all the progenitors at redshift 3 of red ellipticals at redshift so you want to change about the evolution of galaxies find the multiplicity halos depending on their environment like giving the multiplicity haters of halos and voids the muscle did the mass function so these kind of questions we are trying to answer most of them we can not necessarily all of them and that brings us later on to the heart or maybe impossible question so what we'll be working with late next week in our hands-on sessions is a database that we have now here in house or here at SESC and one of the nodes which contains a mirror of what is the melee millennium database so a very useful feature of the whole millennium run was that there was a small version of it so the millennium run is 10 to 10 billion particles in 500 mega parsec box the melee millennium was 512 times smaller so 62 and a half mega parsec box and the reason was that we focus achill II wanted to do a quick run to get all the data formats right and all of that so and for us it's also very useful we have that database which is much smaller than the main database that is publicly available people can try out all the queries they don't have to wait too long for the queries to return and then they can go to the main database at the day the queries are practically the same you have to change some day some names of tables a bit but so what we have is well all kinds of objects now we actually in this database it says slightly bigger than the one that we the milli millennium that we have in at mpa we also have particles the role particle data which is in a table called mm snapshots mallu millennium snapshots we have friends of friends groups and we have another table that relates the friends of friends group to all the particles or each particle to the friends of friends group or sub halo it is in so we'll be asking some more interesting queries and we have mpa halo which is really the merger trees so each sub halo was defined without its merciiii information and then the merger trees are based on that we have galaxy catalogs to two of them here and then also the our collaborators in Durham they ran their own seminole it extend their own halo finders and they call them d halo for durham halo and they have their own galaxy model Bauer 2006a and then we have some other some other fields so this is our relation and this is are these our main tables in the database that you will see here and all these root these arrows are relations between the different tables but there are many more lots of other kinds of relations as well like there's not just a relation from halo from galaxy here to a halo but also from galaxy to galaxy like every galaxy is in a merger tree and it knows who is my descendant so there are point is not just from between tables but also from table to itself and the same is true for the NPA halos etc ok so a very small short word also on generally it's not enough to just have the tables but if you want to execute the queries in reasonable time you need to do some kind of tuning you need to to get around the bottleneck that is I oh I mean generally databases database tools what they do is they optimized I also they minimize the amount of IOT he need to do because it's by far the slowest part a database with SSDs may start changing a bit it's still slower but the patterns may become different but still so what the database just try to do is to avoid it as much as possible but the database is generally are too big to store memory completely our database is now about 10 terabytes and the memory we have on a database machine is about 64 or 32 gigabytes so we can't store it so you will have to do i oh so if you want to find rows of interest in a certain table one thing that you could do with your scan through the whole table now and if you have 1 billion rows then that can be slow I mean it depends on you this speed but it can be order of 10 minutes so to go through our whole galaxy database galaxy table so if I want to find a galaxy with certain properties like whether the mass of the galaxy is between range of values like 10 to the 10 and 10 to the 11 solar masses then if I have to scan through all of that that is a long query know that that would take a long time if I could do something else if I had four ordered my table by a certain set of columns or one column like by stellar mass then and if the database knows about it and databases can know about this stuff that you can actually do a binary search which or something like that or the database implementation of that and there you go in order of log in it's much much faster and that is nice you can do the big and only order a database in one way now you cannot both order by time and by and separately by the mass for example and so for that purpose and there's a separate concept in database called index and indexes is it were a small column a small table sorry so this is our main table of galaxies with lots of different properties like here's Stella Mars here's the redshift which we often represent actually because we have a discrete set of outputs we have generally in almost all our tables a column named snap num that says that goes from 0 1 2 until 6463 for the millennium 67 for the millennium tool that's the the snapshot it's an integer so it's much easier to ask questions on but so there's lots of properties here as the B magnitude so what you can do is you can define you can tell the database create an index on this table based on this column for example here snap num stellar-mass i right here also galaxy ID but it's basically an implied column for if you create an index and the primary key is always included so this is one index that's basically contains it's all at first but snapshot then by stellar mass okay here's an index that only has the B magnitude here is an index it has the snapshot and x the x position so what these indexes allow you to do and well sorry what a database engine allows you to do is ignore these indexes you don't have to know about them explicitly in your language but if you ask a question of this database for example give me all the galaxies whose snapshot number is 63 and whose Stella mass is between a and B two values then the database engine can translate is clear it knows hey there is an index that is ordered in this way so instead of going through this big table which may have hundreds of rows and and this be and is not ordered so i would have to scan through the whole thing read all these rows all these blocks into memory instead it can go to this index first of all the index is ordered and the database knows of this and it can actually could as it were dual buying a research so it's log n steps instead of n with 4 1,000,000,000 is a lot but also it only has to go through two columns so it's also much narrower it doesn't have to read as much data in memory now if that's all that you want only give me the snapshot and the mass and you're done oh you don't even have to look up any of the other information but even if you want to look up more information it's not included in the index and at least this gives you a very quick step to get maybe the few thousand galaxies out of the 1 billion it actually conform to your constraints and now you go and do a quick look up using this galaxy i did this unique ID that along with the main table is ordered to actually find the actual data so this can be a very large speed up and the same would be true with any of these indexes so the trick important part of database engines do is to actually take an SQL query that i will show you later and cut it up and execute it in a way that optimizes the excess patterns and doesn't always work sometimes you still have to help but it does a pretty good job okay so the main part of the rest of this talk is going to go along some example queries just to show you some more of the concepts and so the database is that we that i'm going to consider there in examples are the milla milla are the Millennium databases there's others but I know these better um there's one at MPA which is built in a Microsoft sequel server basically because we already had a sequel server instance with the net CSS mirror and all the collaboration with Alex and this one contains also the full millennium database so some of the queries actually will use data from the for millennium database and then the mill email database here at at here at SDC called a sack tap this is the one that you'll be working with next week as well I'm actually Shh okay the the database is documented here so there's a web page which was basically our original design which was a very simplified version of the whole cast jobs thing to allow people let me just show you so this is our MPA interface to the database so there's some documentation on the left side you can click here you have sorry about that so here's a whole list of tables that we have available there you can find documentation for these tables all the columns that are in there and the meaning then there is a private version so for which you need to have an account and then when you check in or when you log in you see many more databases but also when you register you get a mighty be the way Alex discuss this so you get a 1 gigabyte the database internally and you can actually run queries in such a way that you can store the data in the database for it so you don't have to retrieve the data immediately you can just store it temporarily and then here locally we have now a an alternative interface which is something that we are also going to push more on our own side it implements a a so-called tip serve as a table access protocol from the virtual Observatory Alliance I'll come to talk about this more my next lecture on Monday here the database is divided the schemas and I've already created for each of you a schema here I will send out an email later today to each of you with a personal username and then next week you'll be able to start using that to us start uploading data in your database so this is another query interface that actually also allows you to run queries I mean that's what we don't have yet in our main main site so then there's another interface which is top get which is a visualization tool that has some similarities to the tool that Peter Schultz few points it's good at also a tattoo 3d things so in top credits is built by Mark Taylor for formula StarLink you can send queries directly to a millennium database there was a before this Ted protocol came out he actually added this he also here you can then send queries you see how it goes and then actually you can start just to show this part so this is a I think I showed part of this last week so this is all the halo so this is a similar one to what Peter showed so these are all the halos in the in the millennium database now color-coded so the one of them that somebody asks whether you could actually call a code by a particular column here they're coded by the snapshot so each tool has its own advantages of course but but so the advantage of this is that you can actually query databases using the tab protocol immediately you can even from our interface once you have run a batch career you can pour the results directly into this talk that I'll show more of that later okay so the most important part for using a database is the query language that you can use so this is called SQL or sequel or however you want to say that so it stands for sequential query language it allows you to filter data sets to combine different data sets to right joins etc it allows you to define functions procedures you can do statistics with it there's also a part where you can actually manipulate data can update the table you can insert rows you can delete rows from it we don't allow you to do that with our databases obviously but in your mighty be you are able to do that you can they're actually create tables delete them etc a nice part of SQL is as a result of SQL which works on tables is again itself a table and we'll see that later so the result of it can be loaded into a database again into a table it can even be used as a in sub queries as I'll show later okay so table creation is something that you may well be doing next week so there's a very simple syntax create table and then the table name should be merely mil-dot MP halo here now you just started finding all your columns comma-separated so you have won a halo ID column which is a big in so it's an integer star 8 and it's not null so it always has a value it has to be the case otherwise you cannot uniquely identify them then there's all kinds of other things like a descendant ID a last last progenitor idea come to talk about that later either today or in the beginning of my next section I don't know if I'll be able to finish the whole thing today these are foreign keys and then there is this snap num that I talked about which is an integer even though we also have the redshift which is a real because it's a real and may have five significant digits behind it in the decimal point it's much harder to using clear results it's easy to use this snap num and we have somewhere table that just links it to together so the value 63 is important it corresponds to Z equals zero they have XYZ vxp wife is at all kinds of properties of this table so once you execute this then the table gets created that then you can start clearing that you can then put data in in different ways again things that we'll discuss next week okay but the main thing that you'll generally bill doing and with public databases you will not update them and insert them of course you'll just query them and so the main pattern for SQL queries to query is select from where the order is not quite generally you should read it as from where so from this table and maybe these tables where something is true select these columns so it's just that socialite could come at the end but it's in the syntax it comes in the beginning and a different way so the simplest one is select star from table okay so the table here is identified by the table name and the schema name so in a say in the this a sack tap website where you can type in your own queries I won't really start demoing running the queries here I'll do that for the hands-on sessions but the database is subdivided in lots of different schemas each kima its own name some of them will have your personal name this is where you put your data but there's a couple of redefined one so one of them is merely meal and it has a table named snapshots which has sixty four rows in it that says snap num 0 corresponds to redshift hundred twenty-seven snap naam 63 corresponds to Reggie of 0 so this one will just Desai frequently run just to remind myself if i want to go to Reggie of two with snap num should i use now because it's easier to write it and also most of the indexes that we have in our tables are defined in terms of snap norm rather than redshift ok so star means give me all the columns okay so if I want to get all the columns of a table i can just say select star i could also say select snap num coma redshift like here comma NP in this case here a query the mpa halo table so this is this gives me all the rows all that the halos in the merger trees that we have in a millennium and but what I only want to have is the snaps snap num the redshift and NP which is a column that stores the number of particles in each halo so that's what I can also do so I don't have to take all the columns I can only take this subset and of course if I'm only interested in these then why would I download all the columns which may be for some galaxy catalog hundred as the Assessor stables I think with 500 or more columns if I'm only interested in this then clearly the network traffic goes down a lot another advantage would be that if there were an index on the database as I said on the table as I said before that only has these three columns it's much faster to clear as well particular if I then start adding constraints like this so this one still gets all the columns or sorry all the rose out of the table based on all the columns this on a subset here now I add an extra constraint I'm only interested in rows in halos where the redshift is 0 okay I could have said here where snap num equals 63 this is a little bit more physical I would see here again get all the columns but now only those with wretched 0 and again clearly a big advantage of all of this is of course that I can make the server through the selection now I mean one of the important things is that we always about that s ESS is about the CSS databases that you can filter out that subset of the data that you're interested in and if I'm only interested in things that's redshift 0 why would a download everything now I mean some of our catalogs would be two hundred fifty gigabytes if here i can divide it by 60 then the total amount of data that they need to retrieve is much smaller so this remote filtering is a very important feature that clearly with the databases growing in size is very important so the where conditions can be many notes 0 equals is a common one but you can also have less than let's not greater then not equals this is also not equals there's some other kind of syntax like you can say I want to have all the Halos that have particles that the number of part that have a mass between hundred and thousand parts calls and so as literally this between so I can say NP between 100 and 200 which is equivalent to saying NP greater or equal than 100 and NP less than or equal to 200 so it's a little bit shorter you can also have a like especially for string so you can if you don't know whether joel is in the database with this full name but you know its last name will be there then you can say the name like and so here percent means a wild card you can also have multiple statements you say where a equals B and D equals e so you can have lots and lots of constraint or just single one also you can say I equals B or e equals D so you can take both of them if you want to have a discrete set of values then instead of saying where ID equals 1 or ID equals 2 or ID equals 3 you can also say ID in one two three sometimes useful there is one special feature in databases that is it sometimes certain columns you can define as being not null as a show before so there must be a value in that column if you don't define that then it's not required to have a value there if you want to find out all the rows actually we're there is no value that you have to use this kind of a is null statement if you would say a equals null that is always false it's like a kind of triple valued logic here or if you want to have only those values where a is not null you have to explicitly say a is not now so not a not equals now that now always even if you say we're null is null null equals null you will not get anything so whatever you compare to null with equals not equals whatever it is it's always false and so then there's something else like and it where exists is something i didn't inside of that you could have other query so and also going through some examples so i'll try to phrase the questions in somewhat physical terms and then give you the SQL and then i won't run it here on the hand the first hands-on session i think we'll just start running queries online and maybe you can come up with your own career is then and we'll we'll see if you get all of this going and these can serve as some examples so here what we want is to give all the galaxies that wretched 0 but only in a thin slice defined by z between 20 and 25 so I want ever the thin slice galaxies so what I do here so this is now the query that I asked I'm only interested in XYZ and in the stellar-mass the query so this is my galaxy table that I've selected this is from the previously latest model from grow at all in 2011 is actually one of us this is the one that Joe will refereed so we have a milli millennium version of that as well so redshift 0 instead of that I use this snap number 63 decision just happens to be nicer and on our bigger tables the indexes again I defined in terms of snap norm it's just easier to I mean if in a real you miss one significant digit then you would not find anything with integers are somewhat harder to do so I want to find only XYZ 10 the stellar-mass from this galaxy catalog or the snap not to be 63 and that should be between 20 and 25 so I copy this query into Top Cat and this this one tool it execute the query it's a day then you can plot it directly so this is a plot created by that and so this is a thin slice you see some filaments some clusters there in top get what I could now also do for example is you Stella mass to color this you'll try that out on Monday so this was a simple query and in principle you can do the same also on the full millennium and it it returns of course it's in guarding it takes network traffic the database has to execute its as to put it into a comma-separated format which is the default that we use their it has to return but these kind of queries are relatively fast okay so other things and while going over these examples I'll also use introduce or more new features of the language so one thing that if you wanted you could rename the columns for example if you don't like snap numb as a column name but want to call it snapshot index or redshift you want to call Z door NP number of particles you can do that with this as construct so I say select these columns from them but we named snap num to snapshot index what you get then is a table comes better indeed snapshot index Z number of particles because people can do these kind of things in SQL that's now somewhat of a problem because now I give you this table and if you would just look at the column name she would not know actually where it comes from anymore so metadata associated to this column this is actually one of the tricky parts in this whole virtual Observatory approach to tap to the table access protocol is that these things can be done and people have to therefore we have to start interpreting the queries to know that snapshot index is the same concept as that other thing that is snap num just as I'm a warning okay other things that you can do is that you can do manipulations in the Select part so what we have is we have in the old delucca table we have the old 2006 we have bv hi RK magnitudes Johnson magnitudes not color now we don't have B minus V and why would we because we already have the information so but you can actually in your query you can say select not I select your Meg be dust but I can also then do the difference of these two and call it B minus V so what this gives me is a collar and a magnitude and it's actually a column name type which for us Amina litical model and have values 0 1 and 2 so 0 and 1 are both sent galaxies at the center of a sub halo 0 is also the center of the central sub head over first group and to our galaxies that have lost the subtalar the satellite galaxies there's another small trick here sometimes you don't want to download the whole thing you may want to download a random subset there are ways in some not standard ways in SQL to do a random selection what we've done is actually add a column called random that we gave a random integer number when we ingested the data in the database between one and a million and so this allows you to create some random subsets somewhat more easily so that when you then plot it you don't have 1 billion galaxies or well 20 million but a much smaller part so this again was run now against the full millennium catalogs it's about 1 billion galaxies so near we plot so this is the B magnitude this is bright this is faint this is B minus V this is red this is blue there's a color coding that it's an integer so black is type 0 so those are the central galaxies at the center of friends of friends groups read our type two galaxies that satellites they don't have their just basically dead they can't grow anymore and let's have some cold gas they may form it into stars but generally no new guys were cool on them green are the galaxies that are in satellite halos inside of friends of friends group so you see here a little bit these the red and the blue branch that we know about it's not a detail right of course so so one of the things that you want to do often is to do statistics on on the the data that you have and so one thing that you could do is you can download lots of data and do your histograms and do your statistics if you've downloaded in some other scripting language like for example one thing that we allow you to do is run IDL queries with a simple library somewhere that can actually send using a double you get spots spawn 2w get to get the data from a database directly into IDL and then you could do all kinds of stuff in there but again that you might need to download millions and millions of rows instead the database allows you to do this thing itself as well so you can do kind of aggregation as it's called so you can count things you can count the number of galaxies you can sum certain properties you can take a maximum or a minimum or an average or standard deviation and there's all kinds of features that you can use them so the syntax for that is this so here what i say is from this galaxy catalog again we're at redshift 0 only type 1 galaxies I want to know how many there are so I say count star I could also set count galaxy ID or something like them and I named it numb I won't have the maximum stellar-mass and I want to have the average stellar-mass so this now means that i can just when i run this i get three values back where the number of such galaxies in the melee millennium was about 4,800 the maximum mass was 26 in units of 10 to the 10 solar mass the average mass was point 6.66 so this is data that you could now use I mean counting is often a useful thing to do if you want to do a count you first if you want to download try some data but you might want to know how much is there so you can first around the count query and then after that you could try to download everything but again so this is a few lines that allow you to do these kind of things no well in our particular case it would come back without a name there are depends a little bit of the client tool so sometimes indeed they might call them X 1 X 2 X 3 or something so there are ways of doing that another thing that you can do is to order I see that this is still an old all clear so you can this is not always useful there's a few useful things that you can do by so when I return all my heroes I might want to have them ordered first by snapshots in descending so that's the highest snapshot which is the the most recent redshift first and then by say X if you do that then in your result indeed the result will be ordered some problems with this if you use is really on large data set some people run careers against the database uses order by because they want to still download all the data and they want to have it in this order already but very often the database actually starts slowing down if you order because it needs to first get all the data and then start ordering or if you don't order or big and because it knows it needs to do that at the end sometimes it chooses a different execution plan it uses an less optimal execution plan and since most of the time when people will do is analyze this set of data afterwards they can either do the ordering themselves or the you scatter plot so nevertheless as possible so here the order was first but snap num descending which is why the first row is all at 63 that's our highest value and then by X is sending it so this is the lowest x value point 0 0 1 10.03 etc so you can order by multiple columns I mean if I would order by XY that's not very useful because there are hardly any unique or what is its multiple values of x with these integer columns that is useful so now I first have all the galaxies at redshift 0 then all the galaxies is redshifts 0.02 etc that happens to be the case so this is sometimes useful I'll give some examples later I thought that we're already here sorry sometimes it's useful there's some features where you may want to use in so another very useful concept is grouped by so when we had before discounting what you might want to do is I might want to count all the galaxies but I want to do with / redshift so for each snapshot that we have I want to count how many galaxies do we have now and if i go here but i did here is I only wanted to count galaxies at redshift 0 and type 1 now I might want to say give me also all the galaxies at redshift 0 and type 2 and type 0 then at the previous redshifts oh snap shot 62 etc so I could run all of these one by one which is possible adjust the pain not efficient either so this group I concept actually helps there so what you can do here is you have your ordinary select star from and at the end there might be a where clause here as well at the end what you say group by and then one or more columns so in this case of the say is grouped by redshift and type so I could have used snapshots of snap numb but grew by does is it takes these columns it finds all the distinct combinations of these columns and all the galaxies that fit in one distinct combination they put are being put in a group okay so this gives me now in principle since I've got maximum of 64 different redshifts and maximum and and three different types it will give me three times 64 different groups maximum okay this works again because it's snapshots are discrete so for each of these groups I cannot do something out for each group I have zero or more rows so what I can do with these rows is do an aggregation so I can do I can do a count I can maybe do an average and a mac so this is what I did before but then I headed for one red shift and one type now here if I run this whole thing I get it for each red shift in each type okay so it's one extra line a few extra lines only so so therefore now what I will really do for each group I do the count the average telemar the max LMAO only for this group so if i run this query but i will get back i get back two columns redshift and type and then the number of galaxies in each group for that red shift in that type etc so the only one the only columns that they can have here that are not in an aggregate function like count or max or whatever are the ones which I group so if i run this I get and I plotted with color and now here I've got red shift here I've got number of galaxies vertical and I color them by tight so green is type 1 red is type to the satellites and black is type 0 so the shows is that first of all red shift here is 0 here's high is that you get more and more galaxies no it's not sorry such an it starts tailing off here and then you don't grow that many anymore you see that there are many more type 0 galaxies no and this is because most of our halos have one sub halo so every galaxy they're the type 0 there's a couple of type 1 galaxies with many more type 2 which are the satellite galaxies that have whose halos have merged into a friends of friends group and then the friends of friends the halo itself has dissolved what we keep tracking the galaxy for a while so again this is as you saw this is a simple query to produce quite a non-trivial result so similar I can do multiplicity functions and there's an extra trick here so what I what I want to do is I want to basically count galux or in this case friends of friends groups in masters in bins in bins of Mars I want to make a histogram so what since the mass itself is rather it's real but you can do with what you usually do you count things in cell so there's a trickier first of all I want to do it again for different redshifts so i select snap snap now mandy's redshifts these values that correspond to Reggie of 0 1 2 3 then I group by that snap num so for each snap num I'll have a group and then inside of that I take the log of the mass I mean generally if you want to have logarithmic and then I take a floor which is the largest integer not greater than that value I have actually divided it by point one this log n P so what I get now is basically bins in point one decks and I do I bring that back here as well and then you get this so this was run against the full millennium this is not a friends of friends multiplicity function is this log of n so this is 1 million particles this is redshift 0 1 2 3 and so you see that you get more more higher mass friends of friends groups and again this was this that did that so first yeah so in principle this is already the translation of your science question to the data but it is all still pretty much in terms of the actual object this is the friends of friends table that has friends of friends group that has certain properties okay but the important part for databases is when you start joining things now that's really where database come in their own aware if you wanted to implement this yourself you would have to almost start implementing a database so what I would like to see is for example we know that galaxies are in halos so and we have a galaxy table as a show before and a halo table and this halo ID here points to this halo with these are red green blue and yellow they point to the corresponding things and so what I would like to find for example is all the galaxies in halos of mass thousand particles since I don't have I should not have the the mass of the halo inside of my galaxy table what I do is I do a join I say from the galaxy table coma the halo table says it will make some kind of a you can think of it as like a Cartesian product for each galaxy I take all the halos that's what this means as if I created virtually a table that contained just a join of these two things therefore each of these I puts the whole table so then I started looking now I have also all the extra columns and I say now give me only those four with NP is thousand so it's thousand particles which come from this column here and where the halo addy of G sorry note that there is an alias that you can give to a table so if it's a from Millie mill grow 2010a i call it geesh i went in where or in the Select later i refer to that two-g then it refers to that particular table you can also use the table name but that will only work if actually have unique tables in their cases where that's not like the next example will show so but what the important thing is this is this column so I'm only into it i want to get rose only where the the galaxy halo d is the same is the halo halo d so this tells me really that i have the galaxies in these halos so first you can think of it that the database might first find all the halos with NP is thousand then finds for them all the halo woodys then does a look up in the original Galaxy table to find all the galaxies with that halo id whether the database implements it like that probably not probably faster and more efficiently using merge joins and having indexes on halo ID and all of that but that's what it comes down to so for example what I could do here is I take the halos here again is the same honest before at where the halo is only at redshift 63 so I don't have the the data for that how did in the plot but that so one of the things is also and this is where it's important if you have this aliases you can also link the halo table to itself as i said before every halo knows of its descendant so what I want to have here is is I want to have of all the descendant halos here the head that they call descendant with mass larger than 10,000 particles at which of 0 I want to have the progenitor so I want to just have progenitors of massive halos but you can make it as complex as you want you can you don't have to stop with two you can do multiple things here like for example but I would like to know here is maybe it's on put will come up with some questions and as a test have you created SQL for that in the hands all next week is so what I would like to know is further our friends of friends clusters that have I want to know for friends of friends classes with certain certain Mars I want to know what galaxies do they have and not only do I want to know what the alex is do they have a motor military calculate a luminosity function so the fact is our galaxy's only know of sub halos they don't know actually directly of the friends of friends group they're in so what I need to do is I first find all my friends of friends groups here so I in my first table like this is friends of friends table I call it f I find those who have between thousand and thousand ten particles they may be in this list of snapshots so this was the redshifts 0 1 2 3 so now I've got all the hey all the friends of friends grouped with these constraints now each friends of friends group has zero or more sub halos so now I find all the sub halos for these friends of friends groups using a foreign key between the sub halo table and the friends of friends table called for Phi D then I want to find for each of these all the galaxies that are inside of them so we're G has a column called sup hey Lloyd e it's the same is this a priority so what I now will have is I have lots of combinations about friends of friends sub halo galaxy so now I'm going to group by the snapshot number the snap num of the friends of friends group and I'm going to make another kind of bin in the dust magnitudes the the B magnitude sorry the B magnitude of the galaxy so what this will now then after that give me and then I just count for each of all of these halos at different snapshots I now get a multiplicity function like this so this is arguably more complex SQL that shows you what you can do so what these are so in black we have wretched 0 red red shift one blue 2 Z 3 so this is the blue magnitude so this is bright this is fainter this is the total number in the magnitude bins so this is now for hail of friends of friends group of a certain mass what is the distribution of galaxies magnitudes inside of such friends of friends group so one thing that you see is that it has a peak generally and then it goes down so the central galaxy of these friends of friends group has a characteristic luminosity that actually changes with time gets fainter at later redshifts probably because they just fade out there blue or maybe in there in the beginning or it just just forming where that stars so and so these kind of characteristics this is a rather non-trivial results that you can obtain by sending a query like this without having to download all the data this is all the data that you will download now it's just a couple of histograms so this is one of the big strengths of this that you can ask these rather complex questions so what I said before one thing that you can do is that the the result of a query itself is again a table you can use this to in the front claws I have here another query so this is a sub select so I can use it as well and it's here useful as I said before you can actually limit realized i missed somewhere some slide but you can also limit the total number that you return so what I do here is I find all the halos in my its redshift 0 I ordered them by mass descending and then I say limit to 10 so in other in sequel server you would say select top 10 and what this gives me because it's all that it gives me the first 10 rows so the 10 most massive such halos and then for those only and this is why it's a sub select because I really I need to do this limiting that's the the set of headers for those halos only I now join to the galaxy to find all the galaxies and actually here at its limit 20 so this now gives me the 20 most massive halos at the rest of 0 plus all their galaxies and the galaxies here are colored by stellar mass of that every galaxy has every halo as a red dot in the center which is the most massive mass generally okay so we can do many more complex things and they say I think I won't go also for time through all things I just want to have a quick chance to introduce what I'll go to talk about next so you can do quite interesting galaxy by galaxy comparisons of different models here where it is easy because every the two galaxies models the lucha amber tone I have the same IDs because they had exactly the same urge your history so every galaxy has a one-to-one corresponding correspondent so here we have stella Mars color balls Mars for two different models so you see the Sun correlation the balls to a total ratio is hardly there the color is not that great care well correlated it would be nice to one of the comparisons we want to make is with Peters with the models that you will create with Darren's code which will probably not have the same relationship so we know that each halo will have galaxies inside of them and we had something similar with the Durham model so we dare we don't have a one-to-one relation between the galaxies but we have one-to-one relation between central halos of their halos and are so we can look at the central galaxies of halos that we both recognize and again we get a pretty good correlation there but not everywhere and it's a more complex query now so I think at the beginning of my next lecture I'll go over some special design features still so this will be monday morning lots of nice things leading to talk about thomas we will talk about some spatial indexing features so we'll talk then more about some special design features of our our database like that we have special way of identifiers we restore density feels smooth the different radii you can find galaxies that's so that you can do queries based on environment and particular also how we can find out about the evolution so about Marie's your trees let me just quickly skip over this now and so so one thing if you want to go back to mets categorization of questions because it will also lead to another part of the of the next queries so we have simple questions some of these were relatively simple at least it could all be on sit within the database know both the SQL is available to do it if you wanted to do an FFT on our data that would be a hard question the data is there but what you would need to do is you will need to download all the data and run your f of t locally and get the power spectrum we don't we don't support it in the database the galaxy is working on things to actually do that there as well either ie to make such questions simple so there are also impossible questions like we cannot at the moment answer you questions about what galaxies we have in a different cosmology because we only have one simulation now so if you would want to get our wmf seven cosmology prediction if we just don't have them so that's an impossible question however this is then where it's nice and now also both show is becoming online because they do have wmf seven they don't have the galaxies yet but it will come as well and so what is important there and that would be the second part of my next lecture is that you should go out and see if you can find it elsewhere so there's somebody else publish that kind of data and this is of course where the virtual observatory promises to help that you can actually maybe find these kind of caves and that's what I will talk about next another thing could be that you create your own now say it was impossible because we don't have the galaxies the galaxies run with Darren's latest code but if we were to make it possible to do that actually that would be an alternative solution so make your own data okay so for the next Monday morning I'll continue first with some special features of a database it can also help you then building your own and then I'll talk it a little bit more about the vo and then in the afternoon we'll ever session but we'll actually just start running previous ourselves and thursday will actually start building your own database thank you yes yes so we keep track of it mainly because the people the serious users they all have an account and so for each user they have to log in first and so we we actually keep track of that so we have a think by now 560 almost 600 registered users not as many as I CSS in many respects I think with ten percent of SPSS or somewhat less apart from in the rows returned I think they were roughly equal at least the last times I heard some statistics again probably because in our case people ultimately will need to do statistics on set so their data sets per clear we may be larger than for a CSS but people might be getting some information about specific objects and in principle we could also get IP addresses except that the moment we're behind some kind of a proxy and so it's harder to find out what the remote IP addresses that would be for the public databases I think we all keep them authenticated for quite a while is I'm sorry is what the well alex has his his language yeah so it would mainly be for some of the specific features that you have inside of the database but the first thing I think on our page also I've not tried to write my own tutorial a point to to that 1i there's some other school w3 schools they have some the basics of SQL I think demo queries are also very important for this because they are so we have demo clears on our site and because they teach you both the examples of the SQL as well as the actual data mobile that you have so I think that's very important and there the the multi dark side has spent much more effort on that already so yes no no no that's correct yeah slower yeah I am particular of course if you didn't also want to do some lookups I mean it generally I think what we won't want to try is that the slowest parts would correspond to d2 a sequential scan sometimes the database does it much slower and what I also need to do I mean the concept of index is already I mean to set a concept of databases and you the concept of index requires its own documentation but I still need to do that to add define all to list all the indexes that we have so that actually if you look there you understand the concept of index that you realize oh yeah I don't need to say select star because I am really only interested in these columns and there happens to be in index it uses that as these columns so then i can probably speed up and queries as if I would stay start on the database will actually go out and and probably does a different less optimal access plan but it's yeah I mean I think it's a variable so there's a query on my main website that finds all the indexes that will actually create the database and tell you what indexes there are and that could be made a little bit more user-friendly but it's important information four faultless was a wonderful huh I had questioned about the scalability Arthur scalability limits with the existing hardware software platforms Philly to Rose is also yeah probably Alex knows has thought more about is very painful and it takes it is proud it's generally once we have a leg we attend structure the deal is so we are and one of the advantages of SQL that I find is as I said before that it's quite easy to communicate so I am also the help desk for our site and that's necessary also explaining what's there but also sometimes a database just does actually not choose the optimal plan for whatever reason and it might be that if you said this index here and this index here then then it would work but then for every request you might have your own set of indexes which would also be there but sometimes if you just rewrite the query a little bit using instead of a so-called inner join a left outer join you can actually force the database to put it in a certain thing but that becomes a little bit of an art on the other hand one that once you have it then it's because people just send me the SQL appear say this is slow I can go in a tool I can see why it's slow and problem generally within a few minutes I can find an alternative way you need the experience of course to actually make it go much faster and much might actually be instead of hours a few seconds sometimes because sometimes the database just choose is really the wrong thing sometimes it tries to even make a copy of your whole table somewhere else in a different order where you can I've not turned it on because we only have four cores and then you would have multiple users would get in each other's way but it's there's a these parallelism and that we use a rate system so we have 10 days so that I always faster for us at least but and you can actually turn it off and I actually for our original date is I turned it off I said 11 core / / process so I didn't because mainly because I actually notice it's slowing down certain certain patterns as I said I don't have enough experience with that to me but thing is you have to start paying per core for a sequel server licensing assume at the moment it's still per processor but what Alex tells me we'll be perform and then 256 course all running sequel server is going to be yeah okay but once you have you know let's say love so for us as a small Institute it's now one license for a quad core machine would be 5,000 euros and enterprise license so let me first I did so the instance that we have here is open source it's false caresses is it except that we've installed locally I don't have as much experience with that I started with it in the past but I think sequel server is nicer to to do things with impose Chris but Alex "
KbuVFxT06ik,22,"Introduction to Data Mining by Dr. Julie M David, 
Assistant Professor
Dept. of Computer Applications
MES College Marampally",2018-10-31T09:57:55Z,Introduction to 'Data Mining',https://i.ytimg.com/vi/KbuVFxT06ik/hqdefault.jpg,MES COLLEGE MARAMPALLY,PT9M59S,false,36,2,0,0,0,[Music] welcome to mes College marimba eLearning in this session we are going to discuss about the concept of data mining otis data mining data mining is the process of extracting knowledge from the databases so data means large a huge amount of data is present in the data repositories and these letters are mined we can extract a particular novel knowledge from this databases okay in this classes we are discussing what is data mining and what is the relevance of data mining and what are the steps for required for data mining and what are the different methods in data mining and especially decision tree and neural network so first we will go to Otis data mining what is determined data mining is the process of automatically discovering useful information in large data repositories these techniques are deployed to score large databases in order to find novel and useful patterns that might otherwise remain unknown they also provide the capabilities to predict the outcome of a future observation such as predicting whether a newly arrived customer will spend more than 100 rupees at the department store not all information discovery tasks are considered to be data mining for example looking up individual records using database management system or finding a particular webpage through a quarry data mining and knowledge discovery process is an integral part of knowledge discovery in the databases which is overall process of converting raw data into useful information this process consists of series of transformation steps from data pre-processing to post Pro post-processing of data mining the input data can be stored in a variety of formats and may result in a centralized data repository or be distributed across multiple sources the purpose of pre-processing is to transform the de role input data into appropriate format for subsequent analysis the steps inverted data pre-processing include data cleaning data integration data transformation data mining pattern evaluation and knowledge presentation so first we will discuss what is data cleaning data cleaning means to remove noise and duplicate observations the data is reside in centralized data repository or be distributed across multiple sources you obviously that should contain a large amount of noise data so in this process we applied different intelligent methods to clean the data that means to remove noise or unwanted data or duplicate data from the data sets and second one data integration data integration means integrate the data for the application we are using a particular format that format is for the integration process we are integrating the data into that particular format that will increase the speed of executions and third one is data transformation data transformation means transform the data into the relevant format then next one is the data mining process it is an essential process where intelligent methods are applied to extract native patterns this indigent method has a capability to extract a knowledge that is expressed in a particular format that is called the patterns this pattern patterns can be expelled expressed in different format depends upon the applications and the next one is patent evaluation identify the truly interesting pattern that represent knowledge based on the interesting meshes and the last one is knowledge presentation visualization and knowledge representation are the two things present in this knowledge presentation these are the different steps of knowledge extraction process or data mining process betterment has different types of operations if they can be categorized in two different ways mainly into two ways the first one is predictive and second one is descriptive the predictive operations are regrets classification and filtering and descriptive operations are clustering Association roles and the deviation detections so in this class we consider only the classifications so what is classification classification is a form of data analysis models describing imported data clusters such modest code classifies predict categorical class labels and this class labels gives information about what are the different classes present in that data basis and this classification consists of two step process it is training and testing in the first step a cluster is built describing a predetermined set of data classes or concepts this is the learning step and it is also called training phase where a classification algorithm build with the classifier by analyzing or learning from a training set made up of databases database toppers and their class levels the class label of each training couple is provided is known as supervised learning without class labels we can say that unsupervised learning and this algorithms are also classified into two ways so provides algorithms and learning algorithms and unsupervised learning are the algorithms test set is staples used in the testing phase the accuracy of a classifier is a given test is the percentage of tests apples are correctly classified by the classifier and the one of the main example of this classifier is decision tree it is very user friendly and it can easily extract the knowledge from this decision tree and in the it is a learning of a decision trees from class label training tuples it is a flow chart like tree structure where each internal node denotes a test on an attribute and each branch represent an outcome of the test and each leaf not hold with the class label the topmost node in the tree that wrote not this is the most important attribute in that databases so this is the decision tree and next one we will consider that neural network that is very important neural network and it consists of an input layer output layer and one or more hidden layer neural network is a set of connected input/output units in which each connection has a weight associated associated with it during the learning phase the network learns by adjusting the weight so as to be able to predict the correct class level of the input tuples neural network learning is also referred to as connectionist learning due to the connections between units the units in the input layer our code in good UNIX output layer output output units and hidden layers and hidden units and sometimes these output layers are known as neurons and these are the things about the neural networks so in this session we discussed the two classifiers neural network and decision tree it has so many advantages and disadvantages so what are the advantages and what are the disadvantages of these classifiers first we will consider the decision tree the important advantages of this decision tree are it is very it will take very reasonable training time and which is very fast applications and it can easy to interpret the knowledge from this decision tree and it can easily implement but the main drawback of this decision tree is it cannot handle complicated relationship between the features and the decision boundaries are very simples and lot of it lot of missing detrás are there it cannot it the decision tree has no solution for handling the missing data these are the main drawback of decision tree and when we are thinking about the advantages and disadvantages of neural network the important advantages are it can learn more complicated class boundaries and it is very fast application and it can handle large number of features and the main drawback of this neural network is should take slow training time and hard to interpret and hard to implement and these are the main things we discussed in this class 
GDdd0XNUhPU,28,"This is the ninth session in the 2017 Microbiome Summer School: Big Data Analytics for Omics Science organized by the Université Laval Big Data Research Center and the Canadian Bioinformatics Workshops.  This lecture is by Mickael Leclercq from Universite Laval. 

For tutorials and lecture slides for this workshop, please visit bioinformaticsdotca.github.io. 

How it Begins by Kevin MacLeod is licensed under a Creative Commons Attribution license (https://creativecommons.org/licenses/by/4.0/)
Source: http://incompetech.com/music/royalty-free/index.html?isrc=USUAN1100200
Artist: http://incompetech.com/",2017-08-23T13:34:57Z,ElasticSearch For Data Mining of Microbiome Databases,https://i.ytimg.com/vi/GDdd0XNUhPU/hqdefault.jpg,Bioinformatics DotCa,PT49M28S,false,519,3,0,0,0,[Music] I'm going to present some new technologies know that it has six search to facilitate that the mining of the human microbiome databases I'm Michael no Clara working currently in the armoire laboratory at the the some hospitality near sea level and before GC just getting you the the kursk on top before entering the crowd subject I'm going to talk about just big data in the biology world where biochip Liza and to stick with the subject of the similar school I'll show what resource we have to for the microbiome and those related to it and I think finally I'm going to present the the some solution to the mind by the my chrome data including elasticsearch everyone hear me correctly so big data in the biology world just big data itself in show years ago Eric Smith from Google told us that the from download civilization until 2003 human comes in routine five exabytes of data and now we produce that kind of data every two days it wasn't perfectly accurate because when you're when you're after that a guy from a company really check the did a study about that and exactly 23 exhibit information was recorded and we picketed it just in 2002 so now will be called and transferred much information every seven day which was almost accurate but not exact Eric Schmidt work in social networks Internet technologies we have a lot of things a lot of data coming from that but now in science we have the physics which contribute a lot in the Big Data information with experience with the Large Hadron Collider for examples in biology going to talk about that and the next would be the Internet of Things in terms of objects we can have a lot a lot a lot of massive data coming from that so that's gonna be the future and probably in the tentative seems gonna have some bhaiyyaji stuff ticking record of your simple blood sample whatever what is Big Data it's a defined at the four V's the first one in the volume where we are talking in exabytes now the second one is variety so it's coming from videos internet science whatever you want another another V is the veracity veracity in certain field data it's a major issue for Big Data technologies in biology you can't always trust what you have in your samples or what's coming from your sequencing technologies so this is this is a huge problem - you need to clean before getting too far in Iran and Isis and this last one is the deal as the velocity velocity so you have the same kind of data just coming again and again in different time time lapse so what's generating so much depend biology you already know it's distinct when seing technology is the cost per genome decreased a lot after 2008 and at twenty years ago it was it costed billions of dollars to just sequencing the first leaf reading bacterium and years of data analysis and now for three hundred dollars you can have your personal genomes map in few days with the 25-year company almost but at a certain point now the next generation sequencing grows out spacing the computer resource that's the issue today in the you have free free data here oops the the first one at the the artist storage so in megabytes per dollar it's doubling every fourteen months before NGS with work we have the base propeller are just doing every 19 months so it was ok to stir it to as cheaper and cheaper to store the data but now when the next generation sequencing the of of dollars you need to take on two base pairs his is less fewer so now we have space issues we need two terabytes petabytes of data to store all your sequencing information and maybe at certain point it will become may probably ship cheaper to resequenced instead of storing sequencing data what's the effect of the local sequencing of research before pre before what makes us next-generation sequencing you spend a lot of time to sequence and a lot of money to do it and this is only the part of analysis yeah this is the only storage and for the sample collection experiment experimental design of this mouse is very sensitive right and now right now it just be with beyond the two or two boxes here gosh now the sequencing is very fast cheap and you take a long time to do a sample collection because you can retrieve thousands of people thousands of collection of samples and then then out it don't swim in the races take a lot of time and money so now describe the challenge of the Big Data how efficient is tall everything what kind of volume I can structure we can link together how quickly we can explore and search with all this data do we split our samples can work and if I got algorithm we can use to the at what kind of request what kind of software we can use and artistic architecture to to have to to to store that and analyze everything before entering or discretion and purpose you are some suddenly I see search I'm going to just explore what we have in my crewmen data you already know that the metagenomics thought or maximum data is coming from DNA exploration again extraction and sequencing you have you reads your sequencing revelation okay and once you do you assembly and you searching in this database you want to answer to question who is there in your sample and what they can do the the package any classification very important you want to know what kind of population you have in your sample and what's the pathway is what the gene what the genetic involved in the know this okay so this is a diversity of the human microbiome just a few genetic tree showing all the values of population we know today it you have also their opponent the presence in various tissues such as the steel the plug the tongue skin everywhere everything else and there are moments in each in each tissue someone tried to attempt a map of the diversity of the microbial genome and identified the number of dishes for every gender and this is a clustering of our species specific genome and that's how the complexity of the the microbiome appear by my criminalizes and the diversity of vials collection of samples what do we have in a net microbiome resource the data the main database is known on the human microbiome project the objective of this database is to provide you to characterize all the macro microbial communities in many in multiple body sites human brain sites and it took the correlation within the change in the microbiome and the human health it just came in fact to alt if it can determine the determine if the set of micro biomes come on to each human if change in the microbiome can result in different states you know health of our Netherlands in the disease develop in the same time develop all new technologies for tuning the complex microbial system within the natural environment and maybe another idea is to begin to deal with the big illogical and social complication that may arise from the microbial human microbiome what's very useful in that database is the the reference genome so you have all reference genomes you want to map your samples this is a very good resource you have a lot of child gene sequence and 17 X s sequence these are all samples from bio study so you have all the clinical data from BIOS patient and all the genetic of the sequencing experiment from this patient so you can do exercise to extract all the information from those and map it to infants genome to extrapolate a population of which samples Java's obey pass such a functional database is metabolic reconstruction erroneously too so very very useful database the second one is the mg rest this is an automated platform to analyze meta-genome so it quantify all you microbial population based on the sequence data so quality control automatic annotation comparative analysis and it compatible with the lack of metagenomic data this is what this tool provide you so you just give you a you give the samples where they're coming from and it takes care of checking the quality control and to map against the database of full genome and it tries to quantify the population of each back tire maybe present that may be present in all of your samples this is just a sample test from the website you can try it's very easy you can you have values ways to to represent your data this is cheetah it's a remote the same event erased same thing you have a mic study management platform we can treat and keep track of multiple omics experiment data pradana lies your audit and generation by sensation its nests at once that interests that still I think it's it's older yourself the Caribbean project which aimed characterize the globe and met with microbial taxonomic and diversity function leaders here all over the planet in all human populations it provided in at last at symbol genomes again visualization proton and metabolic reconstruction this is a this is a map of ok dysentery this is an interactive or well map showing all the position of each health microgram sample on earth is connected to its most similar some other sample within the database so you can extrapolate we can of bacterial or microbial similar within population Newman's on the planet so you have your you are no you have all your sequence and your population in your samples once you've done your analysis and now you'd need to dig further and answer various questions such as what the dis population means health of the body is linked to a disease you are studying what are the impact on the health of the patient what kind of project also my back tire population can create and deliver to the body extra and in summary what's what's genetic of the microbiome tells you that's that that that's what that will you don't transfer so for that you need to go to all bioinformatics other related resource the first one is of course sperm med contains more than 26 million documents or you know that the debase all by we can make America torture elections on our books and you have the links for the full-text content through these websites so you have a very simple search there is be very basic efficient stops but it's it's not the perfect search online for for the ajaccio personally I prefer Google Scholar because you have the full-text content it's very useful and you have to put detail database this guys did a manual while manual takes my anyway almost manual curation of the literature so by text mining technologies they did a lot of rock to an attack or biology colon that in the magical entities and the relationship so for example if you look for gene it's already did the work of passing all full-text document to extract to associate the payment number with the gene you have you can filter by by concept chemical disease gene whatever you want I need to give all the people to to research now if you have bacteria so it means metabolites the human metabolome database is easier to study they're all the metabolites and more molecules and that the body and the gut bacteria bacteria will produce so you have all the information from each small molecule whatever I found found in the human body it also includes the bacterial products and so you can do the relation between both search engine so we can prove by metabolic disease pathway whatever you need depending on the genetics of your sub population you're analyzing and you can search by a chemical molecular weight terms or every text glorious isn't that's a very widely and it's very fast of course you have take pathways you have all the high level information function and in peacefull for biological systems for but I use it also some organisms or you have your bacteria you just put in it and it will tell you all the pathways known in the bacteria you can also find by ecosystem so depending on the gut or skin and you can search by many by many types depending pathway functional drugs which in molecules by chemical reaction they also have some songs and for drugs that can be related to gene you are studying Alvin by to pick pathogen plants and even bacteria if you want by any organism it's a it's not super user-friendly but it's efficient you just have you just have to to intro text and we provide you all the pathways so that's the suspect best way or you can do it by otology by various that's SMT D be a small molecule password at the base it's carrying of created with care but as more it's really enticing on the small molecule pathway from the Newman I designed to support the password sedation and discover a metabolic transcriptomic so you have aa drug actions disease and it's a little bit more knew that now than okay again and you have all those passwords again but you can search by drug action very cool if you need to to target the bacterial population and talking about drug you have drug Bank so supposing you are looking for a way to recover from this regulation for example of you microbiome flora almost as easy in the get you will find here or drug that could eventually targets specific bacterial products or even pathways and Dam so you want to point to to target a metabolism so drunk bank as a number but in 2000 interaction between drugs and down targets and you can search by drug all the pathways or genes and provide you all the drugs and effect and secondary effects for each of them another crew DV is the food DB so you have the database all food consistent chemistry biology so depending of the the if you want to study the impact for example on the gut microbiome ah so depending on the room diet of the patient on the food DB for each component you have all these this is just a sample list of the component can found that physiological effect prism it's presumed to be the representative health effects or for should coral taste whatever you want and you can probably associate what the diet of patient with the the the microbe young man to get macaroon with that or very simple bruising as a window or the other non-food for you man then you have a sub toxin toxin DB this crate to estimate the impact of bacteria toxic byproducts on the health of the patient so it combines all the tiny information on toxin data and their targets you have all the bacteria from food and fungi and fungal toxin which are which contain a big database and you have almost 90 fields including of the chemical properties toxicity values molecular interaction medical information this deal may be very useful to estimate the toxicity of this regulation of a population a bacterial population you still only get at this very simple browsing again you can filter by by fungal toxins or bacterial toxins and you have 12 different things even though your knowledge in chemistry let me use words then for the more pure chemical the today's you have Kimball's you have key B this is dictionary of liquor entities this is a at the atomic level so maybe not so biologists moto to chemists to Chemical Guys and of course uniprot or the protein sequence and functionality depending of what or your microbiome samples our secret inning so now that you have all these great resource how to mine them how to D to put in a relation that's that's the challenge for by register many you have so many data days and what I show you is just a sample I didn't do all the literature but probably it's just 5% or what's existing on in the biology so you have media today's mini web surveys some of them can explore explore your data which is nice but you need to jump to website to another to analyze the data I did not know any programming steam you need to use external tools to see also to create the relation for biologists you can do it manually for each database using online tools and again you angle the browser with 20 tabs after DNA analysis and if you know by any factor if a magician then you go to programming you don't know the full set of each database you connect them you manipulate the data with a package conductor do connector connect contains a lot of package and probably you if you have a database in mind you probably have a tool to extract information from that and usually if you are pure programmers or Java or Python Perl you have a lot of library to explode a biological data how you can go to elasticsearch I think that's the future that's elasticsearch is in the middle of the big data tools that were created in the last years this is not the only solution now although but what's nice with that is it's an open source search other so it can index any kind of material genomes data she's perfect exactly what you are with what we have announced it's a new SQL database you don't need to create any tables any schema it's in jeezum so it's a fraud new from from google already compatible with javascript it's a near real-time search sunshine so it's very useful to analyze logs from a computer also data which come every set on several minutes so maybe not exactly our case but maybe in the future one can have a little samples on your blog it's very fast it's you have high resiliency and massively switching what means is you distribute the database or many clusters and they are all integrated in the same times which make so this is only the database in itself and you have already you have tools or provided with a sick search to do the query on the database this is the visualization in Chavez it's very powerful you have the dashboard you create dashboards and crystallisation depending on the data you have on your end and it's capable to provide any kind of graphs 90 exam which fields and do we use in the lab kili kili is the fork of cuba nights it's an English company we tries to implement the relational the relation between the data to indexes so when I say index think to term medicine index should DBAs and indexed because the prime now is that elasticsearch does not support the relation between the database so that's why he be zero it's victory create decorations it's basic concept I won't go into details you have the resource of public database you stir it in elastic search starts with some data processing because depending on the source of the data you have in your hand and you push that in your clusters and you have multiple and excel with takes care of ingesting your data put that in the storage in the indexing index index storage and then you have the elastic search search shells which are Cuban alchemy this kind of this is the kind of visualization you have from dashboards of keeping your Havana you have heat maps map walls any kind of of charts so it's everything is in JavaScript we have some curves instagram's heat maps relation graphs this is the ratio graphs here you have some matrix some just numbers you can pop up like that world clouds really useful too and just very simple tables who have your information and it's it's already adapted to get real-time will time a real-time data so it updates itself so it's very cool for any monitoring systems but it can be also so that's why we're creating currently keep you that science it's a web platform just dedicated for I science and to summarize all by well all a lot of bioinformatics data we won't be able to all do all but the most known and we are currently treating enjoying for frequent updates to match the latest records and it relies on the last excels on TV I'm treating that with the register in students under supervision so the concept is simple the goal gather of the main bioinformatics database within computer science and treating all duration running them together she's just few examples but currently what we are we have created a program that digests every kind of PD database depending there in XML CSV from an FTP site or any API but most a lot of bioinformatics repository proposed now the jeezum directly so we just pack touches on we send it to elasticsearch we don't have anything to do in that case so it creates you also you know elasticsearch have something very cool it's automatic it has an automatic mapping you know when you are Israeli when you have a lock data days on MySQL database you need to create a shimmer of what the what kind of element would go in your database that says ls6 observes everything along so it takes the first date I Tex it generates the kind of data it would desert up that so if you have a stream into girls or whatever all right everything is done automatically the pram is when you have databases like PubMed you have very real information on it so that's why we needed to create add engineering to to out elasticsearch to do mapping so some databases are very used easy to push in there as a search other animal context flutes and we have CB to do the visitation search and all the dashboard we have created with our visualization in future what we want to do if we have the computational resource is for me to accept user data so imagine you are we already have accumulated silence with all time and and could D be taxing database and you arrive with your own data in a CSV for example from Excel and you push that and we do the link with all agree every other database that's what we want to do in the future and also I show you the very basic visualization already implement inelastic search but we want to create more visualizations such as expression it Maps for gene expression even gene networks may be genome broader or more interactive but a lot of people already working on that so probably won't do anything on it so they like survival curves we have a lot of collaboration with all search cell shows in in clinical in the clinics so survival so many vehicles biostatistics they are very interesting that so we need to implement tools that automatically do some calculation and probably some machine learning and for that we won't do we won't recreate the wheel there are lot of already implemented JavaScript libraries to do that all these kind of visualization already are already implemented in JavaScript so our goal will be to adapt it to two key B to Cabana and elasticsearch all this is already known in an indirect chatroom so I'm going to show you just a short hop video which shall not been have not been implemented in computer science directly that it's an Kimball we are working we have a collaboration with key big guys the companies here in solution and we have implementing with them Kimball so let me show you so you have all your own decks Tweety's and once you go to the dashboard you have few tabs depending on and you have already created visualization and it's automatically updating once you go to tap tap because the database is spitting values subsampled and every time you do a search base everything on your dashboard so your automatic filters you can also filter by date so you have the syntax already implemented for is to use to create filters and the blue buttons are two relations so these plugins allows to filter out all their own sub tabs in other bad box and it's every dashboards are linked together at a certain point so when you do a rigorous research can click on everything on the that wall that it could be other filters to figure out to define the reception so you have the water I'll show you on the real on the real example so this is the permit publication I want to retrieve here all the so I have this server is on compute Canada West cloud this is something in test tube is placed okay so it can be because it's it's kind of expensive to to have a a grid seller to digest all this data but still talks so we are going to use it this is a 5 node server so we have five five servers and this is so we have more than twenty six millions documents it does not contain the full text on the abstracts let me show you the content of pell-mell here you have all the fields that primeira med providers for each of one you have the ideas you have the Artic and this is all the information that we have in Perm med all the autos machines permit date it's not very few so friendly to see that like that so that's why we can create visualization and explode it in dashboards so you have here that's the number of publication by date so from 1944 to today in 2017 we haven't updated sign since january so that's why we paper from that and everything is clickable yes i click on the date i have an automatic filter of the filter out everything that it made within that year and the main topics on in that year in 2002 in 1999 t all the main topics that have been published to this year this is the jonah which have been most which view have the most publication that here number of reference journals so and you can go to any world that can you you are looking for and everything is updated automatically and this is all the details so the content of of comment I'm going to fast this is not super nice because it's in construction that one today one day gonna meet a nice earth and you have the every details for each paper all you have new thoughts you can of course do any kind of search so for example if I mean I didn't entertain on my looking for whatever so it's looking within and read it I think yes Pam med it's one hundred and fifty gigabytes of data okay and research buildings in few seconds two seconds exactly on this server so it's very fast and for example in zucchini I seem like tweaking so far I can check for disease on in the microbiome to I don't have I checked one yesterday I don't remember the wall and each time you create a new search you can go to the relation you have done with the TB relation what it does within each and permit document I have the list of genes for example the list of mesh mesh readings or medical headings and if this if an IG in permit exists in another database this solution will tell me so if I go to put a tour for example I click on it it goes directly to the particular dashboard and it tells me how many on trees I have or how many intrusion I have from the cells I'd just done in permit I also put all the toxin that Adam took seen toxin free DB Sophie I'm going to toxin that Eliza I clicked on two things on the same time it didn't like action metabolites I assumed they'd be metabolized I think it's it's 180 gigabytes of data but I shouldn't be lower story than that okay but - I mean good - mmm you have all the disease it you see just in one view that's something we don't have and when you go to database how many countries we have gene IDs gene symbol we can create any kind of matrix we want from the database this is a mapping on all on the genes in existing in the chromosome location and situation from each every genes including in the database you have all you can you can click on everything home in tight school means that's the entry type so you also need to know what's in the data lies I pushed a lot of data days in a success without exactly knowing what's happening in needs in each you can search from you can also click we have genetic means so if you don't want to go directly on the websites websites I won't do it because in some a camera which I'm going to be able to come back you can search for specific countries for example if I want to get for that approach gene symbol I filter out and boom I have everything on that gene specifically it doesn't have routine that's weird okay I would have hoped I'm gonna have to put in so I don't like do something like a cheese scheme and we are putting any kind of gene like bin in the Oh meme rest until maybe that one lucky cause right good we are put in here so I'm going to ask MDB and I take all the that's nothing I have the protein itself yeah it was very fast and the wall cloud provides you we the meteor the metabolites that interact that that protein I can't try to come back on good TV DB walks this is a dead ball I just made yesterday this is all the targets from the get I want you to go to the toxins before okay with category hockey boots so this is all the toxins in to freely poster bones we have all the data base category Santa t-bone pesticide drugs by cellular component by stage you have some matrix we can do we can just create like a very easily refer to two arrays and I know if I looking for something like vanilla read and I doing a filter on it then I have like a Wikipedia page like all the description the search treatment for that specific vaccines mechanism of toxicity risk level metabolism symptoms when I did jested all the content of the toxin that database everything was already provided with the rest so it was very easy to to put and I have the 42 targets for that specific toxin so if I click on it it's filter out automatically on the next on the next dashboard and it provides me every all the targets from bad past specific toxins so all the by entities and so I don't know exactly what saying all the genes are so targeted by that protein and you can export the table one in CSV so if you want to it do your own declaration on your computer so this is this is a computer science right now it's going to be up to date probably in the next oh yeah okay so now in conclusion I could say that your Divine informatics and metagenomics are only for big data tools and elasticsearch maybe one of the solution because you see this is the main that I've known database or at MySQL Microsoft SQL Server on is this elasticsearch it's currently new and it's going up and up and up and I don't know what at what point where it will be but with we hope that technology would continue to to be dated and cream to for a long time currently computer science that is down but we hope to have a public version this fall and if you it you can do it do it you can do it yourself if you have if you have at home you on a simple computer and you want to digest you all your expression data you can install it as six urge upon the data even in CSV because tools exist we will call your CSV tables and it will be converted in jizan formats and you feel as if you have some already databases or song ji-sun you can push them in elastic search and it will be easy for you to explore the data so this is our thank you to listen to me today and if you have any question for all these documents yeah we currently we have 40 millions documents from these values database we are just five notes I don't know power of these notes but it's um come on separately Jesse no it's some you know you just create the virtual machine on the fly and just created five and I we don't know exactly where we are what they are located but directly you need to have each VM separately from others to not have the same e io on the same hard drive it should be the set the decays but we are unsure about that so it should be it should be faster when we gonna show that to be separated for itself yes yes yes in fact when I want to discover something I don't go to to come in for example you have all those all those fields here but you have created before some some search oops the net I don't want it to that one so Oh meme and you extract of columns like this and you want to extract them for example I don't have a plug-in yet to do that but it sounds way from key B so you're gonna have a large table like that and one two back and you gonna have to excite that but sales day it's works by curls so expose things get and it just sent jeezum as query and you retrieve all the document from that in jism format and to to the congressional she has been very easy so you don't this is just a query tool to have a visualization but in the back end with just jeezum query you have all your documents in your and without without using that so you just have to provide your column names and you don't know everything you got no more questions it was just and to the create visualizations this is just a list of visualization you have and if I for example if I do tag tag cloud from the already need tools like okay now when you create the tags you have the aggregation of terms and you creative you have all your fields here so if I go to the chemical list search for text and I want to 25 this is how I create visualization exactly just we just want to try to to to create to pre create all these things because for example it's crazy I mean if you use if you just they get the XML and try to send it to ecstatic search it really crashed a very fast because for example you have volumes it's numbers at the first time so it check one two three just this is an integral and suddenly you have a special issue you have a letter in it it's a string it won't like that so that's why we needed to create an engine to Auto to do the mapping ourselves and we will provide a certain point a big jeezum of altman med you will denote it and we push it on your se search on your lab and it's done that's an example but the work of transferring all the non that to be database within the ASIC search in the JSON format we will do that [Music] 
r50MblPrB70,27,Introduction,2014-01-13T22:41:08Z,Data Mining Lecture 1 Part 2,https://i.ytimg.com/vi/r50MblPrB70/hqdefault.jpg,Utah Data,PT32M59S,false,779,0,0,0,0,but of noise in your data is that you want people want to have privacy of it you are a customer in the database if that database gets leagues or if they somehow out online you may not want to be identified as being that particular customer for instance some there was so who's heard of the netflix challenge so there's this this contest but my netflix any six or seven years ago where they they put out a bunch of information about their customers about what movies they had rented how they rated them and said we want a better prediction out of them how to predict which movies to suggest to the customers and they put it all this data and the research we need dope into it and really improve this and Jenner and created a great new album infer function entre Netflix which worked a lot better than before apparent create a lot of cool research and they were going to come out say okay this was great we're going to give you even more data and hopefully you can come up with an even better you know you've been better out there's like a million-dollar prize the best group which into grouping a bunch of group together so so they're about to do this and then some researchers came and said well I can look at the data up on IMDb which has users who are public you know made them self public so all the data sniffed list was supposedly anonymized and they can find people who made it will be similar and fairly accurately if I a lot of the users he in in the Netflix tape so you know the users and I knew being identified themselves so this should have been a problem but the thing was people didn't always publicly rate all the movies they watch some people watch some movies which maybe they'd be embarrassed to say they like maybe daylight as maybe it was really man the guy like action movies but he also liked like these like these romantic comedies occasionally and he didn't want the world to know that well in that lake so they had all that data he thought it was private so now swimming can identify him on IMDb based on all the action we seized watch and then say oh you also like these these romantic comedies haha sofa but come so this is a model kiss but right there could be even worse cases with with this data leaking and so one way of dealing with this is to add noise into the data in certain ways you cannot recover identities of people and so will maybe talk on it's hard to cover this entire area that I'll try and give some intuition of how this works and why this right and again this is the variable concept in privacy and then David please that really in the last five or ten years has been and so well then along the way in several here is we'll talk about the ethics of dealing with this data and with my use these Cascadia in different ways what are the issues with doing things with some of the model teachers of them but this is a very good example of where the ethics comment so then the next structural topic will be on link analysis and this is basically is these large graphs we think of an edge as as a link between two of the notes and the notes don't have to be customers needs to be web pages and and when you think of the web as a large graph where if there's a link from one web page to another web page that that describes an edge if you think of the Internet is a large brass persuade then this is kind of a really important insight that led to the PageRank coverage so so also who's heard of the page rank of them is this still common knowledge who is not very featuring so this so this is kind of the main algorithm that cause because Google's be a successful searching so we'll talk about what search engines were like before Google invented you know before brand page invented page rank and how it changed things afterwards and this is built on this idea of modeling there as a graph and then using modeling Markov chain on top of that draft and then kind of adding on some extra twist and this really revolutionized the way we understand the importance of different web pages so and then how they do this at scale is a is a long as a general technique called MapReduce which as was kind of invented for this and a couple other internal things where Google and now has enormous applications in lots and lots of data processing most fortune 500 companies are using some form of this adaptive system so and this was initially built to help scale up page rank so we'll talk a little bit about this mattress what works this is not so much of an eight analysis technique but it is really important for the scaling so that's what so then also think perhaps another important question is is the finding the community structure which are close to each other these are important for when you're saying advertising you may want to target a particular group you want to know who is it who is in that group try to identify these groups or who is an influential person in a graph which edges are most important for conveying information from one paragraph so we'll look at will again build on these same sort of concepts to understand the community and structure because large and again this even these questions were not even able to be asked 15 years ago because there's no way to have largely there was no any clutch so these are all very new developers all right so then we may talk a little bit about summaries this will kind of make it makes it beautiful are you mad because it's very related to the first couple lectures and then it will fit in through various parts of the of the course and these are ways of kind of extracting taking a subset of the data and then what can you say about the full data from that subs and it's if it's a perfectly random subset you can say a lot about the data sometimes it's hard to get a perfectly ramps upset but there are techniques to do that and there are other techniques which work better than pretty random subset and so one of one of the places where these comes up is is what's called is called streaming data which is where day is your Excuse data so large it doesn't fit on your computer and it just keeps coming up so you think you're like a router on a network you want to calculate something about all the data passing through without storing all the day you just get to look at it once and then it disappeared people well we'll talk a little bit about this I taught a advanced seminar last semester where we talked a bit more about this so i probably had maybe a third of the classes and there so i probably won't go in a step in this okay so so so kind of kind of look back at these these themes again the goal to set before is the intuition for on for data analytics so we want to look at our data and try and map it to one of these kind of types of problems that will discuss and be able to get intuitional what the modeling choices we make to get it into the form of a question you know how to solve and how this affects both in in the accuracy of the question we're trying to ask and also in how officially we can actually do this how will the scale as our data grows how easy is those circles so and that the key step here is the modeling from from some arbitrary probably messy may be noisy data to some abstract data type where we can do something if you have the raw data out of how many people have experience working with kind of raw and messy data you you get if you are safe try to collect it surveyor and companies and and the this one new dimensions of the so-called big data is the is it's the variation of the day it's very weird and messy lots of different things you don't know do what deal with need to make some decisions on what brother data and some some form you can use and so that's another really keeping trying kind of focus on throughout the class that how to make these violent dresses and then the other half of this is how to process it efficiently given those mountains right so we're balancing he's the second is second or third party the choice of the model and how efficiently can we deal with the model once we get so the general plan is i listed a bunch of topics will spend roughly two or three weeks on each of them and the inputs with inside each area the goal meeting of a general overview it's on the very classic techniques that anyone who's taken of course the day of mine I should know well then we'll be focusing on the model efficiency trade-off and we'll all you know i'm i'm very excited by a lesson in do developers this area slop i'll be telling you about some things I think are really exciting and are really making kind of on the cutting edge as well I'll try new this within each topic so then this is really kind of a broad group of students in this class we have undergrads and graduate students we have you know only about a to thursday classes actually in pure science week keyboard and many other so I'm hoping that you'll everyone will find some interesting you know topics in the class so some of these may be the more advanced topics which some of the pure science students being able to grasp and other times maybe only people with more of a map back or we'll be able to grasp all of the details of those things but hopefully you'll you'll get a big picture of what are the kind of the basic ideas and what is the cutting edge of the same time so with each of these these topics will have a short homework assignment so it will usually have some analysis gifted use some of that will be doing some programming on some actual data some of it will be more kind of you'll you'll be given a question you'll have to do some analysis without you know without actually so just taking it and grungy but the data is not all data money you also need to know how to take the data you know I understand understand what it is so we'll try and have both of these things so and so this will account for captivated the other half the grade is going to be a large course project and i'll talk about minimum slides this will be so a lot of affords I'll try and make the data a little nicer and easier to deal with we'll do a little bit of pre-processing for it so it's a little bit cleaner state so you can really see how the algorithms work and how we modify them what effect this has so this is very different than how you use data mining practice in order to do that you're going to get data which is which is a horrible condition and so all of you are going to have to go through that for the class and you're going to choose when you'll get to choose your own data set and these come in a variety of formats but probably not the format that you want and you'll have to take that data set and do something interesting I want you to get practice working with real data and and kind of taking one of the techniques in class and doing a much deeper exploration and so that'll be the goal of the project um so the project dose of you'll see that a little bit of you scheming course webpage already it's it's fairly structure there in v6 intermediate deadlines throughout the semester so if you're used to these class projects where you try to do them all in the you know in the last week or last night that is not gonna be possible because you're going to have a bunch of intermediate Devils this is a bit challenging to balance with your ongoing learning would be the stuff throughout the class so if you have you're exploring a topic that we cover towards into the class well we'll get around us because it turns out actually using that the techniques there are going to be only part of dealing with the cop a lot of it is getting the data and you know getting the data to the right to the right form and so the intermediate of that lines will help to that on the mend and you'll actually will do some former presentation so the classes basically doubled since last my quiet so last time we have this big poster session which was last two times it was very exciting all for trying to assess and deciding the next few weeks whether I think that will still be be feasible or if there's a something else we can do but i think that scales pretty well on that when you get to see all sorts of other cool stuff and other data challenges other people are getting into ball also doing so um let me say a bit more homeworks will manage the cracks through candidates I don't have it it funky astral yup and so there's not going to be a restriction to a specific program range so some parts are going to especially when you're dealing with some of the matrix e parts oh oh well design them so they're easy to do is specifically Matt have but if you are more familiar with another programming language that can deal with that stuff you know that's also fun other parts are going to be harder to do with something like MATLAB or something like our you're going to be much better off using something like Python or C++ or Java where you can build some very simple data structures or use some built-in data structures I don't you I think you will not be able to use a day my package the way we design things but you'll have to actually see how the data is moving and so if you're low good with programming as as a QQ r you may need to work work a lot harder in this part of the class but the deaths if you want to really understand how things work I think this is really so and so and maybe if you're our computer science student you may be used to programming assignments where you're given a very specific assignment or said here's the exactly we need to build here's a part of the code you need to fill out this code you need to build some unity specifications these these are assignments can be given some some data often but they'll be much more abstract they're intentionally that kind of giving you things initial to work with because when you're in the in the real world that's not how it is right so you they're going to be intentionally not to specifically and this causes some so many difficulties hopefully you can talk you know a bit about yourself as the T days me for some some specifications when you're confused about something but in the past most people have been able fine some people get qualifies but this is a warning and i'll train give some some bonus questions on the homework there are lots of different skill sets while trying to have some challenging questions which aren't required to be answered but you can get some points for some cosplay may be more fun if some other things are not as though those won't cure ok so just mention a couple things if you're if you're interested in kind of the more research side of what of what some of the fat the end and any grad students are doing on theirs we have a group meeting thursdays at 12 15 to 1 30 you're you're welcome to come if your graduate student you're looking for an extra for extra website of class this may be looking to do well some of them will be visiting people coming moving clocks some of it will be students practicing research presentations but if you do for one credit you need to give 11 presentation enough people sign up then it's your presentation will be half of the time for a third of the time you're just weekend too so there's there's a web page though it's going to work there's actually a better step off that way and this thursday we have visitor from indiana university over the comm and so you're all through welcome to come so these topics you know it's it's too bad this had come at the end of the end of the semester this really builds on things that will talk about class and as a recent time extension so what this is really saying is a bus way of doing regression in in high dimension in a subspace of anyway which means that he can scale also really good enough and so this is a talk about a recent paper alright so let me what's wanted to go over the class webpage alright so okay so please this one thing I've show said before please stop and ask questions i know this is now a fairly large room I'm very happy to take bunch of the benefit rather I I'd almost rather just answer questions and actually actually cover the material I think you can get much more knowledge by you know with ways of interacting I'll try and try and find ways of building that into a larger class if there's not enough question answer also be giving a lot of the lectures actually on the whiteboard I'll be trying I think it's better to actually develop to see the material develop on the board and so well so we'll be doing that some of the things are are going to be easier to show a lot of information on slides and we'll use slides right we needed a lot of things will develop and it will try to interact bit more the way we'll see skillet okay so if you haven't found a web page it's um it's it's here if you search for my name in google I'm usually pretty high up how well you know it's hard to know right so if i should i wish they have a profile for me they know i'm usually looking for myself sometimes i'm too lazy to type the title web page or something right size for i'm just i mean most people have searched for themselves at Google right so if your camera over this URL you can find out my web page so on let's see so the very much so so cerveza is is that is the tea is 12 TAS and they'll be none of my one out top somewhere soon we'll figure out we'll try and have them so they're aligned well when the homework assignments really project designers usually do so the TAS will be much more responsible for a lot of questions the full marks and I'll try and manage questions deal with the project so most of the questions on the homework can be better addressed towards with TAS and all the better address the questions regarding so and we'll try and schedule the office hours at okay so there are they're not really any me not really specifically in books in the class although I do linked to two classes it's two books which are freely available online this one I think you can buy our copy if you want to and and I'm fine i'll try to link to the relevant chapters in that part of the brother class this first one this is the page for it it's it's a bit more on on it's a bit on the database side of things then on the data mining data analysis side of things so it's not always the best resource for the way we'll be looking at the perspective of the class but is otherwise a very nice easy book to read sometimes it doesn't have the formalisms of the data as much detail as i will try and go through it you know just their perspective aside the second book is not really a book it's a call collection of lecture notes in a really large PDF and is meant to be kind of a lot of the formalisms and mathematics it has to do with this sort of this new sort of data I've heard some people say that you know in computer science the important mathematics behind it for the first 40 years and this was homina theories and this is not this hasn't gone away entirely is still very influential but this helped develop things like in Computer Sciences that kind of thing one of the things was like the P versus NP question itself developed this and the theory rounded but in now a lot of the computer science is influenced by data and this book covers kind of a new sort of mathematics that deals with the sort of data palms that we'll be talking about this bus okay so these are kind of more mobile books but I I actually have my own lecture notes which i maintain tonight I wrote them up mainly last year and I'll try and keep refining them fixed typos and stuff throughout this semester and may be updated with it and these can be found link under this under this column of the webpage so there's a schedule here and one for coming this will have so this one just has the slides into class but the other ones will have links to let's see to these notes that put together which are written up at PDF supply and so there are pretty pretty detailed notes going through some of the formalisms and try to discuss something tuition so I don't think you really need the book with this class a lot information being here but these are not been as carefully chapter typos and a stay one two textbooks language so if you find typos but no I'll try and keep update and refinement throughout the semester some of them have been up I'll kind of link to new one who's we go some of them version from last year so this link will either link to one of the two books this mmv escort was the first one and CST I a1 is the second book and I'll try and say which chapter is relevant this party class when there's when there's a good sometimes either the books covers the topic very good at in that case all trying link to you know some other experiments the the assignments will all have their two days in this column here and all the due dates will be fixed when the assignments are posted I have rough positions of these so far and this similar of the project I'm going to inquire highly encourage you do your horse in la tech and if you link right here this goes to a directory with some example file selected gabions lockpick before there's a little bit of a learning curve but a lot of other people the class know how to do it and they're fantastic resources up actually we see just a lot where people ask whether there's a stack overflow thing just relaxing it's a very powerful tool for producing nice answers and so especially if you're doing mathematics with retina and so if you can't if the TA is unable to read your over to summit because you didn't use last text formatted correctly then they have my permission at work day so if you're able to format it or write it up clearly in another way that's going to be fine but I'm going to encourage you to selecting I think it's important skill to use and writing up formally and you know things that you have to deal with BS a lot of the publishing companies actually used something especially if there's any sort of mathematics so let's see what else and then the last thing is the project is going to have these various deadlines here so if you go up here there's actually so there's a big PDF which is going to scrap the project i'll probably update certain sections that we get closer sort of deadlines but you can see there are various steps here it's going to be within groups of two or three I'll allow groups of one under special circumstances if you have a very good good reason to that so if you work at a slightly larger I think proofs of larger than three it's too hard to figure out who actually did want to work together and these natural things like your work will go down if you were in a Sunday larger be able to accomplish a bit more there are people with various different skill sets in this cuts and you know this could be an opportunity to work with someone who has a different skill set they even combined together or maybe you have similar background you want to go really in depth and salt in your interest both of those are but you should take a look at this giddy up and start looking at the deadline start thinking about project conducts start thinking about your your partner's will be the project proposal is due the first week of federer yes so you're going to have a few weeks to get together and think about it but I want a proposal project by them it can it can change maybe I'll say this is this is is not a good idea and you can change it but I need you to have some ideas and to think we're going to say this is what we're planning to do but and then the 24th you'll have to kind of a you'll have to have collected your data and have a sharp report telling you what you've collected so this is only two weeks afterwards you already have two striking your data this will take much longer than you expect it okay to have it does like before this will take a lot longer than you expect so I want so that person why they deadline so early because not everyone will actually get this done I'm gonna so I don't want you to run the problems later on because you haven't gotten the data you know if you already are working on a topic we already have some set of data now okay if you if you use this data and this project i want you to carve out something specifically for the class and a lot of this is disgusting so I suggest you going and read through this and get a sense and then we'll be an intermediate report and a final report and then some sort of presentation at the last date so so so the horrible activity would be do we may decide to do something I hand in through Candice I decided yet but no work will typically do you do at the start starter class I want to do this so you go if I had to do an into class and people either are working on it during class or don't know their classes are working if I have to do at the start of class and turned it here then you actually come here so I i I'd like you all to come to class I think it's good to have people here in interactions I think you learn more if you're experiencing and so one of the things is that I set up a discussion group on Google some of you have already applied to join us so that this was very useful last year people i have questions on for order before I can make an announcements so I guess I don't know hey some people may not got the announcement I sent out on Sunday that class there wasn't really class on Monday that's the only way I really have to communicate with everyone in the class the email addresses aren't all be set up to what you'll check I will in the future I will only post things on this on the screw you can see it right out of web page even if you're not a member but you need to be a member to close that's why don't we don't get your stance those things here you can set it up just whatever sort of notification you want you want to email every time update columns or if you want to email a preview topic you can set up so i think this okay so you know I'm still a bit flexible on the schedule if there are certain topics that you don't see listen I knew if you said I took day mine because I really want to learn about this topic you know just just let me know there is a topic I added that like last year because some people wanted to want to know about it I'm having to add in some stuff or maybe I'll say Oh that'll really be covered with their this topic so you can look forward to so so please take a look at the web page so great so so this is what's this way ahead plan for the class if you have any questions about the class or healthcare running let me know otherwise we'll start with some enough material 
vzPgHF7gcUQ,27,"Google TechTalks
May 12, 2006

Eamonn Keogh

ABSTRACT
The problem of indexing large collections of time series and images has received much attention in the last decade, however we argue that there is potentially great untapped utility in data mining such collections. Consider the following two concrete examples of problems in data mining.

Motif Discovery (duplication detection): Given a large repository of time series or images, find approximately repeated patterns/images.

Discord Discovery: Given a large repository of time series or images, find the most unusual time series/image.

As we will show, both these problems have applications in fields as diverse as anthropology, crime prevention, zoology and entertainment. Both problems are trivial to solve given time quadratic in the number of objects, but only a linear time solution is tractable for realistic problems. In this talk we will show how a symbolic representation of the data call SAX (Symbolic Aggregate ApproXimation) allows fast, scalable solutions to these problems.

Google engEDU",2012-08-22T19:25:01Z,SAXually Explicit Images: Data Mining Large Shape Databases,https://i.ytimg.com/vi/vzPgHF7gcUQ/hqdefault.jpg,GoogleTalksArchive,PT51M52S,false,5204,53,2,0,2,"today in and Joe from UC Riverside is here to talk about data mining time stories and images and as usual this talk is going to be archived on Google videos so please don't say anything confidential okay I didn't hang close me and welcome to talk turn it a quick outline the first half talks to become what it's review I'm going to talk about a quick review of time series data mining and a way of looking at the generic out of my noggin I'm going to show the explicit presentation of time sharing data called sex again this is all kind of review and then I'm going to talk about in the next half a talk some new work we're doing in mine and shape databases and in particular to problems shape this core discovery that's gonna find them unusual shapes and shape motif discovery which is finding new duplicate shapes and if I have time I have a little kind of a DVD bonus feature which I'll find get to also let's get right to it what's about what is the time series okay so what is the time series at times here is simply a collection of data like this is sequentially in time and so here's an example here and of course the beauty of this is actually we can plot this and we can now see it's actually a heartbeat and maybe even though what kind of a heartbeat it actually is what kind of a name is so for the rest of this talk I'm gonna actually tend to show pictures more than equations and numbers hopefully you'll go along with that so I want to spend 10 minutes making the following point that time series are ubiquitous they're really absolutely everywhere and the reason why this is the case because people tend to measure stuff we measure how many web clicks we get we measure our popularity in Google we measure whatever it is and things change over time so of example George Bush popularity changed over time down down down down down and your Google stock stock changes over time hopefully up up up up and so the world is full of these time series in medicine science arts but the other point I will make over the next five slides is times are so ubiquitous for another reason because many kinds of data which are not time series can be kind of massaged it's becoming a fake time series let me show you five examples so here's example one I took the Bible as a long sentence and I took a window of length 100 words and I stood it across the Bible and I'm going to measure the local frequency of certain words so for the first example is God so for example in numbers God is hardly mentioned and in Deuteronomy it's all God all the time there is like a pseudo time series and I can use this a various tasks so here's one task if I want to translate the Bible into Spanish or say we're actually want to prove the Spanish text I can look for a time series similar to the blue one and I find in Spanish I get the word dias which of course is translation for God now this actually kind of trivial example there's only better way to translate text but this could be actually a bursty time series a weblog clicks and I could find certain words associated with other kinds of queries oh sorry oh yeah that means the men have fun sake when I did this originally I noticed actually matches overall quite good but in one local area is actually quite a difference here and so the question is why is that difference there so I subtracted the red from the blue and I queried on the difference of those two time series and on the closest match is actually this thing right here which is else senior the fodder is sitting them for God and so for some reason el senior is a synonym for God but it's heaven locally bursty it doesn't happen in the Old Testament very much but happens a lot in the New Testament particularly in Ezekiel so actually nice extra bit of value we can get it is sometimes it's an example to come back to later block can I briefly show you now we can also take shapes and convert shapes in the time series and again one reason for doing this is because we have a big arsenal of tools that handle time series very well they don't work well in video i shape directly so we can actually handle things more efficiently in this space yet another example this is actually every professors worst nightmare a grad student with a gun so in this case actually on the top video the grad student is pointing over there and here's three examples are over there here I'm tracking her hand and in the bottom example she's pointing a gun at somebody and here are three examples of the gun and in this example if you look at the time series it's easy to actually figure out which is which because there's a little bump here you can see which corresponds to the hand pause and over the holster and I'm drawing the gun out so we can easily classify this in the time series space but actually hard to do in the original video space this is the course of a contrived example but for example in Las Vegas there are similar moves you can make with your hand on a blackjack table I think what you want to know about automatically here's another example of a data set which is not time serious but we can kind of fake it to become the time series so here's a handwritten archive let's really mention I know Google interested in doing this because I was in Dublin gonna talk in November and however to the talk the Dean of the college kicked the door in and he said Google has given us the check and the whole room at wild so yeah obviously given these guys too much money and in it one actually index this we can transform a little bit move the slant and then we can make a pseudo time series one above the world and one below the word and I simply index the time series which we can do very very well it's a very simple naive thing but actually works much better than Markov model these are tricks in this domain yet another example I'll go very quickly over this but here we actually have a four dimensional brain volume and of course it's not a time series but we can actually fill the volume with a Hilbert curve measure the local intensity at every part of the brain and then take the helper curve and stretch it out as I've done here into a kind of a pseudo time series and once again the idea is simply that we can't handle efficiently this high volume data but we can handle one dimensional time series very efficiently so the conclusion from all this is that there really is time series all over the world and every kind of domain and then let's do something interesting with it so what should we do this time series that what we actually do everything with we want to do cluster him maybe cluster similar customers similar web clicks etcetera we want to do classification is my heartbeat normal or abnormal we want to do motif discovery perhaps so this basically are the any repeated patterns in this time series we're going to do rule discovery so will this copyrights say if ever you see a double peak like this then within ten seconds we're gonna see this shape here with some support and confidence we may wish to do query by content basically women at Google time series many time series data sets are very very large so we're actually want to visualize massive data sets and finally not the detection so here's a heartbeat and something strange and novel happened here the person either died or a machine gun unplugged so these are a diverse set of problems but they all have one thing in common to solve these problems we need to actually figure out how to measure similarity if we can do that in a meaningful way all these problems are claimed or trivial but we can't do this in a meaningful useful way then all these problems since you're impossible let's talk about similarity very quickly so the classic Sumati measure is you could in distance and here's the equation but the pictures very simple to see we have two x series seeing Q we put them top each other by Zenon ization and now we can simply measure the distance between the i2 point I 2 I take that distance square it sum it up square root of it that's Euclidean distance Euclidean distance is very simple but it's amazingly effective you have to burn off a lot of work to actually beat this this is incredibly simple and easy to do again we're doing the title review here this point look at the new stuff in a moment so one thing about data mining is that most of the time in data mining you're actually gone backwards and forward some disk CPU time is typically not even relevant it's all this access time so to handle that most of the online algorithms don't work on the original law data whatever it is they work at some kind of abstraction of the data so here we have a time series in red but we can't fit it in the main memory so we're going to put instead this blue abstraction of it and that within the main memory so here's one example of an abstraction I'll show you in a moment what important idea is that if you want to measure the distance we'll say between Google stock in Yahoo stock and we can fit it all in the main memory we can actually have the approximations in main memory and we can measure the distance approximately in this space again a simple useful trick now of course the piecewise linear approximation is only one approximation to cut of use right we could have also used wavelets Fourier transforms SVD etc etc so we actually have many many choice and things we could do okay now I'm going to show you the generic data mining algorithm and this basically works and pencil for anything I've shown you index and classification clustering rule discovery etc etc it's kind of a basic generic paradigm for mine and data the first thing I'm going to do it when I create an approximation of the data which actually fits in the main memory so we can't fit all at out of my memory we're gonna approximate it and I fit into my memory and now we're going to in main memory a proxy solve the problem at hand we're at rule discovery etcetera etcetera at this point that we have a naproxen solution which might not be exact because we're working on approximation of the data so the next step we're going to go to the disk hopefully only once or twice to either confirm the model or just slightly modify the model a little bit and the hope is that we're gonna make very few accesses a disk in this stage rather than many many accesses we would do we worked in the raw data so if you believe this model then all you have to answer is which approximation to do use and there's lots of approximations out there this actually is a complete taxonomy of them and don't really can read this I'm an important the point is we have lots not the choices and which one should we pick there is one very very important requirement for an approximation we use and this is lower bounded it turns out that if we do this generic framework and our approximation based lower bounding which I'll explain the next slide then when we do this we can guarantee the answer we have at this point is identical to the answer we would have gotten how do we actually looked at the raw data and then all these accesses right so there's actually a very nice result again all I'm saying is that it approximation of aces lower bound principle I can do this very efficient thing and end up the same answers I would have gotten on the raw data so what is this lower bound on thing well we seem to measure example you could in distance right here but again I can't fit all this raw data the main memory the supposed to example I had a piecewise constant approximation which does get in my memory and I measure the distance approximately on that space here so here's a measure of it lower bound and says that for any possible two objects Q and s that the distance in the reduced space is gonna be less than or equal to the distance in the true space hopefully it could be very closely approximating it but the important thing is it's always gonna be less than or equal to if this is true again we can now use generic data mining paradigm now there's lots of proclamations I mentioned this is not in the complete list but when them is comes special symbolic and although there's lots and lots of ways of taking the time serious like this and convert the symbols I haven't counted at least 200 who might have doing that unfortunately nothing lower bound in contrast I can lower bound wavelets for it transforms SVD etc I can load on all of these but unfortunately after two ways out there none of these actually lower bound in symbolic space so why do we care well just think about what you can do if you can actually use symbols right that you can't do in the real space for example I can hash symbols meaningfully but you can't really meaningfully hash real valued numbers I can use suffix trees and symbols which again you can't really use on unreal value things Markov models example more generically there are people out there who are expert in it with symbols people at Google example or people in bioinformatics who are used to dealing with for symbols all the time and they have wonderful ideas algorithms that that was such as Excel etc and if I can actually have a symbolic representation I can steal all the wonderful ideas and again I can't really do this in this real space so a contribution I want to talk about briefly is we have created the first about presentation of time series that actually allows lower bounds that you could of the Euclidean distance and this means we can do all kinds of wonderful clever things I'm going to give you some examples in a little bit so I representation is called sex and again for clarity all I'm claiming is this but you can give me a time series like this one here I can put it in my little magic black box and I'm gonna spit out a symbol string instead and furthermore I can measure the distance between two sets of symbols and this will be a lower bound to the true distance in the original raw space and if this is true that I can use all these wonderful algorithms and guarantee I get the right answer with my generic that amount and paradigm so the obvious question is how do you get sex okay it's quite simple we have the original time stairs here in blue and I'm gonna simply take a window of a fixed size and local areas look the average this is the average of this section the average of this section and so on and so forth it's a piece of approximation once I do this I'm going to take a little Gaussian curve here I'm gonna divide it into three equal areas so the area here the area here and the area here are all equal and I'm gonna extend some breakpoints out and anything above this breakpoint is a see anything below the breakpoint is an A and in the middle here to be so I can take some real value x in blue and eventually spit out these symbols here so let me preamp the question people often ask so why a Gaussian it's true that for long global time series they may be actually non Gaussian but for small local sub-windows or virtually any time series they do tend to be approximately Gaussian and we've seen as action in 120th and datasets invade over diverse domains so here's kind of a visual kind of proof for you like and I could get I can show you this in many many different data sets so the idea is by using this technique we guarantee that we have approximately equal symbols we have as many aids as B's or C's and most things you want to do like Markov models and hash and suffix trees have their best cases if you have equal number symbols equal public symbols no this is actually arbitrary and we can choose a number of symbols we want to use one thing it could be it's actually um we taught about this adaptive at some point originally but we made an adaptive it always became a Gaussian basically something to be a hard-coded that fact I should say actually that if this actually is a non Gaussian time series then the correctness of all the algorithms doesn't change everything is still correct it might be less efficient right but we're pretty safe there here's a quick visual comparison here's a time series and his exact presentation and visually here's a comparison with these other approximations in visually we don't really lose anything right the reconstruction error is about the same in most cases I can always find you some examples that make one of these look better than the other but on average it tends to be that sax actually works as efficiently as other things in terms of reconstruction error again I told you there's a lower bound here I'm not actually proven to provide it but I want to me emphasize actually is true that euclidean distance between two time series is measured like this as we discussed and in the sack space this function here diminution actually gives us a distance which it approximates this distance and will always lower bound and we can prove it by transitivity basically this is known to lower bound this and we can show that we can lower bound this so we know now we have a lower than presentation there are two parameters in sacks like one is the cardinality alphabet a B and C is three example and there is the number of sections here and our choice that we made here unfortunately oh they're actually it's not clearly sensitive choices in most cases okay there's a bit of a history of sax and I'm gonna go get on to this new stuff with shapes so was it meant in 2002 and the first we could as we showed up for most problems it basically works as well as Fourier transforms and wavelets etcetera etcetera for all the classic but boring problems that's location plus an index and then 2003 we showed ur actually sax allows the in time discovery of time-series motifs so we have a time series and having a repeated past minute we can actually find those in linear time it's been less than sixty times but actually most of these times actually applications this people now use this technique for motion capture the user for telemedicine many medical data sets what not to pee but we really do use this quite a lot and in 2004 we shot at you sack to the parameter free data mining last year we had a real explosion in things with them at Saks so we'll do an anomaly detection visualization of massive datasets joining streams etcetera etcetera and recently for example this shows ktd there was many papers actually submitted at least that used sacks as WAV listen Fourier transforms so I really think that snacks get from critical mass here we've actually take off quite a bit so this all been review that I'm gonna show you that with sax we can actually now do data mining and shape datasets and do very interesting things in that domain right but in general calcium until you get all the symbols and uniform would not that the age will be very rare and the Beasley more common and then up the other savvy alphabet those simple T less common simple street you never actually we can have a little fun you want but in general virtually every time to assess the caption properly it's kinda hard to believe and reviewers often don't believe it it's actually built a webpage for $100 said please show this true at $100 empirically and we had a point of viewers to that people's intuition simply wrong there again globally most answers are not got yet but small local windows virtually always are Sybil's yes I do is simply saying maybe not this wise organization thing like the high values and if you do that we will have symbols that have different frequencies and you generally don't want that you generally have equal frequency because again for our Markov models for hashing for suffix trees it's always better to have you go public symbols but you could do that if you want to do and Sachs will allow that it wanted it that would be fine question empirically that's true you can't prove it because it could always be a special case it's not true for empirical is the case at the lower bounds maximally tight and that's true yes the symbols the number of symbols is fixed as a user choice with the user parameter for this time since sorry needed a way to figure out which number in general we impose three or four at number symbols but actually you could try to learn and use and say in description that sympathetic like that and we've done some work on that didn't it's not a good way because we found basically three symbols virtually any kind of problem any kind of data set tend to work the best so we didn't we didn't make it sex and adaptive distributions of Emma symbols but we need it comes a waste of time the simple thing is this works amazingly well so I can't the miss earlier on but I want to show you getting in more detail that we actually can convert time series sorry we can convert shapes the time series so here we have a shape and all we do is we finally sent a point then mean value of the shape and now we're going to actually like have a clock hands go around princesa degrees measure the local distance here and that local distance becomes the height of the y axis so some observations about this first of all that there's other ways of doing this but it's actually I'd like quite a lot because first of all there are no parameters it's completely free it's very fast and easy and it's very very intuitive so why do this why take shapes and map them into time series but again there's a whole arsenal tools we can use in time series space let me give you some quick examples this is the famous Brotman image of two howler monkeys kind of idealized here's the real deal here's some real howler monkeys and you actually can see that euclidean distance here will really be quite intuitive that howler monkeys gonna be racing on to a similar howler monkey and you couldn't this and can reflect that and life is good so we actually query with this we'll probably find this thing here another tool in our Arsenal is dynamic time warping so in some cases actually the shapes might be morphologically changed so for example different gorillas might have a different prominence of the brow Ridge or different prominence of the jaw but in a time series space that simply becomes the fact that these Peaks move backwards and forwards in the Y acts in the x axis and again we can beautifully handle that with an elected Emmy time warping so how do you so another great questions in the questions rotation invariance basically so if one skull is rotated around 90 degrees lovely we absolutely can handle that but I'm gonna just push that aside moment actually bought some papers to show you or already talked about offline this actually the general question how do you make sure that two skulls are aligned in the right direction and for the moment we'll ignore that we actually can handle that easily because you can't everything actually mapped correctly but the sound because one of the bad ones the law basically is helpful actually changing around it looked at it something at first but yeah I took this one here and pulled it down here I thought we check out the lines look across it's a bit looser this time but if I took this and pull it down here and let me go I know oh sure we all unit Lent so um the size the shape makes a difference obviously have a skull that's five times as big as another skull it's obviously the photograph was taken with it in distance so we normally all unit meant basically isn't enough this is the famous school five skull and when you actually seen universities typically had this example here where they filled in the missing parts in epoxy but the real actual skull that was found is basically missing all the nose region now we want to match this it's problematic because if I insist upon matching then miss and nose region under the pathological results so once again in the time series Arsenal we actually have a tool longest kinds of sequence which basically says try to match the shape but if it's too hard to match something basically give up and so here it matches everything beautifully but just the nose part it essentially gives up in that part here it knows like a wild-card I'm not gonna match this so again here's an example to simply show that we can actually take these shapes to convert the time series and then product it up in tools at them but I wouldn't point out actually you could in business tool works exceptionally well by itself so here's a question of primate skulls based upon the Euclidean distance and it's not the correct evolutionary tree so example the humans here should be closer to the orangutan obviously but it is subjectively sensible and at the lower levels of a tree all these red things here all these blue things here etcetera it is actually given to correct subtree or a tree of life north by divided the humans of an outlier here because a massive brain case so again the point is simply this at this point is that we can take shapes give up the time series and then we can meaningfully measure similarity in that space when do you believe that well never got to do some real original data mine on shapes and again is some ongoing work we're gonna show some interesting things we can do with shuttle shapes so first i want to look at we call image dis chords let that give you a database of shapes could be quite large and the question is what's the most unusual shape in the collection so you can look at visually probably most you will grab this one here and this is actually correct answer that was the most objective that we actually have objective definition to this we caught a shape this court and the intuition is we want to find the shape which has the maximum distance to its nearest neighbor right so in terms of island do you think of Hawaii it's been a discord right its nearest neighbor state if I'll be California velasca which is very far away whereas Nevada has a very close nearest neighbor so of all the possible shapes in here which one has the maximum distance to its nearest neighbor and I happen to be the shape right here so why is it useful let me show you some examples so here's a fairly large collection of M fruit fly wings and often people modify the genes and then see what difference it makes in the phenotype of these insects right and actually if you get the wrong gene like a LOX gene you can actually make a antenna spur out of the wing path example so in this case actually we mined this and this one popped out here it could not a phenotype is simply someone damage the wing basically but the point is that we can actually find this unusual shape quite efficiently as well say Oh it is so much stuff that outliers but there are some kind of differences and outliers new Clinton space easy to find roughly speaking but in this case we could have a rotation invariant metric we can actually embed it you couldn't space directly as well see in a moment but it is myself two outliers yes so here's another example here we have a very large collection of petroglyphs basically an ancient graffiti and what actually find other onion usual shapes in a starter set this of course is the subset of it and so we found the number one this chord here is a shape right here and the question is why if I pick a different shape I'll say this one here and look at it newest neighbor which happens this one here you can see actually quite match very well it sounds small local differences but they're basically very similar and this is true for almost all of them they have some similar shape in the database but that's what here doesn't why is that so here's actually the red one and the blue one together is a local difference here and the explanation is that this one uniquely has an arrow sticking into it it's a bighorn sheep and someone stuck an arrow into it which caused this large increase in the time series space here as it happens as a visual check and this is the only one that actually has this arrow in it two more quick examples and here's a collection of because our points are arrowheads and it's hard our estimate how many of these things actually are at UCR alone we have a million of these things unfortunately most of and not photograph the indexed but with a small set of these a few thousand of them we run the algorithm and it pops out this one here and I chose quite and surprised to know that our ahead can be a symmetric and finally some kind of special fishing tools look like this and this example right here is a subset of a large cell we looked at a large slightly looked at and most cells blood cells tend to be round which is like a flat line in time series space or a figure of eight shape this basically corresponds either two cells put together or a cell divide and again this is not unusual because it looks like this and that's not unusual Chris was like this and so on and so forth but one of them this one right here which comes from here is unusual because actually it's teardrop shaped and it has no new match in the data set and as it happens I think teardrop shapes are indicative of em I could have anemia basically so I'll certain examples right now hopefully believe that this definition can be useful for looking at large data collections I guess the question is now how do we find these things efficiently well first of all how do we find them in general here's the actual MATLAB code it's very simple to find them but unfortunately it's very very slow let me explain the algorithm by looking at this matrix we don't need to build a matrix we can actually simply compute things on the fly but it's easy to understand about you look at the matrix so the code basic was two nested for-loops we simply walk across each column and then within each column visit every single row and they look for the smallest value apart from the diagonals so here I come down I find that in this column here one point one is the smallest value that's my best so far and I keep looking for those at this point I find here that seven point five is the smallest value in this column and it actually the largest smallest value so in this case this is actually the discord in column 6 so the good news is the very simple trivial algorithm the bad news is of course is quadratic it's actually conference space but I got the key this in memory but I think they visit all the cells here is I have a quadratic algorithm and the question is can we do better than that and the answer is of course yes so one simple observation is going to help us here which is suppose I'm searching across here so I've found in this column one point one that's my best so far and then here for example I find two best so far when I get the next column as I scan down if I find the value here I thought that I go one which is less than my best so far I can stop when I find the one point two here I know there's no point continuing on because this is not going to be the discord and so I don't have to visit this cell or this cell so a very simple pruning technique can help us quite a bit and we activate so in this example gender politics actually instructed but a cannot make inside the makeup you kind of build it on the fly make this another flight okay so it's actually tension good help a lot and there's one other trick that will help us quite a bit we can actually have heuristics called outer and inner which tell us the order in which to visit these columns so I'm assuming a for loop right now that goes from left to right and then top to bottom but I could simply permeated that and go on any permutation I could visit this one then that one and that one and then when in each column I can visit this one then that one in this one etc and if were very careful it's like a huge difference right if we actually visit them in a special order things get a lot better how much better let's imagine a magic heuristic which is given to us by God right what I always think actually do it would tell us that which admittedly columns in order of discord value and in the world we should actually jump to the real discord first and then the second best discord second and some support right it happened then we would find a large value here 7.5 early on and looking to very good pruning and likewise for the inner heuristic if you are magical what we would do is that when we got to a column we would jump to the smallest value right away we're going straight to the nearest neighbor and it would be less than our best are far and we could stop we would have to look at the rest of things in the call yes sir the analysis basically is this if we had as a magical algorithm right the first card we went to will actually visit every single thing that's at our OEM and then an order to end number time to make one concept move we jump to the nearest neighbor we'd stop so basically be 2n which is linear no no I agree this is a magical what God can do we can't do this okay so if we were magical and godless didn't see happy heuristics they're organized things would not you go from quadratic to linear right so my potential is very good but the question is can we actually do this so the answer is we can't actually do it we can approximate it very closely so conservation that this that although they admitted the columns in this magical order as nominally actually get a reasonably high value early on let me make some difference so early on in our first few attempts of a outer loop we're gonna find a pretty high value here and if we do that we can prune quite well and your occupation is that for the inner loop we already have to jump to the nearest neighbor in constant time has obviously jumped to a near enough neighbor and maybe I have actually less than the coming best so far so it was a fifth nearest neighbor or a 22news neighbor it might be near enough to stop so based on these two observations we're not the title Cox to make the magic algorithm picking a random value and train jump around space to know we knew that remember so the antonis problem is actually based upon sex what else we take our shape give it with time series convert the sex I now we're gonna put these sex words CIA into this little matrix here so it's the first shape CIA and so see a it goes here and I guess the next shape was M see a B and that goes here and so on and so forth so I'm gonna populate the second Matrix here as I'm building this I'm also going to insert these words into subjects try some example image one was see a day I put a one here because the image one came from there and then for image two CIA see a big I put it to here I have an image three it's all to see a a so CIA I have an overspill I have its reader ok so people leave me that actually I can build these in linear economy and basically constant space like a very small constant space so in linear time I've built these I'm going to use these little parks make magic so the first question is I want to visit the outer loop where I'm gonna find a likely discard very early on and how do I do that well I had this count here 1 2 3 and I can map it back into here so everything that is here at CAA this one here gets a value 3 it's in here at the right of 3 there's simply the account of the number of words that happen to map to that particular the number of those words so the intuition is that if I have a neutral shape it probably had an unusual time series which probably had an unusual Sachs approximation so it's count here is going to be very low in context I have a very common shape is an operational time series very similar sex words and the count probably very high so my outer magic heuristic is omit this find the smallest value in this column and whatever that is I'm gonna jump to that value first the value is typically 1 and if it has some ties I'll jump to them I'll take all the ties and go to those first so my simple approximation to the magic arrow Harris is find a small value here and then go to those values first in the inner loop the tenets tuition kind of works here there were something to my search and I want to see if this is to this Court or if not I want to find out very quickly the intuition is I want to find similar things with early on to be able to prune and how am I gonna do this well similar things to this CAA are likely to be either one or I get 731 because they map to the same sex word they're probably similar they'd have a small nearest neighbor distance and I can actually abandon search early on so this is my heuristic for the outer loop of a sec for the inner loop a my hope is with those two simple heuristics I can actually approximate the magical algorithm so this is gonna ongoing work I don't have a and D covers all see right now but on a small data set like two hundred our heads we find out that to get within like twenty two percent of the magic algorithm and the good news is for larger larger data sets we actually get better and better and better we get a statically close to the magical algorithm essentially what solving a quadratic problem in linear time now one small problem is this is actually empirically true and all that we try but in the worst case is doubly quadratic it'll give me a database with every shape is identical I'm in trouble right but from realistic data set with some kind of variance we actually get basically a linear time algorithm okay let me show you a company problem and I'll do this is kind of a high level details be a little bit faster now we have a data set and the question is are there any duplicates in this data set right so trivially here I guess we have this and this duplicate and then the question is again how can we find duplicates and find them efficiently and once again we have a reasonable mission for this basically we're going to minimize you couldn't distance between a pair of objects and one thing on the question is is this useful and I'm going to be very brief in show use one example so here we have a data set about 4,000 different petroglyphs from all over North America including Canada and Mexico and the questions are there any similar ones I mean subjective are similar ones but what are the most similar pair in this data set and actually here's the answer and it's a quite interesting um well notice actually let me rotate invariant to find this so they're similar but one's rotated 90 degrees what's other interesting is actually at these two came from locations which only a hundred fifty miles apart so potentially actually simply some meaning to disk basically although I shouldn't know this point okay so image motifs are quite interesting in this example but how can we find them and again for time reason they're gonna just kind of gloss over this but basically the idea is once again Sachs we can use this explanation and as it happens repeated patterns in DNA strings are very important so gonna steal from Alice's algorithm take our shapes convert the sax and then find DNA repeated patterns essentially and then because it's low bound property we get the data back to generic algorithm we do some cleanup work and then basically linear time we can find repeated patterns using someone else's impatient and I think the point to point out here is that you can really only do this with sax it's hard to imagine how you do this with wavelets or for it turns on anything else because it relies upon the ability here of hashim in the random projection organ like I'm not include here but I have a five minute bonus pieces feature which I will try to get it okay lovely so conclusions that I go on from there five minutes but the first conclusion is and you probably know this basically right the representation is everything with the representation you can solve any problem efficiently and for time series data mining sax is probably the best answer for most problems but the nice kind of news alt here hopefully is that sax is also be the best answer for all kinds of problems with shapes too and I really can't overemphasize this I really believe that sax actually is the way to go virtually all kinds of problems in time certain shipped out of mine and you can just do things when you have symbolic presentations and you can't do in the real space and you can simply use so many clever clever tricks again I will go on after this but my kind of conclusion was going to be even have enough time sax appeal I'm always interested new collaborators and I'm always on new problems new data sets and of course I'm always investing the money okay so here's the five minute bonus DVD feature feel like we can actually use sex to visualize very massive data sets and here's how we're gonna do that let's start with by aggression here's some DNA from two different animals and the question is are they similar animals right and obviously there's no way you could tell by look at this and of course it's actually small subset of a genome which is about three billion base pairs in a mammal that's finally trick let's build a 2x2 matrix and let's label an ATC G arbitrarily and actually count how often we see those letters in the DNA sequence so in this case at 20,000 time we saw a C and 2015 to the time we saw a TA and so on and so forth I can actually make more complicated matrices is basically recursively do a little trick here and that count every possible combination of to see cCTC excetera and i can do this because many level i want to the robot and I can counter often I see CCS or cts etcetera etcetera haven't done this an avid matrix of numbers and I can map those the colors I can linearize all the colors arbitrarily from 0 to 1 and in this case I see that this pixel here for example occurs 7 central time so I can look up 7 percent here and paint it red once I've done that I have converted DNA string into a color bitmap and I guess the question is now so what let's actually look at some files of DNA under Microsoft Windows the classic thing no para guess and here the icons for Homo sapiens locks around africana etc etc and looking at this you've no way have known are these things very similar but let's actually do this now with this trick of shown you let's replace the normal icon with you I kind of learned with this approach I've shown you in previous slides when I do this I see this and as some suggested that these two things look alike and those two things look alike and as it happens this is the Indian elephant the African elephant and this is the chimpanzee and the human so it's not until it would make some sense you can look at these bitmaps and figure out these are a pair and these are a pair ok that was all the aggression because talk about time series and the question is can I do something some of a time series and so what would it be actually useful and the quick answer is of course yes we have sex in our arsenal tools and with sex I can make a string a pseudo dnh thing either like and I want to have a suit of DNA string I can map it into a matrix I can get the colors and like this good so I can do this way fast the question is is it useful so here's the first example here are some brain waves from rats and if you look at this it's what like the Sesame Street game of which one these things is unusual and the answer I guess is this one looks different those ones as it happens these are all epileptic rats and this one isn't okay let's try this I can actually add value to this by mapping them into Euclidean space based upon multi-dimensional scaling I basically find the similarities and kind of close to them in two dimensional space so this actual data set is monthly power demand for Italian electrical when I do this I said actually all my winter months all over here and all my summer months over here but once in a month August is completely unique it's out there so why does it look different and why is it far from these things here and actually if you're Italian I get your PIN you might notice in Italy in August the entire country closed down feed closes the government closes and Evan goes to the beach and actually we can see this if you plot the data right here now of course in this case we can actually see this anomaly if we plotted the data but I guess utility this is you can see anomalies without actually open the MATLAB or Excel whatever tool that actually you have and in any case this anomaly here or this cluster here is too subtle to see to my eye in this space here we're going to see it better in this space right here here's another example this is a famous data set at least some data mining but tests in class and elegans and it allegedly contains 70 heartbeats here's a subset of them once again in the classic view nothing really is added to the value here but in the bitmap view we see this there appears to be two strong strong clusters these guys here and those guys there so what's going on why these guys so different from these guys as it happens they're not heartbeats a grad student many years ago made a mistake so these five things here are not heartbeats that works called pacemaker cells but natural pacemakers not little antique ones so no one has noticed this even though many people actually use the status of test algorithms and again the value here is that we could actually find this if we part of the data and looked at it potentially but we get it here for free essentially when we copy or paste or edit or move files around for free we can actually see what's going on it's my final example and I'm all done for the day I hope actually use this for basically almost like a Google toolbar tool for query by content so I can search for documents based upon the date it was created its file size its type etc and I can also search based upon keywords but I think about a more general search search upon structure well you simply say find me a file system up to this one there's an example of that it's kind of trivial contrived but it makes sense here I have a data set of large mammals in the Americas and I right-click on the brown bear and I asked for its nearest neighbor and so we start cooking on index and the nearest neighbor is the polar bear and you can tell visually that actually they look similar by these two icons here or bitmaps here and here as it happens last night on CNN they announced that actually had found the first polar bear brown bear hybrid in the wild was kind of interesting coincidence so I've shown you actually take time series and map into this and I think I can take DNA and map into this and right now we're working on doing this for text right so using latex planting index something else we convert text into bitmaps and it actually works quite well if you look at my papers they all are kind of similar could only do one thing basically if you look at Shakespeare it tends to look a bit different and summer and so forth and so the question is could we actually use as we've kind of fast search by type basically find these similar documents to this document right here great alright thank you for indulgence and now I really am finished and so I'll be happy to answer any questions you might have what are some specific application areas where you see this being applied I guess Hank before neither shape thing yeah well what you just probably seen as anthropology so an apologist typically have lots of data as I mentioned and million arrowheads in what part of Nevada alone possess in like 10 square miles that's the event we but a hundred thousand pepper glass an amazing don't know anything about these things surprise me so at the poly is one possible example and definitely looking at things like cells in am biology and it's radius actually look at every single cell the Foo Fighter Wing's that you make some sense too because barlas make lots and lots of data like this so those are some kind potential applications and I'm hoping people suggest more when this work and it gets out there a little bit and then we actually do my Sam we're looking up long video sequences of say martial arts and the idea is we weren't trying to find two video sequences but a person actually has the same pose if we have that kind of collision that may actually run the video and then jump from one video stream to a definite upstream so basically kind of make fake it's called video textures where we can combine to them move kicks and punches people try this most cut the data and it's very hard to do in the simple example you have a shape up which don't have any with me right now it actually looks exceptionally well so basically for video games quite useful to do this right we actually have a kick and a punch and we have a kick and a fall and we can actually kind of combine those any combination of those basically so we are looking silhouettes and obviously simplicity there are four many shapes you can look at silhouette you're going to tell what it is actually that's always the case right something people's faces babies recognizing what's face the based on that for some shapes of course the color the texture or internal features are important and current when I'm looking at those but we have to do something to initially start the slide so but I guess potentially future work and regressions and we do for what reasons because it's very heavily funded ten people combine that kind of stuff so we actually are looking at them in on detection in this kind of domain so you think about unusual shapes and the kind of matched shapes so of example even thing that match in suitcases based on the shape they figure out if it's been transferred between different streams but not citizen we actually look at that at a high level "
WuduHBPButQ,24,Faculty Lecture 1(CSE - Engg. College),2020-06-09T05:09:30Z,Faculty Lecture 1(CSE - Engg. College),https://i.ytimg.com/vi/WuduHBPButQ/hqdefault.jpg,IIMT GROUP OF COLLEGES,PT36M,false,361,44,0,0,1,he was and dear students today I am again here with an interesting topic for all of you good morning oil my name is shruti Mehta I am an assistant professor from department of CSC IMT College of Engineering today I am here with a very interesting topic that is the that is actually needed to know by all of you many of you who are in pre finally er must have studied this subject but for first year and second year students this subject and even for for that matter for finally a student this subject is really crucial so let us begin now in today's lecture I will be sharing the contents let me just go through what I'll be sharing in today's lecture I'll be talking about the database and data warehousing the history of data warehousing how the data warehouse housing technique evolved and then the evolution in organization use data warehousing was there but how in an organization its used was evolved like what were the types of data warehouse that came initially there were some other then so that legacy how we shifted from the evolution we are going to talk about that then we'll come above come to data warehouse architecture we are going to discuss what actually data warehouse architecture is and why it is important then we will talk about the benefits of a data warehousing yes what are the benefits of using a data warehouse why not this conventional databases there must be some benefits that is why we have evolved from simple databases relational database to the data warehousing so it is important to know what are the benefits behind that then we are going to discuss the strategic uses of data warehousing there must be some strategic uses also where it is actually implemented that is by data warehousing is much in demand so we are going to look at those uses then will come under most significant part that data warehousing certain disadvantages to which correlate to the researchers to think upon and develop something something something more or you know advanced that is not that is meeting the limitations of data warehouse so we'll come with notion of data Mart yes data Mart is a subpart of data warehouse but it is more specific yes data Mart is more specific and data warehouse is you know it's it's wholesome so we are going to look that a look at the disadvantages of data warehouse and then we are going to talk about data Mart's and also once we have a be a cupboard data Mart then I take you into what is data mining what is text mining what is data mining and why it is so important for decision supports then text mining and some differences between Ola an OLTP and data warehousing integration and finally the most important aspect we are going to talk is business intelligence yes dear friends and dear viewers we are we are discussing we are learning this technology we are talking so much about data warehousing and data mining text mining why so because actually it is implemented in business intelligence it is making it is used in making decisions for the for an organization to drive the business that is why this whole concept of data warehousing databases data mining data marts is so crucial and I hope with this content slide the basic motive behind me picking this topic is clear to all of you so let us proceed further now database and it of their house we all know that database database we know now we will see the differences between all these terms now if we talk about data warehouse it constitutes the entire information base for all time now whether it it is having historical data whether instead it is having updated it is constituting the entire information but if we talk about database then it constitutes the real-time information so data warehouse supports TM and business intelligence so it has two modules with it why because we need to derive we need to come upon certain decisions that will help in our business that is why we use data warehousing it is nothing but a step but we can call it a repository of data okay and from that repository further what we do we mined the data so as to make important decisions which can further enhance our businesses for example let us take because these terms might be you know going over heads to some of you so let us take an example for supermarket now in case if you go to a supermarket you must have seen that certain items the cutter fast selling they are often kept together now why it is so you know what is the decision like how we have come to this decision that this particular item is a frequent buy among all the customers because we have a huge database but from that huge database we have narrowed down our search to limit it to three items these are the frequent buy for example you must have seen at the super-mo a supermarkets like usually a candies and you know some chocolates are kept at the counter because it is stretched it strategically kept so that whenever you are you know many when you are done with your billing and your carrying little kids they know that they are going to demand for that and as payables you're going to get it for them so it is a potential must buy or frequent buy of that particular also we can take in other examples like ladies and cool drink if you if you buy whole drink then you will buy a packet of Lay's together so this is a kind of decision that is made from the huge database so we can call data house supports DM and business intelligence now this business intelligence we have mined this intelligence because we were having real huge database with us huge load of data but we have mine that particular data and we have come on a decision we have derived knowledge from that data so this particular thing of deriving information from the raw data is termed as data mining okay we will discuss further in our coming slides what is it what it is all about so database is used to run the business so this this is what I was telling you that databases are the crucial step for running a business but data warehouse tells us how to run the business because it will tell you the strategy decisions that will be made in the business and it will help you progress in the business alright and also data warehouse houses are a used in market basket analysis this this is what I was telling you market basket analysis it is a term that tells you how much your market how much your production is going to progress depending upon what items will be sold maximum so this is something that is tells you the scenario of market okay so now let us let me take you to another slide now this tells you a producer wants to know which are the lowest and highest margin customers so how that he will come to know he has a huge database but out of that huge database he is just concerned for this particular thing also who are my customers and what products are they buying if he wants to know out of a if he is from Walmart super Hawker if the supermarkets - sorry wants to know what who all are his customers and what customers are buying what then what this a kind of information how we can deal with this information and likewise we have other questions like which customers are most likely to go to the competition what in fact will new products or services have on revenue and margins what is the most effective distribution channel what product promotions have biggest impact on the revenue so to deal with all these questions we know that we have data that is exploded everywhere and we cannot find the actual data for let me take you to the slider back now these were the questions that you know only the or only after you producer wants to know but hunting for the answers is something that out of the big day out of the huge data how can be hunt for the answers why because data is scattered over the network many of you will agree to that and there are many versions of the same data but there are subtle differences between each version further we need an expert to find the data because our query is specific so we cannot understand the data we found that means sometimes our data is fully documented this also might be a big hurdle also we cannot use the data we found sometimes our results are unexpected the data needs to be transformed from one form to another so for all this this is some time refers to as data explosion problem also data is so vast and it is exploded it is scattered everywhere but we beat to hunt for the right data because we as end users or we as customers have certain specific queries we are not concerned with the entire data we just are limited to a particular set of data for example in this particular slide the user the customer wanted to know only the lowest margin customers or the highest margin customers he is not aware he didn't want to he is not concerned about the whole data so I hope I am able to make sense ah till here now coming on to the conclusion of these three four slides now how to define a data warehouse so weak put a data warehouse and we can call it a repository or a single complete or consistent store of data it s our store just Mohammed need a taco integrate for me except me here or s here except a head achieve a complete or consistent information this again so let us read the definition it a data warehouse is nothing but a single complete and consistent store of data obtained from a variety of different stores sources now important point which needs to be highlighted here is that the data warehouse not be data that is coming from Heathrow genius data sources okay it is not it is not always the case that the data is coming from same sources known because b has vast data so he true genius sources say data are AHA Haley cameos a single complete or consistent storm a drug dealer whose cone nebula Hadid of warehouse by so as it is made available to the end users in what they can understand and they can further use in the business context so they can because ultimately this particular deal there has to be used file users and they have to use in an organization to enhance the growth of the organization so this particular data needs to be consistent it which to be complete and its needs to be integrated so this is this is what is a meaning of a data warehouse now what is data warehousing data warehousing is nothing but transforming the data to information if the data warehouse is definitely a repository where the data is stored but if we talk about data warehousing technique then data warehousing is a transformation that how we can mind the information mind the crucial details that are needed from our raw data and this particular thing is done so as it is you know in a timely manner it is made available and it is of use this is something when we transform a raw data into information this particular technique is referred to as data warehousing all right now it is a process it is a relay now how it is a process because we are talking about it is a relational and multi-dimensional database management system which is designed to make the management decision-making we all have been talking about that it is crucial in organizations in making proper decisions so a data warehousing is a copy of transactional data and it is specifically structured q8 querying and reporting the end task is what we need to query the data that is stored in data warehouse bias so so as we can we could make certain reports out of it we could analyze it this is the only purpose of the data warehouse so a copy of transaction data is specifically structured now coming on to the next point data warehousing is also a technique for assembling and managing data from various sources by so as we can answer the business questions if our technique if our data will be assembled well so I must go manage car buying a chassis or manage can be secured Bob Iger managed can may say how mosquito strategy Utley business portions in Joe Mary end users queries they have on home what effectively deal verifying it so this is a broader definition of a data warehouse and that is why it is termed as a process is the bots are each design Walden thus making decisions that were not previously possible now by these decisions show how the decisions are blinking Mohammad could previously known he's a young reminder am layoffs but query is lagana goggles who analyze carajás data so this is something we call or lap Olsen what is a lap online analytical processing okay now let's move further about now there are certain characteristics also of data warehousing if we talk about the first and foremost characteristic of a data warehousing is it is subject oriented now what do we mean by a subject oriented it means it gives information about a particular subject rather than company's ongoing operation it is catered to a particular subject so we call it subject oriented then second is it is integrated integrated means if the data is gathered into the data warehouse from a variety of heterogeneous sources as I already told you and it has merged into a whole over at Whoville so we call it it is integrated why because if the data is coming from various sources okay now our job is that the job of the data warehouse is to integrate that data that is coming from all segments on heterogeneous sources next important feature is time variant all the day time the data warehouse is identified with a particular time period that means we are particular be interested let us say for example number of students who have done certifications during the lockdown period now lockdown begins in March okay so from March till Bihar in June so from March to June how many students were actually involved in the certifications in the quizzes and in head nap in enhancing their knowledge so you understand it is time the time period is associated so it is time variant okay now non molar time the next feature of a data warehousing if we talk about it is non-volatile that means the data in the data warehouse is stable if more data will come it will not happen the previous data will get erased know it if more data is added the data is never removed that means it enables management to gain a consistent picture of the business that means if we want to have a record of the previous let us say three years data or if we take an example let us say let me take an example of my own department computer science department and if you know some query arises from the management that he wants to know in the last three years how many faculties have left the organization how many faculty have joined so how they how he can work on this data previous three years data we give him see because it has maintained in a data warehouse now data warehousing is also combination combining data from multiple this I have already discussed and the common accessing systems include queries analysis and reporting and uses connotation data warehousing K these are the queries and the end users query the data analyze the data and further report the data okay so we have and the final result is the homogeneous data which can be more easily manipulated okay so now history of data warehousing if we talk about history of data warehousing just I will skip the slide because it is more what is important is in 1980 there were two researchers body Denton and Paul Morphy so they initiated this concept and they worked a lot into data warehousing and then there were stages 1968 development for 1970 may 83 not 88 but the important is body Devlin and Paul Morphy they were the two persons who actually worked a lot now now having understood data warehousing databases now let us move to OLTP now what is OLTP online transaction processing now this is a special type of data organization and access methods and implementation methods now why these are needed because we need to support data warehousing queries for it because if we talk about a data warehouse it is typically multi-dimensional there is huge amount of data that is load across many dimensions so sometimes it is important to organize or to categorize the data okay and we need some access methods also we need certain implementation methods also so well TP systems act even for transactions and workloads so how the transaction process because we all know that in today's time everything has gone online right so online transaction processing that means how these transactions are actually processed they are definitely working on some data that is why how do you pay your bill he's telling the mobile phone Vince it is all done generally everything is done at the click of a phone these days so we need to understand what does OLTP it is something if these systems are tuned in for known transactions and also for workloads ok so now what is now if when you're talking about OLTP and data warehouse it is important also to you know understand that there must be some differences between the two well TT and data warehouse let us quickly just go through the differences now if we talk about data warehouse we have already discussed the major characteristic it is subject oriented subject oriented mean rather than company's all operations it is going to cater to a particular subject now if we talk about OLTP it is more of application oriented why because it is used to run the businesses whereas data warehouse is used to analyze the business by run the business because it is transaction based we have to do certain transactions if we talk about OLTP so we need to understand so it is also it is having detailed data if we talk about data warehouse it is integrated summarized and fine data is there and will TP the current up-to-date data is stored and data warehouse the snapshot of the data is taken also isolated data is there an OLTP and if we talk about data warehouse the integrated data is there also OLTP is for clerical users but data warehousing is for knowledge users now in LTP we have this limitation that few records are accessed at a time because if one query is going on then we can access those particular recalls in which we are we are interested in our transaction but in contrast if we talk about data warehouse this then large volumes accessed at a time now read and update access is possible and if we talk about data warehousing then redundancy is only more a mostly read operation as possible there is no update that it is possible in OLTP there is no data redundancy but if we talk about data warehousing then yes redundancy is present and the database size for 20 people vary from 100 MB 200 GB whereas in data warehouse it is from 100 GB to a few terabytes also transaction throughput is the performance metric how is the particular tons of transaction performing the efficiency of the is the major performance metric there and in data warehousing we are mostly interested in a query throughput why because we are dealing with that will be query analysis and the reporting the reports that are generated so definitely what is a particular throughput of a query is the only performance milk within data warehousing also will TP thousands users can use data warehouse hundreds of users can use because a la a lot of data needs to be accessed then it is managed in entire entirety whereas data warehouse is managed in subsets now coming on to now if you if I need to summarize this then I can say that OLTP systems are used to run a business where as data warehouse is to optimize the business by because we need to query further the data but in case if I need to run a business then OLTP systems are definitely one thing that we need to how to run a particular business now coming on to the next important aspect of data warehouses organizational use we oh well know by now that data warehousing data warehouses are actually used in organizations right now there are certain they have not you know come all of a sudden they were there from nineteen eighties nineteen sixties but they have evolved so earlier what I will just left let us have a look at these these evolution process earlier we used to have offline operational database offline operation database then became offline data warehouse then real-time data warehouse and finally integrated data warehouse now what was happening Oh happening in offline operational database the data warehouses was simply were developed by simply copying the data off from an operational system to another server right where the processing load of reporting against the copied data does not impact the operational systems performance simply the data was being copied or offline data warehouse the second category data warehouse now what was happening is this at this stage they have they were updated from the data in operation system on a regular basis and the data warehouse data is stored in a data structure designed to facilitate only reporting so this was happening offline data warehouse then came real-time data warehouse now data warehouses at this particular stage were updated every time an operational system performs a transaction the everytime and the transaction was being performed it was being updated in the real-time database then came the integrated and the final data warehouse which we are working right now now data warehouse at this stage are updated every time an operational system performs a transaction also the data warehouse then generates the transaction that have passed back into the operational system so the difference is this between real-time data warehouse and they integrated also that the transactions that are generated are passed back into the creational systems now now having understood this much we are now in a position to recall the architecture of data warehouse now as I said the role of the data warehouse is to integrate it it is a repository where the data is - and the data is coming from a pure genius sources so you can see in the slide that there are different sources and the data is coming and it is getting integrated right there's a sign of integration and then it is further getting stored in a repository called warehouse from there in the final end users we have named it as client the query Inanna analyzed and there is one important part called metadata now what is this metadata this is data about the data whatever data is stored in our data warehouse something important related to design lis in retrieving and controlling the rate of your house because if metadata so just if we talk about like data warehouse they can they contain current detail data historical detail data lightly and highly summarized data and metadata so what type of data is actually contained by a data warehouse is it can it works upon the current detail data the historical detail data lightly and highly summarized data and metadata so based upon this particular thing we are further defining our metadata now if we talk about metadata is data about it all right whatever data is - but it is further split into two categories one is your technical metadata and second is your business metadata if I talk about technical metadata then it includes where the data came from how the data was changed and how the data is actually no organized okay who owns the data who is responsible for that particular data and how to contact them who can access that particular data and the date of the last update so this is something all technical met if I talk about business metadata then it includes what data are available where the data are available what the data mean how to access that particular data the predefined reports and queries and how the current data is okay so this is something about metadata now advantages how this particular technology is impacting our businesses the first and foremost advantage is it provides business users with a custom centric view of companies heterogeneous data why because it is helping it to integrate the data from say al service manufacturing distribution or other customer related business now these are different subjects but what is happening is a customer centric view is made available also better to access better information we use it now consolidates the data about individual customers and provide the repository of all the customer contacts by it this particular repository of customer contacts is needed because we further need to deal with segmentation modeling customer retention planning and further cross sales analysis so in business all these things aren't highly needed and highly recommended so that is why this data warehousing strategy plays a very crucial role also if we if we need to you know you need to look at certain reports that are coming on trends across multi-divisional or multinational operating units then what kind of relationships are there if we talk about production planning so this is something where this data warehouse plays a very crucial now coming on to the strategic uses of data warehouse this sub this is something each one of you must know now let us these are all examples how data warehousing is used by each and every industry now if we talk about airline industry we all know that creo sign aircraft development analysis of the route probability frequent flyer program all these are strategic use uses okay now how how P can you know calculate this particular aspect it is only through data warehousing that you know this particular route has the highest probability in an airline industry it is very important I mean like at on which route the flight will go and for example if it is a tourist destination then definitely that particular route profitability and probability of the owner is will be moved so that is why analyst is also mix of fares so these are certain earth strategy cues in the airline industry similarly with banking sector we need to understand the trend trend analysis product and service promotions for which product and which service prefer promotion combinations so likewise and the third example is for credit card for credit card it is very important to analyze at a customer service you know to identify the fraud detection in case if it is happening we all know that with credit cards one thing that you know is we cannot deny slot detection so we need to you know this is also where data warehouse can actually come into use and we can use them strategically also if there is some new information service that we can deal with that in healthcare sectors data warehousing are are immensely used why because they would reduce operational expenses also in investment and insurance like risk management market movements they are used in the retail chain they are immensely used to analyze the trend analysis the market basket analysis which I earlier covered buying pattern analysis what are the patterns of buying which customer will buy in what and in what amount pricing policy inventory control sales promotion like why in telecommunications personal care and public sector also data warehousing are being strategically used so you can I just go through you can just see that data warehousing is like it is so much significant these days that every industry whether it is airline banking credit card health care routine shame personal care everyone is working on the data warehousing now data now you know there are certain disadvantages also of data warehouse now what it would be what could be the disadvantages the first and foremost is it does not provide an optimal environment for unstructured data structured data is fine but if we why because we need to you know integrate the data so it is it does not provide an optimal environment so this comes as and biggest disadvantage also ETL operations ETLs extraction transformation and loading we need to the data needs to go through these steps that is why so it adds it adds you know to working in a data warehouse also data warehouses and higher scores the maintenance costs are high and the next disadvantages it gets outdated relatively quickly so and if you talk about there's a fine line between data warehousing operational systems also so duplicate expensive functionality may be developed right and so these are some of the disadvantages that are there in the warehouse if we talk about now what is a solution since we cannot you know give up working with the warehouse feed we have already understood the significance but we have to you know devise a solution for these disadvantages so what is a solution is data mod now what is data not it is a scaled-down version of your data warehouse but it focuses on a particular subject area that means data warehouse was whole some but de tomate is a subpart of your data warehouse that doesn't only focusing on a particular subject area so we can call it a subset of organizational data store right which usually oriented for a specific purpose right now what were the reasons for creating a data Mart why this is important easy access to frequently needed data this is one reason why data Mart's could have come also a collector view could be taken by group of 
tioXzprhgeo,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-03-21T21:57:23Z,Data Mining  (Spring 2016) Lecture 15,https://i.ytimg.com/vi/tioXzprhgeo/hqdefault.jpg,UofU Data Science,PT40M26S,false,104,0,0,0,0,"thing and i will i will have office hours 39 710 but all five to leave by nine for you okay but i'll try it i'll be very good on email and / canvas so just kind of post questions there or and Sophie okay so also there's two other points there's a regression one this one again is worth about 500 is worth 100 points and the graphs one is worth 50 points it's do kind of very end of the semester um it's not meant to be very hard these are going to be mainly in designed for working in in matlab you haven't done ma'am I before don't worry it's actually really easy to very easy to use it's just it makes these assignments easier to do it them in back um okay so the other thing I want to point out is let's see the also the fan reports to April 13 okay that's coming up soon but the good news is you're basically done with the work for the class of it right so your other classes probably have a bunch of other stuff to this period of time it'll be mainly done with this stuff that the poster and fun stuff after that and you probably did a lot towards interview before already so you're probably really far along in this class so that's great the other thing is the lectures at this point are going to be um they're still going to cover some technical ideas but there's gonna be a lot more kind of more fun content without thinking about data as opposed here's a specific algorithm and yours may be how you apply it so much so I so I know a lot of you have been following along in the note sword or videos um so that's that's fine I think you'll get more out of a lot of these classes by coming to class there's more chances to see more than just skimming through the notes and doing the homework if you want to do that that's fine a few you're going to get more out of it actually how many cuts okay so any questions about the schedule before we get back to the lecture alright so you're still waking up from from rape alright ok so this random projection techniques okay so what is the idea again we're going to have the input our input data in this case is is we're going to think of it as a point set P which is in RP so this is this is going to be a and so this is this is going to be interesting where D is very very large and i'll talk about how large you'll actually want this to be going to be addressed okay so this could come up in now this is going to be similar to these random projection or to these pca problems where you really going to want to be able to think about this naturally rd meaning all of the coordinates are going to be essentially our power going to have the same units on okay so this could be your you're looking at a discrete distribution of something for your you've taken a bunch of stock prices and you've taken a string of sequence of a bunch of them each of them is the price at end of the day or the difference Trina preferences so they all each each of those is a coordinate in a high-dimensional space okay so each data point so each each point is going to have these deep coordinates and they're all going to roughly mean the same thing okay and so the goal of this this windward as we talked about this doing this principle component analysis and with that CD which was doing a form of dimensionality reduction um but what what what that was doing is you have a set of data points and the goal was essentially defined of a subspace through here so that if I projected all of my points onto this subspace but I projector them all into the subspace that the sum of all these I guess the square thinks of these distances was not too much I didn't I didn't have to project them very far and so the goal in this case again will be to go to kind of live in some lower dimensional K dimensional subspace where k is going to be is going to be much less than D you're not used to it if you see a less than a less than sign it's kind of a formal way of saying it's not just less than that value it's a lot less and this is actually you'll this is like a semi formal notation people use this in papers but it means it's going to be a lot smaller so now this this red subspace here this is going to be so k-dimensional where the data original data line is high dimensional subspace and the smaller dimensional one for the PCA or SED allow you to often godowns it's a two or three dimensions where you can make even draw these points and you can start using your low dimensional intuition of what's going on again on the random projections is going to work in a much larger regime here ok I'm going to be thinking of deep going to be very large and K is going to be large too so often you might think of d as in something like a hundred thousand and k's like 500 okay so we're not gonna be able to get down to like two or three dimensions anymore using this um but well once we're down to fight hunger instead of 100,000 or or something we could then say for instance run pca or something else on it and this is not going to take as long as if you had this any dimensions so we're going to get down to a lower dimensional subspace and so so what we had what I talked about before was there we're going to come up with is some mapping that goes from rd to RK and so I written something similar for this singular value decomposition on before but now I really need that RK is not just a subspace of rd it's actually going to be a K dimensional space very very formally so we're going to and the goal is that what we have is we're going to want is on for all p 1 and p 2 and p so for all pairs of points in in our point set p what we're going to want is the property that 1 minus epsilon if I look at the if I look at the distance between this pair of points and I shrink it by a little bit then this is going to be less than or equal to then the distance and again using this norm distance so this is implicitly the this is implicitly the Euclidean straight line distance it's less than the distance between the if I I'm projecting it so these points I've projected using this mapping move into this K dimensional space and I want this distance to be the true distance shrunk by a small amount to be smaller than this distance and I also want that this is less than if I look at the original distance and I increase it by small amount then I want this to be larger than this ok so I took the original distance t1 minus t2 and I shrunk it by a small amount I increased by a small amount and I want the rejected distance to fall in this range so if kept salon is saying point zero one then this is like a one-person air I want all distances in the new subspace to be present served up to one percent air of the of the original distance and I want this to hold for all pairs of points so this is a different sort of guarantee than before before I was talking about projecting to us a subspace and minimizing the cost of the sum of all these projections it's kind of like an average air this is now a worst-case air this needs to hold for all pairs of books ok the lowest possible yes right so the cool thing is that we're going to find so this size of K is going to depend so k for most this talk about some cases all use a slightly different value and so there's an asymptotic notation here okay so k is going to be 1 over epsilon square times log in okay n square let's see and it's going to be the size of P so and as and it's the size of eat epsilon is this aircraft so if this is one percent and one over have some squared is to be pretty large this is going to be like 10,000 okay this log in this depends only on the size of the day no this is independent of D independent of with the original dimension of the space was so let's think about why would they see that makes sense to do something like this to try and find this um so let's start before we talk about this guaranteed which of the interesting its own right let's think about let's say we only had two points but they each had like each of the points had each of the points was in each of the points was in a 1 million dimensional subspace or a 1 million dimensional space again but I only have two points how many dimensions do you think i would need them it's gonna be even even better than this bound and i can have no air I could have epsilon equals 20 yeah I yet right yeah so if you only have two points there's always a one-dimensional line that goes through those points and the distance between those points is that exactly the distance along that line right so if I'm in any dimension it doesn't matter what the dimension of the points are I can always draw a one-dimensional line and then this will be our one and then I can just look at the distance along this one dimension and that's exactly the straight line distance between so in general if if N equals well I can always go to k equals n to n minus 1 dimensions so I can always do it so I can pick a que dimensional subspace with advice where it's n minus 1 dimensions again independent of D so if you think about this yep is this related like um that's what separating points but it just seems like yeah so there's there's probably some relations that this is pretty it's probably pretty distant so the VC dimension which we want to unfortunately get into in this class this sort of comet oriole dimension of points and this is kind of saying was the dimension of the data you have here what this is really saying a better way of thinking about this is that if you go back to the singular value decomposition the number of subspaces are right singular vectors and singular values that matter depending on the rank of the data and the rank is at most the number of of data points ago or most pen lights one of the data points right there was zero was implicitly in those sets so the ranking was actually an but if i take the zero point out the origin out then and i get a nice so this has to do with the rank of the matrix I can preserve this exactly that means that there was some red subspace up here that went to all the native so but you know often this n is is it's going to be too big also the other downside of this approach is that you would need to know all the data ahead of time this random projections approach which we'll talk about is going to be able to define a mapping you with satisfies this actually with some probabilistic terms on without knowing the data ahead of time it's going to be the definition of you is going to be oblivious to the data so I'm actually going to need another term here this is going to be the size of the end over Delta and this will well so full with probability 1 minus Delta right so I'm going to have to say like one percent error failure it holds ninety-nine percent of the time and I'm going to have another terms that it goes inside of the lock so it's it's it's it's not too bad usually the end is is much bigger this case and inside of a log taking product things kind of addition so this is adding like a one or two optimism so this can be a random approach but it's not going to need to see the data ahead of time pca was very dependent on the data this is very dependent on the data and when n is large it's not really necessarily helping courage okay so we're going to get this guarantee how are we going to do this how we're going to define this subspace so let's start by going from rd to our k equals 1 so just to a one-dimensional subspace and then it turns out we're just going to do this K times independently of each other okay so I'm just going to go down to a down to a one dimensional subspace and the idea is that I'll have these these data points and I want to get one dimension which describes this data and so I'm going to essentially draw a line and just like in this picture above I'm going to project all the data onto this line so I'm just projecting all the data on the line and then there at some point there's some there's going to be some some origin in fact i think this red light is going to go through the origin but then each point each place is going to get a coordinate so for instance if this was p 1 then this point here is the distance from the origin here is mu of p1 is the first coordinate here so this is the one coordinate I just give it a distance from zero now each of the points has a coordinate and the distance between the points is just if this was a distance of two and this was a distance of four here then it is between this point and this point is going to be too right right so just use a support okay and so this is called this this this is called the random projection technique so what I'm going to do here is this this red line here i'm going to call you j all right it'll be the it's this is going to be this going to be i'm going to create this with a random come i'm going to create this with the red and unit vector okay so this is a on a random unit vector and so so we talked about how to create these random unit vectors before they would remember how we did this yeah John a normal distribution analyze it that's right yeah so what we need to do is draw from we need to draw for a month from a normal distribution and so this is but this lies in in our d right so what you're going to do is you're going to draw a g1 so so you're going to create GJ 13 g j2 up to g j d ok hgj I is going to come from a each of these that's going to come from a normal distribution and a normal distribution and you can get this using what's called the box Mueller transform which takes in to which I call these z1 and z2 where these two are going to be uniform so you can create two uniform random variables between 012 this box Miller transform this gives you a point from a normal distribution and you're going to create a of these normal points of a normal distribution this gives you a point from a d-dimensional normal distribution okay and then we'll call this vector G and then r GJ and then you J is going to be GJ over the normal of g j right so I'm normalizing this gal she ran D dimensional Gaussian random variable it becomes a normalized vector in our deep that means it's norm is going to be 1 and it's going to be uniform over all directions point equally in in any directions okay and that's going to be this random vector ok now how do i get so this gives me one random vector now how do I get this coordinate its first court this coordinate is value for P 1 so now i'm going to call this point let's call this point let's call this on q1 this will be the end of being q1j so it's going to be the first or the Jade coordinate to this so Q q1j this is going to be equal to just the dot product with you J and K 1 and if you remember the the dot product over some l equals 1 to d is just the sum of the coordinates g j UJ l times p 1l so that coordinate L goes from 1 to d essentially each of these values after I've normalize them I take the product together with the coordinates of this and this gives me q1j which is a single coordinate here okay so this gives you one coordinate of my K dimensional subspace this goes to a one-dimensional substance and and in order to get 2k dimensions I just need to do this whole process independently k times let me try and break this down so um so ok so I'm going to try and write this down in the form of an algorithm just because it's a little confusing probably what is being reused and the order of these things so I'll try right here so for see if I can make sure I get the right medication and consistent notation yeah okay for j equals 1 to K so I did this for this was going to Jade coordinate so I'll do this for j going from 1 to K so I'm going to let you J is going to be a since this me random of the D dimensional so this will be my random d-dimensional up on unit vector so and actually to get the to get the right bounces projection there's some bias when you project so I need to read scale this so I'm going to set you j equals to D over K square root yes so just ensure that this is normalized this could have been the Gaussian random variable going to rescale by square root tea / k i'll explain why that isn't a bit um ok so now for so I I can so these are going to be enough to define this map this is going to define mu this map each of these defines the Jade coordinate of mutant ok so then for each point P I NP for each j in 12 k i'm going to set i'm going to create this this projected point you know let me write this up q Jay is going to be equal to P i dot u J so I'm going to take this dot product this is giving the appoint QJ i'll call this q IJ actually and now this point this mapping of P I is going to be a point Q I which is going to have Ford myths q 5 1 q x 2 up to q I ok ok so now this lives in RK but okay so I need to define this mapping once based on these needs men unit vectors and then from each of the points I reject them onto each of these dimensions and this dot product just tells me the coordinates that's it and so for this point I'm going to get for the point P i'm going to get q IJ for the values j 1 through k and these are the coordinates of the new point and so this is aside from dealing with the the random gaussians which is not too hard this is actually easier than than running this this SVD computation okay and it's going to if I rescale by d / k then I'm going to get this strong the stronger here about if I make a large enough the stronger bound where all the distances are preserved approximately and the larger hands the better you can think of the larger kids the better the approximation guarantees me the smaller than epsilon is going to be okay um let me just okay suppose so this is this is the whole technique the whole algorithm write again very simple this is again it's you can do this much faster right this is going to be your just for every data point you're just doing this dot product and you're reading off the corners very simple very fascinated you don't have to you know call library to calculate that video okay and so this this technique and this approach is known as the so this technique approaches cobbled he is called the Johnson miss Ross lemma or the Justice trust transform named after two guys so and and so this is kind of commonly used to analyze very very high-dimensional data so if you if you look at this your pie see this technique it's very simple what's going on okay so let me just one thing to point out why do we need this scaling by a square root d / k why does this needed mom's what happens if I don't do this so this D over k should be pretty large but what happens if I don't do this do you reckon okay let's go back and look at this picture here so let's say I took 22 points let's highlight two points here and i'll pick these in particular let's say i took these two points and i projected them to here and here right so I projected them onto the red line what happened with the distance it's smaller yet it regionally it was here now it's here okay I picked these two points because they're on the other side of this red line right um you know that makes it clear that smaller but can the distances ever get larger now the distances always have to shrink they always have to shrink one way I've seen this is if you know that the Euclidean distance let's think of the squared this squared euclidean distance right so write something else here this is where you're putting distance between P p1 and p2 is is going to be the sum over the over the D coordinates Red Square right I'm adding up all these terms all these terms are positive what happened in Iran Direction was actually one of the coordinate axes those one of the coordinate axes that means that in this some I only use one of the terms in the Sun I use the term that correspond to the first coordinate axis okay and so kind of on average if there are D coordinates to start with and then I'm choosing k of them I think of it they happen to be the coordinate axes that I'm essentially picking k out of the D terms in the square terms at random and so to get the right expected value I need to kind of estimate the value kind of I have to pump this back up I'm shrinking it by K over d the square value so you need to inflate it back up by d / k and that was the square value if i take the square root of everything that i need the square root do retain okay so every time you do a projection you're always shrinking the distances this is also true if you're dealing with the SVD as well whenever I did the projection I Shrunk all the distances together so this D over K charm is to make sure that I'm unshrinking all the distances okay so that's what's going on if you don't do this you'll find that your distances are all too small okay um so so let me just make a few notes to wrap up and then we'll start this this peer feedback stuff so when this jocelyn estas there came out um this was this was in 1984 so let's it's not that old it seems like a very important part of mathematics that you can always reduce stuff down to roughly log n dimensions it's only nineteen eighty-four the first thing it didn't quite do it this way these it took this this K dimensional subspace here the phone is random projections and then it made these all orthogonal to each other and they had to be orthogonal there was a bunch of kind of follow-up things where they showed each of the cordons can be done independently in this still works fine they don't need to be forced to your doctor they tend to be orthogonal enough on average um also instead of this you can have you can do something simpler with these you JS these can come from each of these coordinates can come from minus 10 and plus 1 you can have them so they are instead of these Gaussian random variables need to actually you can skip that step you can skip going to the Box Muller transform you can take minus 10 a plus one at random for each coordinate and the stories the nice thing is that a lot of them are zeros you can actually make up to two-thirds of the entries 0 and the zeros are good because when you're taking the dot product those you just skip you can skip all the zeros now you want to make even more of these zeros and so there's been these fast Johnson those frost approaches which are trying to even more of these zeroes there are some technical issues you can't always do that you can't make too many of them zeros because you'll just miss entire things when you're doing this projection but this is a much easier and faster way to do it there's some class of these sub Gaussian random variables that you can also apply here okay the one last thing I wanted to point out is that if you're doing this for matrix approximation what you want to think up as now your input data here this is P I called it basically this was a before this is your input data now before i had this was on the number of columns was um was this was n but now the number let's say the number of columns was n and the number of rows was d because we're going to think of we're using the the dimensionality again and so the way to do this sketch is you create a sketch matrix that's going to be um this is not going to be d by K in each of that entries here say is minus 10 and plus one independently and then this will create a sketch matrix B which is going to be this is going to be n by K okay so if you're doing a sketch like the like the freaking precious can just create this matrix full of these these these entries independently and you can create the sketch it turns out that in this setting for this problem you can actually make for each column you can you can pick one element that's on mice 1 plus 1 and the rest are going to be zeros and this still works but it needs a a few more key to be a little bit larger but this is nice because now essentially this this matrix is almost all 0 so this is much faster ok so this technique is going to be much less accurate then the doing PCA directly but maybe you can do it much faster and much easier and you can define this matrix ahead of time you don't need to know your data to do it where the SVD you need it to call this this very data dependence singular value decomposition instant it's also kind of a very cool property all right so this was i would i would say a few more things but i'm going to stop here so we can start the peer feedback on many questions on this before you wrap up ok cool so let's see um the roofs let me see alright so so how are we getting to the script so I'm going to figure out how you guys are going to pair up this is so okay so here's the group of hearing does everyone know what group number there in UC would not know what group number there in check the next email ok so the group numbers Oh okay so these are the group these are the group number eights these are this hearon so we try to pair you up so people working on somewhat similar topics are going to be in the part you can be discussing with each other okay and so how will i'm going to try and get you guys to self goof off a little trying to do it like in this way so the first pairing will be here the secretary third pairing and so yeah so if you need help with this please give us now so you can turn off "
HGQMN4bNkZM,22,Reference Book ---  Data Mining:  Concepts and Techniques(Second Edition)- Jiawei Han & Micheline Kamber,2020-05-25T18:22:03Z,Data Repository for Data Mining : Transactional databases,https://i.ytimg.com/vi/HGQMN4bNkZM/hqdefault.jpg,Ganesh P,PT19M22S,false,26,0,0,0,0,welcome we have already discussed two type of data repositories relational database and data warehouse the next data repository is the transactional data basis so what is a transactional database transistor a transactional database consists of a file whoa each record represents a transaction the transactional database is actually a fire well each record of the file represents a transition transaction one that is past record and section to second record exit typically a transaction includes a unique transaction identifier transaction I and a list of items making of the transaction such as items purchased in a stroke a transaction means the report which includes a unique transaction tid trans ID etc are used to represent and a list of items making up that fancy example items purchased in a school this figure shows a fragment of transactional database for sales at all electronics exam so transactional database is a fire were each record each record represents a particular transaction each record represents a particular transaction each transaction has a unique transaction identifier that is the first column normally and a list of items associated with that transition here in the transaction ID 300 a customer buys item 1 item 3 item a in the translation 200 that Street to p200 only two items are purchased ie 2 & 8 exit the transactional database may have additional tales associated with it generally a transaction database consists of a transaction ID and we stock items it may also have additional tables associated with each the additional tables contain the other information regarding the stay such as date of transaction then the customer ID number ID number or salesperson and the branch where the sale occurred and so when we buy some products then we get a thing that will is a transition in that bill there are many data which items are purchased its quantity price then branch work from work it is bite that is the branch or the short name date of purchase exit so the transactional database may have additional tables associated with it which contains the information regarding sales such as date of transaction customer ID number ID number of salesperson the branch from which the sale average exit in our example transactional database one record per transaction is the knowledge one record per transaction is stored in the transaction table from the relational database point of view this table is a nested finish in the point in the relational database point of view the transaction database is a nested relation because the attribute lists the attribute list contains a set of theories most relational database systems do not support nested relational structures so the transactional database is usually either stored in a file in it flagged by the format of flat file is similar to the figure shown in the example oh it is unfolded into a standard leash in a format similar to that of the items sold table in the relational database most of the relational database systems do not support nest relational structures so the transactional database is represented either in a flat file the format of the flat file is each row represents a particular intersection each transaction has at least two fields first is a unique transaction ID as I can even store or our transactional database is represented as I'm foliage Hindu the relationship the transition database is unfolded into a standard flesh that is in the first concession 300 we have i3i one i3i 860 so when we convert this into a relational database form then we have a transaction ID then item name or item five so first double as a value 800 I won next up it has the value P hundred I three then I sorry T hundred I each so when we convert this transaction database into a relational database table we have two attributes in the take first transaction ID second I delayed so to represent that transaction T hundred with four values we require four records in the relationship so unfold the transactional data so first method is use flash file use flat-file to represent transaction database the second method is unfold unfold the unfold the transactional database can be represented as a flat file or and for the now an analyst of all electronics database asks an analyst analyst ask show me all the items for chased by Sandy's or how many transaction in grow item number show me all the items purchased by Sandy Smith or how many transactions in groov the item number answering these type of theories may require a scan of in the transaction so to answer some curious we need to scan the Ender transaction you you suppose we want to think deeper into the data and asking which items sold well too we born to answer the question which items sold well together this question comes under market basket data in the market basket data analysis our task is to find which items sold well the market basket data analysis would enable you to bundle group of items together as a strategy for maximizing sales the market basket data analysis focuses on the items which are sold very to burn the well together buried together sold items are put as a green so the customer can buy these things is and the sales increases so do some strategy for maximizing sales by putting together the items sold then for example given the knowledge touch printers are commonly purchased together be it computers so you can offer an expensive model of printers at a discount to customers buying selected computers in the hopes of selling more of the expensive printers we have the knowledge that printers are commonly purchased together with computers so we can do or give and offer to the expensive printers when the customers buying celica convenience so the sending of expensive printers becomes more a regular data retrieval is not able to answer these Koreans so the data mining system for transaction data can do these tasks by identifying frequent itemsets that is free combined I frequent itemsets needs set of items that are frequently sold to them so the transactional databases the analysis of transaction databases that the databases comes mainly under the market basket data analysis in market basket analysis 80s finding the items salt well together this is achieved by finding the frequent itemsets from the transactional data so what what is frequent items it frequent item said is a set of items that are frequently solved to the data mining functionality known as Association rule mining he is mainly applied in this transactional database so Association mining is performed in the transactional data we study how Association mining is performed in the transaction databases in module five okay 
5sZdNzB3-Sg,27,"🔥 Want to enroll for Artificial Intelligence classes ?
Join #COMBINECS​ specially designed Complete Courses and taught right strategies to Crack NET/JRF exams with great weightage.
Why #COMBINECS ​?
👉 FREE LIVE CLASSES on youTube
👉 ONE LINER NOTES
👉 Study materials
We believe in to learn everything completely and Revise them quickly with ONE LINER NOTES.
We provide VIDEOS with PDF notes and subject wise Quick Revision notes.
Interested candidates WhatsApp our team at 7666980624
====================================
🎯 ▶️ Useful Playlist links:-
🚀Artificial Intelligence (FULL Course) : 
https://www.youtube.com/playlist?list=PLPO6c6IQvsr-vJy28wojv44qMYLIa2QwR
👉 Artificial Intelligence (ALL PYQs + MCQ) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8eINJDDQ5kGaD-vGiE9gAo
👉 Software Engineering (Concept with PYQs) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr_m5yEebkGdO-Hd_1V5AR11 
👉 TOC (Theory of Computation) | Rapid Revision 
https://www.youtube.com/playlist?list=PLPO6c6IQvsr__WzS6Is4rsKdCas85a5uc 
👉 COMPILER DESIGN
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8I6M6Yiylx6HMueGmMHW_i
👉 Discrete Mathematics (SET/RELATION/FUNCTION)| Rapid Revision :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr9hEZr1bZBYX5xS-fsPh-xT
👉 Discrete Mathematics (POSET / LATTICE) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr-n4beJarvO08Ypod28HoJe
👉 COA :
https://www.youtube.com/watch?v=qc3Naf96XOE&list=PLPO6c6IQvsr8Cz5KoOSHodN8FSGdj5JGF
👉 Object Oriented Programming (All PYQs with Concepts) 
https://www.youtube.com/playlist?list=PLPO6c6IQvsr-btJXBJuldHGDhyXI3ADxq
👉 COMPUTER GRAPHICS (PYQs with Concepts)
https://www.youtube.com/playlist?list=PLPO6c6IQvsr_vtRObYTWdukfxosBJDic4
👉 DBMS
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8AzAMg1PBItXD0b9T-9OI0
🔥 ▶️ Burning topics
✍️ Crash Course Computer Science (UGCNET/NVS/KVS/DSSSB/GATE) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr-EHMkNsA1i7HaBjhei2jPG
👉 Expected MOCK TEST NTA NET/JRF :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8aVT7Lf5QFWZVa45bHXuPn
BIG DATA | CLOUD COMPUTING | IOT with Expected MCQ:
https://www.youtube.com/playlist?list=PLPO6c6IQvsr_6VLupwNxw8woU9NMY4fHU
👉 LAST MINUTE PREPARATION | Tips & Tricks (CS Imp. topics) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8_ovRU7oOlj2D4MZjfLpno
🔥 ▶️ Past Paper Solutions
👉 NTA UGC NET/JRF CSA Nov 2020 (Past paper Solutions)
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8JXxsAD54W2y7e9AqMfLeo
👉 NTA UGC NET/JRF CSA DEC 2019 (Past Paper Solutions) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr9d80GOfi0jaxC1q525WAoW
👉 Gate PYQ : 
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8IGQju_L5ufdsPgp4M8WPP
👉 C Programming (Basic to Advance)
https://www.youtube.com/playlist?list=PLPO6c6IQvsr8OvwMEvOmwqAe6jr026XtT 
👉 OPERATING  SYSTEM
https://www.youtube.com/playlist?list=PLPO6c6IQvsr-BqRE7rqr6DzbO3ZMSKZik
👉 Computer Network (Security In Computing-SIC University students can also watch) : 
https://www.youtube.com/playlist?list=PLPO6c6IQvsr9jcw1-4MNHTCLi5gTYlz4Q 
👉 On Learners Demand (Important CS Topic) :
https://www.youtube.com/playlist?list=PLPO6c6IQvsr9mAWLAQiiIpAMqQoUdTV_B
👉 NTA NET/JRF | Paper 2 | Syllabus Discussions:
https://www.youtube.com/playlist?list=PLPO6c6IQvsr82p6TSHBBpgxHXqUN03A8H
===================================================
👉 Follow us on Social media:
▶ YouTube                  : https://www.youtube.com/c/CombineCSTheExtraStep
👥 Facebook              : https://www.facebook.com/RashmiPrabhaCCS/ 
📸 Instagram             : https://www.instagram.com/combinecs/
 Telegram Group     : https://t.me/RashmiCCS
 Telegram Channel  : https://t.me/combinecs 
👉 Join our whatsapp group for :
 (NET/SET/GATE)    : https://chat.whatsapp.com/GruovhRvste1nL8L2X1YQ3 
 (SCHOOL/JOB Notifications)  : https://chat.whatsapp.com/ExM4CZ2ZKxzEgPvSfOXNFb
For any query regarding notes, pdf, feedback, suggestions etc…..
 combinecs2020@gmail.com 
🌟 For all our latest courses launched visit:
 🌍 combinecs.com
===========================================
“Effort Never Dies”
👍 Like || Share || Comment || Subscribe 
#DataScience #MachineLearning 
#datamining #fuzzylogic #artificialIntelligenceLectures #ugcnet #gate2022 #assignment2 #combinecs #cs #paper2computerscience
#paper2ugcnet #dbms #coa #c #oops #computergraphics #digital #graphtheory #LPP #OS #artificialintelligencelectures,AI lectures, ugcnet artificial intelligence full course, crash course on AI, marathon class AI, quick revision Artifical intelligence,
 data mining mcq questions,
 data mining mcq questions pdf,
 data mining and data warehousing mcq questions,
data mining and data warehousing,
data mining ugcnet previous year questions,
Data Science explained with example,
How data science is related to Data Mining,
Supervised learning,
Unsupervised learning,
Reinforcement learning
Learning algorithm in Machine Learning,
Clustering,
Classification,
Steps of Data mining,,
KDD steps,
Knowledge Discovery Database",2021-05-10T05:30:00Z,Knowledge Discovery Database | Key Steps of DATA MINING | DATA SCIENCE,https://i.ytimg.com/vi/5sZdNzB3-Sg/hqdefault.jpg,CombineCS The Extra Step,PT2M,false,5,1,0,0,2,let's talk about kdd process it is very important process in data mining so sometimes inducing it they have asked what is the full form of kdt it stands for knowledge discovery database so basically it has five different steps and they can ask you the question based on this step so first step is selection selection means you have a data warehouse where you have kept so many of data and on the basis of end user requirement you are just selecting the data then this process is known as selection of data now after the selection of data you are going to pre-process that data you should start the extracting meaningful information so this is the part step two so this is what target data you are targeting certain field and then the basis of that field or that requirement you are pre-processing that data after pre-processing the data it you will transform that data so that's why in data mining etl is a very important concept like extraction transformation and loading you are just pre-processing the data you are doing the analyticals you are generating the report you are analyzing the raw effects and after analyzing the raw effects you are doing the analysis you are transforming the data now after transforming the data in the form of pie chart in the form of histogram in the format of different charts you can design certain patterns you should learn how you are going to use that data so now this is known as data mining and finally on the basis of your knowledge you will evaluate whatever the information you have so that it might be useful to a particular application so these are the five important steps involving kdd and which is a part of data mining so thank you so much 
Niji4qpyJME,27,"#DATAMINING
DATA MINING PHD LECTURE#1",2020-11-10T09:53:43Z,DATA MINING PHD LECTURE#1,https://i.ytimg.com/vi/Niji4qpyJME/hqdefault.jpg,Dr. Dibya Jyoti Bora,PT1H3M8S,false,58,2,0,0,0,right so starting from zoika please give your introduction good morning sir uh so uh i think uh um so uh i did my uh schooling from saint joseph school sonari and then after that i did my college from ellit academy and uh last uh and this year already i have completed my master's from kazanga okay university then you may give your introduction good afternoon sir um currently i'm working as a postgraduate teacher uh for the subject informatics practices in army public school gerard so i did my master's from the sam kasiranga university and bachelor is also from the same university uh each is from uh assam raphael's public school and uh high schooling from st mary's high school so this is all about it and actually i'm completely fascinated towards teaching so that's why i'm currently into this before that i was also working as a software developer in one of the companies iit companies so right now i'm into carson university once again and ready for phd from the assam thousand university that's it good uh my name is good morning yeah so good morning and good good morning to everyone my name is i'm actually working as an assistant professor at leading in college long from past 10 years and i've completed my mother i come so so i'm here and applied for the phd and and looking forward to do something and data mining thank you okay so thank you all of you and the next thing is that while going for uh phd pressures then first we need to identify what is our research interest i hope up to now all of you have made the decision that what is your area of interest right in phd basically we deal with particular specialization we try to explore more and more about it and try to bring some innovative ideas to tackle some problems related to that i hope you all know that so what's your idea what's your area of interest zika have you found something that you can say that that is your area face recognition yeah that falls under computer vision okay whenever uh we go for the face recognition so that is probably in the computer vision where we will apply some digital image processing techniques right so your area will be computer vision under ai and uh manual hog have you recognized some such area of interest for you yes actually my broad area i'm not just looking for data mining and uh uh the area which i'm looking exactly uh to work on is sentiment analysis so uh where yes the machine learning and npl will be used extensively okay good then we have prabhupad have you made such decisions about your idea of interest yes sir kind of uh actually i am interested towards since i have done my master's project on iot data as well as i have been working for iot for the past couple of uh two years so i'm interested in capturing those data that are being recorded from iot devices and analyze those data and find the hidden patterns from those data that has been captured from the iot devices okay so basically your area is under iot right and if you go for the analytics portion that also belongs or a part of the data analytics yes sir okay the reason i have asked this idea of interest because uh this this is your elective subject and another option is also there i think uh cloud computing right so data mining is somehow related to each of your research work now in this course of data mining we just don't go like classroom-based uh means lectures here might approximately what i'll deal with some topics and after that because you have seen that this is of two hours long plus so after that i will give you some assignments right so you have to go through assignments of course that will help you no doubt okay and you need to uh just send me the completed assignments to my mail id or whenever you will get the ku email ids one google classroom will be uh assigned to you okay there are post such assignments right now right now i have created one group i think all of you being assam ninja already there in the group phd data mining yes yesterday this i have created that group hello sorry this is not been added to that group yes sir so i have not been added to that group okay uh so do one thing uh you give your number that is in the what is the word strip number okay so that i can add into that group and so where should i give you the number that's you uh just a minute seven double zero five six five seven seven five seven seven two four two four seven double zero five six five w yes sir okay uh i have send it number to the group uh juhika and prabhat you are i think already there in the group please add this number to the group yes okay okay so let's come back to the course uh curriculum and first i am going to give some introduction on the about the data and sequentially we go for all the means modules that first then we come to the data modeling techniques that we come to the data pre-processing techniques you will have supervised learning techniques then we'll have unsupervised learning techniques that will be your whole syllabus and designed by myself and then next thing in every lecture you will get some assignment so i suggest that i'd like to suggest that you all go through the assignments and you will need to post by the end of the lecture means today's like 10 11 30 to i think uh 120 we have the class figures right so after this discussion of few topics i will give some assignment that you can complete within half an hour and after that you'll send me the assignment i'll go through it right is it clear to everyone yes sir yes sir okay and let's start i'm starting the scene i hope you can see my skin now is it visible yes sir so as i have said i'm going to discuss few things about the data so to go into the uh details of it first we need to discuss this this part that is the tower house architecture and characteristics right so you all know that that has continued exponential growth for the something of a paradox the more data we have the greater our chances for conversion but due to its volume increased data becomes more problematic analysis so what's actually going on you all know that data is increasing day by day you may consider either any big vendors like facebook whatsapp everywhere you will see that the data is growing in an exponential manner right so what would happen when it goes like that then our normal traditional techniques with the help of that we can't analyze those data right so we have to organize it in such a way we have to model in such a way so that we can find in that inside of those data so that we'll analyze it in terms of business analytics point of view okay like you know suppose if you are considering uh some facebook data you need to find a particular person profile out of those millions profiles millions and billions of profile then how that searching can be conducted within say microseconds so you all know do you have any idea what type of charging techniques facebook uses is so in facebook we use some graph based searching techniques we'll go into the details of each folder but why i'm citing here that with the help of those graphics charging techniques within microsecond we may find out that specific profile that we are looking at and how it becomes possible because the data is being stored it's been organized in such a way that you can efficiently conduct those future searches out of that data okay you can out of the data you will find that specific means uh data that you are looking for so one such technique is data warehouse again data warehouse be a yeah you can say that the way you are going to organize the data in just a way that from those data stored in the tower house you can easily make some decisions regarding some business point of view so whatever companies suppose you know ibm you know say that's just like i have given the examples of facebook that our house used from the business point of view his transfer business profit either gain or loss we can analyze those facts so you can say that the tower house is the key to solving this paradox the paradox is what the exponential growth of the data that is raising a very big problem okay so for effective analysis we have to we have to actually use this organization organization means data warehouse techniques so how you will define the tower house have you heard about warehouse before yes sir what is that where the consignments uh get ready or are being stored before sending like flip cards so now they all use us all these online shopping vendors they they use these specs but when we talk about warehouse in our common sense in our day-to-day lives uh means uh then what does it mean by this time the from day-to-day sense of yeah warehouse used why we use this storage place so it says it is a common center where a huge volume of commodities can be placed together so that accessing will become easier yeah accessing will be easier and from that accessing uh means easier access of uh those particular objects that you are storing in that warehouse okay that of course no doubt going to help your business accessibility right so similarly in the case of the tower house also we store those data in just a way that we can easily access those data then we will perform some analysis on that right so if we have been asked about the definition of data warehouse and how we will define we'll define this as repositories i hope you all heard about this term repository right so data warehouses are repositories of high volume information next thing that they are centralized stores of all data a company may generate formed by relational databases and design for query and analysis beta warehouse allow for quick accurate access to structured data via predefined queries now you do remember that i hope you all know about dbms database dbms database right yes yes what we actually do in that uh in those with the help of those dbms basically datas are stored on it and then some kind of relations are set up so that the accessing of the data will become easier right here also i have defined that our house has some repositories and some store where we can store the data for easy access for some analysis then is there any difference between these two dump data warehouse and dbms are you getting my question what i have yes yes uh i think that is a difference it is a huge differences there so it is not mandatory that in warehouse we have to keep the data in a structured way and there that certain rules has to be followed while virusing the data like that we have to follow if we compare it with a dbms office a certain amount of relationship needs to be established between uh data that has been between data that has been stored in rows and columns format right very good okay so there do exist differences between data warehouse and dbms few of them you have already cited and the others i will discuss okay for that you need to go through some technical terms that i'll discuss so i hope that definition is clear that the tower house i'm defining it as a repository of high volume information we're storing the data for some query and analysis that will perform in some future period of time right now utilizing data warehouse makes it simple to generate reports run ad hoc queries and extract near limitless streams of data that can be converted into meaningful business data this is why this these are the reasons why we use that our house first thing that to generate some reports what our first report will see ad hoc queries you all know what does it mean by query with the help of query actually we are requesting some information from the database now we are talking about data warehouse okay so then some data we need we need to extract some data from the data our house so with the help of this information some business analytics will be done so that will have some meaningful business that okay business area that's why we will say that our house is nothing but a tool we use in the case of business analytics and business analytics means to find out the trend how the business has been growing either in positive side or in the negative side and the areas where particularly that positive or negative effect being localized right so i hope this definition is clear to everyone right so now i come to the most popular definition of the tower house from a person who can be considered as a father of this that our house definitions site right is william one and he defined the tower house as a subject-oriented integrated time variant and non-volatile collection of data in support of management's decision making process here's a definition here you all have to remember this is the real or you can say the tactical definition of that our and this definition actually that our house definition is being given by uh many that means computer scientists but the one from bill in mon is being considered by default as a definition of that data warehouse ticket so your default definition from the bill in mon says that a tower house is a subject oriented one technical term then integrated another one time variant and non-volatile collection of data in support of management decision making process ability subject oriented integrated time variant non-valid the areas where we may see some positive or negative growth right positive or negative decision making right a decision making process of course lower level of administration this is actually conducted a per level administration of that company so actually sometimes this question is also asked in some competitive examination that data warehouse or decision making processes level it is not at the lower level of administration it is actually at the upper level of administration like managers okay so why they do that to perform some decision making okay high years of product a particular product this particular period may suppose 2019 say 20. profits that is being also found out with the help of the data analysis from the data warehouse but you have seen that why data warehouse is considered as a tool for management decision making process right okay so as i have told you that he he is the person we can consider as the father of data warehouse he's the one who has given this definition of our house okay so you have to actually remember his name william as the shortly okay and he is american computer scientist now thus i told you few practical terms subject oriented integrated time variant non-volatile is our technical thousand men so let's see these technical terms in details and that will actually give you an idea about the characteristics of data warehouse is clear to all of you up to now right yes yes sir so let's come to the data warehouse characteristics and do remember that this that question that i have asked you that what are the differences that we have in between dbms and data warehouse differences in this characteristics are the differences between data warehouse and dbms because this characteristics will be possessed only by the data warehouse not by the data but that means the properties what we are going to discuss that our housekeeping legal but that dbms are not going to satisfy these things and at that particular point we can say that the tower house and dbms are different form is other right that was but when we define them when we answer them technically that we can mention these terms okay so let's discuss them the first one is time variant so the tower house is a time variant database it supports the business management in analyzing the business and comparing the business with different time periods like you know year quarter month week and day okay actually analyzing those particular data with respect to some particular time periods 2019 to 20 20 to 21 and since then i can say that from january to say march ma april to say june like that are you getting me there or specifically if i am talking about the moon talking about the moon then we can specify that either january february march like that or sometimes we can we may also specifically uh stick to some particular date it's a big data warehouse then obviously some time period will be associated is this clear to have here yes sir yes yes sir okay so let's see that how they are related see uh this you can see that year quarter month week i hope that you all know what uh yeah quarter month yes right now whatever we are studying now these all are practical aspects of practical aspects for srs is practically social data mining why why i am discussing data because you have to have idea about what are data modeling techniques what is actually data what is this time variant okay these facts you have to know then only you'll understand the techniques of data mining okay let me tell you one thing that you know we have some data mining technique um books which book you are going to refer for these subjects can anybody have any idea about the book that you are going to refer you have any idea that what book you are going to respond so far no idea because there are lots of books are available and many uh foreign authors books are there so it's a bit confusing which book to go okay okay don't worry i'll give you the title that you have to actually just for this particular course but also that will help in your research okay if your research is under this domain okay so all three of you note down one book data mining by hannah kember okay so when you'll see the contents of that book data mining by hand and camber you'll see that he or they also start with this data portion and the data warehouse filling portion rather than going into the techniques because the idea is that you have to know isn't every fact related to the data then also then only you go into the uh different techniques materials research massive tools don't just depend on the two tools we use to implement some facts okay and to in tools you will find that already many techniques being already there in built but when you do you want to do or you want to conduct some good research actually you have to analyze the algorithms so while only analyzing the steps of an algorithm then only you get to know that at which portion this algorithm is actually lagging behind and you may target that portion specifically and then only you can bring some ideas to push the performance of that particular algorithm right so so this is the case when you are working with some existing algorithm and in next case when you are proposing your own algorithm then you know that how actually how you have to implement that to implement that you should have better idea about how the steps going to work okay so that's my suggestion to all of you don't just depend on the tools yeah tools you have to use canadian but before that whatever the algorithms you are going to employ you need to conduct a proper study on that right this is the very beginning of your phd career psd period okay so that's why i have told these facts to all of you so that our housing here this actually sticks to time period and time period means you have seen that these are the products and the quarter one quarter two quarter tea that is a revenue generated with respect to each product so you will see that this product x2 revenue is high in the quarter q2 right so is it our housekeeper say with respect to some time periods we will come to some decision that highest extra product is quarter two minutes yes [Music] also who will release more number of x2 products in that particular quarter to that particular location where the growth being observed right so i hope this time variant property is clear to everyone yes or no yes sir okay so sometimes i need responses because you know this is the online platform i would doubt that i how could i know that i'm talking to someone human being or not right here so yes definitely so i can understand sir thank you thank you okay next technique that uh these facts that you need to have all the time idea is that the day name day number and week day number and month day number in here like this this thing these are the attributes of time okay so next technique or i can say the next property i have to discuss regarding this the tower house is the non-volatile do you have any idea about this town what does it mean data is permanent data is what about youtube are you agree with that or not uh yes it's a non-volatile means um it is only read-only you cannot alter the data very good okay so actually that's that's the meaning of nonviolent so how this term is related or associated to this data warehouse is that whenever you put some data in the data warehouse that becomes non-volatile but means what that is kind of permanent right so you are not going to arrest that so that's why this is you know in dbms at this point also in dbms we used to delete the data we used to edit okay but this these things we are not going to do in the case of that our warehouse where this dvms and data warehouse departs for me right also in the case of time variant okay dbms is not stick to some time period but the data warehouse is take to some time periods okay in time period in nasa similarly this non-volatile property also this is in case of death our house only so non-volatile means that previous data is not erased when new data is and cut in it again so we mean say read only and again we are using the term period wise okay this also helps to analyze the historical data and understand what and when happen it does not require transaction process recovery and concurrency control mechanism i hope you all are aware of the dbms techniques dbms mayhem the transaction curtail we have to employ to control that concurrency otherwise you know that our consistency that may be violated respects you know or not there we go idea hogan said is transaction data yes or not yes but yes concurrency control mechanism because what happens we are only periodically replacing the data our data is read-only mode so we are not going to erase or edit those data so they that that's why they are not going to need this type of mechanism now as i have said that this is read only so we have to know that what type of operations or i can say the data operations we used to perform in the data warehouse again operation sum so activities like delete update insert user perform on an operational application environments are omitted we have dml we have similarly we have this but yeah so here in case of data warehouse we are actually omitting these operations okay operation omit yes we are not performing these operations right so what type of operations actually performing that awareness this question may be asked in competitive examination okay it starts already so you have to know that we actually perform two types of data operations their data loading and data access right data loading and data access are the two types of operations that we perform in the tower sir house sorry to interrupt uh survivor yeah sir if uh like insert is not uh if insert is omitted then how data loading is done in uh data warehousing data loading that is actually that is known as data loading okay insert means what you have created some database and in short operations that you are performing stick to that but in this case actually cubes you will get to know about this in later when i will discuss the pre-processing part and then how it was being inserted we insert this data don't confuse it here whatever the oppression is oh i'm talking [Music] okay cookie so that's why we cannot say that we are inserting data into the data because some architectures are there and in those architecture some steps are there that you have to properly follow for that insertion process so near calling it in short operation you'll be violating the rules of data warehouse that's why we are calling it data loading i hope this point is clear to everyone right yes yes sir okay yes sir so uh next thing why the data is statics in data warehouse your question i'm asking it to know because whatever the topics that i have discussed now with the help of that you can answer this question that is that our house may deter why it is static what will be your point of view regarding that so because it is only read only that's right it's static okay robert has given his opinion right uh what about the others so basically uh in warehouses actually we are just only for loading the data and just to access it access the data so here since the datas are already as proper say read only data therefore the what according to you why data is 30. just you don't need to worry about you just give your opinion here we are actually discussing because now you are scholars you are not students do remember it you are research scholars right so i have discussed girls how resourcement was discussed curtin write that in that way we are actually discussing so you can give your own opinion regarding that because the data store here is like a permanent and it's it doesn't change right so you all know that the answer very good this means that whatever i have discussed that is clear to you so data is static because you already got the term non-volatile right means once the data is entered it does not reflect the change which takes place at operational database so that's why we call it aesthetic traditional database we have a lot of techniques to change it right so some someone's our own designs techniques some are some even restrictions that is not in the case of that our house so that's why we call that data in a data warehouse is okay some facts that it generates artificial case or surrogate skates to store the history a surrogate key generated seriousness of numbers it requires more disk space forget about this tax right now but that answer is very clear to you that we are not going to change the data that we have placed in the tower house so this this means that this is not volatile non-volatile so that means that it's not going to be erased so that's why it's a kind of static in cancer right the next property i'm going to discuss is that integrated again so up to now we have discussed two properties one is time variant and another is non-volatile the third property is integrated so what actually happening here is that we are collecting the data from different sources right may insert keys now we are collecting the data from different choices then we'll integrate data okay so that we can place them at one place that's what's the term integrated meaning okay integrated means means what we are collecting the data from different sources so it's commercial so like your operational systems your own systems like uh suppose some flat files okay so your data metadata so that's why we are actually integrating data from many different sources so we actually not depend only single source of data we are actually depending on multiple sources of data when we have completed elements collected those data we then go for integrating those data and that takes the form of metadata in data warehouse right so a data warehouse is built by integrating data from various sources of data okay in addition it must have thank you in addition it must have some reliable naming convention format and codes indication of that our house benefits is effective analysis of data so you just tell me whether this term integrated is clear to you or not you have integrated term that our house may use okay so at this pack also at this point also you will see the difference between dbms and data warehouse dbms may have calculated you just create the database right if it is rdbms you will create somewhat tables right relations yes you are collecting these things and you are storing it you are actually creating some database you have then after that you uh insert some data into that those tables or relations you have created no this is not happening in the case of data warehouse in the data our house we are actually integrating already some data are there in different sources and we are just collecting from data from those different sources and then we are integrating this also answers the question being asked by foreign i hope that this is clear to you right yes sir next property i would like to discuss that is subject oriented i have discussed first one time variant second one non-volatile third one integrated and the port one subject oriented with respect to some subjects from the term itself it is very clear that the data we are going to store we are going to organize in a data warehouse that is a kind of what pattern following the subjects so what are those subjects a kb company declare you can consider any you may consider any company you will see that different particles are there right or wrong like sales marketing distribution yes yes yes yes so this actually follow these subjects means from the sales will collect some data right from the marketing we are collecting some data from the distribution distribution function actually distance to some locations also we are collecting those data so we are going to organize the data in a data house with respect to those specific subjects but can you tell me what type of benefit we'll get by organizing this data with respect to those subject or subject-specific organization are you getting my question yes yes sir accession of data will become faster because uh while accessing you know where you have to go and what kind of data you are doing right okay and next uh anyone else wants to try so informed decisions can be made analysis can be made easy the process of analysis can be made easy very good right so the subsequently what is you you'll get that when in your own pc when you create some specific folders and you store specific files in those specific folders and you give some time names to that yeah this this is the folder for sean this is the folder for research this is the polar post suppose work like that so upgrades up direct work right whenever you need some files from specific to work then you directly go to that workflow right so suppose you have created one folder you are putting all the files either songs your word means research works or your the working some files you are putting all these things into one holder is this going to help you or the horse is going to help you the first one of course the first one because see you in one folder you are putting all the files so it takes time to find out your specific price but now you are organizing specific subject ways so in that way that our house also when we organize the data with respect to some subjects what would happen we can make specific decision making with respect to that specific subject yeah salesman in sales this type of trends we can observe so with that help of those particular specific information we can make the decision making process easier and efficient so that's why data warehouse follows the stand subject oriented right so is this point clear to everyone subject oriented yes sir yes zika are you getting this clear or not yes sir so now as i have said that in every lecture we go to in that way means first portion of this lecture i'll discuss some facts and next question i'll give some assignment you don't need to worry about the states of the assignment this this will help you no doubt so today also i'll give you a question in the group where this you will try you all try to solve or means write answers to that question and submit it by 1 30 is it okay to everyone okay okay so that's it you'll get the question in the group thank you we'll meet in the day okay presentation also thank you sir thank you thank you uh actually i'm a bit confused from where to start and what to do in phd like i'm getting lots of subjects in phd so uh like in the first semester what we need to do here [Music] actually just you you have done mscm in those particular courses you have been given a lot of subjects and you have to give focus on all the subjects but not to an accident that is required in phd in phd you have to target one subject but you need to go inside of it in depth of it okay to a extent that you know that that is not that actually not has a limit um so i have no idea exactly exactly sir yeah people around yeah it's not official okay yeah what i we did in our phd time guide okay you have to follow the guidance property or second yeah yeah this is the site now you all know the specialization of the faculties i would like to give my suggestions on that put my suggestion on that okay so what will it happen in ps3 all of you do remember computer science every machine learning ticket machine learning maybe update supervised techniques and supervised techniques in psd it is not possible that you will go through all the unsupervised techniques or all the supervised techniques is not possible you have to properly conduct a review out of that what would happen you need to find out those techniques where actually your interest lies and then you need to find out the means disadvantage or you can say that the problems associated with those techniques so see s [Music] chosen i have gone into inside of it like machine learning and supervised techniques and in the computer vision so similarly a particular area yes phd is a kind of training it's not a like of the other courses that you have to appear so like no no this is not happening in phd the kind of training that you have to train yourself [Music] yeah i think it is not it may not be a suggestion from the session but measurable yes this is not phd these are some subjects simple subjects that last 12 plus students can also learn research methodologies research foreign because this is met mandatory by ucc that you have to go to this coursework part right yes sir i'm asking that at the end of the semester how the evaluation would be done like we would be at in uh we would be given to like presentation at the end don't worry yes presentation again that is a different thing that you see curriculum guidelines [Music] technical yes don't worry see in phd one thing is very necessary that is welcome so i hope i can end this lecture now and i will give you an assignment please try to submit it to my email id uh that is the visitive or direct kazura university in okay thank you yes sir thank you thank you sir 
nqOFS4W6d_c,28,"It is the discovery of interesting patterns from multimedia databases that store and manage large collections of multimedia objects.
It consists of
Similarity Search in Multimedia Data
Multidimensional Analysis of Multimedia Data
Classification and Prediction Analysis of Multimedia Data
Mining Associations in Multimedia Data.
a) Similarity Search in Multimedia Data
Based on data description / content
CBIR ( Content Based Information Retrieval )  
QBIC- Query by image content (software for handling both description and content based)",2020-11-15T01:43:21Z,20 DWDM -- Multimedia data mining,https://i.ytimg.com/vi/nqOFS4W6d_c/hqdefault.jpg,"UMAMAHESWARARAO BATTA, Ph. D",PT13M6S,false,520,12,3,0,0,good morning in this session we will talk about the multimedia data mining previously discussed about the spatial data mining and then a sequence data mining now you are talking about the multimedia data mining what is the multimedia data mining what are the approaches for performing the multi media data mining here multimedia means video files audio files some kind of files here the multimedia data mining here it is a discovery or finding the interesting pattern that is not all information interesting pattern from the multimedia databases here the different multimedia databases in the sense it comes to the different applications in the mobile phones you are using the multimedia files and then digital camera that means storing the pictures and then video related data and then internet also uses a multimedia data that means the identification of interesting patterns from the multimedia databases and store and manage the large collection of multimedia databases is called here multimedia data mining identifying the multimedia databases from the internet are that means identifying that means discovering discovering the multimedia knowledge from the mobile phones discovering the multimedia knowledge from the digital cameras is called here multimedia data mining here for example nasa also uses a this multimedia data mining for the at the observation system and then coming to the some of the challenges are there here what are the challenges here here we have a the similarity processing the similarity processing and then redundancy removal that means is there any duplication of data remove the duplication of data and then compression comprehension that means indexing these are all are the some of the challenges for the multimedia data mining here the next important blog what are the different types of multimedia that means are you data mining that means you will perform in the audio data mining that means from the audio databases retrieving the audio related uh interesting patterns are your related interesting patterns let's go here are your data mining from the internet um and then image data mining that means getting the from the internet in the sense it's having the huge amount of data from the huge amount of images identifying under discovering the interesting images pick up the dusting images and then video data animation data graphics data sequence data we previously discussed about the sql data and then hyper text links hypertext data consist of some text that means uh some text markups and then linkages these are all connected as a multimedia data and then it consists of similarities in a multimedia data and then multi-dimensional analysis on multimedia data the classification and prediction of prediction analysis of multimedia data the classification techniques you will identifying the decision tree new newbies classification and then comes comes to the prediction techniques linear regression logistic regression and that means cloud the logistic regulation comes under the classification technique and then different types of regression techniques on the multimedia data finally mining the association in a multimedia data coming to the first one similar research in a multimedia data that means how can you identify the similarity here the similarity previously you align the clustering approach you will identify the similarity between the two different objects based on a euclidean distance here the similarity sets in a multimedia data based on the data description or content based on the content here you're performing the similarity here you have a content based information retrieval system and then query by image content and that means software for handling the both the description and content based that is coming here query based image content here the approaches here you are using the different approaches color histogram based signature here what is a color histogram based signature finds a similarity similar to color composition of the image that means you will identify the similarity between a different images the color composition and then multi feature composite signature we are considering the multi dimensions for the analysis and then wavelength based signature that means a single signature for the complete image you are considering and then wavelet based uh signature based on the region granularity considering the only region granularity of the image non-full image that means you are considering the part of the image here these are all different approaches for the multimedia the second important that you discuss with that one similarity sets in a multimedia coming to the multimedia analysis of multimedia data multi-dimensional analysis here what is a multi-dimensional that means a data queue is a multi-dimensional model data cube by using data queue that means previously discussed about the data key data cube consists of different tables the data multi-dimension stores the multi-dimensional data why because with the help of multi-dimensional data you will form the data warehouse here from the data warehouse you will get the some information your different dimensions are there and some of the dimensional tables are there some of the fact tables are there and then from the fact tables dimensional tables you will form the schema star schema snowflake schema that is a thing here this multi-dimensional analysis contains additional dimensions for the multimedia data here those may be color a texture shape those are all here which type of tools you are using here multimedia miner this is a one type of this is a database miner extension for the database miner tool that means multi-multimedia miner used to identifying the pattern knowledge from the multi different multimedia databases like the internet and then the procedure here that means future descriptor that means future future different futures are there the future identification is very important a field identification for the any from any database or any table the field identifying you know and then layout descriptor the image grid that means this is a future the future in the sense features are image image extraction uses image content information and then from there hierarchical keywords searching a dictionaries this is a user information that is a multimedia analysis and then come to the next important topic here is a classification and prediction that means the third important one the the first similarities that you completed multi-dimensional analysis of the multi multimedia data the class educational prediction analysis of the multimedia data here what is a classification how can you represent the classification in the sense grouping the data the grouping the data based on which the class labels here or the future set consists of class label based on the class label you will perform in the that means you will be identifying the the result based on the class label here that means in the classification you are getting that you are providing a training data set to the algorithm from that algorithm classification algorithm from their algorithm you will identify the model here you have to give the the test data to the this model it's get a output unpredictable value you will identify that in the classification the prediction the prediction is a that means for example for the classification you are using the different classification techniques what are the different classification techniques decision tree is one of the best important and another one is a the regression large logistic regression is a wonder one of the classification based on it logistic regression you will have the sr know zero are one and then one r two one not a one or zero like that binary values are there for the classified and then the application here or the this uh classification prediction application the scientific research that means in a geoscientific research in identifying the multimedia information you're using this type of multimedia and the goal what is the goal of this classification image analysis pattern recognition and then digital image content mining these are all some of the information some of the images from the images training data sets that means classify the model here this may be a star model the galaxy model snowflake model that means you are not using the snowflake model here star that means what is the star schema the star schema um identifying the what it identifying you have a fact table one fact table and then a number of dimensional table yes the fact table in the star schema factor will stores the some of the keys of the dimension table as well as the measures for the different the measures as well as the keys let's say the fact table is a central theme and then coming to the galaxy schema the galaxy schema consists of more than one fat table galaxies that's why it is called here galaxies more than one fat table and then a number of dimension table here and then there is a new image to test to find the bodies here this is a some and then properties here in the what are the properties area that means your identifying area for example in case of image intensity intensity what is the intensity of image these are all some of the attributes for the image movement magnitude and then magnitude and orientation of the this multimedia data properties and then mining the association in a multimedia data association means the association that's a relationship between two different attributes a [Music] you identifying the frequent item set from the frequent item set you will identify the association rules that means the main important thing here is a pattern a free frequent patterns identification is a very important in a data mining technique data mining process here the categories here association between a image content and no image content non image content and then association among the image contents that are not related to the special relationship association among the image content related to the spatial relationship this is a multimedia data categories and then for references see this one this is a some information regarding the multimedia 
kHbAvpC_Faw,27,,2017-03-27T05:13:56Z,Data Warehouse & mining 5 components of data warehouse,https://i.ytimg.com/vi/kHbAvpC_Faw/hqdefault.jpg,Sanjay Pathak,PT5M50S,false,16202,84,20,0,15,hi guys in this video I am going to talk about components of data warehouse mainly there are three components of data warehouse load managers their house manager and cumulative that will be discussed each topic in separate videos now there are some more other components of data warehouse that our data warehouse data obviously data warehouse store table data of an organization for particular earth period like for a period of 10 years and so so data warehouse also maintain its own database so first components of data where of this data warehouse database now then field data warehouse database maintains all the data that is need to kept in the database house second 14 cleanup and transformation tool data in the operational database are taken to the dot of a house daughter so this operational database need to help some cleanup and confirmation as every information in the the operational database is not useful so whichever such information we bought so the cleaning is done with help of some cleaning tools and that data is now transformed in the form of is transformed so that it can be used in the database house next opponent is metadata metadata like indexing it is all you can also call metadata is data of data so metadata is like ooh it is towards the index they saw it has the texture of month of January so what was the total number of tests in the January dented March so on so it is metadata is like our index XS pools so when the data is present in the dot of your house then we need different access tools so that we can analyze the data forward business so these Excel tools helpful the organization to know which type of data they want next component is data model data models like small databases or you can just supported some parts of totemism suppose in my data warehouse I have is told the database for the Year 2015 I have the sorted data for 2015 2015 and so showing data Mart you can see it is a support for data warehouses of complete collection of 10 years of data port of pyramids which table but data Mart is of component Lake 2016 database is called as can be called as a data Mart because it ends up dead component of data warehouse data so these components mainly perform this operation this other world operational databases suppose the company has cycle 6 of regular database so data posted means what are the fourth row from layers we are getting data for word database house full of processes exchange we take out the doorknob which we want 2nd stage stage is transformation into on foundation stage data staging cleaning ordering of Donna other suppose a company Java or in operational database company meetings every type of data they occur what was the total number of M lines that were present on particular month or on a particular vegetable or and phone so in transformation of cleaning cleaning is for openings which of which type of data that I want to keep in my database are checked out so this is the second stage third stage is dot load stage when you have selected this type of data I will restore in my database so now you will not put all the data directly you will put the data in this one small supports or dense tournament suppose after cleaning I will put it on top what's in the Year 2015 in at ahkamaat of 2016 and again before the year 2017 I will clean data and put the data in my 2017 dama so this is of presentation media when this data are present in data warehouse then there are data attitudes now there will be some different access tools because we are storing the data in the data warehouse so that we can analyze the data for analyzing the data we have different different ethnicities and different different Curie tools with the help of these tools we extract that information that we want so in the next videos I will discuss main components of that data warehouse that unload manator warehouse manager and co-manager one by one thank you 
4rymD1Hpnho,27,"Looking for a career upgrade & a better salary? We can help, Choose from our no 1 ranked top programmes. 25k+ career transitions with 400 + top corporate companies. Exclusive for working professionals: https://glacad.me/3bf1TEK

🔥 Get the course material and session PDF Here: https://glacad.me/3v1S6Kr 🔥

Mining of Data is extremely important to find useful information, which can help to boost a company's revenue, increase market segment, or even cure a disease. The session starts with a basic overview and the terminologies involved in data mining and then gradually moves on to cover topics such as classification and prediction, decision tree induction, cluster analysis, and how to mine the Web. In this live session, we will also be comprehensively covering all the other concepts of data mining and also see how to implement them using python. 

Visit Great Learning Academy, to get access to 80+ free courses with 1000+ hours of content on Data Science, Data Analytics, Artificial Intelligence, Big Data, Cloud, Management, Cybersecurity and many more. These are supplemented with free projects, assignments, datasets, quizzes. You can earn a certificate of completion at the end of the course for free. https://glacad.me/2PzkCTl

Get the free Great Learning App for a seamless experience, enrol for free courses and watch them offline by downloading them. https://glacad.me/3cSKlNl  

About Great Learning:
- Great Learning is an online and hybrid learning company that offers high-quality, impactful, and industry-relevant programs to working professionals like you. These programs help you master data-driven decision-making regardless of the sector or function you work in and accelerate your career in high growth areas like Data Science, Big Data Analytics, Machine Learning, Artificial Intelligence & more.",2020-07-28T07:38:29Z,Data Mining with Python | Data Mining For Beginners | What is Data Mining | Great Learning,https://i.ytimg.com/vi/4rymD1Hpnho/hqdefault.jpg,Great Learning,PT1H1M40S,false,22334,491,25,0,18,shhh hey folks good morning good afternoon and good evening to all of the attendees based on your time zones and then today's session will be comprehensively learning about the concepts of data mining and then we'll have a case study where we'll be implementing these data mining concepts with python now before i start off with the session just let me know if i'm audible and if you guys can see my screen and if everything is fine we can proceed with the session also in the meanwhile i'd like to state that we have launched a free learning platform called great learning academy where you'll have access to almost 100 courses with respect to domains such as data science artificial intelligence digital marketing cloud and a lot lot more all of these courses have been curated by industry experts who have their masters or phds from ivy league schools and once you complete these courses you will have a certificate which you can go ahead and add onto your linkedin profile or on your resume which will be a huge value add to you guys and also if you want to learn through an app you will we have the great learning app as well you can find the link for both great learning academy and the great learning app in the chat overshow and to folks who are new to our youtube channel i request you guys to hit the subscribe button and also click on the bell icon as it will encourage us to come up with more such live sessions and also it will notify you whenever we have the new live session scheduled and whenever we upload new videos and since everything is fine all of you have given me your confirmation let's proceed with the session so this will be the agenda for today we'll start by understanding what exactly is the need of data mining then we will look at the data mining lifecycle after that we will look at some of the machine learning algorithms and finally we'll have a case study with python now before we understand the concept of data mining i'd like to ask what else data could you let me know in the chat what comes to your mind when you hear the word date so data is just collection of facts isn't it so whenever whenever you look around you we are surrounded with data when you're working with numbers such as 23 8 000 minus 18.76 or 3.45 all of these numbers are just data when you say statements such as this is sparta i love pisa or joey doesn't share food all of these are again all of this again constitute data then data it's not necessary that the data has to be present in the form of numerical format or text format your data can also be present in the form of images and videos and especially in this time of social media boom if you go to facebook or if you go to instagram millions of people are uploading millions of pictures every single day and when we talk about video data data which is present in the video format so right now what you're consuming is video format data when you go to youtube or when you go to other streaming sites such as netflix hot star or amazon prime you are consuming video data so data is present everywhere and it is present in all formats but why am i telling you about all of these different types of data that is because the data which was present back around 20 to 30 eos is totally different from the data which is present now so the data which was present around two to three decades was small and structured now what do i mean that the data is small and structured so back then data was present in kilobytes not even megabytes so around 30 to 40 years back data was only present in kilobytes and if you guys remember we had something called as a floppy disk and as far as i remember the maximum size of the floppy disk was only 512 kb that was the maximum storage space of a floppy disk only 512 kb so you can understand that this is the this this this is the only uh short amount of data that you can store and also the data was very structured back then now what do i mean when i say data was structured it was mostly present in either numerical or textual format and also it was mostly present in a tabular format it was mostly tidy data and since data back then was very small and data and data back then was very structured you didn't have to perform any sort of analysis to find information so 20 to 30 years back there was death of data there was lack of data but right now in today's right now exactly when we talk about today there is no dirt of data data is being produced from multiple sources and we have huge amounts of data we have petabytes and extra bytes of data so when you compare this contrast from kilobytes to petabytes and exabytes this is an explosion of the data and right now this data is unstructured now what do i mean this data is unstructured this basically means that data is present in other formats except tabular data when you talk about images when you talk about videos when you even talk about xml data all of this is unstructured data and since we are sitting on top of all of this data we have to do something with it isn't it so now we have this data but if we don't do anything with it if we don't try to analyze to find out more information then we are just the biggest idiots in the world because this data can solve a lot of problems now if we properly use this data then and if we properly apply data mining concepts on this raw data which we have it can help to boost a company's revenue it can help a company to reach more target audience and right now if you are in this period of lockdown many data scientists and many machine learning engineers are actually trying their best to stop the spread of this virus called as covid19 so there are there are a lot of things with which you know there are a lot of things which can be done with the help of data mining and that is what gave birth to this field called as data mining so now that we know what is the need of data mining let's actually look at some data mining use cases so uh there's this very good example over here and this is from the banking industry so let's say you're just sitting at your home and let's say you're actually living in australia and uh one fine day you get a message from your bank stating that you have spent 10 000 australian dollars on buying a diamond necklace in the united states but the problem is you never been out of australia and you never spent more than 5000 australian dollars in your life so here your bank has immediately flagged this transaction as a as a fraudulent case so though someone from the bank some telecaller from the bank would have called you up immediately and asked you if this transaction was actually performed by you and how did the bank know that this might be fraudulent because the bank uses a lot of data mining techniques to understand how you do these transactions and when your bank keeps a track of your transactions it would understand that you stay in australia you never spent more than 5000 australian dollars ever in your life and you've never been to united states so that is why one fine day when there's a transaction from your account of buying a diamond necklace in the united states for the cost of 10 000 us dollars your bank immediately recognizes that this might be a fraudulent case and someone from the bank would call you and ask you if this is actually your transaction done by you so this is a very good uh use case of data mining then we have another use case where we have gmail so i believe all of you have gmail accounts and in your gmail accounts you'll have separate sections isn't it you'll have one section where you will have all of your regular emails then you'll have one tab for your promotions then you'll have another tab where uh then you'll have another tab where you'll be getting all of your spam mails now how does gmail tag it as a spam or a promotion or a normal genuine mail what is happening over here is gmail again is using data mining techniques at the back end now whenever you receive a mail so a mail consists of a subject line and also body and when you receive a mail these data mining techniques are used to understand the text so let's see if the subject line is congratulations you have won a lottery of fifty thousand dollars and there are ten exclamation marks followed by it so here gmail immediately has some sort of data buying techniques which would recognize that so let's say if there are if there are more than two exclamation marks in the subject line then it it would tag it as a spam or maybe let's say if put would uh you know if the subject line would consist of words such as jackpot congratulations you have won something or maybe prince from dubai wants you to invest in his invest in his company so if the subject lines are sort of like this then gmail would automatically tag it as a spam and in some cases we users also help gmail so gmail is also collecting data from us so let's see if i'm regularly getting a notification from uh from from some insurance to buy maybe a life insurance policy then what i can do is i can tag it as a spam and if 10 more people like me go ahead and tag life insurance as spam then the 11th person in the 11th persons folder if he gets a mail from lic it will automatically go into the spam section because 10 people have already tagged it as spam and this is how data mining technique is being used over here so we have looked at some basic use cases now i'll just go and take up some questions asking me to tell the difference between data mining and machine learning sure i'll cover that so we've just started off with data mining i'll also tell you what exactly is machine learning will definitely cover that so just wait for 10 more minutes we'll have you started over there is saying data can be all the real work we do daily absolutely ganeshwar so whatever we are producing whatever is out there on the social media or anywhere on your laptop right now what you have all of that as data right so i don't see a lot of questions over here so uh i'll take up the rest of the questions later if you have any questions put them up in the chat and i want this session to be as interactive as possible so if you attended my earlier sessions i make sure that i finally also take up your questions so keep your questions coming i'll take them up so radhakrishnan take up your questions about the difference between data mining and ml up in another five minutes because i also have ml the topic ml covered in this and after that if you don't understand i'll come back to this question data is asking what is kdd in data mining so kdd is basically this data mining life cycle so ktd the full form is knowledge discovery database so this knowledge discovery database is essentially data mining life cycle and we'll be covering that right now praveen is asking us data mining related to web scraping it's actually the opposite so you can consider data mining to be the superset and web scraping to be a subset of data mining so data mining is mining of any sort of information and when you talk about web scripting it is mining of information from websites so web scripting is a subset of data mining is asking is data mining and data manipulation the same so here again data manipulation is a subset of data mining so as you see this is the entire data mining life cycle and data manipulation would come under this particular part russian will be covering kdd so this what you see is also the kdd process this is the data mining life cycle amish is asking which is the best data mining tool so um i personally prefer r for a lot of data mining techniques so i'd recommend you guys uh to use r if you want to perform any sort of data mining right so i'll take up the rest of the questions later on and uh again if you haven't yet registered to great learning academy or if you don't know what exactly is great learning academy you can go ahead and check that out you will find the link of great learning academy over here right so now that we have understood the need of data mining let's go ahead and understand the data mining life cycle so here we would have to start off by acquiring the data then we'd have to pre-process the data after that we'll apply some machine learning algorithms and that will give us a result once we get a result we have to evaluate how is the result and finally we'll show out the result to our stakeholders or to our client now let's start off with the first part which is data acquisition as i've told you guys data is present in multiple formats and we've got huge amount of data and all of this data is coming from multiple sources so data can be present in the form of a pdf in the form of excel file in the form of a word doc in the form of an image or in the form of video now let's say if the problem statement is something like this you're working for a telecom company and most of your customers are churning out to a rival company and you'd want to understand why is that happening so this is the problem statement so what you'll do is you'll collect all of the data so there is this universal data related to all of your customers now all of that data could be present in a lot of formats you will collect that and you'll store it in one single place and you'll call that one single place to be a data warehouse now once you store all of that data you obviously can't perform data mining technique or data mining operation on top of this entire or the superset of data so what you'll do is you'll only take what is known as the target data from this universal data now this target data would depend on your problem statement so here my problem statement is i don't want to understand if my customer will stay with me or will he move out and if that is the case i'd want to take only that particular data which is relevant to this particular problem statement so let's say data such as uh whether my uh whether my customer is married or not or what is the age group of the customer or what is the gender of the customer these might be something which might actually affect if the customer stays with me or not so i'll only take relevant data and i'll call that relevant data as target data now once i've acquired the target data i can't again go ahead and directly build the data mining models on top of it why because the data which i have with me is extremely raw it is extremely untidy and that is why i'd have to pre-process my data and what is pre-processing pre-processing is just making sure that my raw data is converted into a tidy format it has correct information it has no outliers in it it has no incorrect information in it and for that uh once i make sure all of these basic stuff is done there are two more sub categories in data pre-processing which are data manipulation and data visualization so here data manipulation helps us to find some easy insights from the data so let's see if your manager one fine day comes to you and asks you to find out so let's say if your company has around 2000 employees and your manager comes to you and asks you to find out all the employees whose salary is greater than 100 000 dollars and whose age is greater than 30. so now you have this huge excel sheet of 2000 rows and maybe there are 100 odd columns so would you actually manually go through each record check the age of the employee also check the salary of the employee and verify if the age is greater than 30 and the salary is greater than 100 000 well that will actually be stupidity on your part and that would be a huge waste of your time now instead of investing all that time what you can do is you can apply some simple data manipulation operations and this can be done with languages such as our python and sql all you have to do is write in one line of command instead of manually going through the entire excel sheet the entire tabular data all you have to do is write in one line of code and you will get this insight and this is data manipulation then we have data visualization now similarly let's say if your manager just shows you this huge excel sheet which has maybe one million rows and thousand columns and asks you what are the insights from it obviously your mind will be blank because this is just raw data there's nothing in it it's just a tabular format with lot of text and lot of numbers in it maybe so how will you find out inside from it now instead of showing you the textual data what if i actually show you on a a geometry so what if i show you a bar plot or a pipe or a pie chart or maybe a histogram so if i actually show you a pie chart instead of the tabular data you'll be able to find insights more easily you'll be able to understand what is happening in this data in a much more better manner and this is uh this is where data visualization comes in and this part in our data scientists role or in a data analyst role this is the major task which will be performing so 50 percent of your time will be going into pre-processing because if if the data itself is wrong and you build your models on top of your incorrect data you will obviously get incorrect information so you'd have to make sure that the raw data is correct and only if your raw data is correct that is when you will get correct information and when you're starting with incorrect data there is a hundred percent probability that you will definitely get incorrect information so this is about data pre-processing now once we have acquired the data after that we have pre-processed the data and we have tidy data then we can finally go ahead and build our machine learning models on top of it so what are machine learning models these are just intelligent algorithms which would help us to find information from the raw data so we will go ahead and build these machine learning models on top of our raw data so we have different algorithms for different purposes so we can build classification algorithms regression algorithms or clustering algorithms depending on the problem statement and these algorithms will give us a result now once we get that result there is a good possibility that the result again is incorrect all the result is inaccurate so here we'd have to evaluate our result so this is where pattern evaluation comes in so this is known as a three number check so here we'd have to ensure that the uh information which we get after applying the ml model that information is correct that information is new and that information is also useful so if all of these three parameters are checked that is when we can go ahead and say that our information can be you know it up it is actually what we want now if we know that we've got the correct information from the machine learning model we can directly go ahead and show it to our stakeholders and this is where knowledge representation comes in so in knowledge representation we just show it out we just use a different visualization method so or different bi tools to show the result to the stakeholder or to the client so this is the entire data mining life cycle or the entire kdd process so i'll take up some questions now tamila nda is asking my questions pie chart bar and this kind of stuff are used for different scenarios right yes absolutely so all of these visualizations have different purposes so let's say if you are building a scatter plot then a scatter plot is used to understand the relationship between two numerical entities if you are building a bar plot a bar plot is used to understand the distribution of a categorical variable similarly if you're building a histogram a histogram is used to understand the distribution of one single numerical entity rahu was asking which etl tools are used mostly in the industry when it comes to etl it is again for all of this the entire data mining lifecycle right now in the industry the two most preferred languages are python and r so all of this etl operations can be done with python and r rohit is asking me to explain pattern evaluation once again sure i'll do that now when you build these ml models you will get a result so i hope this is clear till now we build these machine learning models we get a result and once we get the result that result can be inaccurate and most of the times it is inaccurate so here what we'll do is let's say if we build a classification model and if we build a decision tree model and we get maybe a 60 accuracy then what we'll do is we'll go back we'll make some modifications we'll tune some hyper parameters in this decision tree model then we'll build the decision tree model again then let's say from accuracy of 60 percent if the accuracy goes to 65 percent then this will mean that the the second model is better than the first model now we'll try again if the accuracy can be improved so instead of building a decision tree model i'll maybe go ahead and build a naive bayes classifier so this time if i use a naive bayes classifier i might get an accuracy of 70 and if i want to improve the accuracy again so this time instead of building a naive bayes classifier maybe i'll build a random forest classifier and with the random forest classifier i'll get an accuracy of 85 percent so this is an iterative process where you try to get the best information or the information which is most accurate and this process is pattern evaluation so i hope that answers your question nishant is asking almost every ml model as data pre-processing step so is data mining a part of ml algorithm it's the opposite nishanth so as you have seen over here im applying of these ml algorithms is a part of the data mining life cycle so also you can call the same thing as the data science life cycle as well so as you guys see the entire data mining life cycle comprises of these particular steps and applying of machine learning algorithms is one stage of your data mining life cycle joseph is saying eda's exploratory data analysis it is analyzing the data that we have mined ees that is absolutely correct so eda is basically this stage over here data preprocessing part where we apply data manipulation and data visualization operations on top of our data which we have mined ya 2021 is asking is eda part of data manipulation again it's the opposite data manipulation and data visualization constitute eda right so i'll take up the rest of the questions later on you can post your questions over here and uh guys if you haven't yet subscribe to our channel hit the subscribe button and also click on the bell icon it will encourage us to come up with more such live sessions on a regular basis and also if you have any more suggestions on what sort of topics we'd have to take live sessions on you can put them in the chat we will note it down and we'll have those live sessions arranged and also you can up share these live sessions with your peers or with your colleagues so that they can upskill themselves during this period of lockdown so we have looked at all of these now we'll uh look at our classic data mining task which is known as anomaly detection so anomaly detection helps us to identify unusual patterns or outliers in the data and outliers cannot be present in the data because it will skew your entire information and this is a very good example over here so let's say there's this particular pub or a bar where the average salary of all of the people who go to that pub or borrow us around 100 000 but one fine day jeff bezos decides to go to that particular pub so on that day the average salary of everyone who goes to that particular pub would shoot up from hundred thousand dollars to maybe one million dollars isn't it so here jeff bezos would be the outlier and he is skewing the right data so if jeff bezos would not have come to that pub the average salary of everyone who attend or go to that pub would be around hundred thousand dollars but just because jeff bezos showed up to that bar the average salary shot up to one million dollars instead of staying at just hundred thousand dollars and that is why outliers are are bad to your data so whenever there are outliers present and whenever there are extreme values present you have to make sure that either you remove them completely or maybe you try to reduce the effect of this outlier on your entire data then we have another interesting data mining task called as association rule mining and this method is used for finding interesting association among different entities and here there's this uh very uh good example of association rule mining which is known as the beer diaper syndrome so it's a very uh interesting name isn't it beer diaper syndrome so there's this uh research or a case study done by walmart around uh 15 years back where they wanted to understand the purchasing pattern of all of their customers so they were just monitoring uh you know what their customers bought so there they found this very uncanny relationship that if single that ever came to walmart to buy diaper for their kids that generally buy a can of beer along with this diaper this is very strange isn't it single dads coming to walmart to buy diaper for their kids also buy a can of beers along with it so this is a very very strange relationship normally you wouldn't be able to find out this relationship but with the help of this data mining task called as association rule mining walmart was able to find this out so these are just some interesting techniques which you can apply to find all of these up you know uncanny relationships and data so after finding this out what walmart did is they decided to stack diapers next to beer bottles and somehow the sales of diapers also increased and the sales of beer cans also increased so this is what most of the stores or supermarket use for cross selling or upselling items right so that was all about your data mining life cycle now we will head on and understand what is machine learning and we have this very simple example again to understand machine learning so here what do you see this is a fish now what exactly is this this again is a fish and this well this too is a fish and now how do you know all of these are fish well as a kid up you would have been uh you know you would have been told that uh this is a fish by your kindergarten teacher or your parents and your brain learned that whatever looks like this is actually a fish and once your brain learned about this whenever it came across a real life fish or maybe an image of a fish it immediately understood that this actually is a fish and that is how our brain functions but what if i had fed all of these different images of fish to this machine how will this machine be able to understand that this actually is a fish so this is where machine learning concepts come in so here what i'll do is we'll use the same concept where we had reinforced this concept of fish into our brain so here we'll take millions of images of these fish and we'll keep on feeding these images to this computer until it learns all of these features associated with this fish and once it learns all of these features associated with the fish i'll give it a new data or the test data to determine how much it has learned and if it has learned properly the training is done properly then it will be correctly able to tag that this image is of a fish and maybe if the training is not done properly then maybe it will tag this image as a dog or a cat or maybe something else so this is the underlying concept of machine learning where we first train our model with a lot of data once the training is done we will give it new data or test data to determine how much it has learned so that is about machine learning so again if you have any questions you can go ahead and put them i'll take them out is asking uh he's confused between our language and python are etl simply put etl is just one particular stage etl etl stands for extract transform load now this extracting data or transforming data or loading data can be performed by any tool so r and lang r and python are just languages and with these languages you can perform etl operations now let's say if i want to extract data from a database so if i have a database at the back end and if i'm coding through python at the front end what i'll do is i can connect python with sql and when i connect python with sql i can extract data from the database pull it to the front end and then i can write whatever python code i'd want to do similarly now i've extracted the data if i want to transform the data then i will use libraries such as pandas or numpy to convert them into a data frame or convert them into arrays and then apply some simple operations on top of that prashanth is asking me to give an example of speech and text data sure text data is anything which is written in textual format so whatever you so when we are implementing web scraping web scraping is an example of you know a a text analysis because what you're extracting is textual data now similarly let's say if we are doing sentiment analysis of maybe twitter data so let's say we have to mine everything that is present on twitter with respect to one particular topic this again is textual data then if you want me to give an example of speech so let's say or we can take examples of all of these uh chat bots all of these virtual assistants you have siri cortana and uh you know uh google assistant as well when you ask siri um tell me the distance between uh earth and moon this is speech isn't it you are uh you are speaking something which is which is a natural language and this virtual assistant is able to understand this speech data so darshan is asking me to list this step by step on how to learn ml short as i've been always stating so these are the three pillars which you would have to master when it comes to machine learning the first step needs to be statistics or math concepts because uh statistics and math form the fundamentals of all of your machine learning topics so here what you'll do is uh so pure you'd have to start off by understanding the basic statistics let's see uh uh you'll have to start by understanding measures of central tendency then you learn about measures of deviation you'll learn about central limit theorem then you'll head on to probability you learn about conditional probability then you'll learn about different probability distributions you'll have poisson distribution gaussian distribution normal distribution and so on so you learn all of these so once you're good with stats and probability then you can head on to a programming language the next pillar is programming language and i'll recommend you guys to learn both python and r because both of these are equally relevant in the industry and depending on the project you will be either choosing python or r now once you're done with stats once you're done with the both of the programming languages then you'll head on to the machine learning concepts where you'll learn all of the underlying concepts the math of all of these ml algorithms you'll learn comprehensively about linear regression logistic regression decision tree random forest ny base spm and all of these different algorithms so i hope that answers your question muhammad so priya is asking what is the difference between data encapsulation and data mining so these are two totally different concepts data encapsulation comes in the object oriented programming uh side of things where you only show information that is relevant and you hide all of the information that is irrelevant so that is data encapsulation but when it comes to data mining data mining is simply put you would want to find out information from your raw data again i'll take up the rest of the questions later on so we'll move on now we'll head on to this session over here and again guys if you haven't yet subscribed hit the subscribe button and also click on the bell icon as you will get all of the notifications of our upcoming sessions and uh you know it will also encourage us to come up with more of these live sessions on a regular basis and uh we'll also be glad if you uh if you spread the word of mouth about great learning to your peers or you or or your colleagues as they can also upskill themselves during this period of lockdown and uh you know if they go ahead and register to great learning academy all of your peers all of your colleagues can you know be certified and upscale during this period of pandemic so now that we've understood what exactly is machine learning let's look at the different categories of machine learning so we have supposed learning unsupervised one we have we also have another category called as reinforcement learning but we'll not touch upon that in today's session so we'll only look at supervised learning and unsupervised learning so when it comes to supervised learning we will have input variable and output variable this input variable is denoted with x and the output variable is denoted with y and we try to understand the relationship between y and x or in other words we try to understand how does y change with x here again the input variable will be your independent variable and output variable will be your dependent variable and the nomenclature is very intuitive isn't it because we call this dependent variable because y or the value of y is dependent on the value of x and we try to understand as and when x changes how does y change along with x so this is supervised learning and in supervised learning you also have to keep in mind that your data is labeled and in supervised learning we have two categories which are regression and classification we'll start off with the first one which is classification so classification is the process of predicting the class of a new variable and here we have another good example so we are trying to classify whether a patient has cancer or not on the basis of whether he smokes here if the whether the person smokes or not this will be our independent variable and whether the person has cancer or not this will be a dependent variable and on the basis of this i am trying to understand whether this person has cancer or not so i am trying to classify it's a classification process isn't it so i'm trying to classify whether i can tag this person as whether he has cancer or whether he doesn't have cancer a very simple example of classification next we have something called as a regression which helps us to estimate the relationship between different entities and i'll just give you guys an example of a linear regression so sure again in linear regression we'll have a dependent variable and we'll have independent variable now let's say the we map the independent variable on the x axis we map the dependent variable on the y axis and we have the x variable basically denotes the cgp of a student and y variable denotes the gre score of a student and we are trying to understand if there's a relationship between the cgpa of the student and the gre score of the student so uh and uh when we collect all of this data we sort of understand that there is sort of a linear relationship between the cgpa of the student and the gre score of the student or in other words if the student has a higher cgpa then it is likely that the student will also get a high gre score now where linear regression comes is to answer questions such as let's say if i ask if the cgp of a student is 8.32 what will be the gre score of the student this is where linear regression comes so now let's say if cgp is 8.32 and if i draw a line from here to this regression line and if i draw a horizontal line from here to here then i will understand that if the cgp is 8.32 then the gre score would be somewhere around 315. so these are the sort of questions which can be answered through this linear regression so that was classification and regression was about supervised learning now we'll learn about unsupervised learning so here we have all of this data so we have data in the form of cars and data in the form of bicycles but you have to understand that all of this input data doesn't have any class labels associated with it now even though there are no class labels i am going ahead and building this unsupervised learning model on top of this raw data and when i build this unsupervised learning model on top of this raw data i get two clusters the first cluster comprises of all of the cars and the second cluster comprises of all of the bicycles now you'd be thinking if there are no class labels how is this algorithm able to divide these two into those these clusters and this happens on the basis of similarity of the data points so here there are two things to keep in mind one is known as intra cluster similarity the other is known as inter cluster dissimilarity so if you look at all of these data points over here in this cluster you would see that all of these are cars which would mean that all of these data points are very similar so we have high intra cluster similarity for all of these cars similarly we have high intra cluster similarity for all of these bicycles as well now if uh if you look at these the data points between these two clusters you would see that these data points are very dissimilar so these cars are very different from these bicycles which would mean that there is high inter-cluster dissimilarity now this unsupervised learning algorithm was able to divide this entire data into these two clusters on the basis of similarity of data and here what we did was implemented a clustering algorithm and this is the underlying concept of unsupervised learning so uh this is all about uh you know machine learning categories of machine learning i'll just go ahead and look at some questions and if you have time we'll head on to the case study is asking me to take this session in hindi so right now this session is in english uh so if you want to know when are our hindi sessions scheduled again you can go to great learning academy so that you know that you will know when our hindi sessions are scheduled mohammed is saying uh numpy and pandas are used for descriptive analysis while escalant is used for predictive analysis e yes that is right ansar is asking what is the difference between factor analysis like efa and clustering so that's a broad topic so we'll cover that in some other session since we are also limited on time you only have 10 minutes and also have to look at the case study and we haven't yet started that so we'll have that covered in some other session uh rohit sure so we have we have a couple of tutorials on are already and if you want an advanced session so we'll uh have an advanced session on tutorial ready for you raji i'll be going out with the practical right now i'll start it so we have covered all the theory i'll be starting with the practical great so most of you folks are asking me to start off with the case study i'll go ahead and do that and before i go ahead to the case study again if you guys don't know about great learning academy if you uh haven't yet checked it out uh it's a free learning platform where you'll have almost 100 courses with respect to different domains such as artificial intelligence cloud computing data science stats and a lot lot more and all of these courses have been curated by industry experts and once you complete these courses you will get a course completion certificate which you can go ahead and add on to your resume so you can check out the link for great learning academy in the chat and also if you want to learn through an app you will also find the link for great learning app over here and again if you haven't yet subscribed go ahead and hit the subscribe button and if you haven't yet clicked on the bell i can also do that so that you'll be notified of our new live sessions and also you'll be notified whenever we are uploading new videos so as of now we have around uh three to four live sessions scheduled every single day so you can also share these live sessions with your peers or with your colleagues so that you also have a group learning atmosphere you can learn collectively and also you can spread information to everyone so you can spread all of these high quality tutorials to everyone so that they can upskill themselves during this period of lockdown great so now that we have cleared all of this let's go to the demo part so we'll be up having a case study where we'll be implementing different data manipulation and data visualization operations on the census data frame so we'd have to start off by reading this data frame and for this purpose i'd have to use pandas so i'll load pandas then i'll load this file so to load the file i'll have pd dot read underscore csv and i'll store it in this object called as census and i'll have a glance at the first five records of this data frame which you see over here once i load this data frame i would have to look at the shape of it so shape of it just tells me the number of records and the number of columns which are present and this tells me that there are 32 561 records and 15 columns so let me just go ahead and import the required libraries so i'll have import pandas as pd then i'll just go ahead and read the required file so the name of the file was census dot csv and i would have to i store it in a new object called as census and when i load this data set i'll have a glance at the first five records of this data frame and to have a glance of the first five records i'll use the head method and these are the different columns which are present over here so i've got this age column which would tell me about the age of this person i've got this education column which would tell me if this person is a high school graduate a college graduate or maybe a doctorate or something like that then we have the marital status then we have this race then uh over here we have this native country what is up you know uh what is the native country of this person and then we have this income column which should tell us if this person's income is less than fifty thousand dollars or greater than fifty thousand dollars now let's also have a look at the shape of this so i'll have census dot shape and when i hit on run you would see over here that i have 32 561 records and 15 columns now that i have loaded all of this let me just perform some basic operations on top of these so here i am trying to find out the minimum age and the maximum age so it seems that out of all of the people who are listed in this data frame it seems that there's one particular person who has the minimum age and that person's age is 17 then obviously that'd be one guy who will have a maximum age and that person's age comes out to be 90 and if you want to find out the average number of hours uh these people work so are these 32 000 people it would seem that on an average these 30 000 people work around 40 hours per week then similarly it's in that this one particular guy who works around 99 hours per week so let's implement all of these over here so i'll given the name of the data frame over here i'll given parentheses and if i want to find out the minimum age value from this entire data frame first i'd have to pass in this age column over here i'll just give an age and if i want to find out the minimum value i'll use the min method over here and when i hit on run you would see that the minimum value over here is 17. similarly if i want to find out the maximum age value i'll have census again i'll pass an age inside this i'll use the max method and this time when i hit on run you would see that the maximum age value is 90. and if i want to know the number of hours worked by these folks over here i have this hours per week column i'll write down census here i'll given the name of the column which will be hours per week and here i'd want to know the mean value or the average value it comes out to be 40. so on an average these folks walk around 40 hours per week and if i want to find out what does the maximum anyone work so it seems that there's one guy who walks around 99 hours per week then we'll have a look at the categorical columns so we have some of these categorical columns over here we have the raise column income column sex column and the work class column and i'd want to know the frequency of these uh categorical columns that is the value count of these categories and for that purpose i'll use this method called as value counts so let me just go to jupiter notebook i'll show you what we'll be doing with this value counts method so i'll have sensors over here and here let's say if i would want to understand uh the distribution of the population i'd want to know out of these data frame uh what is the male population and what is the female population what i'll do is i will pass in this sex column inside this and i will find out the value count so here i'll have value i'll have counts let me run this you would see that out of the 30 000 entries sort of this entire population of 30 000 21 000 so the male population is 21 000 and the female population is only 10 000. so it would seem that the male population is more than double than the female population now similarly if i want to understand up let's see the different um the about the different arrays so over here what i'll do is i'll have sensors inside this i'll pass in race and i'll use value counts over here now when i hit on run you would uh see that out of the 32 000 27 000 people are white so it's it's almost 90 percent almost 90 percent almost 90 percent of the population is white then we have only three thousand folks who are black then we have thousand folks who's racist asian pacific islander and then we have only 311 folks who's uh whose is american indian eskimo then we have around 271 folks who belong to other races similarly if i do want to understand about the marital status let me go ahead and do that so inside census i'll pass in marital status over here and i'll use value counts let me hit on run so here uh it would seem that out of the 32 000 population we have we have around 15 000 entries where this category is married civil spouse which would mean that this person is married and as uh as mostly he's a he's a civilian and he has a spouse then we have around 10 000 people who have never married we have 4 000 people who are divorced and we have around thousand people who are separated so again this is some basic information over here then what we'll do is we can also go ahead and change the name of these columns so here let's say instead of work class because this doesn't really convey much of the information if i want to change the name of the column i have this rename method so i'll given the name of the data frame then i'll use rename so when i have census.training here this takes in two parameters the first parameter will be columns and i'll given a dictionary over here and this dictionary has key value pair the key is the column which i'd want to rename the value s to that particular value which i want to rename so here i'll pass in work class and i'm renaming this to employment type and i'll set in place as equal to true in plus is equal to true basically means that all of the changes directly happen in the data frame similarly i'm using the rename method to change the name of this hours per week column to hours worked so let me go ahead and rename these two particular columns over here so i'll have census i'll use the rename method over here and inside this i'll have to given this particular uh parameter which is columns i'll have a dictionary over here and i'd have to given the name of the column which i'd want to rename so we had the work class column so i'll just pass in work class over here i'll have work class and i'd want to re name this to employment type then i'll just set in place is equal to true let me hit run and let me have a glance at the first few records of it so i'll have census.head and you would see that i have successfully changed the name of this column from it was work class and i have changed that from work class to employment type so this was uh about how can we rename the columns so guys unfortunately we are done with today's time so even though we are not done with the case study what we'll do is we'll have another live session scheduled part two of it so in the second part we'll cover uh this case study comprehensively so i apologize for stopping the live session right now because we are limited on the time constraints we are done with the 1r so whatever is left in today's session will be covered in the uh in the next session in the continuation of this and before i sign off i'd like to again tell you guys about great learning academy where you have all of these courses with respect to a lot of domains you have courses with respect to data science artificial intelligence cloud computing digital marketing stats and a lot lot more and all of these courses are you know all of these courses are curated by industry experts and uh once you complete these courses you will get a certification so you'll get a certification which you can go ahead and add on to your linkedin page or onto your resume which will be your value add to you guys and if you want to learn through an app you will be able to uh you know you'll find the link for the app as well in the chat and also if you haven't yet subscribe to our channel go ahead and subscribe and also hit the bell icon as it will encourage us to come up with more such live sessions on a regular basis and also you will be notified whenever we have new live sessions so please do hit the subscribe button please do click on the bell icon and also uh if you are interested to share it with your friends or share it with your peers or colleagues go ahead and do that spread the word of mouth about great learning and uh just just spread the knowledge around let other people upskill themselves during this lockdown so guys thank you very much for attending the session uh i hope you guys are safe thank you very much 
x2RzIBP298Y,2,"eLEAD's Automotive only call center generates proven results.  Let one of our talented agents improve your dealerships CSI or mine your customer base with a live one to one call.  Combine Regional Offers with our best in class automotive call center, to leverage your exisiting database.",2011-06-17T15:52:00Z,Automotive Call Center - eLEAD CRM - Data Mining and CSI,https://i.ytimg.com/vi/x2RzIBP298Y/hqdefault.jpg,Elead,PT1M46S,false,7458,14,12,0,1,we're able to mine your dealers database find the consumers of the most in market today to buy a cart now when we establish who those customers are reach out them with a live call from our us-based automotive call sir hi may I speak with Jenson hi Jim this is Jessica I love may I please speak with Eric for mr. Amano perranoski Carly they had also shown excuse anything the customer receives a dealer branded regional sales offer with the intent of the call center to make an appointment to have that customer come in to your dealership this is Caleb calling on behalf of a short Ford's customer relations department I would like to explain something that I think will be of interest to you in your records and notice it it's possible for you to move up to a new vehicle with a payment that could feel finger Perth dealer incentives which can be passed along to you today's low rates then here at art stores have decided to move up for about the same payment or sometimes if the call center is not able to set a sales appointment and take advantage of that live call and switch to a service message an attempt to set a service appointment with that customer so let me ask you if the numbers line up and it fits your budget you would think about a rice program so working with your schedule what works best sometime today or tomorrow the VIP program allows you to meet with a manager by appointment so you can show you these options and streamline if you have any questions just give the dealership a call the manager can make it augments existing dealer processes it blends regional offers with dealer level messaging and it leverages the best-in-class us-based automotive only khalsa you 
3LEUCV83OWM,28,"In this video, I have discussed the concepts of the FP-Growth (Frequent Pattern Growth) method which is commonly used for mining Frequent Patterns.

► More videos: https://bit.ly/2TLfkDj
► Association Rule Discovery in Data Mining: https://bit.ly/3d48NLH
► Click here to Subscribe: https://bit.ly/3eovHO3

Follow me on 
► Facebook Page: https://web.facebook.com/ashahzad/
► Facebook: https://web.facebook.com/shzy12
► Twitter: https://twitter.com/shahzadali039

#DataMining
#AssociationRuleMining
#FPGrowthTree",2020-05-02T22:19:27Z,Association Rules in Data Mining - 5: FP-Growth Tree by Shahzad Ali,https://i.ytimg.com/vi/3LEUCV83OWM/hqdefault.jpg,Shahzad ALi,PT35M31S,false,621,14,0,0,1,assalamualaikum in this lecture we are going to discuss another important frequent pattern mining method which is known as fp growth algorithm or which is also known as frequent pattern growth method used for frequent pattern mining or frequent item set mining method so uh in our previous lecture we have discussed uh the different extensions and improvements of the apriori algorithm in which we discussed three line of works in the extensions of apiori algorithm the first one is how we can reduce the number of passes against the transactional database for this challenge of the apriori algorithm we use the partitioning method and for the second challenge of the apiary algorithm which is uh the large generation of the candidate item sets so using dhcp or direct hashing and pruning method we try to shrink the number of you know the candidates or candidate item sets and third one is uh we introduced some special data structures in which we discussed e-cloud which is you know equivalent class transformation so in this lecture we are going to introduce a new tree structure called fp3 this approach is represented by interesting algorithm called fp growth algorithm or frequent pattern growth approach so let's look at how this algorithm works the general idea is first we find the frequent single items or frequent item sets of length 1 and then we partition the database based on each such item then we recursively or iteratively grow frequent patterns by drawing the above iteratively or recursively for each partition database which is also known as conditional database to facilitate efficient processing we use a new data structure which is called ft tree the uh whole uh process or whole mining process can be summarized as follows like we recursively construct and mine the frequent pattern tree or conditional frequent pattern tree or until uh the resulting fp3 is empty or until it contains only one path or single path which will generate all the combinations of its sub paths each of which is frequent pattern so recursive foreign so let's look at the simple example for this transactional database it has only five transactions as you can see 100 200 300 400 and 500 and each transaction contains a set of items like if we talk about the first transaction id which contains items like f a c d g i and m and p as item sets so we can scan the database once by using fp3 the first step is we scan the database in order to find the frequent one item sets like which contains single item or single pattern so suppose uh the minimum spot uh for this case is suppose 3 so we will be able to find the following frequent items like f 4 a3 c4 b3 m3 and b also 3 this is what this is just their sport count as you can see f occurs four times in these five transactions like in first transaction in second and third also end in fifth transactions so it occurs four times in this transactional database similarly for a3 a it occurs three times like in first transaction second transaction and last transaction similarly for remaining items these items are frequent one item sets because for each item like f a c b m and p their sport count is three or greater than three as we can see our minimum spot is three so all the items having support greater than or equivalent to 3 these are frequent item sets so we get these item sets as items as frequent items of length 1. so then we can sort the frequent items based on its scored frequency in you know descending order like this based on this we can uh construct a tree by first you know construct a header following this order like this we construct the header like this like and the item set having the highest spot count we place this at the first row then the item set having the spot count less than you know the highest one second highest and then similarly we decrease this places these item sets in decreasing order or descending order but you can see here we have two items uh having the same spot count of four and here we have four items having the similar sport count of three so it's your choice to choose the placement order like you can place c first or f first it's your choice so if you can see here uh it's just the representation of these item sets in the descending order of their sport count you can see in the transaction id of uh 100 here we have one two three four five six seven and eight items in transaction id 100 but in this column we represent only five items as f c a m and t we redo our way discard i g and d from the transaction id 100 similarly for other transactions this is because i g and d are not frequent items so we discard those items from our frequent item sets and like if we talk about i so i occurs in transaction id 100 and then it occurs uh nowhere so the support count of i is just one which is below the minimum spot so that's why it is discarded so similarly for you can place other transactions or item sets in the transactions in the descending order with the descending order of their sport count then based on this header table we can construct the fp3 as follows for example if you look at the first transaction which is fcamp so we construct f c a m and then t with only spot count of one so if uh i try to explain first we start the fe tree with a node having null value after that like uh we are talking about now uh now we're talking about our first transaction so in this transaction we have fca and mp so we at f to the null node and its value is 1 then from f we have to go to c so we extend this by adding a node c and its value is 1 then from c to a and a's value is 1 and from a to m the value of m is 1 from m to b and its value is 1 so in this branch p ends up with the minimums with a spot of one so then we have a second uh transaction which is f c a b m so first we have f now this part or the count of f is 2. so f to c okay again the spot of c is again from 1 to 2 on this path so then we have a similarly this part or or the count of a is 2 along this path so then we have to add d so now from a we have to add another node which is b and this part of b is one along this path f c a and m and then sorry a b and then at the end of this transaction id we have to add m along this path f c a b and m so this is one similarly for a third transaction we have to go along f and b so now we have to add or increase the count of f which is three and now we have to add another node which is b and its count is 1 so from this null node we can go reach to b by going from f to b similarly for fourth transaction we have to follow c b and p so along this path we have only one root which is f so we can not go along this path to get c v and p so we have to add another node which is c and its count is one along this path and then we have to add b which is this and its spot is our count is one then we have to add e which is here and its spot is again one and then for this last transaction we have to add f and its transact and its count is 4 then we have to add c and its count is updated to 3 then we have to add a similarly its count is updated to three and then we have to add m and it's already count is one and it's updated or incremented by one and now its count is 2 and then we have to add p and you can see its as count is 1 so it's incremented by 1 and it's what is 2 now so so you can see how we can construct the fp3 from the header table or item frequency head table so after constructing the fp3 we have to mine frequent patterns from that you know fp tree so for this uh conditional pattern this we mine single item patterns so we construct this fp3 and mine it recursively for example for ps you know p is conditional pattern base you will get only c3 the reason is as you know fcam like fcam the sport value is 2 and similarly for other path of p which is cb and along this path its p's spot is 1 as you know for this fca am path f c a m the p's value is 2 so that's why we have written 2 here so as we can see the f spot is two in these two paths c spot is three two along this and one along this path so c's overall spot is three a spot is two and m support is two as in our example our minimum spot threshold value is three so in this case we only get c as frequent item set in the piece conditional database so we will get only c with spot of three all other items like f a and m and b are discarded and discarded because their spot is true but for m's pattern base or m's conditional pattern base you you can see we have f c a and m or you know f c and a as you can see f c a and m again and it's for one time and similarly for second time f c a and m this m occurs two times along this same path so that's why we have written here f c a two and for the second time or for the third time f c a b and then it occurs one times or its count is one so we have written f c a b with this support value of 1 so in this case we have f count is you know 3. and similarly we have c's sport is 3 and similarly is spot is also 3 so as 2 times occurs in this part and one times it occurs in this path and so the overall spot is ah three for all these items while the spot of b is one so that's why it is discarded so only keep fc and a as the frequent item sets for m's conditional pattern base similarly you can do this on and on and then with this you can uh carry on to transform the prefix paths of b you get the you know following order like for c it's fca1 f1 and c1 so in this case uh so first we look at how these uh chunks we have written so like fca as b occurs along this path f c a and its count is one so that's why we have written here fca one and certainly for the second part of b we have written only f and along this path b occurs one times so it is one and similarly for the other path along this path c and then to b so it occurs one times in this one so in this case we have the you know the frequent or item set in the b's conditional pattern base we have only c as the spot of c is three while for the other items like f occurs only two times and c and a occurs only once similarly you can do this for a and you can also do this for c so for this uh conditional pattern base we mine single item patterns we construct this fp tree and mine it recursively for example p is conditional pattern base we get c 3 as we have discussed earlier and similarly for m conditional pattern base we get fca 3 and similarly for b's conditional pattern base we get 5 because sorry in last slide i have made a mistake because i consider in this path we have also could see so sorry for that uh we have no frequent item set in b conditional vector base as uh c occurs twice f occurs only one twice and a also occurs once so there is no frequent items sets along this path but for a conditional veteran base we have fc both are frequent items and c also has a frequent item actually uh for the single branch you can dump all the possible combinations the sport all three like f3 c3 and f3 4 c c is conditional pattern base but we just we just look at that in the recursive way and how we can mine this pattern base for this pattern base you can see you know for ace conditional pattern base you will get this you get fc 3 like is we get this which is fc3 as you can see from this conditional pattern base and similarly for c this c3 you can get this which is f3 as you can get from this conditional pattern base so you can see that's the same thing so then for uh this particular am's conditional pattern base like you can see this am's conditional frequent pattern base you can mine you know you just take this f 3 so you get this then you can go and dump all the patterns you know like this uh for single branch essentially that's the same thing you dump all the patterns like for single branch you just dump all the patterns like all the possible combinations there are these uh frequent items so how can we get these uh you know frequent item sets we just join the conditional pattern basis with their their you know items like if we want to calculate the m's conditional pattern base we just join their frequent uh items with their conditional pattern ways like fc sorry f m we get its count is three similarly for cm its count is three similarly for a with its conditional pattern which was uh m so a m which is count is again three similarly for now we combine these two with m so fcm is three f a m three similarly for c a m three and then we join all these together with m so we get fcm three so these are frequent item sets so in some summary if we summarize this whole you know the discussion you can like first we just scan the database and collect the or mine the frequent one item sets from the database according to our minimum spot and in the second iteration we sort uh those frequent item sets or frequent one item sets in their descending order and after that we construct a header table and from the header table we construct the fp3 according to the order of the uh items in the particular transactions and from the frequent pattern we construct sorry frequent pattern tree we construct conditional pattern bases and then from conditional pattern basis we get these you know single item patterns frequent uh single item patterns and then we combine these items with their corresponding you know item of the conditional pattern base and finally we will get the frequent item sets and then we have a special case of the uh frequent pattern tree if we have a single prefix path in the frequent pattern pre uh frequent pattern tree uh you actually can partition it into two parts like this uh like this part you can mine it and then this part you can mine it and then you can just concatenate these two together and to get the pattern results so then we have a special case like what about the tree that cannot fit into the main memory if it can cannot fit into the main memory we can use database per projections that means we project databases based on the patterns based on single item set okay so that we can construct and mine this tree for each projected database so we can have power projection or partition projection to different methods so the power projection means for this one for this you know the transactional database you will get f4 projected database and f3 projected database for example for example for the first string or for the first transaction id you will see uh g and h suppose it's not frequent and we'll get only f's are frequent like f1 f2 and f3 and f4 are frequent and g and h are not frequent because that's what is less than a minimum spread support threshold value it's just supposed um meijer like these are frequent patterns so you will get f4 projected database uh you get f2 f3 okay then for f3 projected database you get f2 that's the one so every one of this projected database is independent of the others so you can mine them in parallel but then you can also see you can also have partition projection partition projection the general philosophy is for each string for each string you only put into one place for example this one contains uh f4 uh uh projected database you you only put f2 and f3 but you do not uh put this f2 on to the f3's projected database like we cannot put f2 to this f3 projector database so when you finish uh to mine this f4 projected database then you can do this projection in f3 so you think this one is already to f3 or put this one into f3 okay so that's just uh difference uh different choice choices you know for different partitions or parallel projections it's just different way of implementation you can implement either using petal projection or partition projections so in parallel projection you can place frequent items in different projected databases at the same time but in the partition projections we have to wait for the mining process until an item is in another projection database we cannot place in some other projected database okay so uh here we have some extensions of the frequent pattern mining method uh for mining close frequent patterns uh item sets or max pattern item sets we have close set fp clause fmx these are the extensions of the f growth method similarly for sequential pattern mining we have prefix pen close span and byte and similarly for graph patterns we have g span and close graph and for constraint based mining of frequent patterns we have convertible concentrates and g prune similarly for computing iceberg data cubes with complex measures we have h3 h cubing and star cubing methods and for pattern growth based clustering we have m a p l e mapping and for pattern growth based classification we have mining frequent and discriminative patterns so these extensions of the frequent growth mining or fp growth mining uh methodology we will discuss these uh some of these like closet prefix pen clothespin and some others in our upcoming lectures uh when we discuss on the applications of frequent pattern mining in uh sequential patterns in mining graph patterns and some others so uh these extensions of the aprilia uh sorry of the fp growth algorithm we will discuss in our subsequent lectures so here i have finalized some recommended readings these are all classical papers you know a purely mining i also know the further improvements of february and here we have some vertical methods fp growth closet plus so finally there is an interesting survey article called frequent pattern mining algorithm which contain many more algorithms covered in this lecture if you are interested in go ahead and read in this chapter so that's it for today's lecture so thank you 
4GDiV5X8bZM,22,This sample material is one of the strategies in the asynchronous method.,2020-05-03T11:29:25Z,Lecture on the Four Stages of Data Mining Process,https://i.ytimg.com/vi/4GDiV5X8bZM/hqdefault.jpg,#ANNAlytics [Anna Liza Ramos],PT3M29S,false,202,3,0,0,0,hi welcome to my lesson for this session about data mining so they combining by definition it's a method of extracting information from huge row of data sets which can be used for analysis and discovery of patterns so for this session I will provide you the brief and Doctore background about this data mining and what are those stages in data mining process one of the important aspect of data mining are discs or stages okay on how the data mining will be executed so the first stage is the collection of relevant data from various sources such as in business transaction scientific data medical and personal data surveillance video and pictures games digital media card and software engineering data brittle world's tax reports and memos the second stage is data exploration gathering which we need to do some preparation of the data such as cleaning the noise of the data or what we called irrelevant data and removal of missing values which will affect the analysis of our results also you can do some aggregation of the data select significant attributes that contributes the expected result of the study then you make some generalization like changes the attributes from city into country for some reasons and alike and this are some of the tools to be used in transforming your data like for example we have here up in minor red car k9 Apache mapped out Oracle data mining Tara data in orange these are some of the tools which him use in transforming your table the third stage is modeling where we create a model test it and then evaluate model to perform the modeling you may choose either verification or discovery type of analysis verification refers to hypothesis testing or do some analysis of variance based on the strata of the population and/or discovery which involved prediction approves either classification or regression analysis using various algorithms such as neural networks Bayesian networks decision trees support vector machines and instance based and the last stage is the deployment of the models into a specific application where it is suitable and ready for users utilization okay that's all for our lesson for this session about the preparatory on data mining so if you have some questions you may post your question in our official platform and I will answer that questions before our next session ok thank you for listening and see you on our next lesson 
TrRgx6M-7NU,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-02-29T23:39:50Z,Data Mining  (Spring 2016) Lecture 13,https://i.ytimg.com/vi/TrRgx6M-7NU/hqdefault.jpg,UofU Data Science,PT1H22M21S,false,149,0,0,0,0,I think it will be nice once they're done I think that this projector is already better than the old one so I'm looking forward to seeing what it's like when it's done but you're all sitting in a good spot so okay so before we start I want to point out a few things on the schedule some believe we're starting at a new general section of plastic on like things involving regression and I'm using this term fairly broadly so we'll talk about that soon there are some various things coming up just to be aware of the clustering homework is due on Wednesday so make sure that you finish that and turn think most of you are are are really pretty well in there already talked to ok so a week from today is going to be the first midterm it's going to be here in in the classroom everyone needs to be here for that this is not one of those days you can take the midterm over the video that's not going to work so you have to be here so I expect that you know if you know it backwards affords maybe you can finish in like like if I were taking it I've met acquisition 10 or 15 minutes but most people will take most of the classroom so expect expect to be here for most of the class that's nothing wrong with that double-checking all that stuff so it but it will be it's going to be close computer so you can't have any sort of you can't have any sort of electronic devices ok I don't want it just keeps cheating out of it fairly well but you can bring in whatever you want written on paper so all you want to print out all my notes you can it's a lot I feel bad call the trees but I don't know keep that otherwise you can't monitor when you're doing on your black ops massage no you should not need a calculator either so you won't need to do any real calculations if you have to add something and you don't trust your addition then write out the word and if you had it wrong then then it's not marked wrong as long as we separated but you shouldn't you should need a calculator no calculators are alive okay so that's and it's going to cover the everything up through the clustering are the classes so through the spectral clustering will be the last lecture cover okay so I timed it so the clustering homework is due wednesday and so you'll get to do all the kind of you'll get to practice do everything on the homeworks if you followed and did everything along the Hobart's that you should be in understood all those should be in good shape on the mid rim the there were some delays I had to do in to check over things and releasing the grades for the document has homework with oh should be out later today there it's already and yeah so so so it should be on those parts of the class okay you should be able to do it all in here you shouldn't if I shouldn't need the notes but you're welcome to have adjusted yet those is all true clustering that's correct through clustering with that I thought only the streaming but I decided not to do that because well well I haven't released the homework of that I think I'd just rather you have a chance to work it out with the homework first so that would potentially go into the head interim test which again will only be in half the semester okay um and then after that we get spring break will be a break yay or like I'll be sad not to see all of you so full I heard some of you chuckling to its okay its everyone needs a little bitter gravy so I have not updated the notes for this part of the semester yet from from last year's know so usually update this probably five or ten percent material so i will i will get to these over spring break hopefully and then on monday after spring break the intermediate report is going to be due in half of this class we're going to have half of the lecture and then in the class we're going to i'm going to pair you up with a couple of other groups and you're going to give each other feedback on your project so far and then you'll still turn them in and also if you feedback as well but i think this way you'll get a much better sense what other people are doing it so i hope you can all come to class that day as well not let me know i'll try and make some allowances but the Vittra you have to be here and then so this frequent items on the streaming perkasa we do that following wednesday so I've put it up stuff right after break i'm sorry but this this homework is going to be half the length of the other horse the graphs of the frequent and the streaming ones are going to be happily with the other one so that they should take us all so so that's compromised oh yeah one more thing and on Wednesday I can't make my office hours so i will i'm going to move them i think to tuesday from 230 to 330 and then michael has more 3 30 to 4 30 i think up on tuesday also so so Michaels me this time tomorrow we need to pray i put my right before here if that's a problem please let me know i can try and make time i'll start on all uploads I'll try and be under canvas and answer questions alright so that's update on the schedule let's okay um good so so let's start talking about linear regression so hopefully so hopefully most of you seen have seen linear least squares regression before and during algebra course so we're going to review it quickly and then tell you what watches would when its limitations are and i'll start getting an overview of kind of the rest that this kind of heart of the clocks this series of lectures what sort of other things will be covering okay so so but i'm defining this regression you know in a way that's fairly broad in that so you kind of want to find some so you so we start so the input is is going to be slot data that X and then what we want the fine is a a simple pattern okay we're going to call this pattern is caught l okay and so we're going to think of a way of mapping from all of the data on to hell so in fact you can think of some value LMX for all so for all the data points you should be able to think of there's some value out of X which represents that data point is usually the closest data point that fits the pattern okay so the simplest example of this is going to be we're going to have some some data okay never can see those dots and then so this data is going to be X and then the line and then the pattern l is going to be this line and so then you're going to kind of map each data point on here so this is some data my ex and this is a Levesque that green doesn't work very well it's a really dark me not here under tight connections so no green no all right no greeting let me try it now okay so let's try that before yes okay then no all right let's try the black okay good and then so this will be X this guy will be televised okay good red suit so we're going to map every data point you can map each of the data points onto the pattern okay and so then the idea of a regression because you're finding some simple pattern and you can think of every data point as following that pattern you want the pattern not to be too far from the date of the substance okay so so this is a very general I haven't defined anything you know specific but when you start to find things you're making these modeling choices other other than and these general rule sucks um and so it's so often it it will turn out that it makes a lot of sense to use something that's linear as the pack there are these nonlinear techniques which we'll talk about briefly but you convert them into a linear technique or if you're in really high dimensions often if you're trying to find some nonlinear structure usually there's a slightly larger dimensional structure that's linear that works just as well so will mainly be talking about these linear patterns in this case um one thing you should note before we move on and this all say about it you can also think of your data looking like this right it's so in this case you want to use a different pattern you then want your pattern to be so he'll the case is good to be these three data points and these are going to be the centers for this assignment based cluster and then the mapping is just mapping a big data plan to the closest Center this fits the same pattern so this assignment case clustering would be an example of this and it turns out that when you're dealing with these a very large scale the same techniques will work as well for dealing with things like Vicki means clustering problem as they are going to work for the linear least squares regression so there's there's actually a very kind of like deep deep connection ok so let's let's start defining our come more precisely here so linear least pause squares regression so this often called ordinary these squares so we'll talk about this and will mainly keep today's lecture for the data points and I'll use our I'll switch the data is now going to be a point set P because we use x and y for other stuff so this will be a print set P and will mainly try and talk about the guy that lives in our two so in two dimensional data just to keep all this big picture stuff kind of a little bit simpler we'll talk about higher dimensional data in most of the rest of the lectures and so now for every data point P I can say there's a a x coordinate and a y-coordinate so this is for every for every three data points ok so in and so I may be useful to say that px is going to be subset and py is just the white Ravens so be easier to define based on these x and y codes okay um a xml our our goal is going to be a line L is defined by y equals ax plus B guess it so this is an equation right so for every x coordinate defines a y-coordinate based on two parameters a and b 0 red the color ok so we need to find so these are the parameters ok so we want to find these parameters and so what we want to do is to find a cost that will call this being l2 cost is defined as points that a be it's meant to be ey minus okay so basically I'm looking at the y coordinate that I I want to predict and this is my prediction based on the line based on the line based on Andy and I'm taking that difference in the y coordinate and I'm summing and squaring it and I'm summing these themselves so if i look at at the epsom some data here that draw a line through here then the air as i'm looking at all of these these differences in the y-coordinate from what I predicted okay great good race the Sun I take all these distances here this is called if this is a point P and this is RP this is called me the residual right so it's it's the residual from the data to the line and I'm swearing all those those those residual distances right this is this is basically held a px right so the data fitting the bottle there and I'm looking at all those distances from skory okay yeah yeah good this is a great question so we will look at using the normal distance later that will be called in 2d that's going to be the base of doing the principal component analysis okay and it turns out that using the vertical distance is going to be really useful in that it's really saying we're trying to understand this y coordinate and so when we go to higher dimensions you're going to get more x coordinates but often no more y coordinates and the beauty of this is that the units then you use for each of your co figure your coordinates now don't matter you because you translate everything into the effect on just the white border so the x-coordinate units don't matter they go into the coefficients you're going to have more coefficients a in this case you want to eat so forth and those those are going to go into the coefficients so you're going to solve them so it's not going to die ready yes that's a model choice cut is we'll see this one is is very nice to solve you can solve this one exactly doing the l1 distance be kind of you can set up a some linear program and solvent but you can't this is a closed form solution kind of like with the key means clustering formulation there's this you can every step use bring voids out if you can take the the mean of all the data points and that gives you the best cluster as hopefully you're finding out on the homework for question 3 as you look at the cave median cost and that's something the distances and there is no closed-form solution so you have to do something else right so again this square is going to give us a computational advantage it's gonna be very simple to do yeah so we'll talk about these more for these are great points okay um yeah so yeah those were the next two points of mine that's okay great um all right so so how do we solve this problem how do we solve for a and E it it turns out that the first kind of highly towards this is if you look at this value its uses okay so if you look at this value here this is going to be the mean point and I'll write this as P X bar p Wi-Fi so this one's said right so I'm taking this song for all the x coordinates and I'm averaging them that gives with the meat this line is going to go through the meat okay um so um so you can define this P Y similarly using the summing over the white boards as well and so then what we're going to do is I'll just write out the solution and then well what kind of try and stare at it a little bit and interpret what's going on so the covariance of two points X is going to be warm overhead okay so I don't I and looking at this is essentially on this should look kind of like a dot product after i centered the data this is not some crazy European accent and so then you can also say that just the variance of PX equals the covariance right okay so this would be the variance of the x coordinates and so it turns out that i can solve for a as the covariance of px py / the variance of penis ok which I can also think of as the dot products over the norm and so I'm going through this center point here and then I'm kind of taking the dot product between the x coordinates and the y coordinates and that's kind of giving me the slope so the a is telling me the rate which I changed the x coordinate and that increases the y coordinate and then you need to also solve the intercept this one is kind of a matter of of units this is d y bar minus a as if so this was easier if I were to draw some excellent y coordinates here me red suit so this is going to be x equals 0 this is y equals to 0 right and so now this is the value of P this is distance so it's the intercept here if I all seen the slope intercept kind of traditional four and then pay this kind of the rate of growth so it's kind of like the angle when you're doing angles you think dot products so that there's this nice closed form solution for solving for a and B which makes the least squares formulation very nice tomorrow and so this was and I've heard like I think this was like discovered by by by Gauss or something but he didn't publish it but then you know that this was discovered a long time ago and people use this just because they could calculate it back when we didn't have computers we had to work these out by hand and you couldn't really set up a video program but you could you could work out these calculations it's just a sum of all these things after you subtract them so play hopefully many of you have some juice seen this before any any questions on how you would we do this I'll talk a little bit about the multi-dimensional extensions next the dot product between yeah so is that because there's a bunch of points yes yeah let me now I'm worried about did something wrong is this you need to Center the data point so this is let me make a note here this is after on doing the center so and the centering is what's being being done here you're subtracting out to me okay so so now the dot product is that it define between two vectors and the dap marks is simply the sum over all the coordinates and the product of the two portraits and that's what's going on here that may be after center I need to double check because yes and so the variance yeah worried about this one over enter yeah this is on the covariance and the bearing so it gets cancelled and so the the norm of a point is actually just the dot product with itself so this is the squared all right so that's that's the same thing so that's what the variances and so these are defined with respect to the decision that so it's kind of it's so that the again this simple rule which is kind of useful is that you've two vectors D and you and so their dot product is equal to the enormity the norm of U times the coast of the angle between the two vectors you're angle so it's what any time you do something where it looks like you're doing something like an angle or slow you probably goes the dock box or something okay all right so let's just review one of the other nice things about this is in general as it's really well this is also today about higher dimensions does that give you of a point set which actually lies in in rd instead then so so then you can think of so you're going to break this point settings it into two parts so x is going to be an N by D matrix and so so each row of X each row of X this will be my matrix X each row here is going to essentially be one of these on these data phones so BP so I mean it let me write this as I'm going to have some some data point PL right the coordinates a little bit differently now as r1 r2 r3 up to RT minus 1 r rd ok and so now what's going to happen is this part is going to be going to tax and this part is to be the y-coordinate they were trying to predict ok so now when i write this coordinate in this matrix i'm going to get our 1 r 2 r 3 up to our d minus 1 and then the last rows can be all ones but this last column I guess I'm going to have n things here you know n points of the beds and it's still going to be a d columns okay and now this last data point I'm going to treat especially okay so the last data point is going to be P why I'm going to encapsulate everything from here all these Y coordinates into this into this vector people off okay and so now what I want to do is I want to find a a vector of a where this is going to approximately equal to D wat right so this is this linear equation again and so I have and so that if you want to solve this for all the coordinates a you can just say a is equal to X transpose X inverse x transpose p block okay so so this is going to look like this thing where this part up top here this is going to look like the p PX py and this is going to look like this part here it looks like 1 over the norm of X are peanuts yes so the first part is serving is kind of in verses like one over it's serving the one over the negative x and the second part is is like this stop ok and this solves just doing this solves for these these optimal eight coordinates which are again minimizing the sum of these vertical distances to Y so if I if I were able to draw this in higher dimensions now right imagine these points are actually ended in 3d then what I'm going to get for my surface for my solution this is now he'll which is described exactly by these eight coordinates here this is now surfacing so that this is going to be a d-minus one dimensional surface and I'm predicting the the difference just in the y-coordinate on to whom all the data points ponder this surfacing okay so it's always d minus one dimension name and so what happened to the important well this this B value ends up being this last coordinate care cuz I'm adding that every data point this is the offset and based on multiplying that he times the one right that he gets multiplied this vector this gets multiplied by this column which is all one which is that on there ok so this regulation allows you to write this as a linear algebra form which solves it in higher dimensions as well okay and there's this useful thing called sometimes use will call the half matrix which is X X transpose X X transpose so this is the hat matrix because if you were to you can make your predicted values of y this is equal to X a right so if i take this matrix X i multiply it by the this vector of coefficients a then those are my my predictive values which are supposed to look like are supposed to look like the Y values I'd Scott okay and so it's called the hat matrix because if you were to write if you were to expand out what the definition a was then I basically if you write that p why is this vector Y instead y py then this is putting the Hat on what its approximate usually you put have out something that's your estimated that okay so this is powerful because you can again extended to these these higher dimensional problems one other one other cool thing before we start poking holes in this is that you can also extend this to some nonlinear problems so so I in fact we'll just talk about moms so polynomial these squares so so let's say now that my data kind of hopefully you've seen this in linear algebra but just review your data looks like this instead of getting a line you want to fit something that that instead looks like a parabola so how can i how can I do this well now my goal given this data is going to be so I'm going to fit a polynomial of degree t so in this case here t equals two so just parabola and so then my my goal pharrell here my my my simple rule that I'm fitting to is that y is equal to a0 plus a1x plus a2x squared up to 80 x 2 okay and so this is equal to some t equals to 0 or x equals 20 Alta team of AI x to the okay so so I wanted to magically solve this the same way again well all I have to do is instead of having data that said just had an x coordinate what I do is I take my data point X and then this was and I turn this into this this this vector of one PX p x squared p x cubed and so here right so i just turn it into the vector and then i use this higher dimensional extension right so every data point which is one-dimensional now I turn this into T plus 1 dimensional data points and I'm you know great so and you can extend us if your data originally lies in its stead then one dimension in two dimensions you look at all combinations of up to a power T and you can do this same thing right so this is called this is either called a problem lifting because you're going from some lower dimensional up to a higher dimensional space but it's linear or it's called a a linearization look at this trip and so this allows you to do these polynomial extensions so again the simple linear these wares has this closed form solution it expends the heart of inches extensive polynomial fits their ways you can do this with more general similarity functions and kernels and weighs like that so there's there's it's seems like everything is going great in fact there's what's called the gauss-markov theorem which says that if I if I use this solution so that then the V on the least squares solution as I described above is is optimal so it's so it's minimizing the sum of the squared errors if as long as I require that my solution as zero expected error so zero expect there so that that means that the expected value of l of x minus x equals zero and um all bears these are private wrote these as the residuals RP which was again G of X minus this model why py- LLP of X are not known to be poor lady so that means that I mean the second one is totally common we just don't know that there's some relation that some data points are related to other data points but we don't assume anything about okay and then the first one is saying that they expected area 0 so for every prediction I think I expect there to be 0 which seems like a great thing to do and if I want that then this is the optimal way to do it this least square solution as i described above is the optimal and minimizes the sum of squared errors so red so so you know this is the point where I ask you if we're if we're done if we can stop with with regression at this point and say okay let's skip the next four I have elections right in this stop you yeah this sounds good yeah they're they're plenty of places like in the linear algebra class right you stop here right this was it this was done problem solved there are some issues that we already mentioned a little bit um what is the the squared error okay so there's the squared arikil so there's a square air so we're we're minimizing the sum of the squared error so this minimizes this minimizes the bearings so so what what suboptimal about is you mentioned instead minimizing just the sum of it airs why would one do that instead yeah so so this is not resistant to outliers a single data point can cause this this regression to change vast that seems like not a good property you're going to it's not uncommon to have some crazy readings if you have very big data sets you don't want your total prediction to be skewed by one data point so if you if you minimize the sum of errors or there's just one approach public leaves an absolutely progression but there's another portrait will talk about that's that that is also resisted outcomes based on it son minimizing the breakdown okay so another thing mentioned was the so this when we're measuring these these air residuals what are they looking at the vertical distance we're only looking at the y coordinate and this said this has some some some really nice properties of net it means that you don't worry about the unity and so when you're dealing with things that don't have units that are all the same then then this is great but if they all do the same unit and this might seem kind of strange it means I'm only predicting this last coordinate what if I just want to find a model that fits all of my data I don't want to divide it into the part of soup this input in the cart I predicted right if so so if I want to instead have some some data here like this and then I predict here such that my ear is actually this perpendicular distance here so I want to minimize the perpendicular business okay so this is related to principal component analysis which we'll talk about as well and this kid this essentially minimizes this projection on distance it's treating all poured it equally in building the box okay so that's it also doesn't restrict that that we can also have that this this prediction is is on we'll call it ring k so if you're in very high dimensional space you mean out what your model to be d minus 1 dimensions you may want it to be you have a thousand dimensions you want to 10 dimensional model so you can get a 10 dimensional subspace instead of on 999 original substance right so that this will also work with the pc okay so what else what else this sub is potentially salah you're sampling more points yeah okay so there's there's one issue there's what if n is less than deep is that the problem then you have more coordinates than you have data points it turns out then in this case you can't quite do this this poem is becomes comes on 25 so we'll talk about compressed sensing here and then this will help address this problem in order to deal with you have to make another assumption about your model otherwise you can you can find a model late you can find essentially d minus 1 dimensional subspace that exactly it's all the data from see end up overfitting not sure so if n is too small than this doesn't but you can use compressed sensing as one we will fix this okay what what else let's look at let's look at this assumption here this seems like a very it seems like a fairly reasonable assumption I want that zero expected error so for every data point I expect that my expected prediction is exactly the expected err i get with my prediction is exactly zero yeah so you know that this is going to go into overfitting this is kind of um i kin so we can add a bias into the solution if I had a bias I no longer get expected airs Europe if that's what having a bias feels okay so the sum of squared errors it turns out that this some of you know I'm going to have some py- swear you can break this into two parts the variance like the bias okay and so this solution the least square solution minimizes the variance assuming the bias is zero okay but if my overall goal is to make this as small as possible then I'm not restricted to having the x is equal zero turns out you can do better you can do better if you make the bias nonzero so if you add allow non zero bias it turns out you can make an even better prediction and this is Casa do it for fit so this helps with over things and so intuitively if you understand Beijing statistics at all you're adding a prior in the bias the prior invasion statistics is your bias here and you're going to turn out you're going to bias towards things that have a small slope things with the very large soul are going to tend to be very susceptible to noise so if you beautiful so you want to bias having a smaller slower and this is going to allow you to have better prediction when you get new data files and will will will demonstrate this okay and that the final point is that this can be there is a step in here the X transpose X inverse this can be expensive this step in solving it's the nice closed form solution but you have to do this matrix in first step and this is often slow you have a lot of data points this is going to be a slow process so we want to avoid this and so to do this we're going to we're going to go back to like these screaming techniques and this will go towards sub techniques for so some techniques that generally call in the category was called matrix ketchup which will be approximately a really big matrix which about smaller one so then anything you want to do like the matrix inverse and such you can do much more efficient but also some other nice properties that you'll learn about what what is uh was actually being preserved in this linear encryption so there's the Lord to that story as well but it's kind of you can think I've been dealing with scalability issues okay so so all these issues we haven't really really addressed yet so the rest of this section will be dealing with these issues moving beyond least squares regression ok so these squares regression is cool it's great but you can often do a look at that alright ok so to start off into let me talk about let me talk about how hires and so before I talk about fixing our one potential fix for the least-squares regression of outliers let's talk about just one way of dealing with them and these are called robust or Lincoln or what yeah so we're not going to like release absolute deviation we're going to talk about yeah specifically type of robust s Nair it's called a resistive estimate and so this is based on this notion of of the breakdown point of an estimator and so to talk about this let me just describe a common in one dimension okay so let's say I've got data points in one dimension and I can think of two estimators one is is going to be the mean which is this is just the same thing as the as a average right and we took this average several points during the least squares regression the other is on most normal data is going to give a similar result is the media and the media is again basically you sort all the data and you take the point that's in the middle of the sorted order okay so half the points are the left half of mine sir on the road okay so now what happens if i add in an outlier here so if i add this outlier point what's going to happen is that this this mean mean plus outlier is going to get pulled out often can get pull outside of the regular clothes of your data so it's going to get pulled out here just what L and if you move this out there even further off you had some weird mejor manera got this really a normal value then the mean can be way outside of your data whereas the that the change in the media is going to be very small it's going to change just by one point in the sort of order at all right i'm at one point to the end of the sort order so either I stay the same right move over or one point so it's going to be much more resistive to these outliers it's going to change much less okay and so now with this notion of the breakdown point this is basically going to be the number for the essentially the the fraction of points that I can move to essentially infinity and the estimator does not to so it doesn't move I don't have moved my estimator very far so for instance the the the breakdown point of the of the mean is going to essentially in the limit is like 1 over N or enough in the limit as n goes to infinity is 0 it's like you moved essentially as very small or just one data point up to infinity and the mean breaks down the with the median this is going to be one half so if i can move half the data points to infinity or n over 2 minus 1 data points and the media does not go to go to look good so it's much more resistant albums ok so often these these are resistive estimators are trying things that you can extend this to linear regression in a way that looks like it's taking the media instead of the mean and so there are a couple of famous things so one is called the deal said estimator which is more well-known this one has a it's a little bit easier to work with this one has a breakdown point zero point two nine three there's something called the seagull estimator which has a breakdown breakdown point of 0.5 the Stegall estimator so basically what it does is it or sorry the the deal sin estimator is it is a let's see it's so it takes all the points x x coordinates so px then it it's sorts sorts this into PX 1 is less than equal to PX 2px 3 and so forth and then it's going to calculate the the slopes of all pairs SI j is give me an estimation of the slope which is going to be y pi minus YJ minus y hi XJ minus X I right citizen so for every two data points so this is going to be data point P I and pj and it's going to look at only the pair's next to each other and then sort order so for every hair here it's going to estimate the slope parameter pay here is equal to as this slow as I could find out here so it's going to look at Thanks expect a slope of these and then the the AIDS is going to be turning is going to be the median of all these s YJ for the for the pairs which are next to each other in this orbit no actually it's not the sort of silence it's all of them such that P X I is less than J so for all PX I is less than pxj each year sorry about that this is that's the there's a page break in there sorry about that so for all the ones so in the sort order it looks at all the pairs where one is less than the other and it calculates this slow it takes the meeting of all these so this turns out is Israel robust these estimators because i'm using the median in this place and then the and then similarly b is going to be the median of all these wilds minus a x 0 so I can calculate the median gist of those and so this is going to be robust outlets now there are a few all the Schism doing this the seagull extension which is bit more bit more involved you can look at minimizing l1 areas it turns out this will also be the other bus designers but their variety of ways you can model the problem differently and it's going to be like more robustness one other way which we'll talk about is to use a type of regularization and so this is steal sign estimators and these these are breakdown point-based estimators they often end up being kind of comment orally a little bit trickier this works in two dimensions because I can sort next I need to sort the x coordinates here but these needs us sort these coordinates but you know often people don't want to mess with these things and they use what's called a regularizer instead the regularizer will define us in a second does not have this strong guarantee you won't have outliers but it's going to penalize your solution away from outliers and substance okay so it won't be a strong in the breakdown point sets that I still could have a small number of really bad there's the technique but it reduces the likelihood so there are two types of regression we'll talk about are those the two types of regularization that we'll talk about talk about the first one which will be easier to work with is called regular is regularization the first one's called ed is Cole is called the teak and organization this is sometimes called the rigid regression so these are the same thing ok so now I'm going to come and so for both of these it'll be simpler just to assume it will make the math a little simpler that px the the mean of X and the y coordinates is equal to 0 so assume that it's been centered already and so if you want to run these you may want to do a centering steps where you you calculate the mean and you subtract that all apartments next one okay so so now this one has a different goal which is going to be all right l2a l2s okay l2s which is going to be now i'm going to write the cost much in here now I don't need to calculate the need air subject so this is what's helped simplified I don't need the the B coordinate in 2d and we'll spend a lecture talking about even higher dimensions and so now this is the sum over all of my data points p y minus a p x squared this parts the same but i'm going to add an extra turn onto the cost function okay so i'm going to have this extra term this term here this term this term is this sir is called the regularizer and so I want to make this whole expression small so if the coordinate a the slope coordinate is large this is going to make this expression bigger even if this even if this expression is smaller for large slopes dis terms to make it bigger and s tells me how much what's how do I trade off the actual the variance air vs device alright so this is the regularizer and so this is adding a bias and this part has to do it the variance so before I assume the revised was zero so essentially I set s equal to zero there is no my default there was there was no penalty for having a large slope now I've been penalty for having a large slope okay so it turns out that so this is going to shrink it towards a small slope so you can think of a shrinkage estimator in some sense although the original shrinkage estimators we're doing something very good okay so what's so it turns out that there's an equivalence so then that there's an equivalent formulation and we'll call this l2t of PA and saying and so this will be useful on understanding how it works again I'm going to have some overall points p a p y k times p x squared but i'm going to restrict that such that a squared is less than or equal to t so i'm going to have a hard constraint now that the slope can't be too big let's say the square of the slope cannot be bigger than team now and so it turns out how do i how do i see this equivalence as NASA becomes smaller t becomes bigger right if s is 0 T is effectively infinity I don't have any constraint if I want to find it closed as s becomes larger I put more constraint on the slope then there's an equivalent formulation with the smaller team so as for s larger then equivalently that i have t 50 the smaller and I can see this by I can solve the above formulation for a fixed pest and then whatever the slope is whatever a squared is that's mighty battle right so if i increase s i'm going to have a larger larger slope up to up to a point so I there's this equivalent formulation but this is kind of a hard constraint okay so the other nice thing about this is that if I want to solve this ray there's there's a very simple solution so a s my estimate for a is going to be that's so instead of which is which is going to be equivalent in the matrix notation for higher dimensions X transpose X plus s squared times the identity matrix and when we get to the higher dimensional stuff we'll walk through some of these meetings or okay so um there's a very simple solution again it's I just had this s here in this denominator I add an S to it this for the problem was if these values got really small compared to these values so my ex the variation in X was very small compared to very large Y values this became very sensitive and so you're adding a term here at constant on the bottom so you're not dividing by something close to zero and one way to think about it right so again this is as simple as solving for regularly squares regression as long as they know with this with this value s is here um one way to think about it let me try and draw a picture here and it's a little hard to draw because typically people so if you see data and I'll ultrafine draw this data like this so typically you have data like this and if you draw an outlier you know if you think I've data like this and then there should be some good fit that goes through here you think where am I going to draw that line on this picture I'm going to draw it down here because it's easier to fit to the square picture right right if I if I if I were to draw this inside of a textbook I wanted to fit nicely inside a box right calm but outliers you think of being very far away from your data and so such that they really bias a slope and so really what you want to pick up the outlier that's being all the way up here as one of my data works so my data points all the way up there and so what's going to happen is I'm going to give them different a different slope which is larger than it should be in fact it should be much higher off the pitch and I can't even draw it writes going to bias the slope to be larger and so I want advice I will it's going to usually cause the slope to be larger and so I want to make this look back closer to zero there's going to be more trustworthy it might slow this closer to zero okay if the slope if the best fits looks something like this right so if my slope instead looked like like this this dash line here that if I have so then if I have some x and y coordinates its center to these should all kind of going to the origin then if I have an export you here it's going to predict something way up here so if i change the export a small amount x prime the difference in the Y value is is very large right this difference in the wild I becomes very large because my slope is large so I don't want these instabilities so you want to kind of bias where these smaller slopes and these taken off regular of regularization is doing this book okay um so it it turns out that and this is kind of what the contrapositive essentially to the gauss-markov there i go back up here right the scouts Markov theorem if I relax this assumption this assumed that I have zero expected error I should be able to get a smaller sum of squared errors if i add in some bias you but headed supplies if I relaxes so it turns up there exists some parameter there there exists exists some parameter s such that so I guess to be careful and describe this if you generate new data from the same distribution as the old data so if let's see so P was drawn from some distribution hi ID and then I do a new data point Q from this same distribution and I fit there's there's a hat and they had s then then the expected so there's some parameters here okay then the expected expect it air and now it's the sum of squared error so I want to look at the expected error from q y minus gain q s squared this is going to be strictly larger I guess unless some case unless the data lies exactly on a line or something like that then the expected error from my minus a mess units ok so the if i draw the data from the same distribution i see a new data point i'm trying to predict its value you're going to get smaller expected error in the squared error sense using some estimator chosen but that's okay so this is kind of baseless there is some value s that does better than if you do it with an S soon to be zero you don't always know what that is but you can essentially you can actually figure this out using cross-validation so so how to choose to choose test this is going to use cross so cross validation and we'll talk about this a little bit hopefully you've seen some cross validation in the machine learning class if you haven't taken that we'll talk about this later in the semester hawk how do you go about difference ana i believe over i set of homework where you'll actually see this actually work in practice when you'll have to choose an S that actually works better on the hell delicate ok so so hopefully this seems really cool if you haven't seen this before to be very cool that you can do this you can do better by adding this bias towards a scholarship all right um so you have five minutes left and so I wanted to I'll talk about principal component analysis basically that will be all of like all of next lecture we'll be talking about this singular value decomposition and how to interpret this so linear algebra operation and how to get the principal component analysis out of that basically that's minimizing the sum of this orthogonal business so i'll save that to our next next lecture one more thing i want to talk about is the this is other form of of regularization call call elasa this is often called this this is also called basis pursuits and this is a very small difference that goal now is is is the it is a different cost function it's again going to have this 3 squared term here right so some squared errors but plus s times a so now i'm looking at minimizing the absolute value of it not the square value okay entity and so when we hit the higher dimensions you're going to have more than one coordinate you're trying to fish you're trying to make this is going to be an l1 normal 7l to them okay and so there's again an equivalent formulation l1t is going to be again the sum this is such that such that the apps the value of a now is less than T and so again there's this formulation formalization we're in a hard cap on t now in one dimension these are going to be or two dimension the data these are going to be equivalent because if I have a cap on the absolute value T on this squared size of tea I can change between different values of T whether it's the lasso Awards is the ork ork is the ridge regression because of this equivalent formulation I can set a different value T depending on whether i'm doing the bridge wire or or or I'm four I'm doing the lasso and and I'm going to get a similar thing which means i can also set the same value guess so these really are different in to you these are the same thing in 2d what's going to happen though in higher dimensions is going to be really interesting we'll spend a whole lecture on this is that you often once you're going to get remember the higher dimensional version you could this is going to be a vector of coordinates of coefficients here and one nice thing you might want to say what if I made a bunch of these coefficients zero meaning i didn't need those terms in my aprox in my model my simple model can make it simpler by saying some of the coordinates I don't need those in order to make a prediction if I set these coefficients to be 0 then then I say those are unimportant and so that I do the lasso with higher dimensions it turns out by having this regularizer the l1 regularizer instead of the squared distance then it's good it's going to cut snap some of the coordinates the zero in the hot when you solve this opposition and so it's going to give you a sparse solution so this is going to be something like feature selection tells you which coefficients are more important it doesn't exactly tell you that because sometimes there are two coefficients and they're both tell you ascensions the same thing so you can set 120 and the other one not 20 and so it doesn't tell you this information it could have could have set 10 or not the other and it really doesn't make a difference so feature selection of some kind of issues to it but it's going to give you this nice sparsity okay so we'll have a lecture talking about need to use the difference in the shape between the l1 norm and l2 norm and that's going to give you the sparsity properties in Toronto however the bad thing about this is that unlike bridge regression there's not this nice closed form solution inning I can't just stick it inside of this matrix inverse I can't just do this trick up here I do this I can no longer solve it simply so I need to be use more careful techniques to solve it if I use just the bridge regression to higher dimensions I'm not going to know to a coordinate stuff I'm not going to get these e 0 coefficients I'm not going to get those it's just going to make all the coefficients a little bit smaller but not all the way to 0 so I'm kind of stuck in between these two things it turns out there's still a reasonably good solution for the luck solving Galasso it's just not as simple okay so that's it for today we'll talk about the SPD and PC a.m. wednesday and and don't forget to change my office hours sorry about this again to Tuesday 232 tender 
WMxI5zx9yQY,22,FP Growth Algorithm| Data Mining | MLRITM | Apparao Yannam,2020-09-14T14:19:48Z,FP Growth Algorithm| Data Mining |LH.15| MLRITM | Apparao Yannam,https://i.ytimg.com/vi/WMxI5zx9yQY/hqdefault.jpg,MLRITM BTECH / MTECH - TECHDB7,PT7M12S,false,526,10,0,0,11,[Music] hi friends hope you enjoyed my previous sessions of data mining so in the last class i discussed a prairie algorithm in detail right with an example so today i am going to discuss about fp growth algorithm with an example so if you missed my previous class so that is on a prayer algorithm please kindly watch that video by using eboli fb growth algorithm now the abbreviation of fp growth algorithm is frequent pattern growth algorithm in data mining frequent pattern growth algorithm in data mining now as we know that the operator is an algorithm for frequent pattern mining that focuses on generating item sets and discovering the most frequent item set so as i discussed earlier in my previous class so that is an aprilia algorithm i took one example also so in that so there are only nine transactions right so if it is a nine transactions so to find out frequent item set by using that nine transaction we may not get any problem right so now a prairie means just like focus on generating item sets and discovering the most frequent item set suppose if you want to purchase milk obviously you are going to purchase a bread also so if you want to purchase milk and bread so along with that milk and bread you are going to purchase so butter also sometimes right so so i can say that these are the frequent item set so there are n number of customers may purchase the same frequent item sets so how to find out frequent item set so in the supermarkets by using data sets so obviously we need one specific algorithm right so now present i used a prairie algorithm to find out a frequent item sets right see if it is a less transaction so operator algorithm is really suitable now assume that there are some cores of transactions available in the data set so obviously this apparently algorithm is may not useful right so these are the some shortcomings of a prairie algorithm so now just i want to give brief about a priori it greatly reduces the size of the item set in the databases so however a primary has its own shortcomings as well right now so i want to discuss shortcomings of a prairie algorithm just like limitations disadvantages of a prayer really now there are so here i concentrated only two disadvantages of a prayer algorithm the first one is using a prairie needs a generation of candidate item sets right so candidate item sets are really required for a prairie algorithm these item sets may be large in number right if the item set in the databases is huge now a prairie needs multiple scans of the database to check the support of each item set generated and this leads to high cost so now so in the last class i discussed one example right there i was took six transactions so if it is a six transaction obviously so there are multiple scams by using first transaction to second transaction first transaction to third transactions see if it is a less transactions simply we can calculate by using multiple scans so i think that there are one course of transactions how we are going to so scan this multiple items right so this leads to time taking for time taking uh process and high cost also so that's the reason so a prairie limitations means so a prairie needs a generation of candidate item sets and it needs multiple scans right so these shortcomings can be overcome using the fb growth algorithm so there you may not get any candidate you may not get any candidate item sets as well as there is no multiple scans now frequent pattern growth algorithm now see so this algorithm is an improvement to the a prairie method so improvement to the apparent method a frequent pattern is generated without the need for candidate generation so in fp growth algorithm there is no candidate generation right so we used candidate generation in a prayer algorithm fb growth algorithm is represented represents the database so in the form of a tree called frequent pattern tree or fp3 so this is purely based on fp3 so i'll discuss fp3 now so this tree structure will maintain the association between the item sets the database is fragmented using one frequent item this fragmented part is called pattern fragment so fragment means just like reducing the space right so the item sets of these fragmented patterns are analyzed thus with this method the search for frequent item sets is reduced comparatively so by using one example you will get the idea about tree structure of ffp tree frequent pattern tree is a tree like structure that is made with the initial item sets of the database the purpose of the fp3 is to mine the most frequent pattern so in the operator algorithm also so we find so frequent pattern so but if it is useful for only limited transactions assume that there are some more transactions in the data set so obviously so we need fp3 so this is also just like mine the most frequent pattern so each node of the fp3 represents an item so here item means represents the writer means just like node of the item set now the root node represents null so obviously if it is a tree the initial node is null only right so same way the root node represents null while the lower nodes represent the item sets so if it is item sets available in the root node so obviously first node is null remaining are lower right the association of the nodes with the lower nodes that is the item sets with the order item sets are maintained while forming the t structure so here if it is a tree fp3 so we need to concentrate on root node as well as item sets that is lower nodes so this is the association of the nodes in between lower nodes and the item sets now see i want to take one example of fb growth algorithm so you will get the clear idea about how to find out frequent item sets by using fp growth algorithm free [Music] 
qk9kM-CoTFA,22,"In this video you can learn about Major Issues in Data Mining 


#dataminingissue #issuesindatamining #sankalpinfotech


What is ETL || Extract, Transform and Load with example || ETL Data Warehouse Tutorial || Lecture -1
https://youtu.be/um_WckQkKRE

What is an ETL Tool?  || Comparison and contradiction of various ETL tools|| Lecture – 2
https://youtu.be/MtTvbAK44y8
Basic Concepts of Data Mining || Data Mining Introduction, Evolution, Need of Data Mining||Lecture 3
https://youtu.be/KdX3A-k8t_A

Data Mining Functionalities || Data Characterization & Data Discrimination || Lecture – 4
https://youtu.be/sQtEbrjUwa8

Classification of Data Mining Systems || Data Mining Classification - Basic Concepts ||Lecture – 5
https://youtu.be/b3T3kWEtWZY

Integration Of A Data Mining System With A Database Or Data Warehouse System || Lecture – 6
https://youtu.be/SUWMO5z2Mcw",2020-10-21T07:46:53Z,Issues in Data Mining || Major Issues in Data Mining || Lecture - 7,https://i.ytimg.com/vi/qk9kM-CoTFA/hqdefault.jpg,Sankalp InfoTech,PT33M,false,90,7,0,0,0,hello student how are you i am sanjay hadiyal welcome to again our video lecture series etl in data mining in this particular lecture we discuss today's lecture issue in data mining so again we right now discuss about data mining concept what is data mining how it works and its different functionalities and something so again today's lecture we especially we are discussing about issue in data mining system when we use data mining concept with database and data warehouse system at that time we use a particularly data mining concept but many times when it has some problem or issue when we access uh some particular functionality again see data mining concepts milosevic data mining is not an easiest task as the algorithm used can get very complex and data is not always available at one place simple um algorithm used can get very complex and data is not always available at one place it needs to be integrated from various heterogeneous data source what dimension for again important statement it needs to be integrated from various heterogeneous data source of the previous also create some issues these factors are create some issues we discuss the major issues regarding this concept is given below [Music] different kinds of information stored in this particular data base or data warehouse and that's why each and every records are in different uh concept or different types and that's why data mining concept all over performance is used is generated is now we discuss this concept or a point one by one uh with in detail the following diagram describe the major issues of this something three parts mining technology and user interaction performance issue and third one is a diverse data issue again mining methodology and user combining different kinds of knowledge in database interactive mining of knowledge at multiple level of abstraction incorporation of background knowledge data mining query language and ad hoc data mining presentation and visualization of data mining issue handling noisy or incomplete data and pattern evaluation to direct indirectly efficiency and scalability of data mining algorithms and second one is a parallel distributed and incremental mining algorithm to a performance and complex types of data and mining information from and user interaction issues that can consider circulating which is again divided in five to six parts the first one it refers to the following kinds of issues the first one i promise a mining different kinds of knowledge in database because in data warehouse or database there are lots of huge amount of different kinds of information is stored so based on this particular information we mining uh different kinds of uh knowledge-based information it's one kind of problem or it's one kinds of issues so see here different users may be interested in different kinds of knowledge therefore it is necessary for data mining to cover a broad range of knowledge discovery tasks disadvantages knowledge different users may be interested in different kinds of knowledge sufficient tools and technology the data mining process needs to be interactive because it allows user to focus the search for pattern providing and refilling data mining request based on the written result look marking in so we can say data mining system or concept is process to need to guide discovery process and to express the discovery pattern the background knowledge can be used background knowledge may be used to express the discover pattern not only in consider sorry conscious term but a multiple level of abstraction multiple levels ad hope mining task should be integrated with a data warehouse query language and optimize for efficient and flexible data mining systems say different should be integrated with data warehouse query language and optimize for efficient and flexible data mining concept presentation and visualization of data mining is used a simple equation applications give a data mining and that's a different kinds of users are located different kinds of requests are there queries are there and each and every queries or incomplete data are required to handle the noisy and incomplete object while mining the data regularities if the data cleaning method are not there then the uh accuracy of the discovery will be poorer particularly or foreign is this concerned again very well simple and easy just hold properly next one the second type of foreign in order to effectively extract the information from huge amount of data of data in database data mining algorithm must be efficient distributed of database white distribution of data and complexity of data mining method motivate the development of parallel and distributed data mining algorithms this algorithm divided the data into partition which is further proceed in a parallel fashion then the result from the partition is merged the incremental algorithm update database without mining the data foreign a handling of relational and complex types of data first one the handling of relational and complex types of data multiple data special data temporal data etc etc it is not possible for one system to mine all these kinds of data season [Music] data mining system next one mining information from heterogeneous database and global information system again second issues that on local area network or wide area network this data source may be structured semi structure or unstructured therefore mining the knowledge from them add a challenge to data mining foreign and very well thank you for watching this video 
VSDDKlrVidc,27,"Data mining is used on an existing dataset (like a data warehouse) to find patterns. Machine learning, on the other hand, is trained on a 'training' data set, which teaches the computer how to make sense of data, and then to make predictions about new data sets.",2020-06-15T05:08:35Z,Data Mining Lecture 2 - Data Mining and machine learning Algorithms,https://i.ytimg.com/vi/VSDDKlrVidc/hqdefault.jpg,Charles Edeki - Math Computer Science Programming,PT22M9S,false,184,7,0,0,1,"again welcome to data mining and analysis lecture number two in these lectures we're going to discuss about data mining and machine learning applications such as co-starring classification predictive analysis and Association rules so data cross story is the process of grouping related data set item into one or more clusters which means similar data will be in one cluster the similar data should be in two different process or different constants so one of this is one of the first to say data analysis perform when he or she minds data is again prints the data into a related groups so similar data set may be in one group together and data that are most similar would be in different groups so process uses and what we call on supervised machine learning algorithm which means the algorithm does not use the training data set a training data set again data say that also have their class labor or the target variable what do mean by the target variable is that for example a computer model that can diagnose a patient this is B so the fashion I need is a training data set this is a data set that we know that patients have the disease a so maybe I may have 10,000 patients data set with a disease a they now use that with a Lenny or Gary team to do the model then and we use what we call a test data test data is that the data of the patient with that the target or the cross river so we don't know if the patient of this is a or not we don't know but it's up to the model that learning model to again make the decisions race so a training set is a dataset that also have de the knowledge the target variable so we said clustering is not don't use a trainee date that's it so the go of cluster is to again group the data into cross this is on some similarity image on it most of the algorithms we are going to use the distance measurements so different constant algorithm also here you see that most machine any solution differ by performance memory use hardness or softness data set size will be tested size is very important that can lead us to what we called overfitting on a fitting problem we may discuss that along the way also the need for analysis to specify the starting norm of crosses and also others we get it into crosstalk algorithm may be scarce or so example giving a scroll story the Ares that I said so here we say the iris dataset is a one-note at our money and machine learning it as a used to introduce the customer transmission process so in Aris dataset we have some attributes they attribute to be the Sapa length and the width and also pattern Lantana with now based on the supreme width and the length and patella tendon length we can classify based on those values we can classify whether the flower is our institution or iris diameter or is our is physical line so here we use the concept of cross only now the data so that are similar width and length we have one crossed the sides are the same we tonight another one like there will be different questions now if we have the training set of our rich data set then we can use the total for classification what are you going to be the model next time we have a dataset because make a decision whether is one of these three classes as iris setosa Arista Jenica and also are a special column so we also use a custome by arrow Python we have package or Python lab refer for cross story algorithms this an example here again we may discuss this later on also data classification which is another type of machine learning or determine your garden this is two-step process first you need a training set to build a model then after beauty the model we need a test set to test the model so as we said a la training set is a data set with a class level or the target variable and the test that is used to measure the accuracy after we build it so here we say data crafts kitchen is the process of assigning data to match includes there are many different uses for data transmission example yes a bank so a bank for example might examine customers attributes to determine if the customer is a high or low is low candidate so most of the loans now we apply in the bank the decisions are not made by a person the decision are made by other data money machine learning model system similarly airport security teams might examine passenger data to determine the passengers a potential threat this would be a classification for example we may have an image of say terrorists in our data base system immigration database system so when passengers arrive this can be a passport or take them a picture this image would go to the trafficking classification model to analyze to see if it on much the image with the image that is already in a database system used for the model and also daughters Martin's amid to my Bousfield to determine you get two mice malignant or benign so based on a modern we is the training set and we know what is the tumor being malignant or benign we know it so we use that data set to breed a model lest we may have a data from a patient and now we want the model to determine based on the attribute if the patient's tumor is again maybe now another example security software multi-use classification to categorize an email messages what is valid or spam now this will be based on the subject of the email or the content of the so data transmission uses again what we called a supervised machine learning meaning that the classification of Karen we use the training data cells that are said to teach the or guarding the common attribute for each category now there are many different data transmission carriages which differ by memory use CPU performance so classify an income greater or less than 50,000 this is an example again using Python here we import import the pandas and numpy packages to doing a scale and also a skill a scaling so you can see they're guarded the code here we are importing training tests rate from Skillman dot matrix again we may go through Python packages future we also have a predictive analytics we will say for decades businesses have used at analysis to a scream spring theater for my stream TV of previous year so analysis refer to such analysis as descriptive analysis as analysis use data to describe what happened in the past so predictive analysis and contrast use this data to predict what will happen in the future so classification use that data to classify predictive analysis use the data to predict what happen in the future so a linear regression BIA a good example yeah we are using linear regression we tolling one attribute so our true boot is slope by the slope which is the X so the S will be our independent variable that would determine the why why would be dependent variable so here we say the simple form of predictive analysis is linear regression which creates a linear expression that best smother a given data set in a simple linear regression analysis predict a result based on the value of one variable such as the value of y is a family question is again straight line equation so I dependent removing excess candy tell me why also we have the multi variant regression in this case again we have more than one x value so here we have one two three four so we have four inputs to determine the output which would be y dependent arawa so here we say multivariate regression analysis predator is or based on two or more variables when we perform a multi variant regression the solution will provide you with the coefficient values in this case for ABC and D which you can then use to predict results so credit insurance costs again using title and the next would be the debt Association normally is that an association rule and the concept of Association rule is that we may analyze the data set to Fernet which influenced B for example a very simple start with their Market Basket analysis for example we analyze a supermarket databases and to see if customers buy bread they also buy pin opera so what's the association between the two you know we have to find we have to have a treasure value and we have something called a confidence and support values those two confidential support values can tell as the destination association between bread and a pen opera so death associations the process of identification ships between two the roubles for which the presence or absence of a fence variable called antecedent influence the second variable called a consequence so yeah we have the bread and descendant pinna bara will be the consequence so one of the personal data Association problems is the market basket analysis which is a mini items in the shoppers basket to identify association between them now using a market basket analysis for example analysis found that shoppers who purchased diapers are highly likely to also purchase beer - this again example we searched product inside and then a storm I'd advise advertise a cell on that case while also increasing the price of a beer say my pride - peanut butter and bread since we know that most customers buy both at the same time we are not going to put those two items on sir if I'm the store manager maybe I'm a price a little bit higher so we also have a Python package that make it possible to again do data Association tasks neccessity say Big Data money and also analytics so he was across the internet we can again accumulate data as much as possible so he across the internet we are committing it at this one carrot and respect aspect to double the amount of data in the world every two years so with the advent of the Internet of Things like the IOT that rate is the sweater to increase and this is the concept of big data I think we discuss about Big Data have 4 vs the volume data is very large the variety the variety means we have different types of data structures coming from different sources and also the velocity the speed of generating data very high so the question when analysis asks this when does a lot of data become big data so in the simplest sense we say that an application becomes a big data application when we can no longer manipulate that data using traditional databases or data analytic tools meaning the big data form as see the size of the data to start on support for example when we cannot analyze our data in the memory weaken so say it is a big data which was a big bit I stood like that it can fit in like memorial so we have to analyze the data from storage device and also we have different structure of the data which make it impossible to see the data in a regular relational database maybe we have to use the know SQL concert or a good concert Map Reduce so this is where a data science comes in with a large big data analytics we have to use some other applications that will make the analysis possible so in this course again we are going to do a hands-on data mining and analysis with wake up we said weak I said open source software free we can see the window yes again it's a graphical user interface we don't need no coding there has a lot of features including visualization data preparation which we call the filter we filter the data then the major traitor's classification cross story and association also it's included most of the other it is good so these are some of the few key terms we should know we should know what is a business intelligence again the use of tools determining machine and visualization to convert data into actionable business insights and recommendation that messes the classification the process of assignment data to matching groups or categories also cross during the process of grouping related it I said item into one or more clusters dashboard a visual and of the interactive collection of charts and graphs that correspond to the matrix for the business key performance indicators data Association now that as the process of identifying relationship or association between variables and we saw the well-known example is the Market Basket analysis yeah we are determined for customer by product a what is the chance of buying product B the association between product a and B we talked about a prom or gary game the concept of again support confidence values to determine the relationship also data cleansing data quality data visualization data one way and also descriptive analysis and what is a machine learning we went through the technology also the natural language process we were talking about it tests my knee so natural language processing all as I test my knee but again the has his own algorithms and the concert CCS that the use of software to understand everything as walking words then we talked about predictive analysis and also prescriptive prescriptive analysis is being descriptive and also predictive analysis this will recommend the best choice among the rubber offices we talk about what is a supervised learning what is a test data set when we are doing classification we need any data set to build our model then we need a test data set to test our as the training data set also unsupervised learning is when we are using a data but it's not a training data set mostly clustering custom algorithms are unsupervised then we also talked about what is official programming this is the process of creating a program by dragging and dropping objects good concert is the rapid manner and Wicca we are not going to write the code by rather again smaller circle so you will say as opposed to writing program language statement let me something like a rapid man or weaker and the Lenny curve is very very short a time if you understand the theory concept of data million you understand audio girl readings then using wake up for analysis or rapid manner it's very quick again our and Python is like a learning programming language so if you don't have no background those language means you have to take your time it will take time that lemak efficient hi so again this would be the completion of our lecture number two and so in this lecture again we went through the concept when I say basic I mean again we wanted definitions of the different machine learning determine yoga reading classification cross story association and we went to some day major key was in machine learning data money so then wish everybody the best thank you "
IbfUiNeysTU,27,"Big Data Courses at the University of Utah
Spring 2016 classes (Mountain Time):
Monday & Wednesday 11:50 - 1:10: Database Systems (Feifei Li)
Monday & Wednesday 3-4:20: Data Mining (Jeff Phillips)
Tuesday & Thursday 12:25-1:45 (Tucker Hermans)
Tuesday & Thursday 3:40 - 5:00 (Tom Fletcher)",2016-04-06T22:40:05Z,Data Mining (Spring 2016) Lecture 20,https://i.ytimg.com/vi/IbfUiNeysTU/hqdefault.jpg,UofU Data Science,PT1H26M37S,false,212,0,0,0,0,alright so let's get started so we're starting the strength oh ok so before we get started I wanted to reoffer questions about the due date for the homework or any question about the homework everyone there are people ok with it on on doing monday skill or do you feel the last chance that you want to be there because i had some people come up and say they'd appreciate it being pushed back a little bit then the project is also do then so so who would yet come ok yeah so we haven't the office hours don't line up well so let's uh ok so it's like I don't want it to the same day as the project and I think that's let's just asking for trouble so how would how we do on on like friday is that I feel glad everything's do on weekends so mean the friday of all so next I would move it back I'm not going to say let's make a do early write something so full so I could do it on the 15 April fifteenth that sounds so who wants okay so we'll do a vote once the ball juice two options keep the same due date or april fifteenth ok so the same do today is April 11 so once a poll 11 and 2 10 15 okay see I asked you guys earlier and you're like no no no it'll be fine okay I won't know move it back to let me write it here a 52 April 15 okay and i will get this later into a canvas all apologetic so what's now officially in the notes great okay um any general questions about the project on reports can talk more about it monday too but any general questions about that okay so stay within your page limits don't give me two small font or small margins will be puppy grumpy when did when I'm reading it so okay that's its Jenna it's in your favor to make me happy when you're million so pictures and charts and and not fall small font or small margins will make me happy i'm also very clear writing is good so okay great but any questions about that as you guys are going please contact me several of you have made up appointments to talk about the project and so forth that's new evil 13th so that's coming up alright so we're going to talk about we're going to talk about the markov chains today and this is the first part of the lecture on the section on graphs and markov chains are so it was heard of a markov chain before ok and this and who's not out of it ok that's so which classes or where did you learn about Markov chains just of curiosity and like okay class here or do you hear were just you were just browsing Wikipedia one day you're like there it is on in a sort of project okay yeah so cool okay so I fight even if people have heard of markov chains they usually heard one version of them and there are really two different views of markov chains so we'll talk about both on try to make this clear if you take the promise of modeling class 6190 they use this Markov chains in one way and will basically be unity will talk about them for inside page rank which is a really cool technique for understanding graphs and we use them in a completely different way but it's the same fundamental concept here okay so they kind of have some fun with this I like to say that they're they're kind of on so this this lecture there's going to be a lot of definitions of what is a Markov chain why is it is it interesting have fun they're kind of three so they're kind of three lessons that you can learn out of this so only your only your current position say in life matters going forwards and so don't worry so about the past okay so only thing pop think about where you are going forwards okay and we'll see each of these in a lecture so and so so don't worry about the distance um future um go one step at at a time so if you go one step at a time you'll get there eventually where you want but don't don't worry about things too far and then finally so I'm so funding the limit then everyone has so perfect so everyone has perfect car okay so everything will be treated favorite people get what they what they give okay so the tools will see these lessons in in election okay so this is a review of graphs and some of this is kind of partially have tedious to go through so I've got some slides to kind of go through some of this and then we'll go back well alternate with this and then going through kind of comedy okay on the I'm writing out but just read you this is the favorite graph from song the spectral clustering that didn't remember in this graph use a bunch of these nodes and you've edges between them and so you know you can describe them in a variety of way and we'll be looking be convenient to think about kind of all of these ways but most you know this pictorial way gives you a lot of intuition but for very large graphs you can't you can't often draw very good pictures so there's this graph view of it for this dis matrix U of it where every node corresponds to a row and a column so it's a square matrix and there's a 1 if there's an edge between them so a and we have an edge between them and so there's an entry here and so this graph could be on this graph arms so the scrap might be directed it may have an edge from from A to B but not from B to a but in this case I'll just worried about the undirected case everything will go through if you make this if you make this direct it and ok so for simplicity we'll just worry about the undirected case today ok now a Markov chain is going to be you can drive it from a graph and this will be kind of the view of it that will use within this class and so this Markov chain comes with these these is mainly from the graph it has this node set which is again the nodes of the graph and then it has this probability transition matrix ok now this is going to be this matrix here and so you should be able to drive this from the adjacency matrix that we had for so what we did is we took the adjacency matrix and we normalize every column and there are multiple ways of normalizing in this case we wanted so the sum of all the entries of the columns summed up to one so i divided there were three ones in this column before so that meant a was connected to three other nodes the C and D XY divided by three so that one-third plus one-third plus one-third is equal to one and so forth whereas the is connected to two other nodes so i divided the two ones by two with the upland have some steak okay and so you can think of this as a it's called a probably transition matrix so it's not just an adjacency matrix where can you go but you want to say from every node I'm starting up I can go to a random one of my neighbors I pick one at random and so then if I'm in se1 States and in position d well I can connect to a b and c a b and c and each of them i go with one-third probability and so that's what's going on so the probability had transitioned to one of these other states it's okay the second to transition back to yourself I didn't have any in this example but you can have these soft loops as well so then if d could also go back to itself at the one fourth one fourth fourth and one for that's also fine okay and then the third part of this is going to be the initial state of this Markov chain okay and so this is this vector Q the spectra q tells you a state and so this can mean a couple of things and give a couple of examples up here the first one is saying i'm at state B I'm in location beef amended would be here because I met heavens it's all zeros there are eight entries in here the same as the number of vectors and I have a one in position be saying with probability 1 under position be both probability zero i'm in on any of the other states so i know i'm definitely a position beating but it doesn't have to be all in a single state you don't have to always need be at least in the in one of the views of markov chain and so you could be that ten percent of the time I think I'm in a I'm not sure I'm not to be undone see thirty percent of time hun indeed I'm not sure it and also sixty percent of time I'm going to be in F that's the point six up there so this this probably this initial state should also be normalized so they sum up to 1 think of this as a probability distribution over where you are your state and there so you know in the most general case both of these are valid will be some settings where we only want to consider versions we are always in exactly one state and we'll see kind of some some variants of these okay so that this is the setup of a markup change so this setup is these three things the vertex set the probability transition matrix in the initial state and the first two you can drive from the graph and it turns out for certain special types of graphs which will or special types of dnp probably transition matrices the initial state won't matter so much and if your graphs have this property have the right properties then you won't need q that means you can drive all the parts you really need directly from the ground so then you can save things about the graph using this Markov chain okay so now what is the idea of the Markov chain is all about this idea of of transitioning okay so you want to say I'm in one state say I'm in this initial state while then beat I'm in hundred percent of a chance in B and then I look at this probably transition matrix and I say well after one step I'm now going to be in either a or D right so my new state so that was a huge ero or q q one after one step is now half the time on a half the final M&D okay so you have a new state you can always represent it as this probability distribution over these discrete vectors over these discrete possible vertices and this is just a vector and it turns out that if I wrote this as a vector matrix notation because this is exactly this effect this matrix P times this times inspector cute so if i multiply them together i'm going to get a new vector here it's actually a column vector but fur it's more an easier compaq to write the Rove Ector so i put the transpose of yourself so this is actually going to be a comment okay so um what happens next I I took one step I'm Restless so I keep moving right so I take another step if I'm in state I'm either at stake a or D and so if I'm at a that one-third of the time I'm going to be B C or D and so that goes into here but if I Mindy sometimes I go to to be and sometimes two sometimes to a sometimes to be sometimes to see and so my state is now spread out it's if i multiply this by this project probably transition matrix again I'm going to get this new vector and again these should all sum up to 1 so well and I could write this in a variety of ways it's either this q1 state where I was after the first step times are probably matrix and I get q2 or i can say Q times P times P again my initial state and then I applied the probability transition matrix twice or I've Q and then I apply the proudly transition matrix square you can actually square these matrices square it and then and then just apply at once so if I want to know where we go after two states I can create a single matrix this is now again and n by n matrix and I just have to apply it once okay and then I make another step again and again there's a variety of ways i can i can write it it could have been an a B C or D and the thing is now from state see there is a chance i leaked out into this other side of the graph which was state e right so one-ninth of the time so 26 times so 26 times one-third is going to be one ninth thing is that right 127 is why it should so 26 times one-third should have been to 18 218 619 so I think I'm doing all right ok so those states see that could get out to eat Mackey if you remember back to my graph here steve was connected to eat and so this worked out so i get i can represent this probability solution as a state and i can either do this by multiplying by p or it could have been q times P times P times P or Q times P to be cute p to the third all these who have given me the same answer and so this is basically how this Markov chain works and so the cool thing and benches and then we'll go back to the tablet is that in the limit you want to think of as I don't want to just do this three steps I want to keep doing this kind of a large number of steps I want to see what happens from some initial state and I take P to the power head or I hit it by this privately transition matrix some and some large number of times I'm going to get a state qn what happens when n gets large what happens to this vector if the good thing is I know it's always a probability vector the state's always the problem is always some 21 so I'm not going to get these weird cases where one of these terms goes off to infinity that can't happen the maximum of any of these values is one so it should be at least somewhat well made what's going to happen as I keep keep applying this matrix over and over and so that's that's what's interesting that's what's that's where things get interesting okay all right so let's go back to tablet and then we can any questions about these things like it okay so so just to review to put it up here we're going to have some saw a Markov chain is going to be V which is a node set and roll yeah we'll say label that soapy pull will say use n for something else so let's say that this is going to be and is usually number of edges let's save ok and then p is going to be an m by m m by m and by a matrix B also the probability probability transition matrix and then Q which could be Q 0 is going to be the initial state and so what's going to happen is going to be Qi is going to be lets say qi plus 1 is going to be easy that's qi so I'm going to keep applying this matrix to each state and it in order to get this next day and I'm going to keep getting these these uh and so I'm going to have that Qi is always a a probability distribution on V so it's always a probably just phase it's of where I am after after I steps okay and so this this property is this l1 lesson okay so the idea was that you you only need to worry about your current state going forward in order to describe everything all of your current States or future States I didn't really need to know what q0 was as long as I knew with Qi was if I knew the value of of Qi then I can give state qi+ born and every future stay if I keep applying p I didn't matter with the previous tables and so this goes to when you have some some process that only depends on the on the current state then this is called a it's called a Markov process it doesn't have a history to it this does advantage that if you're working with this you don't need to keep track of all the old states you're going forward you only need to keep track of your current state and so in and then p is described by this graph so you have the graph as your influence okay so this means to go to understand where you're going to go you only need where you are now and and then you can just iterate for okay so now i want to before we go into more kind of details and mathematical properties I want to stop and say there there are two views of these Markov chains there two views of it so um so the first one describes a random walk on the ground ok so if I have I have this this graph right but I have this graph and what I do is I starts at one note like this was like our initial safe be and then I'm walking to one other note at a time okay and you you always are exactly one state in exactly one state okay so in this case your your your initial vector in this case q always look like zero always look like this right there was always all zeros the other view of this is the squeeze this the second view is time but you're keeping track of where you might be okay and so so in this state Qi is going to be dense okay so you couldn't be at and many becomes it is a probability distribution of the random walk so you're not always in one state at you're not saying one stays what kind of a quantum view of this right where I'm saying I know it started in some place but let's say let's pretend i walk forward randomly for a while we are my tidy after chapter 10 states right or after ahead steps right so you've you Qi as p to the i of q0 you view it is i I know where I am but after I steps where am i where am I likely to be described a probability distribution okay and and so um and so these give rise to two different kind of computational ways of thinking about what this Markov chain doing okay so that this the second one is what else describing and what I showed in the slides where I keep applying this probability transition matrix to a state right for this was equal to P of q pi minus 1 right and so at every step I have this problem efficient and I advance the probability distribution that's kind of an algorithmic process the other one is this random walk where I'm not maintaining the dance q I'm only maintaining a single state at a time so my representation here is much sparser so I only I have the graph in the background but I I don't maintain a full probability distribution here I just maintain a single state right so if there n states I need login bits to describe where I on for instance okay and so yes oh so like I guess both the red and walk be used for British like SAP what pals around the blocks and maybe thought yeah right so that's exactly do right you you sample a bunch of random walks and then you see where after a thousand steps where each of them end up typically so i'll talk more about this and after I go through some more definitions there's this this approach called see either the so the metropolis how far though or often it's very related to Markov chain Monte Carlo and basically you're maintaining a single state at a time and you and instead of just running a bunch of these genes independently what you do is you run it for some number of steps and then you just keep track of all the steps as you go so you instance so you maintain a list of your history of where you did and you use that to kind of and then you average out all those steps and that gives you an estimate of the distribution we'll talk more about this one thing cool about this is that sometimes you don't have something like a hard graph here but you describe this probability transition matrix as a function right so now Qi is equal to some function of q i- cool and so I I don't necessarily have to store this it doesn't necessarily come from a graph it could be that this function is just applying this this this P matrix but it could be something else right or but in this case I always maintain this is the single state so I just have some way to transition from the old state to to a new state and so this can be much more general the state space that can actually be continuous instead of discrete I don't have to have notes of a graph i can have any say point in a high dimensional space and we'll see how how this would actually make sense but saw path so okay up of those of you who had heard of Markov chains before enter and they're still wait are how many had heard of the first type of our coach okay and how many heard of the second time and how many heads really heard of markup cheese okay and how many raise your hand when I first said you heard markup James but didn't raise her in this okay okay how many still did with you if you are okay good we're getting pretty close down see where for converging okay so that these these them is get two very different algorithms for approaches so PageRank is going to use this approach most of the Asian statistics uses this approach computational vision statistics but the mathematics and the theory behind them is going to work the same and it's easier to analyze them in this framework to think of them in this way and you can see that this state this describes a probability distribution of where this guy might be right so this is very useful and understanding discipline okay so we'll talk about this one now then we'll come back and talk a little bit about the metropolis algorithm and then on Monday we'll talk about page rank which uses this view of it again to analyze growth and it'll be more than just like page rank in some extent is okay just do this and then qn for large enough n is the page rank vector that's it but there's actually a much more fun story about there's a simple way to model grasp but then there's this controversy or this competition between google which is trying to model the web graph and and spammers who are trying to spoof this spoof this model and how you kind of do some extra stuff on you so you change this matrix a little bit to kind of be more be more robust and so there'll be a little bit more story than just just to find alright so in both of these cases what you are interested in so you're we're going to be inter interested in qn for very large okay in fact what does qn look like as n goes to infinity as n goes to fit it was it's the circle and we want this to be something stable but in order to to actually define this I need some more some more definitions what this looks like with this qn with and we'll call this then we'll say q n equals on q star and as as as long as a the graph for the piece at sites are the properties then this Q star is going to tell us a lot about the underlying probability transition matrix on the graph it's going to tell you if you're randomly walking around the supermarket which I'll are you most likely to end up you would have any suggestions probably not the frozen food aisle because you're too cold so so probably but cereal it's quite a good i'll write it okay um that didn't make any sense okay so so what we're going to talk about is a proper property of these markup jeans we want Markov chain to be this property that's for Dennis property that's that's called forgot so the Markov chain is organic and the definition is a little technical oh so let me write this write this down and then it looks like it's too complicated like you should be able to say something similar but i'll explain that that's not to kick so Markov chain so an M Markov chain is her ergodic if there exists some tea which is on which is some number of steps such that for all N greater or equal than T then p to the n is it's positive in every entry so this doesn't really so the Markov chain technically has this initial state this initial state q0 but this doesn't really depend on that it should hold for any initial state and so it says that PN is positive and so positive you can think of as has non zero because they can't be negative it's a probable the property could be 0 right if it has to be nonzero for um and society and so this is now I'm worried that math Mac they got this right it it probably has to be sufficiently far away from zero actually but let's just think it's it's it's positive here um as for all possible states so it could be that there's some state the future where it's all nonzero but then it zeroes out again that that doesn't count as it too but I can't have at some point t it's it's all it's all non zeros and then it goes to 0 again I can't I can't have that be possible i have to say that for all larger number of steps it's always going to be guests it's always going to be all nonzero and what it means it's all nonzero means that from every possible state I could go to I could go to every other state that means after if I from any initial state right so if I look at this now q 0 so cute n is equal to and 20 right so from any initial state it doesn't matter what it is because it's all positive I have some probability of being in after eqn this is after n steps I have some probability of me in every single state and every time period after that for any initial state because it's for all entries for any initial state every time after that there's always some chance i'm in each of the states okay so I've always has some chance to be anywhere on the graph as I'm wandering around it doesn't have to be large but it has to be bounded away from 0 okay so this is what this is what it means beer guy okay so if i have if i have a graph which is going to be where it starts with an undirected graph so if i can go from A to B and I can go from be back to a and the number of edges is finite and it's connected then I can I can get I can then this is going to hold once i get to a safe i can always get back to where I came from it may not be a high probability but it's finite because the graph is fine yeah so that means nothing he's and so that means that this qn is going to go down to he's going is is not going to go down to zero or anywhere okay so this this is a little bit complicated to understand we does this hold so with this or guide property holds I'll be able to say a lot of cool thing is about the limit of this this qn stink as n goes to infinity but this definition is a little if i give you a markov chain how do I look at it and know if this is true it's easier to say it's easier into a calm when it's not forgotten so it's not forgot it only if it's only if one of three possible things happens if it's what's called if he if the grab so if the graph is what's called cyclic and i'll define each of these if it has absorbing and so transient States so if is if i can divide the states into absorbing and transient and if it's so not pause if the graphic if the graph is not connected or if the markov chain is is not connected ok so it can only not be organic if it satisfies one of these three properties and each of these is going to be easier to understand so that I can check if it's cyclic if it has absorbing transient states or if it's not connected and if it doesn't satisfied any of these then I could say then then ask your gap and then I can and now i can say some more properties about the limit ok so let's let's go and define these so the first one actually this will be easier to do in pictures again here okay so okay so cyclic example so cyclic is going to mean that the graph only consists of cycles of some length that's that's uh that's that's that's greater than one okay for that the greatest common divisor of the cycles is Islam is always larger than than one so in the first example of here I have two states a and B if I meant in a I always moved to be if I Mindy I was moved back to him and this one I three states at five min if I'm an AI was moved to see find that be I was moved to a and if I haven't see I was moved to be it's a cycle of length 3 this one is a bit more complicated let's see what's going on here I can't draw a picture on this version but I'll draw a picture of this in a second so this one this is state a let's try it and he can go to DC d or e but not F and have to do the same and states b c d and e can go to States a or F so what's the length of the cycle Liz the scrapbook you have my tunes themselves the portals in the center all have get in there yeah so um so let me draw a picture of this one here I wish I can't figure out how to have one thing at one side without I don't fill you with an hdmi cable I think they were so let's try and draw a picture of this one so remember the probability so this is cyclic probably transition matrix going to be 0 1 fourth one fourth one fourth zero what half this is all zeros here okay so what's going to look like is a and F are going to be over here and then b c d and e are on this side and a can only go to b c d or e and similar from f can go here and then these edges also go backwards right so I i also have going back this way right so be can go into either a or F and equally likely right there two edges so this graph is known as arms so bipartite there are two sets of notes in this case a and F and that b c d and e and so the ones from the set a and F can only link to the other side and vice versa there are no internal links here no internal links here and so then this is a cycle of link 2 right I'm either in one of these states or in one of these days all right so if I started in an initial state I'm always an A then if I go for seven steps forward can I impossibly beam it if I if I start today after seven steps is there any way I can mean it what's my probably being zero was probably be in half yeah and what's the probability of being an easy dearie twenty-five percent yeah and if I go nine steps it's the same I go 1023 steps it's still the same so I but if I go 1024 steps well then the problem is one-half right so I'm not really convergent I'm alternating between these two things so this is this this case of the these are cyclic and the grass could be the cycle can be linked to three or four but I don't ever converge tucson i alternate between tuesday's essentially or maybe three states in the case of the EDC breath the second exam okay so cyclic this is one case that is not organic let's see the absorbing and transient States let's see some examples of these ok absorbing a transient we're all right um good all right up let's try this one more time okay good so let's look at this first example here what's happening here if I'm in a half the time I'm staying in half the time I'm going to be and if I'm in b1 staying in d i'm not going anywhere right so so if i start a day I'm slowly going to or actually fairly quickly can end up and be in that I'm stuck right in in this case if I'm at a I go to be if I Mindy I go back to a that seems fair but if I'm in if I'm in state see I'm going to hey I'm going to be going to be I can't ever so between a and B that's that's fair but if I don't see i can if i leave see i can never return I can never get back to see so the bottom row here is all zeros right so I can never get back to see if I leave in if I'm in cm going to leave ok and now this one what's going on here so this bottom one if I'm an A or B I kind of usually go back and forth except sometimes if i'm in need one out of a hundred times I leak out into Stacey if I don't state c d e or f I kind of I can go to one of those other states that I can never get back to air beat in this case the probability of eating is very slowly one out of a hundred times trickles out into c d e and f but it can never get back those kind of leaks out here right so this part is you know this part it's like the Hotel California right once you go in you can you can never leave right so it looks looks very fair even in there so maybe that's good but but you can't ever get back out I want to get stuck right so these are called the absorbing states in this case bcde are the absorbing States a and B are the absorbing states and b is the absorbing states from the first one because they're absorbing all the probability and the transient states are in this case a and he be there and a in the first one there transient because you're kind of wandering around but they eventually they're in transition they eventually leave ok so I know it's transient like a non politically correct word for a person was that that just describing their state ok so maybe maybe there are a lot of transient people who wind up in California because the weather is great that's a good way I think ok alright this this makes sense these example so if they're transient you're you're not you're not getting organic because the future if you have transient States those are not getting the probability so then the it could be that the convergence depends on the starting state let me go back to here so let's now look at some transient example absorbing and transient example here and let's look at two states a b c and d and let's say that a and B go back and forth but that a goes to see let's say one tenth of the time he goes to D say what half the time and so let's say this is 84 tense and that this is one half so let's let's look at this example here the plum is if I start one hundred percent of time in hey then I'm going to get a little bit more than I'm going to get something like maybe fifteen percent or so of the maths ends up and see and the rest wind up indeed if I started with more probability in B then I'm going to lose more than 2d unless of its going to wind up and see because i leaked more of it to d before it and get to hand out to see if i started with all my probability and see it never leaves so these these um these um depend on cute 20 these absorbing and transient States and so it's it's harder to analyze them because think they can depend on q0 in strange ways and so you want to take that out of the equation you don't want to care about where you start so that's why these are not going to have this this organic property well is there an ethic organic property because eventually the probability being a and B is going to go down to 0 I mean up the exactly 0 but it will never be eventually will not be founded away from search and then the last case let me just show some examples of that not connected he should be much more obvious so these unconnected ones here they always stays in a he always stays in B a and B go back and forth but see stays by itself right so before the absorbing transient he gave everything up to be before now he's like now he's like screw that i'm just going to keep it for myself fair enough and in here um Amy talk to each other but don't go outside of their be C in C D and E talk amongst ourselves it's not completely even right so d does not ever go to back to itself well c and e do but that doesn't matter they can all get to each other in here and then half doesn't talk to you what else either right so these these are not connected so these again it depends on this Q 0 depends where you start your initial state and that really dictates a lot of what's going on and so these also week we can't analyze as well all right so okay so that these are okay so these are the categories each worth considering for it not to be organic if it doesn't satisfy one of these three properties then it has to be organic and now we can you know actually actually get down to business it's also useful because we just if we wanted to design our probability transition matrix to satisfy this organic property we just need to worry about these three things and so when we tomorrow or Monday when we talk about page rank will need the web graph to satisfy these properties and we'll see that there's a simple trick you can do that that's going to change things a little bit that's going to make sure it says I this this it's this property so keep these in mind all right so now we can say let's consider if it's if our markov chain is going to be our guide then all we need to consider then we then we only need p we don't need this initial state and we need this probably transition matrix okay um and so we can say there's some really cool properties and not going to approve all but I'll trying to this give some intuition so we can actually define so there exists a P star which is equals to P n as n goes to infinity okay so that means there is going to exist a matrix so Q star state is is our converts to state and so this also must exist so this also exists and it's going to be equal to P star for cute right and so I didn't need to specify what Q is here it could be this is true for any for any cute so it doesn't depend on my initial q there exists some matrix for any Q I start with transitions be directly into this converge to state okay and there is a state that it converges to hopefully that those shipments somewhat there from it for a be organic as well so what this this has a few cool implications um so so for all q0 if we run to the Markov chain long enough then Qi is is going to basically be cute stuff so we can just run this Markov chain long enough by just repeating Lee applying this matrix and eventually it's going to look like this converts to state so and this is uh so this is kind of this uh this hell to property that so if you if you take it one step at a time long enough right then you eventually get you know the place you're going you'll eventually get in there in this case it's you know this equilibrium state so this should have something to exam so okay so hey and so and also this is true so this is true which which either approach we take either this technique this is true either for if for Qi equals p2 qi minus one or this this dysfunctional approach where Qi is a so random walk step from Qi minus one where you did a random walk step and you mount up in a new state and you look at the distribution of all right so either of these you'll eventually either that the expected value of the states for exactly the distribution you get here what eventually reflects you star okay another cool thing this is that um q star is equal to P times P star that's cute right so if i was in this converge to state so if i if i hit it by another matrix 1 there are two ways of viewing this this is P star plus 1 right if I've converged to after multiplying this by some large thing then if I take one more step I'm still converged but the other way to look at this is that this is Q star that means that Q star is going to be equal to P times Q star so that if I am in this converge to state and I take one step I'm still in the same state and so that means if this is a probability distribution and I take my original matrix not this one that's been converts to that I'm going to still be in the equilibrium distribution I don't they don't change this at all um for another time view if I'm doing this random walk approach of karma right I'm going to go from one node to another node but some other nodes are going to get me back the same amount that loves right so if i if I'm in this these these states here I'm going to have to see that I think as a random state and then I take one more step I'm still in a random state I'm still at the state from the right distribution wetzel comes this is is uh is great that means it once i'm in this equilibrium state if I'm you know if I get this equal distribution it's not going to converge to being the same puddling on the nodes but then i'm going to have some i'm going to leave this note here and jump to these other nodes but the same if I'm in a equilibrium States I'm going to have the same probability being there the next step because I'm going to get probability mass coming back from all these other boats as well and they're getting even at and so this is kind of cool right think of this as the amount of pressure in an in some system of water flowing around right or what this action will map to is there's a connection between the the current electrical current right through a system this describes how much I forget if its current or voltage or one of these things i'm not an electrical engineer but the current or voltage is described by this equilibrium of this process it's related to this so there's more current running through certain of these than there is other based on the configuration of the network ok um so ends so that means in between two states so you're going to have some property that let's call these states I and state J n sub P IJ is the probability of going from I to J right and so then this means I'm going to have the probability that Q star of I and then going from I to J this turns out is going to equal to probability of going from J to High Times this this equilibrium matrix from state J so the probability i'm enje and I go to I it's going to be equal to the problem I mean I go to jail these two values by themselves are not equal but you multiply them by me this and in there they're going to be okay so if you think I can just look at these and then I can solve for this value but the trick is you need to normalize these across all the matrices at once you kind of know these on to certain value but normalize them in the solving them becomes more more difficult this is the cool property again of this equilibrium state lets you start to see there's a lot this kind of equilibrium this there must be a lot of structure okay so ok so now the coolest thing if you hadn't guessed is Q star is the first eigen vector of p so after if you yeah if that's normal yeah so it turns out it's the first eigenvector so if you like effect remember these related to the this the right or the left singular vectors of the matrices if you make I convectors the matrix needs to be square p in this case is square so if you do the L I of P in MATLAB then this will give you remember or the spectra of fostering much sugar this will give you the eigenvectors is the columns here and these are the eigen values which are important the hugging vectors are tensile then the one is this matrix me so this is the first eigenvector it might not be normalized so you may have to do Q star equals v1 divided by some D 1 so that this will this will normalize it so as a proper thing I think these are normalized v1 is or was so it's unit vector that means the sum of the squared elements is one but you want the sum of the elements as one so you need to know that's it this way so now this Q star vector is going to be exactly on the Q star right I wrote this out in letter so you can copy this directly into matlab you don't want to put a star there I think that all they interprets as a as an operator okay so you can calculate this converts to state by just doing the eigen decomposition and taking the first eigenvector and so this should make sense because i have this property up here if you look at this property this says that it's an eigenvector if i multiplied it hi this probably transition matrix and I got back itself that's the eigenvector property again means something like self and German right so you're getting yourself after appliance and it turns out the first eigenvector is it converts to state but I'm getting myself back okay so second the the first eigen the second eigenvalue the second eigen value is the important I ghen value in this case so this would be the second one this determines the rate of convergence okay the rate of the convergence is determined by the second eigenvalue what's happening is if the first eigenvalue is is a in this case with the proper normalization is one right then then this second eigenvalue in this case is going to be um yes so if you look at the graph i use to me in the initial example on the slides but the eighth notes let's draw this so i have an a/c i think you draw this graph here a CB d II tough hitch but so if I start with this graph and I do the normal process then what I'm going to get here is an example is the first eigen vector with the first eigenvector so v1 is going to be equal to Q star is going to be equal to 0.1 50.1 through a point 150 15 0.15 0.1 0.05 okay so fifty percent of the time i'm in a a CD or eat ten percent of time I'm in D or F and fibers of the time h right so I'm more likely to be in these states than these states are H I'm an H if I'm NH I needed to leave and if i get to f i can get sucked back into this other more dense dense and connected times and if M&B i get sucked into this more densely connected part denser connected parts tend to the kind of grab up the mess the more loosely connected parts that are pulled away and have less of the mats that's roughly how to think about it it's if you can you can get more complicated okay and then the second eigenvalue lambda 2 in this for this example is 0.875 ok so the the the smaller this is the faster the rate of convergence smaller equals faster so kind of as I'm doing this some of the probability distribution tries to fit towards this one and some of it tries to fit to the second one and the third one and the fourth Lancelot so that this is corresponds also to another pagan factor which means that if I was in if I was in that state confined to the second eigenvector I give all my probability distribution back to myself again that's also a an eigenvector rights whether the problem the probability is not leaking out but it's unstable if I'm anywhere outside of that if I'm anywhere else I'm taken to the the more stable state which is the first eigenvector and so mean but there's some part that tries to stick around in there and it's like of them of the the probability that's stuck in the second eigen state 87.5 percent stays there and something like twelve point five percent is leaking out of your tongue that's essential at this number six okay and Whitley sout but if it gets to the first eigenstate it stays there that's kind of how to interpret this number the third eigenvector is can be even smaller so it keeps even less of it state okay so if you compute the eigen decomposition of the matrix and you can look at this value this tells you how fast is converging and so typically a convergence analysis you look for things like this where there's some small cut and it's hard to get from this part of the graph to this part of the graph that's usually of a cause of small convergence a single edge or a small probability state that's causing it to kind of spread so if you start with all your probability distribution here it may take a while for the right amount to get over to this part because there's only a small chance of transitioning each time so that ends up being the these sort of things end up being the bottom of the convergence so if you can draw your graph when you see a very dense part in a very dense part a small ability edge then that that says this is slow convergent if you don't have that if it's fairly well connected then you're going to converge really fast and this and two things that's probably what that means this will be pretty uniform and this will be pretty small if it's fairly welcome okay so yeah okay so this is what I want to say this is what you need to know about Markov chains so there's some pretty kind of cool math lots of connections to different stuff here and so should I only left like six minutes for the metropolis algorithm okay so we'll see this again on Monday with page rank let me give you a little bit of color a little bit of the metropolis and so this was invented by a guy named metropolis and then to husband like pears Rosenblum Rosenblum teller and Teller in 1953 so this was really the beginning of computational statistics and this was part of of of the manhattan project where they were trying to build the first they built the first nuclear bomb and they did this they needed to understand the probability of certain physical states and they did have a way of calculating directly they wanted to simulate these to try and do this and so the probability distribution of these these of these these are these physics of states they could they can do something where they ran they knew the forward physics so they knew what happened if I was in a state where is all the force going where's the force going and it forcing me to go on the next step think of where is an atom going to or an atom or an electron where is it going to be after one step based on on the force and you can write down some simple equations and then they the very very early computers this is like one or two steps after the ENIAC or something and they were running some things like these these markov chains on these computers where they could run these forward one step at a time and they could estimate these product descriptions so and then this was later so that this was by metropolis rosen blue roses with teller and tell her in here and then there was so and another big thing was eastings in 1970 so this kind of connected this to these Beijing statistics invasion statistics people can write down these these are these relations based on on on Bayes rule to model something but they didn't know I calculate those probabilities there was often known closed form but they could say given a current state what is the the the I can make a sable what is the likelihood of a given state I didn't know how to normalize that probability and so this allowed you to normalize that probability essentially and so this opened up how to actually work with beige in statistics there's some follow-up work by gamma and gamma and Delta and Smith and late eighties that eventually made this fun to do this in much more practically okay but the idea here is that you have some state space V and you what you can say is for all elements of the state space you can say eh oh wait WV and what you know is that where the property and and let's say that if i sum over these that over these weights this is going to be uppercase W and and this state space could actually be continuous okay so it could be and then that some is an integral okay and so what you know is that Q star of V is going to be w of the over w you can often set up these mathematical properties like so think of this as the likelihood which you can calculate what you don't want to calculate this sump this would take too long there could be an exponential number of states or this could be continuous state space you can't calculate those but then this would be the essentially the probability of being in the state you just don't know how to normalize it so I I'd like to call this a probe only distribution we can evaluate the likelihood but you don't know how to how to integrate over okay and so now then algorithm there's a very simple algorithm based on work of Genesis where where you have a problem you don't have to probably transition matrix you have a fun which which allows you to do this okay so you start so the metropolis algorithm you start anywhere as your as your initial state q0 and so you initialize it and this should be and it needs to be so it's on you're always in your in one specific state and so what you do is your going to have essentially a very large loop where you repeat you generate some say some kernel like a Gaussian kernel you generate some neighbor of this date so you pick a point from a Gaussian distribution which we talked about how to do this can be high dimensional so this these states you can think of these states instead of being a discrete state can be some high dimensional value right so this could equal v say and RD and so then I i I'm going to pick some neighboring state or this could be a neighbor and a graph right so you is going to be a neighbor of the and doesn't solve matter that is quite so much how you pick the neighborhood except for the rate of conversions and so there's a lot of work and how to pick good neighbors so that the rate of convergence is good and so now if the weight of v is greater equal to the weight this is way too few of you is greater equal than the weight of VI if this greater equal to vi then you set VI plus 1 equals to you so if the weight is bigger i jump to a higher life estate I always move there on house with probability wau over WOD hi I set the I plus 1 view so if you has a lower probability so this quantity is less than 1 I still sometimes move to you I still sometimes move to you and a proportional to that probability else as a VI plus 1 equals to V I if not if it doesn't happen I stay where I am I don't move okay and you just keep repeating this until is converged and then you have this states set of states v 1 v 2 up to VZ and after so many steps and this estimates your probability distribution this gives you should look like a random sample from this distribution so technically you're only supposed to use this last one you run this for a while to use this last eight that's that's to be should be drawn roughly from Q star typically what you do is you divide this up into a paint burn in period say that usually they use the first like a thousand steps here and then they run for six thousand steps more and they keep those so the run for like a thousand steps they throw these away this a roughly gets you an approximate state and a random state and they use all of these massive eight the distribution and this is usually what is done there's a variety of other things around us I won't get into but this is the metropolis army and the good thing is you only you could you just need to be evaluate these weights and they have picked somehow something in the neighborhood for whatever that means it doesn't matter even so much how this happens but because functions tended have similar waits nearby you're more likely to accept a new state if you jump nearby the population have changed too much okay so yeah cool so I don't even need the probability distribution I can do it on any function I can evaluate this unnormalized weight say a likelihood function in patients that is okay so this is probably this be very different than the PageRank algorithm will see you on Monday okay so I've run out of time but feel free to ask me questions but I'll let the rest of you go yeah so you can calculate something so in it is that if you use the same physics what you can do is you can calculate like the Boltzmann distribution the Boltzmann is something like e to the minus the energy of the states and in that sense objective tense the correspond with the probability you're going to be in that state now that energy you don't know the integral of all the energy of all the of all the possible states so if you so to get the probability here in the state you use the weight is the Boltzmann distribution so this is using statistical physics for instance in understanding protein protein folding they use the weight is the Boltzmann distribution of the energy you calculate of the protein configuration it's like you're trying you don't actually need to integrate a difference yeah so that this is kind of in anything a random sample from the Boltzmann distribution yeah and and that allows you to estimate the integral then to escalate the integral over any meeting you just count the number of states that falls in that region cool all right that's a cool stuff 
6GoPjKnSN5Y,24,"Dr.M.G.R Educational and Research Institute University, Maduravoyal, Chennai, Tamil Nadu, India-Faculty of Computer Applications is Organising 5-Days Faculty Development Programme on ""VALUE ADDITION ON EMERGING INFORMATION TECHNOLOGIES"" From 10th August 2020 To 14th August 2020.",2020-08-14T18:55:09Z,FINAL DAY FDP- SPEAKER-Dr.P.THIYAGARAJAN-TOPIC-DATA MINING TOOLS.,https://i.ytimg.com/vi/6GoPjKnSN5Y/hqdefault.jpg,Faculty of Computer Applications,PT1H16M27S,false,278,8,0,0,0,good morning to one and all gathering this is the faculty of computer applications the creator educational research institute motherboard this is the time to welcome our compiler mrs nithya aaron kumar to take away this session thank you thank you ma'am pleasant morning everybody myself nitya assistant professor faculty of computer applications dr mgr education and research institute motherboard chennai knowledge is like a mobile application where we need to update often the main motto of this faculty development program is to update our knowledge in the end nothing is lost every event has good effects forever as we stepped into the last day of five days faculty development program on value addition on emerging information technologies we would like to share our university video clipping followed by our department video [Music] [Music] [Music] hello [Music] me [Music] me [Music] me [Music] oh institute university [Music] so [Music] [Applause] [Music] so [Music] baby [Music] so [Music] thank you ma'am now i'd like to invite poster elementary and assistant professor faculty of computer applications to welcome the gathering thank you nithya very happy good morning who are connected here in this event on this occasion i took the privilege to thank our chancellor through acs chanwagam sir our beloved president engineer is vice chancellor dr gita lakshmi madam for giving us an opportunity to conduct this five days faculty development program to enrich the faculty knowledge to strengthen our department our university is great a lack accredited nba operator aict abroad ugcc approach aimed to be university with a graded autonomy status faculty of computer application is one of 13 different faculties of our university under this faculty we offer general and a different industry collaborated undergraduate courses in bc postgraduate course amc and research oriented degree courses young phil undergraduate with this brief note on us on behalf of the faculty of computer applications i welcome the pillars of our department dean dr yes h.o.d dr vijay vinod madam additional hivs dr ms joseph on behalf of the faculty of computer applications i welcome our chief guest of today's professor department of computer science central university of tamil nadu and thank him for accepting our invitation on behalf of the faculty of corporate publications i welcome the event organizers supporting staffs other faculty friends and other staff members of our department this is the fifth day and the final day of the faculty development program on value addition on emerging technologies from day one to day five all the participants are enthusiastically attending all the events without the participants no even will be succeeded i must welcome all the participants wholeheartedly and thank them to make this event grand success then today topic is a data mining tools i hope you all will enjoy the session and once again welcome you hi thank you thank you so now i would like to call upon mr saladevi assistant professor faculty of computer applications to introduce today's star speaker a very a very pleasant morning to one and all it's a great pleasure for me to introduce our speaker today for the session on data mining tools dr p jagarajam is a professor for the department of computer science central university of tamil nadu india completed his ms from college of engineering hindi phd from pondicherry central university and also successfully finished his post-doctorate from department of atomic energy government of india kalpata his major research areas are machine learning cryptography big data analytics information security and internet of things he has published around 40 research papers in top journals like springer elsewhere ieee and nuclear engineering he also served as a reviewer for few reputed journals he is a founder head of the department of computer science ctn a neural officer of the campus connect wifi project and convener of mhrd institution innovation council of cutn to add prior to his credentials he has completed three research projects both represent file tax and one implementation project worth rupees 5.6 crores for ct cutn campus at wi-fi thank you ma'am for your detailed note on today's speaker now i request our today's resource person dr p tyagarajan to take over [Music] introduction a very good morning to all the faculties who are participating on this ftp hope i'm audible and uh hope you are able to see my uh slides yes sir yes sir okay i will proceed so uh i am from central university of tamil nadu and for the next one hour i am going to deliver a lecture on data mining tools so this is the outline of the webinar so we're going to discuss what is data mining types of data and what is the overall process involved in the data mining various data mining tools and among the various data mining tools we are going to discuss on vca and rapid miner in focus and we at the end of the session we're going to compare weekend rabbit miner and we will come to the session this is the outline of this webinar hope all of you able to see the slide transition smoothly right hello yes sir yes sir okay so okay thank you so much uh so data mining you all know uh it is nothing but extracting the hidden valid and potentially useful patterns from the huge data sets so data mining is all about discovering unspecs unsuspected previously no unknown relationship amongst the data so it is a multi-disciplinary skills that uses machine learning statistics ai and database technologies so the insights derived through this data mining can be used in variety of fields ranging from marketing fraud reduction scientific discovery even in medical fields this data mining plays a very crucial role especially for the cancer reduction and i and also for the kuvit which is uh the pandemic in which we are all facing many issues uh in all walks of life even this data mining will also help us to predict how it will be in future and when it will be under control all things can be done with the help of the data mining so data mining is also known as the knowledge discovery uh knowledge extraction or data pattern analysis or information harvesting these are the different names by which the data mining will be called so data mining you see uh we can mine the data from different types of data so whether it's a text data or web data we can do this data mining on a variety of data some of which have been listed out here so we can do this data mining on the relation databases data warehouse advance db advanced database and information repositories object oriented and object relational databases transactional and spatial databases heterogeneous and legacy databases uh multimedia and steam linking databases text databases explaining and web mining so if we carefully note almost uh all formats of data we can do this data mining and we can hide we can extract the meaningful information out of it so if you see the the steps involved in the data mining it ranges from business understanding to uh deployment so it start with understanding as a particular business so data mining will be done for any particular problem so before venturing into this uh data mining first of all we have to understand what is that problem is all about we have to understand the area about the problem how are you working on the medical data set cancer data set then you should know about that particular cancer type in order to have a thorough study unless otherwise you will not be able to work on this particular data set so you should have the business understanding then after you have a thorough understanding in the field in which you're going to work on this data mining you should have a thorough understanding about the data involved in this mining data mining process once you understood the data which in which you are going to mine it you have to prepare the data so preparation the sensor the data will be uh from any source so there will be some rendering data there will be some missing values and so many other uh irrelevant data will be there so it is up to the data miner who can able to uh prepare the data for the particular data mining process data preparation is one of the main steps in the data mining and almost uh if you see the statistic it says around 60 to 70 percent of the time uh will be dedicated by the data miners to clean the data for the data preparation task it's a very very important task and uh i said 60 to 70 percent of the time for the data mining project will be involved in preparing the data because unless otherwise the data is correct then the data is correct we cannot be able to get the meaningful information from the data because data is a foundation from match we are going to derive the meaningful information so we have to be very sure that whatever data we're doing for the data mining process it should be very valid so it takes around 60 to 70 percent of the time once the data is ready then we will model the data by many of the data mining algorithms is available and after modeling the data the results which is out from the model will be evaluated and finally it will be deployed so this is the six step process which is uh involved in data mining it's a all the stages are very binded so if you see uh there are many many data mining tools that has been involved in the history of computer science so i have taken the five top open source data mining tools uh they have vca rapidminer are python and kind so there are many of course i said there are many but these are the very top mostly used data mining tools so i have listed all these things and in this session for the next 50 minutes we are going to see on weekend rapidman extensively and our python you know these are the again it's an open source programming languages and you know python is coming up very fastly in our race especially developed for this data mining and analytics process and cam is also an open source tool which is having extensive library for data mining so we will discuss in detail about the weekend rapid miner so uh before going to weekend rapidminer just we look into each of these top five tools uh in a brief manner so vika it's a java based customized tool which is free to use so whoever wants to use vca they can able to download it free of cost so that it's not appropriate it does not belong to any companies or an organization so it is free and it is built on java it has extensive visualization predictive analysis and modeling techniques you want to do clustering association rules regression classifications every algorithms which is available in data mining has been incorporated in the weaker the only thing is that you have to upload the data they have you have to upload the cleaner data and you have to select the appropriate algorithms and if you click apply then the algorithms will be automatically coded in the weak car so that the coded part will be updated on the data and only you can see the results so it is a very much sophisticated tool so you need to write any code for algorithms any standard algorithms you need not write the code because the code will be inbuilt in this vcard tool and it is a gui application so only thing is that you have to upload the data and you have to select the appropriate algorithm for the data after we click the ok button up on selecting the algorithms you can able to see the results so it's a very much sophisticated tools because there are standard algorithms which is already built in uh there are many libraries available so it is not necessary for us to code uh for example working on this entry you need not write a code for the decision tree you can simply select the decision tree options in the weekend you can make them activate on your data unless otherwise you are working uh unless uh you plan to work on uh the main core algorithm uh you need not code this decision tree okay and rapid miner uh previously it is called as yale now it has renamed as rapid manage very very popular and it is a again it is an open source ah no coding is required it's a software it has a rich analytics uh facilities been incorporated in it again like vca this rapid manner is also retained in java it incorporates multi-faceted data mining functionality as i said it has data pre-processing visualization predictive analysis and the main advantage of the rapid manner is that it can also be able to interact with other tools for example this rabbit minor if you want to in interact with v car i can able to do that this rabbit manner if you want to interact with r again that option is available in rapid manner so this is a very unique feature make this rapid miner much more popular so it can easily integrate with the week or tool to directly give models and q models from scripts written in form or two so this easy compatibility with other tools like we can r make this rapid main a little more having an advantage than the other tools so our programming tool as you all know it is mainly used for the analytics and data mining this our programming is mainly written on c and proton and it allows the data minus to write the scripts like a programming language in the platform so it is may it is used to make a statistics cell analytic software for data mining it support graphical analysis both linear and non-linear modeling classification clustering and time based data analysis so if you see again in the r uh all the standard algorithms are being available in the library so you need not code you have to import the appropriate package and then you have to use the profit library in the package that's all you your job is done it will be done in matter of fact three to five lines of the code because everything has been already uh really made available ready by this uh programming tool next uh python based orange and ntlk so the python is very very popular open source tool and so now the bigness language slowly uh many of the universities have started removing ce and they are venturing into the python because it's a bigness language and very easy to learn and it's also open source so it has a very very powerful future so whatever you name any contemporary field in the computer science uh you can name a particular package in that python for for example now we know computer science is slowly turning towards the deep learning so now we have a package especially for the deep learning in python so whatever contemporary whatever updated research field is being there in computer science we can able to map packages in the python so what i am trying to say is that the python is being developed by the group of community users who was a dedicated open source developer they are keeping updated this python now and then so that they can able to meet the user expectations so if you see you look back the htf python it is developed in 1990 though it is developed in 1990 still python is able to retain its popularity only because it is keeping updated now and then it is keeping updated it's not an absolute one so whenever things are absolute then we will not look back but uh the main reason why python is so popular even after around the three decades has been established is that it is meeting the expectations of the researcher so whenever a new field has been coming then this opens developer is contributing the python with that particular library for example as i said we have a library for deep learning also which helps the python more popular among the researchers and of course among the students community too so again this python based orange tool is an open source tool that is written in python which which is having a rich libraries for data analytics text analysis and machine learning features numbered in the visual programming interface so ntlk is also a part of python it's a powerful language processing tool so if you want to do some text analytics then this ntlk will play a major role so these ntl cable contents of consists of data mining machine learning and data scraping features that can easily be used for user customized needs the last tool which is very popular for the data mining is nothing but the time so this is mainly used for the data pre-processing that the data extraction data transformation and loading so the chime is a very very powerful tool with the graphical user interface that shows the networks of data node so this scheme is mainly uh used for financial data analytics it has a modular data pipelining uh leveraging machine learning and data mining concepts liberally for building business intelligent reports so these are a quick brief introduction about the top data mining tools that has been used by the researchers so among that as i said we are going to see a little elaborately uh on vika on the rapid miner so we are going to see for the next maybe 20 minutes we will be going to discuss in detail about the vika so this is what this is what the major topics we're going to discuss on vika week what is vika and in vika there are three different paths is that in that we are only concentrated on explorer in the explorer we are going to see how we can pre-process data classifying custom clustering association rules and how we can visualize the data so what is vca actually uh the expansion of weka is nothing but a waikato environment for knowledge analysis so it's a data mining or a machine learning tool which being developed by in the news land and the department of computer science in the university of waikato so if you know if you want to know what is vika vika is also a bird one that is found only in the islands of new zealand so with this uh the university of waikato named this software which is being developed by the department of computer science as we can it has been expanded as we cut to environment for knowledge analysis so anyone who want to download and install vika you can go to that university of waikato cs department and you can able to download it and if you see you have a different support of the week or we want to install in windows uh mac os linux so we have different versions of weka to be downloaded and appropriately or installed in operating system you have to check whether it's 32 bit 64 bits and you have to check whether which ways you're going to work based on that you have to choose the uh download version of vika from the website and you can install it as i said it's an open source so no cost is involved the main features of wika so this week contains 49 data pre-processing tools it contains under six classification on regression algorithms it contains eight clustering algorithms it contains three association rules mining algorithm and it contains 15 attributes of subset evaluators plus 10 search algorithm for future selection these are the things which the weak are incorporated within it so if you download the weak car you can able to do this many uh algorithms are ready available in the week only thing is that you have to use appropriate data with appropriate algorithms then you can easily get the results by just clicking in the graphic user interface these are the big features that has been incorporated in the recap so this is how the week out look like when you launch it so you can see that there are three uh graphically graphical user interface noise nothing but simple cli another is explorer experimenter and knowledge flow so in this presentation we are going to discuss in detail about the explorer that is exploratory data analysis and in the experimenter we have experimental environment and in the nozzle flow it will mainly concentrate on the new process model inspired interface so as i said we are going to discuss only in the explorer part in this weekend so the first uh part in the explorer is nothing but uh the pre-processing the data so pre-processing the data is very very important as i said before selecting the model this pre-processing has to be done so if you see where this data can be imported so the data can be improved imported from variety of files into the weaker environment it can be imported from csv file it can be imported from a rff file it can be imported from c4 dot file binary file it can be imported from the web it can be imported from the xml file so we can import the data into this weca from variety of source files so even as i said the data can also be read from the url or if you want to read the data from the relational databases like sql databases of course it is possible in the week so name any data set available in any uh environment like any different file formats you can able to download the same into the environment so for doing the pre-processing we can also supports these filters and some of the filters are discretion normalization resampling attribute selection transformation company attributes and many more these are the things which is already being available in the v card so only thing is that you have to know which filtering mechanism you have to use for the particular algorithm or for particular data only if you have some that knowledge then you can able to get a successful output from the vca for that you need to have a thorough understanding about the algorithms of data mining so this is the flat file so even this weaker deals with the flat files whatever that whatever maybe the file format in which you're uploading data whether the file format may be csv or the binary if we use uh any of the file format this we can convert the data set into the flat files the flat file is nothing but it is uh which will be like as i uh projected over here it will having the attribute name and it will contains value so this is how the flat files will look like so vika will convert any files into this flat file and can deal with it so as i said uh h so i have listed here two things one is nothing but attribute which is software which takes only the numeric value and i have another attribute called sex which contains a nominal attribute so normal attributes generality uh generally shuffles between two or three three types only so uh general uh gender is always male and female so it is called as a nominal attributes so this is the only file whatever maybe the file format is we are converted to the flat files and you can deal with it easily this is this is how this weaker environment will be like when you launch it so it contains of many tabs like preprocessing tab classified tab cluster tab association tab i'm just looking at the top most column please look at the topmost column select attributes tab and visualize tab second if you look at the second row in this week guide contains open file open url opendb so this second row is always deals with uh from where we can import the data from which of the source file you can going to import the data of the week to do this data mining process as i said it can date from the file you can read from the url from the website you can read from the dbms anything so name a file you can you can open it from the v cup so the second row deals with importing the data if you look at the third row this as i said this weaker support extensively the filter process which is record for the data pre-processing uh it has a choose button if you click on the choose button there are a variety of filters being available in the vca and you have to choose the appropriate filter for the data type okay so this is how the v curve will look like and if we have if you see the left side we have an attributes whenever we say whenever you select a data set there's attribute uh text box which is present on the left side which list the all the column name here so it will in the attributes you can list all all the caller men in the appropriate data set so that will be listed once the file is open then this attribute will list all the columns in the data set and if you see the right hand side we have a name missing value distinct type and unique so it will give the statistics of the data set if i open any data set that status tags mean what is the maximum value what is the minimum value what is the standard deviation because all statuses will be listed in the right hand side okay and if you see at the bottom you have a visualizer if you click this it will just give the particular graphical interface of the attributes which you have selected this is how the wika gui will look like for example here i'm going to walk through each and every step so first you have a open file uh if you click the open file and if you select any dataset by default vika will have this iris data set this iris data set is done nothing but the data set which deals with the flower this is an inbuilt data set which is available in all almost all data mining tools so i'm just trying to open that default iris data set which is being available in the data mining tool so for that to for that data to be loaded in the weak environment i have to open the file and i have to select that appropriate file so that it gets loaded so once i select this ielts data set if you see here this iris dataset contains five attributes nothing but sepal lens sepal with petal and petal width and the class so that as i said in the left hand side whenever you select a data set that data set attributes attribute in the sense the column names will be listed and if you look at the right hand side we have a selected table so among the attributes that among the fire trebles listed i'm just selecting this sample length whenever i selecting this upper length the nts statistics of the sepal length will be displayed on the right hand side column if you look at the right hand side it contain the entire statistics of the separate it says that the minimum value of a supple length is 4.3 and the maximum value of the sepal length is 1.9 the mean of the sample length is 5.843 and the standard deviation of this upper length is 0.828 see how nicely it can able to just give a summary of a particular column or summary of a particular attribute in dataset just by selecting the particular attribute okay so if you look again again the top of the attributes so it says what data set i have selected so i have selected this iris data set and it says instances 150 it means that there are 150 rows that is being available in this particular data set and attributes by attributing the sense column name how many column names are available there are five column names available so on selecting any of these attributes is the right answer will change this minimum maximum in your standard deviation again if you look at the top it says that uh there is no missing value in the sense it the the sepal length has been correctly listed out for all 150 data so there is no missing values and it says there are distinct 35 values distinct means among the sepal length there are 35 distinct values there are 150 rows the 150 rows there are 35 distinct values available in the sample length so this is what uh the quick summary of the sepal length so see here on just clicking it you can able to get the statistics if this weaker tool is not there then imagine all these things we have to do by coding part which will be a very cumbersome job okay and then the bottom you can see uh the graph uh which is in the x axis 4.32 7.9 which gives the class value so we'll discuss what is the class value the next slide good next if you see we have now the class and yeah exactly so this class is nothing but the segregation of all these 150 data into two different categories so nothing but all these 150 data will be segregated into three different classes one is nothing but iris setosa another there is nothing but i is basically another is nothing but iodized virginia so all these 150 data will be splitted among these three categories okay so if we click the class attributes then on the right hand side you can see uh what are the different labels in the class it is the iris setosa it is versicolor and i see virginica and the number of counts of this data set is nothing but a status of 50 basically 15 virginia 50. if we sum up all these three things is nothing but 150 which is equal to the instances which is present in the iris data set so if you look at the graph it says number of classes which is being present in the data set as i said all are equal so all are 50 50 50. okay that is what it's visible see here this entire class and the count will be visualized in the graph so this mining tool will also help us to figure out or classify the classes and we can able to see the distribution in the data set good so this is a very very important visualization tool as i said this graph is again a visualization technique which is clearly portraits how these three classes the three classes which you have discussed is nothing but setosa vesicle and virginica how these three classes have been distributed for a supper length sepal with petal length and petal width so let us take an examine the topmost graph is nothing but sepal length before that i like to say that this blue color stands for iris setosa and this ridiculous stands for iris versicolor and this light uh blue we can say uh light that is strand for eyes is vizinica so each and every color stands for a particular number of classes in this iris data set so the same color holds good here okay so in sepal length it says the supplement value you just observe the graph which is being listed on the top corner so it says it is nothing but iris setosa so in iris etosa the values of the supple length varies from four point three till it is vary from six and if you see uh the values of iris versicolor it varies from around 4.5 it varies to until it varies from around 7. if you see the third class is nothing but virginica it varies from around five values it varies tills 1.9 so it just gives a visualization appearance of all these three classes on the sample length okay so similarly if you want to know how these values of sepal which has been distributed among the classes the graph c here this graph says the sample length value distribution among the three classes the three classes is nothing but uh is uh sartosa it is basically nice is virginia next the second graph top second graph is nothing but sepal with again it shows the distribution of sepal with value among the three main classes which is being classified again if the topmost third graph is say the petal length distribution values in the three different classes and if you look at the second row first graph it says the distribution of petal with values among the three different classes and the last graph says the distribution of records on different classes this is how you not write a code only then you have to select visualize all if you select a visualization on a particular parameter you can able to get this graph and it says the distribution of values among the classes it all can be done just by clicking it you need not write any coding only thing is that you have to know which algorithm and which attributes you have to which filter you have to choose for the preprocessing so again this is a slide which shows if i select this petal length attribute again if you see in the right hand side we can have the minimum value of petal length is one maximum 6.9 mean is 3.759 and standard deviation is nothing 1.763 and again if you see the visualization sector it contains the petal length the distribution value of three classes the three classes i said it has been distributed the petal landfill has been distributed for a vesicular setosa and visinica the visualization will protrude the petal length distribution on the three classes okay next we are going to deal with the filter how why the filter is required we will see this this filter is highly essential because not all the attributes can be uh used for in many number of data number of algorithms for example you want to use the decision tree then you have to definitely have a discretized numerical attributes otherwise this decision will not work so often uh your raw data for data mining is not an ideal for modeling and you need to prepare or you need to reshape it to meet the expectations of different data mining algorithms so this discrete attributes are those that describes the category called nominal attributes those attributes that describes the category that where there is a meaning in order for categorizing all are called ordinal tributes the process of converting a real value attributes into the ordinal attributes is called as discrete discretion discretion this very important thing because as i said there are some prerequisites for in many data mining algorithms in order to make the prerequisites that we are in a position to convert this real value attributes to the ordinal attributes and those conversion will can be taken place by this filtering techniques so if i want to apply this filtering technique i have to select the choose button over the filter sector there and if i select the choose button then this uh this menu will be popped out and among the menu you have to just click this attribute and if you click this attribute there will be many uh pre-processing data pre-processing method will be there you have to appropriately choose the particular particular filter or triples we are choosing this discrete eyes and once this discretise has been chosen you have to now apply it so in applying part uh there are many parameters it contains attribute indices bins find bin pins inward selection make binary use equal frequency so whichever parameter you like to work on you have to you can you can able to change this uh the parameters and suppose i want to change this uh use equal frequency from false to true i can able to change that from false to true and i can just give okay and apply i'll just give okay and i can just apply there now if you see this petal length values has been discretized and if we apply that see here the petal length value has been mentally discretized now it is ready for uh if suppose you want to do this decision tree then you can easily apply that okay so you can also filter this values by applying any of the appropriate filter techniques next we are going to see how we can use this uh classifier in this uh we call tool so classifying week are models for predicting nominal or numeric quantities so for example uh we can this classifies implemented learning mechanism includes we have decision tree and we have uh support vector machine multiple air press spectrons logical regressions ambition network etc so in this we're going to see how we can apply that to the decision trees so for that i have given uh taken a sample example so a sample training data set which contains uh which is data which is the data set which deals with whether uh a person can buy the computer or not with uh the ch income and student and credit rating so based on these attributes uh we can we're going to construct a decision tree which will say whether a person can buy a computer or not based on these contributes so this is the data set and this is a training data set which has been used to construct the decision tree again again we can going to see how this decision tree can be constructed with help of this week or two so if you see if you construct this the decision tree manually this is how the decision tree output will look like so it will just check the age that's the route what it will take and it will just check whether the age of the person is between 31 to 40 or it is less than 30 or is greater than 40 and if it is less than 30 it will ask whether less than 30 is a student if he's a student then he'll recommend that he has to buy the computer and if it's between 31 to 40 directly it will say yes you can buy a computer or if it is greater than 40 it will just ask for a query how is your credit rating if a credit rating is uh excellent you'll say no and credit rating is fair it will say yes so this decision will help to decide on a particular problem and i have taken this data set especially to buy a computer or not and this weaker tool has been applied uh on this data set to construct the decision tree this is the decision tree you can construct manually but now we are going to use this we call tool to construct a decision tree so say again as i said for the test options to construct this uh decision tree you can use this cross validation tools you all know what is cross validation having being experts in this data mining and always you can also split uh how many number of percentages go for the training data set so i normally should be 66 i given the default one so here uh the decision tree which this week is going to use nothing but addition to j48 it's the implementation of algorithm id3 and developed by vika projective again so so i just click on this classifier and again in the classifier if you check there are many different classifiers like base function lacie meta michelin is trees and the trees i'm going to just choose this day 48 decision tree algorithm i was just selected which algorithm i want to choose so everything is there and because only thing is that you have to know which algorithm to be used for this data set so i'm now going to use this j48 for my decision tree construction so once i have chosen this classifier it will be listed here which class we have been chosen for the data again it contains various number of parameters again as i said it will ask for uh the attributes for this decision three what is the conference factor minimum number of object number of fools uh reduced error burning and subtly rising unpuned use laplace mirror the parameter which your parameter like to use you can just modify this parameter and you can fine tune the parameter and after fine tuning if you click ok those parameters will be taken for construction of the decision tree and as i said that uh percentage split is that so it will give you how many number of percent you give for that training the data for the decision tree and if you want to have many more options you can have more options which which will just uh have a checkbox for output model or output class statistics uh output entropy evaluation measures output confusion matrix store prediction for visualization which whichever is required you can choose that you can choose and click ok and you can just start the process so the entire data set will be arched on the classifier which you have chosen with many different parameters once you have start the process then you can able to see the classifier output so the classified output will be there and then just right hand side it will be there but then if you want to have the view of the decision tree which is being constructed you can click the trees j48 you can right click that and if you want to right click if you click this see it will give the timing at which that tree has been generated and we'll use trees or j48 right click that and there will be a menu popped up in which it will contain the visualization so if you click this visualize tree you can able to see this decision tree okay see now your decision tree has been uh created for this irish data set so it says uh based on the petal with uh it takes the petal with the root node based on the petal with on the values i just classify which class of this iris it belongs to so if the petal width is less than or equal to 0.6 it is nothing but irony setosa if it is greater than 0.6 then it will compare again the petal with whether it is less than or equal to 1.7 or greater than or 1.7 if it is less than or equal to 1.7 again the petal length is being compared even if the petal length is less than or equal to 4.9 then you can conclude the particular data belongs to the iris versicolor if the petal length is greater than 4.9 again we have to compare with petal width and the petal width if it is greater than 1.5 again you can conclude that the particular data is belongs to i is versicolor so this is how we can construct a nice decision tree with the help of this we call tool in just a matter of time of course all that all that this decision tree which has been projected that has been given uh in terms of uh the text on the right hand side next we are going to uh deal with uh how we can use this weaker tool on mining this association rules so the weaker contains an implementation of appropriate algorithm for learning the association rules as i said this works only on the discrete data so we have a discretion filter mechanism which is available in the vehicle so we can use this uh filter mechanism to make this data fine tuning for the particular association rule mining so the main objective of this association rule mining is that to find the statistical dependencies between the group of attributes for example if we have this e-commerce data and i want to find association between the two products then that will help us to place these two products together so that the sales will be a little higher for example milk and butter and breads and eggs and so finding this kind of association how how these two attributes in e-commerce data goes together so if we find this association between these attributes in this e-commerce website then that will help us to place these products together and it will in turn it will enhance the sales rate so a priority uh can compute all the rules that can give the minimum support and exits given confidence so minimum support and confidence are the technical terms it doesn't deal with this association rule mining so association rules are then our if then statements that help us to show the probability of relationship between the data times within the lost data set in various type of databases we take the e-commerce website you might have known that will be large number of columns so in order to find this association definite how which are the two columns are related close to each other so we have to apply some of the rule mining rules so that we can able to get this association attributes together so that will help us to flourish the business so association rule mining has a number of applications and mainly it is used in the commerce website and otherwise slowly it is also uh dealing with the medical data set in medical data set if you want to know where association rule is being used it will also check which are the parameters are closely related if the bp is being rice whether the sugar level rising or they are inversely proportional how close these attributes are how these attributes are contributing for a particular disease so for all these things we can use this association rule mining to get how these attributes are closely related and how this contributing for a particular diseases so this is uh how this association rule mining works so it contains item set and as i said support and vector this all technical terms that have been deal with the association rule meanings uh considering the time i will not deal that inextensively but you can apply this association rule mining of course in this beaker and you can able to see you can able to figure out what are the parameters is being are closely associated for example i have applied this association rule mining on a particular data set and i can able to see these are the attributes that are closely associated with each other so we have seen some algorithms how we can work very nicely in this vcore tool without having a much coding thing only thing is that we have to choose the appropriate options and finding the parameters from finding the parameters for the particular algorithm and we also seen how we can able to pre-process the data so i have just shown couple of things like a decision tree and next statistical analysis of the data now we can construct this association rule mining next we're slowly going into the rapid miner so we'll see a glimpses of what is rapid manner and how and why it has been so popular so rapidminer if you want to download the rapid miner again you can go to the website rapid hyphenaid.com again it's an open choose data mining uh apic manner like we call it has been developed on java so it's a worldwide learning and leading open source data mining solutions uh as i said because it can also easily uh interact and you can also easily having a easy compatible interaction with other data mining tools there is a very unique feature about the data mining data rapid data miner which makes it so popular so if you see the application of data miner it covers a wide range of real-world data mining tasks so this is how this rapid miner will look like so i can say ask vika we can also able to read the data from variety of files if you look into the left hand side you can see you can read the data from csv file excel file microsoft access aml erff xrff you can read the data from databases we can also read the data from steam databases it's very unique here we can read the data from spss statistical package we can able to read the data from strata we can able to read the data from sparse dbase we can able to read the data from c 4.5 even we can able to read the data from whip text you know bib text is mainly used for the general writing maybe especially for references you can able to even read the data from that in using this rapidminer and of course we can also be able to read the data from the url any given anywhere we can also able to read the data so you can see from which are the different sources we can able to read the data that's only a unique feature about this rapid manner it supports large amount of function with from which all amount of files from which you can read the data and top button is nothing but this is for running and if you see this rapid manner it will be like more or less like um visual basic gui in visual basic whenever you want to have some controls you will just drag and put that in the working remember the same thing holds good in the rapid manner whenever you want to have any you want to execute another function for example you want to read some of the things from the excel file i have to just select this read exit and i have to drag that to the main working process so i will get a diagram nice diagram like this here okay it will be more or less like vb uh gui visual basics so for example this is the data uh which i am being loaded in this excel file this is the excel file which contains these many attributes so if you want to see what is the excel file contains these are the attributes that excel contains id h income gender marital number of kills number of calls what's the how much you paid whether it is weekly or monthly what is the mortgage you have they have store car loans and risk so on immediate like on immediately loading the data into this rapid manner environment we can able to have the metadata view metadata view about nothing but data about the data so immediately you load any data set to this rapid manner you have a complete statistics about the data which is being loaded so for example here you have uh i have listed the attributes which is there in the data set and the type column will say what is the data type of the particular column that is binomial whether it is an integer it will just classify that if you see the third column which will give you the statistics of the attribute it's very very crucial you have to know what are the statistics it says what is the average id of the employee in the data set what is the average age it also say what is the average income what is the mood see see age id and income are all integer whereas gender is binomial gender varies from male and female so if we look at the statistics of this general binomial it says there are 2017 77 female and 2040 uh made so you can easily classify uh this you can easily find the statistical value of the parameter which is being there in the data set similarly it will also say monetary status again it is not an integer it is a non-nominal so it will say married single or diverse so it says how many number of married people are there how many number of singles are there how many number divers you are there this is a quick statistic that you will able to get when you see when you click this metadata on selecting the particular data center and if you want to look at quickly if you want to look at the data view you can select this data view which will uh display uh exactly how we look in the data set that the entire thing will be displayed in this rapid or minor environment suppose i want to ah see this plot view this plot view i just want to compare how these two parameters in the data search are related for example i just want to know how the number of kids and number of calls are related to each other so what does it do then if you want to if i click the plot view these are the part these are the attributes which is listed on the left and right will be popped out so i have to select which type of graph i want to draw and which attributes i have to do in the x-axis and which attributes have to go in the y-axis and the color of the column which color i just need so if you select these things then immediately on the right-hand side you can able to get a graph on the x-axis number of kits and y-axis number of cars with the plotting so it says uh what is a good race what is a bad loss and what is the profit so quickly you can able to see how these two parameters in uh how those two attributes in your data sets are being related that can be done easily with help of this plot view suppose you want to again want to have a scatter plot between the number of kids a number of cars but then you want to have the jitter introducing brilliant is nothing but a way by which you can introduce noise in the data again you can able to do that by just adjusting the jitter so now we're going to see how we can do this k means clustering using this rapid miner so this is a sample example i have taken so i have at this particular data set which contains the name and the calories of the food items proteins fat calcium ion so this label column will be easily hidden so these are the things which being given here and just want to clap those items i want to cluster these items which are of or or equal calories proteins factors i will clap this item which will give enough number of nutritions to the human beings so on seeing this color is proteins flat calcium and zion i will not able to group this food item but then i want to group this food diagrams based based on some of the parameters how we can done with the help of this rapid manner being discussed here so so this is a screenshot which shows how we can able to load this uh csv file into this rapid manner environment so i'm just this data set is being loaded and this data set in this is in the form of csv file i'm just loading that now here on rolling this the csv file as i said it will be like a drag and drop method so these values from the csv file need to be normalized after normalizing i just want to use this clustering mechanism i just want to cluster the food items um based on based on the category which are which food items falls under one account category which would then food items falls in another category i just want to group this so for this i am descending this model this is a model designing part so see here carefully i have i'm reading the data set and this data set is being passed as an input from normalize and the output of the normalize is nothing but given us an improved clustering and at the end of the clustering i have three results three results means the entire data set has been clustered into three different groups that is what being uh designed in the process so this has to be done by the user so which normalization they have to do which clustering they have to do they have to just click this menu gui graphical interface and they are just dropping to the process environment okay and this connection has to be provided by the user so that the entire process can work on good so if so i have done this and if i want to execute this i can just press this execution button over there and rapid panel which has been projected previously if i run this then i can able to get this result so see here it it classifies the entire data set into three clusters it says cluster 0 cluster 1 and cluster 2 if you look at the cluster 0 the cluster 0 has these common attributes the color is varies from 0.9 and protein is nothing but minus 1 fat is minus 0.7 calcium is 1.6 and iron is 0.615 on label is it means that the entire food items which contain these calories these proteins these for this calcium and this ion will be grouped into one cluster similarly it will cluster the it would also cluster one and cluster two they will always aggregate this wood type down based on these values and you can able to plot a graph in the x axis you can have calories proteins fat calcium ion and you can able to plot against the cluster see here uh zero uh stands for one cluster green stands for one cluster and the red stands for one cluster easily you can do the clustering in a much more sufficient way using this rapid minor and you please look at the right hand side so the right hand side will clearly say is what are the foot items will cover come under the cluster 0 it says raw it says clamps canned merkle canned salmon can't and startling cans so all these food end times will come in the cluster 0 it means that all the food items listed in the cluster zero have the same amount of calcium protein fats calcium iron and if you want to know what are the items which is clubbed under the cluster one it is nothing but beef can the chicken broiled and it varies since two shrimp can in cluster two we have beef rose still to poke cement so all these food items are now segregated based on some values that is done in using the rapid manner in just by selecting appropriate algorithms so now we have seen two different data mining tools uh one is weak extensively another we have seen is nothing but the rapid miner now we can going to see how we can which is the best which uh data mining tool is the best whether it's a week or the rapid miner we're going to discuss that for the next five minutes we're just coming to the end of the session maybe for another uh five minutes we'll be concluding it so so what is vca just going to come back quickly the weekend rapid manager so wika is nothing but it's an open source data mining application it contains many data mining libraries as i said it's written by java and it is the tool which is available and it is a tool which is being authored by this department of computer science wakata university if you see on list of the advantage of the vika it has many landing pages and it has appropriate and suitable graphic user interface it is specifically used for data mining and it has a very very simple self-explanatory workspace it is suitable even for the text mining and we cause ability to run several learning algorithms and we can also able to easily compare it and since this weaker is written in java in respect to the operating system it is platform independent it can work on any platform this has a quick summary about the vika and if you look at the lapid manner why rapidminer what is the advantage of the rapid miner what are the special features in rapid manner so these are the things it is compatible with different operating system so this rapid miner is compatible with linux windows and mac that is all because it has been written in java and even in rapid manner you can find a tool to do this text mining so at all of the weaker learning algorithm so we have all the algorithms which is which is there in the wika tool and i mean even we have some of the enhanced algorithms data mining algorithms which is also available in the rapid manner one more specific point which i forgot to list out here is that this rapid miner can easily interact with other determining tools which makes us more sophisticated [Music] if we quickly compare the similarity and the differences between the rapidminer and the vika both are written by java so it's a platform independent tool and both are published under a gpl license and rapidminer approach many weak algorithms in one workspace these are some of the similarities if you want to compare the difference between the rapidminer and the vika so per performance in connecting to the files containing excel data and the database orient of java base and reading csv file in wika is not that good as in rapid manner so if you read this csv file in vca that if you you can carefully note the data is not organized properly and if we compare the rapid miner has a better graphical user interface than the vca so these are some of the differences and similarities between the rapid miner and the vca so if you see and if you ask me among these tools which tool you would prefer i will always go with the data miner tool when you was there a race and i can say the rapid miner always have an overage than the weaker and we'll see i discussed some of the features of rapid manner which is preferred over vika in a session we'll see why it is being preferred so because data data uh that data mining tool is being preferred rapid management tool is preferred because of its analytical processing design so it is it has more than uh 50 000 downloads uh since 2001 and uh it is uh it got the first place knight challenger zero uh for open source business award and um more uses than kd nuggets challenge and it is a place in the fourth time in 2011. so it has a very good and uh processing uh analytical processing designs that makes it so popular these are some of the factors which user prefer rapidminer than other data mining tools and the second thing is that performance and flexibility it's another advantage of rapid manner it quick fixes there is an error being quickly fixed and we have a metadata transmission for transformation i have just shown how the metadata are being done in the rapid manner and you can also execute the code with the help of the breakpoint and data miner has more than 500 operated like data processing modeling test mining web mining opinion mining and so on we have n as i said 500 operators and it has been listed many ways rapidminer has 20 ways by which you can able to visualize the data next advantage why rapid mining is being preferred is nothing but scalability it it's like radiation database and you can able to scale this rapid manner on the fly and it is famous for reduction customer retention and data flows as i said the the data from which you can read the input from the source from which you can read in the data mining in the rapid miner you can read it from oracle db2 servers equal access access spss and many more things so these are the things which makes this rapid miner very very popular so now we have come to an end of the session i think it's uh 12 5 and so this presentation in this webinar we have listed out what is data mining what is the flow of data mining and we have listed the top five data mining tools that are being used by the researchers and among the top five we have discussed extensively on the week on rapid miner and we also seen how this weekend rapid manner being worked on some of the data mining algorithms and we also had a quick comparison about this week on rapid manner and we also concluded which is much preferred by the researcher and we have also seen reason why rapid manner is being preferred a best data mining tool for the research and this is a quick summary and now the session is open for the questions these are some of the references which we have gone through for my presentation and if you have any questions uh i will be very happy to answer that so now the session is open for question and answer please thank you sir for sharing your knowledgeable presentation now it's time for question and answer i will list out the questions posted by the participants in the chat box apart from data mining what other area of research does vegas support so data mining and machine learning as i said data mining and machine learning goes hands in hands i think you can only use for data mining machine learning apart from that it is not intended for any other research area okay so when complex platform that combines with data mining iot cloud is used which tool is suitable iot cloudy uh why we need iot iot we need only when you need much data okay so this iot is mainly uh seen as a platform by which you can generate the data okay once the data has been generated by the iot uh we need a huge number of storage because uh per second now we are leaving the world where per second we have millions of data being generated so to store these millions of data which is generated per second definitely we need a storage medium and as an organization or as an industry uh no one is offered to have that huge storage to store the data which is generated by this iot devices for that we will go to this cloud okay so if you want to link between these two uh definitely this iot will generate data and the data generated by the iot will be stored in the cloud and easily these data mining tools can able to read the data which is that being shown in the cloud so i think i think i have given you a relationship between this iot cloud and this data mining tool yes yes sir next question what suggestions you give you can give for the researchers to utilize vega or any specific tool for their research implementation so as i said i have clearly shown a comparison between the week and rapid minor at the end of my session so always it is good to use this rapid manner because of the sophistication how much of luxury it provides for the data analyst and for the machine learning experts so if you ask me uh it depends on the research and still if you're working on this data mining you can always use this rapid menu because of the futures we supported yes so next question does it give any analysis support to improvise the output of the test data so as to increase the quality of the output definitely you can fine tune the parameters for this data mining algorithms based on the finding the parameters you can enhance the result that option is there actually okay so next question whether we can import image data for processing into vega and how to create attributes in that image data i think as such there is no any facility to import the image data set into the vika you don't have that option okay so thank you very much for your detailed answer now i would like to invite mrs revati assistant professor faculty of computer applications to deliver the vote of thanks assistant professor faculty of commission dr mgr education and research institute it gives me an immense pleasure to deliver the oath of thanks for the special occasion on behalf of our department faculty of computer applications i wish to express my sincere gratitude to our honorable founder till ac sir president respected engineer acs arun kumar sir registrars deans ho please and other staff members and now i would that i would like to deliver my special guest dr tyagaraja who accepted our greetings and spending his valuable time with us by sharing his powerful knowledge and effective presentation on some top five data mining tools like mika tools rapidminer and other difference between the tools thank you sir thank you very much it is my privilege to thank our deep department and my dear staff members for their external continuous encouragement and support i i mentioned a special thanks to the organizers and technical team of this faculty development program for their continuous support and last but not the least finally i thank all active participants of today's program for making it a grand success once again thank you all thank you thank you ma'am if your actions inspires us to dream more learn more do more and achieve more you are the best leader this is the exact code which suits to our head of the department dr vijay vinod who plays an important role to execute this event in a successful manner i request head of the department to deliver the wealthy note for this entire faculty development program morning and s to meet you all and this is the end of the friday my video ma'am video oh so i know you you haven't kept it off why i think one second okay is it coming i'm sorry i'm very sorry it's not coming start camera man yeah it's not it's stuck is yes what to do now [Music] i know but it is stuck i don't know i so we will uh what should i do i'll leave the studio and enter again let's continue okay i will rather than coming again no i will continue you know yes the start camera is stuck you know the screen i am not able to press any of the buttons out here now it is okay you can carry on you can carry on okay very good morning and happy to meet you or uh should i go out and come inside it'll lag the show for okay it is visible now let's go okay ma'am you can continue yes so uh good morning a very good morning and happy to meet you all and this is the end of the fight international fdp on value edition information technologies on behalf of the faculty of computer applications i wish to show our gratitude to all the speakers who has inputted valuable contribution in terms of knowledge in the academy community the and uh i hope dr petey agrarian is already with us and we are very happy with his uh session on vika and rapid miner tools it was really a great knowledge sharing the public views have crossed more than 2000 is what i have understood and uh the first day was dr gita chutan from national university oman and dr yes mohan gandhi from ibm usa dr thierry murgan college of applied sciences oman and he mr manikantin iit expert from hyderabad and dr ram murthy he is a dean of computer applications and dr pete and he is from the central university of tamil nadu and i wish to congratulate and appreciate my entry team of the computer applications department my colleagues who have done wonderful job on live streaming the entire show to the public the highlight was that most of the department members who showed their face on their life and they supported and participated well a good team effort and a good support is always an asset to my any leader to achieve the department and organization goals i specially thank and indebted to you all for that in this gangster i wish to wholeheartedly appreciate the event organizers for their efficient planning in spite of enormous challenges i remember in the second day there were lots of network issues but in spite of all those they have faced in the network they managed to overcome those instantly and that's only because of the brilliant coordination among the organizers on behalf of the ca team we appreciate dr radhara mohan dr murali and others for inviting excellent international speakers we strongly appreciate you all the department wish to thank our overwhelming participants who showed lot of enthusiasm and joined us for the event i thank you one and all thanks for being with us for the session thanks a lot unity strength where there is a teamwork and collaboration wonderful things can be achieved easily a big thank you to our department staff members who extended their support and cooperation till the end a special thanks to all participants who have given their interest and support proud decision on behalf of organizing team we extend wholehearted thanks to one and all myself nithya mr enter organizing team signing off and will catch you in another event shortly thank you you 
bmCjICrXGLw,27,"Data visualization is the graphical representation of information and data. By using visual elements like charts, graphs, and maps, data visualization tools provide an accessible way to see and understand trends, outliers, and patterns in data. Data mining, or knowledge discovery from data (KDD), is the process of uncovering trends, common themes or patterns in “big data”. ... For example, an early form of data mining was used by companies to analyze huge amounts of scanner data from supermarkets",2020-06-15T04:16:28Z,"Data Mining Lecture 1 - What is data mining, machine learning and data visualization",https://i.ytimg.com/vi/bmCjICrXGLw/hqdefault.jpg,Charles Edeki - Math Computer Science Programming,PT29M50S,false,259,12,0,0,0,again welcome to data mining and analytics lecture number one uh in this lectures we're going to define and also describe data mining also define and describe machine learning so our main goal is what's the difference between data mining and machine learning then also we're going to define describe data visualization also how to locate search and use common data sets and also define and describe data quality so we start understanding data mining here we said that data mining is the process of identifying patterns that exist within data when the patents in on data analysis again can apply them to other data sets so here we can see again data mining is a process of trying to identify a pattern or a knowledge from a large data set so we should think of actual mining as a search for the data patterns or the search of knowledge or information from a data set as opposed to a subsequent use of the patterns now data mining process may involve the use of statics database queries visualization tools traditional programming and also machine learning so most data money organizations are again from machine learning algorithms but again we see the difference between data money and machine learning data mining again is trying to find a knowledge or a pattern in a data set but machine learning is more or less like action oriented and again we will go through the definition first so understanding machine learning here we say machine learning is use of data pattern recognition or guidance which allow a program to solve problems so that's why we said again data mining is a process of trying to get some knowledge or a pattern in a data set but machine learning is it's a concept that we are going to perform some tags based on analysis of data sets using the machine learning or garriding an example of machine learning algorithm we're going to discuss is cross story categorization predictive analysis and data association without the need for explicit step-by-step program instruction to tell organically how to perform tax so for example i can write a program that can diagnose a patient if you have disease a or disease b now instead of trying to write the algorithm to perform the tasks or the instructions to perform at us we are rather going to give the learning algorithm some data set with a known target variable then we know that this data set all the patients have disease a so we are going to use again machine learning algorithm to study this data set basically what we're trying to do is to build a model based on the target variable then next time we have a test data that's a data without the knowledge or we say the target variable now we can use that data set and then the model can predict if the patient have disease a or disease b or to diagnose whether the patient has a disease b so machine learning he said it's a process of performing we are performing a task here we say machine learning is a use of data pattern recognition or guarantee which allow a program to solve problems such as cross story categorization predictive analysis and data association without the need for explicit step-by-step program instructions to tell the algorithm how to perform their tasks but we say a data mining is the process of identifying patterns that exist within a data so with the patent and data analysis them to other data sets so in this way again machine learning solutions can solve compressed problems by using data to drive discovery in using only a few lines of code now the common data mining tools we have databases such as mysql or mongodb database also we can use the comma program application such as excel for again data monitors visualization tools such as tablet uh business intelligence tools such as microsoft power bi and also some programming language solutions such as our program or python also data mining tools such as rapid mana orange and weka this uh application that we don't need to write programs as small as like using a go and use a graphical user interface in this program in this course again we're going to learn how to use the weaker and also the rn pattern so we may write some code in our python and also use the weaker so common machine learning tools can be the python and art programming solutions also visual programming tools such as rapid mana and orange and also wake up it's a machine learning tool also and also accepted party headings such as summer is a program that again is i had it must be added that's like a library file to the asset so data mining versus data science so our course again is data science but most of the time data science we are going to use machine learning and also data mining concepts in data science the diff the major difference between data mining and data science is big data analysis in data science we may use unstructured data and data that are very complex and very large so we may come up with a different concept like distributing system like hadoop or map reduce et cetera so here we say data mining is the process of identified patterns that exists within data by data science now it's smallest same as machine learning data science is the use of statistics programming scientific methods and machine learning to extract knowledge from a data set so the definition of data mining and data science are very similar in fact the two terms are often used interchangeable but the data scientist therefore is an individual who analyze and also interpret data again you find 10 it's like data scientist data analysis is quite similar and again both we use data mining tools or machine learning algorithms to gain insights into one or more data sets but the major difference is data scientists may be working with a complex data set very large and also unstructured data so that was the difference between data mining and statistics here we say data money again is a process of identified patterns that is this within data but statistics traditionally involve the techniques in collecting data so data collection analysis modeling and presentation of the data and statist is one component of data mining meaning it is one two in data analysis to kids now having knowledge and understanding of strategies will surely help us as a data scientist or data analysis better understand the behind the scenes process of many data mining and also machine learning authorities now the good news is that you don't have to be a statistician to use the tools and also essay remains one of one of the most widely used data analytics tools and has many built-in stats functions so many data analysis find success with only a basic understanding of start process another technique which is very important in both data science data mining and data analysis is data visualization and data visualization visualization again play a major role because as we human beings we tend to understand visual concepts items that are more visualized than because when we visualize for example data set we can easily see the pattern or if there's any correlation in the data etc so a few years ago big data was the session now it is the norm big data is the norm one of the first steps that data analysis perform to identify patterns within data is to represent the data visually using charts and graphs etc so data visualization against a very important techniques and likewise also to communicate data trends and findings developers often create data driven dashboard and then depending on information the data analysis must convey and also they will often create creature dashboards that face this very high level often aggregated data upon which the user can click in in order to drill deeper into underlying specifics this is a very common concept in the using the table with the dashboard clicking visualizing the data so this dashboards again to communicate data trends and find things developers often create data driven dashboard now depending on the information the analysis must convey they will often create again creature dashboard which has this very high level often aggregated data upon which the user can click in order to drill deeper into the underlying specifics so common visualization charts and we have the time base time based comparison chart which again represent how one or more set of values change over time this example would be a time series chart a term series chart basically means again the data that we are analysis again is based on period of a time it can be in seconds minutes weeks etc example will be a stock market data set we know stock prices change within a second in a period of time prices varies so this would be a time series chart also we have categorical base comparisons chart which represents how two or more categories of values compare composition charts which represent how one or more values relate to a larger also we have a coalition charts which represent how two or more variables relate to each other and trying to find the association between or even consider between two or more variables and also the dashboard charts which represent the key performance indicators that companies use to track initiatives we also have the distribution charts which represent the frequency of values within a data set geocharts which represent our values from one location compared to values in a different location so again these are different common visualization charts an example here is a google charts and also this example of correlation for example in the first chart here we can see there's a negative correlation which means if x increases y decrease this is a positive correlation when s increase y also increase here there is no any correlation and non-existent coalition also we can use r to plot coalition for example here is insurance charts we have the code here uh python also have its own library file for the protein chart and most again machine learning data money applications like quaker or rapid manner have a tools for game to represent data visually so visual programming for years we say programmers have made extensive use of python and our programming language to create data mining and also machine learning solutions now the problem with creating programs to perform data mining and also machine learning tasks is that someone must know the programming languages and how again to code example would be python and our we must know them how to write python program so we have to learn how to code now visual programming such as weaker or rapid manner eliminate eliminate the concept of coding so here we just have to learn how to use the application but the main thing also is to understand you're guaranteeing the features so that again when we are using it we know what we are doing so to help me eliminate the need for sex statement based programming visual programming environments are emerging such as rapid mana and orange which are presented in this again course so this example predicting who live and died with python and titanic now you can see that here we have to almost console application we need to write our code finding the confusion materials etcetera and we can see the code also here so here for example in pattern we're going to use the numpy a lot with the pandas so first we import their pandas and numpy again they are packages in python which are normally used for data analysis almost like certificates concept funny me media mode correlation summarize updater and they have all the metals in those two packages and also we have skill and scaling scaling concept and also we have the machine learning packages for python and also for our decision so using python we need to write the code we need to know how to the name of the packages how to use them so next is the business intelligence so business challenges again is the use of using a tools now your tools can be data mining to machine learning or visualization now to convert data into an actionable business insights and recommendation example is let's say bank industry we have chase bank citibank and the main goal of this two major bank it's about competition making more profit gaining more or retaining more customers customer retention with reducing the attract so in this concept a city bank or any business organization may use some data analysis tools to basically understand the customer's behavior and also managing this customer relationship management to improve customer services the quality of customer service so we may use data again my business intelligence concept so business intelligence often leverage click through dashboard in which users can click on items to display greater level of details now business intelligence system often include decision support tools that help users to make better decisions also using historical data such as tools can describe what has happened and potentially why using predictive analytics such as tools search tools can predict what should happen in the future and they may possibly prescribe choices the user should make we have different types of business intelligence tools we have microsoft power bi tabroid orange solver rapid mana or even a wake up as we all know business intelligence again business intelligence is using machine learning or data mining capabilities to again analyzing data in order to improve business operations customer services and profitabilities etc so next is understanding the raw databases so before programs and data money tools can perform analytic operation most important data must be available so the main menu again is data a data must reside within an accessible storage location so as the volume update the volume of data that application must process continues to grow so do the demands on the underlying database now for over five decades now table-based rational databases have been the main state for database operations we are talking about relational database system where again data are normally stored in the form of table rows and columns but the explosive growth of highly skilled web and also mobile solutions not to mention the internet of things i mean as motivated database developers to market to different database stacks because they will have data that again they are unstructured example will be xml non-sql database also more is shown in the form of relational database so sql database mssq oracle and others now we have known sql database such as mongodb couch db redis and also cassandra these are most of the databases that are used again like in data science and big data analytics concepts so a word on the data set again in online system and online there are so many main data sets that are made public uh when we go to government websites websites we talk about medical data if we go to national cancer institute institute or national institute of health nih they have so many data sets about proteins and genes data set and cbc so there are so many different agencies dealing with different data in this course we're going to focus on these two websites the uci investor of california data machine learning data repository there are so many different types of data these are secondary data data that has been used and in data analysis concept like data mining data science also one of the famous websites where we also have so many data sets is that cargo.com we're going to use cargo.com also to do some practical examples so most of the work we are going to do in this course lab work our data set we either come from uci or cargo.com so anytime we have data the most important thing is uh she know the term data cleansing and one glue this is the process of preparing our data we know most when we get our raw data there may be an error or missing values or even duplicates so first we have to do our data preparation process mostly in data monitors uh 80 sometimes between 60 to 80 percent of the task is performed during the data preparation process because we know in computer science garbage in garbage art if we don't have the right data set and the right amount of data set most likely our result will be a good result so data preparation process is very very important so here we say before data analysis can start the data money process he or she should determine the quality of the data set and identify any anomalies such as missing or outline data that they will need to resolve now data quality is a measure of the data's suitability for use so data quality attributes first accuracy our data have to be very accurate so here we see the degree to which the data correctly represents the underlying real-world values such as all temperatures from a sense of being in the correct range completeness that's the degree to which the data represents all required values such as data set that should contain an hour of data for a sensor that reports every second having hundred percent of the data values consistency is very important the degree to which similar or related data values align throughout the data set that's consistent such as each occurrence of an address have the same zip code they have to be consistent and also conformity the degree to which the data values align with the company's business rules such as company will measure and store sensor values on one second intervals also the test and photo data mining so yeah we say most people when they think of data money they will envision numerical calculations like in statistics uh most quantitative analysis and a variety of tables charts and graphics but also in data money or machine learning concept we can use test data so example you will be using the testimony for example you can determine the language used to create a document and identify documents topics etc we also have a subset here called natural natural language processing natural language processing is it's a concept of using machine learning organizing or is it in short any algorithm to analyze the test data for example language and tests and it can even be audio language or a test data also we can have image mining so yeah example is the facial recognition system we can use machine learning algorithm again to recognize the facial input data this concept is like australian system in medical field now when we take an x-ray of the patient instead of the doctor to go through we will now have a software that again can read the extreme formats that's image analysis also concept so again we have photo data mining and also photo data money can be again the image analysis or image processing so we're going to end the lectures here and in our next lectures we're going to discuss about different type of uh machine learning or data mining organizations example clustering classification association rule what was the difference between all of them so in these lectures again we went through what is the major thing is what is the difference between machine learning and data mining so we went through what is a data mining uh what is a machine learning what is data what type of data we can analyze whether image tests numerical values also the concept of strategies that can be applied concepts that can be applied to both data mining and also machine learning so again wish everybody the best and see you in the second lectures 
YoWl8ZHfda0,28,This is a quick introduction to some of the most common chemical databases that you may consider using for data-mining and cheminformatics applications,2020-09-04T12:34:07Z,Databases for data-mining in chemistry,https://i.ytimg.com/vi/YoWl8ZHfda0/hqdefault.jpg,Dr Chris Arthur,PT12M26S,false,36,2,0,0,0,hi if we're thinking about data mining and chemical informatics the first place we need to start is what sort of data we might be analyzing now you may have access already to some experimental data that you'd like to data mine if however you don't then there are publicly available resources that you can access to and try asking scientific questions about okay and over the next five minutes or so i'm just going to introduce you to a few of the most important ones i think so the first thing i'm going to do is point you towards wikipedia because they have a list of chemical databases okay as far as i can tell it's fairly comprehensive it covers everything from crystal structure information analytical data um reactivity information and the like across multiple disciplines where you're talking food chemistry the environment drug discovery okay so there's a huge range of of databases in you now some of these are publicly available and some of these are available only by subscription okay so um i think if for a first point of of investigation i'm going to point you towards wikipedia okay so the the next one i'm going to say is i mean chemists are interested in molecules and one of the largest molecular resources that we have access to is chem spider from the royal society of chemistry this is an online database that you can access programmatically if you wanted to which contains molecular information physical chemical data links to literature references links to spectra and the like so if i come along and i for instance search for aspirin here it's a fairly generic molecule that everybody's familiar with we can see the source of information that we have access to we can see for instance different names that it's available under we could see its physical chemical properties and biological properties we can for instance access spectral information so nmr uv mass spec we could even consider where um where could we buy this from okay so this is incredibly useful resource okay that's publicly available the next one that i'm going to talk to you about is structural information so you know many of you will be familiar with the cambridge crystallographic database okay here again i've just performed uh search for aspirin again we can um have a look at the uh molecule itself you can get the crystallographic information so space group and unit cell and the like okay uh you might be thinking about measuring for instance uh distances within molecules you might be thinking about trying to look at 3d conformation and the like so those kinds of questions this is a fantastic resource many chemists are interested in chemical reactivity okay and unfortunately chemical reactivity is something that publicly data publicly available data is very limited for the biggest and most useful actually comes from from this this publication where with the data mined chemical reactions from the u.s patent literature okay so it's got about 40 years worth of of patent literature that has been um extracted to computational um and made publicly available okay this is currently hosted on on figshare which is a website that shares publication data and you can come along you can download this reaction information okay um so uh it it's by text mining and that means that for instance the data that's available varies based on what the what the publication contains so if it contained melting point information then this will have melting point information if it contained yield then it will contain that but it may not have okay so you have to have that little caveat in there that this isn't a complete data set per se if you're interested more in in structural biology crystallography nmr for instance then the protein data bank i'm sure you're already familiar with it's a fantastic resource for studying those structural information okay so you can for instance download specific uh proteins that you might be interested in or specific ligands that you might be interested in or you can do a more comprehensive analysis where you download perhaps the whole of known crystal structures to do a a global analysis of of whatever you were interested in um natural products are amongst the most important and for historic reasons and for drug discovery reasons in chemistry okay and then they've made fantastic targets for synthetic chemistry and also understanding their biology is really important there are a number of natural products type databases available to you this is a natural products atlas where they've tried to provide open access to a wide range of bacterial and fungal natural products research okay and to try to allow researchers to explore that chemical diversity um obviously natural products are just a small part of of chemical space okay um the other one that's of big interest is stuff that we can buy and synthesize okay and this website is zinc okay zinc is not commercial is the name and it's a free database of all commercially available compounds now you can use this for whatever you might want it was originally set up for doing docking for drug discovery but you could for instance download the sigma aldrich catalog from here or another supplier of of your interest okay so you can access the dimensional structures these are large part calculated three-dimensional structures rather than the crystallography graphic data that we saw earlier on okay you can get um simple 2d structures you can then use that for whatever chemical informatics application i know many chemists are interested in in the applications of lipids whether those are for biomedical research or for environmental research okay the swiss lipids database is a a great resource for um finding literature and structural information on lipids and their biology and uses okay so swiss lipids is there um if your interests lie more towards the biochemical biomedical applications in chemistry so drug discovered medicinal chemistry look like okay campbell is the go-to website for that so campbell for instance will allow us to and if we just use our aspirin example again we can cover log and we can access compounds information on on for instance aspirin here we can get links to where it's been used in biological assays we can see what it's the the range of bioactivities it might have okay so you might be thinking of at experiments such as quantitative structure activity relationships you might be thinking about um something like drug repurposing or or anything like that so it's a fantastic resource for for those types of things the next two i'm going to cover are both pub chem and it's related one pubmed okay so pubmed and pubchem are both from the national library of medicine in the u.s and they allow you to publicly access chemical and biochemical data or in case of pubmed literature information that you can freely download okay so you might for instance consider a text mining experiment where you're mining the text literature for um let's say you're interested in in chemical education you might for instance come along and say say you're interested in feedback in education for instance you could then obtain a the literature or at least the the references or the the abstracts and do some sort of text mining on that for instance um pubchem likewise very similarly to kim spyder we can come along and we can access for instance structural information spectral information links to biochemical data and the like okay okay if you're more analytically orientating them then you might be for instance interested in nmr data specifically so nmr shift db is a is a publicly available database of nmr data you could also use it to do things like predict nmr spectra or help you assign nmr spectra okay so this is uh this is a great resource for that if you're interested more in things like um mass spectrometry or complex sample analysis and metabolomics then websites like gnps so the global natural product social networking website or metabolites for looking at human metabolomics experiments are great resources so gnps will make publicly available data on both natural product extracts so for instance lcms data of some extracted fungi and that you might want to look at whereas metabolites is focused on human biology principally um and it is cross species but the species are typically mammalian used in in medical experiments so uh so the the range is less diverse than you might find with gmps okay but um so we've we've seen how we can access a wide range of chemical information across across multiple different databases you could for instance consider blending these methods together for instance you could think about uh can we pair up the data from cambridge structure the chamber's crystallographic database can we look at that and look at spec molecules where the nmr data is known and what inferences could we learn from that so don't look at these sets of data necessarily in isolation okay thank you very much 
9GOzj7V7frA,22,,2016-09-01T13:24:43Z,DATA MINING   4 Pattern Discovery in Data Mining   2 2  The Apriori Algorithm,https://i.ytimg.com/vi/9GOzj7V7frA/hqdefault.jpg,Rγσ ξηg,PT6M12S,false,1870,10,1,0,1,you hi let's introduce the very famous opera Albertson this algorithm is at the first candidate generation and test approach for a frequent pattern mining it is a level ice-candy generation test approach initially the first time you just scan the database wants to get frequent one item set then taking this frequent eye one item sets you're going to generate lends to candidate item sets at the KC iteration you're going to take a landscape frequent itemsets to generate lens k plus one candidate item set then you go against the database to test these candidates generated and to find the rear or frequent k plus one item sets okay every iteration you set k to k plus one so you can go and here no frequent itemsets will be generate or no candidate item sets can be generated okay then after you exit from loop we just return all the frequent items has derived that's the algorithm let's look at the pseudocode we set C sub K to be the candidate item set of size K F sub K to be the frequent item set of size K initially we just get a frequent one item sets then we get into the loop suppose the frequent K item set is not empty then we get in we use case frequent item set to generate K plus ones candidate item sets then we go against the database using the minimum support to see which k plus one candidate items in all frequent okay we derived the frequent k plus one items X okay then we reset k to k plus one here we get out of this loop we just return all the frequent K item sets for all the case derived okay let's look at the concrete example okay you look at this transaction database it contains only for transactions okay the first scan you just try to find their support for every single item set then we find D the support is only one so it's not frequent so then we just remove it we get a frequent one item sets and their support then we use this to derive the candidate to item sets C sub 2 then we scan the database again we find their rear support count then we find these two blue marker ones are infrequent we derive frequent to item sets then using frequent to item sets we derive the frequent three candidate item says remember this one you pro can't see a big cut why because you probably see a CPC a frequent you may think ABC could be candidate item set but ap is not frequent here so ABC will not be derived so we only derive BCE okay with another scan we find its support is 2 so it's frequent as well then we find all the frequent 1 2 3 item sets the concrete implantation accurate involved self Johnny and the pruning surf joining goes like this ok we get an ABC ABD the first two are the same the third one is different so we generate this self joint generate this candidate set the similar thing we get a c-dac e the first two are just the same the sir one we get them together we get this candidate sets but ok we may need a pruning process the pruning is pretty simple before you can't against the database you press e for a be seedy this PCD is there so this is a candidate one but for a CDE CDE is not in the frequent straight item set so a CD cannot be the candidate so it's prompt so that's the simple way you can see self joining and pruning we can solve this problem okay the finer we get only one Kennedy says ABCD let's look at the SQL implementation of this Kenny said generation and test so we Percy the candidate generation is century this you pretty see the self joining how to do self joining you will see the first K minus two they are identical then the last one K minus one item one is smaller than the other we get one Kennedy set then for this Kenny said we still need pruning the pruning essentially to check whether these subsets the K mandolin subset containing this one if s is not in FK minus one then we just delete this can said from the candidates a set C sub K so this is a candidate generation the keys step of a secure implementation of a pari 
JzBdlJBux08,22,,2020-06-22T21:03:30Z,Data Mining and Warehousing Lecture 5,https://i.ytimg.com/vi/JzBdlJBux08/hqdefault.jpg,ZDS content,PT58M49S,false,7,0,0,0,0,okay good afternoon everyone and welcome here we are starting I was just trying to see if I can get more students joining in but I'm thinking Brad may not be happening so we will continue from where we stopped today we are looking at data pre-processing started to confirm how many people are still part of this class data pre-processing is the area that we want to focus on today yeah just to to think through some few thoughts in there around it after processing but also to to engage and see how this can be applicable in our day to day world cause anyway we are in a world of data so it's very important I am not sure if there is any students you know you know I don't know if we have students here we have not met before because I have seen the numbers just skyrocketed or something there are many many more who joined yes but but nevertheless we just continue please make sure that you try and pick up I [Music] know I don't know if there is any one of us who has tried to access the recordings of these corroboree collaboration sessions that we have had before and have you been able to access them I want to know if that is possible or do I need to download them and share them on the dashboard I need to know that so maybe from the chat box I can get feedback about that are you able to access like for example those who are not there in the first session are you able to access the recording of what we did so that you can go through it as you go through the notes study I am posting I'd only that has been possible okay it's possible Thank You Stacey so we we will continue anyone who is not able to join us they will try and catch up through that but of course like I said we need to note that attendance is purely based on our attendance of this collaboration session and so you know the policy this is system so there is no going round about it it's the system that is going to say you are in or out it's not me so let's take care of that but yeah we talked about some few concepts around data mining around Italy a housing last time and just trying to introduce us as to what that means to us and part of that was just to understand why do we have determining what's the importance of this but also we were trying to also differentiate between data mining and data warehousing and just looking at what are some of those systems that are used in data mining and was that we can be able to use in data warehousing it was the difference in there and I think if I remember well some of the things that are coming out very well was yeah when we talk about data warehousing then we're looking at historical data data that has been stored over time we are looking at subject oriented data we are looking at her you know data that is varying with time okay you know and all those features that we looked at in data warehousing but also in data mining we were looking at extraction of to make meaning out of the data that has been collected over time or out of the data that is currently running on a transactional database yes so that's very important but today we wanted to look at data pre-processing and this basically is before we can make meaning of data we may need to look at it critically and see is it giving us a true representation of what we are looking for is it giving us a true representation of what probably has been there before what has been looked at before or what we are looking forward to in some of those is just we are some of it the things that we know there are four facts that you can't expect data that has been collected and the age of a person is reading a 300 for example and so there is always a need for us to look at how can we you know look at the data as rare as it is but also be able to dive into that data and do some few things to make it relevant for us to make meaning out of it and so some of those things are things that we have given definitions around around them data cleaning for example so removing of noise you know adding of missing data ignoring some of them but also data transformation which I know in databases for example you must have looked at normalization to factored form normalization and all those that you looked at attribute selection this discretization but also concept hierarchy generation and then also have data reduction and this may be informed by the fact that we may not say we want to reduce data because we are not the only reason we would want to do that is are we able to maintain to sustain the meaning that we wanted to deduce even after doing reduction so reduction may be attribute subset subset selection it may be just different attributes are being used or we are using different names to refer to different attributes and we want to cut down and scale down or we want to match and and just get the right data and you if you remember last time as we were talking we say that for example you may be collecting data about students and you find that in in in Z Tech University week all students are number we call them a registration number but if you may go to another university that is called at mission number another investor calls that maybe let's say student number and if you collect data in from all those universities you find that the data that you get is actually in one way or the other it's going to have all those attributes reading difference but in actual sense you are referring to one thing and so there is need for you to match some of those things are very very important so we will be just looking at this in details in today's class and see what we can be able to learn out of this so so some of the steps that are involved in data pre-processing one is cleaning whereby data can be can have many irrelevant and missing parts and it's very very and for us to do cleaning and that can be can involve for example looking at what is missing within the data and knowing how to deal with that so some of the missing values may be requiring you for example to construct new attributes if that is needed or it may require you to ignore some of the attributes or some of the data that is in there the other thing is also removing of the noise and we talked about noise within data which is just better that is meaningless it may not add value at the very end of the day to the exact contents that you are looking for remember that at the very end of the day we expect that the data that we get at the end of the day is informed or we are proactive let me say that in our data mining process it's a proactive you know venture whereby we know the end goal of the meaning we want to deduce or we know what we are looking for so you set goals and say by the end of the day I want to establish who are the customers and where do they come from who buy this and these products and so out of that you now come up with data attributes that can help you you know get the results that you are looking for so you have that goal the goal in mind the EDD goal you have you must have that in mind and even as you start the process and so whenever you get you collect data that probably in one way or the other it may not be giving you the exact thing that you are looking for then it's important for you to deal with that because at the end of the day if it's not dealt with it will give you or rather you will have results that are not mirroring what you are looking for so things to do with beining method which is the method that works to unsorted data in order of this in order to smooth the data that is there so a whole data is divided into segments of equal size and then various methods are performed to complete the task and each segment held each segment is hard compare separately and one can replace all data in a segment by its me or boundary values that can be used to complete the task the other one is regression and probably that one is more less closer home and in this case data can be made smooth by fitting in to the regression function so you know the behavior using a particular function you know the behavior that you expect and I know this this is more like what we were talking about our re alone you know line of best fields you know if you parent and dependent variables and just looking at this is what I expect at the end of the day as far as this data is concerned and so any data can be fitted in into that particular function that you have also clustering and this is an area that will also focus more on whereby you can cluster data into particular clusters and so anyone that doesn't fit in that particular cluster becomes an outlier and so you can deal with that either by removing the outlier or by knowing why do we have it as an outlier and how can you be able to the the other thing is on on disciplic processing is also data integration and this is more of combined combining of multiple sources of data into one coherent data stock and remember by definition we say that the data that we are dealing with in data mining is coming from multiple sources and so there is a need for us to combine that and some of those combining is for example like the image I have on the screen right now where you have multiple sources source a B and C and you have a wrapper that helps you to wrap this data and this one and this one and put that data in one particular storage and so that's also very important because at the end of the day we would not want let me give an example of group which is key or they are strong in data analytics you don't want to have Google mining your data from all those sites that you visit and you sign in with their email address you don't want them to mine that data and put each one of them in different places but he wants them to be able to combine that data together about a person about a particular customer and store that data all together in one place and that's very very important so so the issue of just you know combining data together integration of data together into one particular database which may be you know a warehouse where you want to store the data over time or maybe just you know one repository where you want to derive your meaning of that data flow but whichever the case it's important for us to note that it was like our saying you don't want to have so many attributes that essentially mean the same thing you don't want to have and even as we continue I can see the number of us here let me confirm that we are still together robots are used in their robot okay I can see you typing Susan oh poor Dora you still there susana you still there Philemon are you them Naphtali jameelah okay okay at least I can confirm we are still together okay so so that's very important the other thing so we think integration is key and and so even as we look at because from next our next session I would want to start off with Association rule mining just looking at a few competition in there around Association rule mining and introducing access to the concept before we dive in into which I want to do to and the end just looking at the they imagine trends where we are at today so now as we talk about a decision remaining we can understand the historical perspective of it but also we can judge close a home into how that is being applied today what are some of those issues that may come with data integration part of that is a dis matching matching of objects so how can the data analyst ordered the computer issue about the customer ID on one database and the customer number on another database is actually referencing the same actions that can be a challenge and that's something that we may need to think about because in different context we call particular attributes differently in different databases we may have a particular entity referred in a different way and so when we are integrating goals we may need to be very careful even by just engaging the stakeholders and getting to law what exactly do you mean in this database when you talk about a customer ID what do you mean when you talk about a customer number in this database what you mean so that that that integration can be a challenge because each and every database may be using their own schema they may be using their own way different naming conventions and so we need to be very careful about that the other issue that you would expect of course without even saying is redundancy because an attribute such as let's say annual revenue for recent forest and may be redundant if it can be derived from another attribute or set of our needs so you know that we if you have age and you have the date of birth you can derive the a I mean if you have the date of birth you can direct the age using today's dates for someone and so we want to be sure that we are not causing redundancy and remember in when I talk about redundancy here I am NOT talking of redundant the way IP implicate databases for security reasons because we do that quite often even in data warehouses where we want to have this particular database replicated so that we have a downtime for one database then the other one can pick up that is not what I mean what I mean by redundancy here is that we have attributes that are referring to the same thing but two or more attributes still referring to one thing and that in itself can cause a lot of inconsistencies because we would want to you remember in databases you must have looked at acid properties what does acid properties mean those who are here what we mean by acids a stands for words B systems for what I stands for what P stands for what also did database i i i bet you must have looked at passage but honesty courage as he does for word okay c stands for for consistency i stands for words and B stands for words so what we are saying what we are saying here is even though that we have used in you know in databases in in other databases that we have used concepts that we have used used before we are very very important even in this piece that we need to ensure that those concepts are actually achieved so therefore if I I mean in for atomicity see for consistency okay I for isolation and D for durability correct yeah so and that's very important because even in this space when you talk about redundancy we don't want to have you know a transaction that is referencing to old Morty needs within a data warehouse or within a determine a database may it be a transactional database may be you know editorial house at the very end of the day you want to have consistency in those in how the naming so as far I've been talking about things to do with you know things to do with what I call that you know the issues of consistency in many conventions is very important I think the concept of order to repeat as is keeping my mind [Music] Yeah right yeah we were talking about normalization story that's what I ordered to refer to normalization which which is also a concept that helps us to deal with redundancy from database perspective it's a concept to deal with redundancy and and it's very important but also detection and resolution of data value conflicts so for the same real-world entity attributes values attribute values from from different sources may differ you may have a particular entity and the attribute value within that ended entity is different from what the perspective of another database looks like and we may need to look at that so those are some of the issues that we need to consider especially when you talk about integration or of course we may not have exhausted all and he will have some that we can add to the lists please feel free to leave a comment on the chat box so that we can also discuss part and boroughs of course if you have any question but we are saying let's try and resolve those issues as we integrate data as we pull data from multiple sources let's make sure that we are able to integrate we are able to ensure consistency is achieved we are able to ensure atomicity is achieved isolation is English shipped but also of course it's funny that we cannot run away from especially when we are talking about literacy we ever feel is data transformation as a concept in data pre-processing which is important to us and this is in data transformation data are transformed or consolidated into forms appropriate for mining now this be wary or entirely dependent on the data mining a gene that you are using because that may inform the data values that you want to use it may inform the you know the attributes of those data values are all the naming conventions that you want use if you may inform a lot of things and so this may work things like smoothing which we we talked about we want to remove the noise within the data and you know things to do with the regression clustering some of those ways that we can use to remove noise within the data because if we are talking of cluster a B and C and a particular dataset or a particular attribute of a data that is not falling within our classes then it's very easy for us to assume that that particular attribute is actually an outlier and we can do away with it or we can treat that as noise within the data then the other thing is also aggregation and well summary or aggregation operations are applied to and for example this could be the delay cells may be aggregated into a monthly report and that happens a lot [Music] Susan I would want you to keep your mic muted as we continue yes please okay so aggregation is also important the other thing is a generalization in generalization of data maybe based on [Music] what is supreme within our data so you can generalize the data based on the supreme attributes or the data that you have per also based on what is the end goal that you want to have at the end of the T what is it that you want to achieve out of that data by the end of the day and that's very important normalization I may not talk a lot about this you must have looked at it in data databases but basically very important is to ensure that as we build data as killed so as to fall within a specific specific range but to be able to lower that range to a particular level attribute construction or something we call feature construction and this maybe you have different attributes but you want to to create new attributes where you construct those attributes and that you add them within a given set of attributes to help in the mining process so let's say for example I saw this with with with Ministry of Health it was who have worked with Minister of Health if you are here you can maybe contribute those who are in the health sector yeah there is a there is a system within the Ministry of Health Paris called phas holidays in one was interacted with PHAs came here maybe you can if there is anyone you can talk to us so that we can maybe engage together and see but our one of the things that is there in that system is it's basically an eye health information system of course Barham one of the things is they try to capture the records of of health within the country and and what they do out of that is also the reporting procedure for health facilities it's based on the Pieta has a PHAs system and I think it's what I have right now let me just share my screen for you to see what I think they are changing the name to K he tires right away what we have known before in DHS was to share my screen this is what I mean so what were I needed to demonstrate around this is for example what is the attributes what is the the the particular unique attributes unique identifier for a given health facility within you know within this system yeah and so if you are to mind assist a business better and probably wants to be able to see the the reporting procedures and how well the reporting procedures are are being followed because one of the things that happened with the Minister of course the funding comes a lot from World Health Organization and other platforms World Bank and many other and so they will always be killed to ensure that the allocation of resources is based on data and so they may come into such a system and see which are the counties that are doing well in the reporting procedures are defined in this particular system and so those are given more funds comparing to the other ones based on if you are doing well in reporting then you are doing well insulting people canyons and so we can give you more money expecting that the Kenyans are going to benefit and so they they they if you are to mine this data and maybe you want to compare this data with data is in source document within health facilities then you may need to know what is the unique identifier here and what is the unique identifier within the source document so that you can either call you know our construct and attributes that can be able to match or do a comparison between the two documents what is real on the crowd but also what is on the system and just comparing that to see we have correct information ping fed within the system but as the star may be an example our quick example that we may think about yes as we talk about attribute as we talk about attribute construction or feature construction within within data pre-processing stages so the other thing is of course reduction we have mentioned about this and this can be done in various strategies cube data aggregation where aggregation operations are applied on data in the construction of the data cube and remember the data cube in this case may be in one way or the other field as a cluster of get per also attribute subset selection whereby Arella relevant weakly relevant and redundant attributes or dimensions may be detected and removed within within the data what you're dealing with the a level is dimensionality reduction a numerosity retraction so the nationality here is where encoding mechanisms can be used to reduce the data set size as long as and this I need to emphasize as long as you are not tampering with the Eid goal that you have for your process if it's a data mining process it's important for us to have the ad goal and some of you I know you'll be finishing this and you will get into consultancy research consultancy for example or some of you may be in that space and some of the fields that if for example you are to do business with the government in that space one of the things that we will ask you is give us an inception report something we call Inception report because we want to see do you really understand what we are asking and that informs them to know can we give you this job and we will give you this job are we able to see clearly and insertion reporter tells us from A to Z this is how the process is going to be executed so so things to do with so things to do with cube detective aggregation attribute subset selection but also like I was saying now the issue of the intentionality reduction and looking at if I reduce this data to this level do I still get to meet the end goal and so the end goal must be well-defined right from onset before you even get into the data mining process also the other concept is describe discretization and conserved hierarchy generation and that is where raw data values for attributes are replaced by ranges of high conceptual levels and data discretization is a form of numerosity reduction that is very useful for the automatic of concept hierarchies so so that was our some of some of those data pre-processing techniques that we can be able to use and just of course sampling a few I may not be able to to finish all of them by just sampling a feel to see that this is a very important concept that we may not leave out everywhere to achieve you know high level accuracy in our mining processes but also even in data warehousing process because before we do warehousing we need to some of those things will be just even things to do with attribute subset selection things to do is even you know ignoring or producing or constructing new attributes that can be able to fit within the data that really wants to see at the end of the tape so those are some of the thoughts that I wanted us to I don't know if I'm breaking for all of us or am i clear please someone else confirm confirm that I am clear or not so that we confirm in in scaling okay I think it's Kevin I think Kevin check your connection you might be having some issues yes let me hear if there is any question from your site let me know if there is any question from your site now next week I want to start off with just looking at some Fialka rhythm so this may come with Timmy come with some a bit of let me see just to look at construe my concept level some few algorithms here and there around what we are covering and and specifically starting with Association rule mining which is one that is important to us but the most important thing if you remember those who are here when we were beginning one of the most important thing that I said is that you may need to move from a conceptual level of learning to an application level where you can even see how can a system be created that can meet some needs within your area or domain of operation on daily basis so I am really calling us to whenever we are learning some of these concepts think about what is it that you can apply in real world or in real life so that it's not just you know concepts you read the notes or you cram them and you pass the example but we can be able to see that this is a limit we can apply but also we can ignite some some of us here to become data scientists um I don't even for a cube check this site I talked about this site who has checked this site cognitive plus dot e I who has checked that Plus that site it's actually a product of IBM and they have just amazing amazing causes in there but you can take around details around data mining just you know let me say just around what we are doing here but also I realized that concepts are not anything you know far away from what each one of these companies are using yes they teach about data science yeah so if you want to learn on data science that's a place para I don't know if that was a question or you are answering a husband but I what I'm saying is data science and data mind data mining is actually a subset of data science the bigger picture is actually data science so okay okay little question yes we are let me even just open because I know this can this can be a game-changer for one or two of us so it's more of cognitive computing and data science let me share my screen because I would want anyone to consider take one or two causes otherwise before you finish school I don't know what to differentiate us from other graduates and graduates are so many from many many universities I am talking about these sites so you can take free courses just sign up and take free courses here if time allows in this semester I will actually make it a pompous re affair that each one of us would take a cost there and our grade you based on that if time allows me but ah yes it's very very important because at the end of the day it's not just what you learn or what you hear me say all these things that we are talking about here you can learn them by your own it's possible yeah but also one of the very interesting thing that I noticed is that actually the concept for example they are talking about their courses around spark around MapReduce now if any one of you here will take a certification by work or any other people out there were satisfying people on data science or big data all those concepts are actually given here so the concepts of MapReduce which is used by by Google a lot are m and N and the concepts of spark fusion insight I think there is a course on that which is a platform by yeah so there are so many concepts in there that you can learn to be very honest with you and and you can finish this season being better but also just to say that I read of the day the question will be when you go and you want to compete favorably with someone on an interview what is it that is different that you are offering that is the whole difference here it's not about I have a degree it's about I have a passion in an area and I think I have been perfecting this area and I think I'm good yeah so it may be just that and you may need to also know that okay let me not start logging in there but uh okay okay so Susan I can see you have a problem with your connection I would want to end our collaboration here each one of us yeah for today and if there is no question then I want you to engage father one is let's hear less I'm just working on that unlocks discussion forum one minutes but I just want us to share all give two takeaways from today's class that you have lunch it may be just your own you know you are on your own then what you have deduced from what we are discussed but I would want you to be critical is share true takeaways from today's class we do you share with your classmates so this is lecture 5 discussion forum this system will work for us - one minutes let me put these but you see but also I want to take attendance for all of us who were here one of the easiest ways I have seen working for me is um just type a comment on the chat box unable to pick your names very easily from there as opposed to as opposed to earlier so the way or of those people working a Hasan keep cause k and others are typed yeah just any comments like what Michael has done any comments they now pick your names from there but also I want you to go to the discussion forum they are created and share to importance or to take-home points that you have picked from today's class share that with your classmates you just replied to the topic but I have am creating now so reply to that forum that I have posted I hope you can see it from that end okay I think that that is also once you posts you can feel free to interact with the other materials that are available there but also I would interest you to just go to gawk cognitive class dot AI check in there see some of the causes they are they are offering you know see which one can you take a self-paced course that you can take and they will give you a badge of course to certify that you did that wasn't passed yeah and that's the way to learn that's a way to learn of course yes so if all of us have done that then I can releases and see you in the next class but before you leave make sure that you have posted on the discussion forum I'm waiting for those comments as well [Music] [Music] 
SRTSVxUnsNI,28,".
.
.
.",2016-04-13T05:42:17Z,Lecture 36 — Mining Data Streams | Mining of Massive Datasets | Stanford University,https://i.ytimg.com/vi/SRTSVxUnsNI/hqdefault.jpg,Artificial Intelligence - All in One,PT12M2S,false,16307,107,5,0,0,usually we mind data that sits somewhere in a database or a distributed file system and we can access the same data repeatedly and it is all available to us whenever we need it but there are some applications where the data doesn't really live in a database or if it does the database is so large that we cannot query it fast enough to answer questions about it examples include quick streams at a major internet site or observational data coming down from satellites answering queries about this sort of data requires clever approximation techniques and methods for compressing data in a way that allows us to answer the queries we need to answer we begin with a brief summary of a stream management system the analog of a database management system the idea of sliding windows is an essential idea that tells lets us focus on on recent data and the streams I will then discuss a particular problem that of counting ones in the window of a bit stream is the bits flyby the fundamental difference between a data stream and a data base is who controls how data enters the system in a database system the staff associated with the management of the database generally insert data into the system using a bulk loader or even explicit sequel insert commands the staff can decide how much data to load into the system when and how fast in a streaming environment the management cannot control the rate of input for example the search queries that arrive at Google are generated by random people around the world at their pace Google staff have no control over the arrival of queries they have to architect their system to deal with whatever data rate there is you might think a transaction processing system like Walmart recording all the purchases at all its cash registers everywhere as a stream and in a sense it is but Walmart has a large but fixed number of registers and checkout clerks can press the keys just so fast so there's actually a pretty well-defined limit on how fast data arrives in such a system so let's see the elements of the datastream model of computation first we assume inputs of tuples just as in a database system although and many algorithms we shall assume input elements are tuples of a very simple kind like bits or integers we assume there are one or more input ports at which data arrives generally we assume the arrival rate is high although we'll be a little vague about how high is high the important property of the arrival rate is that it is fast enough that it is not feasible for the system to store all the arriving data and at the same time make it instantaneously available for any query we might want to perform on the data as a result the interesting algorithms for data stream dejar generally methods that use a limited amount of storage perhaps only main memory and still enable us to answer important we're ease about the content of the stream streams can be queried in two modes the first is similar to the way we query a database system you ask a query once and expect an answer about the state of the system at the time you ask the query for example what is the maximum value seen in the stream from its beginning to the exact time the query has asked this question can be answered by keeping a single value the maximum and updating it if necessary each time a new stream element derives the other kind of query is called a standing query you write the query once and you expect the system to make the answer available at all times perhaps outputting a new value each time the answer changes for instance a standing query might ask for a report of each stream element that is larger than any element seen so far in the stream we can answer this one by keeping one value of the maximum and each new element is compared with the max and if it is larger we do two things we output the value and we update the max to be that value so here is a very simple outline of what a stream management system looks like okay first there is a processor which is the software that executes the queries the processor that could of course be a large number of processors working in concert the processor may store some Standing queries and also allow ad hoc queries to be issued by the user here we see several streams entering the system conventionally will assume that the element that the right end of the stream has arrived most recently and time goes backward to the left that is the further left the earlier the element entered the system the system makes outputs in response to the standing queries and the ad-hoc queries now usually there is some archival storage this storage is so massive that it is not possible to do more than store the input streams we cannot assume the archival storage is architected like a database system we're by using appropriate indices or other tools one can answer queries efficiently from that data we only know that if we had to reconstruct the history of the streams we could perhaps taking a long time to do so now there is a limited working storage which might be main memory flash storage or even disk but we assume it holds essential parts of the input streams in a way that supports fast query we're going to list some examples of the sorts of streams that it could be useful to mine one example is the query stream and a search engine like Google for example Google Trends wants to find out which search queries are much more frequent today than yesterday these queries represent issues of rising Public Interest answering such a standing query requires looking back at most two days in the query stream that's quite a lot perhaps billions of queries but it is tiny compared with the stream of all Google queries ever issued click streams are another source a very rapid input a site like Yahoo has many millions of users each day and the average user probably clicks a dozen times or more a question worth answering is which URLs are getting clicked on a lot more this past hour than normally interestingly while some of these events reflect breaking news stories many also represent a broken link when people can't get the page they want they often click on it several times before giving up so sites mined there click streams to detect broken links we can view a switch in the middle of the Internet as processing streams one stream for each port the elements of the stream are IP packets typically and the switch can store a lot of information about packets including the response speed of different network links and the points of origin and destination of the packets this information could be used to advise the switch and the best routing for a packet or to detect a denial of service attack now the concept of the sliding window is essential for many of the algorithms were going to discuss the simplest form of window is defined by a fixed length and and consists of the most recent end elements received on a stream notice that each time a new element is received the oldest element falls out of the window a variation is to define the window as all the elements that have arrived within some time interval T extending into the past say the last hour this sort of window has a storage requirement that is not fixed since the number of arrivals within time T can vary in comparison defining the window to hold a fixed number of elements lets us rely on needing storage space only up to a certain limit the interesting case is when we're using a window consisting of the last and stream' elements but n is so large we cannot store n elements in main memory and well we have options to recruit increase the size of main memory use many compute nodes to handle one window or use this in some cases we also need to consider the case where there are many streams perhaps millions of streams arriving at the same stream processor in that case n does not have to be very large before we cannot store all the windows in a way that allows us to get exact answers to queries about the contents of the windows so here's a little picture of a stream and a window of length 6 initially the stream has arrived up to this point J the elements KL and so on will arrive in the future ok now k arise the oldest element s is no longer part of the window which continues to hold exactly 6 elements as it always will now L arrives and D falls out of the window and Z arrives causing F to be dropped from the window let's take a really simple example okay we have a stream of integers the window is of size n that is the window will hold the N most recent integers in the stream and we want the system to be able to answer one standing query great what is the average of the elements in the window often we imagine that a stream extends infinitely into the past so we don't worry about what happens before there have been enough arrivals to fill the window however realistically we have to get started somehow so let's store the first n inputs as they arrive and maintain the sum and count development seemed so far until the count reaches n then the average is the sum divided by the count at any point now suppose we have our window full and it consists of the most recent two n elements we also store the average of these elements that averages in the local storage but it's not part of the window suppose a new element I arrives the oldest element J in the window will fall out of the window thus the change in the average is I minus J all divided by n I over n accounts for the contribution I makes to the average and minus J over and accounts of the fact that J no longer it contributes to the average the important point is that in this manner we can answer the query what is the average of the elements in the window doing only a small fixed number of arithmetic steps with each arrival on the stream that is far far better than having to compute the sum and average of all n elements in the window each time a new element arrives but not every query about the current value of the window can be answered in an equally convenient way 
i-Q0t4Lueeo,27,"This video lecture series introduces the concepts, vocabulary, and procedures associated with E-Commerce and the Internet.  The responsibilities for E-Commerce business managers require a general understanding of basic business management concepts as well as basic technical concepts.  Emphasis will be on web-based e-commerce services.",2021-01-10T13:13:50Z,"IT101   Week16 Databases, Datawarehouse, Data Mining and Big Data",https://i.ytimg.com/vi/i-Q0t4Lueeo/hqdefault.jpg,Lenlen Mañabo,PT14M14S,false,76,3,0,0,0,welcome to our week 16 topic on e-commerce so this week we are going to discuss about the databases the data warehouses data mining and the big data after the topic discussions you are expected to differentiate the database data warehouse data mining and big data and understand the application or uses of database data warehouse data mining and big data what is database the database is the standardized data sets designed to make it easily accessible usable and maintained you may say in a plane turns that a database at a location where the data is stored the library is the best analogy of this it contains the huge collections of books or various reading materials and then the library is a database and the data are the books so there are different components of database we have the hardware the software and the data hardware this consists of the sets of the physical electronic devices such as the input and output devices the storage devices and others it also provides an interface between the computers and the real world systems if we say software this is a set of programs that are used to control and manage the overall database it also includes the database management system software itself so the operating system is the network of software that being used to share the data among the users and application programs that is used to access the data in the database management system and if we say data it is a set of characters that is gathered and translated for some purpose usually those are used for the analysis the database management system collects stores processes and access the data the database holds both the actual or operational data and the metadata which is the descriptions about the data how about the data warehouse the data warehouse is a device that regularly retrieves and consolidates the data interdimensional or compressed data stored from the source system it typically retains years of experience and is being questioned for business intelligence and other analytical activities this is usually modified in batches not if a transaction happens with the source network the source systems are the oltp systems or the online transaction processing system it is a program that aimed primarily at recording and processing business transactions information from the source systems was analyzed using the information profiler to identify the data's characteristics the data profiler is an instrument capable of analyzing that extracting transforming and loading or known as the etl this brings the data from the different sources into a staging area it is a system that can connect the source system interpreting the data converting the data and loading it in a target system the direct digital synthesis or the dsdds is a database which stores the data of the veritable warehouse in a format other than the oltp or the online transaction processing the reason to bring the data from the source system into the direct digital synthesis and then query the dds instead of directly querying the source system is that the data is structured in dimensional format that is more suitable for analysis in a dbs the tools like analytic applications the data mining as well as the scores card the dashboards the multi-dimensional reporting tools and other business intelligence software can interactively extract data from a multi-dimensional databases they collect the data to produce different features and results on the front end scenes that allows the users to have a deeper understanding of their business how about the data mining data mining is the process of exploring the data to find the patterns and relationships that describe the data and to predict the unknown or a future values of the data the key value of data mining is the ability to understand why some things happened in the past an ability to predict what will happen in the future so to refer to predicting the future with regard to data mining some people use the term forecasting and some all some call it as the predictive analysis on the other hand when the data mining is used to explain the current or the past situation it is called the descriptive modeling or sometimes it is called as the descriptive analytics or the knowledge discovery implementing the data mining in the business is growing by the day both through the descriptive and the predictive analysis using the data mining we can find the correlation between the purchase patterns and the customer demographics so for example in the financial services industry the data mining is traditionally used for credit scoring to predict the ability and the willingness to pay the loan based on payment behavior or the past delinquency the current debt the current credit history we have the number of inquiries the address the bankruptcies in the customer demographic so in business intelligence class popular applications of data mining are for prod detections we have the credit card industry forecasting and budgeting the finance we also have the cellular or mobile package we have the development by analyzing the call patterns we also have the customers for filing the usage monitoring at the machine service times manufacturing industry the data mining holds great potentials to improve the health system it uses the data analytics to identify the best practices that improve care and reduce the costs researchers use data mining approaches like the multi-dimensional databases or the machine learning the soft computing or data visualization and statistics we also have the so-called market basket analysis it is a modeling technique based upon the jury that if you buy a certain group of items you are more likely to buy another group of items so this technique may allow the retailer to understand the purchase behavior of a buyer this is an information that may help the retailer to know the buyer's needs and the change of the stores layout accordingly and then we also have the educational data mining or the edm these are identified as predicting the students future learning behavior studying the effects of the educational support and advancing the statistic knowledge about the learning we also have the manufacturing engineering wherein the data mining tools can be very useful to discover the patterns in the complex manufacturing processes we also have the customer relationship management it is all about the acquiring and retaining the customers also improving the customer's loyalty and implementing the customer focus strategies and then we also have the traditional methods of prod detection which are time consuming and conflicts and then we have the data mining ads in providing the meaningful patterns and turning the data into an information we also have the intrusion detection the data mining can help improve the intuition detection by adding a level of purpose to anomaly detection it helps an analyst to distinguish an activity from a common everyday network activity the data mining also help extract the data which is more relevant to the problem and then we have the financial banking wherein the data mining can contribute to the solving of the business problems in the banking and finance by finding the patterns the consul cause quality and correlations in the business information and the market prices that are not immediately apparent to managers and then lastly we have the criminal investigation the high volume of the crime data sets and also the complexity of the relationship between these kinds of data have made the criminology an appropriate field for applying the data mining concepts or techniques the text-based crime reports can be converted into a word processing files and this information can be used to perform the prime matching processes then we have the big data plus big data is images photos videos music streams or as a file 2 or the unstructured text such as the documents and emails the website log files the output or the streams and the scientific or industrial instruments and sensors as thermometers the pressure gauges so the big data does not have to be big even 50 gigabyte can set that as big data if the structure is too complex for a normal relational database management system to store so a small data is a simple data structure and big data is the large collections of small data and can be collected from publicly shared comments on social media network website voluntarily gathered from the personal electronic ops through the questionnaires product purchase and electronic checklist data analysts look at the relationship between the different types of data so such as the demographic data and the purchase history to determine whether the correlation exists such as the assessment that may be done in the in-house or externally by a third party that focus on processing the big data into a digestible format so the businesses often use the assessment of the big data by such expert to turn it into an actionable information so utilize the findings from the data analysis and technology to market the same so the goal of the big data class is to increase the speed at which the products get the market and to reduce the amount of time and resources required to gain the market adoption we have the target audiences and to ensure the customers remain satisfied so those are the uses of big data for further readings of our lecture you may visit these sites thank you and good day 
UP4ezNZfcH0,27,"Watch  ""Best Loop programming and Patterns in C- Tips & Tricks "" in the following link...
https://www.udemy.com/course/patterns-in-c-tips-and-tricks/?referralCode=3FCF915E6CE4219798BD

Watch design and Analysis of Algorithms  in the following link.
https://www.udemy.com/course/design-and-analysis-of-algorithms/?referralCode=45A2086EF2867C2E5E21

Watch Formal languages & Automata theory in the following link
https://www.udemy.com/course/formal-languages-and-automata-theory/?referralCode=00701089E34F78DEB062

You can watch  ""Tricky 150  mcqs in C"" in the following link
https://www.udemy.com/course/best-150-mcqs-in-c/?referralCode=5CEC441877BDC511B57C


Technical lectures by  Shravan Kumar Manthri.
Watch Technical C programming
https://www.youtube.com/watch?v=HgBDS_pA-zg&list=PLYT7YDstBQmGL9xVv8uJm7Vn1wF8jzl70

Watch Data Warehousing & Data Mining
https://www.youtube.com/watch?v=cHcyrza0woc&list=PLYT7YDstBQmE50voZ81eLS0hz2gUdZJwp&index=2&t=0s

Watch Design & Analysis of Algorithms
https://www.youtube.com/watch?v=nBRykKq80KY&list=PLYT7YDstBQmHr7eumHSrdo1aTMpqrpPDa&index=2&t=23s

Watch Formal Languages & Automata theory
https://www.youtube.com/watch?v=EtYsnFGIUkA&list=PLYT7YDstBQmHSRKrNApTqquo2FRlMsoHw&index=2&t=9s

Visit us: www.csegurus.com
Contact me @ fb : shravan.kites@gmail.com
Like us on fb: CSE GURUS 

B.Tech CSE and IT: Data Warehousing and Data Mining.

This video explains Association rule mining with Apriori Algorithm. #ShravankumarManthri#CSEGURUS",2018-12-13T12:30:00Z,8.  Association rule mining with Apriori Algorithm,https://i.ytimg.com/vi/UP4ezNZfcH0/hqdefault.jpg,CSE GURUS,PT23M19S,false,44349,583,27,0,32,hi friends welcome to the class of data warehousing and data mining in this lecture we will see what is this Association rule mining so the main aim of this Association rule mining is to generate what are strong Association rules and what are weak Association rules from the rules that are that can be generated by given database so we will see that one how to do this Association rule mining so this can be viewed as a two-step process in the first step we need to find all frequent itemsets in the sec second step generate strong Association rules from the frequent items in the first step is to find all frequent items it how to find all frequent itemsets again two methods are there one is a priori method other one is FB growth method you can choose any one so after that you will find all frequent itemsets from the aprea method or FB growth method after that will it all frequent itemsets from any one of these methods if you if you use and after that by taking those frequent itemsets we can generate strong Association rules from the frequent itemsets how to do that one where the strong Association rules are which say must satisfy minimum support and minimum confidence anyway we'll see this one also first we will go for finding all frequent itemsets after that we'll come to this part that is generating strong Association rules so first we'll do this finding of frequent itemsets with a priori method then we'll come to this generating of a strong Association rules now we will go for the a priori algorithm so instead of going first to this theory part I will explain this a priori algorithm with an example after that we will come here and we'll understand what is this all those things but remember this a priori property that is all non-empty subsets of a frequent items that must also be frequent and especially we will have again two step that is join step and prune step will have join step and prune step remember this point so let us consider one database here which consisting nine transactions where in each transactions what are the list of items that are purchased by customers so in the first transaction three items are purchased that is i1 i2 i-5 in the second I for like this you have all the transactions now the question is asked like find all the frequent itemsets by using a priori algorithm where the minimum support is given as 2 that is minimum support count is given as 2 so how to process this and how we can find the frequent itemsets here so we will see this what is our first step so here you can see scan that D D means database scan the database for count of each candidate candidate in the sense each product here you can see so how many products are there totally 5 products I 1 I 2 I 3 I 4 ever so scan the database that means you need to find count of each items that means that count we call it as support corn so for Ivan how many transactions consisting of Ivan you can check it the total count is 1 2 3 4 5 6 I 1 count is six like this we can write what about I - so how many transactions consisting I - so 7 like this I 3 6 i 4 - I Phi again like this you need to write and this we call it as C suffix 1 now the next step is compare candidate support count with minimum support con what is the minimum support count is given - so compare it that means compare in this support count of course here we call this c1 as also one frequent item set we call it as one frequent why we call it as one frequent means in each record in each line you can see only one item is there that's why this is one frequent anyway in the next we'll find - frequent after that three frequent like that after finding support cone in one frequent itemsets now you need to compare with the minimum support cone that is - so how many items are satisfying this minimum support count if we observe all the items are satisfying the minimum support count so at least 2 should be the support count so all our side is fine right all those things whatever the items satisfying that you need to write here so now this will become Elsa fix one you have taken c1 then you are getting l1 from c1 after that you need to find c2 from l1 after that you need to find l2 like this after that you need to find c3 from l2 like this so up to if you find that total records are empty you need to do like this so now after l1 you need to find C 2 what is C 2 actually so C 1 means one frequent itemsets c2 means you need to generate to frequent item set you need to generate to frequent itemsets so what is these two filaments from l1 you need to make joints of to frequent joints in the sense I'll make one join that is i1 i2 and either join I 1 I 3 like this so I am writing here - I 2 is a 1 join i 1 i 3 next i even i for i 1 i frying from i to i 1 can I write see eye to eye 1 is also equal to I 1 I 2 so I cannot write I try one next I 2 I 3 I can write next i 2 i for i can write next i - i phi i can write so next what about I 3 I 1 I 1 I 3 is there so I 3 a 2 I 2 I 3 is there so I can write I 3 I 4 I 3 I fine so final one I 4 I 5 so like this I need to generate these things you call them as too frequent itemsets all these things are you call them as too frequent itemsets so now you need to write these two frequent itemsets you call it as C 2 here you can observe this is this part you can observe situ means all the to frequent itemsets i am writing here to frequent itemsets now after getting too frequent itemsets and find again the same thing that is to scan the database count of each candidate this for C to support cone you need to write what is the support cone where the count should be taken for each item set that means how many transactions consisting of i1 as well as I do let me take this example here so I won eight I have written so how many transactions consisting both i1 i2 this is the one transaction second and here i1 is there no item here also no I do I cannot consider here i1 i2 third transaction here for so totally I won I to the support count will be for what about I 1 I 3 so i1 and i3 won - I won and I 3 I won I 94 so irrespective of whether other transactions other items are there or not we cannot check that one now I've an iPhone so I won I 4 I even and I for this a one transaction only one transaction next iPhone I find I have one - only two transactions see like this we need to consider if I write all those things like this I will get the theme as like this so he have the support for all those things so this is we call it as say to say from l1 we are getting C 2 which is a to frequent items it now we need to go for l2 what is l2 again comparing the candidates I put what is our minimum so put constraint it is 2 so minimum support count I need to check how many items its are satisfying you can see here 1 2 this is not satisfying and this is not satisfying this is not certain this is also so I have to remove and I have to write the remaining that is we call it as l2 after finding l2 you need to find what next c3 so what is c3 c3 means which is 3 frequent item set so 3 frequent means from this one I need to find 3 frequent 3 joints like in each line I need to get only 3 items not 4 not 2 ok so if I make joints of these 2 I 1 I 2 I 1 I 3 can I combine ly say I 1 I 2 I 3 if I join these 2 faster 2 part so this is a 1 join - I - I've a knife I can I write it as I 1 I 2 I Phi I 1 I 2 I 2 I 3 so can I write it as i1 i2 i3 already written - I - I - I four can I write it as i1 i2 i four so like this we need to write I 1 I 2 I 2 I Phi if I consider so I 1 I 2 I Phi already I have written this so coming to this I 1 I 3 I 1i v i1 i3 i5 so I 1 i 3 HiFi next i 1 i 3 i2 i3 so can I write it as I 1 I 3 i2 i3 means I 1 I 2 I 3 now see every time if I take 4 no one should be common then only I get 3 joints so next i 1 i 3 i2 i for now there is no common item that's why I am getting I 1 I 3 I do I for for frequent items it we are not finding here for frequent we require only 3 frequent that's why we will ignore that part so next I 1 I 3 i2 i-5 that is i1 i2 i3 i5 again for frequent so we will not consider that one also so i 1i v i2 i3 like this if I consider what happens I 1 I 2 I 3 I 5 I cannot consider i when i phi i - i 4 so I 1 I 2 I 4 I 5 so I cannot consider i when i feii to i5 so i1 i2 i-5 I have cons have to consider I 2 I 3 eye-to-eye for i2 i3 i-4 so that is a new one I am getting here next I 2 I 3 i2 i-5 I 2 I 3 I Phi I'll get I 2 I 3 I fine so like this we need can stir i 2 i for it-- wife high-five so i2 i-4 i-5 so like this all three footprint set I need to get this we call it as C 3 now what is the next step again for C 2 we are getting scanning the database for support corner again fine for support count of each one that means the transactions how many transactions we are getting I 2 and I I 1 I 2 I 3 all these three we need to get so what is that support count and for I 1 I 2 I Phi like this we need to find for all those things if I find I will get all those things but I 1 I 2 I 3 I will get 1 - I I 1 I 2 I Phi I will get 2 for the remaining if you find you can find you'll get here the things which are less than 2 so that's why here directly those things are removed those are not frequent itemsets so that's why finally I am writing only 2 you can find for I 1 I 2 I for let us suppose if I check for I 1 I 2 I for combination I 1 I 2 I for one only one so which is less than 2 you can consider a 1 I 3 I fight I won i3 i5 i even i 3 i 1 i 3 only one transaction so like this will get for the remaining item sets which are less than 2 that's why we are not taking because for L 3 those things will be compared with support code which are not satisfying with the minimum support code that's why we are neglecting those things so like this we need to find L 3 now after L 3 you need to get C 4 so what is this C 4 if I if want to get C 4 means for frequent items what are those 4 frequent itemsets from L 3 so if only I have 2 sets so if I combine this target I 1 I 2 I 3 I 1 I 2 - so if I combine I will get only one thing that is i1 i2 i3 i5 so only one thing I'll get so if I go for the what is that support count what is the support count of i1 i2 i3 i5 4 you need to get so if I consider I 1 I 2 I 3 5 so only 1 is there so support count is 1 so this is c4 now L 4 will be empty why because they decent the only one transaction that is not satisfying so until you will get empty said you need to find all those things if you don't get empty here you need to find see fine external file like that you need to get now after getting empty set and what is our solution if you get L suffix K as empty set now your solution will be L suffix K minus 1 so L 4 is empty that's why L 3 is our solution so what is that solution to item sets are frequent we are saying so why we are doing all those things finding frequent itemsets so what are our frequent itemsets one is i1 i2 i3 is one frequent items it I 1 I 2 I Feist and either frequent itemsets that means those customers who are purchasing items I 1 I 2 I 3 combinely are more frequent I 1 I 2 I Phi is also one more frequent item set like this we need to find now if you go to the theory part a prairie implies an iterative approach known as level voice it's why we are saying it has level by size means where k item sets are used to explore k plus 1 that's what so in order to get L 2 I am going for L 1 I'm checking for L 1 you have to get L 3 I am checking for L 2 like this you are getting so that's why you know where K item sets are used to explore K placements so up really property all non-empty subsets of your frequent itemsets must also be frequent so what is the meaning of this one all non-empty subsets of your frequent what is a frequent itemsets we are getting here I 1 I 2 I 3 is a 1 frequent item set one more is i1 i2 i-5 so if I say i1 i2 i3 is a frequent item set there must be is all non-empty subsets what are the non-empty subsets I 1 I 2 I 1 I 3 i2 i3 and also even separately I do separately I threw some so all these must be frequent if you see this constraint if I say i1 i2 i3 is the frequent itemsets so all subsequent one is i1 i2 is it frequent as you can see here i1 i2 support count satisfying I 1 I 3 what is the support count for satisfying I 2 I 3 what is the support count for satisfying I 1 separately what is the support count 6 satisfying I 2 separately 7 satisfying I 3 separately satisfying I 1 I 2 I 3 combinely satisfying so all non-empty subsets must satisfy the minimum support constraint that is why this property is called as a prairie property and that we are following and we are getting the answer so join step and prune step join step means L suffix whatever we have taken that is a join step that means to find Elsa Fiske a set of candidates cadence is generated by L suffix - one to get l3l one joined elven sets are joined to get L 4 L three sets are joined like that the sets of candidate is denoted by whatever the result we are getting we are represented by C suffix the prune step means how you are pruning C suffix K is a superset of else' fix so all the super segments all the subsets we are writing here that is its members may or may not be frequent but all the frequent K items are also included in we have included everything you can check it to reduce the size of sea-surface k how we have radius we have checked for a prairie property so this is about a priori algorithm now after getting frequent itemsets will go for generating strong association rules from frequent itemsets so how to do this one so those which are strong association rules those rules must satisfy both minimum support ID minimum confidence anyway that minimum support we have done in the previous one that is with Apriori algorithm we have check it for every item sets with minimum support count now the constraint with minimum confidence will come here so we'll see that one so how to do this one it's a process Association rules can be generated as follows the first step anyway we'll do here and here you require how to find the confidence also this is the formula for confidence a implies B is a one rule for that support count of a union B buy support count of here we'll see this one now how to generate so here we receive two items X frequent item that is i1 i2 i3 and the other one is I 1 I 2 I Phi so we'll take any one of this and we will go for generating strong Association rules so I will take I 1 I 2 I Phi for each frequent items it so I have taken L which is frequent item set that is L generate all non-empty subsets of M so what are the non-empty subsets of i1 i2 i-5 if it if this is a one set what are the non-empty subsets so all subsets I am writing here so empty set we are not supposed to take after that I 1 is a 1 subset I 2 is another i-5 is another then held over Ivan and I - then I will go for I 1 and I fine then I will go for I 2 and I fight then I'll take - and I eat - wine - so all these are non-empty subsets of i1 i2 i-5 I have taken so after that for every nonempty subset of s so each one I call it as yes so each subset I will represent with s of L output the rule what he is saying is we need to write a rule that is s implies L minuses so i1 if I 1 is s what is L - L means all these things so L minus I 1 so remaining 3 2 will come here so I 1 implies I 2 and I Fri here I 2 implies - and I Phi I Phi implies I - n Pi Phi here I 1 and I 2 si s is iver and I - what about L I 1 I 2 I Phi if I make - and the right hand side I will get only - so here I 1 I fight that so I will get only I - yeah right - Wi-Fi I'll get only Ivan so here all those things are there L minus s both are same so I'll get PI here so if I observe those rules left hand side right hand side is there for every rule except this one right answer there is no so that's why I will make like this one and we'll take these six rules only first so we generated rules first among these which are strong Association rules how to find it so for that we have a constraint where which one you call it as if support count of L by support count of s is greater than or of course this is we call it as confidence the formula for confidence is greater than or equal to minimum confidence so first of all we will find confidence value for each one so confidence value for the first rule so there's at all 1 2 3 4 5 6 so confidence is equal to support count of L by support count of s what is in I 1 I 2 I Phi so what is the support count of I 1 I 2 I Phi if you see in our previous one in frequent itemsets you can observe what is the support count of I 1 I 2 and I Phi so if you observe here I 1 I 2 I Phi the count is 2 like this we need to consider support corner fell that is to buying support count of s s means left hand side that is I 1 so what is the support count of I 1 so the support count of I 1 is 6 here like this we need to consider so 2 by 6 which is approximately thirty three point three percentage next if you go with the next one confidence that is support count of L by L means of course to only buy support counterfeit what is s value left hand side that is i2 what is the support count of I 2 7 you can observe there you can see in that so this is approximately 30 percent I am taking approximately confidence is equal to next about count of ll means to buy what is on the left hand side I find what is the support counter Phi Phi so that is to only so which is 100% next confidence is equal to here support corner of elements to buy what is on the left hand side I 1 and I 2 we need to check for support count of i1 and i2 so i1 and i2 the support count is 4 so which is 1 by 2 which is 50% next confidence Rizzy cost so to buy so what is there on the left hand side we need to check I won and I 5 what is the support count of I 1 and I effect so that is two hundred hundred percent and the last one so put count of L means 2 by I 2 and I 5 2 so which is hundred percent now we received all these confidence values now you can check out what are the strong association rules which this conference is greater than or equals to minimum confidence value let us suppose if the minimum confidence value it is given as 60 percentage I am taking minimum ground as value as 60% so which rules are satisfying these 60 percent one two three so the rules three five six the rules three five six are strong Association rules like this we can find which are strong association rules and weak Association rules so this is about generating strong Association rules here by using Association rule mining thank you [Music] 
