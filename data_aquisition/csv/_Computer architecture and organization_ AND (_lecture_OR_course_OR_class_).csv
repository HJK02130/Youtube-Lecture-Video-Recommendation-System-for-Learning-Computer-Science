id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
Zfeh2TODr9Q,27,,2016-09-30T09:23:00Z,CS101 L002A Computer Architecture,https://i.ytimg.com/vi/Zfeh2TODr9Q/hqdefault.jpg,Studio IIT Bombay,PT54M46S,false,1381,20,0,0,3,so what we're going to do today is a sort of rehearsal of what you be doing in your most rooms so there are lectures and in today's class and probably also in the next class we're going to play the recorded lectures in classroom and then subsequently in rooms and we will only work with problems with problems and without sin hello and welcome this lecture is about a simple computers architecture here is a quick recap of some relevant topics that we have already discussed earlier we have discussed the dumbo model of computing in which we have seen that there are named dryers for storing values and retrieving values from there are some workbench registers which dumbo uses for doing computation and dumb balls who uses cards to do input and output when we instruct number to do some input-output operation in this lecture we are actually going to see a little bit in detail what is it that we will actually be programming essentially from the dumbo model of computer we are going to move towards a real computer which by the way is almost like a Dumbo we will see a lot of similarities between what we have seen for Dumbo and what a real computer contains but we would like to see what this real computer is water it's different parts what are its architecture what do the different parts do and how is information represented inside a real computer so that's our agenda for this lecture so here is a very high-level block diagram of a simple real computer I'm going to explain each of these color-coded parts in more detail but just to get going here is something that I have labeled as main memory and in it I have put these labels location 0 location 1 location 2 and so on now these are like Dumbo's dryers remember that number needs named dryers to store information and also to retrieve information so this main memory in a real computer is much like Dumbo's dryers then recall that Dumbo had a work bench in which number would keep values and then do some operations with them number would fetch values from the named dryers to the work bench and then do some operations with them so in a real computer that part is what is called a central processing unit or a CPU a CPU has several registers and the dumbo model as well we had seen registers we will study a little bit more about what registers really do a CPU also has an arithmetic and logic unit where the real computation happens and it has a controller now dumbo also shipped information from one part to another for example from the named dryers to the workbench or from the name dryers to the input or output devices by means of some cards now in a real computer the analog of these cards are what are called buses so here you can see I have written a bus and on this bus there are disks input devices output devices here are some other buses which is called an address bus and a data bus and these also transfer information between different parts of a real computer so having seen the similarity between the dumbo model of computation and what a real computer looks like inside let's go a little bit more in detail into each of these parts here I've highlighted the main memory that we just mentioned now this main memory is usually also called random access memory or RAM or it may simply be called memory basically it contains a large number of addressable locations just like Dumbo's chest of drawers had a large number of named Troyer's each location in main memory stores exactly one value at a time what is very important to remember is that even if you did not store a value at location of memory there is some garbage value already there so every location in memory at every point of time has a value if you stored it then it contains the value that you stored if your program stored it then it contains the value that the program stored otherwise it contains some arbitrary garbage value so when you are accessing a location in a memory you should be prepared to encounter garbage values if you have not already saved something or if your computation has not already saved something there this is very important when you get to real programs now writing to a memory location destroys the value that was originally there however reading from a location makes a copy it does not destroy the the value that was there note that these are exactly how Dumbo's Dreyer's also operate let's now focus on another part of a computer which is the set of registers or the bank of registers now registers are pretty much like main memory except that they're much faster you can read and write from registers much faster than you can do from main memory and of course you pay a price for it they are much more costlier to design maybe 32 registers you may have to use the same amount of resources as to design maybe a thousand locations in memory so that's why registers are much costlier and because they are costlier they're usually present in smaller numbers which means you have fewer registers then you have locations in main memory now for most of the programs that we are going to study in this course we will not directly see these registers although of course these registers are going to work behind the scenes the real computation happens in the central processing unit by taking values from registers and storing values back in registers however in our programs we are not going to see too much of these registers nevertheless it is important to remember that advanced programming can refer to registers directly we will not cover this in this course now here I've tried to highlight two other parts of the computer the arithmetic and logic unit and a controller which are also parts of the central processing unit the arithmetic logic unit is really the workhorse of the computer all the arithmetic operations like addition multiplication division subtraction and also are logical operations like logical anding oaring ixora ng complementing all of these happen here that it may take logic unit usually takes operands from registers and stores the result back in registers the controller if you see the controller here has some arrows going to the registers and the arithmetic logic unit so the controller is responsible for sequencing various actions that are involved in executing instructions on the computer we are not going to directly see this controller as we write programs to execute on a computer nevertheless this controller plays a very central role in the functioning of a computer in this picture I have highlighted the disk and some input devices and output devices which are connected on the bus disks are basically longer-term storage for files and files could contain your programs could contain some data it could contain some logs some outputs and so on no disk serve the purpose much like main memory they are used for storing information and retrieving information however they're much much slower than main memory they also have much larger capacity many more locations than main memory and they're also significantly cheaper than main memory so as you can see in this picture there is main memory and then there's this register file which is faster smaller and costlier and then there's this disk which is slower larger and cheaper than main memory now we are going to look at disks much more later on when we study about files input/output devices also called IO devices these basically allow a computer to interact with the world outside for our purposes an input device is going to be a keyboard and output device is going to be a console although computers can interact with a lot of other kinds of i/o devices finally I want to highlight the buses here there is an address bus data bus and this is a bus on which the disc and input/output devices are connected the buses are really the highways for transferring information between different parts of a computer and the dumbo model number would transfer information using cart in a real computer buses serve that purpose we can have different kinds of buses like address buses in which addresses for specifying locations of memory are sent data buses through which data may be shipped between a memory and central processing unit IO buses through which data information could be transferred between i/o devices and the computer external device buses for example all of you are familiar with USB which is basically a bus the universal serial bus for our purposes as we write programs we are not explicitly going to see buses they are going to be working behind the scenes nevertheless they are an important part of the computer now how is information represented in a computer all information in the computer is represented as sequences of zeros and ones so if you were to take a snapshot of the computer that we just saw it would probably look something like this that there is main memory and there are sequences of zeros and ones stored in various locations on the address bus and on the data bus there are sequences of zeros and ones and everywhere it's basically sequences of zeros and ones now why is it that computers talk only in the language of zeros and ones because electronic circuits in the computer can efficiently store and process electrical signals in one of two states for our purposes we call them 0 & 1 and therefore all information in a computer is basically going to be represented as 0 & 1 so this is also called a binary representation and these two binary digits are also called bits information is usually accessed in a computer not in terms of individual bits but in chunks of eight bits which is also called a byte so for example when we access memory i/o device registered or when we send an address we usually send chunks of eight bits and it's very important to understand how numbers characters strings the different kinds of things that a computer is going to process that will write programs to manipulate are represented using bits and bytes but this will be the topic of another lecture so in summary what we saw in this lecture is the architecture of a simple but real computer the main parts are of main memory registers and arithmetic logic unit input/output devices disk bus and of course there is also a controller which along with arithmetic logic unit constitutes the central processing unit we have also seen that information is represented in a computer using zeros and ones which are called bits and we we access bits and chunks of eight bits at a time which are also called bytes thank you so this is how a lecture module is going to look like this is roughly thirteen or fourteen minutes and it will discuss one topic here it was the basic architecture of a computer you can rewind it and see which other part you did not understand but basically in a couple of classes from now we will not be playing these lectures over here you are supposed to be seeing these in your hostel rooms before coming here what we will be doing mostly in the classroom is what we are going to do now you've seen a lecture and now we are going to have some discussion we are going to try to you know solve some problems answer some questions okay so was there any doubt about what you saw so far No so here is a question that you know I'd like you to think about an answer so we've seen how computer looks like this main memory ALU buses disks and all of that and of course this entire course is about programming a computer so we are going to run programs on a computer the question is when a computer is running a program why does the program recite is the question clear right so you think of the dumbo model of a computer so Dumbo executing a program means whatever it's opening drawers taking out things taking to the workbench and so on but of course dumbo needs some instructions and it's just following those instructions and the question is where are those instructions okay so the way I would like you to answer this question is that on your notebook please write down what you think is the correct answer you've written down so now please swap it with your neighbor and see whether your neighbor agrees with what you have said and if there is disagreement you know in low decibels try to convince each other whether you're correct or your neighbor is correct and if there is agreement then at least try to say why you think you wrote a particular answer and your neighbor say why he or she thought the same answer so I mean this is a simple question so I hope you have convinced your neighbor or your neighbor has convinced you about the correct answer so now now let me ask how many of you are now convinced after discussions with your neighbors that the program resides in registers please raise your hands okay but your neighbor doesn't agree with you is it oh okay so so some other people who said registers your registers please raise your hand those who answered register ok and your neighbor doesn't agree with you is it because I don't see pairs of hands going up so so let me ask your neighbor why he doesn't agree no but why is registered not to answer ok so why do you think something like but not in the main memory I mean I like hitting the road in the meanwhile I go to usually have to compile it okay so so he thinks that the program has to be stored in the register okay now what about main memory how many of you think its main memory we 50% of the class and how many of you think it's the disk you people and I would devices anybody nobody ok so I would device is out of our consideration now now those who said main memory can you please raise your hands again so you know the point of this discussion is I mean even if your answer is correct the reason why you're saying that answer may not be correct and what we'd like to I mean more than the answer this discussion is about trying to figure out why it is the answer and why something else is not the answer right so I mean you said a lot of things in favor of main memory but it didn't say why anything in against registers or anything against risks okay so so that's an interesting answer the disk is too slow and the registers are too smell so the main memory is somewhere in between okay anybody else you decide in the main memory because when we give some input for the execution of a data it will first of all it will be stored in the main memory then it will be copied from there so one copy will actually remain in the main memory and the other will be executed on the register right so far so far it is in main memory part of it is in registers what do you think so you would pick both registers and main memory from here one part will reside in the main memory and now the other or wipe out execution will be done either register the white plate okay so so you're basically saying both registers and main memory right part of it is enriched Oh so there's somebody from there yes so also which do you think what are you arguing a for or against yeah I'm for me in memory because I think because main memory always creates a copy of that thing and registers once we do something or calculate it just goes away means it's not it's kind of empty memory as we saw it from the demo model and main memory always create a copy so whenever we want to execute we can execute register so when you main memory creates a copy when you read from the main memory that's what do you mean right yes so from the register also when you read right so if you look back at the you know so these are the slides so you know for the registers they are just faster costlier smaller versions of main memory so other than that they're the same reads or anything in a range there are if we could not store anything in the register why is it I mean we can store I mean they're just like it is just in there faster and there are fewer registers than main memory so other than that you know when you read something a copy will be made when you write something the previous value gets destroyed so I think that it's it's almost the same as this memory has got large number of addressable location and each location shows a particular value so it's very easy for number to work with the main memory besides registers work too slow and they have a very limited memory so it will be very difficult to work with registers and this if it works with the disk it will again be very slow so I think it should be mmmm okay so so that's you know very nice yes so I'm sure you know there are other people who want to add that but in the interests of time you know I think we can continue the discussions in the interests of time so the right answer is indeed main memory but then the right answer is the reason why main memory is the best place to keep it and as some of you have already pointed out so I'm very happy that you know this came out in the discussion in the class is that a program typically will be you know I mean have you heard about so for example you know a banking application do you have any idea of how many lines of code it is or any industrial application let us say an airline's ticketing application so so any idea of how many lines of let us say C or C++ or Java code would it be ten lines or the Windows operating system on the window operating system how many lines of code is it there's the program write an operating system is a program that's running yeah lakhs and lakhs of lines so you have it's a very large amount of a program is actually a very large amount of information so if you want to you know store it in registers I mean that it won't fit in your registers right so some of you already said that registers are too small then - fewer now what about the disk the disk is of course it's much larger than main memory you can store whatever you want over there but then these are very slow right so I mean often times you know you can see your you know computer or laptop you ask it to read a big file and you can sort of hear the disk spinning car car car car car / sound happening right the disk platters are spinning and it takes time to access that so how many of you sort of run a computer which which is running on solid state device memory SSD memory yeah No okay anybody who has a tablet and iPad or something so okay so there are different kinds of you know memory there are different kinds of discs also these days there are SSD disks and what you would see immediately is that the time it takes to access data from disk with it to read or to write is significantly more than the time it takes to access data from main memory so program is roughly about the size that is okay for the main memory and we also want to access it fast enough we don't want too much time to get one instruction after another so the way memory is kind of the right place for it okay so so it's good but this came out in the discussion from the class now related question that we now that we have discussed many memory registers and disks is you know why not just work with disks right so of course you know we've just discussed that if the program were to reside only on disks it would take a long amount of time but then one might ask okay then maybe main memory on disks why why do we need registers at all if you remember out three of them main memory registers and disks are for reading and writing information register is a smaller faster costlier disks are slower larger cheaper and main memory somewhere in between so it's kind of like a hierarchy there so the question is why do we need all three of them so once again you can write down in your notebook a couple of lines and then after that we'll exchange it with your neighbor so let me pick somebody at random the program code in the main memory will be original Natron if another program is coding that so we have to use this to show the program as a register for a very small who we have to use the dixie in place of registers and the main memory and to do their programs faster disks are not useful so we have to use their religion rather than main memory okay so it's more or less the same answer yes make a mean memory sympathy so we will [Music] resistors are neither completely calculation whatever sort of and it does money to remember [Music] remove the final violence well maybe the [Music] so so you're saying what the different what the three different things are to be used for okay okay good so you know I mean ideally what we'd like to do is we'd like to have fast memory and cheap memory and a lot of memory right you would like to have the fastest cheapest and highest capacity and if somebody could give that to us we wouldn't need all three right unfortunately today's technology says that we don't have all the three together if you try to make something fast it becomes cost year right if you try to increase some you know if you try to make a large fast memory it will be really really expensive so you know these are sort of the three levels of a hierarchy as you know we have discussed in the lecture discs are slower but they're much larger and cheaper main memory is faster much faster than the disks but they're also much smaller because they're more costlier than disks and registers are even more costly or even even faster and therefore they're even smaller so it's kind of like an engineering optimization problem right I would like to have the best of all worlds right I want the same price as I pay for disks but I want the same speed as I get for registers so therefore I have to organize things appropriately you know things which I do not need right now for my computation can stay the less expensive part which is the disk because that's what I don't need for computation things which every really need for my computation I want high speed that should be in the registers and the main memory is of course you know as somebody pointed out it has large enough number of locations that most programs would fit there but the speed is still not as high high enough to match the speed of the ALU I mean if we could make main memory fast enough to match the speed of the I mean if the main memory just became a huge bank of registers probably we wouldn't eat registers but that machine would become very very expensive okay so it's really this served three different purpose and somebody also pointed out disk serve the purpose of more permanent storage right you write some data there and it's certainly magnetic disks are much more you know more permanent storage and these days we have solid state disks there are so much more storage so these three things actually serve three different purposes so while one could think of a hypothetical computer which runs only on disks or only on registers you'd be sacrificing if you run only on disks you have to sacrifice in speed if you run only on registers you have to sacrifice on cost right it'll become very expensive so this is a kind of an engineering optimization problem where you want to get speed and also reduce the cost and so you maintain it in this way okay but the part that you're really computing it you want it in the fastest memory the part that you're not computing can stay in the slower memory the part that you need much later why keep it in the fastest memory let it stay in the slow memory right and as we said in main memory the program also stays because we do want to access the program at a speed that is faster than accessing disks and so there is a separate part of main memory when the program will reside the data will be in another part of the main memory and they're not going to interfere with each other okay good so now what we are going to do is we are going to proceed for the subtopic what we will do is we will start the second lecture module so this is again you know around 13 14 minutes and once again so now you know the way we are going to conduct the class you're supposed to see the lecture and then there will be questions some of these questions may be simple some of these may be thought-provoking some of these may not have one unique answer but we are basically going to have the discussion in the class okay this lecture is about how computers internally represent integers here is a quick recap of some relevant topics we have already studied we have seen the architecture of a simple computer and we have also seen that information is stored in terms of binary digits or bits which are usually grouped in chunks of eight called bytes in this lecture we will study how numbers and specifically how integers are represented inside a computer and we are also going to see how in a C++ program we can declare integer variables now from the earlier lecture you would recall that a snapshot of our simple computer looks like this where we have different parts of the computer and in each part we basically have sequences of zeros and ones so the question is in such a setting how do we represent integers like 56 or minus 37 in terms of zeros and ones inside a computer now when we talk of representing integers the most natural thing that comes to mind is a decimal representation with which all of us are aware so for example when I write a decimal number like 233 I am really using base 10 which basically means that to interpret this number I take two and multiply by a power of 10 in this case 10 squared when I take 3 multiplied by a power of 10 in this case 10 raised to 1 this 3 is multiplied by 10 raised to 0 and I add all of them to get the value of this decimal representation of the number so we call it base 10 because this number which we are raising to various powers and multiplying it with the digits here is 10 and in base 10 representation the digits that we need are from 0 to 9 now there is nothing very special about a decimal representation we could also do this using base 2 in which case we will call it a binary representation and just like in base 10 we need numbers from 0 to 9 to to represent an integer in base two we need numbers from 0 to 1 which is one less than whatever the base is to represent an integer so here is a binary number 1 1 0 please do not mistake this as 110 it would be 110 in decimal representation but in binary representation it is 1 1 0 and let us see what this number really represents what 1 1 0 represents so just like in the decimal representation I have got to take the numbers that are appearing here multiply them by an appropriate power of 2 the base is 2 here and then add them up to get the number represented over here so for example here it is 1 into 2 squared plus another 1 times 2 raised to 1 plus 0 times 2 raised to 0 or 6 so this is the binary representation of the number 6 and if you understood how I interpreted the sequence of zeros and ones as an integer you would realize that any sequence of zeros and ones can be thought of as representing an integer now that we know how to represent numbers in both binary and decimal notation how do we go from one to the other so for example I could ask you given this binary number I am going to denote binary numbers in green and decimal numbers in violet so given this binary number what is its decimal counterpart so the first thing to note here is that this binary number has 4 bits so this bit is going to be associated with 2 raised to 0 this with 2 raised to 1 this with 2 raised to 2 and this with 2 raised to 3 so the maximum power of 2 that we're going to use is 3 that is the number of bits minus 1 and then how do we figure out what integer this is in the decimal notation we take this numbers start with the bits and keep multiplying them by the appropriate powers of 2 so this is 1 multiplied by 2 raised to 3 plus 0 multiplied by 2 raised to 2 plus another 1 multiplied by 2 raised to 1 and the final one multiplied by 2 raised to 0 and that gives us the number 11 note that in this representation which is the decimal representation of the number 11 I am so using the digits 1 and 1 this is the binary representation of the same number 11 in which I am also using the numbers 1 and 0 however this representation is very different from this representation this is a base 10 decimal representation this is a base 2 binary representation of the same number 11 so if you look at 1 0 1 1 this bit which is associated with the highest power of 2 is called the most significant bit or MSB this bit which is associated with the least power of 2 which is always 2 raised to 0 is called the least significant bit or LSB so we saw how to convert a binary number to a decimal number let's see how we might convert a decimal number to a binary number so given 23 written in decimal what is its binary counterpart so the way we are going to do this is since we're interested in binary numbers we are going to repeatedly divide this number by 2 and collect all the remainders as we go so I first divide 23 by 2 I get a quotient of 11 and a remainder of 1 and we will see that this remainder of 1 will be only significant bit in the binary representation of 23 now what does this mean the 23 divided by 2 give me a quotient of 11 and remainder of 1 it really means that 23 can be expressed as the quotient times the divisor 11 times 2 plus the remainder now what do we do with this quotient 11 we repeatedly divided by 2 again and we get a quotient of 500 remainder of 1 and now I could express 11 as 5 times 2 plus 1 and if I did that in place of this 11 if I substituted 5 times 2 plus 1 I would get this expansion of 23 and if I expand it out further I would get this representation and here you see now that this one corresponds to the least significant bit which is 1 this one corresponds to the next significant bit which is another one and I could keep doing this I could take 5 divided by 2 get a quotient of 2 remainder of 1 then Express 5 as 2 times 2 plus 1 just as I've done here and that way I get this representation of 23 which actually gives me the next remainder and finally I take the quotient here which is two divided by two again there a quotient of one remainder of zero and now if I express this two as one times two plus zero this is the expansion of twenty three I get which finally gives rise to this expansion of twenty three now where do I stop I will continue until I get a quotient of zero so I divide one with to get a quotient to the zero and it's only then that I stop I get a remainder of 1 and this becomes my most significant bit so as you have seen that we took the decimal number 23 repeatedly divided it by two kept collecting the remainders in order from the least significant bit to the most significant bit and that gives us the answer one zero one one one as represent binary representation for the decimal number twenty-three here what we discussed how to represent unsigned numbers in binary so now what I'm going to do is I'm going to ask you to answer this square I give one zero one zero one zero I ask you what is it as an unsigned integer please write down in your notebook okay now swap your notebooks with your neighbor and just see I think for most of you this probably will not require any discussion at all most of you would agree what the number is okay so no discussion no argument nothing so this is 42 essentially this this is 0 into 2 raised to 0 that's 0 into 1 that's 0 this is 1 into 2 raised to 1 so it is 2 from here then 8 from here and 32 right so 32 plus 2 plus 8 that is 42 so now what we'll do is we'll see I mean in a computer when you're working with you know numbers it's not just you know unsigned numbers or positive numbers that you want to deal with you also want to deal with negative numbers so let's see how we're going to represent negative numbers in particular now we are going to build both with positive and negative numbers so we call this signed integers and let's see so far we've talked about representing unsigned integers so how do we represent signed integers like minus 1 or minus 23 well the most simplest way to do it might be to just treat one of the bits as the sign bit so we could choose the MSB to be the sign bit and if this bit was 1 we could treat the number is a negative number otherwise we could treat this number is a positive number so here if we consider integers represented using 3 bits I have placed all these 8 valuations of 3 bits along a circle marked and colored the most significant bit the ones marked in red are the ones where the most significant bit is 0 the ones marked in blue are red the most significant bit is one so in our notation these would represent positive numbers these would represent negative numbers and this is what I would get so all of the numbers with MS be 0 are positive and then the black part just gives me the value of the number 0 1 2 3 all of these with MSB 1 and negative and then the black part of the number of the binary representation just gives me the value of the number the magnitude of the number which is 0 1 2 3 so you see that the values represented increasing this way 0 1 2 3 and also increasing this way minus 3 minus 2 minus 1 0 and in fact we have two representations of 0 here so this is kind of wasteful and so what we might want to use is another representation which is called the two's complement in which we still consider the MSB to represent the sign of the number and so in our circle along which we have placed all the valuations of three binary digits we would still have these four representing positive numbers these four representing negative however we are now going to treat these positive numbers is 0 1 2 3 and instead of these being minus 0 minus 1 minus 2 minus 3 we are going to now consider minus 4 minus 3 minus 2 minus 1 so effectively we are starting from here continuing to increase minus 4 minus 3 minus 2 minus 1 0 1 2 3 so we have one complete sweep over here in which all the numbers represented increase and of course finally when we go from here to here we would go from plus 3 to minus 4 the largest integer represented to the smallest integer represented so that's the wraparound that must happen so you see that in this way of representing signed integers we have been able to represent eight numbers minus four through three and there's only one representation of 0 this is also called the two's complement representation and one might ask if there is an easy way to figure out given such a binary sequence what number does it represent in two's complement we could draw the entire circle like we did here and try to figure out what number it represents but is there an easier way to figure out it turns out that there is so you first look at the MSB and the MSB here is 1 which means the number is a negative number then to get the absolute value of the number you ignore them SP and look at the rest of it so in this case it's 0 triples 1 the first thing you do is flip every bit in this part which you obtain after ignoring them is B so 0 triple 1 with every bit flip becomes 1 triple 0 this process is also called taking the ones compliment so 1 triple 0 is the ones complement of 0 triples 1 and this as you would recognize represents the decimal number 8 and now what we do is we take this number and add 1 to it to get 1 0 0 1 or the decimal number 9 and this is now called the two's complement of 0 triple 1 and this gives you the final absolute value of the number that was represented here so the absolute value here is 9 the number was a negative number so 1 0 triple 1 in two's complement represents minus 9 now a C++ program how do we go about saying that something is going to store integer values we use this keyword int which represents the integer datatype I have highlighted the keyword representing the datatype and read and C++ actually allows different kinds of integers for example I could have short integers long integers long long integers and sign long integers and what all these different qualifiers do is they tell us how many number of bytes are to be used to store the value of the integer in a short integer I use two bytes and standard integer I use four bytes in a long long I might use eight bytes and they also tell me whether I should treat the integer in two's complement or not so all integers unless explicitly specified otherwise are signed two's complement representations if you specify something as unsigned then of course it's an unsigned integer and it's not hard to see that the maximum and minimum values will depend on the number of bytes and whether you are treating it as a signed or unsigned integer for example if I take a standard integer which is signed four bytes then - 2 raised to 31 is the smallest is the smallest number that you can represent and plus 2 raised to 31 minus 1 is the largest number you can represent whereas if I take unsigned long long which is 8 bytes I can represent values from 0 through 2 raised to 64 minus 1 in C++ you declare a variable of integer type by putting the type name here int then putting the variable name my marks or you say unsigned shortened that's the type name with the qualifiers and then the variable name you can also represent constants integer constants in C++ they could be represented in the decimal notation which is the usual notation that we always use a sign followed by digits from 0 to 9 the only thing you have to notice that you cannot start it with a 0 unless you're representing the value 0 because if you do start with a 0 it represents a constant in the octal notation which is basically base 8 notation so if I say 0 3 7 2 it's not the decimal number 3 72 but it is the number three seven two and you can work it out yourself what it represents in decimal and in binary similarly I could also use hexadecimal in which case I have to prefix 0x and in hexadecimal which is basically base 16 I need numbers from 0 to 15 but we only have the numeral 0 to 9 so we use a through F to denote 10 through 15 so if I have a number like this prefixed with 0x it's a hexadecimal number and using the usual expansion in base 16 I could figure out what number it is in decimal and what number it is in binary as a declaration I could say that something is a constant integer so here this constant name scale factor is a constant integer and this is the value of this constant and when I say constant in the value of this cannot be changed during program execution so here is the summary of what we have studied today we have looked at binary representation of integers conversion to and from decimal two's complement representation and C++ recreation of indigents thank you so that was unsigned integers and signed integers and now I would like you to answer this question so it's the same binary string that we saw earlier 1 0 1 0 1 0 but now I want to treat this as a signed integer ok so I hope everybody has come up with an answer for that the answers may differ now swap it with your neighbor and in this particular question in the previous question when we were interpreting 1 0 1 0 1 0 as an unsigned integer I did not expect any discrepancy between what you wrote and what your neighbor wrote but in this case I wouldn't be surprised if there are some discrepancies and both of you may be correct ok so have you swapped with your neighbor and discussed in how many cases are there mismatches there is a pair of students whose answers do not match for all pairs answers match that is a bit disappointing I was hoping there will be some because there are two different answers to this question so how come nobody came up with the other answers okay so so let me ask yes can you the person behind ya so what is your answer pardon me - 20 - yeah - 11 okay and your neighbor also got - 11 oh so there is a discrepancy and have you convinced each other why there is a disturbance no so please try to tell him why you think it's minus 11 and he is going to try to tell you why it's so so somebody have about minus 22 and your neighbor also about - ready - -20 - so anybody got something different from - 20 - yes - 23 okay so we have minus 11 minus 22 - 23 and your neighbor agrees with you so who is your neighbor all three of you got - 23 oh okay okay and whatever resents the two birds - 23 so all four of you got - 23 okay so so let me ask you how did you get - 23 so anybody got - 23 just the four of you okay so you are a bit isolated okay so so tell tell us how you got - 23 we need to just invert like 1 will be 0 0 1 it will become 0 1 0 1 0 1 first of all ok when you have pain but it will be 0 1 0 1 0 1 but what is the last 0 1 0 1 0 if you throw out the MSB there are a total of 6 bits there if you throw out the MSB you will be remain you'll have five bits remaining it will be 1 0 1 0 0 1 then you need to add 1 so in binary so one will be transferred to the left side wait wait wait it's too fast for me so so this one is for a negative number does everybody agree with that this is a negative number okay so now what is saying is that one zero one one zero one zero one okay you don't need to do it in binary so whatever 1 0 1 0 1 you are getting converted to decimal and 1 ok so what do you so after you throw the MSB so the MSB is 1 and the other part is 0 1 0 1 0 and you flipped it so it became what 1 0 1 0 1 so what is this number in decimal what is this number in this one huh 21 okay I'm going to add 1 to it to add 1 to it so 21 plus 1 so you mean 21 plus 1 and binary will give you 23 and 21 plus 1 and decimal will you just need to add 1 to that number answer should be the same whether you whether you represent a number in so 21 plus 1 is 22 please please let us listen to argument because this is a common source of mistakes so you know I'm not asking you to add in binary I'm just saying you get a number add 1 to it add 1 I mean if you know how to do binary addition by all means go ahead and do it find it in Evernote 10 1 plus 1 in binary is 1 0 it is not 1 plus 1 is 2 right so 2 in binary there is let's look at this 1 plus 1 in binary no no I mean so so 1 plus 1 in binary is what is their question so let us answer that but this is a question 1 plus 1 is how much in binary right now you first told me 1 plus 1 is how much in decimal right how do you want to in binary so 1 plus 1 in binary better be 1 0 I mean we cannot change addition whether we are representing binary or in decimal Dre yeah so 1 plus 1 is 1 0 but how does that make any difference 1 1 ok so you don't need to do you know when we send here add 1 this is just add 1 it's as simple as add 1 so you are getting some number here converted to decimal add 1 ok so you have minus 22 now ok so everybody has - 22 anybody got more something different than - 22 you got something - ok the first method which we used in which there were two zeros the ambiguity was the plus 0 and minus 0 using that method I am also getting - in - very good so did anybody get minus 10 so why were you not raising your hands when I said Paul did not get huh what both answers okay but if I'm asking it would anybody get something different from minus 20 - because this question actually has two answers I mean if I if I just ask you this question that what is 1 0 1 0 1 0 as a signed integer what have not specified and I was expecting somebody in the class would say you know what the hell do you mean I mean signed integer is it the first way of representing signed integers of the second way of representing signed integers we just discuss two ways of representing signed integers so and you'll of course get two different answers in these two ways so you know somebody they're about minus 10 using the first way what is the first way did you treat this one as a sign bit so it is - and just drink the rest of it as a normal individual which is ten so it's minus ten but then I mean that is a wasteful representation it has two ways of representing zero or not of birth but nevertheless it's a representation and we studied that right so minus ten is a correct answer for this if you take this signed representation to be the first representation very just have a sign bit but if you take the two's complement one then this is minus 22 right so there are two different answers minus ten or minus twenty two depending on how you interpret the representation to be is it two's complement on just the sign bit representation and both of them are correct answers as long as you you're clear in your mind why you're getting the answer that you're getting okay good so we will end here today 
8ywD-A6hzAo,27,"Computer organization lectures for GATE, Complete Computer Organization lecture series. Computer Architecture and Organization for GATE, Computer Organization tutorial. 

. _______________________________________

1. Digital logic design tutorial (DLD Tutorial):

https://www.youtube.com/watch?v=baF-cxSl8TA&list=PL4hV_Krcqz_J4K8dEFsqzm3zJIgzFF9MQ&index=2&t=16s

2. Computer Organization Tutorial: 

https://www.youtube.com/watch?v=ayJBTJLt4cQ&list=PL4hV_Krcqz_JaY3JmbrDgy5tipHrOmGBW&index=2&t=6s

3. Computer Networks Tutorial:

https://www.youtube.com/watch?v=yTnAB4IMU8g&list=PL4hV_Krcqz_KLIzfuShdbDiAdyrhJbwF6&index=2&t=1s

4. Operating Systems Tutorial:

https://www.youtube.com/watch?v=Fd9ucp6_hho&list=PL4hV_Krcqz_KyOBQEm6825QQJ6m2c1JRY&index=2&t=1s

5. Database Tutorial (DBMS Tutorial) | SQL Tutorial

https://www.youtube.com/watch?v=mwlKkUmhLeU&list=PL4hV_Krcqz_IbPUf3mAJbje5XQdPORrYi&index=2&t=0s

6. C programming Tutorial:

https://www.youtube.com/watch?v=zmLv-IjU000&list=PL4hV_Krcqz_JhUAojsTolbrTarJPrtzvM&index=2&t=1s

7. Algorithms Tutorial (DAA Tutorial):

https://www.youtube.com/watch?v=l51gzYCnA8k&list=PL4hV_Krcqz_L_qeClFzxcr9sJCIF5MUGe&index=2&t=10s

8. Data Structures Tutorial | DS Tutorial:

https://www.youtube.com/watch?v=56OA2C9Uxmc&list=PL4hV_Krcqz_KzWhCr3zJj3_z4wkSLSP7v&index=2&t=0s

9. Problem solving using Data structures and Algorithms Tutorial:

https://www.youtube.com/watch?v=wwWGOkYk500&list=PL4hV_Krcqz_LqMkNHswMN868hL1Klj2Li&index=2&t=1s

10. Probability Tutorial | Permutation and Combination Tutorial:

https://www.youtube.com/watch?v=6DeqpQFUPpI&list=PL4hV_Krcqz_Kp449S66_fmbaaVakWAAjc&index=2&t=7s

11. Interview Puzzles Tutorial:

https://www.youtube.com/watch?v=eOUYaaSkwq4&list=PL4hV_Krcqz_JyjXz-8DDzz3XgdkZiMF3F&index=2&t=258s .
.
12. Aptitude Video lectures for Placements | GATE | SSC | Bank PO | Quantitative Aptitude lectures

https://www.youtube.com/watch?v=oiPb-qAWME0&list=PL4hV_Krcqz_KjZl0UzQGnXTPC4xsbA0fS",2018-12-16T09:25:39Z,Memory Organization-27 | DRAM example GATE-2018 | Dynamic Random Access Memory,https://i.ytimg.com/vi/8ywD-A6hzAo/hqdefault.jpg,GATE Video Lectures - Success GATEway,PT4M4S,false,1679,31,3,0,1,now next example on dear M is this it is a gate 2018 question a 32-bit wide man memory unit with a capacity of 1 gigabyte is built using 256 m into 4-bit beer and chips the number of rows of memory cell in DRAM chip is to keep our 14th humko number of rows he need ot he therefore am Louise a circle carded them time to taken to perform 1 refresh operation is 50 nanosecond ok the Refresh period is 2 millisecond the percentage rounded to closest integer of the time available for performing the memory read or write operation in the main memory unit but so much may know about the on Dana what is RT it is 15 nanoseconds sorry Arty's 50 nanosecond time to refresh d them ATM refresh Gernika to keep our 14 into PP nanosecond if you convert this into milli second kyoki unko milli second main refresh interval given a that is point 8 1 9 2 8 1 9 2 milliseconds got a point no 8 1 9 2 milli second is time to refresh 1d REM and parallely home look sorry dear emsco refresh coordinate 18 max no refresh overhead what is refresh overhead point 8 1 9 2 upon 2 milli sorry - milli second ok point 8 1 9 2 milli second upon 2 milli second that you will get forty point nine six percent forty point nine six percent time you are refreshing memory for that if time period make up memory may read or write operation intercepted you open a conceptually pirata so how much time yeah how much percentage keep memory available hey that is total hundred percent minus forty point nine six percent what you will get fifty nine point zero four percent that is closest integer is fifty nine percent but so much may hundred percent main forty point nine six percent after memory refresh overhead later I Makabe Pusa memory may read or write operation nahi kar sakthe to fifty point nine four percent time up memory may read yeah right operation kar sakthe Tom nineteen variety equations all kids dismay to previous year gate question Joe up to this topic pappu chiger whoa be omni salah cardi you 
_BMCBBAtM80,27,,2021-01-23T07:04:18Z,Lecture 14 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/_BMCBBAtM80/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT9M4S,false,49,2,0,0,4,[Music] an interrupt cycle is added to the instruction cycle in the interrupt cycle processor checks to see if any interrupts are occurred or the interrupt cycle processor interrupts security on show interrupt handler program is generally part of the operating system typically this program determines the nature of the interrupt and performs whatever actions are needed operating systematic controls to interrupt handler [Music] computer science and engineering 
jc7ycQVRSBI,27,"In this live lecture, you will learn the Computer Organization & Architecture (COA) for GATE Computer Science Engineering. Vishvadeep Sir has covered Instructions in Computer Organization in detail during this lecture. âž¤ Use Referral Code: VDEEP10, To Get 10% Discount on Unacademy Subscription. Also get to know Instruction format in Computer organization.

Subscribe Unacademy Computer Science Channel now: https://www.youtube.com/channel/UCFWCFYvqnAMT-jcCqTp_SlA?sub_confirmation=1

Follow your Favorite Educators on Unacademy:
1)Vishvadeep Sir: http://www.unacademy.com/@vd_gothi

Join our Telegram Group here for all the useful Resources: https://t.me/UnacademyGATECS

Welcome to Unacademy Computer Science Channel, your one-stop solution for all Computer Science Engineers. Indiaâ€™s top educators will be teaching you daily on this channel.
We will cover the entire syllabus, strategy, updates, and notifications which will help you to crack the GATE exams.
During the live session, our educators will be sharing a lot of tips and tricks to crack the exam. GATE aspirants who are preparing for their exam will be benefited from this channel. Unacademy platform has the best educators from all over the country, who take live classes every day.

Subscribe to our channel!

âœ¤ Download the Unacademy Learning App here:
âž¤ Android: https://goo.gl/02OhYI
âž¤ iOS: https://goo.gl/efbytP

âœ¤ For More Such Classes Get Subscription Advantage:
ðŸ‘‰ GATE & ESE: http://bit.ly/2uoyyVQ

âž¤ Use Referral Code: VDEEP10
To Get 10% Discount on Unacademy Subscription

Unacademy Subscription Benefits: -
1. One Subscription, Unlimited Access
2. Learn from your favorite teacher
3. Real time interaction with teacher
4. You can ask doubts in live class
5. Limited students
6. Download the videos & watch offline

#COA #ComputerScience #Instructions",2020-08-04T13:37:46Z,COA | Instructions in Computer Architecture -2 | Lec 6 | GATE Computer Science/IT Engineering Exam,https://i.ytimg.com/vi/jc7ycQVRSBI/hqdefault.jpg,Unacademy Computer Science,PT55M39S,false,10110,451,3,0,38,good evening everyone very very good evening uh i hope you all are fine good healthy fit and learning right my name is vishwadi and i welcome you all in this special lecture series which is named as computer organization and architecture it is targeted for the people who are preparing for gate in computer science and it is targeted for those members also who wants to learn computer organization for maybe multiple purpose things like for their other competitive exam preparation or from for the semester exam preparation so if you are preparing for this particular course called as computer organization this is the lecture series definitely which will help you all so all the people who are here just like this video share the video with all of your friends so that more people can join you here and another thing is what exactly we are going to discuss today in this lecture is this so in the previous lectures till the last case what we discussed was the instructions and their types today we are going to solve the questions so the agenda of the day is we are going to solve the questions based on the instruction chapter yes so type of instructions how many instructions supported maximum minimum all these things we are going to discuss in today's special lecture on youtube which is available for you free for the learning i just want to hear from you that if you are listening this lecture right now in the live video then can you mark your attendance so that i can understand that you guys are listening through one thing you just do one thing that you please uh the comment box in the live chat that you are listening me or not so that so that what happens i can understand that yes you are listening and you are able to hear me properly so that i can start further i can go further you guys are preparing for gate before you go through the lecture and all other things this is my profile my name is vishwadeep and i have given date exam multiple times in computer science which is with the rank this this is my final year rank in engineering final year yes and before that i have given the computer science gate exam in cs yes in my third year also and that rank was 689 yes and apart from that multiple time times i have written the gate exam i got the rank 119 440 and there are few more my masters two masters one is from iic bangalore one from bits pilani another mtek in data science from bits pilani other than that this is my experience overall 13 years of experience nine years of experience in gate and engineering services and these are the courses which i think now very very important thing people in computer science specifically in computer science it happens in the computer science students always think that we okay we will prepare we can prepare from the free material there are so many sources which are available free recorded videos and youtube videos nptel lectures we can prepare every single thing so let me tell you one thing people if you are preparing that that kind of mindset then i'll tell you there will be two challenges you will be facing one is the content which you will not get at the single source for that you will definitely struggle a lot one faculty one particular source cannot teach you all the things hence what you need to do you need to find proper content for each and every course which is not easy enough today second thing is the motivation just because you will not be able to find out the answer of your queries and the remaining other problems you will be facing so what happens you will be struggling yes and when you struggle your motivation goes down and at that time what happens your gate rank will go down so the best thing what you can do here is you need to join a specific institution and the best institution right now in the current situation is the on academy plus subscription which can give you the best content with multiple options of the course to be taught by multiple faculties unlimited content unlimited access to the videos recorded videos live lectures and multiple things out there with very nominal cost so if you are really really aware of your career one thing second thing you don't want to struggle in your preparation you want every query to be answered immediately you want to learn every single course up to the maximum highest level then you need to join this plus course and if you will use this code vd10 then on the on academy plus you will be getting 10 percent discount so these are the different different plus subscriptions benefits but i'll tell you exactly what we tell or what we exactly provide during our lectures one thing which is most important is for every doubt i clear every single doubt of the student so no doubt remaining unanswered at any moment of the time second thing so many practice problems these are called as dpp daily practice problems i am giving to students and those students are being benefited for the highest level of the study other thing that i am going to give them a quiz which is another level quiz what i saw in the classroom what i give them in the classroom for the practice problem beyond that level so that quiz is beyond the level quiz hence you will be benefited with all these apart from that you will be getting a personal mentorship yes from my end you will be getting a personal mentorship if you are going to take subscription on this particular code all these you will be getting so that your rank will be very very close as you intend to so if you are really interested in your preparation just this particular button this get subscription button you need to click as soon as possible yes as soon as possible so go to the gtac get subscription and do it very with very nominal cost with one year subscription comes in 22 500 after the discount after you use this code use it and make your career with very very very very less struggle so you need to do only on one thing you need to work on only one thing that whatever we are teaching you you just understand it whatever we are giving you just practice it you don't have to write the very first place so this is the question it says consider a digital computer which supports only two address instructions each instruction is of 16 bits if the address length is 6 bits then maximum minimum how many number of instructions are supported fine so first of all if any such question comes what you need to do you need to make the format you need to make the format of the instruction which they have given right now only one type of instruction is given which is the which is what to address instruction hence i am going to draw the format of two address instruction so two address instruction can be what it is having an opcode which is mandatory part apart from the opcode you will be having two fields one field is one address another field is another address apart from the opcode two addresses are supported within an instruction now see what is given in the question in the question given that instruction length is 16 bits total 16 bit instruction address length is 6 so this address will be of 6 bits this address will be of 6 bit if that is the case then can you tell me what will be the opcode size of code bits will be how many total six plus six twelve gone from 16 the remaining four bits in the op you only tell me from here now with the help of 4-bit how many maximum combination of opcode you can have maximum op code combinations you can have is equal to 2 power 4 2 power 4 so answer will be 16 answer will be what 6 right if answer is 16 which is maximum number of instructions supported by this particular computer will be how many 16 so a concept one answer i got from here is maximum number of instructions supported is equal to 60 now minimum let's talk about minimum minimum how many number of instructions supported if any case any type instruction they are asking to address or one address or zero address you understand the minimum answer will always be one that type instruction should always be existing then only you can say that okay two address instructions support so if you are saying to address instruction supported comes at least one should be there then i can say minimum is one if this particular question will be asked only for minimum i will not solve anything i will directly answer one i will directly answer one no any thinking at all fine let's solve another question this question says what this particular question series consider a digital computer which supports 32 to address instructions if address length is 8 bits okay then length of instruction is how much let's talk about the instruction first what type of instruction it is to address instruction hence let's draw the two address instructions format so let's draw the format of two address instruction very simple simple question these are for your practice only we are solving here for your practice only to do address instruction will have three fields total one is opcode that is the mandatory part for every instruction apart from which i am having two addresses address number one and address number two right total how many number of instructions supported 32 so i am writing here number of instructions supported is equal to 32 what does it mean in the case so 21 bits will be the answer for this particular question instruction length is 21 bits very simple question it is next question is this next question is this a processor has 40 distinct instructions this is a gate question which we are solving asked in 2016 git a processor has 40 distinct instructions and 24 general purpose registers so let let's write down these things number of instructions is equal to how many number of instructions is equal to 40 just now only i explained you if number of instructions are 40 then we need to generate 40 distinct opcode combinations to generate or code combinations is equal to 40 can you only tell me if you are listening me and learning here how many opcode bits you can have how many opcode bits you can have for 40 bits or 40 distinct instructions page bits second with five bits can you get the answer no you need to get the answer using how many six bits with the five bits you can generate maximum operation only so six bit op code fine now next is general purpose registers are how many 24 so number of registers given in the question is equal to 24 i am writing that to here number of registers is equal to 24 a 32-bit instruction word has an op code so in the instruction what is given let's let's draw the instruction format let us draw the instruction format what is given in the instruction in the instruction given as one op code is given okay fine so let's let's write all those things in the 32-bit instruction one op code is given fine other than that what is given two register operands so there are two register operands so one resistor operand you can specify here number one and other register operand you can specify here apart from that anything given yes and an immediate operand so another field is given for the immediate operand immediate operand immediate operand is that particular operand which value or whose value is immediately given within the instruction only so if any constant you want to be given then that should be given in this particular field now this is the 32-bit instruction okay fine 32-bit instruction total fine 32-bit instruction total now what is the opcode six bits we already calculated can you only tell me how many bits are used for registers if total 24 registers are there then for register numbering or register reference how many bits are used to give each and every register a binary number first register zero next register one in binary next register two in binary maximum 24 registers means last register 23 in binary 0 to 23 0 to 23 if you want to give in binary then how many bits are required i guess five bits will be your answer why five bits with the help of four bits you can generate only 16 combination so five bit binary required means for register numbering 5 bits here 5 bits here now among 32 bits 6 plus 5 plus 5 16 bits are gone the remaining 16 bits will be remaining for the immediate operand that is the answer the number of bits available for immediate operand field is how many 16 bits that is the answer for the question fine let's go further and percussion this particular question is very very simple again git cup 2016 computation it is this particular question you can homework if you want to solve this particular question you can solve if you will get any problem i will be there to help you this is the question another same almost same type of practice question it is a very extra information some little bit extra information is needed to solve the question very important yes this particular topic is very very important multiple instruction support you know why just because in last three years two questions came from this particular topic yes multiple instruction support so if you guys want to really score in the exam you need to put more extra efforts to learn this specific topic multiple instruction support two questions in last three years from the same topic and it will be very very easy to understand if you put little bit more concentration let's see let us see let us see so what you are going to do whenever you solve such questions in which multiple type instructions are supported then you draw the format you draw the format of all the instructions of instruction supported two address and one address instruction supported both the system has six bit instruction two bit addresses if there are three two address instruction in the system then maximum and minimum how many one address instruction can be there fine so first of all draw the format first of all i am drawing the format for both to address and one at this so this is two address this is one address instruction format i have drawn here let's do this fine so in the two address instruction you will be having an op code and two addresses here you'll be having only one op code and one address let's see so here this is two address instruction here it is one address instruction both format i have drawn let's see how many bits of code this is address 1 this is address 2. so let me make it more clear here of code and address now see how many bits are there in each bin first of all instruction length given as six bits here and six bits here this address field is two bits this address field is two bits this address field is two bits so how many bits remaining for opcode in this type instruction is four bits but here in the two address instruction you will be having only two bits fine you remember first you remember first that using using if the system is using two types or more than two types instruction support in a single computer system first of all draw the format i have drawn the format and now what i'll have to do is i'll have to i will have to make i have to make i'll have to start with that instruction i'll have to start with that instruction which is having minimum op code bits if you are good in your memory system means your this one memory system well and good if not mark it down lik just because this is the starting point where every student makes mistake yes if you will remember this specific point then definitely you will be able to answer all the questions based on this kind of questions okay so what you need to concentrate on remember you need to start with that of code which takes minimum of code bits you need to start with that instruction which takes minimum op code bits here output bits are four here opcode bits are two so i will have to start with two address instruction let's start with two address instruction in the two address instruction using two bits can you only tell me how many maximum op code we can generate how many maximum of code which can be generated using two bits maximum op codes can be two power two answer is four i'm just taking that example also what are those four combinations zero zero these are the four combinations of opcode which can be generated maximum using two bit of coding to address instruction now see in the question just look at in the question that how many two address instructions are used there are three only two address instruction how many two address instructions three two address instruction if there are three two address instructions which means which means among all maximum possible op codes in the computer system the implementation team the design team has taken only three opcodes in the use the thin op could use so here if i will write how many used op code we are having how many used op code we are having then it will be equal to three just for the assumption i am writing okay so assume those three op quotes which are used are 0 0 0 1 and 1 0 these three are used so how many unused combination of opcode we will be having from this to address instruction format how many unused combinations will be having from here one how four were total among those three were used the remaining one so unused combination will be equal to unused pop codes will be equal to one which is that based on my assumption unused combination is that one so now listen if unused combination among two address instructions format is one only which is one one then in one address instruction then in one address instruction if we are having though we are having four bit of op code you can have here you can have here or code like this only that you can have here the opcode like this only that it should start with the first two bits as one one kyosakiya why we have done this just because this one one is only combination which has not been used into address instruction so any op code which can we have which we can have any op code which we can help with the four bit of four bit of opcode combination in one address instruction that can be that can be only used or that can be only which is starting with one one means last two bits i can keep as zero zero or zero one or one zero r one one so these are the four different opcode combinations which are maximum possible for one address instructions i will give you i will give you exactly why it happens i'll give you the reason also instructions i'll give you the reason also exactly why it is so i'll tell you one thing suppose okay so bear with me just be with me and just try to understand suppose for execution cpu receives or cpu fetches an instruction suppose an instruction comes to the cpu an instruction comes to sleep of course that instruction will be of six weeks now cpu can take this six bit instruction is to address instruction also can take as one address instruction also possible yes so there are there are two interpretation of this instruction possible yes possible there are two interpretations possible first interpretation is as to address instruction second interpretation is as one address instruction so i am writing both the interpretations here both the interpretations i am writing here only and what are those interpretations what are those interpretations as to an address instruction are as one address instruction right as one address instruction if suppose cpu has taken this instruction as to address instruction then how it will look it will look like this starting cut two bits for opcode next two bits for address one next two bits for address number two so opcode is one zero address one is one zero and address two is one one right or in one address instruction if this instruction is treated as one address instruction then it will be like this one zero and one zero and address is one so opcode is one zero one zero and this is one one now which particular interpretation is correct now see any instruction comes to the cpu cpu needs to decide cpu needs to decide that this instruction is of what type to address or one address so if we need to decide with two address or one address if if this this particular instruction is it is as or is a type two address instruction then the same instruction cannot be off to entrance instruction or sorry same same instruction cannot be of one address if one instruction is of one type then the same instruction will not be of other type so if suppose one zero one zero one one is two address instruction type then it can be only to address instruction type it cannot be of one address instruction type of course so how cpu will decide create to address type or this is of one interest type so if we can take a very simple example here and you can understand very very simple example very very simple example from the real world from the real world let's say you are having a shop you are owning a shop of some some goods some goods maybe some grocery or something okay and you are having one nephew niece or one one particular kid of just maybe 12 years of a kid and that particular kid is watching the shop behind you behind you means on behalf of you you are gone somewhere you are you have gone somewhere and for some urgent work and that particular kid is handling the shop if somebody comes and asking that particular kid for a very small thing like what is the cost of a toothpaste or something that fellow can read mrp is this give me that money and then so if simple thing is that then that fellow can be helpful if a customer comes who requires lots of things then that customer will need larger person the elder person is needed to fulfill that demand then that fellow first will ask the kid can you be able to fulfill my dream fulfill my uh requirements that fellow says the kid says no i can't then we'll have to wait for the area right so here also is the same thing when an instruction comes to the cpu we need to ask this younger one younger one means the occurred with minimum bits of code with minimum bits we need to check fulfill requirement if this particular instruction will fulfill the requirement well and good if not then we will go for one type instruction one address type instruction so what cpu will do cpu will first will check for s2 address instruction and will check that if weather weather one zero this is the op code one zero is used as to address opcode or not so in the computer system if one zero is used as to address opcode or not if if yes then treated as one address instruction oh sorry date to address instruction otherwise we'll go for one so we will check if one zero is used as two address instruction yes which means what which means this particular answer this particular specific instruction will be treated as to address instruction only this interpretation is correct this one is wrong which means from here you can understand very clearly you can understand very clearly all those instructions all those instructions which are having starting two bits zero zero two address instruction treated as zero one then two as two address instruction one zero also treated as two address instruction why just because these three zero zero zero one and one zero these three are used as opcode combination for two address instruction that's why any instruction comes to the cpu which is starting with two bits zero zero or zero one or one zero cpu will take all those instructions as to address instruction now which instruction can go as one address instruction name which instruction can go as one address instruction then it's very simple thing that particular instruction that particular instruction which will be which will be starting with starting two bits two one one starting two bits as one one y just because these starting two bits one one combination in the op code is not used in the two address instruction so those combination which are not used in two address instruction as of opcode only those combinations are left to start the one address instruction card charge four bits of one address instruction can start with only those combinations which are unused here which are unused here that's why maximum one is four and what is the minimum of the total value i told you just few minutes before only i told you if somebody asked you minimum how many number of instructions of that type answer will be one always so this is one address instruction which will be having only one opcode and one address here to address instruction which will be having one op code and two addresses now next is what here is the op code and address one address two here is the op code and only one address what is the length of entire instruction 6 bits to turn how many bits in addresses 2 2 bits so how many bits here in the op code 2 bits what is the length of instruction here 6 bits how many bits in address 2 so how many remaining in opcode 4 bits now what you need to do you need to start with that instruction you need to start with that instruction which is having minimum of code which you only tell me which is having minimum upward bits to address instruction or one address instruction this is to address type instruction and this is one address type instruction in the right you only tell me start curry boss from where we need to start one address instruction or two address instruction which is having minimum output bits from there we need to start which is two address instruction you guys are my fighters you need to fight hard to prepare for this particular course and how you can fight hard by learning and how you can learn by responding by interacting by just listening the video lectures it will not help you much so you need to interact you need to answer yes so with the two address instructions we have minimum of code which let's start with this how to start karana see first from here you need to calculate how many maximum op code you are having using two bits but now how many maximum op codes you can generate using two bits two part two answer is four among those four based on the question you just see how many are used three based on the question how many are used three so i am writing used opcodes is equal to how many used opcodes is equal to three how many are unused then kittens okay only one is unused right so unused op code is unused op code is equal to one now what we need to do using this unused one op code we need to create the number of one address instructions how far one address instruction for one address instruction how many bits we have in the opcode four bits now what we need to do we need to divide these four bits into two parts how you will divide into two parts first part this one remember this one will have exactly same number of bits as we have in the previous level output exactly same number of bits as on the previous level of code bits previous level of code bits two that's why first part will have two bits and remaining two bits here after after having two bits in this part among four bits two bits here then two bits again simple now you see this using starting two bits unused combinations you are getting from the previous level how many unused combinations you are getting from the previous level only one so using that one unused combination using that one unused combination you need to generate the next level k of codes which is one into all the combination of these which is two power two all the combination using 2 bits 2 power 2 answer is 4 and this is your maximum 1 address instructions so that's how you are going to solve such questions maximum 1 address instructions possible that's how you are going to solve such questions okay so if you are done with this then the same type of question i am having next let's solve this and let's understand if you are able to understand this explanation or not so what we will do here we will have a game against me against you against me against you means you are against me i am against you you are going to solve this question i am also going to solve the question and let's see who solves this first of course i will be solving little slowly with explanation you will be solving only to get the answer let's see if you can solve it before we let's see let's see if you could understand definitely you will be able to solve it consider a system which supports three address instruction and two address instructions both okay fine it has 30 bit instructions okay fine and eight bit addresses fine if x three address instructions are there then maximum how many two address instructions can be formulated same type of question instead of some number value instead of any number or value x a variable is given okay no problem i will formulate i will give the answer exactly with the same method let's do it let's do it first of all the very first step is formulate the instruction formulate the instruction let's do it so first of all three address type instruction i will formulate three at this type instruction i will formulate it will have four fields one is opcode and three addresses other than that i'll be having what other type instruction to address instruction in which i'll be having one op code and two addresses let's draw the format and let me write down all the fields so here is the op code then three addresses address number one address number two and address number three here i am having op code and two addresses address number one and address number fine next 30 bit instruction total so this is let's let's write down this is three address type instruction and this is two address type instruction now what total 30 bit construction is given among those 30 bits how many bits are there in the address 8 so 8 8 and 8 24 bits are gone how many bits are remaining here six bits let's draw the format and number of bits formatting also here in the two address instruction also 30 bit total in the instruction addresses are of 8 8 16 bits are gone how many bits remaining here then 14 bits what i need to do i will have to start with that type instruction which is having which is having minimum number of so six bits here 8 bits here now okay uh i guess some some glitch was there at my end no problem so six bits are here using these six bits can you tell me how many maximum combinations we can have not maximum how many unused combinations we are getting from the previous level using these six bits unused combinations from the previous level is 64 minus x these are the unused combinations from the previous level into all the combination of these eight bits which is two power eight right so how you will solve such questions if if such kind of question is given first of all formulate two types of instruction or three whatever is given and based on that what you are going to do based on that what you are going to do you need to start with that instruction you need to start with that type instruction which is having minimum number of opcode bits this was having minimum number of opcode bits so i have started with this using these six bits i will start that i will check that how many maximum outputs i can have which is two power six sixty four how many used opcode which is x so x unused 64 minus x based on that based on that number of unused combination i am having 64 minus x now next type instruction will be going how we can divide these 14 opcode bits in two parts first part will have exactly same number of bits as many bits as we have in the previous level top code previous level of code may check here it's a six bits so six bits here in the first part remaining eight bits among 14 is in the other part from the first six bits how many are unused 64 minus x into all the eight bit combination using eight bits to power eight this is what age you have what is this what is this this particular thing is the maximum number of two address instructions maximum two address instructions possible very very simple very very simple very very simple okay so before this i will solve one more question for you guys if i'm saying i am giving you another question before we wrap up the series this particular today's lecture question is if in the previous question there are 512 two address instructions used then how many three address instructions are there how many three address instructions are there so look at this what is the trick here yes look at this what is the trick here trick is this till now i have solved two questions for you in those two questions in those both of the two questions i have given you number of instructions for the first level first level means here the first level was minimum of code which first level was to address instruction its number was given yes three address or three two address instructions are there here also in this question also if you see there are x three address instruction based on that we were calculating how many two address instructions are there in the next question the trick is opposite thing is given if two address instructions are given then how many three address instructions are there so will we change the strategy of solving the problem no will we change the strategy of solving the problem no we will have the same strategy we will have exactly the same strategy strategy is what draw the format draw the format these two after drawing the format start with that instruction only which is having minimum number of now suppose nothing is given about three address instructions and then nothing is given all right we can assume that let's say let's say i am not writing i am not writing it's a previous question i'm not writing it's a previous question i'm giving you a fresh question a fresh question only like this two types instruction two types instructions supported which are three address comma two address instructions same same example i am taking same kind of thing i am taking instruction length given instruction size given exactly same whatever is that in the previous question 30 bits so exactly same 30 bits address size also saying 8 bits and given that there are 512 to address instructions and the question is how many number of free address instructions are how many you need to solve this particular question exactly same way as i explained you till now draw the format and do the same thing same exactly same way so i am drawing the format first three address and two address both now this is three address instruction format in this we are having an opcode and three addresses here i am having opcode and only two addresses 30 bit instruction total among those eight eight gone i'll be having six bit in the op code 30 bit instruction total eight eight are gone 14 bit instruction sorry 14 bit op code here i will start still with this only whether they have given to address instruction how many number of instructions are there in two address instructions that is given but still my method will remain same my method will remain same if six to six bits are there in the three address instruction you need to tell me how many maximum op codes we can obtain using these six bits which is 2 power 6 answer is 64. among those how many used combinations i don't know if i don't know what you did since your childhood in the mathematics if you don't know any quantity initially let's assume x right let's assume x and then how many unused op codes you'll be getting from here unused op codes will be equal to 64 minus x using these unused combinations using these unused combinations you need to generate these two address instructions so first of all let's divide these 14 bits into two parts how you divide you know that right okay first part will have same number of bits as many as we have in the opcode in previous level six and then remaining eight using these six bits how many combinations are in use from the previous level 64 minus x 64 minus x into 2 power 8 y 2 power 8 just because these are 8 bits these are the two address instructions yes and this is given how many paths given right how many 500 512 so i will equate i will equate this particular equation as 512 which is given in the question now what i will do what i will do from here what i will do from here i will get the value of x simple so 64 minus x is equal to 2 x is equal to 62 calculation you need to do perfect so there are 62 two addresses three address instructions 16 2 3 address instructions based on that i can have 512 to address instruction simple so if if if this particular if this particular type instruction also given then to the method will start from here only you remember if you remember this the same method you will apply if these numbers are not known make it as xyz as many as unknown you are having just as you and go further method should remain this so that you can get correct answer easily you don't have to you don't have to assume anything at all right okay so if you understood this particular thing i guess you will be able to solve all those questions based on the such kind of concepts yes you will be able to solve those questions and get those questions done so that one particular tick mark you can have in your preparation that fine so today's lecture is up to this level only tomorrow we are going to have very very important discussion about addressing modes so before that i will explain you a few more basic about our basics about the instruction like what is the effective address what is an instruction cycle fetch cycle execution cycle how it works how it affects the system and after that we will start the addressing modes so that you can understand addressing modes even in more depth yes learning is more important so directly addressing mode i will start then that will not be digestible for you guys so when i will explain your addressing modes before that i will cover some basics for you and then we will go for the further things okay so today the learning was good yes or no you can type down here you can give some feedbacks in the comments also after the lecture is over and this particular code so you all are my fighters remember you all are my fighters and you need to fight against every single odd by this specific word learning keep learning keep learning and keep learning if you are liking the lectures if you are liking the lectures share the lecture link with all of your friends all the social medias all the groups which you are connected to so that maximum people can be benefited and why i am asking this just because you are not paying any money to watch such videos which are very very high quality videos so something you need to pay back yes what simple sharing okay you share the videos you make us famous and i will make you an iit and that's what that's what sure so for the day that's only it if you want to if you want to contact me you can contact me on this number seven eight double nine three one triple to six this is my personal phone number you can call me on this number or you can whatsapp me on this particular number if you want to know how to prepare what to prepare and on other other discussion that if you are facing any kind of problem in your preparation for date if you want to go for an academy plus subscription then you can go with this vd10 code use this code you will get 10 discount apart from that you will get some mentorship from me who has cranked the gate multiple times yes multiple times so i am the ranker i am the preparation person i am the preparation mentor also so if you will be trusting over me definitely your preparation will be on top notch so see you again next lecture tomorrow till then happy learning enjoy see 
ihZHUdnsNyE,22,Instruction Medium: English,2020-06-21T00:51:55Z,Computer Architecture and Organization Lecture Annotation Video 6,https://i.ytimg.com/vi/ihZHUdnsNyE/hqdefault.jpg,Khalid Mahmud,PT31M11S,false,150,2,0,0,0,i welcome you all to this session of computer architecture and organization lecture annotation video the topic of this lecture would be design methodology as you are saying in the screen right now previously we have discussed about the basics of computer architecture and organization how they are different from one and another one another and actually we have discussed the components and the architectures of the system in this design methodology topic we will discuss about how this system actually performs a task if we are to say that a calculator or a computer performs the specific arithmetic operations for example addition subtraction multiplication and division how actually these things work now let's move on forward this is the formal definition of the system design now let me read this out a computer is a example of a system so what is the first thing that we come to the system design in computer science is that a computer is a it's itself an example of a system which is defined informally as a collection of a large and complex one of objects and compound objects called components and are connected to form a coherent consistent entity with a specific function or purpose so when we decide to make a system whether it is a computer system or other it is designed for a specific purpose it is a very important topic the function of this system is determined by the function of its components and how the components are connected this is the basic functionality of a system here it is being told to us that a systems function is determined by the functionality it can perform by the individual components of the system and in the short version what do you mean by some computer system is that a computer is a system which performs on some specific functions and when we get this or integrate these functions together it performs a specific function or purpose it fits some specific function or purpose therefore we call that collection of objects and components together with the computers together as the computer system mathematically speaking if we have a set of numbers in a and a set of numbers in b now if f is the function of uh which maps elements of a to elements of b then we can write in this kind of scenario b is a function of the elements of a or b is derived from the function of elements of a and so on so this is these are the functions that we are going to be elaborating in this system designs so let's move forward system representation we will go to the figure in a bit but let's discuss what is what we will see in the figures the first figure we will see is kind of a graph and the second figure we will see a block diagram representing a small gate level logic circuit called exclusive or or exclusive or modulo to adder this circuits has the same general form of figure 2.1 etc now the important topic here is the structure versus behavior in the last figure of this 2.3 shows one kind of behavior behavioral description of the logic circuits for the figure 2.2 that it gives the results of the input and output output result based on the based on different inputs which represents the circuit of the figure 2.2 which is generally a truth table this stabilization of all possible combination of input output values is called a truth table so here we get a definition a formal definition of a truth table so i think it is very important for the quiz so take a look another description of the same exclusive or behavior can be written in terms of mathematical equations for mathematical functions uh this is kind of a function if we input two zeros we will get a zero output if we get a zero and a one as the input we will get a one in the output and noting that f of a where a is the output and x and i i will denote them as input so x x 1 and x x2 will be the input and we will get f of a as the output so we already know that if we if the inputs are of similar kind that is uh both of them are zeros we will get a zero in the output or both of them are one we will get zero in the output for the exclusive uh exclusive or get results xor result and if the inputs are different uh if the inputs are of different type we will get one as the output now let's move on forward this is the structure that we will see in the block diagram this is this is the generalized form this 1 and 5 will act as input i will denote them as i and this number 8 the node will generate the output [Music] which will we will denote as o but i have written it in full the world so here the inputs are processed and here the inputs are generated and as a whole we can see by the numbering that we have here is eight nodes and nine edges and what are edges the connectivity between the nodes is determined or called edges it is very fundamental topic of computer science uh that we understand what are nodes and what are edges and what are graphs you have already learned them so let's move forward here we see here we see the block diagram the actual block diagram that we would have guessed in the previous from the previous picture so i have already told you this will act as input and this here we will get the output for an xor to be determined as that we take a value 0 or 1 in x 1 and we take another bit 0 or 1 in x 2 in an and get we input a value directly and and not value of the another input we perform an operation and then we process or proceed that result to another or gate these two structure are kind of similar only the inputs are being reversed that we are seeing here and as a result after this all get implementation we will get x1 xor x2 so these are the block diagram to represent an exclusive or logic circuit so our computers are based on these numerous logic circuits integra or called integrated circuit or ics and the density depends on them and how much of functionality we can use is very much dependent on this kind of circuits so let's move forward in here we see the inputs and outputs that are being separated this is kind of a mistake these are the inputs x1 and x2 and the output will be the function f of a input is the function of x1 and x2 it is very familiar to you guys and those who have already completed the digital electronics course so here we get if we input two zeros we get a zero in the output and if we get two ones in the input we get a zero in the output if we get a zero and another input one and or if we get a one in one input and a 0 in the other input we we get 1 in the output so these are basically the behavior of the exclusive or functionality so by implementing this circuit we can generate this kind of input and output so our computer system is very dependent on this input and output functionality the reason is that whenever we try to represent something it really depends on how we implement the circuit for each and every input if we want to design a circuit or design a block diagram it would be very very costly so we design this functionality in such a way that if we design a circuit it can take all the informations all types of informations possible and generate all the outputs possible so this is all about exclusive or let's move on forward we have already kind of discussed about the hardware description language we will see here formally and in more detail so what we can fully understand is that we are going in the deeper in the computer architecture organization course a bit by bit so let's begin we can fully describe a system's architecture and behavior by means of block diagram the other term of block diagram is schematic diagram so you have to remember it in which we identify the functions of the components previously i have told you that computer is nothing but the collection of the components that they represent and to represent the function of a computer we need to describe it in a block diagram or a schematic diagram and we can convey the same detailed information by means of the hardware description language or hdl a format that resembles a high level programming language such as ada or c [Music] now let's move forward design process given a system structure the task of determining its function or behavior is termed analysis the converse problem of determining a system structure that exhibits a given behavior is design or synthesis so in a word if we are given for example a structure let me draw a figure these are the basic functionalities of a calculator of arithmetic operations addition subtraction deviation multiplication now these are the functionalities or the behaviors uh the calculated or machine represents or exhibits now if we want to know in detail how this machine works or the structure of this machine it would be called synthesis or design now if we design say for example this is input 1 and this is input 2 and this is the addition functionality or addition performing structure which gives the result if we have that if we have this and then we can tell that it it performs the structure performance addition [Music] then it is called analysis if you are given a structure say for example various components of your mobile together and you are ordered or instructed to find its behavior you analyze each component and then tell the result it is called analysis now if you are given the whole of the set the mobile state set of device and you are asked to determine its system structure you need to go through each and every functionality and analyze how they are being discussed or performed and then determine its structure which in turn will be called design or synthesis now let's move on further so it is uh to be noted that this is a very important topic the difference between analysis and design or synthesis you have to provide a good example if you are asked about the difference after giving them in the definition you have to state an example each for design and synthesis and analysis now here we have the flowchart and of an iterative design process the first thing that we would like to we i would like you to know that flowchart here are our two answers to the question is possible yes and no it is kind of a mistake so please correct it when you draw it in your copy now in the design method methodology we first begin with a vision that we want to describe or we want to perform a specific functionality and then we construct that initial design which is kind of a prototype and we construct them and then it we evaluate in cost and performance if the cost and performance are not to our satisfaction we didn't do not proceed further at the decision stage if our design goals are met that we want to design a machine which performs addition functionality now if i construct a design and evaluate its performance and its cost and it performs actually performs the addition then my designs goals are met we will end the design process and we we get the final product if it performs the desired function but it not in all the test cases then my design goals are not meant and we need to modify the design to meet all the goals that we have said for our machine and then we again evaluate the new goals or designs that we have incorporated in the machine and evaluates his cost and performance and now if the design goals are met we end the design process this is how the iterative design process works if we cannot perform a specific design if we cannot produce a design to our satisfaction we repeat it again and again until we can find the actual design that we required now let's move forward we will discuss about the design levels the uppermost level is the processor level which is also called the architecture behavior or system level the middle level is called the register level which is also called the register transfer level and the lowermost level is called the gate level which is also called the logic level we will go in details for these topics in the next slide but before that we see how these levels [Music] share with one another the difference and the components of these different levels the ic density information units that we use and the time units that's required to transfer that information units from one place to another or to generate some output or input we start with the get level or logic level the components that we use here are the logic gate for example xor x nor etc or in or a term called flip flop you have you know in details about this flip flop d flip flop jk j flip flop etc and the ic density here is a small scale integration or ssi the information bits that we use here in the logic level is called bits and it is the fundamental unit of information the lowest level of information so it takes a very short time 10 to the power minus 12 to 10 to the power minus 9 seconds to transfer the informations in the register level the components that we use are registers counters combination circuits etc small sequential circuits if you do not remember what are combinational sequential circuits i would advise you to search them before you start reading this materials it was discussed in detail in your digital electronic course and now the ic density of the register level is medium scale information medium scale integration or msi and the information units that we transfer here are words the information as a whole a block of information or or a block of bits so it really takes a greater number of time compared to gate level to transfer the words from one place to another and in the processor level we use the component cpus memories of input output devices etc and here we see the ic density has very large scale integration and it uh deals with the information units as block of words which is a kind of a collection of words so therefore it takes a huge time compared to bits and words or the get level and register level to transfer the data i hope you understand what we have we are talking about in the up to this slide so it's all for this lecture annotation video hope to see you in the next one in the next one we will discuss this gate level in detail and about some combinational logic it would uh be easier if you got this book of morris manner and a review revise a topic called full adder your you will be quite familiar with it and i will also go into a review of some sorts so you don't need to worry too much so see you in the next lecture goodbye thank you 
Z4AvPVffNM0,27,"Von Neumann Architecture | Von Neumann Bottleneck | Computer Organization and Architecture

#BikkiMahato
The best part is: it is all completely free!
------------------------------------------------------------------------------
Follow :)

Youtube : https://www.youtube.com/c/BikkiMahato
Facebook : https://www.facebook.com/mahatobikki
Facebook Page : https://www.facebook.com/youtubebikki
Twitter : https://twitter.com/mahato_bikki
Instagram : https://www.instagram.com/bikkimahato
Google+ : https://plus.google.com/u/0/+BikkiMahato
Blogger : https://bikkimahato.blogspot.in
Pinterest: https://in.pinterest.com/bikkimahato123/
LinkedIn : https://www.linkedin.com/in/bikkimahato

------------------------------------------------------------------------------

DONATE Support :)
Patreon : https://www.patreon.com/bikkimahato
Instamojo : https://www.instamojo.com/@bikkimahato
Paypal :  https://www.paypal.me/bikkimahato",2018-06-10T12:09:55Z,Von Neumann Architecture | Von Neumann Bottleneck | Computer Organization and Architecture,https://i.ytimg.com/vi/Z4AvPVffNM0/hqdefault.jpg,Bikki Mahato,PT5M24S,false,4367,19,17,0,3,welcome everybody so today we'll be continuing from Victor organization and architecture and today we will be learning about one human computer or architecture so it's another name is I guess that is Institute of Advanced Studies so in 1945 one human and his colleagues began the design of a new computer now referred to as IES computer so with reality exceptions all of the today's computers have this same general structure and function that was designed in the 1945 so this was published by John von Neumann and his computer architecture consists of a control unit arithmetic and logical unit memory unit and registers and input outputs so von Neumann architecture is based on the stored program computer concept where instructions and gaiter program are stored in the same memory now what is different memory they are stored in the same memory so this design is still using more sort of computers so this is just the architecture so input device so this is the structure of it so Center for CPU the control unit the automatic and logical unit here you and agree so all of this will be used in the same memory and it will after that it will go to the output and so I'll just drawn in so the size starts from 0 so this is the figure 1 so the figure 1 shows that the data for NAND so figure one is the data format so data for now how did I stood so leftmost bit represents that sign bit that is 0 is for positive so 0 is for positive and one is for negative then the remaining that the remaining 39 bits so this 39 bits from 1 to 39 these are used for numbered size for indicating the number size this was number format or data on that next we have figure 2 that represents instruction format which is a 40 bit long but each instruction is 20 bits you can see it is divided from 0 to 20 and then 20 to 39 okay it is divided into two so that store in a 40-bit memory and a bit of code that is known as operation also of code that is known as operation cone operation code defines operations to be performed and the 12 bit address that is used to store the operand of the instruction so 0 to 8 stores the OP code and this 8 to 20 that is 12 in stores the address and again z22 28 of code and this against towards the address so this is known as the left instruction and this is woman the right instruction the memory of IES consists of thousand storage locations called words so thousand storage location called words each consisting of 40 bits so that is very important okay so now we have something known as born new man bottleneck so this is a very small topic but it is very important okay so one of the major factors affecting a computer's performance is the time required so time required to move instructions and data from CPU to the main memory so you need to cope back and forth from cpu to main memory so see you can see here this is the thing I am talking about from CPU to main memory so the CPU has to wait longer so CPU way has to wait longer to obtain data from the memory than from its registers because registers are very fast so registers are very fast and are placed logically inside the CPU so logically placed inside CPU at this point is very important these are very fast very logically inside see pure processor whatever you can say so this speed so this CPU and memory this speed disparity so speed disparity this is the key word right here so this CPU memory speed disparity is referred to as pawn Newman bottleneck okay so I hope that you have understood the lecture and if you had any problem just comment on the comment section and if you did like the video hit that thumbs up button and subscribe to my channel for more videos so thanks for watching it 
3Z1FmyL4wCM,22,"In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.

In this introductory lecture, we have discussed about  the differences between the Computer Architecture, and Computer Organization.",2020-09-23T04:45:12Z,Lecture 01 CSE 231 Computer Architecture and Organization Spring-2021,https://i.ytimg.com/vi/3Z1FmyL4wCM/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT23M22S,false,293,23,0,0,29,[Music] university of science and technology computer science and engineering [Music] [Music] [Music] foreign [Music] [Music] [Music] s [Music] computer architecture and organization morris manuel and partnership patterson and hennessy computer organization and design the hardware software interface william sterling organization and implementation of computer systems a computer system implementation organization it will functionality describe correct technology standards interact to form a computer system or platform after that computer system give away hardware technology setup software interact corby director detailed a specification camera computer architecture computer system of the computer architecture so shared computer architecture refers to how a computer system is designed and what technologies it is compatible with authored computer architecture keep having computer system design definition of australia computer architecture is the science and art of selecting and interconnecting hardware components to create computers that meet functional performance and cost goals orthodontia connection engineering computer architecture is a set of rules and methods that describe the functionality organization and implementation of computer systems computer architecture camera arrow defined code computer architecture is nothing but a specification detailing how a set of software and hardware technology standards interact to form a computer system so um cj computer architecture needs currently how a computer system is designed keep a victory computer system creates encouragement what technologies it is compatible with computer organization computer organization refers to the level of obstruction above the digital logic level but below the operating system level computer organization foreign corbin nike uh dual core processor battery corbin or corey three bauer corbin quebeco i7 vapor coordinate shitty abner organization decision so computer organization refers to the operational units and their interconnections that realize or recognize the specifications of computer architecture so um what are the differences between computer architecture and organizations the first difference is a computer architecture structure model system attributes into organizational architecture rules follow god that means joking hospital design academic building today residential building design [Music] by britain so computer architecture is concerned with the structure and behavior of computer system as seen by the user but computer organization is concerned with the way hardware components the computers components connected together so i'm going to do the bullish a computer architecture multi describe korea what the computer does but computer organization describes how the computer does computer architecture is concerned with the structure and behavior of computer system as seen by the user but computer organization is concerned with the way the harder companies operate and the way they are connected together to form computer systems in porosity it describes other computer architecture describes what the computer does so knowing about the architecture number architecture basically what functionalities will the system display capability what the computer does but jakku number computer organizationally how the computer does so computer organization tells how exactly all evidence in the system have been announced keep up the system and given the company school oh erin shkara to help they realize the architectural goals uh the system claims to have achieved other computer architecture designs functionality systems m is realization of what is specified by the computer architecture computer architecture [Music] camber hard disk beverage implement coaching token sheet of a particular organization so computer architecture deals with giving up additional attributes of the computer or processor to be a specific so yeah organization is a realization of what is specified by the computer architecture so computer architecture is a ram particular processor by working with talking shitty who is a computer organization so a computer organization deals with how operational attributes are linked together to meet the requirements specified by computer architecture so i'm a computer architecture of an organization is subject to um i'm not the starting coach but why should you start the computer architecture and organization computer architecture computer organization understand higher computers are going so future capabilities drive the computing world real world impact no computer architecture no computers super computer i'm addicted to this so what will be the capabilities about future capabilities of the creatures so while computers are going computers subject to their computer engineer computer architecture design computer design microprocessor design ram design how efficiently they can handle data so computer architecture among organizations course to study course so understand high level design concepts shitty airpods computer performance is [Music] all the students of computer science and computer engineering so obviously the basic computer science stuff when a computer science is student on camera computer engineering and student umbrella of the check on your product computer science and engineering so both of computer science engineering you should acquire some understanding and appreciation of a computer systems functional components their characteristics their performance and their interactions so there are practical implications student needs to understand computer architecture in order to structure a program so that it turns more efficiently on real machines so i mean java programmer hardware and knowledge at the connect in selecting a system to use they should be able to understand the trade-off among various components japan cpu clock speed memory size ignore the programming execution to need for college shield uh efficiently after system after program design so computer architecture and organization and technology computer science and engineering 
c3V8w2Wk-D0,27,"COA
In this video lecture you will learn memory cells like RAM,  SRAM and DRAM",2017-12-13T16:05:07Z,SRAM and DRAM | memory cells,https://i.ytimg.com/vi/c3V8w2Wk-D0/hqdefault.jpg,Education 4u,PT11M45S,false,119426,1105,98,0,31,hi students let us discuss a psych next topic that is memory cells SRAM and DRAM cells so SRAM is nothing but static Ram and this is the dynamic Ram so what is it Ram so if everyone will know that Ram is a volatile memory it is a volatile memory so what is this volatile memory means it does not retain data when the electric paths power is turned off okay or if failure so if you turn you turn off your computer all memory stored in the RAM is lost so whatever the files that you open and you're doing the work so that complete files information will be stored in the RAM so suddenly if the power is off or if you close the data files the data that is present in the RAM will be lost okay means let's remove so that's what we call it as a volatile memory it is not a permanent memory whenever we switch off the computer all the data that is present in the RAM will be lost so whenever your computer is turned on again suppose if you turn on again don't on again so the BIOS means the basic input/output system the BIOS reads your operating system reads your operating system and related files from the hard disk and load them back into RAM so whenever your system is on first what it does the BIOS reads your operating system and loads the related files from hard disk back into from okay so this is a ramp a ram is a volatile memory so whenever you turn switch off the computer all the data that is present in the RAM is lost so whenever you turn on again first the BIOS reads your operating system and loads the related files from hardest and place them again into run okay so let us see the block diagram of this Ram block diagram of so this is a chip the RAM chip will be like this here the size of this ranch it will be 2 K by n memory will be there here address data chip selection and right okay so the pins that are connected to the RAM devices address bus data bus chip selection and right so the output will be this is the out pin so connect it to the CPU okay so this is the block diagram of Ram so let us see the functional table of this this is a chip selection right so when we are able to write the data memory operation so based on these two pins selection the chip selection and the write the memory operation will perform if the chip selection is 0 memory write don't care then memory no operation will done in memory okay so whenever it is 1 0 so it's nothing but read Reeth selected word so one particular selected word will be read whenever the chip selection is one and the right is zero so whenever it is 1 1 if it is 1 1 it's nothing but it is doing the right right selected word okay so this is the block diagram of Fram so let us see how the address will be allocated in the ramp how it will be how much memory will be stored in that Ram suppose the memory size will be I said this is 2 power K my and means here n is nothing but n bit word so the bits maybe it is 8-bit or 16-bit or 32-bit whatever it may be so the bit length and this is the memory size so suppose for example 2 power 24 into 16 your ram size okay so if your ram size is 2 power 24 into 16 so it contains it means it contains 2 power 24 that is the 16 megabytes of words that are present that can store in ram and each word like this 16 bit longer so the meaning of 2 power 24 means to power 24 is nothing but 16 mega words will be there so here each word length will be 16 bit long okay so now the ram would need 24 address lines ok the meaning of this is here the ram would need 24 address lines and total storage capacity is 2 power 24 by 16 that is 2 power 28 as a capacity of the total storage capacity of the run so this is about the memories representation in the RAM sets so this is the RAM cell memory so in the RAM sorry in the random access memory chip your memory will be allotted like 2 power K in to n memory locations so this is a representation of your memory okay so let us see different types of ramps will be there so those are the SRAM and Dida so let us see what are the different types of tramps so those are the SRAM and Dena so let us see the difference between these two SRAM and freedoms s1 is nothing but it is static Ram and the dynamic Ram okay so as RAM is a type of semiconductor memory is used to store each bit so it is you call it as a one type of semiconductor memory it is a semiconductor memory here we used memory to store each bit each bit will be stored in each bit will stored in the esra so it does not need refreshing circuit SRAM does not need refreshing circuit so static Ram need not any refreshing circuit will need so it retains contents as long as a power applied to Qi so the data will be presented in the chip as long as the power is applied so retains content as long as Bubba applied so as long as power applied it retains content in the chip so whenever the power is off the data in the SRAM will be lost here the access time is around 10 nanoseconds so if the access time is 10 nanoseconds and it is used for cache memory and also for date and time settings as powered by small battery ok so it is used for this static Ram will be used for cache memories okay and also fun date and time settings so which the date and time settings will hold having the smaller batteries will be there so for those batteries you are allocated to the SRAM so SRAM will be used for the cache memories and for date and time settings okay so this is about the essence so let us see what is a day run so DRAM is nothing but is a dynamic Ram the basic storage device this is one of the basic storage device here is not a flip-flopper but moss and a kappa capacitor will be there in the DRAM you have you are having a moss and a capacitor will be there for basic storage so for basic storage it requires a moss and a capacitor here the charge determines the stored bit will be like zero and once the data stored as a charge not remain infinitely due to leakage so currently therefore the periodic refresh cycle is required to maintain stored data so this DRAM will require a refreshing circuit will be there you require so here SRAM doesn't require any refreshing circuit so it automatically it will be refresh whenever the data will be accessing so whenever the accessing at the time it automatically refresh okay suppose if you open the word files or PowerPoint files whatever you are working on that if your system is having the static Ram it automatically refreshing your files means it automatically saving your files without to avoid the loss of data so whereas in DRAM here the data storage as a charge not remain infinitely due to leakage current so therefore the periodic refreshing cycle is required to maintain stored data so to to overcome that design disadvantage of DRAM we are going further as some static Ram okay so this is the difference between this SRAM and the tearin' thank you 
Z6NERIPT440,27,"These videos are helpful for the following Examinations - GATE Computer Science, GATE Electronics and Communication, NTA UGC NET Computer Science & Applications, ISRO, DRDO, NIELIT, Placement Preparation, etc. 
If you want to enroll for our courses please visit our website https://www.Digiimento.com or call 9821876104
You can also add me as a friend on https://www.facebook.com/HimanshuKaushikOfficial

#Call_9821876104 #GATE #NTAUGCNET",2017-04-10T00:05:26Z,01 Introduction to Computer Architecture and Organization,https://i.ytimg.com/vi/Z6NERIPT440/hqdefault.jpg,"DigiiMento: GATE, NTA NET & Other CSE Exam Prep",PT9M25S,false,31976,128,16,0,16,so the next topic is computer organization and architecture and the subject for slaves for this particular subject can be categorized as follows so we'll be having the first chapter which is the memory interfacing then we will be discussing about what is are you interfacing that is input/output interfacing after I you interfacing we are going to discuss about what is machine instructions machine instructions then we'll be discussing about what is control in a design control unit design after doing the control unit design will discuss about the ALU I take apart lÃ¼ data path then we'll be discussing about the addressing modes after addressing modes the next topic will be the number system so this number system is a common topic as it is also their digital logic so we are going to discuss the number system in computer architecture organization then we will be discussing about the data presentations and then we'll be discussing out the last topic which is a pipelining so by planning is on the most important topic in computer architecture okay and the books which you need to refer for memory interfacing an i/o interfacing you can refer the book which is says for machine instructions on the controller design ALU data path addressing modes a number system editors presentation for all these topics you are going to refer the book which is Maurice Meno and then for pipelining again you can refer the book which is says so every wise you don't need to buy books for this subject for the students were preparing for gate exemption the subject is for eight to eleven marks and even for your Ethernet legs I mention the subject is containing high voltage because there are few subjects which are interrelated with each other so whenever you are preparing some subjects you are when you prepare those subjects in combination with each other because there are lot of topics which are inter inter lapping with those particular subjects for example we have digital logic computer architecture organization and operating system these three subjects are having topics which are overlapping with each other so if you prepare any of this addition you have to prepare all the three subjects thoroughly to get a thorough knowledge about all these subjects again the same way if we discuss about the C programming data structures algorithms and foresee process for most points are returning for use Ethernet examinations and there are some programming language knowledge then these you consider going masajid these topics contain you know there's one single package in the same way there some subjects which are always studied in a package so for this subject because there are some topics which are overlapping with operating system and computer architecture topics like memory management and IU Management also so that is why I wish to read them together in this able for digital logic and computer architectures there are some topics which are overlapping with each other for example if you are discussing about the data bus or system bus design then it is a made up of mighty Texas again we studied multiplex and digital logic there is a number system which we start we also spread the number system in digital logic and income-tax picture organization so instead of covering there two times I covered only one okay so we are going to discuss about all these topics in detail and for that first so first of all you need to have an introduction about what is the computer architecture right so when we discuss about the computer architecture or when you discuss about a computer first what is a computer what is the computer so computer takes some data or some programs as input and when I'm saying programs then these programs have set of instructions the programs are set of instructions and what is the set of instruction these are the commands to perform certain tasks commands to perform tasks are called as instructions okay and by after taking this data and the program the computer gives us some result or you can say it gives us some output right and here this computer converts all the programs or whatever data it has taken in machine language with the help of you know the operating system or the compiler set sector X vector so there are multiple layers of this and balancing machine language that means it understands with zeros and ones let me tell you a very simple example you have computer is in like electronic machine electronic device if you are going to give an instruction to the computer by saying at 56 + 97 now this 56 and ninety seven are not known to the computer in the same way if you're going to say ABCD to the computer then it isn't it will not be able to understand this one by because it is a electronic device generally the electrical circuits which we take here they understand only zeros and one so you can consider eight it's like a switch either the switch can be on or it can be off so that means there are only two states for zeroes and ones and with the help of all these different combinations of circuits we try to make an organization of these circuits to make a machine we should be able to understand all these different things very easily okay so in the know we have three different disciplines here number one is the computer architecture what is the computer architecture number two is what is a computer organization computer organization and number three is what is the computer design what is the computer design okay so there are three topics computer of architecture second computer organization third one is the computer design so when we discuss about the computer architecture and computer architecture it deals with instructions addressing modes ALU pipelining and internal design of the computer so we can say it deals with instructions addressing modes Lu pipelining etcetera so when I am saying these topics that means it deals with the internal design of the computer internal design of the computer so complexities of the internal design of the computer so balancing computer organization now this computer organization deals with various memory and i/o interfaces or can say how the how the various memory and are you interact with the system so deals with it deals with how various memory and are you interact with a system with a system and when I'm saying computer design now this complete design deals with the hardware designs it deals with hardware design with hardware design so combined with all these things combined with all these three topics here it is it is called as computer architecture and organization and sometimes it is also named as computer architecture and design sometimes computer organization and design etc etc so there are multiple names for this particular subjects but the same one thing is common identity computer architecture an organization or it which computer organization and design or it will be computer architecture and design so these three things will be common in the names of this particular subject ok so we will start with the first topic which is the memory interfacing and slowly we are going to cover all these topics one by one and will be seeing the various you know the things which are required for this subject and we are going to cover them one by one whatever things are required and we are going to take it step by step okay now let us start with the first chapter that is a memory interfacing int int terms of contact okay 
oppI4ppEPFc,27,,2021-01-06T05:16:07Z,Lecture 10 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/oppI4ppEPFc/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT14M38S,false,47,3,0,0,7,computer the processing record for a single instruction is called an instruction cycle um cycle and execute cycle program execution holds only the machine is turned off some sort of unrecordable unrecoverable error rockers or a program instruction that hosts the computer isn't covered economically memory taken in a typical uni uh typical processor processor actor registered a high program counter above a program counter address tracking the address of the instruction to be faced increments the program counter after its instruction phase so that it will face the next instruction in sequence 80 bala who says microprocessor keycard j program counter takes a increment so i are registered in ammunition that says the ir resistor contains the phase instruction of the data structure physical instruction instruction register the instruction contains bits that specify the action the processor is to take the processor interprets the instruction and performs the required action economy processor a obstruction t interpret correct monitoring process instruction case is for a single instruction is called an instruction cycle and it consists of phase cycle and execute cycle or that after single instructional processing creating a required construction virtually all computers provide a mechanism by which other modules may interrupt the normal processing of the processor interrupts are provided primarily as a way to improve processing efficiency we with interrupts the processor can be engaged in executing other instructions while an io operation is in progress if an interrupted definition is supposed to be a balancing interrupt actor signal ct microprocessor interrupt another eight zero eight six five zero eight five microprocessor spinner microprocessor s timer io even harder failure what about you the power failure high token a type of interrupt generated harder failure type an interrupt to signal normal completion of an operation request service from the processor or to signal a variety the operating system to perform certain functions on a regular basis timer set code on a regular basis program type in the program class of interruption by hovercoat so what are the clusters of interest there are four classes of interrupts computer science and engineering 
54E1xkwkOR8,22,"This class considers and teaches computer organization, different parts of a computer that makes it run and talk to other devices around it. This discussion involves the details of a generic cpu design, instruction set architecture which is the interface between the hardware and software, i/o devices, memory, and other advanced topics which made our modern  processors run as fast. Current trends like multi-core architectures, vector instructions for fully utilizing the available paralelism in hardware, piplining etc will also be covered towards the end of the class.",2016-12-13T16:45:56Z,Introduction to Computer Architecture (Comp206 class0),https://i.ytimg.com/vi/54E1xkwkOR8/hqdefault.jpg,Åžuayb Arslan,PT15M53S,false,152,3,0,0,0,hello everyone this is the first class on computer architecture so I'd like to take this opportunity to welcome everyone and give you some information about what you're about to learn in this series of lectures in the coming weeks this class considers and teaches computer organization different parts of a computer that makes it run and talk to other devices around it this discussion involves the details of a generic CPU design instruction set architecture which is the interface between the hardware and software the i/o devices memory and other advanced topics which made our modern processors one fast current trends like multi-core architectures vector instructions for fuel utilizing the available parallelism and a hardware pipelining for example these topics will also be covered towards the end of this class and I hope you enjoy to enjoy the show so let's start with the basic question why do we learn about computer architecture well first of all to be able to say where the computers are headed right so this involves predicting the future terms of computer technology noble capabilities which may appear in different form factors new and branching applications in every engineering discipline you can imagine secondly to be able to understand all level design concepts that's the safe from the very law of digital circuit level to the right top of the programming stack where you can write applications with little if not no information about the hardware and to be able to compare performance computer performances so compared to computer performances can for example helping in daily lives we want to buy a laptop or a desktop computer and another reason to be is to be able to write better software and when the hardware will ultimately enables you to write efficient and fast fulfill for it of course this is based on the assumption that underlying architecture exposes muscle to keep lose of hardware to the software level and most of the job so you so we'll learn more about this and the final reason is to be able to design hardware and the most of the you know giant companies you know have their own hardware design team so learning about computer architecture can help you become you know a good hardware design engineer so let's start with the computer architectures definition there are two definitions the most conventional definition is the science and art of designing picking and interconnected hardware components and establishing the hardware software interface to create a computing system that meets functional performance energy power costs and other specific goals there is also another definition given by the famous a computer scientist abdol in april nineteen sixty four and that definition is the term architecture is used to describe the attributes of a system as seen from by the programmer and will basically develop the and the system as seen by the programmer at the end of this class in other words the conceptual structure and functional behavior is distinct from the organization of the dataflow band controls and the logic design the physical implementation of that logic design the main goal of this course they learn and think about the machine from an architectural level a top-down approach would entail learning about high level as well as the assembler level and finally logic level languages to control the hardware and different levels of abstraction which we will discuss in a bit constitutes the basis of a computer design so one of the other goals of this course is to understand the relationships between these two these levels of abstraction and hopefully by the end of this course you will be able to recognize computer components know more about the processor design memory cache hierarchies i/o devices storage subsystems and a large set of trade of space in which you can achieve a particular design point and eventually my role would be to push you to learn something new and after establishing the background hopefully we'll explore the newest trends together so let's look at the contents of this class will make more focus on the computer organization I learn more about the general-purpose abuse processors that can do anything that's that that's what the meaning is when we talk about general purpose for example running a full operating system examples include intel core i7 AMD athlon ad IBM powerpc are more Intel Italian viewers there are also application-specific CPUs like a six applicants Pacific integrated circuits they can do video coding 3d rendering imaging but but yeah so the 11 notice that to make is that hardware is less flexible but more parallel than software and hardware is faster and and and most of the time is lost less costly learn about also one also this the contents also includes the learning about micro microarchitectures and instruction set architecture I the vols they're learning about memory caching i/o learning about pipelining multi-core architectures multi-threading bench prediction of time allows let us to you know start with the fundamentals let's take a look at the moon's low and remember what muslo is was so it was about adding more transistor move you know garden mall the CEO intel has said a 40 50 years ago by adding more transistors to the cpu we can make them work faster so this is also the name called transistor miniaturization and it's about doubling the number of chances to every 18 months so one you know when we look at the microprocessor history and the revolution that happen in the history one significant technology chatted a threshold was was crossed in 1970s because it was the time where enough number of transistors are put together to design a 16-bit processes on one single chip with this development micro processors have a lot of new market segments example desktops computers CD CD DVD players laptops game consoles set-top boxes why mobile phones digital camera mp3 players GPS automate you know automatic electronics so microprocessor based system basically replaced all super computers then the second coalition followed that and it's it's the implicit instruction level parallelism hardware now provides parallel resources and new techniques in that in that time frame allowed us to learn how to use them software is by the way is oblivious to these changes in the hardware because it's not exposed and it basically started initially using some technique called pipelining which also enabled increased clock frequency and then it followed these caches you know is become necessary as processor clock frequency increase and the integration of floating-point unit also followed that set of sequence of you know advancements then deeper part mines brand speculation you know the multiple instruction per cycle like super scalar technology period and finally dynamic scheduling out of order execution and these advancements were applicable to single core then processors you know start this support explicit data tread level parallelism and the hardware provided you know parallel resources again software specifies is now usage for these parallel resources so why software is involved because all instruction level parallelism has shown diminishing returns in the past and then you know people now wondered about if software can be involved in that process and this product progress involved vector instructions for example Intel's SSE instructions and one instruction does for example for parallel multiplies multiplications or additions and also these processors have the general support for multi-threaded programs first with a single chord multi chatting and then multi-core multi training and now when we look at around we can see that there are a lot of other techniques like GPUs with thousands of course incapable of running multiple task at the same time so that's what the trend is going so to give you some examples from the passport of three revolutionary points of history let's consider one of the dominant key players of the microprocessor market today intel intel corporate corporations 1971 for bit microprocessor that's as shown in this slide it was named for all four and with the limited capabilities this microprocessor was used in calculators and you can also see the set of specifications of hardware saying this in this line when we reach the 2003 intel pentium 4 has been released this product was too sure the desktop as well as server level needs speeds reaching up to 3.4 g guards it was the maximum frequency at single core has reached at the time that this was possible mainly due to 5 55 million transistors and advanced technology such as deep pipelining and super scalar technology enables caching etc this process was kind of Intel's last and best effort for single core microprocessor era because after that the church detergent volution has happened and microarchitectures become predominantly multi-core the details of this processor is summarized in this slide you know when we reach the 2013 a great example to the final revolution of microprocessor has happened and Intel you know released their product called core i7 and this was intended for desktop and server computers as well the main difference of this piece of hardware was the the multi course staying on the same die and 55 million transistors have not climbed up turbhe X you know compared to previous generation Pentium for each course frequencies stayed the same and advanced caching is allowed while the legends in fending for capabilities are all included in different flavors into these products making them one of the best processors in the market so with each court can run to treads imperil the microprocessor that technology is called by the way hyper threading the process can run now 8 threads all in parallel so that's a huge improvement from the previous generation of CPUs so as you can see as what we watch the revolution steps of computer CPU overall system is becoming more complex with each generation so the question is how do we really deal with such a complex system the answer is obstruction and layering obstruction involves dividing the world of jobs into objects in each having their own interface and one module does not know what the other is doing really so it's like a black box to them only specialist deal with the implementation and with the interface secondly layering is key however layering is nothing but repeated abstraction it's about dividing the objects of the system into smaller but simple layers so that a given layer do not need to know you know does not need to know what the other layers are doing but just interfaces however there are disadvantages associated with abstraction and layering for example layer an abstract and mojos interfaces become entrenched over time in other words one once they become part of the standards it's very difficult to change even if the benefit is clear and under descend that disadvantage is that layering an abstraction make it hard to reason and predict about the performance across layers so how do we really make electronic circuits solve our problems using this kind of abstraction and layering well true to the abstraction and layering the ideas are suggested from the previous slide a complex computing machine can be designed to something called stack the complete stack is summarized for you on the right the top of the stack has the problem layer which involves the literal definition of the problem then the algorithm is designed to solve that problem a program is written next using one of the high-level languages such as C and Java this language this language got goes to a process called compilation and the runtime environment allocates the resources for this program to be translated using the hardware software layer layer language known as the ISA the instruction set architecture or and then with the ISA one can talk to the underlying microarchitecture the microarchitecture is nothing but a particular set of complex combination of logic circuits which can serve a specific set of purposes and finally this is the logic circuits consist of gates which are made of electronic components and circuits you know may help you execute those instructions this course is about is a micro architecture and some digital logic i also have summarized several the classes you will take in math university to complete to complete learning process about these step letters and fresh our memories and remember the basic building blocks of a computer the big basic basic computer consists of three main elements actually memory CPU and input/output so CP was the brains of the computer where the computation takes place whereas the memory was used to store data for later use io was the interface to outside which included network devices keyboard mouse monitor and printers etc the CPU has two units called data packet control CPU is connected to memory and i/o devices as shown in this in this picture and the connections are established to communication links and protocols so if you look at the computer from a program's perspective instead we can draw a picture as shown in this line in this example we'll assume that we use a you know GCC software compatible operating system and we start with the high-level language programming as usual you know this could be written in C Java or Python so let us say our source code is in a file hello dot C the first step to the process is the pre-processing in this state text are substituted comments are stripped appropriate addresses are replaced with full locations for fully include for file inclusions the pre-processing produces the output file hello I so this file also includes all macros expanded appropriately so it's ready for compilation next compliation take place and produces the appropriate assemble language instructions in a file called hello s of course to be able to do that compiler needs to know the underlying is a do you know the architecture otherwise it will not be able to generate the right set of instructions right the assembler Nexus is it takes place takes role and it's nothing but a translator the language translator it translates the holidays to hella o the object file which has the corresponding binary machine code of the assembly instructions if the source code has procedure calls the other functions from other see runtime libraries the sampler leaves those particular place is blank and these places are going to be filled in by the linker using other object files the completed object file called the hello out is now executable and loaded in proper memory location for the instruction fetch cycle to take over and execute the instructions alright so in this class we briefly introduce the computer architecture basics you know the programming point of view and motivated you for the next lecture hopefully so we have also mentioned about important topics like such as abstraction and layering all these concepts in the intervention in this class will be explored in detail later in the course but these are to have you you know get start thinking about it and investigating and learning more about these concepts and particularly what they mean in the next lecture we'll explore cost and performance fundamentals so that we can compare computers we'll also talk about energy power you know most critically reliability aspects of computing systems so a lot of excitement and adventure waiting for us just stay tuned needless to say please feel free to drop any questions you might have about today's lecture in to my mailbox and i'll be more than happy to help you with your questions thank you very much 
8OziJIiNP6Q,27,"Computer organization lectures for GATE, Complete Computer Organization lecture series. Computer Architecture and Organization for GATE, Computer Organization tutorial. 

. _______________________________________

1. Digital logic design tutorial (DLD Tutorial):

https://www.youtube.com/watch?v=baF-cxSl8TA&list=PL4hV_Krcqz_J4K8dEFsqzm3zJIgzFF9MQ&index=2&t=16s

2. Computer Organization Tutorial: 

https://www.youtube.com/watch?v=ayJBTJLt4cQ&list=PL4hV_Krcqz_JaY3JmbrDgy5tipHrOmGBW&index=2&t=6s

3. Computer Networks Tutorial:

https://www.youtube.com/watch?v=yTnAB4IMU8g&list=PL4hV_Krcqz_KLIzfuShdbDiAdyrhJbwF6&index=2&t=1s

4. Operating Systems Tutorial:

https://www.youtube.com/watch?v=Fd9ucp6_hho&list=PL4hV_Krcqz_KyOBQEm6825QQJ6m2c1JRY&index=2&t=1s

5. Database Tutorial (DBMS Tutorial) | SQL Tutorial

https://www.youtube.com/watch?v=mwlKkUmhLeU&list=PL4hV_Krcqz_IbPUf3mAJbje5XQdPORrYi&index=2&t=0s

6. C programming Tutorial:

https://www.youtube.com/watch?v=zmLv-IjU000&list=PL4hV_Krcqz_JhUAojsTolbrTarJPrtzvM&index=2&t=1s

7. Algorithms Tutorial (DAA Tutorial):

https://www.youtube.com/watch?v=l51gzYCnA8k&list=PL4hV_Krcqz_L_qeClFzxcr9sJCIF5MUGe&index=2&t=10s

8. Data Structures Tutorial | DS Tutorial:

https://www.youtube.com/watch?v=56OA2C9Uxmc&list=PL4hV_Krcqz_KzWhCr3zJj3_z4wkSLSP7v&index=2&t=0s

9. Problem solving using Data structures and Algorithms Tutorial:

https://www.youtube.com/watch?v=wwWGOkYk500&list=PL4hV_Krcqz_LqMkNHswMN868hL1Klj2Li&index=2&t=1s

10. Probability Tutorial | Permutation and Combination Tutorial:

https://www.youtube.com/watch?v=6DeqpQFUPpI&list=PL4hV_Krcqz_Kp449S66_fmbaaVakWAAjc&index=2&t=7s

11. Interview Puzzles Tutorial:

https://www.youtube.com/watch?v=eOUYaaSkwq4&list=PL4hV_Krcqz_JyjXz-8DDzz3XgdkZiMF3F&index=2&t=258s .
.
12. Aptitude Video lectures for Placements | GATE | SSC | Bank PO | Quantitative Aptitude lectures

https://www.youtube.com/watch?v=oiPb-qAWME0&list=PL4hV_Krcqz_KjZl0UzQGnXTPC4xsbA0fS",2018-12-16T11:57:31Z,Pipelining-17 | Stage division to gain performance in pipeline | GATE 2017,https://i.ytimg.com/vi/8OziJIiNP6Q/hqdefault.jpg,GATE Video Lectures - Success GATEway,PT6M54S,false,1296,28,0,0,3,now come to next example on pipelining it is gate 2017 vision instruction execution in a professor is divided into five stages I F ID o Fe X and W be this statistics five for 2010 and 3 nanoseconds respectively a pipeline implementation of the processor requires buffering between each pair of consecutive states with a delay of 2 nanoseconds to pipeline implementation of the processors are there first is for name pipelined implementation known as NP with five stages okay an efficient pipeline EP where the Oh F stage is divided into stages f1 and f2 with the delay of 12 nanosecond and eight nanosecond respectively the speed-up corrected to two decimal digit achieved by EP / NP means efficient pipeline over NEPA implementation in executing 20 independent instruction with no results ops it's a Capuchin eternity Jancy regne a Gotham East Asia skied aleyko further divided car death ahead to home frequency car curtain or Barada t because our delay reduced Sujatha a sample a be examples medic chicane a beam a october taro cases course all Carabas just simple metal a couch mode korean no para get me stages and AF ID o f8 n WV got a point delays are five for 2010 three clear no number of its pages are five maximum delay now what is your tea o'clock tea o'clock is max of five comma 2 a 4 comma 20 comma 10 comma 3 plus overhead before overhead is given 2 nanoseconds clear that is 22 nanosecond for one clock how many clocks for 20 instruction and is 24 both the implementation 20 nanoseconds number of clocks we need is k plus n minus 1 always you will go for this only that is 24 4 5 sorry 5 plus 20 minus 1 that is 24 clear it now what is the total delay total delay is 24 into 20 2 nanoseconds clear it in ep sorry in NP first now optional hum Laguna Katya I have ID o f1 f2 e x NW be clear it now that Oh F is divided into two stages tour is number of stages its increase oviya is coddled a 5 for 10 sorry - well it that is given in the question itself now 10 n3 now what is the number of each stage that is six now t o'clock now it's made you maximum iboga that is 12 plus two because of overhead it will take 14 nanoseconds clearing no number of clocks number of clocks that are K plus n minus 1 now K is increased by 1 that is 6 6 plus 20 minus 1 that is 25 got a point total delay total delay is 25 into 14 now how much speed of you achieved that is 24 into 22 upon 25 into 14 when you will divide this you will get 1 point 5 0 8 got a point now your answer is 1 point 5 0 correct to 2 decimal disease got a point just follow the instructions Jimena up Kapadia head up her affirmation instruction pipelining main number of clocks nickel nickel EA Cattaraugus k plus n minus 1 that will give you number of clocks needing cloth delay Kitana hoga maximum of a stage delay plus overhead if any got a point that will give you the clock delay clock delay will be multiplied by number of clocks will give you total time to execute that instructions in pipeline gotta find no matter how much sorry no stages of tieh a-hou yay execution of Diandra Keoghan maximum question is patterns joby new variety hoagie whoa upcoming lectures mayhem look purring you got a point is there any doubt you 
K8c5rnCUXRU,27,"I have Collection Of Advanced Computer Architecture-Princeton University Videos and I Want to Share With You.If You Like My Videos Please Subscribe To My Channel Below://
https://www.youtube.com/channel/UC1SsXZdTPDLUAEGtPwc02SA

EXTRA TAGS::
Princeton University,Advanced Computer Architecture,princeton university tuition,Advanced Computer Architecture,advanced computer architecture notes ,computer system architecture,computer architecture book,parallel processingÂ ,parallel computingÂ ,parallel programmingÂ ,advanced computer architecture syllabusÂ ,computer architecture courseÂ ,software architectureÂ ,computer architecture booksÂ ,computer booksÂ ,fundamental of computerÂ ,computer networkingÂ ,advanced computer architecture lecture notesÂ ,advanced computer architecture and parallel processingÂ ,parallel computer architectureÂ ,component of computerÂ ,computer architecture and parallel processingÂ ,parallel systemÂ ,cs2354 advanced computer architectureÂ ,parallel processing in computer architectureÂ ,computer systemÂ ,advanced computer architecturesÂ ,parallel computerÂ ,computer systems architectureÂ ,computer engineering courseÂ ,computer engineering booksÂ ,architecture of operating systemÂ ,computer trainingÂ ,advanced computer architecture courseÂ ,advanced computer architecture question bankÂ ,fundamental computerÂ ,architecture classesÂ ,parallel computersÂ ,parallel and distributed computingÂ ,books on computer architectureÂ ,advanced computer architecture question papersÂ ,computer hardware booksÂ ,what are the component of computerÂ ,computer and societyÂ ,advanced computer architecture booksÂ ,computer architecture pattersonÂ ,parallel processing applicationsÂ ,computer instructionÂ ,information about architectureÂ ,operating system and application softwareÂ ,about computer engineeringÂ ,computer system softwareÂ ,advance computer architecture pdfÂ ,application of parallel processingÂ ,advance computer architecture notesÂ ,parallel computing architectureÂ ,books on computerÂ ,applications of parallel processingÂ ,computer system and architectureÂ ,patterson computer architectureÂ ,example of parallel processingÂ ,massive parallel processingÂ ,computer architecture patterson pdfÂ ,advanced computer architecture ebookÂ ,free architecture booksÂ ,book of computerÂ ,parallel processing applications in advanced computer architectureÂ ,advanced computer architecture hwangÂ ,information on architectureÂ ,parallel processing systemÂ ,software and hardware of computerÂ ,about computer architectureÂ ,be computer engineeringÂ ,parallel processing architectureÂ ,computer architecture and design 5th edition pdfÂ ,software application architectureÂ ,www computer architectureÂ ,computer architecture slidesÂ ,component of the computerÂ ,computer system architecture bookÂ ,fundamental of computer booksÂ ,books of computerÂ ,about computer systemÂ ,computer engineering bookÂ ,distributed and parallel computingÂ ,operating systems booksÂ ,computer systems design and architectureÂ ,computer text bookÂ ,applications of parallel computingÂ ,computer instruction setÂ ,parallel processing computerÂ ,pdf architecture booksÂ ,computer systems bookÂ ,computer software architectureÂ ,information about computer engineeringÂ ,computer course bookÂ ,set architectureÂ ,books computerÂ ,information of architectureÂ ,pdf computer hardwareÂ ,advanced computer architecture previous question papersÂ ,",2015-12-10T01:19:58Z,Advanced Computer Architecture-,https://i.ytimg.com/vi/K8c5rnCUXRU/hqdefault.jpg,Preety Singh,PT9M52S,false,47,1,0,0,0,okay so now we're to sort of go through different problems with vliw and different solutions to that problem so the top one on this list is a a problem of hard to predict branches and how that can limit instruction level parallelism so you just remove the branch and we're going to call that predication so we're actually going to add instructions to the hardware which we're actually to add to instructions here so this is this is limited predication or we're add two very simple instructions and if you look at these instructions they're very similar to the ? colon or the Select operator and see so what is what does that operator do we have a equals i no see ? d colon e semicolon what does this do well it loads a if C is true it loads a with D if C is false it loads a with E well you can think about actually doing this with some sort of if then else piece of code which is pretty common if a is less than B so you can sort of put that here X gets a versus X can be that's our Select operator well we can add to two and special instructions here for our limited predication move if 0 and move if not 0 well what does this do well if this operand is equal to 0 then this rd gets RS else that's all it does that's all that instruction does and the flip one here is HX if it's not equal to zero wise is cool well this allows us to transform control flow into a data instruction so we take it a branch out so if you look at this piece of code if we were doing with branches set less than we do a brand so this this computes our condition Co flag here branch equals and if it's the one way it branches here if not it jumps over it so we have much control flow here we have to control flow operations the branch and the jump when we add these instructions we can basically do that if then else in in an instruction and basically every vliw processor you're going to look at is going to have predication or at least limited predication this is this is not full predication this is limited predication we'll talk about full predication in a second okay so that lets let's think about that for a second we just took control flow we turn it into something which is never going to take a branch miss predict that sounds pretty cool because brush miss predicts you know we're pretty pretty bad if we had a branch which was hard to predict we didn't know with high probability if a was greater than B or not we can just sort of sick this code sequence in here and just be done with it and when this is really important for very long instruction word processors is because whenever you take a branch miss protect you basically having a bunch of dead instructions you can't schedule something in in that point but a VA out-of-order superscalar can attempt to sort of schedule things in there we can try to schedule non-dependent operations but our compiler has to come loose some code sequence and has to make them parallel at compile time ok so a few questions here what happens if then if the if then else has many instructions this was a very simple case here we just sort of had one thing inside of each of these FN else's it's not the end of the world what you can do and typically what people do with partial predication which is what this gives us is they'll actually execute both sides of the if statement inter leave them in your view liw somehow and then choose the result at the end with a predication or a move z instruction or this is either typically called conditional moves if you go look in something like x86 I think these are actually called see move if you go look in mips it's called mu z but people's are named these things slightly differently so it's not not the end of the world but when you go to do that you're actually going to execute extra instructions that you may not have to have executed that's that's a bummer because you could very easily if there's a lot of code in here and a lot of conan here and you're executing both code sequences your basic leaving twice as much work and if the if it grows large you're doing lots of extra work and you may not have enough open slots to sort of fulfill that at that point you have a choice you actually put a branching if it's unbalanced also not the end of the world it's probably actually a bit easier you're probably have to execute twice as much code at some point though you may want to actually super unbalanced like a thousand instructions of the one side of the branch and like two instructions on the other side that branch you may just want to put a branch at an actual branch there and not try to predicate it because if you took the side which only has two instruction or the two instruction case well all of a sudden you've bloated that by an extra thousand instructions kind of in the common case and that's not very good so that's that's partial predication let's talk about full predication which is kind of the extension of that instead of just adding a simple instruction which moves data values dependent on another value it being zero or not let's say every single instruction in our instruction sequence except for maybe let's say branches or something like that can be nullified based on a register what does this look like well here we have some little bit more complicated piece of code we have four basic blocks it's roughly an if-then now see if else then and then sort of some code at the end and let's see how this works with predication well well you can do is first of all you smile set the predicate registers so typically these architectures have extra registers which we call predicate registers the predicate registers get loaded with some values sort of early and then let's say this instruction in this instruction execute in parallel different notation you know say this is a semicolon here and there's our brackets around that and this in front of the instruction here in parentheses we have a predicate register and which says whether this instruction was supposed to execute or not supposed to execute now we can do more complex things that are partial predication instead now you can basically execute everything and not have to do any moves at the end you know training bookkeeping and you could only you can execute just the sort of side of the branch that you need to execute Scott melky in esco 95 showed that if you do this and you sort of have a fancy enough compiler who's working at UIUC on the impact compiler you can remove let's say fifty percent your branches a lot of these branches or short little branches and your programs and a full predication you do some pretty fancy stuff this showed up in the Plato compiler which is HP or Plato architecture by HP and the sort of compiler for that which was the one may whose project at UIUC the impact compiler so you can sort of see that you know you can get a lot of benefit from so we're going to I'm going to stop here today but i just wanted to really wrap up and say we started talking about how to deal with dynamic events and how to get a lot of the advantages of speculative execution from out-of-order superscalar 'he's but in a statically scheduled regime and we're to talk more about how to do some of this code motion how to move instructions across branches how to move memory operations across other memory operations and we're going to talk about how to deal with some dynamic events which are hard to deal with in a stack they scheduled environment in the next lecture okay let's stop here for today you 
vjNWAY3a_IM,27,,2020-10-10T06:26:59Z,Lecture 04 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/vjNWAY3a_IM/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT30M11S,false,81,5,0,0,19,shopper jt camera builder computing system the processor is a controlling unit of a micro computer fabricated on a small chip capable of performing arithmetic logic unit alu operations upon communicating with the other investors connected to it jackhoon microprocessor shampoo balahi ambra tutorials.com microprocessor is a controlling unit of a micro computer fabricated on a small chip capable of performing arithmetic logic unit operation by alu operations and communicating with other devices connected to it microprocessor keyboard microprocessor hood safety computer microcomputer in a sequential order traffic instructions google memory the sequential order store karateka microprocessor a memory they get product instructions instructions so then corey airport it sends the result in binary to the output port airport output portable output unity by a result a binary part between this process the register stores temporary data about arithmetic frequently used in a microprocessor clock speed mentioned questionnaire clock speed determines the number of operations per second the processor can perform processor pro the second day operation performed speed and motherboard length already happened word length would change the number of bits uh at a time microprocessor t processor basically would say the word length so 8 bit in microprocessor can process eight bit data at a time the word length or the depending upon the type of the micro computer but depending upon the type of the microprocessor so word length word link neighborhood could be unit ignore product network correct maker processor word link are the bit microprocessor can process 8-bit data at a time what length or the prediction should be airport microprocessor has multiple data type format like binary bcd ascii signed unsigned number error computer data type airborne features of a microprocessor so some of the most prominent features of any microprocessor german cost effective size low power consumption versatility versatility among reliability so each features a german microprocessor economic apache careful overly microprocessors are manufactured by using metal oxide semiconductor technology which has low power consumption microprocessors are versatile as we can use the same chip in a number of applications by configuring this software programmable microprocessor microprocessor using microprocessor so microprocessor get them right to local coverage microprocessor camera risk processor process sorry special processor category so tutorials 6016046 so reduced instructions that computer shanky people high school risk processor it is designed to reduce the execution time by reducing uh by simplifying the instruction set of the computer reduced instruction set instruction set computer so using risk of processor is instruction requires only one clock cycle to execute results in uniform execution time risk processor global instruction execution by instruction execution correct this reduces the efficiency as there are more lines of code hence more ram is needed to store the instructions compiler also has to work more to convert high level language instructions into machine code so this reduces the efficiency risk processor instructions simplifying execution the compiler also has to work more to convert high-level language instructions into machine code economy risk processor and architecture that converges a risk process or architecture uses highly optimized set of instructions it is used in portable device like apple ipod due to its power efficiency microprocessors instruction hardware so what are the characteristics of risk processor the major characteristics of a risk processor are it consists of simple instructions it supports various data type formats it utilizes simple addressing modes and fixed-length instructions for pipelining it supports register to use in any context one cycle execution time load and store instructions are used to access the memory location larger number of registers to achieve among less number of transistors so risk processors which is instructions could be simple i've been predicting instruction cpu this is characteristics of a risk processor also what are what briefly discuss the architecture of risk microprocessor although what is risk microprocessor but briefly explain the classification of microprocessor other microprocessor grammar is processors processor among a special processor category microprocessor complex instruction set computer by sisk processor it is designed to minimize the number of instructions per program ignoring the number of cycles per instruction aggregate risk processor other architecture numbers instruction execution minimize the compiler has to do very little work to translate a high-level language into machine language because the length of the code is relatively short jotu instruction and length computer that means number of prescriptions ibm 370 by 168 bax 11 by 780 intel 80486 a processor below cisc architecture processor economics architecture architecture is designed to decrease the memory cost because more storage is needed with larger programs resulting in higher memory cost so risk processor and the number of instructions memory number of instructions variable several cycles may be required to execute one instruction instruction decoding logic instruction decoding logic is complex cis processor category but risk processor instruction decoding logic is very simple microprocessor which can handle its particular function many times faster than the ordinary microprocessor eight seven by eight zero two seven eight seven by eight zero three seven so eight zero eight sixty micro processor processor processor with the height eight zero eight seven eight data especially design microprocessor having a local memory operation you show local memory above 80 ml of our current to control io devices with minimum cpu involvement cpu involvement shops foreign h6 microprocessor a [Music] [Music] [Music] memory taking instruction face card decoded instruction execute correct so microprocessor is nothing but a semiconductor device um jt biology program control semiconductor device at that program could trigger [Music] ability to address large memory as possible i operate greater number of levels of subroutine testing among better internet handling capabilities rtb2c microprocessor popularly well known german intel 8085 fourth generation data intel 80386 8032 processor fifth generation gta pentium high fifth generation and computer global high pentium processor so a connecting the various conditions of the results are stored as status bits called flags in the flag register or flag raises strategy called beat third corey sends information to the timing unit and control unit a polyester timing and control unit generates control signals for internal external operations of the microprocessor microprocessor solder a bundle no cuts correct microprocessor processor shitty eight zero eight five keyboard eight zero eight six q by eight zero two eight six q eight zero three eight six g first generation second generation but third generation fourth fifth generation by painting processor functionally blocked into the thumb or the arithmetic logic unit register or internal memory instruction decoding unit pc program counter by ip above timing and control unit among flag resistor local corbin timing and control initiative control bus connected program counteraction connected so control bus [Music] technology computer science and engineering you 
L0qqdnWxMAY,27,"https://www.youtube.com/user/cbhalodia?sub_confirmation=1

#Crossbar_Switch
#Interconnection_Structure
#Multiprocessor
#Multiprocessor_Interconnection_Structure",2020-04-12T21:30:12Z,Crossbar Switch of Multiprocessor (Computer Organization and Architecture),https://i.ytimg.com/vi/L0qqdnWxMAY/hqdefault.jpg,Chirag Bhalodia,PT4M15S,false,2723,55,4,0,2,hello friends myself Shira today I'm going to discuss with you crossbar switch multiprocessor system topic of computer organization and architecture first of all let's see there are four different schemes in the interconnection structure of multiprocessor system first one is time set bus second one is multi port memory third one is crossbar switch and fourth one is multistage switching Network today we will discuss crossbar switch and remaining three videos are available on my channel you can watch there now this diagram shows the crossbar switch let's see first point the crossbar switch organization consists of a number of cross points now you can see that this is the cross point CPU 1 and memory module 1 is connected over here are placed at intersections between processors buses and memory modules and paths so you can also say cross points are used to join memory module and CPU so there are a number of cross points are available so you can see over here this is the CPU 3 using 3 this cross point it connected with memory module to figure a below figure shows crossbar switch interconnection between 4 CPUs and 4 memory modules the small square so this is the small square in each cross point it is a switch that determines the path from a processor to a memory module now we will take one example like CPU one wants to access from memory module 2 so at that time CPU 1 send one request this cross point check the request it recognized there is for this request for memory module 2 it forward to this cross point this chorus part also recognize or find and it finds that this request for memory mode will true so it forward directly to the memory module to each switch point has control logic to set up the transfer path between a processor and memory next one it examines it means this that cross point examines the address that is placed in the bus to determine whether its particular module is being addressed so it's sent to its requested module it also resolve multiple requests for access to the same memory module on a predetermined priority basis so we can say there is a memory module 1 and there is a number of requests from different CPUs at that time it is resolved by based on priority system now this is the circuit of crossbar switch the circuit consists of a multiplexers that select the data address and control from one CPU so it is for all the CPUs you can see over here data bus is bi-directional address and control buses unidirectional from one CPU for communica munication with the memory module sometimes it is possible there are more than 2 CPU or more than one CPU will request to the multiplexers to access the memory module how can it solve priority levels are established by the arbitration logic to select one CPU so this multiply multiplexer and arbitration logic is available in between the memory module and processors so priority level will be decided by the arbitration logic so when two or more sip your attempt to access the same memory advantages support simultaneously transfer from memory modules in different processes it provides full connectivity a highly useful in multiprocessor system maximum use of bandwidth compared to other networks disadvantages a number of cross points I is equal to n square there are number of cross points will be increased complex Hardware requirement because cross points are increased it means logic also complex vulnerable to single fault thank you for watching this video please like share and comment subscribe my channel on YouTube she Rockville area press Bell icon to get new video notification thank you 
be9q_9Hcips,27,"GTU - Computer Engineering (CE) - Semester 4 - 2140707 - Computer Organization

Computer Organization PPTs are available here: http://www.darshan.ac.in/DIET/CE/GTU-Computer-Engineering-Study-Material

This video is recorded by Prof. Hardik A. Doshi (hardik.doshi@darshan.ac.in, +91-9978911553) at Computer Engineering Department of Darshan Institute of Engineering & Technology, Rajkot as per GTU Syllabus. 

Darshan Institute of Engineering & Technology, Rajkot is a leading institute offering undergraduate, graduate and postgraduate programs in engineering. The Institute is affiliated to the Gujarat Technological University (GTU) and approved by the AICTE, New Delhi. 

Visit us: http://www.darshan.ac.in
Write us: info@darshan.ac.in
Facebook: https://www.facebook.com/DarshanInstitute.Official
Twitter: https://www.twitter.com/darshan_inst
Instagram: https://www.instagram.com/darshan_inst/",2019-03-14T14:02:49Z,5.01 Stack Organization,https://i.ytimg.com/vi/be9q_9Hcips/hqdefault.jpg,Darshan Institute of Engineering & Technology,PT14M22S,false,35351,354,16,0,19,hello everyone in this session we'll discuss about stock organization now what is Tec basically a stack is one kind of data structure using which we can store the data in such a manner that the data which is stored a last is fetch the first that means last in first out so the data which is stored last is fetched first and the organization we are terming it to be the stack organization now this stack organization requires certain address pointers that means we require certain pointers which points to the address of the stake in that case we use one pointer named as SP that is a stack pointer which stores the address of the top of the stack that means the item which is on the top is pointed by this stack pointer so we are using the stack pointer as a date as an address pointer to point to the top of the stack to maintain the positions in the stack now there are two types of stacks which we can implement in our computer hardware organization the first type of stack is a register stack built using registers and the second type of stack is memory stack which is the part we can say the logical part of the memory derived for implementing the stack that means it is one part of our RAM only so a logical section has been made that this part of the memory can be used as a stack so a memory partition which is then logically is used as a stack now let's start with the implementation of register stack that means how register stack is made and how the operations are carried out onto the stack because we are knowing that that on the stack we can implement two operations which are the main made to operations that is push and pop so we'll see to that for both the type of stack organization but if we discuss about the application of stack then one application of the stack is to evaluate the expressions using reverse polish notation and another application which you can say for stack is we are using functions or subroutines and during that time the current is content of the registers are stored onto the stack so there are certain applications of stack let's see the first type of stack that is register stack this is the organization of register stack which uses the registers as well as certain extra registers for implementation of the stack let's see to them one by one now as you can see into this particular picture that this is the stack which is made using registers now as you can see starting with number 0 1 2 3 4 and so on till 63 that means total we have made the stack of 64 registers and this we are denoting as the address that means this is 0th register first register second third fourth and so on till 63 along with that we are using one pointer as we had discussed also we are using one pointer named as stack pointer SP which points to the top of the stack that means it is currently pointing the item onto the stack which is at the top and two more registers we are using that is you can say it to be the flag registers the one is full and another is empty now the thing is full is one kind of register or flip-flop which mentions whether the stack is full or not because when we are storing the items on to the stack at that time it may happen that stack may get full so this full register or flip-flop mentions that whether the stack is full or not if the value is 0 then stack is not full if the value is 1 that means we can say that the stack is full similarly empty this empty flag mentions whether the stack is empty or because while retrieving the values from the stack it may happen that stick make it empty so this flag means this empty flag will state whether the state is empty or not if the value is 0 that means it is not empty if the values 1 that means it is empty so this is how the register stack is implemented one more register we are using that is our normal data register through which the data is transferred to and fro from the stack let's see the operations which are carried out onto this register stack we are knowing stack always are having major to operation one is pushed in another is pop so let's see first of all the push operation let's see how the push operation is carried out now in that case the first step is to increment our stack pointer that means the top of stack is to be incremented by 1 from the current position and the data which is pushed on to the stack is stored on to the address current address pointed by the stack pointer so M of SP is equals to dia that means whatever the content is there in data register is to be transferred to the M of s P that means into the stack which is pointed by this stack pointer that means the current address of the step that means the current address of the stack is pointed by the stack pointer where we want to transfer this data along with that we need to check one more thing if while saving this data on to the stack it may happen that stack may get full so we need to check if the stack pointer is empty then we need to mention that stack is now full so we are setting the full flag to be 1 and we are setting the empty flag to be 0 because both cannot be 1 at a time or both cannot be 0 at a time both cannot be 1 at a time because if one is full it cannot be empty at a time and if it is empty then it cannot be full so if full is set to 1 and we need to set empty to be 0 similarly let's see the pop operation so pop operation in that we are fetching the data first that the content of the stack pointed by stack pointer is transferred to dr that is data register then SP is to be decremented by 1 so SP equals to SP minus 1 and if SP becomes 0 then it's very obvious that the stack is now empty so we are setting the empty flag to be 1 and full flag to be 0 so this is how the push operation and pop operation are carried out on through the resistor stem let's see the second type of organization that is memory stick let's see the memory stick it is almost similar but let's see to it this is our memory now the memory is divided into logical parts so the first part which you are seeing is program where the programs are stored the second part is you are saying to it to be the data operands where the data are stored so you are divided the memory into two parts program and data and one more part which we would be seeing is stick part so our actual Ram is divided into 3 parts that is program data and stack and we have mentioned that the program section starts with address thousand data section starts with address 2000 this arrangement can be done anything now this it depends on you whether you keep data first and then programs or stack first then program and then data it is the logical arrangement so it is to be decided by the designer in the stack is you can say that it has been implemented at the address 3000 till 4000 1 so this is what is the stack part this is the data register now you are knowing that the program is always pointed by the address stored in one register named as PC that is program counter now the data is also pointed by some registers that is address pointer AP and the stack is pointed by the SP that is stack pointer now let's see the push and pop operation carried out on to this or here slightly there is mind difference in this organization with respect to register stack let's see to it over here in push operation as you can see we are detrimental Poynter earlier in case of register stack we were incrementing the stack pointer now over here the question arise what what we should do now it depends on the designer it can be incremented also but this is one another type of organization which we can say so stack pointer is decremented by 1 and the content of stored in dr is to be transferred to M of s P means on to the stick pointed by the stack pointer similarly pop operation now it is reverse first of all would be fetching so from the stack pointed by SP will be transferring the content to dr and later on we would be incrementing the stack pointer because in push we had decremented so in pop we have to increment it so this is the basic organization of memory stack so the two types of organization register stack and memory stack but now let's see the application of this stack where we can imply so the first is reverse polish notation now our mathematical operations which we are writing is not easily implementable in the computer so for that a Polish scientist named Lucas Rix who designed the system for expressing our arithmetic operations in a different way the first way which he said is this is our normal in fixed notation in fix so the first way if what he proposed is prefix means the operator in our expression is to be written first operator so a plus B what is the operator in this plus is the operator so plus is to be written first and then the operands so this is our prefix or polish notation so a plus B is to be written as plus a B and the another type of notation is postfix or it is also termed as reverse polish so in that case it's what we are supposed to do is we are supposed to write the operands first and then the operator so this a plus B becomes a B plus so let's even more example our expression is a star B plus C star D now in this we are knowing the precedence so in this if we go from left to right the first priority is for multiplication so a star B would be performed so in that case it would be a b star then later on plus will not happen it will be C star D so it is C D star and then the result of this two would be added so a B star C D star and plus so you can see a B star C D star and plus so this is what is the reverse polish notation so what happens over here is the a and B would be multiplied first C and D will be multiplied second and the result of t2 would be added later on so this is what is the reverse polish notation let's try to implement one example so evaluation of arithmetic expression let's say we are having 3 into 4 plus 5 into 6 so you are knowing that the bracket should be solved first 2 so 3 into 4 so how this can be evaluated using stick so the expression we are having 3 4 star 5 6 star n plus so the first of all 3 would be there the current pointer when we start it would be 3 so it is to be pushed on to the stack so 3 is to be pushed over here so 3 then next comes is for current the top of the stack is 3 so 4 we want to push so what happens the stack pointer is to be implemented by 1 so it is over here and 4 is to be pushed now next comes the operator is star so what happens over here this two elements are popped that means 4 & 3 are popped and they are to be multiplied because operator has come and we need to store the result back onto the stack so 3 4 star means 3 into 4 that is 12 and the result is to be stored on to the stack back now moving further we are having 5 so what happens is currently 12 is on the stick top of the stack so 5 is to be pushed onto the stack so current point to step pointers to be incremented by one Feist pushed later on what we are supposed to rue is six comes so 12 is on the stack five is on the stack which is on the top of the stack six has arrived so increment the stack pointer store six now what happens next star is there so the operator star has come so we need to pop the two elements that means six and fives to be popped and multiplied so 6 into 5 30 and results is stored back into the stack so this becomes 30 so now the current stack pointer is at 30 now next again another operator comes that is plus so this plus has come 12 n 30 and 12 are to be popped out so 30 and 12 to be added so the result becomes 42 so this is the top of the stack so now the string is over so our final result is 42 so actually this is how the computer evaluates our mathematical expression written in any of the programs so this is one of the application of stack 
4z_M4GMeovE,27,"https://www.youtube.com/user/cbhalodia?sub_confirmation=1

#Multiprocessor
#Interconnection_Structure_Multiprocessor
#Omega_Network
#Binary_Tree_Switch
#Time_Shared_Common_Bus
#Multiport_Memory
#Crossbar_Switch
#Multistage_Network

Thank you so much ðŸ™ for supporting me. There are 1.6K+ subscribers completed of my channel. 

Please share, like and comment on all videos and gain your knowledge.",2020-04-12T20:45:10Z,Interconnection Structure of Multiprocessor System (Computer Organization and Architecture),https://i.ytimg.com/vi/4z_M4GMeovE/hqdefault.jpg,Chirag Bhalodia,PT2M48S,false,2303,30,0,0,0,hello friends myself Shira today I am going to teach you interconnection structure of multiprocessor system topic of computer organization and architecture in interconnections structure we can first of all see the important characteristics of the processors used in multiprocessor system is its ability to share a set of main memory modules and possibly IO devices so there are more than two processors are available in multiprocessor systems so all the processors say the memory modules and also say the input/output devices the components that form a multiprocessor systems are CPUs IO P is connected to i/o devices and memory unit that may be partition into a number of separate modules so we can say that there are number of CPUs are available so that all CPUs connected to i/o devices through IOPS connection so we can say that input/output processor connection in memory unit it is said by all the CPUs so it may be partitioned into a number of separate modules third one the interconnection between the components it is the required in all the computer system so we can say the interconnection between the components can have different physical configuration so all components are connected physically with each other depending on the number of transfer paths that are available between the processor and memory in a shared memory system so there are so many paths available because more than two processors are available so all the paths there are different paths available in shared memory system or among processing elements in a loosely coupled systems so we can say that interconnection structure are required in both the multiprocessor system so there are several physical forms available for establishing and interconnection networks so we can see or hear some different schemes are presented in this section interconnection structure it is divided into four parts so we can say there are four different schemes are there time said common bus multi-port memory crossbar switch and multi stage switching network that all are all schemes video are available on my youtube channel you can watch there thank you for watching this video please like share and comment subscribe my channel on YouTube JIRA valeria press Bell icon to get new video notification thank you 
SaFvx6MOcPU,22,"In this lecture we will learn about different parts of Instruction set register ,Addressing modes,Opcode, and operands precisely.
For funny and amazing stuff must watch and follow:follow:http://www.instagram.com/dr.usman700/
#Opcodevsoperands,#addressingmodes,#computerregisters, #addressing modesincomputerarchitecture",2020-07-08T23:44:06Z,"Lec3,P2:Parts of instruction register,opcode vs operand,memory modes in computer architecture/COA",https://i.ytimg.com/vi/SaFvx6MOcPU/hqdefault.jpg,Incentive FootSteps,PT10M9S,false,125,6,0,0,5,bismillahirrahmanirrahim welcome to the series of computer organization and architecture so the topic of today's lecture is parts of instruction register so what are different parts of an instruction register how the data is stored in register how we access the data and how we manipulate and interpret and then perform operations according to the instructions so let's see parts of is a programmers write instructions or commands for processor to perform operations machine has to interpret it according to the programmer instructions so the machine will put these instruction into its instruction register so the basically instruction register has three fields number one is addressing mode number two is opcode and number three is de aprender addressing mode is optional in some instruction addressing mode is specified and some are not well the second field is the opcode that this stands for operational code that basically define what type of operation we have to perform so after the operational code the third field is the operands operands are basically the data or the memory locations on which we perform the operations according to the operation code so let's see all of these in detail so the first one is operational code upcourt is operational code which tells which type of operation has to perform we have discussed multiple type of operations like addition subtraction multiplication increment or the comparison operations are uncontrolled slow operations and many many more operational code define in bits form like in 0 & 1 form so machine understand from bits that what type of operation we have to perform all functions are of God like add multiplication increment etc as we have discussed every operational function has different bits like + operation has different bits as compared to the - bits on the basis of bits a processor inferred that what type of operation it has to perform normally operation code size is 6 bits so with these 6 bits different combination we can perform 64 different operations for example for an arithmetic calculation like 2 plus 3 here the plus sign which we use is the opcode that is the operational code well 2 & 3 are the operands the next field is the operands so what are the operands operands are the data or memory address of data on which we have to perform operations so keep in mind then operands are not only the data it can be memory address too so we can't access operands directly operands has addresses to perform operations on operands we take operand addresses from memory in short operands can be a delta or a memory location or a combination of both an operand address can be any memory address CPU register address or input/output device address if the data saves in register then the processor will execute instruction faster while if data is in memory then first the processor has to fetch it decode it and then perform the actions so this is a lengthy procedure so saving that data in the register is a faster way to perform operations so the next is addressing mode data saves in memory we access from memory bring it fast in register than ALU that is the ratha Matic logic unit where all the operations are perform all the mathematical calculations and operations are performed so addressing mode basically depicts how we will deal with data how the operations will perform on data addressing mode is optional in instruction system addressing modes specify how the operands will show in instruction set it defines rules for interpreting or modifying the address fields of the instructions before the operand is actually executed memory address is mode deliver flexible access to memory letting you to easy lexis variables arrays records pointers and other complex data types so addressing mop provides flexible access to memory and memory content so operands are accessed according to the access mode addressing mod defines how our a parents are defined in register and how the look addressing mode is optional in instructions that like incremental accumulator directly perform actions there is no memory address given in this instruction while on the other hand at a where a is a memory address so where does instructions that architecture resides where we should place instruction set should we place it on the hardware or on software so instruction set can be constructed into the hardware of the processor that is more efficient and faster for running programs or we can emulate it in software using an interpreter but that is a slower procedure and a slower mechanism for running programs let's summarize the topic so the instruction set consists of addressing modes instructions native datatypes register memory architectures interrupt exception handling and external iOS we will see all these terms in detail in the coming lectures so first of all we must know that what is processor and what it does the CPU that is stands for central processing unit or the processor performs four basic tasks number one it's fetching it fetches the data from the memory as we have earlier discussed that we save data in the memory so to perform operations on the data the processor will fetch the data from the memory and then decode it as the software engineer or a programmer give instructions to computer in high-level language so the ISA that is the instruction set architecture converts these high-level language into the low-level language so the processor using the ISA decode the instructions into the low-level language so that the processor can understand it and perform actions according to the instructions after that it executes the instructions it performed the operations on the operands and finally after the execution it write backs the result to the memory and save the results into the memory so these are four main basic tasks that a processor performs there are two main types of Isis the number one is cyst and the other one is risk so the boat is a architectures sisk that is complex instruction set computing and the risk that is reduced instruction set computer computing primary job is decoding and executing the instructions how the sisk and RISC decode and execute the instructions we will see it in the next lecture thank you very much 
3ikhZKUTNYI,27,,2015-10-21T09:53:35Z,Computer architecture: introduction (CS),https://i.ytimg.com/vi/3ikhZKUTNYI/hqdefault.jpg,Vidya-mitra,PT31M17S,false,2414,12,0,0,2,welcome to the epal shala lecture series in computer science we are going to deal with the topic on computer architecture so the first module is on introduction to the course so the objectives of this module are to understand the importance of studying computer architecture indicate the basic confidence and working of the traditional von Neumann architecture discuss the different types of computer systems that are present today and the types of parallelism that exists among them and how to handle those parallelisms so first of all let us look at why we should study this course at all why study computer architecture first and foremost it is an exciting subject you find many interesting facts that are thrown open to you and you'll find it a very interesting course any computer engineer or scientist should basically know the underlying details of the machine he or she is going to use you may be an application programmer you may be a compiler writer whoever it is if you know the underlying architecture then you'll be able to use the machine much more effectively and your performance will also improve to become an expert on computer hardware you need to know the underlying concepts of computer architecture also to become a computer system designer you definitely need to know the internals even if you are only looking at becoming a software designer you need to understand the internals of the machine in order to improve the code performance also to explore new opportunities you need to be updated about the latest technological improvements that are happening only if you know the latest technological improvements you'll be able to apply those technological improvements you'll be able to use those explore those technological Romans to your advantage and you know that it has an impact on all fields of engineering and science because computers are present everywhere and whatever field of engineering and science you are at you know that computer are very predominantly used and this study on computer architecture will be very useful in order to use your machine more effectively now what is a computer by definition a computer is a sophisticated electronic calculating machine that accepts input information it processes the information according to a list of externally stored instructions and finally produces the resulting output information if you look at the functions performed by a computer based on whatever operations it does it accepts the information first of all so you need to have some functional unit which will accept the information to be processed as input it stores a list of instructions to process the information it processes the information according to the list of instructions that you've stored and finally provides the result of the processing as output now based on the functions that we have identified for any digital computer you also talk about different classical components of a digital computer now first of all we saw that you need to get input information you need to process the information and then you need to output the information you should also have provision to store the instructions as well as data now when you talk about the classical components of a digital computer you talk about a data path the data path is the path through which your information flows the data flows so you have an arithmetic and logical unit the called the ALU which includes functional units like adders subtractors multipliers shifters etcetera and you also have registers which are used as storage media within the processor because the data has to be stored somewhere for processing so you need to look at registers registers which are inbuilt storage mechanisms available within the processor and the ALU which is used for performing all arithmetic and logical operations now this forms the data path of you now if you look at the control path you need to have some unit which will coordinate the activities of the various units you should know when data flow should flow from one point to another point when an addition operation has to take place when a subtraction operation has to take place so on and so forth so you need to have a control part so the control path coordinates the activities of the various units of the computer system and the data path and the control path put together is called the central processing unit or popularly abbreviated as they CPU so the CPU consists of the arithmetic and logical unit they Lu it consists of some registers which are used for storage and it also consists of the control unit which forms the heart of the computer system and coordinates the activities of the various functional units of the computer system now all this put together is normally called the central processing unit so this has been depicted pictorially here so the five classic components of additional computer so you have the processor shown in green in the middle you have the processor shown in green that is the central processing unit which consists of your control unit it has the ALU and it has storage you have the memory unit which stores all the information that is required for processing so it stores the data as well as the program program is nothing but a list of instructions all of you all know that additional computer is only a dumb machine it works according to the instructions that are given to it if you instruct it to add at the add if you instruct it to shift attrition so you need to have structures given to the processor these instructions as well as the data on which these instructions will have to operate on will have to be stored somewhere and that storage is what is called your memory and you have your central processing unit which consists of the control unit and ALU which is responsible for executing the stored program from memory so initially the program is stored in memory you take the instructions from there you execute them and then you have and when an instruction instructs you to do some arithmetic or logical operation you have the ALU which does the arithmetic operations or logical operations as requested by the program and your input output unit is basically for communication purposes so you need to feed your instructions as well as data into memory so you need an input unit for it similarly you need to take out the result and display the result to the outside world communicate the result to the outside world for that you need an output unit so communication with the outside world can be done with devices like a monitor a keyboard or SEP different types of storage devices now apart from these classical confidence every machine typically has a network component for communication with other machines so we know that it is most of the machines these days we don't operate them only as a standalone machine we need to communicate from one machine to another machine either within a very short distance or across the globe you may have to communicate with other machines so you definitely need to have communication from one machine to another machine so you definitely have to have a networking component which is used to communicate within the various machines now having looked at the functional components of additional computer having identified the basic components of a digital computer now what is it that we are going to study in this course on computer architecture I pointed out that computer architecture is an exciting course and is important for everybody we've also identified the classical components of a digital computer now what is it that goes into this course on computer architecture when you look at computer architecture we say computer architecture composes of computer organization plus the instruction set architecture now what you mean by the instruction set architecture or what is abbreviated as ISA is what the computer does it is a logical view of what the computer is capable of doing and when you look at computer organization it basically talks about how the ISA is implemented so which gives you a physical view of how whatever you've specified that the processor will do is actually getting implemented so basically we give a specification here we say these are the things that the machine is capable of doing that is specified in your instruction set architecture and how it actually gets implemented gives you the computer organization part so both these put together is normally called computer architecture and in this course we are trying to cover both the computer organization part as well as the is a part now what are instructions we've been talking about instructions we've been talking about data now what do you mean by instructions instructions basically specific commands to either transfer information from one point to another within a computer say for example we've looked at different classical components of the processor we looked at an ALU we looked at a memory we looked at storage units like registers etcetera so you may have to transfer information from one register to another register you may have to transfer information from a register to a memory location memory location to a register or an input/output device to register so all these types of transfers will have to happen now how is this going to happen so you will have specific instructions which will say transfer the information from this source to this destination so instructions basically specify commands to either transfer information from one point to another within a computer it may be transfer of information between the computer and the i/o devices so it is not necessarily between registers and memory or between registers themselves it may also be transfers from i/o locations or i/o devices to computers you may have instructions which will instruct the computer to perform arithmetic and logical operations like multiply these two numbers add these two numbers and these two numbers shift write this number by so many bits so all those instructions which will tell the computer what arithmetic logical relations to do and apart from this it is not just enough if you can transfer information from one point to another and instruct of what operation is to be done and store it in some place but you also have to have some instructions will control the flow of the program say for example I am trying to add two numbers if the result is greater than something I want to take one course of action if the result is less than something I want to take a different course of action so you need to have some instructions which will allow you to control the flow of the program so you need to have instructions which are specifically used for that so some instructions like jump I may jump from one point to another or may I have a subroutine called subroutine call is a function cut so you write modular programming so when you are executing something I need to specifically go to a point execute a function get the result and then continue with my main program so all these instructions are examples of control flow instructions so you need to have instructions to specify all this and once you have these instructions now all these instructions you put some of these instructions in a sequence so our sequence of instructions to perform a particular task is called a program and which is stored in memory say for example if I have to add two numbers so you what you will have to say is initially those two numbers let me say those two now numbers are stored in memory so from memory you can't you will have to bring them to the arithmetic and logical unit specifically be added to add those numbers so you will have to first of all data transfer instructions to transfer these two data from memory to your adder instruct the adder to perform an addition operation and if you want to put the result back in memory then you will have to again use data transfer instructions so store the result from your registers or from the added unit to your memory location so whatever task you need to perform you need to have a sequence of instructions that need to be written to perform the task and that is basically what is called a program and what the processor does this the processor fetches instructions that make up program from the memory and performs the operations stated in those instructions exactly in that order so suppose if you have a control flow instruction in between and it says don't execute the next instruction but jump to some other location and execute that instruction it leg and jump to that point so whatever is specified in your sequence of instructions is exactly executed by the processor and now once we know what these instructions are get a at least a rough idea of what these instructions are we also need to know on what data will these instructions operate on so we talk about data data are the operands upon which these instructions operate now the data could be numbers it can be decimal numbers it can be binary numbers it can be octal numbers whatever it is and all the data can be encoded characters data in a broad sense means any digital information and computers use data that is encoded as a string of binary digits called bits now if you look at the memory unit the memory unit stores instructions as well as data as a sequence of bits and instruction is also encoded as a sequence of bits a data is also encoded as a sequence of bits and the processor reads instructions and reads and writes data from or to the memory during the execution of a program group of bits stored or retrieved at a time is normally called a word the word length of the processor depends upon the processor that you are looking at suppose if it's an 8-bit processor you talked about a word length of eight if it's a 64-bit processor you talked about a word length of 64 and the number of bits in a word is termed as the word length of a computer now in order to read or write to and from memory a processor should know where to look and memory consoles of a number of memory locations you may have say for example if I'm looking at a 1k memory I may have thousand 24 memory locations there now how do you identify which memory location to read from or right into so just like how we have unique addresses to identify our houses each memory location has a unique address so in order to access a memory location we need to know the unique address of the memory location and the processor reads or writes to and from memory based on this memory address a random access memory provides fixed access times independent of the location of the word which is known as the memory access time so you normally define memory access time as the time that elapses between the initiation of a request and the satisfaction of the request say for example I've put in a memory read request so the time between the requisition that has been putted and the time when the data actually arrives is called the memory access time so the memory access time depends upon the speed of the memory unit a slow memory has larger access times a fast memory has slower lower access things now the memory and the processor which you have to communicate with each other in order to read and write information so in order to reduce the communication time a small amount of RAM normally known as the cache is tightly coupled with the processor and also modern computers have three or four levels of RAM units with different speeds and sizes so the fastest and the smallest in size is normally known as the cache and the lowest slowest and the largest in terms of density in store in terms of storage capacity is normally known as the main memory now the primary storage is insufficient stood for large amounts of data and programs though we look at a main memory which is very high these days the main memory is not obviously enough to store all your programs and data so you need to look at secondary storage also so you look at secondary storages which are capable of storing large amounts of data examples are magnetic disks and tapes optical disk CD drives and the access to the data stored and secondary storage is definitely slower but you take advantage of the fact that some information may be accessed and frequently and the cost of the memory unit obviously depends upon his access times so the closer the memory is to the processor it is obviously going to be the fastest and it is going to be more expensive now coming to the arithmetic and logical unit the arithmetic and logical unit has already pointed out it is responsible for performing all your arithmetic and logical operations and in order to execute an instruction operands need to be brought to the ALU from the memory so we need instructions which will bring the operands from the memory to your ALU and perform the operation and similarly the results of the operation are stored back in the memory or retained in the processor itself in the registers for immediate use now if you look at the control unit the operations of the input unit the memory unit the ALU and the output unit are all coordinated by the control unit instructions control what operations take place whether it is a data transfer that has to take place or an ALU operation that has to take place the control unit in order to do these coordination activities generates what are called timing signals which determine when a particular operation should take place now how now we've looked at the different functional units of a processor how are these functional units connected together so they can come so that they can communicate among themselves so for our computer to achieve its operation the functional units need to communicate with each other and they need to be connected with each other the connection is done by means of a set of wires called a bus so the bus that has been shown here is responsible for establishing a connection between the various components of the processor a bus is nothing but an interconnection of wires so if you are talking about an 8-bit bus it will be capable of carrying eight bits of information functional units as I told you is connected by means of a group of parallel wires called a bus and each wire in a bus can transfer one bit of information and the number of parallel wires in a bus is normally equal to the word link the computer because when you talk about a processor which has a word length of say 64 bits it means typically we are going to operate on 64 bits of data so it is only reasonable that we also have a bus which can transfer 64 bits of data from one point of the computer to another point of the computer now having looked at the functional confidence we look at what is meant by the traditional von Neumann architecture traditional architecture with the classic components mentioned earlier which uses the concept of a stored program concept is normally called a von Neumann architecture what we mean that by this is instructions as well as data are stored in memory a sequence of zeros and ones and the processor executes these instructions sequentially and program flow is controlled or governed by the type of instructions and other factors like interrupts etcetera there and the fetch and execute cycle is repeated continuously so an instruction is fetched from memory it's executed and then you go ahead and fetch the next instruction from memory so this indicates the fetch execute cycle so first of all we do a fetch cycle where the instruction is being fetched from memory so you recollect that instructions are being stored in memory so you need to have the address of the instruction using the address of the instruction go and fetch the instruction then decode the instruction decode the instruction is after all it is a sequence of zeros and ones so you need to know what what is to be done with those zeros and ones whether it is an addition to be performed or what operation to be performed where the operands are available so that entire information has to be decoded which you will understand as we look at the instruction set architecture and once you've understood where the operands are you will have to fetch the operands and once the operands have been fetched you will have to go ahead with the execution of the instruction perform an addition or subtraction or whatever operation and then finally store the result this is called the fetch executed cycle which is done repeatedly in the case of for anointment architecture this gives you an overall picture for general computer system which follows the more norman architecture know the advantages of the stored program concept is not programs can be simply shipped as files of binary numbers where you can maintain the binary compatibility and computers can inherit ready-made software provided they are compatible with the existing ISA this leads industry to align around a small number of biases now when we look at computer organization organization as we pointed out earlier is the realization of the instruction set architecture and you will have to look at the characteristics of the principal components that make up your computer system you will have to look at ways in which these computer systems are interconnected you will have to find out how information flows between these components you will have to look at how what is meant by such information flow and how is this information flow actually controlled and you will also have to have an RTL description of what is happening within the machine now all this put together is what is called computer organization so as I already pointed out we are looking at the ISA which gives you the specification what the processor is capable of doing and you have an organization which basically tells you how this gets implemented and this is traditionally implemented on a war norm and architecture which repeatedly goes through a fetch execute cycle so this gives you again an overall picture of the internet West netburst microarchitecture which gives you the different levels of caches the fetch and decode unit the retirement unit you will understand and the caches and all that you will understand all these diagrams in a much better way as we proceed through the course so we've had a lot of technological improvements that has been happening so starting from 1951 from vacuum tubes we went into transistors icees VL s i--'s ultra scale icees so on and so forth and we find that the processor transistor counts have increased about 30 to 40 percent every year thanks to Moore's law more slot was basically proposed by Gordon Moore of Intel in 1965 and he proposed that the transistor densities are going to be doubled every 18 to 24 months and that has really been holding good the memory capacity also has gone up to about 60 percent per year the disk capacity about 60 percent here and you have opportunities all these technological advancements have given rise to opportunities for new applications and this has given rise to better organizations and designs I've already pointed out Moore's law and this shows the processor performance over the past two or three decades from 1978 to 2005 you find that after 2005 you find that the performance has actually slowed down by what is called the power problem and the memory latency problem now before we look at more details of other components we look at the classes of computer systems you have different classes or different types of computer systems that are available one is the desktop or notebook computers where this is the most competitive market you're looking at general purpose applications here where you plan to spend a run a lot of applications here and main constraint is the cost performance trade-off here the next category of computer systems is your server systems where they are network based you need to have high capacity for them performance is very important for them reliability is very important reliability and availability are very important here and it basically ranges from small servers to huge servers that are even building-sized the next class of computer systems is again an important classification embedded systems where the computers are hidden as part of as components of other systems for example when you look at a mobile phone when you look at a mobile phone you don't realize that it is a computer system but you know that there are many processors inside your mobile phone that is a classical example of an embedded system a washing machine is a simple example of an embedded system these embedded computers have a stringent power performance requirement can't they have stringent cost constraints and they are specifically meant for application specific performance you have a processor which is meant to do a particular task because unlike a desktop processor you're not going to run a range of applications there it is going to main be meant to do a particular type of application so you expect it to perform well with respect to that particular application and this is a class of computer system which covers a wide range your requirements may range from a very small toy car application to a very sophisticated diagnostic system for example or surveillance mechanism so depending on that all your requirements are going to change last of all you also have the personal mobile devices which are very very predominant today where cost is important energy is important and media performance becomes very very important for personal mobile devices and personal mobile devices also we'll have to lay a lot of importance on the responsiveness once you put in a query to your PMD you expect to get an answer immediately so the responsiveness is very very important when you're looking at personal mobile devices and of course these days you also have clusters and where of sale computers which are becoming very very popular so you have a large number of computers put together and called a cluster or aware of sale computer so here again price performance becomes very very important throughput is more important here the number of transactions done per unit time or the number of web services that have been serviced all that becomes very important when you are looking at clusters so that is again the same as that of your service and energy proportionality also gains a lot of importance when you look at this type of computer system so having seen the different classes of computer systems will have to point out to what are the driving forces of computer design so the main driving forces of computer design the primary constraints are energy and cost today everybody is striving to design computer systems which will minimize your energy and cost and also we'll have to look at the different types of parallelism that your applications exhibit and try to explore this parallelism in the computer systems that we design so that becomes the primary driving force of a computer system so if you look at the different types of parallelism programs may exhibit what are called data level parallelism and task level parallelism so when you try to exploit this parallelism you have to bring in computer systems design computer systems which will try to exploit this parallelism that is available in your programs so you need to look at exploiting data level parallelism task level parallelism is more independence so you can have different tasks of execution and you will have to exploit this instruction level parallelism s if you look at different instructions in a program if you find that instructions are independent of each other can you try to look at executing these instructions independently that's executing the instruction level parallelism thread level parallelism is exploited more in terms of task level parallelism and when it is a more loosely coupled architecture we call it a request level parallelism so applications exhibit different types of parallelism and the computer hardware that you're designing should try to exploit that parallelism and try to give you better performance so in order to summarize whatever we did in this module we first of all pointed out why you need to study computer architecture itself so the motivation for the course what is it that you're going to study in this computer architecture course so and then we pointed out the functional units of a digital computer how they are interconnected what is meant by a traditional warning and architecture and what is meant by the fetch execute cycle and last of all we pointed out the different classes of computer systems and the driving forces that are driving us to come up with better and better computer architectures in order to exploit the parallelism that is available among the various applications and also bring down the energy and cost thank you 
6GH4Xe_h55Y,27,"Part 3 : Computer Architecture and Organization - Control Unit and Micro programmed Control 
OPEN BOX Education
Learn Everything",2018-08-19T18:14:21Z,Part 3 : Computer Architecture and Organization - Control Unit and Micro programmed Control,https://i.ytimg.com/vi/6GH4Xe_h55Y/hqdefault.jpg,OPENBOX Education,PT18M49S,false,1351,5,1,0,0,control unit and microprogrammed control learning objectives at the end of this topic you will be able to predict the possibilities of developing a control unit in two different ways understand and describe a hardwired control unit understand and describe a microprogrammed control unit which involves micro instructions and micro programs outcomes by the end of this topic you will be able to analyze the functionality of control unit designed control unit hard-wired control micro programmed control for various applications we know what machine instruction instruction set an instruction cycle are and what the relationship between them is as discussed in unit 2 we also know that all the activities inside the machine are directed by the control unit which is a part of the CPU or the processor here is a list of move instructions for Intel 80-85 microprocessor this instruction copies the contents of the source register into the destination register the contents of the source register are not altered if one of the operands is a memory location its location is specified by the contents of the h-l registers for example move B comma C or move B comma M it is an assembly language which is in human understandable form move B comma C means copy source register C's content a destination register B we can express this action in another language called register transfer language RTL the RTL equivalent instruction to this instruction is B reverse arrow C see if this happens a tea not cycle then it is written as T not at B reverse arrow C micro-operations are those operations that are performed on the data stored in registers the control unit initiates a micro operation by sending control signals to the intended components after the register to register movements happen we say the micro operation is executed to summarize we can say that the execution of a program consists of the sequential execution of instructions each instruction is executed during an instruction cycle made up of shorter sub cycles example fetch which involves a few micro operations in direct address calculation which again involves a few micro operations and similarly operand fetch executes interrupts so like we have categories and machine instructions we have categories and micro operations too like there are data transfer micro operations arithmetic micro operations logical micro operations and shift micro operations all these micro operations may involve a transfer between registers or a transfer between a register and an external bus or an arithmetic operation or a logical operation a few examples of arithmetic micro operations are few shift micro operations are few logical micro operations are let us see now what those micro operations which are involved an instruction cycle are assume that an instruction resides at a 16-bit address 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 which is in binary and hex equivalent is 0:06 for this address resides in the PC this address is moved into M AR which points to a location in memory the memory vert at the set address is then transferred into MBR this is an instruction which then is moved into IR the fetch phase is done let us see the micro operations involved in the instruction cycle going to the first face the fetch phase in the first time unit move contents of PC to MA our second time unit move contents of memory location specified by M AR to MBR increment by one the contents of the PC third time unit move contents of MBR to I our indirect cycle the address field of the instruction is transferred to the M AR this is then used to fetch the address of the operand finally the address field of the IR is updated from the MBR so that it now contains a direct rather than an indirect address an interrupt cycle in the first cycle the contents of the PC are transferred to the MBR so that they can be saved for return from the interrupt then the M AR is loaded with address at which the contents of the PC are to be saved and the PC is loaded with the address of the start of the interrupt processing routine these two actions may each be a single micro operation the final step is to store the MBR which contains the old value of the PC into memory the processor is now ready to begin the next instruction cycle the execute cycle this cycle actually causes the last fetched macro instruction happen a binary adder realizes the binary numbers addition a 4-bit full adder is shown here now let us take an instruction add r1 comma r2 which adds the contents of our - with our Vaughan and stores the result in Arvind the micro operation requires one cycle because it is a pure register reference instruction consider another add instruction add r1 X where X is a memory locations address this instruction requires three cycles in the first cycle the content of the address part of the instruction register is transferred or copied into M er which points to a memory location in the second cycle the memory content of the location pointed by M AR is transferred MBR which is a processor register in the third cycle the content of MBR is added to the content of r1 and the result is stored in our von we can define the functional requirements for the control unit those functions that the control unit must perform so what are those functions that a control unit must perform in order to cause micro operations a definition of these functional requirements is the basis for the design and implementation of the control unit after a review of what we've seen so far we can say that all micro operations fall into one of the following categories transfer data from one register to another transfer data from a register to an external interface example system bus transfer data from an external interface to a register perform an arithmetic or logic operation using registers for input and output the control unit performs two basic tasks sequencing the control unit causes the processor to step through a series of micro operations in the proper sequence based on the program being executed execution the control unit causes each micro operation to be performed since the micro operations do not happen at one time timing the micro operations is a feature that makes that task of Martling a control unit easier a general model of a control unit showing all of its inputs and outputs the inputs are clock this is how the control unit keeps time the control unit causes one micro operation or a set of simultaneous micro operations to be performed for each clock pulse this is sometimes referred to as the processor cycle time or the clock cycle time instruction register the opcode an addressing mode of the current instructions are used to determine which micro operations to perform during the execute cycle flags these are needed by the control unit to determine the status of the processor and the outcome of previous ALU operations for example for the increment and skip of 0 i SZ instruction the control unit will increment the PC if the zero flag is set control signals from control bus the control bus portion of the system bus provides signals to the control unit the outputs are as follows control signals within the processor these are of two types those that cause data to be moved from one register to another and those that activate specific ALU functions control signals to control bus these are also of two types control signals to memory and control signals to the i/o modules this example shows a simple processor with a single accumulator ac the teeter parts between elements are indicated the control parts for signals emanating from the control unit are not shown by the terminations of control signals are labeled as C with a numbered subscript and indicated by circle the control unit receives input from the clock the instruction register and flags with each clock cycle the control unit reads all of its inputs and emits a set of control signals control signals go to three separate destinations data paths the control unit controls the internal flow of data for each path to be controlled there is a switch indicated by circle in the figure for example the various control signals that are generated from fetch to decode is illustrated the control signals see two moves the content of PC to MA are with CR to memory and c5 the memory content is transferred to MBR with C for the content of MBR is moved to IR and with c-13 the content of IR is moved to control unit for decoding a control signal from the control unit temporarily opens the gate to let data pass Lu the control unit controls the operation of the ALU by a set of control signals these signals activate various logic circuits and gates within the ALU system bus the control unit sends control signals out onto the control lines of the system bus example memory read control signals for other operations are shown in the table in a hard-wired implementation the control unit is essentially a state machine circuit its input logic signals are transformed into a set of output logic signals which are the control signals first consider the instruction register the control unit makes use of the opcode and will perform different actions issue a different combination of control signals for different instructions to simplify the control logic unit there should be a unique logic input for each opcode this function can be performed by a decoder which takes an encoded input and produces a single output in general a decoder will have n binary inputs and 2 ^ n binary outputs each of the 2 to the power of n different input patterns will activate a single unique output say the decoder is 3 cross 8 decoder it takes 0 1 0 as input and 0 2 is the output that goes into the control logic that generates a combination of control signals the decoder for a control unit will typically have to be more complex than that to account for variable length op codes an example of the digital logic used to implement a decoder is shown control unit logic to define the hard-wired implementation of a control unit all that remains is to discuss the internal logic of the control unit that produces output control signals as a function of its input signals essentially what must be done for each control signal to derived a boolean expression of that signal as a function of the inputs let us define two new control signals B and Q that have the following interpretation PQ is equal to 0 0 fete cycle PQ is equal to 0 1 in direct cycle PQ is equal to 1 0 execute cycle PQ is equal to 1 1 interrupt cycle then the following boolean expression defines in a modern complex processor the number of boolean equations needed to define the control unit is very large the task of implementing a combinatorial circuit that satisfies all of these equations becomes extremely difficult the result is that a far simpler approach known as micro programming is used the term micro program was first coined by M V Volquez in the early 1950s in addition to the use of control signals each micro operation is described in symbolic notation the two basic tasks performed by a microprogrammed control unit are as follows micro instruction sequencing get the next micro instruction from the control memory micro instruction execution generate the control signals needed to execute the micro instruction in executing a micro program the address of the next micro instruction to be executed is in one of these categories determine by instruction register next sequential address branch the first category occurs only once per instruction cycle just after an instruction is fetched the second category is the most common in most designs however the design cannot be optimized just for sequential access branches both conditional and unconditional are a necessary part of a micro program based on the current micro instruction condition flags and the contents of the instruction register a control memory address must be generated for the next micro instruction address information in a micro instruction can be to address fields single address field variable format consider that there are a total of K different internal and external control signals to be driven by the control unit and will key scheme key bits of the micro instruction would be dedicated to this purpose this allows all of the 2 power K possible combinations of control signals to be generated during any instruction cycle but we can do better than this if we observe that not all of the possible combinations will be used summary let's summarize the topic micro operation is an elementary operation performed on the information stored in one or more registers binary adder is constructed with full adder circuits connected in cascade the control unit performs two basic tasks sequencing the control unit causes the processor to step through a series of micro operations in the proper sequence based on the program being executed execution the control unit causes each micro operation to be performed hard-wired control unit is a state machine circuit whose input logic signals are transformed into a set of output logic signals which are the control signals the two basic tasks performed by a micro programmed control unit are micro instructions sequencing get the next micro instruction from the control memory micro instruction execution generate the control signals needed to execute the micro instruction 
YawWXVU3lDk,27,"This video is a lecture for Chapter 6 Part 1 Interfacing & Communication for DCN1013/BCN1043 Computer Architecture and Organization. You can gain access to more information in our google classroom account.


For Universiti Malaysia Pahang Students Enrolled in this class (BCN1043/DCN1013). Please complete the google form in the link below for confirmation of your attendance:
https://forms.gle/c77g2WxxzmTsc36b8

 You can download the google classroom app at: 
https://play.google.com/store/apps/de...
or
Open it directly in your browser at: 
https://edu.google.com/products/class...",2020-12-29T08:08:16Z,DCN1013/BCN1043-Chap.6-Part 1 Computer Arch.& Org.: Interfacing & Communication,https://i.ytimg.com/vi/YawWXVU3lDk/hqdefault.jpg,Syafiq F.K. Academia & Solutions,PT22M57S,false,533,23,0,0,0,salaam alaikum very good morning dear students so today i'm going to continue our next topic for that for our computer architecture and communication class and the chapter six which is under interfacing and communication so before we start our class okay i would like to give my thanks for you guys uh to you guys for your commitment in completing all the chapters and then continue with me up until the end of the semester so hopefully we will finish actually around two more topics left apart in chapter eight so this is the third last topic that we have here so without further ado let's start our topic now so okay so in this chapter we are covering and the interfacing of communication means how data travels inside your computer especially between the referrals and the processes within your computer so firstly okay i would like to start with the referrals that we have first so these are the examples of our three photos that we have in our computer so this one okay you are familiar with it which is the game controller keyboard speakers this one is for your cd this one is for your video tape printer and also for mouse so all of this require communication between the computer and the outside work okay so you cannot use them without a computer and they are not working without any interaction from outside world so these are what we call input and output so what are input device so basically input device is those who take data who takes information from the outside world and outside world and converts them into a digital data so example is that your keyboard takes the information from you pressing the keyboard and turns it into a digital data so other than that we have this mouse and then we have this game controller all of them place the same information say play the same role as an input device so cd for example it can behave both as an input and output device as well so next we have the output device so the most common output device in your computer's setup is usually the monitor and sometimes you might have a printer with you okay so printer is called as an output device as well if it's uh to the scanner so scanner is an input device so output device is where you have the the digital signals and you want to convert them into a physical information like for example screen or for example audio from your speaker okay so some of them can behave as both okay for input and output device example is your network okay network devices so this one example of a network mode app so previously we have this one this one is in late 1990s that consists of a dial-up communication within this modem and here is example of a network card for your pc okay previously 20 years ago 30 years ago some of the computers did not have the network card included inside the pc so you have to include an expansion board to include the network card as well so so some of the device they behave as both like for example input and output okay example if you put an output uh the touchscreen okay and then the pendrive cd dvd they can be input and output as well so this is the difference between the input and output device for your pc so within the input and output usually we have a wide variety of peripherals and they are delivering different amount of data okay and then they have different speeds and they are usually presented in a different format so example imaging you have a different format than audio so we need an i o module for us to supervise all this information coming in into the pc if not our cpu will be burdened of all those information coming in so the i o module basically they are not connected directly to the cpu so the devices they usually connect to this io module first another module will control what kind of information is coming in into the computer and our module they are connected to the cpu using three lines of the system bus they have the address line data lines and the control lines so address line includes address of any kind of location or within the pc data is data involved and controllers means either it's for read or right so iel modules basically they act as interface for the external device to cpu and memory via the system bus and they interface to one or more external devices by a tailored link so this is example of the i o module architecture they are connected into the address line data lines the control lights and the bottom one is where the links to the frequency device is developed so okay so why can't we connect the peripheral directly to the cpu and the buses without io module okay so one of the reason is that we have different kind of referrals so they have different functions does require different control logic to gain information so some of them also have a different data transfer rate so some of them they are very fast and some of them they are very slow for the cpu to process so this one create a problems where there are a lot of preferred for you to attach to the pc and you have problems in timing distribution of those data and those data also they have different format and different word length depending of what kind of information they bring so some of the bigger data information that coming into your pc comes from your camera okay the less data coming to your pc like for example from your keyboard keyboard doesn't have much data information to come into your pc because they're just a button but camera they carry images so they have a huge data type hue size the data type is huge size for your cpu to process so that's why we have for each different very first we have this device interface that operated by the device driver so depending on what kind of device we have uh different types of interfaces within your pc so all these interfaces they are connected into your i o module and i o module will control the distribution of the information from your devices into the processor so basically if you look at memory memory they have what we call the memory addresses and also data so memory address represent where in the location within the memory hardware your data is located in but for interface device okay io module control address from which device the data is coming in from or coming out too and based on these addresses okay it will communicate with the processor so so example of external device okay we have three type of external device basically that we have so the first one is the human readable the second one is the machine readable the third one is for the communication device so human readable device example is your monitor okay and also your speaker or even we can't recount as your sound also as human readable even though you hear it okay because you can read the data by your ear okay so we call it as a human readable as well next one we have what we call the machine readable so machine readable suitable for communicating with equipment like for example we have a cd your pen drives okay uh your a lot of previously we have magnetic these tape systems okay so all of them are machine readable means that the external devices they only can be used with a pc okay you cannot use it uh independently than the pc so this is what we have machinable and the final one is for communication device so communication device is for communication between one device to another so this is basically uh what we call modem your network interface controller okay your network devices switch everything and sometimes we include sensors and actuators as well so for all those device external device external peripherals so we call it so the basic architecture of those peripherals is display here okay they must have three things within their component they have a transducer they have a buffer and they have a control logic so transducer is the one responsible of getting the information from the environment so anything happening in the environment they will convert them into a data and then we have a buffer to host all the data and we have the control logic that depending on what kind of signal coming into the device okay it will send the data or receive the data so if the control signal want to read the device we can miss the csp you want to read the device control logic will give a response either it is ready or not ready okay to give the data and once it's ready it will transfer all the information stored in the buffer to the cpu so this is basically the architecture for all the external devices in the world so the function of the io module basically they are for five things okay so first one is for control and timing means that they are used to coordinate the flow of the traffic between internal resources and external resources and also they are for processor communication means that everything wants to communicate with the processor either for the purpose of read write okay exchange data status reporting okay and also address recognition they will need to go through the i o module first so this one is like a guard before the boss okay and then we have the device communication so the device communication okay this is for you to communicate for the pc to communicate with the external device and then for data buffering so means that it try to temporarily hold data between the being transferred between the i o module and the external device so sometimes data processed by the cpus a lot so when you want to transfer them to the outside device it takes time because the device is occupied with something or maybe quite slow so the buffer okay the buffer of those data is located sometimes it's located into the device but okay uh there are limited space in the device so most of them they are located inside the i o module and finally the final function is for error detection so this one will detect error and report error to the processor regarding malfunction device okay problem with any external device that involved that attached to the pc so for control and timing okay so this is other step for control and timing okay like for example step one let's take a look at step one the processor interrogates the i o module to check the status of the attached device the i o module then okay returns the device status if the device is operational and ready to transmit the processor will request the transfer of data by means of a command to the i o module okay i will tell i want to get the data to the processor okay the module obtains a unit of data okay from the external device and then the data transferred from the i o module to the processor so everything when the cpu wants the data from the device okay they will send the request first basically to the your module i o module we control whether they can get the data or not okay so you will check whether the device is ready for the cpu to get the data or not so we don't want the device to give up uh to give to the cpu for the data everything not unfinished data so they make sure that the devices provide adequate data that the cpu needs for the cpu to process at the time the cpu needs okay so for processor communication okay they involve four things okay basically the first one is commands from the processor and then we have the data and then we have the status reporting and also we have the address recognition so basically for cpu communication so when the i o module communicate with the cpu okay they just communicate with exchange of command whether it's either for the cpu want to read or write the data okay and then the io module reply either busy already okay and other than that is the data and also the address of the components that within the i o module so when the cpu ask for control basically just send i want to read and then the module sent back to the cpu okay i want to i am busy okay i'm not ready i'm busy oh i am ready you can get it from me so when they say that they are ready data will be given and also address recognition will be given to the cpu okay and then we have the status reporting okay whether the device is low which means it's busy or it's ready when it finish processing and finally the address recognition okay it's a kind of uh setup as a block of one or more unit address to itself to allow referencing so means that when the cpu wants to send information to a device you need to refer which device you want to send to so basically the io module will determine what kind of address this device have like for example if you are familiar with your windows you might notice that we have what we call a com port inside your pc there are a lot of port so these are the port example of uh addressing that you have for each of your device so next we have okay we have uh for device communication okay so here specific to each device each io module able to establish communication communication invoke comma status and data so as mentioned before using these three lines and then we have data buffering so in case they are slow they will hold the data first okay before either cpu or the device can get the data and then for error detection okay they will detect male function of the device level in case of your printer you have paper jam okay in case of your hard drive you have bad distract and then you have for data errors which the automotive rating shoe will issue parity checking at the device level in case of your faulty external devices so these devices they are connected to the i o module as in this architecture here okay so all these three lines they are connected to the i o module this left three lights oh sorry this right three lines so this one okay this module uh for the peripheral module here they are representing one of this external device interface logic so they are representing into connecting to one of this okay so here data coming into either any interface logic here okay this is one example of one interface logic and from here they will go towards the iologic so iology the purpose of biologic is for it to to allow data coming in or coming out of the io module so we depend on the control lines provided by the cpu okay iology will give out the address lines of what kind of information what kind of address each of these interfaces have and from here you have a control register that provides the status of the device or even sometimes it provides the data directly when everything works fine and you want to transfer the data okay data is coming out from the data lines okay so basically this is the whole story of those io module processes and then we have the instruction okay so basically when the processor issue something to the i o module okay they will issue according to these two syntax which is the address and the io command so address mean the particular device that is connected to the pc the location of the device basically and then the io command is whether to control test read and write okay so all of this basically normally they have only read and write so and then when you want to to to read okay the module will reply busy or not so these are the common syntax that we have when the cpu issue a command to a style module so there are two types of addressing we have the memory map i o and also the isolated i o so the memory map i o basically single address phase for memory and i o device okay processor 3 the status and data register of the i o module as a memory location so it means that i o module behave exactly like another memory so they use the same instruction to access both memory and io module and for example of that so for example in this case of 10 address line you have 2 power of 10 memory location plus io address that can be supported so what is a memory map io okay so memory map i o basically instead of having special methods for accessing the values to be read or written you can just get them from memory or put them into a memory the device is connected directly to certain main memory location and two status of information okay whether it's coming in or coming out from the device whether it's a status or is a value for read or write okay so why do they use the memory map i o so memory map io makes the programming simpler because io is a part of the memory as well so it doesn't need to change much to differentiate between those two components so they don't have special commands to access the old device so however it takes some of the memory location if you compare it with the size of the main memory current memory even bit even better there are very few so it's not many because you don't have a lot of device attached to your computer at one time next we have uh tools that we have what we call isolated io so isolated io is where address space of memory and io is isolated so the command line inform whether address refers to the memory location or an io device so example they can be in case they are 10 address line it can be 2 power of 10 memory location or it can be 2 power of 10 iod address so it can be either one it cannot be combined together so differences between this kind of memory map this kind of mapping i o for isolated io they use separate memory space memory map io they use this the same memory location within the main memory isolated io okay daily me instruction that can be used those are like for example in this case in out ins and outs and then for memory map by o we have any instruction which references to memory can be used which means that this there are more universal instruction available for your devices the addresses for isolated i o devices are called ports and memory map i o devices are treated as memory location on the memory map and finally okay iorc and iowc signals expand the circuitry however for memory map io iorc and iowc signals has no functions in this case which reduce the circuitry okay so up until now is the first uh part of this chapter so i'll continue again in our next chapter later on uh thank you for staying with me hopefully you stay tuned for our next video that maybe i will post in the next one or two days later so thank you for your support stay safe stay healthy bye bye 
p_DHzCgDl2k,22,"Ù…Ù† Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø³Ø§Ù‚ ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" Ù„Ù„Ø¯ÙƒØªÙˆØ± ÙˆØ³Ø§Ù… Ø¹Ø§Ø´ÙˆØ± Ø§Ù„Ù…Ø­Ø§Ø¶Ø± ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¨ØºØ²Ø© .

Computer Architecture and Organization course by Dr. Wesam Ashour, lecturer at the Islamic university of Gaza - Palestine.

Course language: Arabic",2016-12-15T21:01:04Z,"21- Computer Architecture ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" - Chapter 5",https://i.ytimg.com/vi/p_DHzCgDl2k/hqdefault.jpg,Abdallah Safi,PT39M11S,false,2081,10,0,0,1,[Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] yeah [Music] Oh [Music] a lot of receivers receivers but in my business Oh [Music] [Music] [Music] [Music] [Music] [Music] it's cycle cycle having about history shall have psychic powers if there wasn't believed lega Tristan has about could even lead a new way [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] we [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] could signal that [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] that was Mary theater herring tilapia burger Geneva which signal to do you jo-jo [Music] 
8QkiZ82sxow,27,"lecture on Computer Architecture and Organization: GPU Part 3/3, programming reduction
at Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand on 7 Apr 2020

Materials for this class can be found at the website:

https://www.cp.eng.chula.ac.th/~prabhas//teaching/comparch/2020/index-ca.htm

All rights reserved by Chulalongkorn University",2020-04-07T08:16:57Z,"lecture on Computer Architecture and Organization: GPU Part 3/3, programming reduction",https://i.ytimg.com/vi/8QkiZ82sxow/hqdefault.jpg,prabhas chongstitvatana,PT19M21S,false,89,0,0,0,N/A,so it is the first program that I show you and the actual program has the data section as well or the syntax you can see the example from the full screen you don't when you write the assembly language of this file you don't have to put any first line here you just write like this it is a cross section and the next one is data section I think the tool is slightly different because it's developed I think five or ten years apart okay and the forum for the one who used mac system i we also i'm working on adding all the tools to macbook probably will be finished given the assignment after this you don't have to run the simulator so don't worry okay so let's welcome back the data section is just check the address and write your number you want but if you just sequentially store in starting at least address and then and it stopped and how to do we with repetition even this cpu and of course we have come jump conditional but again it is single instruction multiple data so the condition that we choose to integrate may be true for all processing because they work in lockstep so little summer piece of code show you how we look let's say we we move we want to repeat the loop in time don't we go number in in to register and then perform some action decrement at HSN and jump condition in the loop so it's it look like this normal single call no no special trick on that okay the neck is how we access and agree the first example we use absolute address okay now we want to do something a bit more involving we need to be able to index the evening and increment the index so this is instructor deduce it's quite similar single call but the destination is the local school local store is a number of the core so we have four force or is number zero one two three all right remember that these have to be serialized so you can get one piece of the main memory at the time okay so let's go to the last example the last example we'll show you two ways to program it and if you add Simon I asked you to do a easy way I think in the homework I asked you to do the difficult alright we are most familiar with its kind of thing which is sum of beginning in an array which terminate by zero okay so we kept the array and check it it is not zero then resolve it and increment the index when it is zero the result is in the Sun okay so first example we use all the process women to do the ten texts that mean we are not exploiting GPU in any way it is just family ourself with programming the GPU and again I'll just run this program so I will stop with the attention moment go back to I try to copy this program now with my property bomb I can copy it and could even take five okay let's save it happy hopper the literate I just unzip that the tool set here okay so it is ready to it and the directory is a yes so we should call it okay all right okay show you the command console assemble them nothing so there's a lot of things wrong with that some dot txt let me get them ah that's it we need data okay then let's see already okay okay you say one two three four five and then you see you it's a bit fuzzy we can say you have to have the data all right I think that's should work okay it's not the output and you can see that it will end with the data here so we similarly okay let's I accompanied is running demo with the actual code so the first tree it's just setting up the register okay we are clear and clear I could see myself it's just the result is zero so it's clear tree register and then move the base of the array which is at 100 into which is a number five okay we use all the call so it is doesn't it appear possible even at once and then we get it data to which is another zero boom all right you can see that we kept one here and you can observe that it is sequential so if it kept just one day today and the next one next one the next one we have to use also call although it seemed nonsense but remember that we have to jump and add the jump we have to test for the value of the array that is zero and all of the core have to give the same result project so we have to put there down to our car but the work that is done it's not using the parallel of positivism all right and then we move the first element into you can see that we move into them and then we have if it is eroded it somehow otherwise F so let's go all at once boom okay 100 okay I just wondering where yeah okay we did not say when we give up straw the result but the result is in in which is a number one mm-hmm so let's see the kisser okay so rich is a number one the result is 15 so 1 plus 2 plus 3 over 5 is 16 so we get the correct result please show you how to use a tool and run a simple program ok so it executes 56 instruction in 400 cycle ok so let's go back to our presentation ok we continue from here ok yes programming GPU in a simple way but not using the strength of GPU at all but but it works right so the next one is using the strength of the GPU how to use it we divided the world between four core just like this imagine the array an array and so for call take four piece different data and then we move all the car to the second piece of digits and so at one for at once for under okay so we get all these each car the p0 will have some 0 for it call one with someone five nine etc so we have the party or something again right we call s 0 to s to D and didn't store in the register of each other thinking mmm and we have to do the final bit which is a bit complicated so you have to do something out of this four piece together okay don't worry about this bit too much we come together on the first [Music] okay so this program show how to use all the processing element to compute on different piece of data using the striping that we just explained um the first part of the code get the director into our processing element so the first line set up the bit address because we lured from we address 100 and the address 100 is draw the number which is the address of the vector that we want to sum so it said that into the local store and the local store kept that into the register of every processing element next is a tricky part we store we retrieve the index initial index of that array into speech processing element one by one you have to remember that the index our fish element will start with a different index so they get a different piece of data won't we be get that which do you have to beat one by one because this getting from the main memory to local stock it had to be done sequentially when we get all of that in local saw then the Lord all at once into each possessing evening the next part is the main rule in this program I use one counter and initially the total just two because we call through the loop just twice as the length of the array is just H then our admin so one loop through is program you get for living so Tolu get all the middle each blue will not each processing element with one piece of data they all remember that they all loaded different data okay and then get that into a tree and each presentiments on their own data resident and he index now increased by four as a first-person element the first thing that is zero then next data element if index five no index for and car number one the first index is one and neck in the fifth five and then we decrement the blue count and jump okay now the difficulty now that the partial sum is in all the riches of fall okay of each element and then we this which is a six of I think of a part of each elemental to sum this person so Indian which is a six will contain the total sum so we started clearing them okay and [Music] we store partial sum to local storage again broadcast to each other without vodka ds2 which is a number five of Oh pausing Lehman so we repeat as from our four into of each pin into our five of all please okay so I put it this notation for you to see but it action so we add right and then we broadcast again to now the first car taken call that call first call and do the same so we stumble and we can store the resulting 105 this bit may not be clear to you right now you can come back and think about it and is how we straw for the easy bit address and the second Lisa the imagery do you want to treat it and the this one is a result the it what was it 100 one two three four five 105 and the like the resistor did the array of the ax actually said to be padding by zero but not the economy this example okay so that is how we can use all the 14 elemental process 
SobSaxPEOrs,27,"Broadcasted live on Twitch -- Watch live at https://www.twitch.tv/engrtoday

Part 1 of an introductory series on Computer Architecture. We will be going through the entire book in this series. Problems and examples will likely be solved in the upcoming series, Page By Page!",2018-06-16T04:04:19Z,Computer Organization and Design (RISC-V): Pt.1,https://i.ytimg.com/vi/SobSaxPEOrs/hqdefault.jpg,EngrToday,PT2H33M4S,false,6160,72,3,0,5,"alright we got some good stuff today we've got the introduction to computer architecture this is going to be a fun stream everything you could ever possibly want to know in one stream because we're gonna pick we're gonna pick between two of the most widely used books for introductory computer architecture and I figured out how to do the low latency thing so this is this is live and real-time this is the best it's going to get alright we've got two books here we've got two wonderful textbooks classics if you will both by David Patterson and John Hennessy teach you everything absolutely listen you don't get to leave the stream you don't get to leave this dream until you've learned everything I'm saying I'm laying down the law now can't leave both of abacuses absolutely that's that's peak performance how do I times to speed this live stream what's the vaad and times to listen I'm gonna put all the vod's up on YouTube and I can do that so quickly because I have 800 up but what are the numbers representing I don't know anything everything is represented by a number in a computer yeah there's no YouTube there's no there's no YouTube command yet there's nothing yet this is brand new we're fearing this out together all right we got a pic it's okay this is like a this is like a web version of the book so that's why it's 1,600 pages it's really only like 800 are these PDFs legally obtained of course why would he's illegally obtained using Apple who's using Apple I don't know who's using Apple here I'm not using Apple this is a Linux machine I mean I'm not a bun - I'm I'm literally this is live from the lab you guys are getting it live from the lab there is no fresher computer architecture than from the lab so we got to make a decision here now there's not much difference between these two books not much but there are some subtlety mainly because one of them is written completely for MIPS one of them completely for risk five wiesen is from your home because I don't get 800 up at my home I don't get 800 up at my home and I also don't have a desktop I moved up to where my university is MIPS is probably easier to begin with they're the same it's a reduced instruction set architecture they're they're both fine there's not a huge difference I mean if you're really learning like loads stores arithmetic operations I mean what's really the difference I mean this isn't like a optimization of some x86 terrible instructions that that no one what is computer architecture that's a great question yeah that is a absolutely amazing connection if I can get like let's see which one is higher Amazon reviews York doesn't matter this is Patterson and Hennessy this is the only book to care about all right so we got to make a decision right now I just came here to call you nerd that's fine I'll take it I deserve it you should put a link to your YouTube in your twitch description I need to that's right I need to do that I need to make it so that I tweet out with everything and advertise my Twitter everywhere but guess what I'm a PhD student and I'm lazy I'm lazy with this because I got other things to do I've got to fly up to New Mexico in about a month then I got to fly someplace else okay so we got the decision mips or risk what do you got face cam please don't have a camera you guys don't need a camera you guys need fullscreen architecture that's what you need don't get distracted you should also post in our learn programming I should even though this isn't programming so what do you want do we want MIPS or about risk I got these I got I have both of these open right now which are we gonna go through now I should note that this is a this isn't a web version of the book so it's exactly like the let's see my stomach hurt can you explain why No do you didn't learn enough computer architecture so your stomach started hurting that's why okay risk risk risk due risk because I learned MIPS in class okay so if you already took a class on this well it depends if you took a computer science class you probably focus more on the software side the white cover has to be better than the dark cover mm-hmm I mean if you like MIPS it is all right what's the difference between the two just the instruction set one uses MIPS one uses risk a lot of people like like risk because it's new and it's hot risk was invented by it came from Berkley I don't know where MIPS came from MIPS came from someplace okay we're gonna do we're gonna do risk because nobody said MIPS besides one person that said they already did MIPS wait no nevermind everyone said risk we're doing risk do both I mean I could teach you both instruction sets but that would be like learning C and C++ no it wouldn't all kind of there's a lot of overlap between those two we're doing risk we're going into risk do you have a French version of that stream of this stream no I don't because I only speak English and a little bit of German and a little bit of Icelandic all right we're going in we're getting this done okay so if you haven't been to the stream before welcome to the stream my name is engineering today and today we're going to learn all about computer organization and design and we're gonna learn that through risk five okay so this is a pretty good book it's good for both computer science and computer engineering people this was done like I said by David Patterson and John Hannah John Hennessy I think Patterson is the president of Stanford and I think or know it so Hennessy and Patterson are like the presidents of Stanford and another UC system and some UC system school they're pretty much the most well-known people in computer architecture I think they both won the Eckert and Mauchly award which is like the Nobel Prize for computer architecture so yeah pretty cool so let's see what the books about so we're gonna go for computer abstractions a good fundamental thing instructions language of the computer that's important if you ever wanted to know what you you know when you compile something what happens or this is a really a compiler thing but you know you know what's actually executed by the processor you know that's that's what we learned here let's see supporting procedures in computer hardware parallelism and synchronization a see sort of example put it all together arrays versus pointers pretty useful and it even covers some stuff like MIPS and x86 here's a fun here's a fun fact I'll probably say it again when we get to the x86 section but how long will this take it'll take however long almost 1,700 pages takes this is listen I need to change I need to change the title of the stream it's computer architecture architecture 101 part one that's what we're at we're at part one listen we're in here for the long haul listen I want you guys learning what do you think about J Cole last album I have no idea who J Cole is Oh fun fact about x86 did you know that x86 processors don't actually execute x86 the x86 instruction set is actually translated into a a RISC instruction set or yeah we'll the exam after assigned reading well has helped me my first uni Comp Sci it'll help you in computer science okay what what is computer architecture so computer architecture is this great level between software and then the actual physical silicon basically so here's why I like computer architecture because if you're if you're deep down they're doing stuff on silicon you don't need to care about software really you don't really need to care you just really need to care about your own stuff your silicon stuff and you need to look up maybe one level and look at architecture now if you're a software guy you don't care about silicon but you maybe care about some architecture now architects architects is really kind of in the middle we do those kind of high-level designs but we have to look at both what's possible on silicon and we need to look above us and what people are actually doing with software and we need to kind of take both into consideration now I wanted to know about RAM what is silicon though silicon is an element fun fact though a lot of chips fast integrated circuits aren't made with silicon though I think they use like gallium arsenide so like yeah gallium arsenide and what is it what's the other one there's a lot of different ways because you need a semiconductor right and so gallium arsenide works pretty well that all depends on like the energy band diagrams of the elements are the energy band diagrams of the semiconductors that's gonna that's gonna determine you know what properties that you can have from that chip so like I think gallium arsenide is used in things like night vision for night vision applications and silicon actually won't work there that's a that's a fun fact for you okay so what else do we have as I skipped over some stuff alright so after that we've got some arithmetic stuff that'll be pretty interesting learning how computers do math the processor nice we're gonna actually learn about processors some basic data paths hazards listen we're gonna figure out that it's absolutely it almost seems impossible that computers work with everything that they have to take care of it's gonna be fun the real stuff we're gonna learn about the arm cortex a53 and i7 pipelines isn't that neat we're gonna learn what's in some of your guys's computers let's see okay large and fast exploiting the memory hierarchy alright we're gonna learn about memory virtual memory basic memory topologies where I learned about cache hierarchies that's gonna be fun parallelism we're gonna learn about real stuff to arm cortex a53 and Intel Core i7 if I'm not mistaken dude does uh does Apple use the a53 what does Apple use I have no idea I'm trying to think of the reason why they would include intel core i7 and arm cortex a53 pipelines sounds like Unix I mean pipelines have a lot of different things this is a hardware pipelines pipelines is a concept this is hardware pipelines you're probably thinking of software pipelines what book is this this is computer organization and design the risk 5 version I use AMD ok md's ok they have some software problems because they don't hire developers but you know ok these a lot of the same I mean everyone really uses a lot of the same concepts and processors all of these concepts you'll learn here are you know the foundations for all that stuff what else do we have Oh parallel processors that would be fun which is GPU do any pipeline of course you can't get away if it if a GPU doesn't do pipelining that means you have a single cycle processor and we'll probably see from examples in this book that that's awful yeah GPUs you know I need to get permission to do this but listen the first ever book on GPU architecture is coming out and I have early access to it I'm gonna see what I can do I'm gonna see what strings I can pull and see if we can go over that because guess what if we do that you guys will be ahead of the game no this isn't beta testing this is just preprint this is this is it just hasn't been printed yet the book is done you know it's listen I'm not listen I'm not even gonna charge you guys I'm not even gonna charge you guys for doing that we're just gonna we're just gonna have a wonderful time and we're gonna learn a lot of great stuff and I think we're gonna have a lot of fun okay so parallel processors we're gonna learn about all of these things which most of you probably have no idea what it is sisty mindy sim d SP md and vector that's pretty cool hardware multi-threading multi-core and other  memory micro multi processes our multi processors introduction to GPUs nice you're already ahead of you are already ahead of the game listen if you can take whoa what do we got here the nvidia tesla GPU all right so you guys you guys know about you know the graphics cards that you know most like you know gamers buy and what a lot of like the general public buys right so what if I told you there's an entire class of graphics cards that don't have a graphics pipeline they're only meant for general-purpose computing these graphics cards cost 2x usually 2x plus yep GPGPU that's right they cost 2x plus and a highest in graphics card like if you try to get a so the most recent architecture is Volta if you try to get a pass so the most recent architecture is the for the for the GPU from Volta that has that has a graphics pipeline the titan v the titan phase about what's it called the titan v is about three grand right so the Pascal previous-generation the Pascal Tesla card the P 100 that's like minimum price 6500 but that stuff is expensive that stuff is good and guess what I work in a lab and all I do is work on these cards and it's so much fun okay and then okay we got some appendix stuff the basics of logic design gates and truth tables combinational logic clocks flip-flops SRAM's D Rams I read an interesting paper dealing with D Rams today that's pretty cool DG x2 easy 400k what's the GG x2 is that like one of the is that like one of the dedicated machine learning boxes where they just like shove in a bunch of GPUs like I know it GTC Taiwan they or maybe use regular GTC they release some box it has like 16 Volta cards in it and it's just for like machine learning computing graphics and computing GPUs nice this is the stuff I work on I work on GPU architecture ok somebody asked in another chat hey buddy when you mean computer architecture do you mean programming sometimes this is more architecture this is more the physical stuff we'll do some programming stuff on GPUs but a lot of this stuff will there's going to be some assembly level programming that will probably go over to understand that which is good for people that are computer science focused because I mean you compile I mean if you write C your job or something your CPU is not executing Java or C it gets compiled down into the instruction set of the processor if that makes sense ok appendix D a survey of RISC architectures for desktop server and Computers okay spark Maps alpha oh this is it ooh see this right here this one the thumb instruction set why why this is a this is an arm instruction set it has like a it has like a combination of 16-bit and 32-bit instructions why why would you do this you can also execute assembly directly and see so it might be useful yep that's true you can in fact if you actually want to if you want to if you care about high-performance computing you generally have to do things like that there's a lot of optimizations that require you to deeply understand the the assembled the the actual assembly level code and so here's a fun fact from from the GPU world I did a I did matrix mom with a text nice so here's a fun fact dealing with matrix multiplication so all the libraries for machine learning that nvidia makes to run on GPUs so we're talking like kudi and n KU blas so coupe loss is the the linear algebra library those are all hand written and hand tuned and invidious hidden instruction set architecture called sass listen if you didn't learn anything if you don't learn anything about this computer architecture you've got that fun fact but hey I hope we can learn something about instruction set architectures and computer architectures but guess what we're gonna have fun here we're gonna have fun all right so let's see in praise of computer organization design I don't care some guy from Xavier some guy from st. Lawrence mark Hill this guy's a big guy will probably read some papers from him on this stream mark Hills a big guy from wisconsin-madison I probably should know some of these other guys from Northeastern Dave Kaylee I am NOT I'm unfamiliar with his work same thing with Christos clothes Iraqis something like that guy from Syracuse not sure Mark Hill is the only person that I really know all right all right all right yep keep going dedication to Linda thanks Linda doesn't say who it's from this was written by Patterson and Hennessy who said this to be both said did they both dedicate this book to Linda are they both in love with Linda acknowledgments iFixit chip works listen you get Intel you get Intel to give you figures that's pretty nice IBM giving you figures Cray Apple that's pretty cool MIPS NASA that's pretty neat Patterson the man the myth the legend okay okay about the book that's okay risk five for this edition let's see yeah it's a moderate it seemed its instruction but it offers a simple elegant modern take on what instruction sets should look like in 2017 okay and it's not proprietary there's an open-source risk 5 simulator so if you guys so will probably I'll probably look into that I don't I haven't looked at MIPS or risk in a long time because well I do GPU research but we'll look into that we'll see what we can do we'll see what we can rustle up all right okay okay changes for the fifth edition who cares alright so if you guys actually you know want to tune in and out or you want to know you know what who's gonna get what from each chapter this you need to you guys need to take like a mental screenshot of this or you need to take an actual screenshot I would suggest an actual screenshot and so this is the this is basically what each of these sunglasses or glasses means so we got software focus and hardware focus and then some of them are both so that's both blue that means look carefully at this like look carefully look carefully for both look carefully for both but some of these are you know completely one or the other I think so like right here hazards and exceptions you know you don't really care about hazards and exceptions will this be archived on twitch right after yeah it should be I have it set up to automatically save bogs so I hope it does I'm gonna try to upload them all to my youtube and if you want to know what my youtube is I have to find it listen I can't wait till I pass like the subscriber thresholds on YouTube where I no longer have this enormous link that doesn't even say my name in it when I can get youtube.com slash channel slash engineering today that's the dream but until then this is what we have to deal with okay so let's see all right so we have this let's see instructor support I don't need instructor support I am the instrument click the bell click the bell I don't know what the Bell is I'm gonna I guess the Bell is probably like follow or something listen sis smash that's a button I would I would see just not smashing that sub button because it doesn't exist yet there is no subscribe button as of yet maybe someday maybe someday we'll be there but today is not that day we'll get there but guess what it's not about subscribers it's not about donations listen just like I said and show it at the beginning of this stream I just want you guys to learn something new listen we just we just want to learn something new today that's all we care about we just care about learning something new okay let's skip all this we got a lot of thanks look at all these people they contributed to this book that's a lot of people ok do I know any of these people maybe there's a lot of people here I don't know though mm-hmm David Wood David woods also another big guy he's a big guy in computer architecture some wisconsin-madison same place is Mark Hill okay and then the guys that wrote or that converted everything to risk five nice too many cooks nope never enough cooks all right here we go we're starting we've made it computer abstractions and technology okay so what are we gonna do in this chapter let's see let me get my cursor thing so we're gonna see we're gonna look at some information on cost performance of computers let's make that a little bit bigger let's see yep they're gonna be much better than they are today and then to participate in these Vance's computer designers and programmers must understand a wide variety of issues that's true we're gonna see that we need to really really be cognizant of current technologies where technologies are headed and that's why I think it's really important that I present some research papers on this stream because we're never gonna advanced if we don't you know look to the future we can't just you know learn what everyone before us learned we have to you know cut that off at some point and move to the future so while this chapter focus on cost performance and power it emphasizes that the best designs will strike the appropriate balance that's true so what happens if we have a memory intensive application a cash sensitive application and a compute sensitive application and we we have some processor to design now a compute sensitive application is exactly what it sounds like it puts pressure on the number of compute units you have so the more compute units the better a memory intensive one it's gonna soak up a lot of those up memory channels in a processor and then a cache sensitive one it's really gonna put the cache to the test ants might thrash the cache so better cache is better in that sense so you know we could design a computer that's really really good for cash sensitive apps we could make this giant cache we can make this a really big cache but we're not helping out through we're not helping out memory sensitive applications and we're also not helping out compute sensitive applications so that's what it means what we have to strike this balance so they're just keywords we can skip over that outline nice okay welcome to the book welcome everyone to the book book say hello Chad say hello to the book okay so let's get right into it let's see if there's anything interesting here so yep we're an economy that has become dependent in part on the rapid improvements in information technology promised by Moore's Law and we'll go into Moore's law in a minute this unusual industry embraces innovation a breathtaking rate and we're gonna see that with Moore's Law how amazing that computers are one of my professors had this great example and I can't remember the exact figures of it but it was comparing the automotive industry to Moore's Law and if if the automotive industry had scaled the way Moore's law scaled we'd be driving cars that cost pennies that get you know hundreds of thousands or millions of miles per gallon you know that's the kind of scaling we're talking about an innovation that we've seen with computers over the last you know since since the 1940s really when when this came about and more more along the lines of when the first transistor came about so let's see the race to innovate has led to unprecedented progress since the inception of electronic computing in the late 40s this was the paper we read from john von neumann talking about edvac the first stored can stored program computer that was designed and built by Eckert and Mauchly here we go had the transportation industry kept pace with the computer industry for example we could travel from New York to London in a second for a penny now you know just let that sink in for a little bit that's the level of improvement we're talking about this is why it's such a great time to get into computer architecture it's so fast-moving there's so much innovation going on you know this is why I love it all right so let's see take just a moment to contemplate how such an improvement would change society living in Tahiti while working in San Francisco going to Moscow for an evening at the Bolshoi Ballet and you can appreciate the implications of such a change right think about it think about when computers became a household item think even more recently for smartphones look how fast smartphones became everyone has a smartphone now or at least that's the impression it's such a common thing now which didn't exist early 2000s no one had one your fridge is smart that's right that's right Robbie it is smart it probably has you know connects to the Bluetooth my phone has a gaming mode absolutely there's all kinds of crazy stuff that happens now and its own it's just started existing because we can make transistors at you know production level we can make it at 10 nanometers now you know they can even make smaller transistors not a production scales though I think they've they've made seven or maybe even five but not in a production scale like you're not gonna so I think the Nvidia Volta GPU has like 21 billion transistors or something you're not making a 21 billion transistors chip in 5 nanometer tech yet your yield would be zero we get no chips out of production ok computers have led to a third revolution for civilization with the information revolution taking its place alongside the agriculture and industrial revolutions this is true see how fast things have moved as far you know data science how many job titles you see of data science nowadays a lot there's a lot of data scientists out there now ok let's see it so another amazing thing so it really enables a lot of fields astronomy biology chemistry and physics you know these only exist a lot of the advancement in these fields are only possible because computing has got to where it is there's a lot of problems that were previously unsolvable that because it can because of compute were able to you know if if it was too hard to come up with a clever solution we can just iterate through all possible solutions or all all possible inputs for something and find the solution that way that's the point that we're at MRI is pretty dependent on computing - I think listen anything anything that has a circuit board in it is dependent on computing because that motherboard was designed with the computer so let's see the computer revolution can continues each time the cost of computing improves by another factor of 10 the opportunities for computers multiply applications that were economically infeasible suddenly become more practical so machine learning is a very interesting one to consider here so a lot of people don't know this but machine learning it actually came into existence I think in the 40s or 50s right 1940 1950 that's when machine learning came to fruition and existed and then it died away and then I think in the 70s or 80s there was another revolution where there was more work into machine learning and then after that today today is the third and this one's here to stay because we finally have the compute to make machine learning viable ok computers and automobiles until microprocessors improved dramatically Reisen performance in the early 1980s computer control of cars was ludicrous that's right way too big way too power hungry today today computers reduce pollution improved fuel efficiency via engine controls and increased safety through blind spot warnings anything most electronics or all electronics are controlled by some brain within the car cell phones the Human Genome Project that you know it was a neat idea but it didn't end up being as revolutionary as people thought I don't think the World Wide Web yep the Internet is only possible because of computing and search engines listen everyone likes Google everyone likes how you can you know search up anything and often time and Google presents you exactly what you're looking for that's great so let's see traditional classes of computing applications and their characteristics although a common set of hardware technologies is used in computer computers ranging from smart home appliances to cell phones to the largest supercomputers these different applications have distinct design requirements and employ the core hardware technologies in different ways broadly speaking computers are used in three dissimilar classes of applications okay so we have personal computers which is something that everyone is probably familiar with we have servers and so let's see let's see what they say about PCs first so as we all probably know they emphasize delivery of good performance to single users at low cost so it's not going to be your highest performance but it's going to be you know some of the most cost efficient performance because it's meant for the general market this class of computing drove the evolution of many computing technologies which is nearly 35 years old yep that's true so I guess it's been around 35 years now since the PC revolution you know really started to take over so that's pretty cool I like that so let's see servers so most people are familiar with servers as well so they're the modern form of what we're Munch what were once much larger computers and usually accessed only via a network so you know Amazon for instance with cloud services you know they rely on servers Microsoft Azure they rely on servers so let's see do they say anything interesting here servers are built from the same basic technology as desktop computers but provide for greater computing storage input/output capacity in general servers also plays a higher emphasis on dependability so when we get into servers later in the I don't know thing over in this book but in the next computer architecture book that we're going to go over we'll probably talk about raid which stands for redundant array of independent disks and we'll learn about you know how exactly servers provide such dependable dependability and fault tolerance that's a really interesting area I'm actually getting into the fault tolerant computing world a little bit in my research I'm at least looking into it for GPUs it's a very interesting field okay and then we have okay so we're still in servers so supercomputers are you know another form of servers tied together processors so they have many terabytes of memory and you know petabytes probably yottabytes of storage because we want to store everything nowadays you know every everything you know just save it all I mean the most amazing example is something like twitch or YouTube we're just they will just take your video and they will just store it and you know I recorded a video the other day about 45 minutes and it by itself was 2.1 gigs and that has to go someplace to get stored and YouTube's cloud and they just do it for free so it's truly an amazing amazing time so here we have a little bit on kind of the number systems that we're gonna see so there's often the confusion of these two number systems because sometimes people we use them yeah of course it's compressed yeah of course but if you think about the amount of videos it gets stored it's still an insane quantity so you know when generally we're talking about binary we do 2 to the power of 10 so 1 if someone says 1k or 2k or they say a Meg or a gig most of the time they're probably referring to the base to number they're probably referring to 1k is generally 2 to the 10th which is 1024 or if yeah so it generally will people try to use these interchangeably but most of the time when we're doing sizing we're using the base to number okay so let's see so then we have the last one which is the most low-cost which are embedded computers these are the things that are everywhere there are micro processors everything that makes IOT possible basically so everything that has a little little bit of chip in it that controls something these are micro processors embedded computing if you have a SmartWatch or something you know these are all embedded systems okay so let's see anything else interesting care despite their low cost embedded computers often have lower tolerance for failure since the results can vary from upsetting to devastating so if you think about something like a pacemaker right so if your computer messes up okay that's bad we don't want that we might lose some data that's pretty bad if a pacemaker messes up that's a little bit worse someone's life hangs in the balance or some medical equipment that they use embedded computers or a little micro processors you know we don't want those messing up okay so let's see welcome to the post PC era this is an interesting thought so the continuing march of technology brings about generational changes in computer hardware that shake up the entire information technology industry okay replacing the PC is the personal mobile device most people don't need a computer anymore so they have a computer at work maybe if or maybe they maybe they don't if you have a server it's very likely that you don't need a computer for a lot of things and most people what they use technology for is personal media though or they use it for personal entertainment purposes so social media Netflix YouTube twitch etc etc so you don't really need a you don't really need a PC for that anymore something like an iPad or a tablet or a smartphone will work just as well so you know we're in this age now where you know the markets are shifting a lot and one of the things that a lot of manufacturers of PCs are dealing with right now is that personal computer sales are they're drastically decreasing and companies like HP and Dell and Lenovo they don't make anything off of Hardware nobody makes anything off of hardware if anyone thinks that some company is getting rich off of selling laptops they they're not nobody makes money off of Hardware what they make money off of is services to support that hardware they make all their money on the warranties or the service support packages which is mainly from selling things like servers you don't make money on the hardware from selling servers to companies or to data centers you make money on selling the three-year five-year support plan for the servers that you sold them that's where all the money comes in from ok so here we have a pretty interesting graph so this goes up to I guess 2012 and so we see that you know despite what some people think non smartphones a lot of money is still being shoved into phones that are not smartphones which are you know they're just about a billion dollars now decreasing but still just about dollars smartphones increasing just past 600 million in 2012 ish PC is not including tablets very slow rise tablets are increasing not as high as smartphones but looks about two to three times faster or maybe about 2 times faster than pcs so that's fairly interesting I had no reference really for tablets ok yeah pcs isn't declining but it's definitely not growing very quickly and it's even one could argue that it seems like it's it's at least slowed down from this 2009-2010 upward trend ok taking over from conventional server is cloud computing yep this relies upon giant data centers they're now known as warehouse scale computers so Amazon Google build these warehouse scale computers containing 100,000 servers very interesting stuff things like software as a service so you basically build the software and instead of companies having to design their own software for a specific task some place will just say hey we'll sell you a template of ours to use for for you to use so that's pretty interesting today's software developers often have a portion of their application that runs on the I think this is personal mobile device and a portion that runs in the cloud that just makes sense so this is kind of where the markets going as we see from here with smartphones so you know smartphones are still very limited in size and you you know there's so many things that go into designing a smartphone you have to consider your battery size you have to consider the performance you're going to have and quality of service for the number of apps you're running and you have to consider things like you know even thermal is so important you don't want your phone to get burning hot so that's very so let's see what can you learn from this book or what you can learn from this book chat this is you this is what you're gonna be learning from this book successful programmers listen if you're a programmer this is it this is what we've been waiting for we're here now we're here so let's see what are we actually gonna learn from this book so 60s and 70s constraint and computer computer performance was the size of computers memory so what we wanted to do back then was minimize memory space to make programs fast in the last decade advances in computer design and memory technology have greatly reduced the importance of small memory size in most applications other than those in embedded computing systems ok ok here we go here's what we're actually going to learn these bullet points so one how how our programs written in a high-level language and how does the hardware execute the resulting program so this is going to go from that high-level abstraction and then we're going to see what happens on the instruction side I can finally unroll my loops absolutely doesn't MPI just have a can't you just do like pragma unroll and it automatically does loop unrolling for you I'm pretty sure you can just do that also most compilers probably do that for you automatically if the compiler is smart it should okay so let's see next thing we're gonna learn the interface between the software and the hardware okay pretty interesting we're gonna learn what determines the performance of a program and how can a programmer improve the performance ah this is gonna be a very interesting part how does knowing the architecture of the platform you're working on influenced your how you write your code that's gonna be very interesting that's something that interests me a lot it's very relevant in the GPU space let's see we're gonna learn what techniques can be used by hardware designers to improve performance so we're gonna learn on the hardware side how we can improve this and then this is the advanced book this is the book we're going to do after this book computer architecture a quantitative approach this is where you really start to learn about modern computer architecture it's very interesting then we're also gonna learn about energy efficiency we're gonna see why that's so important especially in the modern era with transistors being so small now we're gonna see the shift from sequential processing to parallel processing with multi-core processors if we're going to see what impacts that has on the architecture and why this seems like magic once you start to learn how this works things like consistency in memory it's you know when I first learn this I all I could think to myself was how does any of this actually work it it doesn't seem like it should be possible okay and then we're going to see since 1951 what great ideas did computer architects come up with and lay the foundation foundations of modern computing so we're gonna we're gonna really see the history of computer architecture and the logical steps that took us from you know where we were with edvac back in 1945 1946 when it was completed to today where we have these massive chips with billions of transistors that can crunch data faster than somebody in 1945 would have ever thought was possible it's gonna be really really great all right so let's continue on so okay so they're gonna go over a couple things so there's a lot of words that we'll go over and we want to make sure that everyone understands what these means what these mean so BIOS CPU dim DRAM PCIe SATA I actually took an entire course it was on a protocol like SATA it's called sat or yeah sass it's what data centers data center hard drives so here's this here's something brief side note the data centers don't use SATA drives because SATA drives are awful and they have no redundancy so usually the most cost efficient balance between performance and cost is something known as sass which stands for serial Attached scuzzy and scuzzy stands for what is its SC si I think it's like serial computer systems interface basically it's you know a dual ported drive that has some failover okay so they even decided they even define what an acronym is isn't that nice okay so let's start out with talking a little bit about program performance yep nested nested acronyms listen ever there's so many acronyms that are horrible I think sass is one of the worst because it's an acronym within an acronym let's see so you can't get this book anywhere for now no that's not true you can get this book everywhere right now the GPU architecture book that I was referring to isn't out yet for publishing it doesn't exist I have a pre-release PDF of it that has a password I think it's getting published sometime this year but it's definitely not out for sale yet okay leak it no I'm not going to leak it I know some of the people that wrote the book I'm not gonna leak the book how do you think I got access to the book I did just contact the publisher and ask if they have any books that have been released yet and they gave them to me understanding program performance the performance of program depends on a combination of the effectiveness of the algorithm use in the program the software systems used to create and translate the program into a machine instructions and the effectiveness of the computer in executing those instructions which may include input/output IO so yeah so let's talk about this a little bit so be heavy algorithm so the algorithm is going to be what you know a computer scientist has come up with this is going to be some or maybe even a mathematician has come up with and it'll be a way to solve a problem in a it's a programmatic way to solve a problem and then we have after that we have the programming language compiler and architecture now it may seem a little confusing right now of why we have architecture and compiler in the same in the cell you know I'm the same in the same box but it's really impossible to have these two separated because what happens is you write a program it goes to the compiler all what the compiler does is the compiler will translate that language that you wrote the program in so like C and it will change that into the underlying architecture so if you're on an x86 processor it's going to translate it into the x86 instruction set and it's going to do whatever optimation czar built into the compiler and there's a lot of crazy stuff that compiler people have worked on to make that really really good so then after programming languages and compilers there's of course the processor and memory system so this is when your code is running on the processor that's where we are at this point and then following that here let me go ahead and full screen is I don't know why I haven't full screen done so following that is of course the i/o system hardware and the operating system so this is a little more abstract this is going to be you know the human interaction or maybe some other devices interaction with your computing system and it's also how the software is interacting with this computing system so yeah this is going to determine how fast IO IO operations may be executed so yeah so we're not going to cover algorithms in this book there's a great book by cormen leiserson rivest and stein called introductions to algorithms maybe we'll go over that that'll be interesting for the computer architects and computer science people alike maybe we'll go over that sometime so to demonstrate the impact of the ideas of this book we improve performance of a C program that multiplies the matrix times a vector in a C in a sequence of chapters a matrix vector of multiplication nice so we're going to talk about data level parallelism and using sub word parallelism via C intrinsics we can get the performance over the baseline to improve by 3.8 X that's pretty good not the best but still pretty good so when we do instruction level parallelism or ILP that's when we'll do loop unrolling to exploit multiple instruction issue and out of order execution hardware if you guys didn't know meltdown the security issue is a result of out of order execution hardware and will improve it by another factor of 2.3 so keep in mind this isn't three point eight or two point three this is three point eight and two point three in the category of memory hierarchy up optimization so this is just looking at the memory subsystem we're gonna use cache blocking I think this is the same I think cache blocking just means the same thing as tiling and we'll go into what that is to increase performance on large matrices by another a factor 2 to 2.5 and then finally we're going to use in this same level as our so we're gonna use thread level parallelism or TL TL P we can use parallel loops and OpenMP so I would prefer CUDA to exploit multi-core hardware to improve performance by another factor of 4 to 14 so we're getting this huge huge jump in performance here over time so this is gonna be really interesting to see because we're doing the same application for all of these so let's see so check yourself sections are designed to help readers assess whether they comprehend it the major concepts introduced in chapter so the number of meta processors sold every year greatly outnumber us and of our PC or even the post PC processors can you confirm or deny this insight based on your own experience alright Jeff this is your first question and there are no wrong answers here except their ass so can you confirm a deny it listen I'll say that I can confirm it you know you see so many people now with smartphones and you know many people are you know I think there was the what was that Apple commercial that a bunch of people got mad about because you know had the girl in it that you know somebody asked you know is that a computer and she said what's a computer right there's that commercial that everyone got angry about but you know in some sense it's true and we actually have some data in this chapter that proves it right so you know we've got some we've got some interesting that's an interesting point that's being made here how does it compare let's see how try to count the number of embedded processors in your home listen like all good research papers I leave this as an exercise to the audience okay so let's move on to the second one as mentioned earlier both software and hardware affect performance of a program can you think of examples where each of the following is the right place to look for a performance bottleneck okay all right the algorithm chosen can you think of examples where each of the following is the right place to look for performance bottleneck okay so clearly the algorithm chosen can be an issue so this is there's a very so every algorithm has a different cost related to it and they have different orders of performance one of the easiest ways to think about this time complexity is a great you know it's a great way to think about and the algorithm chosen but you know there's there's a subtle thing if you want to think more like an architect when you're thinking about the algorithm chosen we need to also think about the platform that we're working on so if we're working on a CPU we have to keep in mind that CPUs are you know they're not very parallel compared to a GPU so when we're optimizing an algorithm when we're optimizing an algorithm to run in a CPU a lot of times we're optimizing its serial performance and when we do that often times we wind up with an algorithm that is doesn't work as well with we all wind up with an algorithm that doesn't work as well run a GPU in fact I'm trying to remember exactly so there's a there's iterative so iterative methods for solving matrices right for so for solving systems of equations so there was a this very classical way of solving systems of equations iteratively that was all but forgotten for the longest time and you know the CPU community worked for a long time and created a great serial solution to the problem and I'm gonna see if I can find that real quick but one of the interesting things is now with the mass the mass adoption of GPUs for general-purpose programming that old application that wasn't as good serially turns out the serial one you can't parallel eyes but the old one that at worst performance on CPUs is it's a term that a lot of GPU people like to use embarrassingly parallel so let's see iterative method solve linear equations let's see if I can find what this was I won't look too hard if we can't find this quickly it'll be fine actually I know exactly where to look I can look on this site I can see if it's still here okay it's still here here we go so the Jacobi iterative method the Jacobi iterative method was a means a long time ago of solving systems of linear equations iterative iterative Li however it got replaced by another let me see if I let me pull it up on stream so okay here we go so there's two different order of iterative methods here so there's of course the Jacobi iterative method and so that's this one right here and this is iterative method that you can make extraordinarily parallel and it works absolutely wonderfully on GPUs the Gauss Seidel or Gauss Seidel method I don't know how to pronounce that that method is not parallelizable at all it's great for serial performance and so it works great on CPUs but it doesn't work great on GPUs so there's a quick example for you of the algorithm and I spent way too much time on that so the programming language or the compiler so of course depending on your application you really need to look at the compiler for you know what you're doing and you need to look at the optimizations that your compiler is doing maybe even look at the assembly itself the operating system so there's going to be some operating system overhead there's a lot of research papers that go into this and understanding how something like the tcp/ip stack actually you know incredibly can get in the way of communication or did it can really slow down a make communication overhead a bottleneck for NIC processing I think this entire paper that I read on that a while ago I don't do a lot of NIC stuff so I don't remember it but the processor so here's a fun fact if we go back to the Pentium days and we look at something like if we look at Pentium three versus four so Pentium three I think was 750 megahertz and then Pentium 4 was like 1 gigahertz right so you would think ah Pentium 4 wins yay wrong Pentium 4 was slower than Pentium 3 because Pentium 4 use something ridiculous like 1224 stage pipeline while the Pentium 3 only had like a 10 stage or 12 stage pipeline and pipelines have diminishing returns so they added so much overhead here that even though there had a 1 gigahertz clock the 750 megahertz processor was better and in the IO system and devices so clearly if we have some IO device that's very slow we can see that you know it could cause a bottleneck so you can think about a USB Drive so if we have a USB Drive and we're trying to copy data clearly if we get a better USB Drive that has a that you know has a high it can take a higher bandwidth that has a higher it's a better buffer inside of it for the data transfer you know it'll be it'll be better now there's also the limitation of USB and whether or not it's the actual USB interface that is the limit but yeah it's a it's it's very interesting ok any questions on any of this stuff so far or elsewill kit and we'll continue going to the eight or eight great ideas in computer architecture SATA vs m - what's him - I know it's SATA is SATA versus M - I'm gonna look at them what's him - is that a I mean I get I'm guessing by the interface let's see are all M two drives in via me no M 2 is just a form factor so that I don't think that's a very good comparison because this is just from what I'm looking up but it says in 2 as a form factor it's not a protocol sad-ass a protocol ok nvme is also not in via me is also not a protocol nvme is a is non-volatile memory so it would be how it's connected so if you want to make the comparison it would be something like SATA versus PCIe versus USB versus SAS versus scuzzy all of those are communication protocols but it's you can't compare and you can't get better something again via me which is you know a type of memory versus a protocol okay so moving on eight great ideas in computer architecture so we now introduce eight great ideas that computer architects have invented in the last 60 years of computer design these ideas are so powerful that they have lasted long after the first computer probably referring to edvac these great ideas are themes that we that we will weave through this and subsequent chapters as examples arise okay so we're gonna go over eight themes and it's going to be you know exactly you know how most computer architects think and what we keep in mind when we look at new ideas and maybe I'll try to impress this more when I go over something like a research paper okay so let's see number one design from oops that's not right let's give it a highlighter okay so one designed for Moore's Law so the one constant for computer designs is rapid change which is driven largely by Moore's law it states that integrated circuit resources double every 18 to 24 months and I don't think it says it here but I think it also price decreases by 1/2 I think that's the whole idea so this is a very interesting thing so let's just take a second to think about what this means so this means that we get twice as many transistors to work with every two years now that's crazy so when we talked about you know going from I think it was like California to New York in a second for a penny this is why we've had such amazing change it's all because of Moore's law and keep in mind Moore's law isn't like a strictly defined equation this is just a trend or a pattern in computer architecture that just happens to have persisted over the past you know 40 years or so um it resulted from a 1965 prediction by Gordon Moore one of the founders of Intel and I think before that he was at Shockley labs if you know anything about Electrical Engineering you probably know that name from the shock of Shockley diode so the resources available per chip per chip can easily double or quadruple between the start and finish of a project like a skeet shooter computer architects must anticipate where the technology will be when the design finishes up rather than where it starts okay so this is a very interesting concept because from here we can understand that you know we have to plan for the future and that future comes very quickly and so we have to make designs that are one of the most important things making designs that are scalable we need to design that not only works at the current technological generation but we need a design that can span multiple generations okay any questions on Moore's law before we move on also I think this is a good time take a little let's see oh there we go we got something isn't Moore's law falling off absolutely Moore's law is falling off so well they use this way to see that is with frequency scaling so we used to see in the same thing as the doubling of number of resources we also used to see doubling of frequency for generation but now we've noticed we've saturated around three and a half four gigahertz and it's likely that we won't be able to get much more just on in frequency wise frequency scaling wise because of that because of physical constraints so a lot of people are predicting that five nanometer is as small as we can possibly get seven nanometer is something that's being worked on right now right now between I know Xilinx and Intel are working on that together right now the path to seven nanometer technology and even Intel itself is having trouble getting to producing large chips at ten nanometer they're not getting the yields that they would like a 10 nanometer I think even the Volta architecture I think Volta was originally planned to be 10 nanometer technology they didn't quite get there I think it's I think it's 14 nanometer technology okay so moving on let's see let's see what we got so using abstraction to simplify design so this is our next key concept both computer architects and programmers had to invent techniques to make themselves more productive for otherwise designed taught or for otherwise design time with lengthen as dramatically or as dramatically as resources grew by Moore's law so we need ways of designing things where we don't get stuck in the weeds we need to be able to see the path forward and you know hide some of the low-level nitty-gritty details that would otherwise get in the way and so we do that by abstractions to characterize the design at different levels so you know if you're a can't Canyon like I don't know what generation I don't I don't remember what all the generations are is it coffee like the newest architecture I think coffee like is the newest okay we'll use abstract painting icon so this is this is going to be when we're talking about abstraction you're gonna see this little graph in the book when it's something related to Moore's law so make the common case fast this so this is a dolls law and we'll go into on those all later so we can think of it this way this is the easiest way or one of the easiest ways if we have a car and somebody says if we put the best wheels on the car we're going to go five miles an hour faster what if someone says okay if we put a new engine in the car we'll go 70 miles an hour faster which do you think is more important if your only concern is speed which in many cases where architecture it is it's probably you're probably gonna care about the engine more so in this case we want to optimize for the common case which is going to be not only which part of the architecture is being used the most but also something such as what is the program doing so you know what so those are kind of tied together right so if the program is very memory intensive that means we're gonna be spending a lot of time in the memory subsystem so we should probably focus on making the memory subsystem fast so we need to find out what the general trend is and we need to optimize it and then once we've done a lot of optimization and we're running out of design space to look at we can come back to some of the other way so after we put the new car engine in and maybe we've you know put a lightweight frame on the car then we might go okay so what can I still do to make my car faster and then we you know we put on the tires the new tires they'll make us go five miles an hour faster so this common sense advice implies that you know what the common case is so this is often true because we can find out the common case through benchmarking and so that's what they mean by careful experimentation and measurement we oh so they actually use a sports car okay so that they use a sports car so they're trying to make the same analogy that I was that's interesting I promise I didn't steal it from them I've never read this book I used a different book when I learned computer architecture so this is a new experience for me reading this so performance via parallelism since the dawn of computing computer architects have offered designs that will get more performance by computing operations in parallel we'll see many examples of parallelism in this book we use multiple jet engines of a plane as our icon for parallel performance this is the worst picture I would have picked for parallelism really the best thing you'd come up with is an airplane what no I don't believe this they use an entire airplane because of engines what doesn't make any sense this makes sense making the common case fast you got a speedy racecar this makes sense it's abstract art abstraction Moore's law it's doubling so you've got a linear graph why would you use okay we're gonna move on from this this is that's slightly frustrating that's okay okay so performance via pipelining I bet all the other ones make sense except for parallelism okay so performance to be a pipelining a particular pattern of parallelism is so prevalent in computer architecture that it merits its own name pipelining okay for example before fire engines a bucket brigade would respond to a fire which many cowboy movies show in response to a dastardly act by the villain sound okay so a bunch of people carrying water to the source of the fire passing buckets down as they could much more quickly move buckets up the chain instead of individuals running back and forth so that's what I mean by pipelining we mean that every single step of a processor is going to be doing a different thing and they're not going to be relying you know on the previous step or the next step no that's not entirely true because you're going to have dependencies where things are going to rely on different stages but we're gonna see that there's a lot of ways to overcome that through things like instruction reordering and the worst case we're going to have to stall the processor but a lot of times we can come around oh we can get past some of those issues so performance via prediction following the saying that it can be better to ask for forgiveness than ask for permission so if you're a Python programmer you know that pretty well the next idea is prediction in some cases it can be faster on average to guess and start working rather than wait until you know for sure so here's a fun fact so you know how we talk to it earlier that out of order execution is the reason why meltdown exists the meltdown security vulnerability so prediction and speculative execution is the reason why Spectre that vulnerability exists so there's another fun fact for you if you didn't already know that assuming this is of course assuming that the mechanism to recover from miss prediction is not too expensive and your prediction is relatively accurate so this means that okay we need to be able to predict things fairly well and then when we have to rollback if we did something wrong it has to not cause too many problems and look prediction it's a crystal ball that makes sense so hierarchies of memory okay so let's see programmers want the memory to be fast large and cheap his memory speeds often shaped performance and we'll see that memory scaling has been horrible compared to performance wise compared to you know the performance of frequency of a processor architects have found ways they can address these conflicting demands with hierarchy of memories that means we have memories that handle different tasks so we have you know parts of memory that are geared more towards very high-risk very fast response and then we have other parts of memory that deal war more with you know massive or that with more bandwidth concerns so sometimes we just want something quickly sometimes we want to make sure that you know a lot of things can be captured so as we'll see in Chapter five caches give the program of the illusion that main memory is almost as fast as the top of the hierarchy and nearly as big and cheap as the bottom of the hierarchy and so they use this triangle because you know most of the time when we're talking about caches we're referring to this top as being the out one so oftentimes is written with a dollar sign so this will be the l2 cache and this will either be the l3 cache or the last level cache okay so what's next dependable dependability via redundancy so this is going to be expecially important when we're talking about servers but it's also important with just computers themselves because memory has issues you have to refresh memory if you store something in memory it can't just stay in there forever after it's been written once it's going to have to be refreshed at some point or you're going to lose that data and that's part of fault tolerant design is that you prevent things like crosstalk where there's a coupling capacitive coupling between physical locations in memory that can cause you know bits to change which is very bad so computers need not only need to be fast and you can be dependable since the physical device can fail we make systems dependable by including redundant components so this is important that can take over when a failure occurs and help detect failures so things like exceptions we use the tractor-trailer as our icon since the dual tires on each side of their rear axles allow the truck to continue driving even when one tire fails so this makes way more sense in the airplane still I'm still angry about the airplane okay so okay so now we've got what our main core ideas for architecture going to be it's going to be designing based on Moore's law it's going to be looking at abstraction to give us you know ways of breaking down ways of breaking down different levels of design into manageable bite-sized pieces we're gonna try to focus on the most important parts of making good architecture first before we do the minor parallel are the minor optimizations we're going to find ways to exploit all the parallelism we have to increase performance we're going to use pipelining to make sure that everyone's doing something at the same time and then we're going to use prediction in order to compute things ahead of time so that we don't have to we're not waiting around for data and then we're also going to focus on having hierarchies of memory in order to fulfill this need of fast large and cheap memory spaces that programmers want ever so much and then we're going to focus also on dependability via redundancy so that if a problem happens in the processor during execution we don't lose everything that we're working on okay one second to respond to a message okay and then back how is everyone doing everyone's still doing well how's that when doing this lovely Friday evening or Friday morning I don't know where everyone's from how's everyone doing today livestream sniper pretty good that's pretty good 2 a.m. here oh man that is late does that mean UK ish area it would be Saturday morning all right Saturday nice see it's 9 P I'm in my lab at 9 p.m. we're having a grand old time left okay yes UK very nice I I had something accepted into a conference that was in Belfast a couple months ago that was pretty neat 3 a.m. alright nice that's gonna be a little further over that's gonna be what gmt -1 okay so what everyone have weekend plans they don't have anything fun going on this weekend ok Denmark gotcha very nice I've always wanted to go there I was thinking about going there for my honeymoon either Denmark or probably make a first stop in Iceland because it's so cheap to go there so what is everyone doing this Saturday or Friday evening besides learning everything that there is to know about computer architecture that's always fun you know that's why I'm that's why I'm in my lab on a Friday night seeing it home all day hey nothing wrong with that I've had those days see I can't I can't say that you know I'm not I can't I can't say that you know I'm doing something else because I'm literally just sitting in my lab all day basically what happens is they work out at home in the mornings and then I sit in my lab and then I go home at around midnight that's that's my life listen I'm not complaining I love everything that I do and getting able to being able to teach stuff like this it's a lot of fun I enjoy it okay okay we can go ahead and do you want them so let's see what else we have what else we working with today so below your program a typical application such as a word processor or large database system may consist of millions of lines of code and rely on sophisticated software libraries that implement complex functions in in support of the application as we will see the hardware in a computer can only execute extremely simple low-level instructions to go from a complex application to this to the primitive instructions involves several layers of software that interpret or translate high-level operations and do simple computer instructions an example of the great idea of abstraction so with something like that we're talking about compilers compilers and the instruction set architecture ok so moving on so figure 1.3 I guess we'll see it down here shows these layers of software organized Pro ok we can just get this so this is basically what we're talking about the different levels of abstraction that we have so we have you know at this very high level we have the application software so this is whatever app you're going to be running on your computer when you write an app as you know I'm sure some people in here that our computer science focused have done before you don't care really about what's going on at the hardware level you can really just kind of worry about okay where's my hooks or where do i plug in to the operating system so then that leads us to system software so system software once we start getting more into things like operating systems we have to kind of look above us they have to look above themselves and go ok so what are these applications that are surrounding me trying to do how do they hook in to me and then what are they trying to accomplish and so with this they have to have system software itself has to have hooks into the hardware they have to know how to speak the language of the hardware in order to calculate things and get those results back and push them back to application software so of course it's a simplified view you could you know throw in some extra layers here and there depending on you know what your focus is but in general this is this is about as much abstraction as you probably need so among the most important tab so an operating system interfaces between a user's program and the hardware so like I said we're talking about operating systems at this level and then this is some application and then of course this is just the hardware so examples of operating systems so Linux iOS and Windows okay so we know what operating systems are so compilers perform another vital function the translation of a program written in a high-level language such as C and C++ Java or o Visual Basic why stop into instructions that the hardware can execute listen I have to ask a question has anyone here ever use Visual Basic I've used Visual Basic before has anyone here ever used it okay as I MOS you've used it please tell me is it not the worst thing in the world the fact that if you write something that's seen tactically incorrect it immediately creates a pop-up telling you what's wrong why does it do that you can't okay Cameron wouldn't use it okay listen I I I did an internship and I was automated a bunch of stuff there and they had a bunch of Excel spreadsheet stuff and so I wrote some VBA I couldn't take it I wolf I did not have fun it was not fun but it's okay this is this is an engineering today promise this is even a why so green promise we will never look at VBA on this stream unless somehow I get tricked to buy it popping up in the book and even then I will delete that page from this print out version of the book that's a that's an engineering today promise okay so let's move on so from high-level language to the language of hardware okay to speak directly to electronic hardware you need to send electrical signals so electrical signals we mean on and off ones and zeros we mean of course binary numbers and if you caught any of the stream yesterday we talked about number of systems for a bit so we refer to each letter as a binary digit or bit usually we just call it a bit computers are slaves to our commands which are called instructions okay that's an interesting way to phrase that you could have just said that computers only execute the instructions we give it that works just as well I don't know why you need to get so radical by calling computers slaves instructions which are just collections of bits so something like this could be an instruction and we'll see how we actually encode assembly or we can encode an assembly into instructions and that assembly actually has set patterns that are predefined to translate from you know English words into binary digits okay okay so this is pretty simple so then we can talk about it assembler so the first program is communicated to computers and binary numbers but this was so tedious that they quickly invented new notations they were closer to the way humans think so of course you know initially we had you know only binary and then after we had binary we had assembly and then after assembly we had higher-level languages like C and Fortran and then even later we have things like Python write or scratch I've never used scratch and I will never use scratch probably but that's that's a funny thing to think about okay so the lowest level that we didn't really care about today is assembly we don't need to ever code anything just in binary sometimes we need to edit binary files but usually we can get away with just editing some assembly okay so let's see so using the computer to help program the or using the computer to help program the computer the Pioneers invented software to translate from symbolic notation to binary so that's what an assembler does so when you write down an assembly code you give that to an assembler that assembler is actually what turns it into binary so it translates you know the language that you write into binary just like how a compiler will translate your high-level code such as C C++ into assembly but then the assembler has to take it to binary okay the instruction tells the computer to add two numbers a and B the name coined for this symbolic language is assembly language okay and then when we're talking about these binary strings up here we're gonna call that machine language so that's pretty easy to understand so we assemble a assembly language into machine language so although a tremendous improvement assembly language is still far from the notations so that's where we get into high-level programming languages like we talked about and this is abstraction this is that key concept that we were talking about earlier Hey look there's a pretty picture of abstraction so a portable language like C C++ Java not this visual basic we're not going to talk about visual basic that is composed of words in algebraic notation that can be translated by a compiler into assembly language okay pretty cool pretty cool so here we go we've got C we've got an equation for swapping and then this is given to a compiler this is turned into an assembly language so it's going to have some function called swap still and the way that this will work in assembly is that when swap needs to be called there'll be a branch someplace where it'll tell the program counter to jump to this function or branch to this function and then we'll get here and then it will do the same operations here and so here we're doing some loads and we're doing some stores and we doing an ADD and then we have this jlr and we'll go into what these mean later so this is going to be jump and Link return okay and so we'll go into what jumping and linking when you return means later and then the assembler is going to take this and put it into a bunch of binary bits that the computer will label be able to understand okay so you know this is a real world example and it's pretty neat to see so let's see so basically what we're going is from a plus B to something that's in assembly such as add a comma B and then we'd turn that into ones and zeroes that's how you can think of this so let's talk a little bit about high-level languages so high-level programming languages offer several important benefits first they allow the programmer to think in a more natural language using English words in algebraic notation resulting in programs that look much more like text than tables of cryptic symbols okay that makes sense moreover they allow languages to be designed according to their intended use hence Fortran was designed for scientific computation COBOL for business data processing Lisp for simple symbol manipulation which Lisp isn't I don't know how common it is sees Lisp anymore I feel like not many people use lists Lisp anymore and I feel like that's a little sad I always wanted to learn Lisp but I never have because I'm not a computer scientist okay the second advantage of programming languages is improved programmer productivity one of the few areas of widespread agreement in software development is that it takes less time to develop programs when they are written in languages that require fewer lines to express the idea okay so what are we talking about here so if we want to write a program that does something we don't want to have to write 500 lines of code that you know are mainly managing control flow and what I mean by control flow is I don't want to have to write lot many lines of code that are simply saying very low-level tasks such as go here go here go here going to different spots of the program I want to be able to describe the algorithm how it would work at a very high level and I want everything underneath that high level language when it gets compiled to figure that out for me so conciseness is a clear advantage of a high level language over sembly language and we can see that here so here we wrote four lines of code and we can see even with this simple example we're going to seven lines of code so this is you know almost 2x the number of lines of code to the same function and assembly language as it took in a high-level language that's the key idea here and that's why abstraction is so important okay what else do we have the second advantage of programming languages is improved program okay so we talked about this productivity the final advantage is the programming languages allow programs to be independent of the computer on which they were developed this is another very important thing it seems like it's less important in some circumstances but for people developing software that's going to be sold commercially it's a very important basically what they're saying here is that when you compile a language like this in see how this output looks depends on what system you compile it on so this code will only work if what you're running on uses risk v if you're on an x86 architecture processor this code will not work your processor will have no idea what these instructions mean because they're going to be encoded completely differently so it's important that we have these high level abstractions because when we have these high level abstractions it can simply be compiled on someone's local machine and now the assembly language is generated there instead of you know where we're distributing and we don't have to host you know instead of having to host you know in copies of the program in assembly we just need to maybe host a single version written in C okay so I think that's about all we have for this section okay so 1.4 under the covers ooh now we're getting now we're getting actually really cozy in here now we're getting cozy okay so now that we have looked below your programs to uncover the underlying software let's open the covers of your computer to learn about the underlying hardware okay that's important we want to know more about the hardware you know if we want to be good architects or even solve good software developers if you want to run fast-moving software or software that execute quickly and we do high-performance computing we want to know the underlying architecture in fact it's necessary so let's see when we come to an important point in this book a point so significant that we hope you'll remember it forever we emphasize it as identifying it as a big picture item we have about a dozen big pictures in this book the first being the five components of a computer that performed tasks of inputting outputting processing and storing data ok so this is a very this is a very easy thing to kind of understand it's that you know every optimization that happens in a computer every single architectural detail is not weighted the same and that's pretty that's the same in in life as well as in computers so we really want to understand what are the important topics what do we really care about okay let me respond to a message real quick and I will be right back okay and I'm back so let's see two key components of computers are input devices and output devices so this is IO what we talked about earlier so that's these first two in putting in outputting such as the speaker and a microphone like I'm talking into is a input device pretty simple concept [Music] okay so chapters five and six will actually talk about i/o here we go this is our big picture so down here we have a computer and we'll see that we have you know very important components in here so we want we want to understand how things get into memory and into our machine that we're working on just as importantly we want to know how things flow out of our machine that way we know how does the outside world talk with this and how can we make this usable then we want to understand memory we want to understand memory because when we're inputting something and outputting something we want to understand where that's coming from then we want to understand the processor and when I meet what I mean by understand the processor we want to understand what we put in memory we're going to do something with it we don't want to just put something in memory and then do nothing with it then it serves no purpose so you understand the processor so that means you want to understand the control how does the processor know what to do and in the data path how does our inputs move throughout the system to and from memory so that it can both you know do some number crunching here in the processor and then get back to memory so that it can be pushed out to the output so this is really our big picture and so the key out outside things we want to look at is okay here's our code coming how does that code come into the computer or into specifically how does it interact with memory and the processor how is this code written and then when we're actually running our designs we want to see how do we evaluate the performance of our designs that's also a very interesting thing we want to look at so through the looking-glass so the most fascinating IO device is probably graphics displays so they're talking with LCDs here um anything interesting okay we can skip a lot of this we don't need to really care about what makes up an LCD so this rods when current is applied and no longer been the light since Luke Chris material is between two screens polarized at 90 degrees light cannot pass unless it's bent today most of cities use active matrix that has tiny transistors switch at each pixel to control current precisely okay so now that we know that each pixel is controlled by a single transistor it's fairly interesting but you know we're architects you know unless you want to design LCD displays this isn't particularly interesting okay active matrix display sure the image is composed of a matrix of picture elements or pixels of course which can be represented as a matrix of bits so called a bitmap so this is what the computer sees the computer doesn't know what a picture looks like it just knows what a bitmap looks like it knows what that math actually looks like which is fairly interesting okay so depending on the size of the screen and the resolution of the display matrix in a typical tablet ranges in size from okay 1024 768 by 22 2014 5 1536 okay so we're talking what resolution here not that interesting we can skip over this okay well this is more interesting now we're getting into how the architecture actually interacts with something like a I display so how does the hardware know how to calculate these things so the computer hardware support for graphics consists mainly of raster refresh buffer or something called a frame buffer and so this is what stores that bitmap that's eventually going to be printed to the screen the image to be represented on screen is stored in the frame buffer so the frame buffer over here it has the information for what what all the pixels are going to look like and gets printed on the screen and if we end up getting that permission to use that computer architecture book we can really look at details of things like frame buffers I think that'll be very interesting oh I got a message let's see let's see if I can respond to this really quickly okay we'll try to wrap up this chapter so I can take this call back okay let's it will finish this chapter and then I may take a quick break get some water and then I think we'll be back and we can maybe move on a little bit I'll probably be here for another two ish hours so that'll probably good okay so we can talk a little bit about touch screens and so how they differed from a simple screen we're just right into a frame buffer because now we're taking input on something that's typically an output so while PCs also use LCDs the tablets and smartphones of the post PC era have replaced the keyboard and mouse with touch sensitive displays which have wonderful user interface advantages of users directly pointing whatever they want to use blah blah blah okay okay so it's just as the people are electro conductors and then an insulator like glass is covered with the transparent conductor touching distorts the electrostatic field changing capacitance and then it allows for multiple touches simultaneously okay I was hoping they would have done more with this but that's okay you can't always you don't always win you don't always get you know exactly what you want that's fine we can we can continue on we can we can push ahead so let's see opening the box so figure 1.7 shows the contents of an iPad - with the five classical components okay io dominates this reading device of course because we have a giant screen and it has speakers and both of those are input/output and it's got a button on it it has multiple buttons for power and for the home button and for volume it has input for of course the headphone it's got an accelerometer that way it knows how to spin when you twist it on its side Wi-Fi and Bluetooth are considered IO of course and then data path and control or of course tiny things so when we're talking about you know anything like data path and i/o we're talking about stuff on you know these tiny chips right here in fact probably not this this is probably like the Bluetooth module right here or the Y file chip I would guess I've never looked at the guts of an iPad so I don't know we can skip over that caption courtesy if I fix it so if you want to learn more go to ifixit.com nevermind I'm not gonna say go to ifixit.com I've never been there don't go there I'm not endorsing them okay okay so the small rectangles of course integrated circuits so we know that those little black boxes that we see on motherboards or integrated circuits and we call those chips so the a5 package seen in the middle so a5 is just a standard package size that's fine so dim is another just a package name it's not specific same thing with dip which stands for dual inline package I think sip is also one single inline package maybe I don't know if that's actually a a used term maybe it is maybe it's not that's fine the processor is the active part of the computer of course I think everyone can kind reason about that and of course for an actual computer it adds numbers test numbers and signals IO devices to activate so there has to be some set process for when you turn on a computer for a new turn on an iPad for it to know what to do so those those basic fundamental types of applications are usually our hard core hard coded and built in to the computer itself with system management chips occasionally people called processor the CPU central processing unit so there's another acronym so if you didn't know what CPU meant it means central processor unit sometimes people call it central processing unit ok so there's that a5 chip so a5 is the processor and then I don't know enough about what these other things are so you've got a toshiba part over here this is probably so toshiba this is probably a drive so this is probably like a solid-state drive and then you have ok the similar size chip on the left so i'm guessing that's this is going to be your memory so 32 gig flash memory chip for non-volatile storage or i don't yeah i think they mean this one okay that's fine i think we're good okay then we have a Broadcom chip over here this is gonna be something networking related this is gonna be your Wi-Fi or your probably your Wi-Fi maybe your Bluetooth maybe both I don't know I don't think it's an accelerometer this might be an accelerometer maybe it's hard to say it's hard to tell without actually looking at the part numbers sometimes they tell you what it is okay so let's look even deeper into the hardware now so descending even lower into the hardware reveals details of the micro processor the processor logically comprises two main components data path and control the respective brawn and brain of the processor the data path performs the arithmetic operations and control tell and Control tells the data path memory and i/o devices what to do according to the wishes of the instruction so basically you have a data path which performs the arithmetic and the control as it as it would sound like controls the data path okay so we have any questions so far on any of this stuff this is still fairly introductory type architecture stuff it should be fairly easy to comprehend but I don't want anyone here getting lost early you know I want you guys to really fundamentally understand these concepts I'm having a great time I'm really enjoying teaching teaching something that I'm enjoying more than I thought I would okay so we can move on so let's see what's going to be next and our lists of things to do so the next thing we're gonna look at is okay so this is going to be a breakdown of the a5 package so this is going to be this is going to be what your actual process is going to look like so so process of data path 1 so these are going to be our cores so that's a 4 core processor we're gonna have GPIO of course which is our general purpose input output on the outside so these are just gonna be pins that we're gonna be connecting to other places we've got some more i/o here so just input/output we have oh we know maybe I'm wrong sorry that I'm wrong I'm wrong I'm wrong so it's a 2 core processor these are just data paths yeah so our arm cores are actual processors so this is a dual core ok and then we have our dual literate SDRAM and we'll probably go into how SDRAM actually works ok so a guy from chat asked Joe wait that's pre-recorded right and I'm multitasking chat and this lecture I'm absolutely multitasking I'm doing a great job ok thanks for the stream it's too late I'm going to watch the vod's about hey that's a ok listen the vod's gonna be up for you you know all I hope is that you learn something and I hope you have a pleasant sleep and I hope you have a wonderful day tomorrow ok for everyone else still sing in the stream thank you for being here still let's continue on with what we're learning so down here we have PLL's and if you don't know what PLL's are that stands for phase locked loops so if I'm not mistaken PLL's or phase locked loops is the components that are going to control the different frequencies that we need around the board so not every chip runs at the same frequency so Ram of course to run at the same frequency as the cores are going to run which don't run as the same frequencies as the ale use are going to run which don't run at the same frequencies as USB is run so we need a way of generating different clock speeds and so that's generally done I think by PLL's I think that's what that's for we have a video DAC so this is going to be our digital analog converter we have our audio input and output probably we have a Wi-Fi little block here more input output more general purpose input output and of course these are gonna be a processor data path so if I'm not mistaken this is going to be things like alias and you know our computational units and we have digital logic blocks in here and so it doesn't say what the rest of these things are which mean only one thing means that Apple didn't want you to know what these are okay so and here we have more memory so more do the double double data rate SDRAM interface okay so let's see what else we go so this is like I said this is this AFI package this ARM processor used by Apple which like we had said previously this is for iPad 2 I think is what they said so another thing in order to another thing that'd be interesting to understand is from a security angle what can be done to actually steal these chips and reproduce reproduce fake copies of these chips so it's very interesting there are actually machines now that can etch off layer by layer of an integrated circuit and look at it and a transistor level and you can actually recreate a chip using these machines so you can create a phony little processor or a phony arbitrary chip using some of these machines so I find that to be very interesting that seems like a very a very cool thing that can be done right now very scary from a security point of view but a very interesting thing okay so let's see what else we have so let's move on so dram stands for dynamic random-access memory so multiple D Rams are used together to contain the instructions and data for the program so we have instruction memory and data memory that's important to understand okay so memory built as an integrated circuit that's DRAM so we can talk a little bit about how DRAM is actually constructed later but this is going to be our densest memory object because DRAM if I'm not mistaken takes only a single transistor and a single capacitor to store a single bit so something like SRAM which is what your faster memory is gonna be made out of uses something called cross-coupled inverters and every inverter is I think every inverter is two gates and then there's also some passengers in there so you're already minimum 4x the size of DRAM okay so access time is 50 nanoseconds it's not bad it's not great something bad and cost per gigabyte in 2012 was five to ten dollars so it's fairly cheap that's what you need to understand it's fairly cheap then we have our caches so caches are small fast memory that act as a buffer for D around so we're not you know pounding on DRAM all the time we have caches as a buffer and these are these are what use SRAM which is static random-access memory so these are much faster but much less dense like I said they're much larger than individual bit storage that DRAM has the bit storage in SRAM there's a lot less dense but it's faster so SRAM and DRAM are two layers of the memory hierarchy okay so we're gonna have SRAM up here at the top so we can go ahead and write that down it'll be important for us to remember sew up the top here we have or SRAM which remember is static random-access memory and then here at the second level we have DRAM which is our dynamic random access memory and wonder it's gonna be down here get in with take a guess I'll even hint it's not hard it's the only part that we don't have yet it's gonna be disk disk is the only thing we're missing from this so we have SRAM and DRAM and then at the bottom of the memory hierarchy we have disk where the regs you generally don't consider regs in the memory hierarchy now right yeah regs aren't generally considered captured his memory no so generally when you're talking about the memory hierarchy you're talking about things that are shared registers aren't shared I think you can do register sharing in certain things but that's that's way more complicated generally we're talk about the memory hierarchy we're talking about usually shared resources and things that store a lot more data okay so as mentioned above one of the great ideas to improve design is abstraction so like we said abstractions this picture so what are we talking about as far as abstraction so there isn't because of its importance it is given a special name the instructions set so the instruction set architecture or simply architecture of the computer so that's what we mean when we're talking that's one of the biggest abstractions that we're going to really talk about in this when we go through this book is the art of the eisah the ISA the instructions that architecture the instruction set architecture includes anything programmers need to know to make binary machine language programs work so this includes how does how do the high-level languages map to the instructions what do the instructions do how do the instructions encode into machine language or ones and zeros and then how that eventually gets executed by the hardware so typically the operating system will encapsulate the details of doing IO allocating memory and other low-level system functions so that's what we mean by like system software so that application programmers don't need to worry about such details the combination of basic instruction set and the operating system interface provided for application programmers is called the ABI or application binary interface so the ABI so ABI is how we're going to how these applications are going to interface more directly with the hardware so it's gonna be through this ABI interface okay so let's move on okay so we've gone over at the instruction set architecture or eisah is a and we've gone over application binary interface or ABI an instruction set architecture allows computer designers to talk about functions independently from the hardware so this is important if we're trying to talk to somebody about what a computer does we don't need to teach them what hardware is before we teach them software we want to be able to talk about things generally like we've said so many times before okay for example we can talk about functions of a digital clock separately from the clock hardware so we don't need to tell teach someone about what an LED is to tell someone that we set in the LARM ok so computer desires distinguish architecture Architecture from implementation so let's try to keep that in mind and you know what you can kind of see is this is something that's very important for all walks of life you're if you're talking with anybody you know just haulage you we don't want to get lost in the weeds in a conversation so if someone asks how we're doing generally they're looking for a response such as I'm doing well I'm not doing well I'm very busy I have tons of free time I'm happy I'm sad we generally don't need to tell them you know okay here are all the things that led up to me being this way so we can consider that the different levels of abstraction so we have our emotions and behind our emotions we have all the events that led us to having those emotions just like we have our high level programming languages and then we have our eisah and then we have the hardware that the icer runs on okay so the big picture so here we have a big picture moment both hardware and software consists of hierarchical hierarchical layers using abstraction with each layer hiding details from level above one key interface between the levels of abstraction is the instruction set architecture so this is a bridge between layers the interface between the hardware and low-level software this abstract interface enables many implementations of varying cost and performance to run identical software very interesting okay a safe place for data okay so we're getting I think we're getting fairly close to the end of the first chapter so let's see if we can finish this out and then take a quick break thus far we have seen how to input data compute using the data and display data if we were to lose power you know we could lose things inside memory if it's volatile so that means that when it loses power it forgets so this is temporary storage otherwise we have forms of storage such as DVDs or you know magnetic tape that are non-volatile memory so that means if we lose power they you know they retain all of their information and so this is an interesting thing because servers a lot of times have a lot of failover so that if you lose power they have batteries and some of the components so that you know even if you have things in memory it will keep the things in memory just long enough to write them all to a tiny piece of flash memory so non-volatile memory so that when you regain power it can write that bass back to where it was supposed to go to the correct place okay so to sing which between volatile memory used to hold data in programs while they're running and non-volatile memory to store data and the programs between runs the term main memory so it's always gonna be called main memory I don't know anyone that calls a primary memory and then also secondary memory secondary memory is are things like DRAM and primary memory so it says magnetic disk so that's like 1975 like it says so flash memory is more common things now to have flash okay well slower than DRAM and it's much cheaper than DRAM so yeah flash is slower than ear I'm much cheaper than DRAM so flash memory is a standard secondary memory of PMDs personal mobile devices alas unlike disks and DRAM flash memory bits wear out after 100 K - 1 million writes so that's bad so that ain't true introduces another thing to keep in mind with fault tolerance is we don't want to pathologically write to only a few bits because we're gonna wear those out really quickly and then we're going to have a limited space for where we can write our data after a while so that's fairly interesting so yeah here we're talking more about the memory hierarchy ok so magnetic disk also called a hard disk in flash memory slower than DRAM but fairly cheap so compete communicating with other computers let's see how much more we have in this chapter I think we're fairly close okay so I think we'll go through this one last section and then I'm going to take a break for a little bit okay so communicating with other computers we've explained how we can input compute display and save data but there's still one thing missing from today's computers it's computer network so this is a very important thing and I'm actually going to do an entire course on this and probably do some streams on this so networks have become so popular that they are the backbone of current computer systems a new personal mobile device or server without a network interface would be ridiculed you know you can't have these things that are just an isolation it doesn't make sense unless it's for some specific application it doesn't need to talk to anyone so here the advantage of network computers so communication so we can exchange informations at high speed we can share resources instead of having to have everything on our on a single device and we can access things over long distances which is a very important thing and then we have you know we can talk about a couple other different key concepts which is so let's say networks vary in length in performance it can be up to so Ethernet perhaps the most popular type of network is Ethernet so an Ethernet network can be up to a kilometer long and transfer up to 40 gigabits per second its length and speed make Ethernet useful to connect computers in the same floor of a building hence the example of what is general generically called a local area network okay so local area networks are interconnected with switches that can provide routing services and security now when versus LAN wide area networks across continents and are the backbone of the internet so you know the big tunnel between or the big pipe between Europe and the US we got a giant fiber optic connection between the two continents may be interesting to look at how those are those come to fruition okay so networks have changed the face of computing in the thirty years both by becoming much more ubiquitous am I making dramatic increases in performance in the 1970s very few individuals had access to electronic metal a male Internet and web did not exist physically mailing magnetic tapes was the primary way to transfer large amounts of data doesn't that sound horrible man that sounds awful I can't believe they actually laid all the cable yeah no that's awesome right there's so many really cool things that once you start getting into computer architecture that you find out and when you get the things like computer networks just the amount of infrastructure that's been built is amazing okay so a networking technology improved it it became considerably cheaper to move and had significantly higher capacity so standard local area network developed 30 years ago was a version of Ethernet that had a maximum capacity of 10 million bits per second typically share it by tens if not hundreds of computers so now you know local area network offer you know one to 40 gigabits per second shared by a few computers and then optical communications technology so be interesting to do a little stream on fibre optics I know a little bit not a ton but it'd be interesting to look at so we've got a dramatic rise in deployment of networking combined with increasing capacity making it central to the information revolution for the last decade another innovation networking is reshaping the ways computer computers communicate so wireless technology that's what we're talking about the ability to make radio in the same low-cost semiconductor technology so CMOS which stands for a complementary metal-oxide-semiconductor so CMOS is combinations of in moss and peat moss together so used for memory and micro processors enabled significant improvement in price leading to an explosion of in deployment so how did this lead to a significant improvement in price can anyone answer this in a very simple way you can answer it in two two words who gets the bonus points by being able to answer it you know I'm gonna get the bonus points come on you guys can do it low power uh low power is not going to change how much it costs anyone else although one more person answer if not I'll go ahead or if you want to give another answer that's fine two words and I'll give you a hint it's one of the eight key concepts of why it's cheaper okay that's fine so we're talking about why a wireless technology once it started using CMOS or transistors became way cheaper and so I'll tell you why right now Moore's law so if we start making network technology with transistors yeah that's correct so smaller Moore's Law so the basic idea is we get twice as many transistors for the same money or for the same amount of money so if we use something like that that means that our price is going to decrease every generation or two years by about half for the exact same product so that's why you know you started using CMOS it was a significant improvement in price so Moore's law is the answer so smaller is also correct low-power it's not a bad guess but look low power typically doesn't influence price so currently available wireless technology so I Triple E standard is 802 11 which is one to nearly 100 million bits per second okay okay so that's pretty good so we learned a lot in this in this session we learned a lot about some introductory into computer architecture and some foundations so let's check to check yourself so semiconductor DRM technology flash memory and disk storage differ significantly for each technology lists volatility approximate relative access time and approximate of approximate relative cost so like we said you know DRAM is volatile and then flash and disk are both non-volatile with meaning we're not going to lose it if we lose power and access time we know that disk is always gonna be the slowest we know that DRAM out of these three is gonna be the fastest even though that flash while not the slowest is still Oh is still gonna be slower than DRAM okay so after a quick break we're going to we're gonna get back and were going to talk about technologies for building processors in memory so we're gonna talk about performance analysis and we're gonna talk a little bit about very large system integration VLSI which is really just chip design so we've gone through 84 pages of this book so far so I think we're making good progress okay so I'm gonna cut the stream for a little bit I want to thank you all for joining us if I don't see you if I don't see you when I return but I hope you all have a wonderful evening and thank you for joining the stream so in probably about a half hour we'll start the stream back up and if you're able to join me I'll make sure to try to post around that I'm starting the stream again but I hope you have a wonderful time until I return like I said I'll be returning in about a half hour or so yeah but if you're not here I hope you learned something today and listen I got a couple seconds if you have any final questions before I grab some water and make a phone call I'd be happy to answer or if you want a little fun fact let me know what you want a fun fact on and I'll see if I can help you out since I'm doing real-time streaming I'll give you about 30 seconds see if there's anything Hillside thanks for the stream no problem in live stream sniper hey if I don't see you again thanks for joining the stream fun fact please okay let's see if we can come with the fun fact here's a very interesting fun fact so we know that going to disk storage is incredibly slow and we're talking tens of thousands maybe even hundreds of thousands of clock cycles so there's actually a very fun fact that the communication rate for i/o we can build systems that have incredibly fast communication but between systems like physically between different systems so there's actually been designs where we have a pooled cache so we have a system that has extremely fast memory as a shared resources for as a shared resource so a processor can actually send a request over IO to this shared resource pool access something in the shared resource pool have it sent back over that link back to the processor and orders of magnitude faster than going to disk so basically it's faster to go to another computer than it is to actually go to your own disk so there's a quick fun fact then we'll go into why that is in a bit yeah really cool really cool right we'll go into that sometime that's actually a really cool idea and I think I saw a paper on that it's not my area but I wouldn't mind going over a paper on that I think it'd be really cool not in the same process or going over a network so not like over like the Internet I'm talking about I'm talking about computer of talking about systems connected together so this is this is about or DMA so our DMA is remote direct memory access so it's direct memory access from the memory of one computer into that of another so if you want to look that up it's our DMA all right I will see you guys in a bit and as always learn something new ciao "
G4paPIMtxk0,27,"This video is a lecture for Chapter 5 Part 2 Assembly Language for DCN1013/BCN1043 Computer Architecture and Organization. You can gain access to more information in our google classroom account.


For Universiti Malaysia Pahang Students Enrolled in this class (BCN1043/DCN1013). Please complete the google form in the link below for confirmation of your attendance:
https://forms.gle/npLx6on7mwLArrFj9

 You can download the google classroom app at: 
https://play.google.com/store/apps/de...
or
Open it directly in your browser at: 
https://edu.google.com/products/class...",2020-12-18T03:51:24Z,DCN1013/BCN1043-Chap.5-Part 2 Computer Arch.& Org.: Computer Memory,https://i.ytimg.com/vi/G4paPIMtxk0/hqdefault.jpg,Syafiq F.K. Academia & Solutions,PT22M37S,false,490,20,0,0,0,assalamualaikum and a very good morning to all students today we meet here again for our next video on the chapter five memory for our computer architecture immunization course so i am the bishop evangelical obama presenting to you from ump okay so today we are on our part two of chapter five so let's start our class immediately so last week we introduced to you about i interest to you about current memory technology that we have in in the world okay and how they are divided into some characteristics physical characteristics and also functions so today okay we are going to discuss regarding okay how do you classify those technologies okay so you usually we design the memory based on certain certain key features which is like for example capacity performance access time and also the cost so major design of a major design objective of this memory system is basically to provide adequate storage capacity and then to end at an acceptable level of performance at sex and at a reasonable cost so means that how big is the size how fast is the memory and how expensive it is so you can have a trade-off among this characteristic okay in order to optimize the usage of those memory like for example in case you want larger capacity but at a lower speed you can have a cheaper hard drive okay but if you want a very fast memory you don't need that much capacity it can be almost at mid-range price like for example ssd okay so these three elements is how you determine the necessity of your hard drives okay so we usually have a hierarchy to determine which which memory technology that we need for our computer so basically this memory uh memory hierarchy okay so the memory you need okay since it's an essential computer component in any digital computer scene it is needed for storing progress and data so not all accumulated information is needed by the cpu at the same time so it is economical for you to use the low cost search device to serve as backup for storing information that is not currently used by cpu so in order for us to distinguish which one is less important which one is more important we will use the memory hierarchy so the memory hierarchy actually okay is created due to the gap of professor processor technology among the years okay so the according to most law okay so we have a gap of 50 of uh technology growth comparing from processor and memory so how do you compensate okay the performance of processor uh the performance difference between the process and memory okay we compensate it by using different kind of technology or a different kind of purpose so how do you address this gap actually so how do you address this gap okay utilizing the memory hierarchy so basically you put a smaller smaller capacity memory type bypass faster okay which is the cache memories between the cpu and the dram and then here you create a memory hierarchy so based on the cpu okay we should have a memory register you have a cache memory in the middle and a dram so for this is to compensate the difference in speed between the cpu register and also the ram so at the bottom area we have this storage device type like for example hard drives aka removable device so this one is much smaller a much much larger space but way slower comparing to other memory type like for example ram cache memory register so they become at the lowest point and finally this is where our input source is okay so some of the input source also have a memory inside it so this one is considered as the slowest one since they are the furthest away from the cpu so this main memory okay they consist of all storage devices employed in the memory hierarchy actually consists of all storage device employed in a computer system from the slow by high capacity memory to a relatively faster main memory to an even smaller and faster cache memory so this memory you need to directly communicate with the cpu usually we call it main memory devices that provide backup storage are usually we call auxiliary memory so when you talk about the memory hierarchy okay so the main memory occupies a central position by being able to communicate directly with the cpu and with auxiliary memory so this is the memory okay through an i o processor okay so we have the i o processor okay this is the i o model itself okay so this main memory okay authentic memory cannot be accessed directly to the cpu so in case you need to use any data from auxiliary memory they need to be transferred first to the main memory before being taken into the cpu so in this case okay the memory is kind of like a little bit crowded since there's a lot of data being uh being uh will be coming in and coming out of the main memory so in case you are using a lot of uh using one program a lot of times okay like for example your chrome browser okay your your internet browser okay some sort of software so some of them okay they put it into the cache memory so that the cpu access will not burden the main memory they can get it faster at the cache memory okay so this means that your cpu can work even faster without having to access the main memory all the time so most of your cpu if you buy amd or intel they have this cache memory inside it okay so the certain size okay how many threads it has okay so this is where the cpu comes in so cpu and cache usually they are abandoned as one component okay so cpu logic is usually faster than the main memory access time with the result that processing speed is limited primarily by the speed of mid memory the cache is used for storing segments of programs currently being executed in them cpu and temporary data frequently needed in the present calculation so when you look at volatile and non-volatile okay so we have two types of memory okay like i mentioned last week we have volatile memory and non-volatile memory so volatile memory they need to hold they need electricity to hold the data and not volatile memory they don't need electricity to hold the data unfortunately those who are close to the cpu they are usually a volatile memory so you need the power most of the time which is not sustainable in terms of long term consumer consumer computer so in this case we have what we call non-volatile memory just to hold the data permanently when you're done with your pc so this is what we call a secondary memory so the voltage memory usually you have when you are doing some process within your computer so this is where you have the random accessory the ram okay where data is used just uh to bridge between the secondary memory and the cpu so example okay of the volatile memory and non-volatile memory so let's just mention like you work on a table okay on a desk okay you work on a desk and you have a books a cabinet full of books behind you okay so the cabinet full of books behind you are what we call the is what represent the hard drives okay the secondary memory and your desk is what represents a volatile memory so anything you want to do your work okay imagine that you are a cpu working on a desk with behind you the hard drives full of books so you as a cpu when you want to work on something you have to put the books from your cabinet okay at the back onto your desk first so that you can work on the book at your desk so this is what ram represent to the cpu okay it's like a desk for you okay so you work on your desk and once you finish working on your book you will return that book back into the cabinet at the back so this means that from your desk it transfers back to the secondary memory at the end so this is what is different between the volatile memory and non-volatile memory so volatile memory like for example your desk okay it needs to be uh it cannot be crowded most of the time since you are the one who are using the desk for a lot of things okay so sometimes okay you have to lose some data to insert new data so they keep coming and going so this is not efficient if you have a lot of data at the same time to process okay so you have to save some of the data that you have been you have done doing something with it back to the secondary memory so when you look at the memory hierarchy so it's like a pyramid so when you look at it at the top usually they are very fast okay they are very fast and they are very small and they are very expensive especially if you want to make a big one okay so when you have this at the top fast and at the bottom usually they are very slow however they are very huge you can store a lot of space and they are least expensive because this expensive is in terms of the size the size they provided okay according to the price okay so when you decrease the memory hierarchy okay you decrease the cost per bit okay the cost means that it releases specimens at the cost per bit of storage but it uh okay it decrease the cost a bit but it increase the capacity okay and then it increase access time means if is a little bit slower okay there will be maybe a lot more slower comparing to the top okay and then they are usually used less by the main memory or the by their processor so this means that they are decreasing frequency of access by the main memory and the processor so less thing you use you put it in the slower drives okay the most common thing you use okay you put it in the uh in the upper level okay like for example cache memory or ram so why do we need the memory hierarchy okay so this is for us to this is for so right there is a memory hierarchy so memory hierarchy usually we use to have uh to analyze the relationship between the size capacity cost and speed so relationship in memory implementation means that faster system greater cost per bit greater capacity smaller cost per bit greater capacity slower access time therefore computer system is equipped with a hierarchy of memory subsystem some internal to the system okay and some external to the system okay so one of the memory that is inside the memory hierarchy is cache memory so last week i did not introduce to you about the cache memory so the cache memory says in between the cpu and the main memory okay so if you i mentioned before as an example cpu is you and memory is the dash so what is the cache memory okay so the cache memory is basically a small cupboard under your desk okay small cupboard under your desk small rack under your desk or even your desk this drawer okay so anything that you use inside your main memory okay you want to use they will put it into your on your desk first okay for you to process and this is your desk means the main memory okay so when you have finished something with main memory okay you want to like for example you think you will going to use it again later on so it's on the kiv you put it in your cache first and when the cpu wants to use the same data they can just refer back to the cache instead of the main memory so it's in the middle technically so okay how does it work so basically when the processor attempts to read a word from a memory a check is made to determine if the word is in the cache means that if they are looking for data inside the main memory they check first in the cache is it available or not so if so the word is delivered to the person if available they will put back into the processor if not a block of main memory consisting some fixed number of words is read into the cache okay means that if they are not available they will refer to the main memory and take a block of the memory and put it into the cache okay and from the cache they will use it so this is one way of the how to use cache memory so cache memory is usually in level two okay between the cpu and the main memory so if you look at this figure here this is exactly how many bytes they are storing right now so usually in professor a processor they have four to eight bytes in level one cache eight to two divides level two cash one two four blocks okay so eight to two bytes is usually a one one block okay the two black is actually one block okay they put one two two four blocks and from main memory okay they have uh they have around okay one thousand gigabytes so currently we have about the 16 gigabytes here right we see so other than that okay they will transfer according to this this sector from the secondary memory okay so this is how your main memory works with the cache memory so when the cache memory we look at the cache memory so cache memory is not a big a big memory so they usually just use to refer to certain certain area inside the main memory so assume an access to the memory cause a block of k words to be transferred to the cache okay excessive main memory cause block of k was to be transferred to the cache the block transferred from in memory is stored in the cache as a single unit called a slot line or page once copied to the cache individual words within a line can be accessed by the cpu so means that from in memory they transfer one block into the cache memory and from the cache memory they line by line will be inspected by their cpu because of the high speed involved with the cache management of the data transfer and storage in the cache is done in hardware so os does not know about cache so this doesn't mean this means that any os is okay since the cache memory works according to the hardware okay they don't have different management of cache according to different os so if there are two power of n words in the main memory there will be m times two power of n divided by k blocks in the memory okay so m will be much greater than number of line c in the catch so every line of data in the cache must be tagged in some way to identify what main memory block it is so this is the relationship between cache and memory so let's go towards a little bit of detail okay so usually in cache memory they have several factors in cache design first the size okay and then the mapping function mapping function is how they are mapped from cache towards them in memory so they have three type direct associative and set associative and then we have the line replacement algorithm okay so means that how they replace the line inside your cache memory so they use certain algorithm for for algorithm here that i have given example least recently used lru first in first out i mean the first data in will be the first data out it gets new in and at least frequently used the number of time it being accessed is less so they will be removed okay and then randomly remove so this is an example of how they remove data from cache and replace with data from the ram and then we have the right policy which is the right through and right back we have blog line size okay and then we have number and type of catcher single or two level and then unify or split so let's take a look about one of the mapping functions here okay i'll introduce one first okay so here is what we have a direct mapping so direct mapping okay so the main memory block is assigned to a specific line in the cache okay so here i equal j modulo c so where i is the cache line number cycle memory block j so example if m equals 64 c is equal four so let's see line zero can hold blocks of zero four eight one two so here we have uh we have four lights and each four lines they can hold four units okay so they can hold a one unit so zero one two three after finish first line they will second line is 4 5 6 7 8 9 10 11 12 13 14 15 up until 64. so let's take a look at direct mapping first so we have a tag identifier line number identifier and word identifier so what exactly the tag identifier land number is we have this figure here so any memory address they have a tag line and word okay tag line at work so this is example one block of data okay one block data so here okay the memory address from the memory okay they will refer back towards okay like for example if the the request is memory address here okay so the tag will be compared from with cash first okay if they are tagged from the requested memory address inside the cache okay the data word will be taken from the data inside cache okay the red one will be taken from data inside cache okay so the line is here representing which line inside cache the memory location is okay the data memory data location is if there are no data the tag is not recognized inside the cache they will refer back towards the main memory and take this one okay and put it inside the process under the cpu okay so this one will take longer comparing to when they need from the cache memory okay so that's why we transfer them into cache memory so once they already finish with this by the processor they will move it into the cache memory or sometimes they will move it first into the cache memory and use it inside and replace it according to this depends on the line replacing algorithm itself okay so example of a memory address so this one is representing memory address okay so memory size of one megabyte the address bits addressable to the individual byte cache size is one k lines each holding eight bytes okay so here we have around 20 bit 20 address bits so within the 20 address bit we have seven feet that representing tag 10 bit representing line and three bits representing word so imagine that okay where is the byte store admin memory location a b c d e stored so the abcde tag id is 55 line id is 39b one id is six so this is representing the information according to this memory address okay so that's all for today i will continue with our third part of the memory organization later hopefully this will be useful for you i'll see you guys in my next video thank you for the likes and subscribe so next time we will look into memory organization so stay safe stay healthy and take care bye 
bYwG8kFUZgY,27,"York University - Computer Organization and Architecture (EECS2021E) (RISC-V Version) - Fall 2019

Based on the book of ""Computer Organization and Design RISC-V Edition- The Hardware Software Interface"" by David Patterson and John Hennessy
#computerOrganization #computerArchitecure #pattersonHennesy",2019-09-12T23:44:41Z,Lecture 3 (EECS2021E) - Chapter 2 (Part I),https://i.ytimg.com/vi/bYwG8kFUZgY/hqdefault.jpg,Amir H. Ashouri,PT1H8M58S,false,5469,40,1,0,N/A,so I guess it's time for us to retire wiki page now and then next week on Monday you have your first lab Monday and Tuesday please check your lab sessions if you're assigned in lab zero one is Monday is your in zero two it's on tuesdays labs are super full we don't have enough oh we don't have no extra seats for those who are swapping elapsed and you're gonna run into a problem for grading as well so again all right I have received many emails about the labs I'm gonna iterate another time so before each of the labs around to three days before the labs say on every week on Fridays I'll post the pre-lab PDF for you guys on Moodle you can download it we can play around with it you can see what's going on you can use the RVs simulator on your laptop you can connect remotely to the machines you can go physically to the labs and run it and see how does it work how does it feel and then you have to physically attend the labs either on Monday or Tuesday half of the session you're gonna run that pre labs on each of those assignments you're going to submit the files we'll let you know on Monday and Tuesday I'll be I'll be personal there so you don't have to panic on how to submit that if the instructions are on the wiki page I'm gonna migrate them to the tomorrow and then after the half of this after we reached the behalf of the the lecture we're gonna turn the machines into lab test mode so you don't have the access to the outside world and it's just you're gonna be treated as sort of a mini exam so the rest of the questions that were the questions of your lab you just have to run it on the simulator and then save the files and then submit those files as well and yeah that would be all so this process will be continued for four weeks lab a 2d and then we have one makeup sessions before the before the reading week and or perhaps after the the reading week and on the midterm week and then we're gonna start the last four laps for you guys so the last four laps we use that very luck simulator that that compiler Icarus ya know if you check the so the number of assign students to lab are like right at the edge of the lab being food so it's going to be sort of unfair for the other people who've been assigned to the lab send me an email let's see what we're gonna do for you perhaps your issue is not you know among the rest of the people so but if we can work work around but III do ask you guys to just attend your own session yeah it's gonna be it's gotta be in class perhaps but we'll see we'll see there's no way you can you can free it all in the same lab yeah alright so last lecture we we finished chapter one it was sort of an introduction to the computer organization and architecture you see some of the metrics about how to you know calculate can compute the performance and with what angle you can look at the performance of the computer and specifically the CPU so I'm just wrapping up here real quick so as a recap we were talking about the CPU time in terms of instructions per program multiplied cycles per instructions and multiplied seconds for each of those clock cycles that are taking right so if you combine them all in a more fine-grained matter we're gonna have it at a better CPU time to compute and then we talked about the you know dependence of performance over several layers of computer organization which are algorithms that are affecting ICS programming languages and compilers affecting mostly instruction counts in CBI and on the lower side lower lower end you have the I is a or instruction set architecture that affects IC and CPI as well so we wrapped up the previous lecture by saying that power is currently a limiting factor for companies that you know fabricate chips and produce chips because not only it's not linear anymore is that you can no longer be able to just you know increase the clock rate and expect your you know CPU to work just as fine we double the performance okay all right so the major concentration of the chapter 2 is around instruction set right we we sort of naively talked about it in the previous chapter so we're going to dig deep into how we're going to define instruction sets and how we're going to use it right for different sort of instructions or it mething instructions control instructions and so on and so forth and how does those is a looks like in a risk v I say right okay so as a definition we can say that instructions and instruction set is sort of a command given to a processor just think about it as sort of a vocabulary right so instruction set is your tools is your vocabulary that you have in order to communicate between your hardware and software right so different computers of course have different instruction sets right but big many aspects in common so we have two set of different eyes a s so risk RI SC and sisk C is C I'll talk about it in the next slide so although these two instruction says are different but you can find similarities between these two it's sort of a metadata of a vocabulary that that that you want to use and recently many Computers started to have more simpler you know simpler instruction sets although the the majority of x86 infant computers are still using axis I say as well so I talked about the the two types so risk which we are focusing on in this in a course and Sisk CISC so recent stands for reduced instruction set computer right and we are learning about the risk v is a so it is a fixed instruction size so the the width of your instructions are always fixed and that's sort of a good you know good aspect because it makes it much simpler your load and stores are much much simpler we're gonna see examples later today and if you want to compare risk whereas Sisk you see that the emphasis were on the more on this software side or the compiler side whereas the six is more it is understanding for complex instruction set computer the lengths are variable the instructions itself yelled more powerful results however there are much more complex to understand it's not a good start you know if you want to learn and I guess a and there are mostly focus on Hardware intensive instructions so they need genuinely more transistors in order to run the same set of commands so if you want to know that what type of computers are using I which of those normally in Intel x86 they are still using Sisk but recently risk has been you know brought into life and there's a huge research community working on that as well okay another major difference between risk insisting is risk is meant to be run each instruction is is meant to be run on one cycle and this is a very positive I would say point that it makes it much simpler if you want to you know learn or just analyze it so you're sure that one what instruction takes at least definitely one cycle which for which for some of the cysts they have some complex instruction that takes more than one cycle so it makes it much more you know harder to to analyze and to start with so so what's risk five now here so risk five is has it started with a risk project in 80s but then 2010 they they introduced it they set up the the risk five foundation you can see the website here so it was developed at UC Berkeley as an open I say and now you have the information are available at your hand if you have obtained a book the the first page of the book it has a green green card a green sheet so you can see actually it's working as a sort of a reference for risk five on your exam perhaps we got a copy rather than you know you can use it if you I mean because you don't have to memorize all of those so it was a question I say it's actually instruction set architecture so that does what we are talking about so in just one slide you're gonna see one example of that but iced instruction set architecture is sort of a medium you wanna communicate between hardware and software right so think about those instructions as sort of a vocabulary that that you want to use in order to communicate right so instruction set architecture is the whole set of the architecture you have set up in order to communicate between hardware and software okay so let's see a very simple example of that it's an an arithmetic operation so could be add subtraction division so this example has add right so we want to add two variables and we want to save the result in a third one right so in risk five the way we do is we have two sources and one destination right so when we use add a comma B comma C it actually tells the hardware to add B plus C right and store the results to the destination which is a so just you have to read it this way so a gets B plus C right question okay so all the arithmetic operations have sort of the similar you know form except the fact that their operation their respective operation is different could be sub would be many sort of division it could be shift could be a die and you can have a look at your green sheet so you know as we said all the instructions the number of instructions provided in an is a is limited right you cannot just devise in your instruction it should be implemented in the hardware because the hardware at least need needs to understand that right you need to have the compiler to generate that for the hardware so as you see this is a prima it's a very intuitive it's very simple and you know when when you are dealing with complex scenarios always simplicity as you see favors regularity so the the maintenance is easier and it's kind of lower the cost okay so now in in a second.we you're gonna see a more sort of complex or compound instruction set okay so say we have this C code F is equal to the addition of g plus h minus i plus j right so if we compile it you can even try it at home or using your RVs simulator and you pass it there you pass this and you see the the hex dump of the results so when you compile this high-level language I see like code with the risk 5 compiler that's what you get as the output so it's gonna break this compound and strong is this compound line as to first is kind of add these two variables store them in a temporary register we call it t 0 here then it's gonna add the second compound here I plus J that's what the second line for and it's going to store it in T 1 another temporary register and then at the end it's gonna subtract these two okay so for this very simple C code we need to have three instruction code yeah oh yeah apparently the author is a slide had a typo right yeah this this is a pieces are of course this is stop let me see if I can fix it right now for as well but I'll double-check the rest of this like later but if you found a typo just let me know yeah let's carry on here so now we be proof to come compound C code instructions to two ads and one sub right now as you notice the arithmetic instructions they use register operands so these T's here are representing one of the registers inside the RISC architecture and risk we had 32 registers again you can refer to your green card and see all of those I'm gonna let you know in the next slide so RIS file has 32 registers each of those are having a 64 bit with right so if you want to like visualize it so it could be something like this so you have so say you have T 0 to T 31 and this is 64 bit right which is pretty hard to write here yeah yeah that's almost like that so so so risk 5 has 32 registers each of those have 64 bit right so each of those registers are meant to be used for four specific cases I'll let you know in the next slide we cannot just randomly assign things to different registers each architecture has you know it has its own register and each of those mint views for different things okay and hearing this fight because 32-bit data as a word right so 64-bit we call it double word and the second design principle that we are talking about is most of the time smaller is faster because accessing me needs you know lower latency if you make it way way bigger first of all your design gets bigger your to spend consumes more power and then accessing those even in nanoseconds is gonna affect your you know performance and that's why the closer the things are have our next two CPUs like registers and then afterward cache afterward memory afterward the hard drive increasingly it's gonna get a slower although they have different architectures inherently like hard drive has a different architecture diagram but being even you know be here makes them slower that's right they're way way too quickly yeah so when you write programs in sisk as well you can use you can use a for instance if you want to write codes on Intel you can use intrinsic that takes into account the SS e of the registers of Intel right depending on the architecture you can use you can just write assembly that takes into account their specific register but this requires the knowledge of a compiler developer or a developer that knows what's on the line hardware is otherwise if you just use whatever compiler that you had and write C high-level C code or high-level Java code you're not sure if the compiler was sophisticated enough to take use of all those registers right this is one of the pitfalls yeah so these are a high level view of refire registers so the first one x0 meant to be used for constant value right x-1 meant to be used only for return at this x2 is for stack pointer X trees global pointer so we're gonna we're gonna touch on every one of those in a chapter as well x4 is thread pointer it's going to point you at red so X 5 - X 7 and 28 to 31 are temporary so in a specific cases that you need to have temporary results stored somewhere so you can use those it's 8 is frame pointer it sort of acts as a as a as an offset for a stack pointer X 9 and 18 to 27 can be used for safe registers and the rest are function arguments so later on when we talk about more examples you kind of you're gonna just learn them you know by heart that you can use how are you gonna use some of those and you know how you can actually take the the best use out of these available registers okay all right so let's see another example yeah the old code that we had F is equal to the addition of G Plus H minus I plus J right so now you see that the compiler is five code it's going to use X 5 and X 6 because X 5 and X 6 we're in the temporaries right so if we need to store temporary results for these two here the results of this and result of this we can use X 5 and X 6 right and the next 19 you come back and see it was in the where are you yeah for the same registers and that's for that okay questions right an X 20 X 21 yum I keep losing these save resistors Yeah right in this case there are temporary values that we are but what do you mean by anything if they were constant you could you could be abused e the other one there are variables containing so they're variables actually it's recon sent you would have used another register yeah because anything means yeah you have to just specifically mention that what that is yeah very good question yeah good question in this case since the dependence and the precedence doesn't matter because it's just two ads and once up yeah you could have just swap these five and six instructions but sometimes you need to have the order because you need the results available and just think about a longer compound that had like twenty thirty right so simply gonna run out of your temporaries so you have to wait for the first one and then perhaps you know add the next two with the first one and you know it just go one by one or one by two and then to buy one again and go on and so forth so it depends on the code you have right not everywhere but given the the knowledge that was present at up to now yes yeah but yeah there are some instances that you can't okay you have to find a way either to store the the up comings with the current temps and it's stored I mean in the next time or if you just have like 25 of those it's guys gonna take a longer time you have to wait for the result of the first one because at the end of the this doesn't have any end right if you even had 30 mm some but we'll just come back okay what up what about this formula you had like two million yeah at the end everything you have to find a way to figure out yeah when yeah because in so this is store uses decide is sometimes by compiler and sometimes you can add optimization to that if you don't actually need them if you don't actually need the result of G + edge and you just get rid of it after the final stop you can store it in term otherwise if you were if the compiler when the compiler parses a code it understands that how many times and what was the last occurrence of one variable if it didn't involve anymore you can just get rid of it right these are the optimizations the compiler at but if you want to write it by hand as someone who just writes assembly why we can see these are different versions you can write your point would be a little slower yeah because when you save into memory you you'll pay some penalty in order to access it again because you have to load it right a good question yeah okay so by now I hope that you saw even small arithmetic functions are very delicate right there are many ways you can you can run them alright so now let's talk about the memory operand or yeah here in risk 5 we have sort of a sort of an agreement sort of a rule of thumb that is little-endian so what does that mean so that means at least significant byte is normally at the least address of a world I'll have an example the next slide so you'll see how that works so the the contrast to list two too little in the end is called big endian so some of the other architectures are working that way and I showed that represents actually when we we use the other way around so the the most significant byte uses the list address so when we start addressing those opcode we start from bottom up instead of top top bottom top to bottom I approach ok so let's see an example here so actually when you have this 32-bit right we call this this bike because each PI has a piece right you have four bytes so the first on the left is called the most significant byte right on the left side this one on the right represents those eight bits is called the least significant byte and we have 32 or four byte here right so say we want to store this value this hex to address 20 right when we are using a little in Indian architecture so it starts from the the address that had to be started and then it's kind of put the values from the least significant byte right so d4 here c3 here b2 here and then a 1 here okay and it goes on and on on the other side if it was a big endian architecture it would go the other way around it starts from a1 okay it's pretty intuitive just don't get confused about the names and you know because I did I went when you you need to have sort of rules in if you want to you know parse them because you need to have some ordering fix otherwise or somebody would just read the other way around and you have an invalid value right is it clear right yeah so when you define numbers right in binary or in in in a decimal so in a decimal word so you have like I don't know eight five six - right this is the most significant side of the number because if you change this it's gonna impact the whole value a lot right so this is this most and it is least so go back to your binary case but treated each of those treated as eight weeks or one byte right so you have each of those bytes you know eight bits I want to make sure all of you got the idea is that clear it's the way we spyboss is the convention for that whis five is Beatrice so here I was trying to so this this is the it is the least significant anyone use it here right and this if you sometimes guys so we got a good question is your friend that is asking why do we have such Commission because I can't just read sometimes from the less and and I can't just assign from the the first otherwise I'm not sure when I'm but I'm trying to access this and what I'm I'm gonna yeah that's right that's just you have to keep one Commission that's a good question I'm not sure perhaps there are all the top check it for you for the next legislature sir so the way risk 5 goes is addressing in memory goes byte by byte so each memory address when you increment but my new menu traverse that in a stack is like 1 so like from 0 to 1 and 1 to 2 you are traversing one bite right yeah yeah yeah that's right so I've put another one here actually here so here this is your this is your memory right so now is so your first the starts at 0 right so then the second one is 8 because you're one byte up and then so on and so forth right is your memory in risk 5 so now you have another code here so we are fighting the 8 elements of a ok and we want to store that in 12 element apart so the re a a 12 now as h plus a 8 right so first of all h using X 21 why okay we had it perhaps somewhere safe and then now we want to make sure we are collecting we are fetching the eighth and the eighth location of eight right so we need to have yeah repeat the question because it was served it was saved perhaps in the previous instructions than we had it ready already yeah so now we want to access a of eight right the 8th position of carry 8 are a right so since the addressing starts byte by byte so if I want to fetch the result of this value where should I look at so I have to go a times by 8 right because each of those addresses are bytes so that the the zero byte represents a of zero the eight represents a 1/16 it presented a two and so on and so forth so if I if I want to have this available I need to load 64 and a story it's 22 is that clear so here in this case since since we started games from zero we base it to 0 but you can't just start in the middle of a program and you did another pointer to start that offset for you yeah that's right and that's one of those pointers that you saw in the resistor like this a frame point or a stack pointer eeeh right right exactly so now it's not right so this represents a load instruction now I'm in this in this you know session I'm just giving you some high high level ideas we we're gonna focus on each of those starting from next lecture so you actually know exactly what's happening when you load menu add menu with Stephen or sheep or or any of those other you know I said so this just this will give you an overview of how the things are working yep what is one upside yeah so I think it is is offset yeah suppose so a 0 in this case starts from 0 which is this right a 1 the address would be 8 for that and then if you go up to 8 a 8 you need to look for a dress 64 right in computer architecture we say so we have to offset it by 64 because you have to start from here to 64 and find the value corresponding to that because each of those addresses are they call it byte address shamea is another variable that we sort before so we're using it to to add to a 8 that's right x9 is another register that you know the result of these I mean do you have to have the books you can have a book yet on the green and but but you don't have to worry because we going to touch on all of those later no actually it's already questions up to size so it's up to double word if it depends I mean if you're storing a huge amount you need to just find other ways to shift and break it and yeah all sort of things that we learn later we should that we already see the first line now okay so now we are off to the point yep if you had a zero yeah we had to use zero instead of 64 0x20 p.m. that's right which line okay so X 22 is the register containing the result of a write that a is on that register so when you access the address of 64 on on X 22 is kind of actually fetch the a of 8 for us right and then finally when we wanted to store it back we are restoring it in what 12 write any number 12 so if you carry on going higher after 64 for of for 8 more so you're going to reach up to 96 right so now you want to update a 12 which is a 96 address of X 22 we are actually overriding that because we don't need that ok so guys there's a question here so I told you that reads 5 has 32 registers of 64 right and we call a 32-bit word first 64 bits called the dollar word so the whole board is now using all of that specific register right because the register X n whatever that is has 64 so each of them are 8 right 8 8 8 8 so these two we call it one word in other word the whole we call it double work I'm sure you didn't see it but yeah we call all of them so it's a temporary register that we we use it to add and then be overriding again so so we are loading we are loading the address 64 register each 22 and that corresponds to 8 so we're loading it to this so remember that this side is not available in you have to find it in a specific location in memory and load it and then use it again ok it still reads still double work yes or wherever it was it started perhaps if you if you're running and you're running in the middle of your code perhaps your base was like 2 million who knows but there's another point that shows your face ok all right so let's have a comparison about registers and memory by now you understood that we have 32 registers they're pretty fast we want to make sure that we are using them but many times in quite often we need to access memory right so in general registers are way way faster than a memory axis and as you just saw operating on memory requires loads and store so here we were trying to make some of these values available to to us so we had to just check on memory in a specific address and load them and store them again right and it was on the memory side so operating on memory does require loads and stores and tasks in order to do that with a very simple instruction that you saw more instructions should be executed to do this task and you know compiler domain they try to optimize the process to use registers as much as possible I was just telling you guys five minutes ago if you if you run your if you compile your high-level code like Java C C++ with with the general-purpose come compiler like Jesus the LLVM or until ICC many of the instances they are not using registers they're just using memory in order to run if you want to force your compiler first of all if the compiler was enabled to use registers first of all and secondly if you wanted to force a compiler to use the registers you need to instruct them with special instructions right in Intel that's why we we use SS e format it's a little bit lower than you know C or C++ but that is easy for instance this is the 4.1 4.2 make sure that some of the instructions are actually using registers right does anyone use V so far has anyone written in SSD just to let you know that sometimes when you wanna completely leverage your CPU just writing a Java code or a C++ code or a c-sharp code it's not enough most of the applications that you are using most of the commercial ones that are specifically optimized and tuned for specific hardware is right so those developers actually wrote in SSC if they were using in until you know hardware or using other intrinsic if they were using you know other than Intel so those codes look like more like assembly rather than a high-level language right if it's a little bit lower level than your high level C or C++ code right but you're making you're forcing compiler that use these specific registers for these right other than that some of the you know more advanced say programmers they just complete it right in assembly it's just when they want to write something I mean most of the application that you're using in your cell phone that they're running super fast and you're competing market they have all been reckoning assembly directly like they have people's people that they can understand assemble and in their right hand because register is inside your CPU module is killed within that reach but is further further away and your register is using the same eyes as your our CPU right you don't have to change or gonna have to access it just there it is much much faster it's just like you think ferrari is faster of course I mean it depends on the driver it depends on the zero in general the higher the language is yeah it is easier to work with but it might be slower unless you have a very good compiler that understands how to parse the program and you know produce that optimized code for you ok so let me give you an example for instance many of the Android phones Android phones that are you know produced by Qualcomm obvious not dragon phones they have a specific processors they call it accelerators and they're designed to run some of your machine learning applications like face recognition or smile detection so those software's have been written 99% in in in assembly by the specific engineers in Qualcomm or elsewhere that they run super fast that I mean for instance the smile detection or just when you take a photo it just you know process the your image in less than a fraction of a second so most of them are running in a special code they are not in high-level languages if you if you could have written those in higher blank but it would take like three seconds there as these optimized versions might run in 30 milliseconds right and they have to beat the market so for them it's very important to us to make it as optimized as possible so these are the advantages of you know working in lower level languages or just writing in high level languages which is more friendly and you know more beautiful one myself so this is the way it is okay all right so we're worried so let's talk about another set of operands we talked about some few examples of what our if mythic so let's talk about another sort of operands that that we want to focus on and they call immediate and that's what that I means hearing add so if we were to add two variables or two registers in variables so we would have used add right since but since one of them is is a number it's a constant right so there is a special instruction for that called add I and that represents that I'm gonna add for 2x22 and a story back here right so now you can distinguish between adding to a and B as variables or adding a plus four right and this is much much faster because constants are process much faster than okay and this immediate and other advantages it's gonna avoid a load instruction for you because you don't have to just worry you can just process it right away I mean they're not actually stored physically but they're embedded inside a cheap so when you want to when you want to come when you when you use that compiler that omits that code it is that what up code to output and what what binary childhood right and that binary is can be executable on that a specific hard way I mean in there is five we have only 32 registers and we have some up course you can have it you can have a look at it on the first does anyone have the book I can show you to the other guys nobody I'm saying yeah perhaps to rented it nobody has the book great so whenever you download the book you download the green sheet as well let the green card but you'll be the only user if you take a look at the if you take a look at the wiki page I guess I've sent some link for you guys to to perhaps have a look at it all all migrate those two to Moodle later on alright so we talked about the arithmetic operations we talked about immediate operations now let's talk about how are we gonna deal with sine plus and minus in binary integers right so as you know as you know we are you're dealing with the binary board right so he's either 0 or 2 and we are in order to you know understand what the value of a number is in decimal you have to convert it this way so say as an example here this this number 64-bit right so these three dots shows that there are all 0 up to do point and the last four are one zero one one and these two represents that this number is a binary number right if you were if you were to write one in in hex or in in decimal we would not just write like I don't know eight seven blah blah and then we would have put 10 here or 16 here or two this shows that this number is in that specific dimension right so now if you wanted if you want to convert this how are you gonna do that right this is a very intuitive formula since all of them are zero you don't have to do anything about them right it's n minus 1 multiplied two to the raise of n minus 1 so there are all 0 we have these last four so the very last one is always 1 it's zero total raise of 0 is always 1 I chose the last one which is this right this one shows this mine and the other two are that so the only thing in this specific example will Kinman be happy can't wait is the last 4 so 1 plus 2 which is 3 and 0 so this binary number actually means 11 right in decimal that's why I put that 10 underneath that as a subscript ok is that clear so now if you want to calculate the upper bound how did we come up use numbers right so if if you have these 64 bits right a maximum number that this space in show is what when all of them are 1 right compute that using it for me well you're gonna end up having this so that's the highest value a 64-bit system can present right so if you recall some years ago shorties perhaps it was in my time before before 64-bit systems we used to have 32-bit systems right so some of you and you had elimination in RAM usage right right so 32 systems I believe that the limit was like around 3 gigabytes of RAM so in order to use more of your RAM say you had even 16 gigabytes of RAM available but since your system was 32-bit your OS your perhaps or Windows you you wouldn't be able to take use of the rest of your RAM because the addressing wouldn't you know let it go so if you were to use all of that you had to reinstall it with a 64-bit OS I'm sorry so the hardware actually so we're supporting 64 at that at a time that was a OS limitation it was a Windows 32 bit but the hardware beneath was designed to you know work with both 32 and 64 right but if the heart that doesn't know and for sure you can't install it I mean now most could be but most of them support smoothly perhaps yeah yeah there are 120 28-piece but you need to either change completely changing new and divisor on is a and then after that you have to completely change the way you you assign and you search a memory your op codes would be higher larger because they are 128 everything is gonna be you know more complex I would say yeah because remember is everything is is completely logical here right so by changing one thing in the middle you have to change the whole spectrum alright so now let's see another another type of operations we can do with that so to complement or sign so here the largest not the the largest integer we could have shown because it was unsigned right we didn't we didn't care about the sign of that number it was always plus however if you want to make sure that we have signed values right how we're gonna represent that because we only have 64 bits so we need to at least find a way to use one of those or several of those to to showcase to the compiler and the hardware you need that this value is not a positive value perhaps is a negative one right but taking use of that single bit for instance in this case the the most significant bit the one so say you have say your hat so yeah that's 64 here right if you are if you don't care about the the sign so you can use them all right all of them give me one but if you want to take into account a sign number so what we're going to do here we take use of the first one and that's got to be the sign it and then we have now 30 63 left here right whereas we had 64 here so if it was zero or one that shows that the number is either positive or negative right this is a pretty neat convention we can use to define - and pluses however using this we're going to lose half of the upper bound we had right instead of that 18 really only Millian whatever the number was now we have nice from minus 9/2 plus nine is that clear why so now using this with that shiny beads we can show from this number the smallest number we can show in a 64-bit - the highest number we design value because we start from zero and that 164 which will start from 0 to 63 and again there when when we combine it is one less yeah yeah if you see it's 15 at the end yeah that that is eight and seven here so if you so I was talking about this let me find some space here so it's in in the old system when you didn't have I mean it's not just different conventions if you are in unsigned version so gonna say unsigned so you can take use up all of those 64 right if you are in sine mode what you have access is one for the sign bit and you have 63 because this is reserved right so now if this one is set one that shows a negative number so you have these values to play with if it is set to zero the other way around that shows the other side of the spectrum you have 63 beats two to nine is minus 9/2 plus nine that's why from here from zero to here is as if you go from minus 9/2 plus one plus nine okay so that's why here that number you saw these actually these two right so the most negative one and the most positive one so after we saw how we're gonna represent sign and unsigned let's let's see how we gonna play around with sign negation right so we are negating a variable so as if you have like 20 it's going to be minus 20 well you have a minus 20 it's gonna make it plus 20 you're just negating that right and that's what is what the you are show here okay in computer a picture when we say a compliment right a compliment means a 1 as turning to zero so zero is a compliment of 1 and the other way around and 0 10 to 1 right so for sign negation you're actually talking about compliment and add 1 and that's why when you add X plus X bar its own compliment it's gonna return it to minus 1 on the other hand if you add X bar plus 1 it's gonna return minus X right and if you want to show it here in binary that's gonna be how you do that so let me just wrap this up with in the next slide so for sign-extension so we have many conventions in in in different architectures there are many ways that people have done it but it's just a matter you know having a convention and just follow through on that right so in whisk 5 the instruction set is lb loaded by an lb u is for zero extent load byte so we're gonna talk on this more next session so just make sure on Monday after the class we have the first lap for those have to have a sign in lab01 and on Tuesday is for the last 0 2 meanwhile check your Moodle and find the pre labs on Friday yeah good night 
c-3AASq15pA,27,Architecture and Organization,2017-03-16T04:43:33Z,Lecture #1 (Computer Architecture),https://i.ytimg.com/vi/c-3AASq15pA/hqdefault.jpg,venkatesan ramachandran,PT1M43S,false,3034,25,5,0,N/A,computer architecture and computer organization computer architecture always refers the system attribute so these system attributes gives the direct impact on the logical execution of a particular program in same time if you are taking the computer organization always it refers the operational unit and their interconnections example how you can put the difference between architecture and organization the example we just considered the multiplication operation so whether your system will support or whether your processor will support the multiplication function or not that comes under the architecture for the same multiplication operation so how we are going to implement so that comes under the organization so if you are doing this you add a separate multiplication unit to the system or you are going to use the addition operation or addition unit for the repeater addition so this is how you can give the difference between architecture and the organization so computer architecture will have always refers the unreviewed that visible to the programmer so if the programmer chance changes any things in the attributes automatically it will give the direct impact on the logical execution of a program but here the compute organization always it refers the functional unit so how the functional units are interconnected how these functional units are going to be implemented 
GTlBpllJ2PY,27,"Part 4 : Computer Architecture and Organization -  Memory System 1,2,3
OPEN BOX Education
Learn Everything",2018-08-20T11:11:15Z,"Part 4 : Computer Architecture and Organization -  Memory System 1,2,3",https://i.ytimg.com/vi/GTlBpllJ2PY/hqdefault.jpg,OPENBOX Education,PT35M3S,false,592,5,0,0,0,memory system 1 learning objectives at the end of this topic you will be able to define memory system of the computer state the need for memory system explained memory hierarchy of the system describe the function of ROM and RAM chips demonstrate the connection between CPU and memory system outcomes by the end of this topic you will be able to apply the basic concepts semiconductor gram memories and draw memories in computer design identify the memory technologies in computer and be aware of the way in which the technology is changing [Music] memory refers to the physical devices used to store program or data on a temporary or permanent basis for use in computer it is a collection of storage cells together with associated circuits needed to transfer information in an out of storage what is the need for memory unit according to one Neumann all the programs and related data are supposed to be stored before it is executed and if the computer system is to execute instructions one by one receiving it from the user the speed of work would be slow so a fast memory device can be used to store a huge set of instructions together before it can execute a memory hierarchy in computer storage distinguishes each level in the hierarchy by response time since response time complexity and capacity are related the levels may also be distinguished by the controlling technology the three key features which are a constraint to the design of a computer memory a capacity access time and cost the memory must be capable of storing huge amount of data to achieve greatest performance the memory must be able to keep up with the processor the cost of memory must be reasonable in relationship to other components there is a trade-off among the three key characteristics of memory namely capacity access time and cost a variety of technologies are used to implement memory systems and across this spectrum of technologies the following relationships hold faster access time greater cost per bit create a capacity smaller cost per bit create a capacity slower access time as one goes down the hierarchy the following occur decreasing cost per bit increasing capacity increasing access time decreasing frequency of access of the memory by the processor main memory the main memory is the central storage unit in a computer system it is large and fast memory used to store program and data during the computer operation the principle used for the main memory is based on semiconductor integrated circuits gem is the buckling memory in a computer it is a computer storage location that allows information to be stored and accessed quickly from random locations RAM chips are of two types static dynamic static Ram SRAM the static Ram consists essentially of internal flip-flops that stored the binary information the stored information remains valid as long as power is applied to the unit it is easier to use and has shorter read and write cycles as SRAM is used in implementing the cache memory dynamic Ram D Ram the dynamic Ram stores the binary information in the form of electric charges that applied to capacitors it offers reduced power consumption it has larger storage capacity D Jam is used in implementing the main memory read-only memory ROM ROM is a type of built-in memory that is capable of holding data it is used for storing the bulk of the program and data that are permanently residing in the computer the ROM portion of main memory is needed for storing an initial program called a bootstrap loader the bootstrap loader is a program whose function is to start the computer software operating when the power is turned on since SRAM is volatile its contents are destroyed when power is turned off the contents of ROM remain unchanged after power is turned off and on again RAM and ROM chips a Ram chip is convenient to communicate with a CPU a bi-directional data bus that allows the transfer of data either from memory to CPU during read operation or from CPU to memory during write operation a bi-directional bus is constructed with three state buffers the output of three state buffers can be placed in one of the three possible States one signal equivalent to logic one to signal equivalent to logic zero three high impedance state let us consider the example of RAM chip the memory size of the chip is 128 into 8 bits per word this requires a seven bit address and an 8-bit bi-directional data bus the read and write inputs specify the memory operation and the two chips select CS control units are for enabling the chip only when it is selected by the microprocessor the read and write inputs are sometimes combined into one line labeled RW when the chip is selected the two binary states in this line specified the two operations of read or write the example operation of the RAM chip the unit is in operation only when C s 1 is equal to 1 and C s 2 bar is equal to 0 the bar on top of the second select variable indicates that this input is enabled when it is equal to 0 if the chip select inputs are not enabled or if they are enabled for the read or write inputs are not enabled the memory is inhibited and its data bus is in a high impedance state when CS 1 is equal to 1 and cs2 bar is equal to zero the memory can be placed in a write or read mode when the WR input is enabled the memory stores a byte from the data bus into a location specified by the address input lines when the Rd input is enabled the content of the selected byte is placed into the data bus the Rd and W are signals control the memory operation as well as the bus buffers associated with a bi-directional data bus the size of ROM chip consists of 512 by 8 by 1 2 into 8 the data bus can only be in an output mode since ROM can only read the 9 a trace lines in the ROM chips troll the memory operation as well as the bus buffers associated with a bi-directional data bus the size of ROM chip consists of 512 by 8 by 1 2 into 8 the data bus can only be in an output mode since ROM can only read the 9 a trace lines in the ROM chips specify any one of the 512 bytes stored in it the two chip select inputs must be CS 1 equal to 1 and CS 2 bar equal to 0 for the unit to operate there is no need for a read or write control because the unit can only read in this diagram RAM and ROM are connected to a CPU through data and a trespass the low order lines in the address bus select the byte within the chips and other lines select a particular chip through its chip select inputs the selection between RAM and ROM is achieved through bus line 10 the rams are selected when the bit in this line is 0 and the rom when the bit is 1 address bus lines 1 to 9 are applied to the input address of ROM without going through the decoder this assigns addresses to 511 to Ram and 512 to 1023 to ROM the data bus of the ROM has only an output capability whereas the data bus connected to the Rams can transfer information in both directions summary let's summarize the topic memory refers to the physical devices used to store program or data on a temporary or permanent basis for the use in computer memory system serve as a backup restoring the information that is not currently used by the CPU the three key features which are a constraint to the design of a computer memory of capacity access time and cost RAM is a computer storage location that allows information to be stored and accessed quickly from random locations the dynamic RAM stores the binary information in the form of electric charges that are applied to capacitors ROM is used for storing the bulk of the programs and data that are permanently residing in the computer memory system two learning objectives at the end of this topic you will be able to list the characteristics of the auxiliary memory devices explain the working of magnetic disks understand the working of magnetic tapes know the method of accessing associative memory describe the functioning of associative memory outcomes by the end of this topic you will be able to evaluate the need for storage standards for complex data storage mechanisms such as DBT understand the necessity of memory hierarchy to reduce the effective memory latency devices that provide backup storage are called auxiliary memory it holds programs and data for future use and because it is non-volatile it is used to store inactive programs and archive data they use for storing system programs large data files the important characteristics of any storage device are its access mode access time transfer rate capacity and cost access time the average time required to reach a storage location in memory and obtain its contents is called access time the access time consists of a seek time required to position the read/write head to a location and a transfer time required to transfer data to or from the device transfer data transfer rate the transfer rate is the number of characters of words that the device can transfer per second both magnetic drums and disks consist of high speed rotating surfaces coated with a magnetic recording medium bits are recorded as magnetic spots on the surface as it passes a stationary mechanism called a right head stored bits are detected by a change in magnetic field produced by a recorded spot on the surface as it passes through a read head magnetic disk magnetic disk is a circular plate constructed of metal or plastic coated with magnetized material all discs rotate together at high speed and stopped or started for access purposes bits are stored in a magnetized surface in spots along concentric circles called tracks the tracks are commonly divided into sections called sectors a track in a given sector near the circumference is longer than a track near the center of the disk disks that are permanently attached to the unit assembly and cannot be removed by the occasional user are called hard disks a disk drive with removable disk is called a floppy disk a magnetic tape transport consists of the electrical mechanical and electronic components to provide the parts and control mechanism for a magnetic tape unit the tape itself is a strip of plastic coated with a magnetic recording medium bits are recorded as magnetic spots on the tape along several tracks read or write heads are mounted one in each track so that data can be recorded and read as a sequence of characters magnetic tape units can be stopped started to move forward or in reverse or can be rebound the tape starts moving Violin a gap and attains its constant speed by the time it reaches the next record each record on tape has an identification bit pattern at the beginning and end a tape unit is addressed by specifying the record number and the number of characters in the record records may be a fixed or variable length a memory unit accessed by content is called an associative memory a content addressable memory see a.m. it considerably reduces the time required to find an object stored in a memory the instruction available in memories like cache ROM and virtual memory are addressed by content and not by address location hardware organization associative memory consists of a memory array and logic for M verts with n bits pervert each word in memory is compared in parallel with the content of the argument register the words that match the bits of the argument register set a corresponding bit in the match register reading is accomplished by a sequential access to the memory for those words whose corresponding bits in the match register have been set the key register provides a mask for choosing a particular field or key in the argument word only those bits in the argument that have ones in the corresponding position of the key register are compared suppose the argument register a and the key register K have the below bit configuration only the three leftmost bits are compared with a memory words of K because K has ones in these positions let us consider a is equal to 1 0 1 1 1 1 1 0 0 K is equal to 1 1 1 0 0 0 0 0 0 word 1 is equal to 1 0 0 1 1 1 1 0 0 no match by 2 is equal to 1 0 1 0 0 0 0 0 1 match the relation between the memory array and external registers in an associative memory associative memory of M verts and cells per word the cells in the air are marked by letter C with two subscripts the first subscript gives the word number and a second specifies the bit position a bit a in the argument register is compared with all the bits in column J of the array if a match occurs between all the unmasked bits of the argument and the bits invert I the corresponding bit M I in the match register is set to one if one or more unmasked bits of the argument and the word do not match mi is cleared to zero associative memory consists of a flip-flop storage element and the circuits for reading writing and matching the cell the input bit is transferred into the storage cell during a write operation the bit stored is read out during a read operation the match logic compares the content with a corresponding unmasked bit of the argument and provides an output for the decision logic that sets the bit in match register match logic the match logic for each word can be derived from the comparison algorithm for two binary numbers arguments a is compared with a bit stored in the cells of the words two bits are equal if they are both one or both 0 the quality of two bits can be expressed logically by the boolean function each cell requires two and gates and one or gate the inverters for even and Kevon are needed once for each column and are used for all bits in the column the output of all or gates in the cells of the same word go to the input of a common and gate to generate the mad signal for MI M I will be logic one if a match occurs and zero if no match occurs read operation if more than one word in memory matches the unmasked argument filled all the matched words will have ones in corresponding bit position all matched words are read in sequence by applying a read signal to each word line whose corresponding M I bit is a one in applications where no two identical items occur only one Verte may match the unmasked argument field in such case M output is directly connected to the read line for the corresponding word the content of the matched word is presented at the output lines and no special read command signal is needed right operation an associative memory must have a write capability for storing the information to be searched if the entire memory is loaded with new information by addressing each location in sequence if unwanted words have to be deleted and new words inserted one at a time for replacing a word tag register which has many bits as there are words in the memory is used for an active word the corresponding bit in Tag register is set to one and word is deleted from memory by clearing its tag bit to zero summary let's summarize the topic obsolete memory holds programs and data for future use and because it is non-volatile it is used to store inactive programs and to archive data it's a stored in the magnetized surface in spots along concentric circles called tracks a memory unit addressed by the content is called an associative memory a content addressable memory C a.m. camp it considerably reduces the time required to find an object stored in memory the match logic compares the content with a corresponding unmasked bit of the argument and provides an output for the decision logic that sets the bit in the match register memory system three learning objectives at the end of this topic you will be able to define cache memory and explain its importance explain the cache memory structure and it's working know the concept of virtual memory that involves defining address space and memory space explain page replacement algorithms explain the memory management hardware [Music] outcomes by the end of this topic you will be able to apply the concept of cache coherency and multiprocessor organization utilize the various methods of organizing cache memory and analyze the cost performance trade-offs for each arrangement analyze the need for virtual memory to the operating system according to von Neumann the CPU fetches each and every instruction with the related data from memory and executes set the instruction from the CPU domain memory is very fast and the same instruction from main memory to CPU is slow to overcome the slow transfer we introduce a cache memory which is placed between the CPU and main memory cache memory is intended to give memory speed approaching that of the fastest memories available and at the same time provide a large memory size at the price of less expensive types of semiconductor memories size of the main memory consists of 32 K by 12 and size of the cache memory consists of 32 K into 12 512 into 12 varies the cache is a small and fast memory how does the cache memory fat when the CPU needs to access memory the cache is examined if the word is found in the cache it is read from the fast memory if the Verte addressed by the CPU is not found in the cache the main memory is accessed to read the word a block of words containing the one just accessed is then transferred from main memory to cache memory the block size may vary from one foot the one just accessed to about 16 words adjacent to the one just accessed in this manner some data are transferred to cache so that future references to memory find the required words in the fast cache memory how do we measure the performance of cache memory the performance of cache memory is frequently measured in terms of a quantity called hit ratio when the CPU refers to memory and finds the word in cache it is set to produce a hit if the word is not found in cache it is in main memory and it pounds as a mess the ratio of the number of hits divided by the total CPU references to memory hits plus misses is the hit ratio the hit ratio is best measured experimentally by running representative programs in the computer and measuring the number of hits and misses during a given interval of time mapping of data from main memory to cache three types of mapping procedures are of practical interest when considering the organization of cache memory one associative mapping to direct mapping three set associative mapping associative mapping the fastest and most flexible cache organization uses an associative memory the associative memory stores boats the address and content data of the memory but a CPU address of 15 bits is placed in the argument register and the associative memory is searched for a matching address if the address is found the corresponding 12 bit data is read and sent to the CPU if no match occurs the main memory is accessed for the word the address data pair is then transferred to the associative cache memory if the cache is full and a trace data pair must be displaced to make room for a pair that is needed and is presently not in the cache Derek mapping the CPU address of 15 bits is divided into two fields the nine least significant bits constitute the index field and the remaining six bits form the tag field the future shows that main memory needs an address that includes both the tag and the index bits the number of bits in the index field is equal to the number of address bits required to access the cache memory in general there are two keywords and cache memory and two inverts in the main memory the n-bit memory address is divided into two fields K bits for the index field and n minus K bits for the tag field the direct mapping cache organization uses the N bit address to access the main memory and a key bit index to access the cache let us consider the example organization of direct mapping the index field is divided into two parts the block field and the word field in a 512 word cache let escape let us consider that memory address is divided into two fields k bits for the index field and n minus K bits for the tag field the direct mapping cache organization uses the n bit address to access the main memory and a key bit index to access the cache let us consider the example organization of direct mapping the index field is divided into two parts the block field and the word field in a 512 word cache there are 64 blocks of eight words each since 64 into 8 is equal to 512 the block number is specified with a 6 bit field and the word within the block is specified with a 3 bit field the tag field stored within the cache is common to all 8 words of the same block every time a Miss occurs an entire block of 8 words must be transferred from the main memory to cache memory set associative mapping it is an improvement over the direct mapping organization in that each word of cache can store two or more words of memory under the same index address each data word is taught together with its tag and a number of tag data items in one word of cache is set to form a set the fundamental idea of cash organization is that by keeping the most frequently accessed instructions and data in the fast cache memory the average memory access time will approach the access time of cache although the cache is only a small fraction of the size of the main memory a large fraction of memory requests will be found in the fast cache memory because of the locality of reference property of programs cash addresses when virtual addresses are used the system designer may choose to place the cash between the processor and the memory management unit MMU or between the MMU and main memory a logical cache also known as virtual cache stores data using virtual addresses the processor accesses the cache directly without going through the MMU a physical cache stores data using main memory physical addresses virtual memory is a concept used in some large computer systems that permit the user to construct programs pass though a large memory space were available equal to the totality of auxiliary memory each address that is referenced by the CPU goes through an address mapping from the so called virtual address to a physical address in main memory a virtual memory system provides a mechanism for translating program generated addresses into correct main memory locations the translation of mapping is handled automatically by the hardware by means of a mapping table address space and memory space an address used by a programmer will be called a virtual address and a set of such addresses the address space an address in main memory is called a location of physical address the set of such locations is called the memory space mapping using memory table the mapping table may be stored in a separate memory or in the main memory mapping using paging a page table the physical memory is broken down into groups of equal size called blocks the term page refers to groups of address space of the same size as block the basic components of a memory management unit are one a facility for dynamic storage relocation that map's logical memory references into physical memory addresses to a provision for sharing common programs stored in memory by different users 3 protection of information against unauthorized access between users and preventing users from changing operating system functions summary let's summarize the topic cache memory is intended to give memory speed approaching that of the fastest memories available and at the same time provide a large memory size at the price of less expensive types of semiconductor memories the performance of cache memory is frequently measured in terms of a quantity called hits ratio the fastest and most flexible cache organization uses an associative memory the direct mapping cache organization uses the N bit address to access the main memory and a give it index to access the cache set associative mapping is an improvement over the direct mapping organization in that each one of cache can store two or more words of memory under the same index entries virtual memory is a concept used in some large computer systems that permit the user to construct programs and so a large memory space who are available equal to the totality of auxiliary memory 
3UddrvigqYk,27,comparision between risc and cisc,2018-02-26T14:53:25Z,risc and cisc in computer architecture,https://i.ytimg.com/vi/3UddrvigqYk/hqdefault.jpg,Education 4u,PT3M57S,false,107002,1033,74,0,28,hi students welcome back in the previous video I explained about thesis processor andrÃ¡s processor now let us see what is the difference between the sisk and wrist so actually this is keys emphasis completely emphasis on hardware and whereas the RISC processer is emphasis on software so next since consist of multiple clock cycles multiple clock cycles to execute a single instructions if you want if because it is a complex instruction set computer so it needs a multiple clock cycles so that's why it's ignores the how many clock cycles it is taking just because for each instructions it requires multiple clock cycles whereas sisk take only single clock cycle for per one instruction to be executed so the clock cycle is nothing but it is fetching decoding and executing the instruction is called a one clock cycle so this uses multiple clock cycles whereas rest uses single clock cycle next coming sis consists of not or less pipeline so it may use may not choose the pipeline but when compared to the risk this uses only less pipeline structure whereas risk is highly pipeline highly pipeline okay and here in the SIS process assist uses memory to memory that means it also load and store incorporating instructions it take any instruction so this sisk will be performed on memory to memory a memory to memory whereas risk is registered to register registered to register and next coming to next point that is Sisk having the variable format instruction so the format instruction whatever it is using that should be a variable format instruction whereas risk always follows the fixed format instruction fix-it format instruction and coming to e one more different studies the system the complex instruction set computer uses any instruction may transfer may refer memory memory so any instruction whatever the instruction you are taking that refers memory whereas here in the RISC only load or store reverse memory so if anything wants to reverse the memory means for transferring or copying or something we that risk uses only the load and store instructions to refer memory but whereas in system the complex instruction set computer it may take any instruction to refer memory for referring memory the sistex any instruction whereas for risk takes only load or store instructions to refer the memory so these are the differences between the risk and this person thank you 
1jSDVbaKsrw,22,Computer organization and architecture lecture series by Rosna P Haroon,2020-03-20T08:19:41Z,Static and Dynamic RAMs,https://i.ytimg.com/vi/1jSDVbaKsrw/hqdefault.jpg,Computer Science and Engineering mentor,PT11M50S,false,1193,40,0,0,2,hello but Emily no no come on the static memories and in a memory sauna the semiconductor on cynical citizen occasion video Department under no oppose Ramson main it then the detail around Sun and Earth on the static memory una de enemy when Marissa started memories Michelle actually memories there consist of circus that are capable of retaining their state as long as the power is supplied are known as static memories a then a power and or power or not at all I did everything I wrote an Qing I mean I remember raising a number static member is in the parameter put a static Ram selling an an orphan you see they come on I'm a determinant come on so look at this figure this figure is actually showing the static Ram said happy from selling net organization yin and the pariah today then denote gates internal but no beta of inverters nope area here two inverters are there these are cross connected two inverters are actually cross connected to form a large large to know another storage on either under the same light act another anomaly then occur a box the site around cellular organization know what you know about the ball every cell another I am the inner and inverters a cross connected or larger the foam chilly corner about interconnection points along X Y and these latches actually connected to bit lines one bit Elaine is B and this is B - complement of B a B latch they run the bit lines likely connected at under through a transistor t1 and t2 be like a connect a take another t1 on the burner transistor ready be rationally connected a identity to know an eternal transistor it in it e two transistors are actually connected to a word line you Donna waddling a little configuration and I'm selling in an organ a see they can the two inverters a cross connected to form a large and this large is connected to two-bit the lines b and b dash through two transistors t1 and t2 and t1 and t2 are connected to it word lengths okay any engineer and Agnetha to read operation right to operational never come in the panel please read operation an American language namacarya I'm over sailing game over for Latin day in July or a sense and like a right to circuit is there a sensor to turn a read operation performs a another about the ball is an easily northern connected at the end of the line what since all right circuit will be there above a large scenic at the store is a with a cannery information in manana number now put like it done so this is the read operation initially word Elaine is activated to cross the switches t1 and t2 verdolino other than activity hourly know each other is runnable readily you've already number activity you know even little inactivating himbo actually eat transistor cause out here insist on an associate on a tortilla transistor is naturally acting us so just we've already Lane active una Samantha he switches in the him cross out t1 and t2 will be crossed say is in state to 1 if P is equal to 1 and B Dash is equal to 0 it would have no Kerrigan and it would be you then the complement a bearish one okay Celina gotta put a 1 under the power element will be 1 over Anna having a compliment as the one in the column and is a Europe me - no we're not I'm going on an agenda kill the sensor cute is actually outputting the value was 111 take it away the B is equal to zero and bearish is equal to 100 the cell is actually containing in a value 0 the same case in state zero this sense alright seclude at the end of the circuit transmit the state of b and b dash to the output length both we embrace your output upon an illegal operation say yeah about great operation Shaniqua rather than ever delay inactivate area to cross which is t1 and t2 the cell is in stage 1 if B is equal to 1 and P dash is equal to 0 and the solution state to zero if B equal to 0 and D dash is equal to 1 and the sensor could transmit the state to B and P dash to the output length so this is the read operation any rival to operational megawatt is the difference is write operation now and then a fair idea render the upper water value how to be returned a value B&B - don't go to cough then activate the word length which causes the switches t1 and t2 the cell will appear the state of billion B - above one nano to contain Guinness one non-story a negative being anyone go to come wait I shoulda see over the top where I didn't activate helium which will cross the searches t1 and t2 this way cell will appear the state of what miss aeronautica negative B is 0 in D - the one number that is the difference between another hidden right operations a static Ram cell in the performs area so the static Ram cell Innova rainbow atom in the configuration Beria how the read and write operation is nice we're discussing aboard deira the ROM is nothing but dynamic random-access memory now with linear immersed at the concern of another no dynamic branch we started prompts roller problem there's an apparent start a function of our new our dead and cross can either end node he said they're ending waters of course connected to larger forms a the tilde other than the transistor like an eternity they connect up over sanjana me : dieter Athena when did a corresponding ultra turn the transistor and a Petrocelli no utterances indeed two or three entrances a Savannah Papa transition course in the vegetal angle it is high so I expense the coracle and intercourse would inexpensive ayatollah Ramallah courses in the chapel on a dynamic Romano and ruins of to another dynamic actually placing one transistor with a capacitor compared to the transistor capacitor post is small so we can reduce the cause when we implemented a dynamic Ram with dynamic family actually the information is stored in the form of charge on a capacitor at the capacitor a lot a third chance in don't know create anythin at the zero or no one on a story they didn't agree Monica actually we are giving one threshold value I'm gonna show the one in a cardinal core of our new capacitor the charge and it is taken as zero either shoulder body regard important and a capacitor and recharge and it will be taken as say one buyer a pita or a dynamic remember every single transistor dynamic memory Solano charging mini Rica everybody or the power line under where people information you sell liquor store chain um then mengele appropriate voltage will be applied to the bit delay and that charge will be afraid of another positive and based on the capacitor value we can know the state of the same material depression on the burner mighty really as long as the power is afraid our data now would say listen data retention definite information is a story in a real capacity yeah Kiron a capacitor discharge electron did again so I'm gonna do voltage leakage of a limbo capacitor voltage Lakeisha of automatically our state right NJ yeah I reckon so tens of milliseconds pour in the room other wonder number of capacitor charge which I don't reckon that is a disadvantage of this case have no the capacity to store the data at in different amount of time so we need to refresh the capacitor every tens of milliseconds okay for the machine in a channel a probe in at the read operation of analyst read operation and I'm gonna other than a borderline activate that were across the stitches teeth that were across the Kansas City and we're checking the capacitor voltage capacitor voltage or the threshold voltage in Carlinville ornament that is taken us a zero as the capacitor voltage below threshold voltage on an and angle a initially we are actually we are discharging the full capacitor value and the state is taken as zero in Iyanla capacitor is no combo it is value is greater than the short run and the capacitor is fully charged and a value of one will be getting upon Union read operation Erica near the body while cooperation and angular a property voltage will be updated a bit claim and where lane will be activated for the appropriate is the number you know one day not responding high to the particular voltage and our children now a voltage normally bit line making a play chamber capacitor a trench Archer storoe that means a value 1 is stored near the wall is zero value in a story and angular island settler capacitor voltage and we are playing or the bitly Berlin will be activated so that value will be stored in the form of charge on a capacitor so always remember that in the case of a dynamic Ram information is stored in the form of charge on a capacitor very soon the value of the capacitor C based on the value of the voltage in the capacitor capacitor C we can so our you know freshness nothing but Tran system will be Tehran send such as the voltage of a capacitor if voltage is less than the threshold value the capacitor is discharged and it represents a logical 0 if voltage is a both at the shoulder value capacitor is charged due to full voltage and it represents a logical one that is a very operation if I get operation again nothing transistor is turned off and a voltage is applied to the bit delay for transistor annika a voltage will be applied to the whitley that information will be told and that information will be stored in the form of charge on the capacitor C so these are the two type of DRAM cell one is static mr. case of a static cramps when we are comparing from the bare surface kinetic lumps we compared it to the day in a hammock line because the number of transistors is more in the case of static Ram cell but in the case of dynamic Ram cell one of the transistor in the cell can be replaced with the capacitor so compared to the static parameters inexpensive but the problem is in the sacrum cell we can store the data you can retain the data as long as the power is applied but in the case of a dynamic Ram cell the information is stored in the form of charge on a capacitor other wonder than that every day like a personal education down information indefinitely to story and patina so we need to refresh the capacitor again and again they have written so milli circus animal capacitor charge you don't reckon up that is a disadvantage of this case but I am a ransom starter foundationalism on regarding whole step on regarding the retain of dead okay thank you 
W8n2h__arIY,22,Tutorial,2016-10-24T10:38:44Z,Computer Architecture and Organization,https://i.ytimg.com/vi/W8n2h__arIY/hqdefault.jpg,sunki kim,PT5M18S,false,18,0,0,0,0,good day everyone today i will discuss all about computer organization and computer architecture but before i proceed i just want to introduce myself first I am Jermaine Lewis 19 years of age a second year student and I'm taking up bachelor of science and information and technology at such a fickle state university of agriculture I made this video in a partial fulfillment of our project and computer organization now let us define what is computer and join computer organization first operational units and their interconnections that realize the architectural specific insurance except also architectural include the instruction set the number of bits used to represent various data types example of this is number character input output mechanism for addressing memory i have here an example of organizational attributes which is included those hardware details transparent in a programmer such as control signals interfaces between the computer and peripherals and the memory technology use example of computer peripherals is printer keyboard mouse flash device and etc ok let's come now to the computer architecture what is computer architecture computer architecture refers to those attributes of a system visible a programmer or put another way those at regime attributes that have direct impact on the logical execution of a program for example it is an architectural design issue whether a computer will have a multiply instruction it is an organization issue whether that instruction will be implemented by special multiplied special multiply unit are by a mechanism that makes repeated use of the ad unit of the system the organizational decision may be based on the anticipated frequency of use of the multiply instruction the relative speed of the two approaches and the cost in physical size of our special multiplied unit the distinction between architecture and organization has been an important one mini computer manufacturers offer a family of computer models all with the same architecture but with differences in our genetic short but sequently the different models in the family have different price at performance characteristics furthermore a particular architecture may span many years and encompass a number of different computer models its organization changing with changing technology a prominent example of both this phenomena is the idling and suspend 370 architecture this architecture was first introduced in 1917 and included a number of models over the years IBM hostilities mainly new models with improved technology to replace older models offering the customer greater speed lower cost or both these newer models retain the same architecture so that the customer software investment bust protected remarkably the system 37 feet 37 the architecture with a few enhancements has survived asleep as the architecture of IBM's mainframe product line in a class of computers call microcomputers the relationship between architecture and organization is very close changes in technology not only influence organization but also result in the introduction of more power tool and more complex architectures generally there is less of requirements for generation to generation compatibility for these smaller machines thus there is more under claim between organizational and architectural design decisions and that's all for today I hope you learn a lot and thank you for watching 
7gXw6w-HerQ,27,Describes basic concepts of Memory system and Memory hierarchy,2020-03-24T09:20:18Z,CS 202 Computer Organization and architecture-Module IV-Memory system-Asst Prof Shiyon P Johnson,https://i.ytimg.com/vi/7gXw6w-HerQ/hqdefault.jpg,Toc H Kochi,PT8M27S,false,142,N/A,N/A,0,0,we'll move on to the next module which talks about the memory system we'll discuss the basic concepts here the first is the maximum size of the memory the maximum size of the memory that can be used in any computer is determined by the addressing scale so let's say our system has 16-bit address line now how many addressable locations can this particular system have 2 to the power 16 that the 64 kilo bits of memory if our system has 32-bit address line then the maximum size of the memory of the system is 2 to the power 32 if our system has 40 bit address line the maximum size of the memory is going to be 2 to the power 40 that is one tera bits of memory so the maximum size of the memory is determined by the addressing scheme next is the connection between a memory and a processor before moving on to the description I would like to give you a review on the major registers that we have in a processor unit we have memory address register ma r which holds the address of the memory next is memory data register MDR which stores a data that has to be written to the memory so look at the memory unit here what is the maximum size of this memory unit it depends on the address line so if we have a cable address line the maximum size of the memory is going to be 2 to the power K let's transfer n bit of data between the memory and the processor this transfer happens over a processor bus this processor bus consists of three lines address line data line if we say we have a K bit address line that means the maximum size of the memory is going to be 2 to the power K if the data line is of n bit that means any bit of information can be transferred between the processor and the memory the control line coordinates the data transfer it has certain signals which helps it for that like example a read or write signal if this read or write is set as 1 that means a read operation happens if the signal is set to 0 that means a write operation is going to happen another signal is memory function complete signal so once the memory is done doing a particular operation it will assert a memory function complete signal so let's see when a processor reads a data from the memory what are the different steps so the first step is addresses plays it on the MA ID register when processor wants to read something from the memory that particular memory locations address should be store in a processor register next is the control signal that is read or write signal should be set as 1 because that denotes a read operation now how does a memory respond to this read operation the memory will place the particular data in that location it will place it on the data online also it will assert the memory function complete signal once the processor sees a memory function complete signal it will place the data what is there in the data line to a memory data register this is how a read operation happens similarly the write operation then processor writes a data to the memory first it loads the address of the location into the MA register processor wants to write something to the memory that addressed should be placed in the EMEA register the data to be written should be placed in an MDR register also the control signal read or write should be set to 0 because 0 denotes a write operation next we'll see certain measures which determine the speed of a memory there are two measures memory access time and memory cycle time memory access time is read time that elapses between initiating and operation and completion of a particle operation for example if you want to read something from the memory so the time between the read signal and the memory function complete signal given by the memory that time is called as memory access time next is memory cycle time it is a minimum time delay required between initiating two successive memory operations so the minimum time between two read operations as far as memory cycle time so based on the speed of measure especially the memory access time a hierarchy is designed so it consists of several units a processor unit secondary cache main memory and a secondary memory the fastest unit of the computer system is a processor compared to the processor the main memory is very slow so in order to transfer something between the main memory and the processor takes a long time to avoid the speed gap between the memory and the processor a new memory called as cache a memory is introduced the speed of the cache a memory is approximately similar to that of her processor unit the lowest level is the secondary memory like disk storage so in this hierarchy the speed will decrease and the size will in please from top to bottom so the secondary memory will have the highest space and the processor unit will have the highest speed next we'll move on to the semiconductor memories so semiconductor memories are available in a wide range of speed so their cycle time ranges from 100 nanoseconds to less than 10 nanoseconds that is the time between initiating and operation and completion of an operation that can be even less than 10 nanoseconds so the semiconductor memory is broadly classified into two categories volatile memory and non-volatile memory so what two volatile memory a memory which holds the information as long as DC power is applied random access memory is a volatile memory RAM can be divided into static Ram SRAM and dynamic Ram indira again the classification for a dram is a synchronous DRAM and synchronous deira non-volatile memory can be classified into a read-only memory and flash memory read-only memory has different classifications like programmable read-only memory erasable programmable read-only memory electrically erasable programmable read-only memory we'll go into detail about all these memories in the coming lectures so again in the basic concepts we learn about the maximum size of a memory which is dependent on the addressing scheme we discussed how a data is transferred between a memory and a processor we also discussed on the speed of the measures like memory access time and disciple time depending upon this speed of measures a memory hierarchy is defined in the in this design you can see which unit has the highest speed and which unit has the largest size so the rest we'll discuss in the next video 
pJ_XIpU8kH4,19,"Stanford EE282, ""Computer Architecture and Organization""
Lecture #3: Instruction set design and measurement
Professor: Len Shustek
January 17, 1995",2017-10-09T19:41:49Z,Shustek EE282 1995 01 17,https://i.ytimg.com/vi/pJ_XIpU8kH4/hqdefault.jpg,Len Shustek,PT1H13M26S,false,65,0,0,0,0,"hello welcome sunny day for a change there are three handouts one of them is an update of the information sheet labeled version two one of them is the next set of homework homework for chapter three and the third is another set of lecture slides how much would do today they're up here for people here and the science students I guess are sending the mouse over the career system a couple of announcements one is and you would have known this if you've been reading the class news group which you should be doing is that the review sessions will be held as advertised on Fridays they'll be physically here in skilling 193 but I say and if I can has changed the channel from e2 to e1 people out on the network should be aware of that the first review session is this coming Friday and it'll be an introduction to Vera log which you'll be needing for doing the first programming project another an outfit is that we still need graders we have one or maybe two we could use one or two more so if you're interested in grading you get not to do the homework you get to learn by reading other people's papers you also get the earn I think it's eight dollars an hour so if you're interested in doing that please see me or one of the TAS or secretary Lilly I'm reading exactly a fun thing to do turns out and if we don't get any graders I may be having a fun also the TA okay any other questions or comments or statements before we get started what we've been spending the last couple of lectures talking about is the design of computers in general in particular the cost and performance trade-offs of various parts of computer design trying to analyze when it is we ought to do something how much performance you can gain from it how much it costs you or might cost you to add a particular feature or change the design of an instruction set we're going to do now is bore down a little deeper into the design of instruction sets of themselves it's very important layer in the computer hierarchy which is basically the division between software of the computer and the hardware of the computer the first thing we'll do is try to figure out a way to classify the different instruction set architectures that have been designed and really if you look at commercial computers there aren't that many probably in the history of computers there may be are a few hundreds of instruction set designs that have actually been built not thousands certainly not ten some some number in the hundreds enough of them so that we can see the general categories into which they they fall when you look at them in certain ways so we'll do that and we'll identify or try to identify the advantages and disadvantages of various of the features of those categories of instruction sets and then we'll look in particular at a few in great detail and a few more in in lighter detail the example instruction set architectures we'll look at in detail are the VAX instruction set which is an example of a complex instruction set of the 8086 instruction set which is an example of a popular instruction set which may have no other redeeming features and and a typical RISC instruction set and we'll look at something which looks most like the MIPS although many of them are so similar so it hardly even matters although in fact we'll be talking about some of the elaborations on the various RISC processors that make them different from each other and then we'll look at some measurements of instruction set usage of those instruction sets in real computers on real benchmarks and see what we've learned from their design in their youth and and map it back to our general classification this is all great and good stuff and in fact designing instruction sets is lots of fun the bad news is you probably won't get to do one real instruction set design these days happens on the order of about once a decade for significant instruction sets now that didn't used to be the case it used to be the case that every new computer had a new instruction set until people recognized that the software burden that that implied was so outrageous that it was necessary to stop in inventing instruction set architectures at that rate it also even in these days of using C and C++ primarily for programming it also is the case that binary compatibility is more important than what once thought in the older days people would design a new instruction set and expect everybody to recompile their programs and and run them in a new instruction set architecture well one of the problems turned out to be that many people in commercial establishments wound up for the last five years running a program for which the source was now lost and you were trying to convince them to migrate their computer architecture from the old version to the new version there was no way that they could run the old programs because they no longer had the old source and in fact it was standards in those days for every new architecture to come with a simulator program which was designed to simulate the old computer in such a way that the programs for which the source was loved could continue to be run the problem of course is that in almost all cases simulation of one architecture on another is slower than the original machine even though the new machine by itself in its native mode is much faster this kind of technique actually still exists then you see lots of people trying for example to emulate Intel 8086 class architectures on other machines to get advantage of software that runs on those machines in this case it's not so much that the source code doesn't exist anymore but it's the case that the source code is unavailable because its proprietary yet they still want to run the binary instruction set the binary compatibility is is one of the reasons why e constructions that design doesn't happen very very awesome the other is that there is general agreement these days on water good instruction set architecture should look like and it's this kind of risk machine but I caution you into not to think that risk is the last word in instruction set design and in fact these things wind up like politics happening in waves and the current waves and current politically correct instruction set architecture are the RISC instruction set but in fact people are beginning to realize now that they too needs to be elaborated for additional features and additional performance and they're beginning to change and 10 years from now instruction sets won't be the same risk instruction sets being designs now I should also point out that if you were interested in designing your own instruction set architecture probably sitting in this room is a pretty good thing to do because if you look at the people who have sat in this room before a fair number of them have gone on to in fact design significant new instruction set two of my classmates taking 282 back in the 70s were the two primary architects of the motorola 68000 series one of at about that time I think probably took to 82 is one of the primary architects that son and was responsible for a lot of the SPARC architecture design other people associated with this course like John Hennessy whose textbooks were using was much involved in the design of instruction set so although the chances are you're not gonna get a chance to do one you're increasing the odds type by taking a class like this especially it's Stanford they're probably another half a dozen institutions around the world where that might be true so what are the categories or what are the criteria that we might use to classify instruction sets into different categories well since after the major one is how do you store variables when they're not in memory variable computers are in memory most of the time but they need to be in temporary locations for operations and the question is where are those swords well you couldn't choose could in fact choose to store them nowhere and have nothing except memory and a memory the memory architecture is an example of a machine that does exactly that the most popular over machines though are ones that had a single accumulator basically a single temporary storage register in which all arithmetic operations were done a third kind of computer is a stack based computer where instead of having a single accumulator you generalize that accumulator to a kind of structure that can hold multiple operands at once but in a very limited way you have access only to the one on the top of the stack well it turns out that that is fairly limiting so another more general way which turns out to be the most popular these days is a register set based which is sort of like a stack that you many manually manage that you have multiple registers all of which can store temporary variables and you access them in random random order another measure that you can use to apply to most of these is for each particular instruction how many explicit operands does that instruction get to name for a stack based computer for example you need to name know listed operands because they're all implicitly on the stack for some of these other architectures you in fact have a choice about the number of explicit operands or an instruction and we'll see what those are like another classification category is this risk vs. disk dichotomy which is how atomic or how complex are the operations that you're going to provide and that has significant trade-offs or both the ease of compilation and use of an implementation of high-performance computers which will see another issue is instruction encoding once you've decided on the basic organization and complexity of the instructions you've got to figure out a way to encode them that is both convenient and efficient branch architecture is becoming a more and more important part of the architecture classification because branches are one of the things that turns out and makes it very difficult to make high-performance computers if we only did not have to branch than many of the computers could be a lot faster than they are but of course computers make decisions and branches or the mechanism by which they make decisions and then we have to look carefully at the way that at that part of the architecture is designed and then there are sort of the more exotic features memory structuring features some of which like caches ought to be transparent to the instruction set architecture but sometimes or not and others like virtual memory and paging which are very much involved in the operating system level programming in addition to memory structuring the operating system is also concerned with the other features of the instruction set architecture that have to do with protection processes tasks and all of that mechanism let's look at one at a time the various construction architectures with respect to how they treat temporary operands the accumulator architectures were the original old machine type architectures there was typically a single register for holding temporary results often in fact there was a pair of registers but they had special-purpose function so in many machines for example there might be an accumulator whose width was the size of a word in the machine and then another register for holding multiplier and quotient parts of operands because multiplying two 32-bit numbers for example can produce a 64-bit number and there needed to be some way to store the entire 64 bit quantity but these weren't general-purpose registers they were explicitly used or rather implicitly used by instructions that knew that register pairs had to be used instead of a single register it was typical in these machines for each instruction to have only one operand so a sequence of instructions to produce the sum of B and C into a would be something like this these mnemonics are in fact 70-90 mnemonics from the IBM machine which is popular in this is clear and ad B which is to say clear the accumulator and then add to the accumulator B which is another way of saying load in fact it was exposed as clear an ad because that's the way internally the Machine actually implemented that function the instruction format simply had an opcode of a certain number of bits and a field which was big enough to contain the entire address of the operands V so it would load D from the address indicated by bits and that part of the meant instruction put it into the accumulator same thing for ad would fetch C added to the accumulator and store would take what was currently in the accumulator store it into the address like by the instructions notice that that this kind of fixed with instructions in simple decoding format is exactly the kind of RISC like instruction that we'll be seeing later and that you probably are already familiar with in many ways the very early machines can be viewed as a kind of RISC like machines one of the things that this didn't have that RISC machines do have are a whole variety of registers into which you can store temporary results in this case it only had a single register they also typically had additional fields for other things on the 70-90 this was a field that was used for example for indexing and there were in addition to these operand registers a set of index registers that could be used to compute addresses in the case where the address of the operands are not known at compile time or for accessing multiple variables which represent parts of an array so what's good about this well instructions are generally short an opcode plus an address and a few additional bits is all you need and they're often very fixed with kind of structures in fact on many of these machines memory was not even byte addressable so the instruction would typically fit into a single word which had a unique address and there was no subdivision into individual bytes it's easy to build these machines they have minimal internal state you I mean to the store the accumulator and the quotients and additional indexing registers so they're simple the problem is or one of the main problems is that the traffic rate to memory is extremely high there are lots of loads of storage when there are temporary results to be produced there's no way that's saved them unless you store them back into memory which takes an additional full instruction plus traffic to memory so it's it's a machine which works out well when the instruction execution unit is not that much faster than the memory but in these days when instruction execution rates and CPUs are so much faster than the memory having this kind of high memory traffic design is a bad thing to do nonetheless it was a very popular kind of design the IBM 7090 was the premiere scientific computation machine back in those days the dec pdp-11 of dex early many computers also designed for scientific computation that used this kind of design the mos 6502 was a machine that was one of the first microprocessor is on a single chip and was used in the apple ii and was probably at that point the single most successful computer architecture in terms of number of computer shipped today well if one register is good maybe no registers is better and in fact the stack architecture is one example of architecture which has no registers everything is done in memory in a very kind of stylized fashion I'm sure you all know how effects work in order to evaluate the expression B plus C times D you push beyond to the sack address back here then push C onto the stack and push D on to the stack back is growing up multiplying now notice that multiplied is an ALU operation but takes no explicit operands in the instruction at all implicitly the operands for multiply are the two operands that are currently on the top of the stack so multiplied takes d and c wipes them off the stack and replaces them with the result of the ALU operation C times e same thing for red it takes the current top two elements of the stack which is C times D and B replaces them with a new top element of the stack which is B plus C times C and then pop a takes whatever is in the top of the stack stores it into memory this is equivalent of the store operation plus removing things from the stack if you want HP calculators at least the older HP calculators this is familiar to you with the kind of reverse polish notation my point of view it's not a desirable architecture for a calculator it doesn't happen to fit the way my mind works because I grew up and most people did grow up with operator precedence languages rather than reverse polish languages one of the advantages here well very short instructions are possible at least some of them the ALU operations have implicit sex addresses and no explicit memory addresses so so you wind up getting the effect of having multiple registers without either having to name the registers or having to load them from memory the compiler is easy to write the technology for taking an arbitrary priority priority operator expression and turning it into a kind of reverse polish is it's pretty trivial so that's the good part what's the bad part well the code is very inefficient one problem is that there's no way to do a very it's very difficult to do optimization that compilers like to do like common sub-expression elimination if for example I had an expression that was an array reference with a subscript like I times 3 and then I'm going to operate on that with another reg reference with an identical subscript it would be nice to be able to compute this item 3 1 save it somewhere so that it can be used two different times for the two different array references well there's no convenient way to do that in a stack architecture you could compute it you could store it into memory and then load it back into memory but that's a lot of memory traffic and might in fact be more than you save doing the computation well you can fix that there have been proposals and implementations of stack architectures that allow you for example to compute an intermediate expression to duplicate it so you'll wind up with two copies of it on the stack and then in fact later on to randomly access stacked values if the stack value that you want happens not to be at the top of the stack but it was a pushed value that you saved from earlier you might be able to have an ALU operation that says don't add the top two elements of the stack but at the top element of the stack and the stack element underneath the next element well what you're really doing is creating a random access to the top few elements of the stack which begins to look much like a multiple register implementation and in fact that's what it is the other problem is that the stack itself is often a bottleneck and if it's implemented by in the normal fashion by having simply a stack pointer as a register and the stack itself in memory then we're back to the case where the stack is a bottleneck by being a memory access well you can fix that too there have been very clever implementations that treat the top few elements of the stack as special by putting them in registers basically by cashing them into a high-speed storage rather than putting it into memory but it gets very complicated because the stack unlike a register set has an infinite number or unbounded number of locations so it's very difficult to design the Kings which efficiently dump the stack to real memory when it gets too large and pull it back when those pops have been done that require old elements on the fact though the problem is that it's often the case that a single stack isn't enough we've been talking about a stack primarily for expression evaluation even that has a problem because that stack may be required to hold different types integer versus floating-point operands versus character operands and then what do you do when you say add if it's a floating point operator operand and a fixed point operand on the fact there's the add instruction though that those are two different types to be marked with a stack data with a tag to indicate what type it is begins to get very complicated in addition there are other kinds of stacks there's a stack that's used for control transfers for doing subroutine calls and returns and then there's another kind of stack which is used for variable storage in modern languages that have dynamic storage allocation and procedure entry or that allow recursive procedures you need to have some kind of a stack like structure in order to be able to hold those local variables well here are three different stacks are they the same stack are they different stacks back architectures quickly begin to become very complicated there aren't too many examples of successful stack architectures and none of them are pure the the original most purest one is probably the Burroughs 5500 machines and these machines had limited popularity notice that although generally speaking we've abandoned stacks for the use of expression evaluation facts are still in use for control and for variable allocations or for functions although generally speaking the instruction set architecture itself doesn't have anything in it to support those stacks except perhaps a simple push and pop type mechanism most of the complication of that stack architecture for control and function variable allocation is done by the software one example of a machine where a great deal of that kind of stack manipulation was in fact embedded into the instruction architecture okay here's another example of an architecture that has no registers registers are complicated let's get rid of them memory the memory architecture every operands reference is explicitly noted in the instructions and in fact you can have either two or three listed operands for instructions so one way for example to implement a equals B plus C is to have a simple machine language instruction that does add and names those three variables one of the problems with this of course is that the instruction gets very long because this may be say three 32-bit values to specify the three operands a compromise might be to allow only two operands for instructions in which case you wind up having a destructive ALU operation so in order to get the equivalent of equal V plus C you first have to move one of the operands to the target location move B to a and then add into a which is now the value of the D was C to get the sum of the NC into a another alternative so what are the attributes of this well the instructions are very long but on the other hand the code is very compact because there's no waste for things that really don't have any correspondence with the source language program after all the register designator in fact I remember people writing papers decrying register based machines because registers were arbitrary things foisted on the compiler writers by the hardware designers and they had no correspondence to anything that was in the source code why are we bothering with them and why do compiler writers have to worry about them all the time well in fact there's a good reason why they have to worry about them and that has to do with making efficient programs but the easiest kind of compilation in the most compact code because it doesn't reference any of those registers is this kind of memory to memory architecture but again memory is an extreme bottleneck you don't wind up having registers that save temporary values that you can use for common sub-expression elimination and those kinds of optimizations so these machines are not popular anymore certainly there was a time in the old days when they were fairly popular but mostly for non binary arithmetic it was very common in fact the stew was coming to some extent now to have data types that allow you to store directly decimal arithmetic in fact in those cases the size of the data is unknown at compile time or even at execution time in a particular number might be n bytes wide which makes doing stacks or register machines extremely problematical because you don't know how big to make the register or how wide to make the entry on the fact so in fact memory to memory architectures are perfect for that sort of thing the IBM 1620 was a BCD machines primarily for scientific computation that had memory the memory architecture the IBM 1401 was the equivalent side of machines for doing business operations which also used the BCD and even in more recent machines like the IBM 360 and in the backs memory the memory architecture instructions tend to be a subset of an existing register like machine in the case of the 360 it was used exactly for this reason to do SS instructions or either strings or for vegetable data and in a complex instruction set like the Dex packs which we'll look at in detail memory the memory architecture is used even for regular binary operations and I would say a convenience to the programmer well one of the motivations for decimal number storage was that there aren't any rounding errors and if you try to represent certain numbers in binary they turn out to be repeating fractions which they aren't in decimal in fact the business community was very excited about not losing that last tenth of a sense in any of their computations and in fact did and still do do a lot of their computations in binary machines are ubiquitous now but that wasn't necessarily the case early machines often were a decimal or by quinary or other base number systems in fact easier than the proposal back in the early days to do a computer based on base 12 numbering systems and there was a group called the duodecimal Society of America who proposed that not only should you design computers based on base 12 but we should all be doing arithmetic in base 12 and they had some interesting propaganda demonstrating why base 12 was a much better number system than base 10 and we shouldn't base our arithmetic on the fact that we happen to have ten fingers they weren't too successful and neither were base 12 computers the last style we'll talk about turns out to be the winning style and the architecture that most current computers are built out of and that's the register based architecture and the idea is to have several where several can be defined as any number from four to 64 or 128 of mostly equivalents accumulators and this is very much like to the accumulator style architecture we saw before except if there are more of them and you have to say which one which ones you want one way of viewing registers is that they are very much like a cache or recently use values and the difference between registers as a cache and the cache memories that we'll see more of and have seen before is that registers are exclusively managed the programmer or the compiler needs to indicate when it is things should be brought into this register cache and and dumps from this register Catherine cash back into main memory but that's really all they are is a way to get access to a small amount of very fast memory which can be addressed with a very small field in the instructions rather than a big field as you need for a memory operand given the register architecture we there a couple of options we can choose one is we can choose to have two or three explicit operands for instruction as we saw before in the memory the memory architecture you can add two things and put it in the third location or you can add two things to something that's already there and replace that's something that's already there which has the effect of destroying an operand but the instruction is shorter because you're only specifying two things instead of three things another choice you get to make is whether or not ALU operations can have memory accesses and typically for the Syst machine for the complex instruction set machines the answer was yes so you could write an instruction sequence like this for a equals B plus C load into register 1 the memory location B add to register one destroying register 1 notice the contents of the memory location C and then store register 1 into 8 short 3 instruction sequence most of the RISC machines and we'll see reasons why later don't allow ALU operations to reference memory so what you need to do is in fact expand that into the following sequence of loading two separate registers with the two memory locations and then adding those two things together into typically a third register for RISC machines and then storing that into memory locations we've added an additional instruction you've now simplified the add instruction Cystic note it denotes only registers for its operands resident memory location which gives you incidentally more space so it's typical for a RISC machine to make this choice which is no memory operand and this choice which is three operands four instructions and we'll see some examples later of why this is a fact a very useful thing to do as I said register-based architectures are the thing these days it's the dominate architecture from everything going back to the 60s with the first arguably first crisp machine the PVC 5600 the IBM 360 which is certainly the most popular mainframe machines pdp-11 one of the best mini computers 58,000 and all current risk machine advantages we've mentioned some of this before it allows fast access to temporary variable it permits some people would argue required compilers to be clever and do clever optimizations to manage this cache of recently use values and because there is a cache as in all caches it reduces the traffic to main memory what are the disadvantages no instructions tend to be longer because you've got these fields in the instruction that now have to designate which registers are being referenced so it's instructions are longer than accumulator designs they're shorter than memory to memory architectures all these things the other problem which can be a big problem or a small problem is that now context switching is more expensive remember though all computers these days run multiple jobs or multiple tasks at the same time and periodically have to switch from one to the other that's switching involves saving all of the machines states that currently is not in memory well on a register to register or a registers set based architecture that machines state and keeps all of the registers so any time you take an interrupt or switch from one task to another you've got a dump all of those registers into memory reload them before you can switch to the new process or for the interrupt routine we'll see some examples of RISC machines namely the SPARC architecture which has taken the number of registers to an extreme to a point at which context switching becomes an extremely expensive operation this I think we've actually talked a bit about the question is for general purpose register ALU instructions what are the operand types but you can choose to put memory to put no memory addresses in the ALU operations in which case their register to register only it typically requires bigger encoding but simplifies register allocation because you don't destroy a register by doing an operation which makes it very easy to do cogeneration and as we'll see later it makes it very easy to pipeline the computer which is a technique we'll look at in detail for speeding up the execution the downside is that there's a higher instruction count typically because you need to have a separate instruction to load the memory operand into the register the alternative is to have or at least allow a single memory address in the instruction it's typically the case that these kind of register to memory machines like the IBM 360 also have register to register operations so in many ways the one memory address architectures are a superset of the no memory address architectures what are the trade-offs in deciding about registers or the lack thereof all of these architectures though have to access memory somehow and what we do now is go over some of the various different ways in which you can specify inside an instruction what memory location to reference or if it's not a memory location where else the operand is coming from of this list the first two are non memory operands and the rest of them are memory operands and the simplest non memory operand is a register designator very simple to encode in the instruction you put a few bits in the instruction indicating which register number the operand is in fact the contents of that register or if it's a destination means that the destination value should be placed in that register every one up the next non memory kind of addressing mode is an immediate instruction in code within the instruction itself the value of a constants to be used as a source operand and obviously this the source is only not destination you can't store into a country this has a problem or at least some considerations one is what's the type of that country is that type implied by the opcode well probably is but what about the length of that country is the length of the country applied by the up code it could be or maybe it's not maybe you need additional specifiers indicating the value of the immediate field the advantage of immediate is that it saves having to load a constant from memory you can design a computer which has absolutely no immediate instructions whatsoever and anytime you need a constant value that we load from a memory location which has been preloaded with that conference will see later on from some instruction statistics as if from a performance point of view that would be a poor choice because many many instructions use immediate fields and therefore save an additional reference to memory okay what if we want a reference memory well a direct memory address within the instruction was the simplest thing to do you simply put inside the instruction the absolute address of the memory location to reference popular for very simple kinds of programs useful for getting at global data but it turns out that in fact much of the data in modern programs and modern programming languages is not at fixed locations and is dynamically accessed and is not known at compile time or even at length or load time so this is not typically a very popular kind of memory access mode more popular than that is register indirect where the contents of the register contains the address of the memory location to reference you'll see indirect and deferred register deferred so somehow at execution time you've got a ring arranged to put within that register the address of the memory location you want to reference displacement addressing is a variation of that where in addition to taking the contents of register 1 you add to it a constant which is in the instructions so in fact register indirect is a subset of displacement modes where the displacement is 0 this kind of addressing is very useful for example for references to a stack if you've done as I alluded to before variable allocation on the stack because you have to do that a procedure entry if r1 is your stack pointer then 100 might be the offset from the current stack pointer indicating the variable to be referenced the next elaboration over that is adding the contents of two registers together either with or without a displacement which is to say either with a displacement of 0 or not nice actually I said that wrong you can do index in several different ways one is that you can add the contents of two registers together the other way is that you can add to the right and in fact the notation is wrong here well it's more complicated than that actually index there can be a variety of forms of indexing one is that you take the contents of two registers and add them together where might you want to use that well let's say that you're accessing an array on the stack where r1 is the stack pointer and r2 is the computer offset into the raid array that you want to access that variable that what I had originally written here was a large country meant to meant to refer to a memory address and then index that memory address by r2 one of the questions about displacement for my addressing is how big can that displacement be is that simply a small constant indicating an offset off of register one where register one is really pointing to the memory location or can that displacement be as large as a memory address in which case you get this form where you're adding what probably is a smaller number and index in r2 to a larger number which is a constant in the instruction which is the base memory address of that variable really these two are the same kind of memory access modes and the difference is whether you consider to be considered a constant to be a small offset or the constant to be the address of the memory variable itself or at least the base is a memory variable in any case both of these schemes basically are the way to take one or more registers add their country contents and add to that a constant which comes out of the instruction I hope I wasn't too confusing there any questions about the next elaboration is scaled indexing and that takes notice of the fact that for doing array indexing if you're doing something like a sub i if a is a four byte array which is to say it's a it's a collection of elements each of which is four bytes long and you want to reference a sub i you can't just use one of these index for displacement modes and put i in the register and use a let's say as the displacement or as the base of the variable because you've got the first x four now you can do that in explicit instructions you could load i multiply it by four to get to the byte offset within that array and then use one of these other forms to add the base of the address well scale does that for you and in fact it's typically a memory mode where you add a constant from the instruction which is either a small offset or could in fact be the base of the array you add another register which might be let's say the fact pointer if this is an array that based on the stack and then you add a third quantity which is a register containing a subscript subscript expression and then the instruction automatically multiplies it by the width of the elements in that array and typical architectures that allow scaled addressing allow that number to be one to four or perhaps eight notice that it's almost always a number which you the power of two so in fact the implementation of that instruction and the hardware can be a simple shift of that register before adding it to the other register memory indirect is a way that says that once you've done any of these other things you can't in fact say that that memory location is itself an address and not the operand to be referenced so that says for example in this case that register 2 contains the address of a memory location go to that memory location take its contents presumably a word size quantity and use that in turn as the the memory location a very good way to do indirect tables some kinds of pointer lookup that sort of thing notice that in fact it can be and it has been in architectures that the location that you reference with that first memory specifier can itself specify something in memory which is a memory specifier so instead of just being the address of the address of an instruction address or the address of an operand address it could be the address of something that looks like a little bit of an instruction which itself says how to do the next level of memory and direction in fact there are machines that were designed that allowed a single instruction to take an arbitrarily arbitrary number of cycles by chaining through memory to look for subsequent in memory indirect until it found a memory addressing mode specifier that says this is not indirect you can stop now and exit your upper end makes the instruction extremely hard to pipeline last kind of memory address mode I'll talk about the auto increment or decrement one where you use the contents of a register to point to a memory location but then either before or after referencing that location you increment or decrement the contents of that register useful for stepping three array for example in particular if the addition is done not necessarily by one but by a value which is the size of the operand being referenced then you can step through elements of an array without ever having to increment the pointer the VAX from the pdp-11 we're famous for these kind of auto increment or decrement operation notice that it's very typical for auto increment to reference the memory location through the register first then increment the register afterwards and for auto decrement to do the reverse to decrement the register first then to access the variable that that register point to why anybody know why those things should be designed asymmetrically exactly that's the answer for sex if you're trying to use that register as a stack pointer auto-increment allows you to push something on the current well you can do it either way but let's say Auto decrement allows you to push something on the stack by first decrementing the stack pointer then storing the location there when you want to pop something from the stacks the stack pointer is pointing to the last pushed element on the stack so you pull it off the stack first then you increment it to remove that item from the stack so these work well in tandem to implement a stack using any general-purpose registers in addition any of these memory addressing modes in theory could be used in pc-relative form in fact computer 11 the PC is just one of the general registers so any of these instruction for instruction addressing modes if they specify the register which happens to be the program counter does addressing relative to the program counter which allows you to do things like positioning the Kendon code whereas the references to the data moves along with the code when it moves in memory obviously the complex modes are all useful in some instances and they reduce the instruction count when you get to use them but the cost of implementation of these modes is extremely high built-in to an instruction architecture yes these are built into the instruction architecture and in fact built into the encoding of the instructions one of the things you've got to in shoes and that's the next slide in designing your instructional architecture is how in the encoding of the instruction to indicate to the processor which of these instruction modes is being used and those are modes are in fact executed by the hardware implemented by the hardware with that that's a sense of your questions with the direct reflection of the microcode the instruction set for well you're asking a couple of things at once there when you talk about micro coding you're talking about a particular implementation techniques by which the hardware might choose to do these instruction modes but the general answer is yes these are if you choose to view them like the VAX did those are in fact the instruction modes that the hardware somehow has to support it's not just a an artifact of the software and that's what you're asking yes that would be hardwired in you would have to indicate in the instruction that what you wanted to multi multiplies by was for and that could be implicitly part of the opcode or it could be separate bits in the instruction somewhere that indicates that you want to do that but that has to be in there somewhere so this hardware knows which of those modes you want to use in fact that's talked about a little bit here that in encoding which of those addressing modes you're using and you could implicitly put it in the opcode or you could explicitly when there's a large number of modes use a separate byte or field within the instruction to indicate the address access technique that you want to use I think we'll skip over now this is an embarrassing slide it's an embarrassing slide because we have to talk about it because it exists in the real world but it shouldn't be the case this is this issue of big endian versus little-endian memory layout cause no end of headaches because computer designers in their infinite wisdom have made two different choices for what essentially represents an arbitrary convention for storing multibyte pieces of data let's say that we have a word which is meant to contain a 16-bit value which in hexadecimal was represented one 1 two 2 three 3 four 4 now if this were a word oriented machine where each word contains 32 bits then there's no problem about memory addressing that word has one address and the no issue of which part of the word it's addressing but all current machines are byte addressed in which case you've got to decide when you divide this into bytes which order those bytes should be stored the little MV ins stores the littlest end of the word first and then ever-increasing bytes of the word in that direction so location X is 44 X plus 1 33 and so forth the big-endian Convention does exactly the opposite the first byte is the first most significant byte of the word and they decreased from there it really doesn't make any difference unfortunately both conventions have been used by popular machines it doesn't have much of a performance effect it does have a major effect on exchanging data between computers that use the two different formats and interestingly enough it is not something that's easily fixed by recompiling because it's very easy and in fact common for people to write C language source codes that depends on memory convention used for storing large operands one example of that is what one example are unions anytime in C you write a union that overlays smaller items with larger items than the order of the bytes within that larger item becomes significant because the smaller items will different another simple expression that demonstrate that demonstrates that is that if I have a pointer to a long integer for example and I recast that pointer as a short integer but this does is say that P normally points to a long integer let's say a four byte integer I want you to interpret P as a pointer to a short integer and take the contents of that short integer well that takes the first two bytes of the variable 0.22 by P which might either be 1 1 2 2 or 4 4 3 3 depending on which machine you happen to have compiled your C code for so it's a it's a major headache and architectural non non issue but a practical practical issue I see people still bringing a pen hum works here which is fine remember the home roots are due into class today this is an issue which I've beaten you up about the four make one final attempt that convincing you that it's important provide enough address bits in your instruction set architecture or the expected a lifetime of your machine remember that it grows at about two bits every three years or maybe more due to the T - 1962 and there are lots of good examples of this transition that was done extremely painfully sometimes in multiple steps the Intel 8086 first came out it's actually the 8080 came out with 16-bit addressing extended later to 24-bit addressing actually extended from the 8088 16-bit addressing to the 220 bit addressing in the eighty-six 24-bit addressing in the 286 the 32-bit addressing in the 386 and every one of these transitions was extremely painful to me IBM did it in having 24 bit of dressing for the 360 later extended the 32-bit addressing which they originally thought would be an easy transition turns out as I mentioned in previous lecture that have been a harder transition than they thought I think people are getting the idea and even machines now that are designed with 32-bit addresses like the MIPS r2000 were designed in such a way that the extensions of 64 bits of addressing in a machine like they are 4000 is a lot less painful than any of these other transitions and whether 64 bits will really last for another 40 years anybody's guess floating-point operands wanna go over this briefly basically you have several different choices one is that you can choose to use the same register file that's used for integers for floating-point operations well that's useful because you don't have a have to have a separate set of registers so it takes less hardware which means that you can use that chip area for something else for making your machine go faster or caches or whatever it's simpler the program because you don't need to move from the integer register files to the point register file it's only to do convert if you have a separate register file though you may be able to get higher performance because you can have special-purpose data paths for the floating-point execution units that go directly to the register file and essentially what you're doing is giving a private register file to each of the execution units one to the integer unit one to the floating-point unit which gives you a higher bandwidth for registers as a whole a third approach is in fact not to use a register file at all but to go back to a stack design remember the stack design is reasonably good for certain kind of expression evaluation and floating-point operations are mostly expression evaluation so why not use fax intel 8087 is an example of an architecture which is basically a stack machine with again some of the elaborations we talked about that stack machines find convenient to do what amounts to some kind of register memory instructions for that so they major point is that the choice for floating-point operations and the choice for fixed point operations and don't have to be the same you can make those independently okay branches branches are becoming more and more important because of the performance effects of being able to implement branches successfully so let's look at three different ways conditional branches might be arranged the first one in the one which probably dates back farthest in time is condition code the condition code is essentially a slow register it's a piece of state information in the machine that's set by well maybe not all but by most operations it's often done as a side effect of an instruction let's say you do an add instruction you might set the condition code implicitly to indicate whether the result of that add instruction is positive or negative or zero condition code and then the following branch instruction let's show how that works the following branch instruction would test the value of that condition code so let's look at for example a condition code machine architecture you would do something like compare register 1 and register 2 that would that would implicitly set the condition code saying whether those registers are equal or which one is greater than the other and then you might do in a subsequent structure the branch greater than or equal to 2 some locations well one of the problems about this is that there is now some hidden state information namely the condition code that is valid within these two between these two instructions condition code here must be preserved and we'll see later that one of the techniques for increasing the performance especially a pipeline machine is the ability to reorder instructions and in fact to insert between two instructions like to compare in the branch other unrelated instructions that basically fill up time it's very difficult to do that in the condition code architecture because chances are the instruction that you're going to insert in here is going to destroy the condition code that was set by the compare nonetheless condition code was and still is a very popular scheme for doing branch instruction certainly the IBM 360 370 series had it the Intel 8086 architecture has it the backs at it and even a risk like machine the SPARC architecture has it although in later variations of the SPARC they make some changes that allow you to avoid the use of conditions one of the problems with condition codes is that you've got to be very careful in deciding which instructions should set the condition code and which instructions should not example in the 8086 architecture Inc and Dec the increment and decrement instructions do not fit the condition code anybody guess why they might have chosen to do that it has to do with not destroying the condition code when it's in use for something else the original heritage of this instruction set had very small memory operands one or two bytes and if you wanted to do multiple precision arithmetic you had to preserve the carry instruction from one ad to the next so if you were adding four bytes you had to do with two bytes at a time in order to address the next set of bytes you had to do the increment and decrement or the pointer to be able to point to the next set of bytes or set of words to be added if increment and decrement destroyed the condition code you would lose the carry information that you had saved by doing the first edge so even you couldn't use increment and decrement for pointer arithmetic to exit the operands or you had to save and restore the condition code around the increment and decrement so their choice was for example not to have the increment and decrement change the condition code makes this kind of architecture difficulties the alternative is to put conditions in a general register what that basically does is take the condition code out of this sort of magic implicitly set state information and put it into a register just like all other state information or most other state information where the programmer has explicit control about where it goes and when those conditions are set so going back to our example if we had a condition code in register machine you might do something like prepare again register 1 and register 2 but this time puts the result of the comparison that say 0 or 1 in the third register and then sometimes later you might say or in the following instruction you could say branch if register 3 is 0 to whatever location notice that there's no hidden state information in between these two instructions which allows you or allows the compiler to schedule other instructions within there that don't affect the sequence of the compare in the branch one of the disadvantages of this compared to the condition code architecture is that it winds up for using up a register you just wasted a register that otherwise you might not have to use but these machines that implement condition code and register architecture tend to have lots of registry like the mist from the deck Dudek alpha a third kind of architecture is one that allows you to basically combine the compare in the branch in a single instruction rather than having to save the result of doing the comparison either in a register or in a condition code you just do it all at once and you do something like say we have a compare branch architecture and do a single instruction that does something like compare and branch of equal by comparing register 1 register 2 if they're equal branch 2 location well this looks great a single instruction save space might even be faster to execute but it's a more complex instructions in fact this is a case that we looked at in an earlier lecture where we decided to try to combine an ALU operation and a branch instruction into a single complex ALU and branch instruction the result in most machines will be that you have to lengthen the time it takes for instructions to execute so certainly it's not structured to execute and it may not be a worthwhile trade-off there aren't too many machines that have these kind of complex branches especially when the comparison operation is general the VAX is an example of one that does regress all right let's talk a little bit more in general about risks versus ifs I'll go over this quickly because we've probably talked about it in various bits and pieces throughout generally speaking risks architectures have fixed instructions they're typically 32 bits wide they are register based machines general register machines with 32 or more registers they only have load and store operations to memory none of the ALU operations have family operations they often although not exclusively don't support complex data types strings decimal instructions and so forth and they try to avoid special case instructions they try to make it orthogonal in the sense that any operation can use with any register and to avoid registries which are special-purpose and more difficult for the compiler to descale the use of although they compromise on it as well the other kind of feature is that they're willing to make architectural compromises to allow efficient pipelining would allow efficient compiler optimization for the more complex instruction sets and the older instruction says people were very concerned about making the instructions look good to the assembly language programmer and easy to use for the assembly language programmers and these days there aren't any assembly language programmers anymore or hardly there are so you're willing to allow the instruction set to have certain architectural warts in order to make it more efficient or better for the compiler now why is it that for like 25 or so odd years people were making instruction fish which were ever more complicated these weren't dumb folks these were people who were pretty smart and had an argument that said that in fact making the instruction complicated we're better but what were those arguments well one is that the more complicated instructions that you design the fewer instructions you need the density of code is therefore higher program is smaller in that the exit memory less time this is true although the density of instruction sets in so far is the amount of memory space you have is no one with a consideration because memory is really cheap there is an issue of low instruction counts being useful in order to make a machine run faster because there are few instruction set but in modern machines we can get by that by using caches rather than the other another famous argument was that it's a better it's better to match the design of the instruction to high-level languages and let the computer do the work rather than the compiler do the work well it's often turned out that trying to do those kind of high-level language instructions was counterproductive because the instruction could rarely be use the special cases in which that instruction was designed for either never existed or changed as compilers changed and languages changed in particular the counter examine argument for risk is that if you in fact make the instructions simpler then you can not only reduce the number of cycles for instruction but reduce the cycle time so what risks they're doing are lowering the cycles per instruction at the same time though they're raising the instruction count the big breakthrough came in recognizing that using the RISC implementation and the high performance pipelining that we'll talk about later you can get a much larger effect by lowering the cycle per instruction by factors of two to five or greater and the corresponding increase in instruction count is much less it's one point five or two or some number like that measurement support that later so although the people proposing this construction sets had the right idea the numbers were against them notice that risks are reduced instruction sets they're not necessarily simple instruction sets and we'll talk later about some of the complications in some of the more modern RISC instructions well those are some of the trade-offs yet to make and doing instruction set design how do you make the decision about what to do well the only way to do it for real because this is not a theoretical exercise to measure real applications and I'll go over some numbers now that have been collected that measure real applications across a variety of architectures and we can look at how they they claim those these particular numbers use Peck which is a formatting language mostly character and ring manipulation now in GCC which is the compiler and spice which is the numerical analysis kind of a simulation language there's a lot of floating-point operation doing these real-world instruction studies are sometimes frustrating because the measurements vary greatly and the conclusions you reach will depend a lot on what mix of applications you choose but if you choose an appropriately rich set of applications then you can use conclusions which don't vary very much one of the questions is how do you get these these numbers and there are several different ways one is that you can simply write a software simulator you just simulate the instruction set architecture which is useful for the case where it doesn't exist yet and in the simulation you count things like number of times memory operands are used and so forth another way is to add software to an existing processor if the processor is micro coated you can add stuff to the micro code that collects statistics about instruction execution it may slow down the processor but you don't care what you're interested in for these kinds of measurements and it's not tiny but counts and with time things happen another way to do it is that most instruction architectures these days have an instruction trapping mode it allows you for debugging purposes mostly to take an trap or an interrupt in between the execution of every instruction which case you can look at the instruction being executed decode it manually in the software and figure out what it's doing and they had appropriate counterfeit of it you can't do any of those and you can always go in and add Hardware instrumentation wires and probes in the old days of mainframes this was a fun thing to do when there were companies that made their livelihood by building Hardware probes that you could attach to various pins in your computer and measure the performance and where the time was going and the execution rate of instructions and so forth seconds to be much more difficult when the processor is entirely embedded within a single IC and you can't get access to anything except the pins on the outside ok so a bunch of those measurements were done you don't have to do them let's look for example at what addressing modes are most common this is data from the VAX which is a good one to use to look at addressing modes because it has every addressing mode known to mankind so what in fact does it use well first thing to notice is that remember the top 2 on our list of addressing modes were not memory accesses but either register accesses or immediate access well lo and behold the vast majority of addressing modes are either register modes or mediums the question you don't have that life yes you don't have this life what you have is a slide which is a composite of two different slides from from the book and you will have this flight in the next hand what you have is a slide that expands only the memory reference part and shows how they distribute a net up 200% but I did for this slide was to combine that with the non memory reference mode to look at them as a whole so the non memory reference addressing modes in fact correspond to what 50 plus 20 maybe 70% of the accesses so obviously in designing your computer you want to design design encoding and implementations for those modes which are efficient of the addressing modes that actually reference memory the two that are most important are the displacement and the register deferred or register in direct mode remember these are basically both the same things they're the register indirect is the mode where the displacement is zero so obviously if you were to choose any single addressing mode for your machine that's the one that you do want to use most of the others are in the noise and not implementing them unless it's extremely expensive to do in software it's probably not a risk probably not not a poor choice to make but given a particular application that might not be true notice that for spice which does a lot of array accesses that in fact is a scale addressing mode which is used for array axises of word like quantities turns out to be a important one and in fact we'll look later at some evaluation of this mode and what it costs in a machine that doesn't have it to implement it in the more primitive addressing mode is that the Machine does have looks like we're about out of time so I better stop there we'll continue in this week no we'll continue on Thursday "
4u9VdzbNdCA,22,,2014-10-10T13:07:39Z,COMPUTER ORGANIZATION AND ARCHITECTURE,https://i.ytimg.com/vi/4u9VdzbNdCA/hqdefault.jpg,3G School of Entrepreneurship,PT26M35S,false,291,1,0,0,0,"hello students and welcome to the lecture on computer organization and architecture after this lecture we will be able to learn the following objectives understand the central processing unit explain the communication among various units discuss instruction format an instruction cycle describe the instruction set and components inside a computer explain data representation in a computer understand coding schemes in a computer let's start with the introduction of computer organization and architecture the components from which computers are built is computer organization in contrast computer architecture is the signs of integrating those components to achieve a level of functionality and performance it's as if computer organization examines the lumber bricks nails and other building material while computer architecture looks at the design of the house at each level the designer is concerned with structure and function structure the way in which the components are interrelated function the operation of each individual component as part of the structure now we will study the central processing unit or CPU the CPU is the primary component of a computer that processes instructions it runs the operating system and applications constantly receiving input from the user or active software programs it processes the data and produces output which may store or be stored by an application or displayed on the screen what is the CPU CPU is the central processing unit everyone know that so I'm not talking about it so let's move to are just like if you use in your computer your calculator your mobile phone car toys and a lot of lot of lot of components and in CP you have main three parts the first one is the seal that means the control unit the second one is the aerial that's mean arithmetic logic unit other one is the memory let's talk about it each and everything what is happening and what add is a loop it's done lot of calculating that is the part of doing the calculation every single calculation in your CPU in the registry and the cash that we in the memory the memory location will write into your chip set in C peep said I know it's it is real fast it's not like your ddr2 ddr3 memory it's really fast but it's the capacities total capacity is around cashy's around 2 MB some they are telling me but to Amin out anyway I three process having 256 K so in the CP Cu is control every single thing in your computer like the input devices output devices everything is controlling by the Cu the processes most of people know about the process the Eddie and the in there there are the prototypes like Apple g4 process processor and Motorola process and there are some other process they are not famous but like a VI a but I'm not included so it you do not worry about yes you won't remember about that three-part about that the L you'll see you and the memory that is the thing you have to remember there are three major parts of the CPU arithmetic logic unit or ALU register set control unit arithmetic logic unit an arithmetic logic unit ALU is a digital circuit that performs arithmetic and logical operations the ALU is a fundamental building block of the central processing unit of a computer and even the simplest micro processors contain one for purposes such as maintaining timers registers registers are temporary storage areas for instructions of data they are not part of the memory rather they are special additional storage locations that offer the advantage of speed registers work under the direction of the control unit to accept hold and transfer instructions or data and perform arithmetic or logical comparisons at high speed the control unit uses a data storage register the way a store owner uses a cash register as a temporary convenient place to store what is used in transactions computers usually assign special roles to certain registers an accumulator which collects the result of computations an address register which keeps track of where a given instruction or piece of data is stored in memory each storage location in memory is identified by an address just as each house in a street has an address a storage register which temporarily holds data taken from or about to be sent to memory a general purpose register which is used for several functions control unit or Cu the control unit is that portion of the processor that actually causes things to happen the control unit issues control signals X ternal to the processor to cause data exchange with memory and i/o modules the control unit also issues control signals internal to the processor to move data between registers to cause the ALU to perform a specified function and to regulate other internal operations input to the control unit consists of the instruction register flags and control signals from external sources for example interrupt signals system bus a bus is a communication pathway connecting two or more devices a key characteristic of a bus is that it is a shared transmission medium multiple devices connect to the bus and a signal transmitted by any one device is available for reception by all other devices attached to the bus if two devices transmit during the same time period their signals will overlap and become garbled thus only one device at a time can successfully transmitted main memory unit memory is that part of the computer which holds data and instructions for processing logically it's an integral component of the CPU but physically it's a separate part placed on the computer's motherboard memory stores program instructions or data for only as long as the program they pertain to is in operation the CPU accesses the main memory in a random manner that is the CPU can access any location of this memory to either read information from it or store information in it the primary memory is of two types random access memory or Ram read-only memory or ROM cache a memory it is a small fast memory that is inserted between the larger slower main memory and the processor it holds the currently active segments of a program and their data types of cache a memory the cash a memory is of two types they are primary processor cache a level 1 or l1 cache a secondary cache a level 2 or l2 primary cache a it is always located on the processor chip secondary cache a it is placed between the primary cache a and the rest of the memory did you know the first documented computer architecture was in the correspondence between Charles Babbage and ADA Lovelace describing the analytical engine now we will study communication among various units all units in a computer system work in conjunction with each other to formulate a functional computer system to have proper coordination among these units for example processor memory and i/o devices a reliable and robust means of communication is required one of the most important functions in the computer system is the communication between the units processor to memory communication processor to IO devices communication processor to memory communication the whole process of communication between the processor and memory can be divided into two steps namely information transfer from the memory to processor and writing information in the memory processor to IO devices communication when IO devices and memory share the same address space the arrangement is called memory mapped i/o let's now understand the instruction format an instruction format defines the layout of the bits of an instruction in terms of its constituent fields an instruction format must include an opcode and implicitly or explicitly 0 or more operands the format must implicitly or explicitly indicate the addressing mode for each operand for most instruction sets more than one instruction format is used computers understand instructions only in terms of zeros and ones which is called the Machine language to accomplish significant tasks the processor must have two inputs instructions and data the instructions tell the processor what actions need to be performed on the data these are transferred one at a time into the processor where they are decoded and then executed each machine language instruction is composed of two parts the opcode and the operand the bit pattern appearing in the opcode field indicates which operations for example store ad sub and so on are instructed now we will learn about the instruction cycle the basic function performed by the CPU is the execution of a program the program to be executed is a set of instructions which are stored in the memory the CPU executes the instructions of the program to complete a given task the CPU fetches instructions stored in the memory and then execute the fetched instructions within the CPU before it can proceed to fetch the next instruction from the memory this process is continued until specified to stop the instruction execution takes place in the CPU registers which are used as temporary storage areas and have limited storage space the instruction cycle includes stages as fetch read the next instruction from the memory into the processor execute interpret the opcode and perform the indicated operations interrupt if interrupts are enabled and an interrupt has occurred save the current process state and serve as the interrupt let's now learn about the instruction set processors are built with the ability to execute a limited set of basic operations the collection of these operations is known as the instruction set of a processor an instruction set is necessary so that a user can create machine language programs to perform any logical and/or mathematical operations the instruction set is hardwired or embedded in the processor which determines the machine language for the processor the more complex the instruction set is the slower the processor works based upon the instruction sets there are two common types of architectures complex instruction set computer CISC reduced instruction set computer our ISC CISC architecture C is C which stands for complex instruction set computer is a philosophy for designing chips that are easy to program and which make efficient use of memory each instruction in a CISC instruction might perform a series of operations inside the processor this reduces the number of instructions required to implement a given program and allows the programmer to learn a small but flexible set of instructions our is C architecture the R is C architecture is a dramatic departure from the historical trend in processor architecture an analysis of the our ISC architecture brings into focus many of the important issues in computer organization and architecture although our ISC systems have been defined and designed in a variety of ways by different groups the key elements shared by most designs are these a large number of general-purpose registers and or the use of compiler technology to optimize register usage a limited and simple instruction set an emphasis on optimizing the instruction pipeline now let's take a look at what is inside a computer computing machines are complex devices from numerous electronic components many of these components are small sensitive expensive and operate with other components to provide better performance to the computing machines therefore to better ensure a performance and increase the life of these components they are placed inside a metal enclosure called the system case or cabinet the system case is a metal and plastic box that houses the main components of the computer it protects electronics hardware against the heat light temperature etc it serves important roles in the functioning of a properly designed and well-built computer several areas where the system case plays an important role our structure the system case provides a rigid structural framework to the components which ensures that everything fits together and works in a well organized manner protection the system case protects the inside of the system from physical damage and electrical interference cooling the case provides a cooling system to the vital components components that run under cool temperature last longer and are less troublesome organization and expandability the system case is a key to the organization of physical systems if a system case is poorly designed up gradation or expansion of a peripheral is limited status display the system case contains lights or LEDs that provide varied information pertaining to the working of the system to the user power supply SMPS all electronic systems and equipment regardless of their size or function have one thing in common they all need a power supply unit or PSU that converts input voltage into a voltage or voltages suitable for their circuits the most common type of today's PSU is the switch power supply SMPS there is a wide variety of SMPS topologies and their practical implementations used by PSU manufacturers however they all use the same basic concepts AC to DC converters and DC to AC converters belong to the category of switched-mode power supplies the various types of voltage regulators used in linear power supplies or LPS fall in the category of dissipative regulator as they have a voltage control element usually a transistor or Zener diode which dissipates power equal to the voltage difference between an unregulated input voltage and a fixed supply voltage multiplied by the current flowing through it now we will study the motherboard the motherboard is a printed circuit board that is the foundation of a computer and it allows the CPU RAM and all other computer hardware components to function and communicate with each other the motherboard is also known as mainboard mobo M OBO which is an abbreviation MB which is an abbreviation system board or logic board expansion cards an expansion card is an electronic card or board that is used to add extra functionality to a computer it's inserted into an expansion slot on the motherboard of a computer expansion cards contain edge connectors that are used to create an electronic link between motherboard and card thus enabling these two to communicate ribbon cables a ribbon cable is made up of a number of wires that run alongside each other in parallel the majority of ribbon cables are found on the inside of electronics it's also called a multi wire planner cable memory chips it is a chip that holds programs and data either temporarily or permanently RAM chips are the computers temporary workspace while flash memory chips are used like disk drives permanent until erased ROM and prom chips can never be changed while eeproms can be modified and also II eproms can be modified storage devices storage devices are the data storage devices that are used in the computers to store the data the computer has many types of data storage devices some of them can be classified as the removable data storage devices and the others as the non removable data storage devices processors a processor or microprocessor is a small chip that resides in computers and other electronic devices it's basic job is to receive input and provide the appropriate output while this may seem like a simple task modern processors can handle trillions of calculations per second let's now learn how data is represented in a computer data representation refers to the methods used internally to represent information stored in a computer computer store lots of different types of information numbers text graphics of many varieties stills video animation and sound memory structure in a computer memory consists of bits 0 or 1 a single bit can represent two pieces of information bytes which equals eight bits and a single byte can represent 256 2 times 2 times 2 times 2 times 2 times 2 times 2 times 2 equals 28 pieces of information words which equals to 4 or 8 bytes a 2-byte word can represent 2 5 6 2 or 2562 pieces of information approximately 65,000 binary numbers normally one writes numbers using digits 0 to 9 this is called base 10 however any positive integer whole number can be easily represented by a sequence of zeros and ones numbers in this form are said to be in base two and they are called binary numbers text text can be represented easily by assigning a unique numeric value for each symbol used in the text for example the widely used ascii code American Standard Code for information exchange defines 128 different symbols when one saves a file as plain text it is stored using ASCII graphics graphics are displayed on a computer screen which consists of pixels tiny dots of color that collectively paint a graphic image on a computer screen compression the basic idea of compression is to make a file shorter by removing redundancies repeated patterns of bits from it this shortened file must of course be decompressed have its redundancies put back in in order to be used however it can be stored or transmitted in it's shorter compressed form saving both time and money types of number system there are four number systems that are decimal number system binary number system octal number system hexadecimal number system now we will study the coding schemes in today's technology the binary number system is used by the computer to represent the data in the computer understandable format numeric data 0 1 2 etc 2 9 is not the only form of data which is handled by the computer binary coding schemes the alphabetic data numeric data alphanumeric data symbols sound data and video data are represented as combinations of the bits in the computer the bits are grouped in a fixed size such as eight bits six bits or Orbitz a code is made by combining bits of definite size American Standard Code for information exchange code the standard binary code for the alphanumeric characters is the ASCII this code was originally designed as a seven bit code several computer manufacturers cooperated to develop this code for transmitting and processing data they made use of all eight bits providing 256 symbols in the ASCII the uppercase letters are assigned codes beginning with the hexadecimal value 41 and continuing sequentially through the hexadecimal value 5a and lowercase letters are assigned hexadecimal values of 61 through 7a now in the end let us summarize what we have learnt in this lecture the CPU is the primary component of a computer that processes instructions it runs the operating system and applications constantly receiving input from the user or active software programs registers are temporary storage areas for instructions or data they are not part of memory rather they are special additional storage locations that offer the advantage of speed the control unit is now a portion of the processor that actually causes things to happen the control unit issues control signals external to the processor to cause data exchange with memory and i/o modules to accommodate interrupts an interrupt cycle is added to the instruction cycle in the interrupt cycle the processor checks to see if any interrupts have occurred indicated by the presence of an interrupt signal you "
lr0gOpCWxPU,27,"The video explains the basic flow of operation between memory and CPU. Various components of CPU explained in the video, such as MAR, MDR, CU, IR, Program Counter, etc.

Basic operational concepts (CPU and Memory connection) is important to understand the Computer Organization and Architecture.

Twitter: https://twitter.com/rajasekhar_nv
Facebook: facebook.com/rajasekharreddy.nv
Instagram: https://www.instagram.com/rajasekhar_nv",2020-06-21T15:51:28Z,Introduction to Computer Architecture and Organization || CAO,https://i.ytimg.com/vi/lr0gOpCWxPU/hqdefault.jpg,Programming9,PT12M25S,false,3761,105,4,0,10,hello yes in this video we'll discuss about the basic operational concepts of a computer this is one of the very important concept in computer organization because we should have to know what is happening in between CPU in the main memory this is one of the very important concept that we must have to know it so let's get started with that so basically I'd like to try a simple diagram to represent how memory and CPU are communicated each other so let's get start with that basically we have a CPU simple CPU architecture would like to take it CP o stands for central processing unit everybody know it and then we can draw an another simple diagram it is called a program also called as the main memory right random access memory we also called it so whenever any particular program that you are going to execute in the computer first the instructions must have to be loaded from the hard disk because user will write a program right for example you as a user you have written a particular program write that program must be saved with its dot C extension r dot Java extension you want whenever you want to run that program that entire program must have to be loaded from the hardest to these Ram right let us see that how we can represent it in my case I would like to consider this as the hard disk drive right in this hard disk drive we may have for various programs in it like for example you want to run a VLC media player VLC media player is a program that you're going to run Photoshop it's a program right web browser you are currently running for surfing the internet that is also a program so it doesn't matter whatever it may be the program that you are running in the operating system that must have to be saved in the hardest first so whatever the program that you want to execute it whenever you open that program that entire program must have to be loaded from the hard disk TV main memory main memory are also called as a van right from here the RAM the instructions will be passed to the CPU from where the instructions are being executed one by one and if it is required it will store back the result to the memory otherwise it will simply pass through I won't so this is one of the basic operational flow that means they can say whatever the program that you saved that really than in the hardest and from the heartaches that will be saved to the gram from the RAM that will be passed to the cpu please try to remember this entire of flow because this is really important without knowing the flow of operation computer organization is really difficult to understand so please try to understand this one right let us see the simple diagram we have a ramp here the RAM is actually arranged as different size here some of the instructions we kept in some of the data we kept here like these are all the different artists locations of this particular memory right next come to the CPU in the CPU we have various registers here right we have special purpose registers and general purpose registers along with that there is an ALU in the control unit so let us discuss about the individual parts of this particular CPU it's come to be program counter whenever you initiate any particular operation the program counter first initiate its value for example in my case instruction 1 that is is 0 0 1 so the program counter currently holds the 0 0 1 address because the program counter always fetches the next instruction to be executed by the CPU this is clear I'm telling you again that the program counter very fetches the next instruction to be executed by the CPU in this case we have no instruction so this is 0 0 1 will be the first instruction so program counter will fetches 0 0 1 now next whatever it may be the address fetching by the program counter will be copied to the ma R that means memory address register the memory address register holds the address location which is fetching by the program counter right one important point eMAR always holds the currently executing instruction address right next if we go for the i/o instruction register the instruction register will hold the data that means it holds the instruction that was specified in this particular address location right next from ir from i/o instruction is already stored so from ir whatever it may be the instruction that is in specified there some format for the instruction every instruction there will be some format whatever the instruction that is specified that will be positively control unit down control unit is always controls the entire operational flow in the CPU but anyway one of the very important point one of the very important module in the control unit is a decoder the decoder will actually decode the instruction it will identify what operation it needs to be performed based on the instruction that is retrieved so now in this case I can say simply see control unit I'm simply representing it as a decoder right normally every instruction have a simple cycle that is fetch decode and execute every instruction for example if you have a ten instructions ten instructions must have to do the same process like fetching of instruction decoding of instruction execution of instruction now in this case this till I are it is completely called as a fetching phase and then whenever it reaches to the decoder it will go for the decoding State after that it will be passed to the ALU alias transfer at the Medic in logical unit it will perform all never the automatic operations as well have the logical operations in the computer mathematical operations means plus minus sin2 / right as well as logical operations means logical and logical or illogical not right less than greater than etcetera etcetera so all those operations come under thee all those operations are performed by the a loop like this is the thing and whenever there is a temporary values that you want to hold so those temporary values can be stored in this general-purpose registers are not r1 etc they are in they may be different various registers we have you those are all called as the general-purpose registers but of course we can say program counter M AR AR MDR right these are all called as these special purpose registers hard when Arnot r1 r2 till RN is called as the general-purpose registers so there is an another registered by MDR MDR stands for memory data register we also call that as memory buffer register why we use that whenever the instruction is ready with us we need to fetch the data for performing the calculation for example in my case the instruction is ad that means we can simply say as a simple example we can say addition we are performing some addition we have the instruction in the higher right whenever the tire is positively ALU for performing some operation it needs the data right so where the neat data will be available the data is still available in the RAM again so we need to fetch the data too so further we need to use the MDR as a major source that means MDR will pass the MDR will connect this Ram and the data will be fetched and that will be assigned to thee that will be sent to the ALU for performing the automatic operations that may be the logical operations this is the entire flow of operation okay now let us do the division of the things whatever we discussed now saying we have a ram here run is called the primary memory the countess of different instructions as well as the data right there is an interaction between RAM and the CPU so here we have a set of registers program counter which is used to hold the address of the next instruction to be fetched next Mao Mao stands for memory address register whatever it may be the instruction whatever it may be the address that is fetching by the program counter will be copied into the M AR right yamir means memory address register which is holding the currently being executed instruction address right next instruction register we have so whatever it may be the address that is holding by the m AR that instruction will be fetched by the ir that means that instruction entire instruction will be copied to the year in this case ad is an instruction right the ir will hold the ad right MDR which is used to take the data and if it is required it will store back to the memory that means it can it is a two directional we can simply say the data can be fetch it from the memory as well as detect can be sent back to the memory whenever it is really required and then we have a decoder let the decoder be decode this decode its operation right I don't say that it will convert into binary form no not at all so whenever you are storing some data in the memory you are simply telling to the system that all the data all the instructions that you have taken are storing in the form of a binary numbers right that is already converted into binary form so whenever it is received by the decoder decodable decodes it has an opcode right whenever it is say for example there are some indications you can see one one perform addition to perform subtraction like the tariff specific quotes instruction set will be there so the decoder will take the help of the instruction set it will perform the appropriate instruction to be executed right in my case I can say I will receive a code as one the decoder will identify code has one it will perform some kind of operation by the ALU right here ALU performs a either automatic as as well as logical operations ok let us see the same process in the step by step approach right what is going to be happen in the first step second step third step etcetera so first whatever the program that you want to run that will be loaded into the RAM first from the or hard disk right this is the step one right one have once done that after being loaded all your data so that instructions must be loaded into the CPU so who is going to be initiate that operation a little mainly the program counter we already know that program counter will always holds the address of the next instruction to be fetched of executed right in this case we have an address of 0 0 1 0 0 1 will be loaded into the program counter that is a step 1 what will be the step 2 so whatever it may be the address that is holding by the program counter that will be copied into the Amir so now ma r contains an address of currently being in executing instruction that means in this case we have instruction 1 address in the Amir that means instruction 1 address here is 0 0 1 so here we have ma are holding the 0 0 1 address initially we have program counter on storing 0 0 1 once it is being copied the next instructions will be the next processor will follows but the program counter will increment its value every time it increments it value so now 0 0 1 will be replaced with 0 0 2 that means the program counter value got incremented because the instruction is currently being executed in the CPU so program counter no need to hold the same address it will increment that address so now the program counter value is 0 0 to program counter always holds the address of the next instruction to be fetched or executed right now ma r is holding this 0 0 1 in that particular memory location zero zero one we have an instruction called ein sd1 R we can say that addition instruction so that addition instruction will be passed through the data lines to this instruction register so this is the third step now we have an instruction near perform some addition now after that that will be passed to the decoding this is the fourth step after that it will perform some operation this is the fifth step right next whenever it is performing any particular operation it needs to retrieve the data right so the data can be retrieved through MDR so whenever it is really necessary the data will be fetched from the MDR so we can say this is a six system it will perform the fetching operation directly right again it will take the data it will send back the required data to the ALU right similarly it is the retaining process eight nine it will perform the operation if it is required the it will use the general-purpose registers that means are not r1 r2 any of the resistors it will use for the internal calculations now once ALU perform that operation that will be sent back to the MDR MDR will store back to the memory where it is really required if it is really required that's it guys that is the basic operational flow of a computer 
wm-4EBApwco,22,"Subject: Computer Science
Courses: Computer Architecture and Organization",2019-03-25T07:03:28Z,Processor Memory Interaction,https://i.ytimg.com/vi/wm-4EBApwco/hqdefault.jpg,Ch-13 Computer Science and Engineering,PT31M6S,false,206,6,0,0,0,[Music] welcome to week five in this week we shall be discussing on memory system design so we will look into the various technologies that are used to build the memory that we use in computer and we will also look into how these memory are used to design and organize it in the system so memory is one of the most important functional unit of a computer we all know that what it is used it is used to store both instructions and data and it stores bits like zeros and ones so as we have already seen how we encode an instruction we encode an instruction with bits of zeros and ones so in the memory location when we say we store both instructions and data those instructions and data are organized in bits of zeros and ones and they are usually organized in terms of bytes so we will see here how are the data stored in the memory are accessed so like in memory what we are saying we store data so we need to know the mechanism how we can access the data from the memory we should also know how we need to store the data into the memory so these are the two things we need to look into every memory location has a unique address and a memory is byte addressable that is every byte that is a group of eight has a unique address some memory systems are word addressable and by word addressable we mean that each location consists of multiple of bytes depending on the word size if your word is one word is four bytes that is 32 bits then the memory location will be changed after every four zero four eight and so on and if the every address location consists of like multiple bytes so if it consists of eight bytes that is 64 bits then the word length is 64 then the memory system in the memory system it will be incremented by 8 now see the connection between processor and memory so as you know that in processor we have two important register one is memory address register another is memory data register memory address register contains the address of an instruction or data that is to be read from the memory or the address of a data that is to be written into the memory and that particular data which is to be read comes through this data bus so this is the data bus and whatever addresses here that addresses hit and then from that address whatever data is present that data comes through this data bus and it comes to memory data register now you see that the data bus is bi-directional because we can read the data from the memory so the data is coming from the memory to MDR and for right we have to write the data so from the date from MDR it will go through this data bus into memory and along with this we also require some control signals like read write etc so if we have a n bit address bus then the memory addressable memory location will be 2 to the power n like we already discussed if you have a 3 bit address bus then the total number of location will be 2 to the power n so there will be 8 location starting from 0 0 0 0 0 1 we go on with 1 1 1 so n bit address bus can have a maximum of 2 to the power n addressable memory location and we can have M bit data bus so in that particular address the data which is present is M bit and M big data at a time can be transferred to memory and we have other signals like read write and chip select we will be seeing that why chip select is required in course of time so this the maximum number of memory location that can be accessed is 2 to the power n M bit data line the number of bits stored in every addressable location is M and the read write control signal selects the memory for reading or writing so for reading it is 1 for writing it is 0 as I said chip select line when it is active that is it is active hi this is active low so it is active when it is 0 so this will enable the chip when it is 0 otherwise the data bus is in high impedance state so this memory module will not be selected in that case so here we have n bit address bus so we have 2 to the power n addressable location and MB data bus so the size of the memory the total size of the memory which is addressable is 2 to the power n cross M now classification of memory system how we can classify a memory system so one way to classify a memory system is volatile versus non-volatile so with respect to volatility meaning a volatile memory system is one where the stored data is lost when the power is switched off that means as long as the power is applied to it the data will remain but as long as the power is taken off the data goes off that means it is volatile it goes off after the power is cut off so this CMOS static memory and CMOS dynamic memory both these are volatile memories that means as long as power is supplied the data remains but in case of dynamic memory even if we are we are supplying the power then also it is it requires periodic refresh so data cannot be retained for longer period of time so periodic refresh is necessary now what is the non-volatile memory a non-volatile memory system is one where the stored data is retained even when the power is switched off so where you will see such kind of non-volatile memory we see that in read only memories where once the data is there it it it retains magnetic disk is also one cd-rom DVD flash memory and some resistive memories so here these are all non-volatile memory so even if the power is not supplied the data will remove again we can differentiate a memory with respect to random access versus direct or sequential access what do you mean by dharak random access by random access we mean that when the read or write time the read and write time is independent of the memory location being accessed that means you either hit location 0 or you hit the last location or the middle location the access time is same random access so whichever location you access the access time will be same irrespective of the location the example is CMOS memory that is RAM and ROM both are random access and then what is sequential access a memory is said to be sequential access when the stored data can only be accessed sequentially in a particular order like the examples are magnetic tape here the data are accessed sequential so one by one by one a memory is also said to be direct or semi random access when a part of the access is sequential part and the part is random like your magnetic disk here we can directly go to a particular track but after reaching that particular track if you have we can sequentially get the data one by one by one so here this kind of memory is semi random access so which is somewhat sequential somewhat random next let us see read-only versus random access what are read-only memory read-only memory is one where the data is one stored is permanent or semi-permanent what do you mean by permanent what do you mean by semi properly by permanent we mean that once we write into it and then no changes can be made to it that is permanent and semi-permanent pimps we write into it it remains but if later we want to change it we can still do it so it is semi-permanent it remains permanent for a period of time and again if you want to change it we can change it and then it will again do you mean so the examples are wrong random read-only memory from programmable read-only memory it is Apple programmable read-only memory electrically reasonable programmable read-only memory so these are all glass of read-only memory where the data written or program during manufacture manufacturing process when they are manufactured at that particular time the data is written into it but yeah there it can be changed with progress so ROM came first then P P Dom then e P EEPROM and so on now random access memory is one where the data access time is same independent of the location so we access the first location or the last location the access time will be same and where it is used we will be talking extensively about your main memory and your cache memory so in both the memories such kind of memory that is random access memory are used some of the examples are static Ram here once the data is written it retains as long as the power is supplied to it and dynamic Ram is having the same feature of a ram but even if the power is supplied to it the it requires periodic refresh so periodic refresh is required even if the power is supplied to it and here the data is stored as charge on tiny capacitor we will be looking into more details of static RAM and dynamic RAM in course of time at this point of time we need to know some of the terminologies that we will be using it very often they are called access time access time of a memory latency and bandwidth so what is memory access time by memory access time we mean that the time between initiation of an operation what operation either it can be read or write and the completion of that operation access time how much time it is required to access the particular data that is memory access time next is latency latency is the initial delay from the initiation of an operation to the time the first data is available so let me tell one thing at this point of time that when we access a particular location in the memory we do not just access or retrieve that particular data we always transfer a block of data that is why this latency is an important term because latency will give the time required to access the first data and then the subsequent data that are present can be accessed in a much faster rate so latency is the initial delay from the initiation of an operation to the time the first data is available and what is bandwidth bandwidth is the maximum speed of data transfer in bytes per second so in modern memory organization every read request reads a block of words into some high speed register first that is the first word is available and from where the data are supplied to the processor one by one by one okay so the total access time will be depending on not only a single word but block block transfer so subsequent words how they are transferred now this graph I have already shown you earlier while talking about evolution of computer system but now let us see the design issue of memory system so this red line shows the growth of processor in in in course of time and this green line shows the growth of memory technologies although you can see both are growing but processor design is growing at a much higher space and memory speed memory design memory advancement is coming at a lower rate but both are advancing technology is advancing in both both processor design as well as memory but this speed gap is steadily increasing so the most important issue is to bridge this processor memory gap that has been widening with every passing year so this gap that we can see here is actually increasing so see this gap is increasing so the advancement in memory technologies are unable to cope with faster advancement in processor technology but there are many techniques that are used to bridge this speed gap so at this point some important question arises how to make a memory system work faster it has a limitation but how we can make it faster such that the processor and memory speed gap can be reduced how to increase the data transfer rate between CPU and memory the transfer of data how it can be made faster and how to address the ever-increasing storage need of application we need large memory as well not only we need faster we also need larger memories because we need there there are various application which requires larger memory space so we need to look into all the issues first issue is how we can make this memory work faster how we can have a larger memory and by all these things how we can reduce the speed gap between processor speed and memory speed so some possible solutions are cache memory and virtual memory what a cache memory does we will be looking into detail of this in later weeks but what it does is it increases the effective speed of memory system and what virtual memory does it increases the effective size of memory system so we will be looking into these in some detail in the later part of this week so very briefly what is cache memory it's a first memory that sits between your CPU and main memory and we can have many levels of cache memory and why cache memory is in place we will see this because of two computer two properties of computer programs what is that one is that is called locality of reference basically one is temporal locality of reference other is spatial locality of reference so we will see this in detail later but for now let us understand that cache memory is a memory which sits between CPU and your main memory and there can be many level of caches but the cache memory cannot be very large it is much smaller compared to our main memory so frequently accessed data or instruction can only be brought here and executed and what technology is used to build this cache memory we use static Ram technology to build this cache memory moving on with virtual memory virtual memory is basically it's a concept that is used to give an illusion that we have a very large memory space at our disposal but actually we have space equal to main memory but it gives an illusion to the programmer that you have a largest piece to execute so the technique used by operating system to provide an illusion of a very large memory to the processor program and data are actually stored on secondary memory that is much much larger and data and instruction are brought into main memory as and when it is needed so secondary memory is a concept where we say that we have a very large memory but whenever we want to execute it we need to bring those data or instruction into main memory and then it can be executed now let us see how a memory chip looks like so this is on a PCB printed circuit board so these are memory this is a separate memory modules that are placed these memory cells are organized in the form of array so these you can see this is one this is one this is one this is just this may be a 4gb memory and each having say 1 GB 1gb 1 G like that so present day VLSI technology allows one to pack billions of bits per chip so per chip billions of bits can be putting memory module used in computers typically contains several such chips so these chips are put into the slots memory slots that are present in the PC now let us see organization of cells in a 8 cross 4 memory chip so this is a 8 cross 4 memory chip let us see how it is organized so you have 8 rows this is the first row second row third row fourth row and is the 8th row and in each row there are 4 bits which can be taken out an individual memory cell is a row let let us consider this as a row this row is connected with the word line this is called word line W 0 W 1 W 2 these are word line an individual cell is connected to 2 bit lines one is B another is B bar that is complement of the other and which is connected to the sensor right circuitry and this sense or right circuitry is further connected to the data lines that means this is one memory chip where there are eight rows so we have to select any one of the eight rows so for that reason we require a three cross 8 address decoder so these a 0 a 1 and a 2 are applied to this address and then base depending on this particular dis let's say it is 0 0 0 then the first word line will get selected and then all the bits of these word lines may be transferred to through this sense or write circuit to the data lines if you want to read the data and suppose I want to write the data into the cells then what will happen the data present in this data line that is coming from your MDR will get stored in this through this sensor right circuit so in this organization we can see that 8 cross memory chip is there so the address is decoded with using a 3 cross 8 decoder and then each of the bits each of the memory cells are connected to 2 bit lines one is the complement of the other which is connected to the sensor right circuitry and through the sensor right circuit read is connected to the data lines and we have to also supply these signals that is either we want to read a data or we want to write a data so this is how the memory chip 8 cross 4 memory chip is organized so as I said a 32-bit memory chip is organized as 8 cross 4 as shown in the previous figure each row of the cell array constitute a memory word so the entire row or entire one word is the row so every of the cell will constitute a memory word we need a 3 cross 8 decoder to access any one of the eight rows and the rows of the cells are connected to the word lines individual cells are connected to two bit lines one is B another is its complement and it is required for reading and writing and cells in each column are connected to sense or write circuitry so cells in each column so this is one column this is the next column third column and fourth column so the cells in each column is connected to this sensor right circuitry other than the address and data lines there are two control lines read and write and chip select and why chip select is required it is required to select one chip in a multi chip memory system we will be seeing this with examples later so basically this read write and chip select is connected such that either it will specify that you have to read the content of any one of the cell one of the word or you have to write data into one of this cells now in this diagram how many external connections are required what do you mean by external connection externally that is provided not within this memory chip you can clearly make out that these address lines is 0 a1 and a2 are externally provided to this decoder and then it is decoded and a particular word is selected now once you select a particular row then all these bits will be transferred to the data line through the sensor write circuit so it is connected to 4 bits are there so there will be 4 data lines through width through which this data will go so those are four more external signals that are required here so three for this address for for the data lines and then you have two more signal that is read/write and chip select that should be also provided externally because the processor will tell either you have to read it or you have to write editor and there will be two more that is power supply and ground so external connection requirements that are there for this eight cross for memory that is three external connection for address for external connection for the data because in each row there are four bit to external connection one is for read/write and another is for chip select two external connection for power supply and ground so a total of three plus four plus two plus two that is 11 is required for this 8 cross 4 memory chip now let us see what about this one 22:56 cross 16 memory how many 256 so there will be 256 rows to select any one of the 256 rows you require a 8 cross 256 decoder so the address decoder size will be 8 cross 256 so 8 external connections for address will be required then the data output is 16 so 16 external connection will be required to transfer the data either to read the data or to write the data similar way to external connection for read/write and chip select and 2 for power supply and ground so a total of 28 external connections will be required so we came to the end of this lecture where we briefly discussed about what is memory how our memory chip can be organized and in the next few lectures we will be seeing how what kind of memory technologies are actually used to build this thank you 
9pGJSqaX-oY,27,"Part - 2 : Computer Architecture and Organization - Instruction Set , Processor Structure , Functions.
OPEN BOX Education
Learn Everything",2018-08-19T15:52:21Z,"Part 2 : Computer Architecture and Organization - Instruction Set , Processor Structure , Functions",https://i.ytimg.com/vi/9pGJSqaX-oY/hqdefault.jpg,OPENBOX Education,PT30M33S,false,2679,21,3,0,0,[Music] CPU instruction set learning objectives at the end of this topic you will be able to recognize machine instructions in an instruction set identify the elements in machine instructions and their importance know the different types of operands that expected in an instruction Rison the existence of a large number of instructions and a machines instruction set learn the usage of machine codes to write simple programs outcomes by the end of this topic you will be able to apply the different modes of addressing the contents of the RAM to improve the performance of a computer system implement different computer instruction sets a general-purpose computer CPU consists of a control unit which is defined to generate a unique set of control signals for every instruction in the instruction set it also has a set of registers to store data and instructions and has ALU to execute arithmetic and logical instructions instruction set is the software component of a computer system usually a computer programmer writes codes in high-level language like Pascal or C but what connects them to the machine is the instruction set instruction set consists of machine instructions which are in binary language like the one shown 0 0 0 0 1 0 1 0 which is load MQ and which means to transfer contents of register MQ to the accumulator ac a register these instructions are binary codes which are realized in hardware and the control unit is programmed or hardwired to generate the corresponding signals for these realizations in the hardware to respond and act each instruction contains all the information required by the processor for execution the instruction shows what operation is to be performed what type of instruction it is depending upon the type of instruction how much data are required to be operated from where the data or result is to be moved or shifted and to where way to jump while in the instruction cycle in order to execute another program as sub program or interrupt handler program computer designer comes up with a general instruction format for accommodating all these issues the format of an instruction is usually depicted in a rectangular box symbolizing the bits of the instruction as they appear in memory words or in a control register and the bits are divided into groups called fields or elements a simple instruction format is shown the most common fields found in instruction formats are an operation code field that specifies the operation to be performed called up got an address field that designates a memory address or a processor register and the mode field that specifies the way the operand or the effective address is determined let us take the field opcode which tells the CPU to perform an operation operations or instructions can be of four types data processing which are for arithmetic and logic operations data storage for moving the data into or out of registers and memory locations data movement or i/o instructions and control instructions that instruct the CPU to test and branch or jump the operations must be executed on some data called operands stored in computer registers or memory verts the data within registers can be called by their names and the data within memory can be called by specifying the address of location within the instruction this is direct addressing where the operand comes from the address specified for example the address of the operand in this instruction is 457 and the operand is found at four five seven sometimes the operand is located in a place whose address called the effective address is stored in another location and that location is specified in the instruction this is called indirect addressing in this case the address of the operand is three zero zero and the content of three zero zero is one three five zero which could have been operand but it is not instead it is the effective address of the operand apart from indirect and direct mode of addressing there are other methods which will be dealt with under addressing mode let us consider a simple basic computer of accumulator type organization where all operations are performed in an implied accumulator AC shown in this organization see a memory of size 4096 words of 16 bits each instruction register IR is of 16 bits similarly data register D our temporary register TR an accumulator our 16 bits each address register a are stores the address and memory of 4 0 9 6 is 2 to the power of 12 words so AR is of 12 bits similarly BC which stores address so is 12 bits size the instruction formats were the types of instructions categorized on the basis of references for operands is shown here I is equal to 0 or 1 an opcode bits 0 0 0 2 1 1 0 are for memory reference instructions I is equal to 0 is for direct and I is equal to 1 for indirect the resulting instruction set is shown here memory reference register reference and instructions try interpreting the instruction 0 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1 first we break the 16-bit instruction into three parts this shows I is equal to 0 which means the addressing mode is direct from the format for memory reference instructions the next 12 bits correspond to an address in memory which in hex is d69 and 3 4 3 3 in decimal the instruction means end and the content of memory location 3 4 3 3 with the content of accumulator similarly 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 0 interpreted as register reference instruction 7 0 8 0 in hex which means circular right shift a C and E e is a carry flag machine instructions operate on data which might be one of these addresses numbers characters and logical data addresses can be considered as data when effective addresses of operands are sought calculations must be performed on the operand reference in an instruction to determine the main or virtual memory address numbers are another type of operand it can be a binary integer binary floating-point a packed decimal number involved in calculations the third type of data is characters these are text and character strings represented in some character codes like international reference alphabet IRA or American Standard Code for information interchange ASCII logical data an n-bit consists of n 1 bit items of data each item having a value of 0 or 1 when a data unit is viewed in this way when they are considered as logical data the number of different up codes varies widely from machine to machine but a typical categorization of all the instructions can be done in the following way a few instructions of the basic computer instruction set can be recognized as one of these types a and B or UT are data movement or i/o instructions sta Lda a data storage and D ad D si la clÃ© si ma si ma CI r CIL are data processing instructions and B un BSA are control instructions similarly another example could be IAS computers instruction set here we come to addressing modes their immediate mode direct mode indirect mode register mode register indirect mode displacement mode let us say e represents the contents of an address field in the instruction are the contents of an address field in the instruction that refers to a register ei the actual or effective address of the location containing the referenced operand and X within square brackets represents the contents of memory location X or register X the simplest form of addressing is immediate addressing in which the operand value is present in the instruction that is operand is equal to a another simple form of addressing is direct addressing in which the address field contains the effective address of the operand e a is equal to 8 with direct addressing the length of the address field is limited in address range one way to have the address field refer to the address of a word in memory which in turn contains a full length address of the operand this is known as indirect addressing ei is equal to a within square brackets register addressing is similar to direct addressing but in this case the address field refers to a register rather than a main memory address here e e is equal to R where R is the name of register register indirect addressing this mode is analogous to indirect addressing here e a is equal to R within square brackets which means R is a register in the CPU whose content is the address of operand in memory next is displacement addressing this method combines the capabilities of direct addressing and register indirect addressing the effective address is obtained by the following computation effective address is equal to the sum of address part of instruction and the content of CPU register that is e a is equal to a plus R within square brackets this way there are three addressing schemes relative addressing based register addressing an indexed addressing in relative addressing R is PC that is program counters content is added to a to get the effective address EA the received address is relative to the address and PC base register addressing in this mode the content of a base register is added to the address part of the instruction to obtain the effective address next displacement addressing is indexed addressing mode in this mode the content of an index register is added to the address part of the instruction to obtain effective address an address field in an instruction contains decimal value 14 where is the corresponding operon located for a immediate addressing be direct consider a instruction format we have already seen the format for a basic single accumulator organization computer the types of instructions were based on references to processor registers or memory or input/output the format varies in length and content according to the organization and complexities involved in addressing for example the formats for pdp-11 are shown here which is different 16-bit 32-bit and 48 bit formats there are three types of CPU organizations single accumulator organization general register organization and stack organization in accumulator organization all operations are performed with an implied accumulator register the instruction format in this type of computer uses one address field for example at X where X is the address of the operand the instruction format in the general register organization computer needs three register address fields for example add r1 comma R 2 comma r3 to denote the operation add the contents of the registers r2 and r3 and store in r1 computers with stack organization have push and pop instructions which require an address field the instruction add in a stack computer consists of an operation code only with no address field look at this example x and y are pushed into the stack using the first two instructions the third instruction ad adds the last two operands from the top of the stack most computers fall into one of the three types of organizations that have just been described variations in length of the instruction format are also expected to be implemented in different situations like to evaluate the arithmetic statement X is equal to a plus b into c plus t0 address of an address or to address or three address instructions can be designed what is an assembly language and how is it helping us we are going to see that now a processor understands and executes only machine instructions which we know are in binary language according to von neumann the binary codes are stored in memory and are fetched one by one and are executed if a programmer wished to program directly in machine language then it would be necessary to enter the program as binary data is it possible in all situations to answer that question let us convert a high-level language statement into machine language consider the statement n is equal to I plus J plus K which is written in basic suppose we wish to program this statement in machine language and to initialize I to 2 J 2 3 and K 2 4 we can do that in the following steps say in the single accumulator organization consider the following memory setup for I J and K the program consists of four instructions load the contents of location 2 0 1 into the AC add the contents of location 2 0 2 to the AC add the contents of location 2 0 3 to the AC store the contents of the AC indication to 0 for for more improvement we can make use of the symbolic name or mnemonic of each instruction this results in the symbolic program to store arbitrary data in a location we invent a pseudo instruction with a symbol dat the is merely an indication that the third field on the line contains a hexadecimal number to be stored in the location specified in the first field for this type of input we need a slightly more complex program a much better system and one commonly used is to use symbolic addresses this is illustrated in the figure summary let's summarize the topic instruction set is the software component which is realized in the hardware every hardware corresponding to the instruction is activated through control signals by control unit calculations must be performed on the operand reference in an instruction to determine the main of virtual memory address the operand is viewed as numbers which may be a binary integer binary floating-point of packed decimal number involved in calculations assembly language is an intermediate language between high-level and low-level language CPU process structure and functions learning objectives at the end of this topic you will be able to conceptualize the working of processor in detail involving instruction cycle [Music] understand the importance of CPU registers which are employed for different purposes visualize the concept of pipelining for better CPU performance outcomes by the end of this topic you will be able to analyze the formals instructions throughput and pipelined implementations of a simple instruction set apply the concept of pipelining and improving the speed of system performance we already know from unit 1 that CPU consists of control unit arithmetic logic unit and general-purpose registers and that it does the following in cycles fetch instruction interpret or decode instruction fetch data and execute instruction to do these the CPU has to one store some data temporarily to remember the location of the last instruction in order to fetch the next instruction 3 stores some instructions and data temporarily while executing another instruction this means the CPU needs a small internal memory to store all these information internal memories role is played by the registers in CPU execution of arithmetic and logic operations is done by a L unit the control unit controls the movement of data and instructions into and out of the processor and controls the operation of the ALU we see the internal CPU bus connecting the three components this element is required to transfer data between the various registers and the ALU because the aleo operates only on data in the internal processor memory this picture shows the internal structure of the CPU the registers in the processor perform two roles user-visible registers enable the machine or assembly language programmer to minimize main memory references by optimizing use of registers control and status registers used by the control unit to control the operation of the processor and by privileged operating system programs to control the execution of programs a user-visible register as one that may be referenced by means of the machine language that the processor executes we can characterize these in the following categories general-purpose data address condition codes general-purpose registers can be assigned to a variety of functions by the programmer sometimes they use within the instruction set is orthogonal to the operation that is any general purpose register can obtain the operand for any up code this provides true general purpose register use often however there are restrictions for example there may be dedicated registers for floating-point and stack operations in some cases general-purpose registers can be used for addressing functions example register indirect displacement in other cases there is a partial or clean separation between data registers and address registers data registers may be used only to hold data and cannot be employed in the calculation of an operand address address registers may themselves be somewhat general-purpose or they may be devoted to a particular addressing mode examples include the following segments pointers in a machine with segmented addressing see section 8.3 a segment register holds the address of the base of the segment there may be multiple registers for example one for the operating system one for the current process index registers these are used for indexed addressing and may be Auto indexed stack pointer if there is user visible stack addressing then typically there is a dedicated register that points to the top of the stack this allows implicit addressing that is push pop and other stack instructions need to contain an explicit stack operand a final category of registers which is at least partially visible to the user holds condition codes also referred to as Flags condition codes are bits set by the processor hardware as a result of operations for example an arithmetic operation may produce a positive negative 0 or overflow result in addition to the result itself being stored in a register or memory a condition code is also set the code may subsequently be tested as part of a conditional branch operation control and status registers there are a variety of processor registers that are employed to control the operation of the processor most of these on most machines are not visible to the user some of them may be visible to machine instructions executed in a control or operating system mode four registers are essential to instruction execution program counter PC contains the address of an instruction to be fetched instruction register ir contains the instruction most recently fetched memory address register ma r contains the address of a location in memory memory buffer register MBR contains a word of data to be written to memory or the word most recently read now that we know the internal organization of a processor and the uses of its components we can now have a closer look at what exactly happens in the instruction cycle and how the CPU tells it a more elaborate instruction cycle is shown here so first is the fetch phase in this phase an instruction is copied from the memory into one of the richest errs internal memory of the CPU what we require here are the following where is the instruction located how does the CPU know where the instruction is how is the instruction going to come into the bus who points to the instruction location containing instruction where is the instruction received in the CPU memory how does the corresponing register know that it is entitled to receive the instruction let us answer these questions one after the other in order to understand the whole process of fetching instruction 1 the instruction is at some location in the memory - from a register called the program counter PC 3 through the relevant control signals to the memory for the memory address register M AR 5 in the instruction register IR 6 by the control signals to the register so in this face the PC contains the address of the next instruction to be fetched this address is moved to the M AR and placed on the address bus the control unit requests a memory read and the result is placed on the data bus and copied into the MBR and then move to the ir meanwhile the pc is incremented by one preparatory for the next fetch the fit cycle is over the control unit examines the contents of the IR called the decode phase to determine if it contains an operant specifier using indirect addressing if so an indirect cycle is performed the rightmost end bits of the MBR which contain the address reference are transferred to the m ER then the control unit requests a memory read to get the desired address of the operand into the MBR the execute cycle takes many forms the form depends on which of the various machine instructions is in the IR this cycle may involve transferring data among registers read a ride from memory or i/o and are all the invocation of the ALU like the fetch and indirect cycles the interrupt cycle is simple and predictable the current contents of the PC must be saved so that the processor can resume normal activity after the interrupt thus the contents of the PC are transferred to the MBR to be written into memory the special memory location reserved for this purpose is loaded into the m ER from the control unit it might for example be a stack pointer the pc is loaded with the address of the interrupt routine as a result the next instruction cycle will begin by fetching the appropriate instruction consider a non-pipelined system which contains five step process instruction one is fed into the system time taken by instruction 1 to complete the process is 50 units now instruction 2 enters the system and it takes another 50 units to complete the process now consider a pipeline system with 5 segments or stages instruction 1 and test the system when instruction one enters the second stage instruction two enters the system in the same way instruction three enters the system when instruction two is in the second stage while instruction one is in the third this process is continued the first instruction comes out of the pipeline after fifty time units instruction two after 60 time units instruction three after 70 time units and so on now if we compare the term required to execute instructions in both these cases we find a huge reduction in the time required to execute instructions and pipelined system by pipelining there is speed-up in the process summary let's summarize the topic user-visible registers Oliva the machine or assembly language programmer to minimize main memory references by optimizing the use of registers control and status registers used by the control unit 
sWa3GaK-7j8,22,7th semester,2020-06-25T16:12:39Z,2nd lecture computer architecture and organization,https://i.ytimg.com/vi/sWa3GaK-7j8/hqdefault.jpg,Abdul Qayum Sabir,PT23M45S,false,11,0,0,0,0,"so second chapter zookeeper computer organization key and architecture computer organization Kate a lecture came another computer buzzy somebody preaching organizations at the Orca textured set of Adar a difference and viscous Cara notice that the CPU main carbon and Amish and the main memory the chicken them very successful second lecture chicken days among apathy key the history of computer lab she the computer history chicken s Rivera Kubelik Barsky she got a hammer gotta see the computer for his team and the pressure among a koala which emerged up computer to come protection vapor Yemen is for a person is rapid duration understanding the economic relations department campus and I serve as a restoration way the regime the history chicken the our gazebo an arrival chunky dark ribbon for 1943 ki j 1943 key the u.s. barrage from the Hawaii the mesial - nada para chicken s LD no the para and we the calculation koro no carburetor Bonnie Bedelia calculation corrosive Ikeda became a solution Okazaki dakari chief Callahan Michelle Affairs initial B makea beam do de Soto Adams Aaron immunity yeah chicken day hewan de Buci como take a holiday yeah after the Wednesday dr. chicken have acidity bio ha stable basement of the women is the key matter observe a magnificent animal movie ordinary firstly as a weeper I was the time and it's actly Apple had a few come day Oh No do you divert the quorum of tariff calculations called Jeremy God have a calculation to calculate room and the opossum and Somali cool now the habanero studied so catchy 300 dollah Jeremy electronic numerical integrator computer guru bandhu no double you're gonna steal that the friend Sal weapon of Mandy husband's away no sticky a Dutch company design show that the architecture of an takuna that will show yo Cosmo the Eckert poem Mandy the Accord should come to you professor Dirk now estado responds answer you fire a sure shot got too much lip on Amanda's anthological Audrey borrow Porsche command a demo project you can lead the army no Arista share among each come the Dakar Co the tracker chicken track objective trajectory table secondly the weapons the para the diamond gap computer man did a calculation on aeropuerto three Chicano products that Cora 1943 key element of the project for starters a computer they do redo the para o finish a crop a 1946 game Mary MATLAB do you say that you're rewarding to come now I'm sure who dunya to do die Okajima digital computer second a Hadi dunya Stabler sport or GU better computer dis you watch up 1955 for a that they are used to come there I wanna die you scared of the dissimilar binary no of the kitchen come they accommodate have succumbed EDD Mathematica jicama calculation of Eric Iggy another 10 days ago I'm Donna Jean program merrily by switch switch man is due to our program una caÃ­da MATLAB you switch and converse which of cappadoccia Carmen Wichita consistent jerk on a dark day in Jihad system are it thousand chicken patty service refugium teams negative welcome tips Haskell curry Makino's Harouna and the other Tyrion you can accommodate a CEO she shan't awake about a CEO pep when they were negative 30 ton there wasn't chicken that's odd 30 eternal 15,000 square feet they are as in over a 15,000 square feet that's what you pay somebody anyone else when I got system bow on duration 140 kilo power consumption of the name and a master of Cola a 140 kilowatt 100 car Chicano and a cooperation today called investor a chicken they have a deep your secret decoder none Saba she come dozen computer did you see Kincaid you're drilling kikyo Qaddafi program P the Depot which that do Maura Healey world katomina highly wanna hide our capacity particular caterer how about you to come they didn't use everyone knowledge company no stood program consider when a Newman training Wyndham and training chattering chicken D Newman poohmandu cuts off the computer the para you oughta see the Belvoir I overcome the Hajj or periodic architecture to be a parable on this but organization awake already under rush an umbilical nepotism no dinner Newman poem on the yuccas which architecture design color computer the para o architecture it's a pathological bondo she stored program concept accompaniment the program singing opera custodiat Sangha they have a conceptually man memory to come therefore a detached single-story paddy money operations in a geeky no torture come the new month on Monday of course the architecture for ha but a shackle bond until a capital design cathedral Cara Jim and memory to come the data body taraji or the main memory roost aboard our data now she calmly the sea pewter of touch no Chicanos see pewter of the Xenopus II puke amongst our a bear adjusters hum dear just a little travesty Hamish an art emetic logic sandy a units accompany any in duty to week on the Russian bank program called would you not predict mo no Twitter Karachi the deep basement ichika refers a news anchor zoo one man she 2 plus 3 2 + 3 d but record our program chicken daily did our Delta G commanded the have a panther power just a cicada or did she come operation with the operation ship together the kinetic logic unit together - on the day to come a result to resolve Ahom did it can control other chap was demonically program control unit patrol and Akina there are here isn't verbiage a parrot a new turret law - an input/output chicken equipment they have um dip okay include Oh Jerry and put Papa tickets a new kick your Papa D kissing like you know completely logical day nah d-does he establish credit - it is n pero do energy cognitive upper Boehner leader Hammond elected 0 1 chicken apartment would be dr. napur harmful control unit interpreting instruction from memory and executive we conclude you not Sakura uhas instruction by our code memory that you have of pearl come better to come there I execute kill devaÃ­s patrimony muskie kuruvi and put an output equipment operated by the control unit the the input of output chicken I seized ypres have what happened devices Chico does supposed to manage Gator dad control unit for the money how it's an Institute for Advanced Studies I I experimented show Institute Advanced Studies for Montevideo and the haouka beat up a 1952 Kira cam Pedro the beach come our Corso dava Institute to come the data architecture the qorikancha Newman Girona haha be a Potemkin show show complete roof Hodari architecture to come the implement co g turnip or aha implement the aura one day the degree the logical name institute advanced studies to come over detail d Papa - Kahneman D jeopardy sheep Yucatan memory buffer registered memory but disappear Kadena registers handy a camera tropic mo l homme de Marchand Padilla key and the watch calmly elude a hamburger no dub registers become the dad she said you know survivor just a small amount of memory de memory storage de monserrate Jadakiss icky and data store key neither data addressing understood Keegan sections toolkit is smart amount they got a hot mi munger G memory buffer register lemon syrup when manzara said a memory address register de I Homme diversion instruction register them in Sarah and section buffer registered a program counter they accommodated a multiplayer portion T memory buffer register consecutive memory buffer register key Barbara jicama da histÃ³ria the Delta organization c'mon as an architecture the world is key Chariman got some store Konoha a world man did you come down a different way Shira Lazar yo world check America Altima per their desk basement which it better know Delta world one world to come the the yaha sequential the data the storage the power for organization per detail money no the memory buffer register so l ke g mb are homotopy MBR card adage they used to store data share data store give us the data war war part you come the Akatsuki I was joking hustling Sarah to come they did a powerhouse table I removed dinner table prepares my daughter nici belchy condoms our memory address register day the data to come that she did the memory buffer posit Amanda Rocha Delta's Soto has addressed Larry she have had the convener out which at a computer get up of music and up become the Agera know I have I addressed you come Dino have an address torpor the para the open register a new scale a charity memory address register rainy from the Russian badminton section register Oh Matilda body magic operation kid epidemic nickim Norris ok the new instruction register put room and a massive Cudahy key instruction buffer register chicken they have a hum the HMDA cadapah raja baath signals were a NAPA to money generated using a multi-term of kuru the developer has instruction to the route to know a humppa memory when instruction buffer registers are circular I'm Donna Jean monks are a buncha common a program counter adage the program counter put Amanda body how about the program chicken signal to general she did a perigee Madhab the program museo de la medida know about the program counter put romantical a community to come de monserrate this ecology accumulator no program counter my program contra su coche a program counter means had a dark our college is among the next in Section matter bo instruction completes row yo cars one completion of one computer tasks is among a sure ok do no instruction the program counter over perot non-commissioned a community to come de montserrat the apparel ski each is a manga chicama a lipid room and a result rivality matlab have a multiplication career division could ever bless a man asked already know a result she come to have a pyometra prototypic through money was together I would put the work either she come multiple quotient you come there there's a very skilled with MATLAB come a result were on over result I would call you come Arabic Yunnan upper multiplayer quotient come also a proto next having say a command seÃ±orita the structure of Institute add for advanced study spoon Amanda Chickamauga without these internal architecture now array architecture cupid Asha karma and a chicken the input and output equipment Lee dinner USADA monks are registered with a camera to the arithmetic logic MD there are told myself that if registered people they register okay then I be a bodega come Rock'em carcass they a do that these navona these diamonds are asserted I see purely the DC pupae man K done like a sense a car now a carburetor in you a looky then I bet you come they have way by the Karuna and perform columnar Ashanti Beckham program counter the other she come day a instruction before registered a memory address register they am diversion instruction register they now control circuits become the Padilla Delta K available D no da ha ha count-o u not succumbed a / - calm and oh yes is detailed to come they have next commercial computer resume was the computer Baba rocky the data history chicken dip America to ignore Bosco avid algebra 1947 key occurred our chilli chicken day away your computer corporation predict our crow Shaddaa our innate computer chilli PADI key melody Rosetta Laguna architecture mind essential now venadu stop game l computer chicken universal automatic computer pollen money no computer show sherry ambassador delenn i'm da hua shan / 1950 key welcome to teach you is broken off since with Mandy you know calculation the power design shall be Archie come the ditch come they buy the operation one rapid Amanda Caden but 1950s key to come to M number you an IVAC pin mannequin computer design show the faster hammer or more memory became available I'm the deeper base Monica gentlemen got the IBM machine do guru know IBM she come there will be about 700 of initially that any person one 101 to come they have the IBM yamashina jeopardy key it's a speedy hum come on the ocean I will in a computer do these scientific calculations were added Apogee cater gherdÃ«ina roost up a 1955 Kitsch accommodate the IBM one hour chicken maybe I better computer design color cuts energy core i7 core FM from Devon Dasha Carmen do and Savannah Honaker / c10 - Poonam and in the business application the party you build our generator oh so we had the c1 Nash start with these heels oh there are 7000 for mo 5 series the dress dodgy the we come to Jupiter Monday and computer do she know the heavy computer have a sentence there to command as a GPU often Jericho aruki charondas money do you have a car Hamish Gordon the deep baseman dick chicken and scientifically the caution rebuttal to the data paradesio computation will teach there with us sighs honking me the cavalry of questions and here the computer the size come or car come they have a eulogy turkey no dinner hasta chemistry key element Iran for ciccone rapid Ajo Torito silicon we di silicon de perak a doe sardinia mocha mal improvement Oh cocoa Malta liriko ha senator video Kochi the silicon Papa dakedo Sarah the silicon a selkie NEMA had a MATLAB burn Imperial military nobody boon yet Monday have to approach the element other on surgical appear - oh no - I fill tube dilly recorded our it her transistor D neurological transistor ooh no how are you cutie ChiCom they have narrowed our episome and the transistors on our transistor KO card out the the tomb to pin his Batman de la guarda any says it come on the Russian cheaper ha Marzano well the digicam heat production or to come here to do rule nor a heat of a todo ha jicama diagram cannot cut us and criminology panic a power to come near 142 odd power per diem and a consumption on the Monday Natasha's paraone full-t kilowatt chick-fil-a pie your computer Monday electronic bar comes on should we know Hara becomes Authority and it is a huge and awesome and they hit hump to do ski the catchy to mark the birth mother loves it you know Pamela and Azam and they have a hilltop to come the harem they are pretty ski ghee salt state devices will come the wash on Monday made from silicon sample Amanda does you dhaba DQ she is silicon Padilla Kyushu by 1940 c1 capability that she can they achieve you know what a noxious does the fam'ly cheaper than a shepherd Asha calm and engage watershed upper 1946 Nara third nineteen fifty-seven poori David Hume to become the W SCADA the arena boosted the data cars angle the have a carro 40,000 per second the operations become the core no food deters an operation deep IO second people but 1957 our a 58 now a lot 64 for a transistors rather yeah and she come damage community transistors you can be down now that transistor chicken need the deed chicken computer generation welcome computer today faster - oh no Tahari computer chicken operation hoo-ha operation she come they have a persecute to lock yamaha g2 200 thousands you come there the operation perspi to be up or 1965 now a lot the 1971 small and medium scale and integration pull money yo has custom circuit voltage that is circuit Publius Mandeville cicada there are computer to come they've actually the lady the operation chicken pursuit artery Luda one-millionth herbivorous ADA and imagine for yo second killing 1 million operations Arthur as well in December 1972 Nevada 1977 for a large-scale integration to mnemonic yamaguchi array she comes out of the live pictures be chair picture - no c'mon man me know how I would less milena purpose economic Amanda Kaur : 1978 Nevada 1991 for a very large scale integration she said median Tom da Vinci Beckenham 1991 a corridor from the yoora you won't draw large scale integration Poonam man do the technology develop stretcher there it was the money bar 1 trillion the watch you come they have a podium and circular design together our operation before the money performed Kida next among sahaja come there for the basic a second-generation machine the hum transistors the real key eg that the second generation aa property de NCR and RC produced margin system like a planet e GD coupling sukhothai our transistor machine of our water chakra the IBM was Rho come they have a hump a pas de aqui - come these karappa 1957 game produced DPD one per demanded how computer with am the Dean who's the better generation the Rizzuto generation came micro electronics Poonam mandy smile electronics period IQ ski the java smart electronics geometry that computer to come day gets our memory cells to come the bodega interconnection to the car to come name and affection succumb to the semiconductor rumor dub am da Vasily company Kyushu Oh the silicon so you see me conductor MATLAB behalf and she condemned ago Sarah Hana duo have to come the higher the the generation computer to come the ultimate role discussed see diamond command padishah Carmindy the day mores large come to have a paper pen next lecture came journalist at eco Lobby H come Natasha's and Sarah discuss came a tiny agree "
JFZNftDbcAg,22,Computer organization and architecture lecture series by Rosna  P Haroon,2019-05-24T20:52:46Z,Microprogrammed control(COA KTU Syllabus),https://i.ytimg.com/vi/JFZNftDbcAg/hqdefault.jpg,Computer Science and Engineering mentor,PT19M58S,false,4674,78,6,0,11,next topic is micro program to control micro program to control this is one of the other techniques who controls it a control unit organization under sickness intervention okay so initially I will tell you the difference between Hardware the controller and micro program does under the hard way control we have seen in the previous video what is doing in Hardware control is in Hardware panda we are using and designing a lousy secure and based on that accommodation circuitous design initially we had this scene according to the instruction set and is fixed but in such cases what is the problem is there is no flexibility in the flexible and dynamic input instruction set in Chittenango in session I'll take a pull the instructions in Miami and say and an option if new instructions is finding out we have to add that instruction also but existing instruction set which it already the same Shh de caminha or combination circular engineer approve the instructions are the same or so on the variable other entity another flexibility and the para another hardware control in India that is one of the main problem of hardware to control okay Filipa guru haya UE hardware and now the or software revealing an AVO control unit in and essentially among the Piranha maestro problem to control no one her idea my pro program to control is actually working on the basis of my programs my pro problems in her chair and initially it is actually like a machine program machine program than the reality grows and once in the combination animation program at the pool at the next save Oh someone's in the combination level in the root problem on a micro program a prequel death at micro program to control the hand or sickness to generate to say another with the help of a micro program and okay about inordinate of my flow problem and camera the power the anomaly instruction sector and Jana and instruction the instruction that Nana does specific exam related is enough people are American Bar Anu you would have ro signal in the corner but to be Co term they are in everyday set up time they wrote in a quarry a condo sickness endure okay ba them even in the McCarthy Amanda Vaughn septum I through instruction as well my program Donna LaDonna but a micro program and in a little on the cello and papered another Hana Fuda Fourier columns in entertain them if all I'm sayin in India or condo city animus into the Toro : every piece your denominator volum ma are in unity to the bottom and they are they are two separate bits essentially separate economist inches in under one big piece actually has say the four pieces in one bit we sell cigars in for PC out in Marin Mayor out I'm gonna weigh around Tory less or a marriage MDR out Isabella aura with the over individual with these are saying the for individual contraceptive okay how you would add 4x and I'ma leave it overnight but the render can prosecute under me Jerry can I'm gonna in a Connecticut throw corner man up now put her in the corner and I'm up in there now put into me to incur a pig in yawning in a quarry columns it to people yeah the can one money I can never tell if you would have control first tip control step on a one second gun doorstep to 31 December 3 animal Katherine under step under 700 steps are there a porno model a in a very low condo steps and I'm already taken a people now but the condo steps Allah are a key activity in the instruction of the PC out activity Anna a mariner to attain a real activity a sell it for activate a annum and activity okay about if there is sickness a one night is a TIA barking a zero to Cynthia as I never achieve a sinner then you would have one night to satiric another one night is a 2d gaming Tomoka PC out one night under ma are in one night endures we Heda Ardeth we said it with three other people I'm gonna know peter the grouse step Pinocchio my name is that I'm steps away I can activate I asked automata and imatra one Magda buck in Lusaka and over a pan-roasted column the spooning I didn't ever die so you know someone's in a combination is a gender you see Rossum and Cynthia combination and microprogram never I am I the first in under step you know keep me fasting under tsipras you're almost combination and along even a or a micro instruction the barring micro instruction and the angle is in a control world a number I cut up on a lake and all border island with micro instruction other Wally second step uremic in the zero sermons combination where it micro instruction Alan Clara control bird I'm gonna break through micro instructions in there one two three four five six seven eight and reciprocal spawning height is 7 micro instructions are there and collection of these micro instruction is known as a micro program or micro rupee a micro instructions in a land go to the collection along the barrier my pro program in the pariah Alan and then Daniel a micro routine and amplified okay micro program my draw rotate about micro program in the concept even have a with the help of this micro program my property under control is actually generating that kind of signal okay this we can actually shows that is sign of my crop of undergone drop and we would have gone anime in broker Laden Jana bring me the idea earlier an early instruction register instruction is ISA Rana I had a holiday another in session currently executing instruction of whole day in it is this run instruction register their starting address in the trees their new PC mu pcs micro program counter new PC micro program counter the network signal is there then control store okay it can't all store on the pan and I'm gonna be microprogram in the Havana low-e microprogram sell now got a store in their place and I can restore this is actually a special type of memory this memory can store only micro instructions away micro instructions on a micro programs in the sector story they take in the memory per in a piranha Condor store okay initially only instruction register the instruction X again instruction lorry in Assam here that instruction will be decoded by the instruction decoder about insertion register their output I'd like a path that instruction decoder and now in session the border Emma decody in the record I'm the type of instruction and on the recognize another responding micro instructions every store everything man attaboy Leck instruction coriander ka operon say the polyline instruction the LRD jason deep order and the people in the book okay buddy pod would especially I desire nothing load instruction is another tough instruction they've got rotunda up I'm gonna wait for a they return information in and I'm again look at them II information are the instruction there micro pog emember they are no control memory storage a became that are storage a break in the memory rat or something so initially the starting at the center will be loaded with the current execution instructions control memory address above a instruction I inspection a micro program story brick in the control memory address which it is starting address and it'll load is a although mu P simply damage any anything happened look under stupid abundant anomaly you come to the hard way to control you see the other than there is no one does then we're over Oh step number automatically in Romania many die other polar able to use anywhere microprogram condom apart wrestle control a kilo days for example in the instruction in the story the rick another micro instruction number editor Micronesia story is it again the thousand a more than a thousand seven variable which i recap again and again eventually the thousand digital audio from u-kiss is also loaded with thousand okay controller Towson in the one Anne and Marilla Bolton either reading controller really I think I might throw in specially the CW in a control we're done up and already say that they're a control signal each and great to see him there you can talk about in the canal equal number instructional a dog and already Arthur can go step to one minute are spawning I disappear on atop another zero one one one one another combination Lee first you can draw stepping are spawning at a combinational a in the Henry control heard go for initially and I'm okay generating do you can deliver done okay about the house in the not instruction account already building then mu peace in the new PC will be again automatically incremented with them what I do drop person applies Asma the new PC will be again incremented with one upon a thousand eleven thousand one night adapter control store in a thousand one be relic underwear Ghana other reading I'm gonna undo thousand seven and uh know whether even gonna play you receive don't anything okay then the control signal at Brown and go under our another other word on my degree new PC will be incremented about our instruction execution girl in the a significant instruction execution well you know we wind up instruction is the setup of the instruction which it will lower down our then every see the starting address ended tomorrow you see cancer continue chain up or ill condo board is in written some material can afford an illicit dealership and our condo sickness the north insanity okay other than a technical how positive normal execution on the cement that you blow up their grandchildren this will work old buck shred would you branching one which I recap for branches up glutenin handles have the capability we saluted enough about a branching border knuckle did you get more if you like this conduct if it if I'm making the my terminal is starting at the center it analogous connecting and a branch at this end I tried to mihari pinning the branch address is generating Islamic then connect in its unconditional difference vada and the branches are in a Sunday if they're ready for you control store in the middle can't overtake it together branch instruction and acharya other the branch instruction iron and then gil i instruction in them is na a branch instruction i don't know i'm especially for divine strength and then next at the branch at the sale of the kingdom so the branch of the similar item will be loaded with that branch enters ok but branch it was like initially the normal execution I get thousand thousand running meaning a bow a MIDI push a bra - mmm I'm Acharya yep okay everything villarrica micro instruction stories which turned our branch in there Abraha we understand where the branches research it alodia again similar process in finding a banana micro-programmed controller block render diagrams on the regular execution and the normal execution of a sequential execution of an instruction I did that branching here to add a little bit pin a word and then you go work on another namelaka renew a micro program counter inertia and angular micro program does not remove PC new PCs actually incremented every time a new micro instruction is fetched from the micro program memory Ln but except in the following situations are the mute PC automatically increment this area the moon scenarios and then other can order your parent okay I'm at the scenario in which it did maybe that oh I I have another order for the instruction lorry in Assam either my mu PC will be loaded with the starting address and I didn't put the instruction I and they're only in the semi if that ah in section they're starting underside the garden tip and Roberts everyday unknown control stole story they can add in the starting under search it'll give me a PC load our life okay pinion but do branch a micro instruction executes a in a Samantha branch a condition conditional branching and a condition called source artists is a they turned on okay I'm gonna run and then I'm gonna branch at the cetera community say Lord our other Paula or en do micro instruction and wondering Gilliam a counter is actually loaded with the address of the first to Condor word in the control store rather if I were to put the other the order put the instruction - Samantha Harlow you same thing happens rather would enter my procession and so the instruction to them they you know I nearly new PC will be loaded with that yeah next discarding undressed okay we move on the condition and immune piece automatically increment the air to them I select wrong and I shall upon Amica posted an element Expedia I mean PC automatically increment a Akebono situation Amna but when you address I are like you new instruction yeah like an order in the Samantha on a branch in your birth from China Samantha on the indigent or signaling and going around the summit micro program to control the same any even automated Narendra time on a horse owner instructions and but they get micro instructions Corazon de lengua take the micro instructions mainly remove the two types of instructions on there but take an instruction for assorted instructional we horizontal instruction which other phenomena piranha Lake you see your awesome once in a combination with changed yellow even a number for ozone a micro instruction of Aria individual big T's are saying before individual and rose again if over 1000 do now potentially under 16 and Maggie and it can be given any instructions or pacinian nothing to be driven and a Apple over or under so play in our potential bittern our unit representing us a meaning and a horizontal micro instruction for an individual piece we have assigning to each control signal Adana instruction being in Reverse on an instruction wide understand either I need a brain okay tell me comments love you very much would it is in Rosana la I've actually memory management thing about OS and memory management a is the memory management ALS sometta why a memory management is a very effective way to memory you see an app a the Bora Bora is passed date and I'm at the store is a shitty have 0 provision Amelia I'm gonna depression already mater only three let me write or an inspection on anomalous you see in it a baby even able to come back this came directly might demand I allow a piranha vertical instruction already scheme in Amla pickle instruction and the chain in Xela mutually exclusive I tell a sickness in a number of group pain mutually exposes sickness a group right over the other you are now looking if unit 1802 operational but when she I'm a troll other pilot Amity cooperation so on and add a sub per multiplication lesion of Corey operations in the ray will be mode of operation and contention but Europe only a pope and suppose so to party now the function compose am but till now but a which any function functions a very irregular or actual time that I get me about your opinion and the panel is signaling whatever that you to group me don't know people you know who play the field I can take a knife and if two or three or four or five if she says Sarah it's like a glue pocket that the instruction back next up over there in for the big day for in the Marin Annie a little bit there according if Alan our operations about chemical in our operations neighboring anomaly channel grouping them I'm the chief abuelo a people in our and nothing Oren Anthony raucous a delicate rabbit to Anna naivity for a dual resistance is equal to sixteen it's easy 2400 Savard and angle is zero zero one subject I'm gonna appear the operation of odd function are the new board signal which she didn't replacing upper normally any game if they're horizontal shooter a person in and get through certainly we're not  our signal even aghanim overall I don't know that was boring I believe it discipline a girl's phone and I take a picture on a further now ALU operation any pardon had a bit doin pop up in Arabic in Arabic okay on the poor little nap I logic even a little single bus organization Anna normally you see in England a single Priscilla act at a moderate Armstrong got a folder or output order out protonic today I'm a crab to Atia material alone in an invasion and shallot in outer signal at the Quran which should be to be with you if anything were then Bundrant our supreme and I will share again I'm gonna need for handouts equina no recent activity our I don't even know your group they make a profound and I can include cable HL it and it should be for a certain irony you see a new parent is sitting there and I don't know other communicate something nobody now is it remember so Rinaldi book to Mary Poppins and a bit to know that we need in a North America dated furniture a minute me nothing you know that you can you push I am but demo mutually expose the sickness or actually grouping together a mutually exclusive vegetal in your active time or Ilana you see em but your honor sickness many mutually exclusive in sorry they mutually exclusive sickness I to learn a clue - yeah m-net another and you fall in the world form bites key market I pay the water or no camera ever don't know for takin over it - oh but they can open another mutually exclusive sequencing of your operations IQ data they even I don't even run on a I think what function work together one LS we take an auto sickness in military school severe disciplines an advantage in Magellan erotism Okinawa 2016 in America I didn't tell you nothing should be to an emotive and already I horizontal instruction AMSA made that even a little bomb but scheme Aki irritation and enough attention on one gamma guru now I ain't in provincial again I ain't into temptation is like an aggravation haha for another inspection in length or it gives me a button that is a main advantage of or take a micro instruction though you come by this and make your own differential between from this former take a micro instruction supposed to me apologize I guarantee Corazon ll destruction than is a vertically inspection Canada been in this big advice the comparing both speed and I'm gonna come bearing Ranger this PE device can really use of resources no matter fact another you Horace own instruction and a concentration saying go to the speed to go to daily back share a number of realignment of things sessions can be reduced drastically by using the compatible core neck better than micro instruction other power than neighbor take a micro session and I'm gonna be chillin is for seasickness and group put it together other than and I'm okay Aviva no particular sector ll be expected economical the ponies are totally female Mohammed other than a metrical instructions you still in some way the decoder circuitry notion very more automatically and the program cost over a proposed for a soda water between the everything but take an instruction artifact share memory management a mother father than net length of the instruction of the Epiphany ago instruction any language about the main an amount under caution for difference did a horizontal number at the micro instructions okay and another common dog sled chose somebody share with them okay okay thank you 
egogKbl_BkU,22,,2014-07-11T10:09:02Z,QEEE - Computer Organization and Architecture - lecture 4,https://i.ytimg.com/vi/egogKbl_BkU/hqdefault.jpg,IITM QEEE,PT58M3S,false,1736,6,1,0,1,okay good morning and welcome to the fourth class in this 15 lecture series last class we talked about number representation we discussed how to convert an exit assimil number we to a binary number octal number to a binary number a decimal number to a binary number and vice versa and we also gave some insight into why those conversion procedures are correct and in the whole of the previous lecture we were assuming that the numbers were unsigned numbers so today we will be looking at signed number first and foremost in the hardware there is nothing called a subtractor everything is adder so given two numbers the adder will just treat them as the hardware the adder which is part of the hardware which is part of the ALU will treat these numbers as unsigned numbers and it will just do an addition there is no notion of a subtraction in the hardware so it is our responsibility that is the compilers responsibility to give to model subtraction as addition this is a necessity from the hardware point of view now how do we do that there is a very simple intuition for this suppose let me say I want to do a minus B this should become a plus some minus B as far as the adder is concerned it will as far as the hardware adder is concerned it will assume that a is an unsigned number and whatever this minus B is another unsigned number it will just do addition and give us a result and that we have to reinterpret it and find out what the answer is right so one of the important thing that we will be learning today is how we perform the operation of subtraction as addition the necessity comes from the hardware as I will repeat again the necessity comes from the hardware because there is nothing called a subtractor everything is addition and this adder will see whatever we give give we give two numbers to it and say add these both these numbers the adder will look at it as unsigned numbers and perform the addition so for we should use an adder which is capable of adding two unsigned numbers and do signed arithmetic using that hardware namely the setter so this is what we will be covering today what sort of representations of numbers that we need to follow so that we can model this subtraction as an addition the computing system is not one that can handle an infinite range of numbers there is a finite range of numbers that the system is capable of handling let us again start with what we call as the what we call as the unsigned integers suppose I have only two bits to represent an integer so using these two bits I can do these two combinations namely 0 0 0 1 1 0 1 1 which will represent 0 1 2 & 3 these are the decimal values of this suppose I have 3 bits then I can represent 0 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 which will represent 0 1 2 3 4 5 6 7 so if I have 2 bits I can represent up to 4 numbers if I have 3 bits I can represent up to 8 numbers so if I have n bits I can represent up to 2 power n numbers this numbers actually stands for unsigned integers a very simple proof of for this statement is that every bit so I can just say in which every one of this bit can assume one of the two values namely zero and one so if you take a combination across n bits then since each can have two possibilities and since there are n I could have 2 into 2 into 2 into 2 n times possibilities which essentially is 2 power n so if I have 2 bits I have 2 into 2 possibility which is 4 because each of this can take 0 or 1 and for each of this the other bit can take 0 or 1 so I assume the first if it is 0 the second bit takes 0 1 I assume the second with this first with this one the next bit sake 0 or 1 so like this for if I have 3 bits since then since each of this can take one of the two values so totally I can take 2 into 2 into 2 which is 8 so if I have in which the same argument I can extend and say that I could have to power in unsigned integers right so the moment I want to represent signed integer so by using this I can represent 0 1 2 till 2 power n minus 1 0 1 2 2 2 power n minus 1 for example n is 3 I can represent from 0 1 2 3 because since the N is 2 sorry n is 2 this is 2 power 2 minus 1 which is 3 so I can represent from 0 to 3 when n is 3 I can represent from this is 2 power 3 minus 1 which is 7 so I can represent from 0 1 to 2 7 the moment I am looking at signed integers wherein I am interested in associating a signed a bit so I want to say represent between - 4 - 3 - 2 - 1 0 1 2 3 on both sides right so one of the thing is in the number representation I need to associate at least one width which I call as the sign bit and the most obvious choice for this sign width is the most significant bit I did mention in the previous class that in a positional representation say 569 this is a positional representation because based on the position a different weight is associated with every digit for example Phi is associated with the power of 100 well 6 is associated with a power of 10 - associated with a power of 1 in a positional representation this is the least significant digit and this is the most 2 significant digit similarly in a binary representation say let us take favorite this is the least significant bit and this is the most significant bit the obvious choice for representing the signed bit would be the most significant bit so one of the immediate or quick way of representing sign number is what we call as this sign-magnitude representation what do we understand by this sign magnitude and representation so in the entire representation there is one bit which is the most significant bit which would be there will be one bit which is the most significant bit which is associated with the sign and the remaining part of the bit will be associated with the magnitude fine so so let us take this three bit example so I have 0 0 0 0 0 1 0 1 0 0 1 1 all of them will represent positive numbers because the most significant bit is 0 and I say if it is 0 that sign associated with that number is plus so when we take 0 0 0 or any of this the first bit is basically the sign bit while the remaining 2 bits give the value of the number namely the magnitude of the number so that is why this representation is called sign-magnitude representation so here this will stand for plus 0 this will stand for plus 1 this will stand for plus 2 this will stand for plus 3 and after this everything starts with 1 1 0 0 1 0 1 1 1 0 1 1 1 this will stand for - 0 this will stand for - 1 this will stand for - - this was banned for - so let us just do it for 2 bits 0 0 0 1 1 0 1 1 this will stand for plus 0 this will stand for plus 1 this is stand for - 0 this will stand for - whew so using a sign-magnitude representation the range that I could represent is minus of 2 power n minus 1 minus 1 to 2 power n minus 1 minus 1 using n bits if I put n equal to 2 here this would be minus 2 minus 1 so minus 2 power n plus 1 to 2 power n minus 1 minus 1 so this will be if I put 2 here it will be minus 2 plus 1 to 2 minus 1 which is minus 1/2 plus 1 if F for n equal to 2 if I say n equal to 2 then it is going to be between minus 1 to plus 1 which is what you see here if I have 2 bits I can represent from minus 1 to plus 1 if I have 3 bits then I can represent from minus 2 power 2 plus 1 to 2 power 2 minus 1 which is nothing but minus 3/2 plus you see here this is minus 3/2 plus 3 so this is the sign-magnitude representation so one of the obvious drawbacks of the sign-magnitude representation is we are wasting one important thing namely plus 0 and minus 0 there is no necessity to have I can have only one zero there is no necessity to have a plus 0 and a minus 0 from an arithmetic perspective because both have the same value unlike plus 1 and minus 1 they have two different values plus 0 and minus 0 plus 0 is equal to minus 0 so there is no necessity for me to have representation different representations 4 plus 0 n minus zero so this is one small drawback of this method okay now let us let us now go and consolidate this whole thing that what we have discussed in the last fifteen minutes given in bits the unsigned numbers can be in the range 0 to 2 power n minus 1 sign the numbers in what we call as the sign-magnitude representation will be from minus 2 power n minus 1 plus 1 to minus 2 power n minus 1 sorry 2 power n minus 1 and already I told a very simple drawback of this if you had given complete attention to my previous 15 minutes of lecture you would also understand that there is another problem here that we have posed that the hardware knows only adding and saying numbers so even yours when you want to subtract or do any arithmetic on your signed numbers it is the compilers responsibility to convert that signed arithmetic operation as an unsigned addition that signed arithmetic addition or subtraction should be modeled as an unsigned addition and given to the adder and it will give a result which again has to be reinterpreted and the necessary answer should be taken so one of the important exercise that we would do in the next 20 minutes is how do we model signed arithmetic signed addition / subtraction as unsigned addition and and why we are doing it I again repeat we do this because the hardware can just do only signed unsigned addition and we have to model all our signed arithmetic using this unsigned addition so what sort of representation we will look so I am going to first in the next ten minutes I will be making the foundation for understanding this representation okay so let us say first and foremost please understand that we cannot represent infinity in computer we have a finite representation so let us take this if they have three bits and you start counting 0 0 0 then it will become 0 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 1 1 after that you count another one what it will happen you will get 1 0 0 0 but since you can't represent more than 3 digits or 3 bits this one will will be lost because I can only represent 3 digits so this one will be lost again you start counting 0 0 0 so in some sense what happens is if I start counting 1 by 1 from 0 0 0 you will end up with 1 1 1 and again you will start with 0 0 0 so in some sense this addition that we are trying to do is cyclic in nature because you go on in cycles so we start with 0 0 0 you keep counting once again you come to 0 0 again you start counting once again you come back to 0 0 0 so every 8th edition you come back to the same point and keep repeating so when I have finite arithmetic and they keep incrementing by 1 essentially I land up with a cyclic traversal of the number system of the number line so I can represent this entire operation as a circle first let us concentrate on unsigned edition suppose I want to just do a plus B say I want to do two plus three how do I do I start from two and go clockwise three steps start from two and go clockwise three steps from two first step second step and third step so I reach five so when I so when I say let us do another thing suppose I want to say 4 plus 2 what I do I start from 4 and go two steps clockwise five six similarly I want to do four minus two for some reason the same cycle can be used start from four and go two steps anti-clockwise so start from four and go two steps anti-clockwise I go back to two so that at this point just understand this arithmetic only at this point addition is a clockwise operation while subtraction is a anti-clockwise operation now what did we tell we want to model word subtraction and addition in the same manner so I want to model subtraction as addition if you just if I want to model subtraction as addition then I have to translate this anti-clockwise motion as a clockwise motion because for us addition is clockwise subtraction is anti-clockwise suppose I say I want to model subtraction as addition then I have to model this anti-clockwise motion as a clockwise motion how do we do it instead of moving from so how many steps are there in this whole arithmetic there are eight steps after the eighth step this whole thing again comes back so instead of moving two steps anti-clockwise can I move six steps clockwise that's the question instead of moving two steps anti-clockwise if I move six steps clockwise then essentially I am going to get back this 2 right so this 4 minus 2 can be realized as start from 4 and go 6 steps this 6 is realized as 8 minus 2 6 steps clockwise so go from 4-6 steps so one two three four five six and I get two so this subtraction operation I have actually modeled now as addition exploiting the fact that the arithmetic is cyclic in nature so let us do one more example here suppose I want to move suppose I want to do 5 minus 4 5 minus 3 so one way is start at five and go three steps anti-clockwise so start at five and go three steps anti-clockwise one two three so I get to the other way of saying is start at five and go eight minus three steps equal to five steps clockwise so from five if I go five steps clockwise one two three four five I get two so if I say anti-clockwise is subtraction clockwise this addition I have modeled this subtraction as addition okay I hope there is no doubt it's a very very simple concept anyway if there are doubts I will take up after another ten minutes but I hope all of you have understood this concept so I am modeling addition subtraction as addition both these cases essentially explains you that concept so in essence what we have done now I want to so if I have minus one so if I have minus three if I could model it as 5 then 8 minus 3 can be handled as that 8 plus that 5 if then that 8 minus 3 can be handled as addition if minus 2 could be modeled as 8 minus 2 then I can handle 4 minus 2's addition so if minus 1 is actually modeled as 8 7 minus 2 if it is modeled as 6 minus 3 as 5 minus 4 as 4 then all those 8 minus 4 or 6 minus 4 everything could be handled as additions Hey so let us go and look at this now I am this is completely we did it with using unsigned numbers just to have some clarity the moment I want to go ahead and have signed the numbers then I told you the most significant bit should be representing the sign so in this case there are 4 digits which are positive because their most significant bit is 0 and there are 4 digits for for representations are 4 combinations that are have a one in their most significant bit and hence they can be they are negative so let us go and number them so I will call this as minus 1 minus 2 minus 3 and minus 4 now what happens please understand 7 is modeled as minus 1 6 6 is modeled as minus 2 5 is modeled as minus 3 4 is modeled as minus 4 so we have achieved what we want so in a to complement trip this is called the two's complement representation why it is called two's complement I will explain you towards the end but what we have done again simply we have listed down all these numbers as a cycle and all the positive numbers we have done increasing from 0 to 3 all the negative numbers we're in the most significant bit are 1 from 0 anti-clockwise I have numbered them as - 1 - 2 - 3 - 4 so if I want to do say 2 minus 1 I start from 2 and instead of going one step on to clockwise what I need to do I have to do seven steps clockwise and as minus 1 is represented as 7 if I just take the representation of minus 1 and I had it with this 2 it essentially means it is going to take me seven steps clockwise because minus 1 is represented as 7 I want to do 3 minus 2 so I can go from 3 and go two steps anti-clockwise or I can go eight steps clockwise to reach this one and since 2 and sorry I can go six steps clockwise to reach this one and since - 2 model already at 6:00 right so if I just add this three to this representation of minus two I get this right so when I represent so in the two's complement representation let us take three bits your minus one is represented at one one one your minus two is represented as 1 1 0 minus 3 is represented as 1 0 1 minus 4 is represented as 1 0 0 your 0 is represented as 0 0 0 1 is represented as 0 0 1 2 is represented as 0 1 0 3 is represented as 0 1 ok so two things we get here first thing is all positive numbers whether a number is positive or negative we can immediately find out using the first bit the first most significant bit so for all the positive numbers the most significant bit is 0 and for all negative numbers the most significant bit is 1 and the second thing is by this two's complement representation we can model subtraction as addition for the simple reason as I again mentioned I am going to tell you suppose I want to do 3 minus 1 it is 2 steps anti-clockwise our it is 6 steps clockwise and since -1 is modeled as a sorry seven steps clockwise since -1 is modeled as 7 it automatically whenever I find -1 is of going one step anti-clockwise I will do seven steps clockwise whenever I see - - instead of going two steps anti-clockwise automatically when I do an addition it goes six steps clockwise which is what we want here right the interesting thing here also is that suppose I want to do minus three minus minus one which is nothing but minus three plus one so if I want to do minus three plus one I give the representation of 1 0 1 and 1 1 0 so so minus 3 plus 1 is what minus 3 plus 1 is minus 2 right so if I give the representation of 1 0 1 and 0 0 1 automatically I get 1 1 0 right suppose I want to do minus 2 minus 1 minus 2 minus 1 is nothing but from minus to go one step anti-clockwise are do seven steps clockwise so 1 2 3 4 5 6 7 I get minus 3 so even minus 2 minus 1 you just take the representation of 1 1 0 and the representation of minus 1 which is 1 1 1 and then just add it so if I add this 1 1 1 with 1 1 0 I get 1 plus 0 is 1 1 1 is 0 and I get it's just ignore the next next digit so I get 1 0 1 which is nothing but minus so I just take the representation of these numbers and give it to the adder that'll just treat this as 2 unsigned numbers added and the result answer I can basically interpret it in whichever way I interpret it back and I get the correct answer so let us do a set of 3 to 4 editions here to basically explain this in more detail let us say 4-3 sorry this is not possible in crib it arithmetic because the maximum I can represent is only 3 so 4 minus 3 is not actually possible in 3 bit arithmetic so let me let me say 3 minus 2 so what is 3 0 1 1 what is minus 2 1 1 0 add it I get 1 1 plus 1 is 0 1 plus 1 is 0 and I get an extra one that goes off because I can only represent 3 bit so what is 0 0 1 0 0 1 is 1 0 0 1 is 1 so 3 minus 2 equal to 1 works let me do say minus 2 plus 2 so representation of minus 2 is 1 1 0 representation of plus 2 is 0 1 0 so add this I get 0 1 plus 1 is 0 carries 1 1 plus 1 is 0 carries one carry goes so the answer is 0 let me do minus 2 minus 1 the representation of minus 2 is 1 1 0 the representation of minus 1 is 1 1 1 add them 1 1 1 is 0 1 1 1 is 1 1 goes off I get 1 0 1 and what is the representation for 1 0 1 it is minus 3 it's also correct please do understand that I could not do 4-3 because the maximum positive number I can represent is 3 I cannot also do anything like minus 5 plus 1 I cannot do because the minimum representation I could have is only minus 4 please note that so assuming that the inputs are within this range all the inputs that I am going to get should be within this range say 1 2 3 0 1 2 3 and within this range of minus 1 2 minus 4 I cannot have anything less than minus 4 nor something greater than 3 now let us take two interesting situation suppose I want to do 1 plus 3 what is 1 0 0 1 3 is 0 1 1 when you had this 1 1 is 0 1 1 is I get 1 0 0 and note what is 1 0 0 it is minus 4 why I am getting such a wrong result because the answer godbye this addition is out of the range of what I could represent it is more than 3 I am getting minus 4 here right and so this is basically called as overflow similarly let me do say minus minus 2 minus 3 so what is minus 2 1 1 0 what is minus 3 1 0 1 I get 1 1 1 1 is 0 I get 0 1 1 what is 0 1 1 this is plus 3 which is wrong why I am getting plus 3 here because - five which is the correct answer is out of the range of this it is less than - for this basically I can call as an underflow so how do I detect this overflow and underflow please note that an underflow can happen assuming that the inputs are within this range an underflow can happen only if I am going to have two negative numbers for example I should have - 2 - 3 I can't have one positive number and another negative number and create an underflow because the least positive number is zero it or the most positive number is three and and the and the least positive number is minus 1 or minus 4 I can't use a positive number and a negative number and create a underflow because you take any combinations of one number being positive and another number being negative do do an arithmetic on it you can never create an answer that is greater than three nor it is less than minus five so if I have one positive number and one negative number as my input to the adder the result will never be and both these numbers are within this range of minus 4 2 + 3 I can never create an underflow or overflow that we can recently verify here the very straightforward exercise so an underflow is created only when I am handling two negative numbers your flow is created only when I'm answer when I'm handling two positive numbers so when I take these two negative numbers and do the addition and if I get a positive number as an answer then it is an underflow if I take two positive numbers and I get a negative number as an answer then it is a overflow essentially it says that I have crossed this boundary or I have crossed this boundary I am crossing this boundary so when will I get a overflow when both the numbers are positive and I get a negative number as an answer as you are seeing here or when both the numbers are negative and I get a positive number as an answer as you are seeing here these are the cases where I will land up with the underflow or a overflow now so this is another thing so it is very easy for somebody to check the word flow or the under flow if you look at traditional computer architecture there is something called a flag register which basically tells you whether the previous arithmetic operation had indeed landed up with the overflow area under flow rate if it has landed up with the word flow or an under flow that means your arithmetic is wrong so the program has to take some remedial measures are come out and indicate to the user that something has gone wrong in the arithmetic so this is very important whenever we model arithmetic operation it is very important that we detect the overflow or underflow now the very easy thing for us to do in this arithmetic is when the two numbers are positive and the answer is negative then there is a overflow when two numbers are negative and the answer is positive then there is an underflow so a simple logic can be used with some basic someone gets can be used to detect this this particular overflow or underflow condition so now with this three bits what did they what could a represent I can in the two's complement arithmetic I can go from minus 2 power n minus 1 to 2 power n minus 1 minus 1 essentially I could represent one more number than the sign-magnitude with 3 bits I can go from minus 2 power 2 which is minus 4 to plus 2 power 2 minus 1 which is 3 well this is called the two's complement please understand that I want see the cycle has so many steps to power n steps and I said instead of moving say K steps clockwise I would like to move to power n minus K steps and so the case the instead of moving stay K steps anti-clockwise I can move to Perth n minus K steps clockwise this was our argument in the whole of those arithmetic if you remember I said instead of moving six steps clockwise I can move eight minus six so everywhere instead of moving three steps anti-clockwise I can move eight minus three steps clockwise rather that eight come eight came because of to power three and that is precisely what we are doing minus one so I want to represent minus K as 2 power n minus K if they represent minus K as 2 power n minus K the moment I have minus K since it is represented as 2 power n minus K subtracting K from a number can be modeled as adding 2 power n minus K to that number that's what we have seen here so then precisely that we have done so this is this is 0 this is 2 power n minus 1 and that has become minus 1 this is 2 power n minus 2 and that has become minus 2 to power n minus 3 that has become minus 3 to power n minus 4 that has become minus so exactly that is what we are seeing here so your to power in your minus K is actually represented as 2 power n minus K in this representation now how will the given -1 to the compiler how quickly will the compiler convert this minus 1 into 2 power n minus 1 so I am given minus 1 I should realize 1 1 1 how will I get this what I do is I take minus 1 which I take 1 the magnitude path alone I neglect the sign bit so what is 1 in 3 bits it is 0 0 1 then I compliment every bit so it becomes 1 1 0 then I add this ok right so I get one one one so so what I the wainy represent - one is that we take a number and complement every bit sorry we take a number complement every bit and add one to this so we get 1 1 1 so let me do one more addition for you let us take minus 2 so you take 2 which is 0 1 0 complement all these bit you get 0 1 1 and I add 1 to this which is going to give me 1 0 sorry 1 1 0 sorry so I take - two - two first take two which is zero one zero compliment every bit I get one zero one add one to this which I get 1 1 0 which is nothing but minus 2 let me do one more example - 3 take 3 which is 0 1 1 compliment every bit I get 1 0 0 add 1 to this I get 1 0 1 note that 1 0 1 is minus 3 correct so how do you realize that two's complement representation of a number if it is a positive number it is just that positive number itself around plus 1 it is just 0 0 1 but if it is minus what you do is you take the binary representation of the magnitude complement every bit and then add 1 to that since you are adding 1 so just the complementing every bit is called ones complement to which I am adding one more so it essentially becomes two's complement okay so why does this work if I take a number please understand that if I take a number and complement it and add it suppose a say n bit number a plus a complement is 2 power n minus 1 A plus a complement is 2 power n minus 1 why suppose I take any number say say 3 for example I complement every bit 1 0 0 I add them I get 1 1 1 which is 7 which is 2 power 3 minus 1 let us take another number 1 I complement every bit and I add I get so if you take any number complement every bit and then add I get 2 power n minus 1 to this I add in our operation after complementing these bits I all add also one so if I say a plus a compliment plus 1 which is nothing but 2 power n 2 power n so a compliment plus 1 is equal to 2 power n minus a and this is exactly what I want I want - a to be represented as 2 power n minus a and what is 2 power n minus a is nothing but a complement plus 1 so if I complement every bit and I add 1 to it I get the desired representation 4 minus a so the compiler when it sees a positive number it immediately represents it in the equivalent binary form when the compiler actually sees a negative number the way it is translated to two's complement is you take the binary representation compliment every bit and then add 1 to it and you basically get the two's complement representation for that negative number and the proof for this is shown here so I we will be giving a set of tutorials by Monday wherein we you will have these type of arithmetic you can practice this meanwhile you can go and refer your old books like he mature computer organization Maurice Mann Oh digital logic design all these books have tried and explained this to complement arithmetic so he can take some examples and work for yourself so I will open up for some questions now so why can't you use he uses a little bit as a sign bit sir why can't you use the least significant bit as a sound bit why you can't be used least significant bit as a sign bit yes I am aya got your question why can't we use the least significant bit as the sign bit correct why can't I say yes if the entire computation that will appear will go burÃ§ak if I use the least significant bit the the sign bit as the least significant bit right and secondly it is more of an human so we can say that we essentially what we can do is now push this bit to the last right and then do the arithmetic on the remaining bits but it's not just you try operating like this by taking the least significant bit as a sign with the entire operation of modeling addition using some modeling subtraction using addition we'll go burÃ§ak number one number two is that we need to have representations that are actually close to the human perception in a human perception you never say - - as - - right you only see - - as - the - is always on the leftmost side so even from a human perception point of view we see that this representation works right so that is again from there so somebody sees a number so the computer dumps a number I want to find out whether it is negative or positive I would normally try and see the most significant bit rather than the least significant bit in addition the least significant bit can be used for another constructive purpose like whether it's an odd number or an even number that's how we did it - given a decimal representation we go and test the last two digit for it's even our prayer odd and we say that the entire number is odd or even similarly I can go tastes a bit and find out whether it's odd or even leave alone that if I am going to use the least significant bit for doing this to complete this arithmetic this whole operation becomes extremely complex you may realize it but it's not going to be as easy as this for example if I want - say I can't do some simple arithmetic like this please note that every conversion of this should happen very fast I need very quick algorithms to do these type of number representation because these are fundamental I can't actually run a loop testing every bit to convert a minus eight into its corresponding two's complement or the corresponding representation I need a very simple way of converting that and this actually provides you that way right so I leave it as a very simple exercise to you to go ahead and find out what it means to make this least significant bit as the sign bit and try and do just that this three bit arithmetic and come out with a circuit which can realize this arithmetic right please note I have only an unsigned adder I should use an unsigned adder adder which will take two numbers as unsigned numbers and add and give you the result now I have to model subtraction and signed arithmetic using this and same data and for doing that please try your representation and you will understand how difficult it would be to implement you will realize certainly there is a way of realizing because everything is boolean function but the the amount of circuitry needed would be extremely complex I am giving an example for three bits because if I draw four bits it would take almost the entire class for me to complete that four bits representation now imagine we are looking today at 64 bits so imagine the size of the circuitry when I am looking at 64 bit conversion of - a - - / - a into whatever representation you want so this but issue of looking at large size which is called scalability so when you design such type of hardware when you propose this type of representation we should also look at the scalability point of view so these two are my answers to your question so that the so to sum up yes you can realize you can make the least significant bit as a sign bit where then the entire solution will become unviable for doing specifically for modeling subtraction as a as a addition of unsigned numbers doing signed arithmetic using an unsigned adder would be the real complexity okay okay 
XerzI6WATM4,27,,2018-10-22T12:29:00Z,1 1 1 Definition And Objectives,https://i.ytimg.com/vi/XerzI6WATM4/hqdefault.jpg,Prof. Dr. Ben H. Juurlink,PT10M35S,false,21283,224,1,0,4,[Music] welcome to this first lesson of the specialization on high performance and a better computer architecture my name is Benny Yulin and it's my honor to be the instructor of this course the purpose of this lesson is to give you a definition of computer architecture and to present a high-level objectives of this specialization every day we use many computers without really thinking about them computers really have changed the world I can give many examples of this but in the interest of time I will give only one the announcement of the new pope in 2005 and 2013 this picture here shows a photo of Saint Peter's Square during the announcement of Pope Benedict in 2005 as you can see at that time there were not many cell phones there's a person here on the right who's taking a picture and there's a person on the Left who seems to be reading a text message this picture on the other hand was taken from the same few points during the announcement of Pope Francis in 2013 now everybody is using a cell phone and the person here on in the middle is carrying a tablet computer each and every cell phone contains a small computer a small processor and the architecture and organization of such computers is the topic of this specialization let's start with defining what computer architecture is this is a definition that used to be on the www computer architecture web page and that also used to be on my home Pope the home page computer architecture is the science and art of selecting and interconnecting hardware components to create computers that need functional performance and cost goals so computer architecture is not only a science but also an art implying that imaginative or technical skills are involved so the more it's about selecting the most appropriate hardware components and connecting them in the best way to build computers that can deliver a certain functionality with the highest performance and with the lowest cost possible computer architecture has nothing to do with using computers to design buildings publishers often send me building architecture books but these publishers don't really understand my profession this is a picture of the world's first electronic general-purpose computer the ENIAC designed by john mauchly and jay presper eckert of the university of pennsylvania its construction started in 1943 and it was announced in 1946 we have come a long way since then let me now present the objectives for this specialization the objective of the first course fundamentals is to review performance processor instructions pipelining and the basics of caches for example this figure illustrates Amdahl's law it shows the speed-up as a function of the number of processors for different parallel fractions there are not many equations in computer architecture but this is one equation that you should know by heart this shows a snippet of mips64 assembly code after the first course you will be able to program an optimized mips64 assembly program this figure illustrates the canonical five stage pipeline consisting of the stages instruction fetch instruction decode execute memory access and write back I will review this pipeline organization as well as the challenges it has and how they can be solved finally this figure shows the organization of a 2-way set-associative cache in the first course I will review the basics of caches so that you will be able to for example compute the number of caches and cache misses given a sequence of memory references and a certain cache organization in fact I assume that you will know most of these topics since they are typically in introductory computer architecture courses but since my students come from everywhere they do not always have the same background therefore the main objective of the first course is to align these different backgrounds one of the objectives of the second course is to explain how instruction level parallelism parallelism is dynamically exploited in modern processes here I will discuss topics such as out of order execution register renaming branch prediction speculation etc to give you a preview of these topics this figure shows a block diagram of Intel Nehalem microarchitecture which was the organization of the first core i7 and core i5 processors unfortunately this slide is too small to clearly see all the components therefore I will highlight some blocks this block called reservation stations is needed for out of order execution out of order execution means that instructions are executed in an order different from the normal program order in modern processes instructions are executed as soon as their operands are available and the functional unit that can execute the instruction is free the Nehalem microarchitecture has six functional units it can execute six instructions in parallel this block the register allocation table performs register renaming register renaming as the name implies renamed the registers and this eliminate certain dependencies which allows more instructions to be executed in parallel the Nehalem microarchitecture actually has two register allocation tables since it can execute to three threats in parallel this block the reorder buffer is needed for speculative execution speculation means that an instruction can be executed before it is known that it should be executed finally this block performs branch prediction branch prediction means that the branch direction and the branch target address are predicted based on the history of the branch and perhaps also on the history of other branches Intel processors have very advanced branch predictors and I will explain how they work in the second course I will also explain how you can accelerate your applications using so-called Cindy instructions Cindy or SIMD stands for single instruction multiple data these are instructions that process multiple data data elements at the same time in this example two registers x and y contain four data elements each and the operation star is applied simultaneously to all elements by a single instruction many applications can be accelerated significant by significantly by saying the instructions and therefore it is important that you know about them another objective for the second course is that you should be able to name and describe several core multi-threading techniques core multi-threading means that a single processing unit or core as they are called nowadays is able to execute several programs or threads at the same time to provide a preview of this topic this figure shows a part of the Nehalem microarchitecture highlighted in red or the retirement register file and the register allocation table notice that there are two of each this is because each core is dual multi-threaded or hyper threaded hyper threaded that likes to call it to support it Intel has duplicated the retirement register file and the register allocation tables but the reorder buffer the reservation stations as well as the functional units are shared the topic of the third course is advanced memory hierarchy design after taking this course you should be able to describe and apply several advanced cache optimization techniques for example this figure illustrates attacked a technique of a prediction this technique predicts the way of the next cash exists it attempts to combine the advantages of set associative and direct map cache as I will explain in the third course the fourth and final course covers multi-core architectures modern desktop and server processes contains several cores and can execute several processes at the same time for example this figure shows a die photo of a processor with 4 cores with private level 1 and level 2 caches and a shared level 3 cache in this course I will discuss topics such as Snoopy cache coherence distributed shared memory atomic read-modify-write instructions etc this specialization is based on the textbook computer architecture a quantitative approach by John Hennessy and David Patterson you can either use the most recent edition the fifth edition or the third edition I am NOT a big fan of the fourth edition sometimes the lessons will be based on other sources such as research papers or other textbooks I remarked that although the specialization is based on this textbook it can also be taken without the book nevertheless it is a classic textbook in computer architecture that should be present on the desk of every computer architecture finally let's enjoy the right you [Music] 
mIGRlytVQ4s,27,"Different Types of Computers | Computer Organization and Architecture

#BikkiMahato
The best part is: it is all completely free!
------------------------------------------------------------------------------
Follow :)

Youtube : https://www.youtube.com/c/BikkiMahato
Facebook : https://www.facebook.com/mahatobikki
Facebook Page : https://www.facebook.com/youtubebikki
Twitter : https://twitter.com/mahato_bikki
Instagram : https://www.instagram.com/bikkimahato
Google+ : https://plus.google.com/u/0/+BikkiMahato
Blogger : https://bikkimahato.blogspot.in
Pinterest: https://in.pinterest.com/bikkimahato123/
LinkedIn : https://www.linkedin.com/in/bikkimahato

------------------------------------------------------------------------------

DONATE Support :)
Patreon : https://www.patreon.com/bikkimahato
Instamojo : https://www.instamojo.com/@bikkimahato
Paypal :  https://www.paypal.me/bikkimahato",2018-06-10T11:31:48Z,Different Types of Computers | Computer Organization and Architecture,https://i.ytimg.com/vi/mIGRlytVQ4s/hqdefault.jpg,Bikki Mahato,PT2M58S,false,1223,14,1,0,0,welcome everybody so today we'll be continuing computer organization and architecture so today we'll be studying types of computer there are four types of computer from supercomputer mainframe computer mini computer and microcomputer so first we will start from supercomputer so these are the most powerful computers in terms of performance and data processing so these are specialized for specialized enough task specific computers used by large organizations like very large augmentations so these are used for research exploration purposes like nasa uses supercomputers for launching space shuttles and controlling them and for space exploration so this is one of the examples of supercomputer so next we have mainframe computer so mainframe computer run business operations so like the manor of the builders can be accommodated in large air-conditioned room because of its size and supercomputers are faster computers with large deters computer but maintance mainframes can also persist and store large amount of data so these are comparatively slower transfer computer so banks education institutes in service companies these use mainframe computers to store data about their customer students and insurance policies and it is faster than mini computer or microcomputer so next we have mini computer so many computers are used by small business firms small business firms it is also known as mid-range computers so these are small machines that can be accommodated on is not as processing and it as storage kept this will it is a super computers or mainframes so these computers are not designed for single user so individual departments of a large company use mini computers for specific purposes a protection department will use many computer for monitoring certain products and processes so finally if we have microcomputers basically all general-purpose computers are known as micro computers these are like a personal computers so desktop computers laptop personal distills PDA assistants tablet smartphones are our types of micro computers so these are widely used growing faster because these are very small and cheap and I say cheap point is also there so these are designed for general user like entertainment education and work purpose so the manufactures of utah dell at the mall this so this was all about types of computer sutured any problems just comment and if you didn't like the video hit the thumbs up button and subscribe to my channel for more videos i'm so watching it 
eleRJ-y9qo0,27,"RGIT Nandyal - NPTEL Videos (CSE Department)

Website : http://rgitnandyal.com/",2015-06-22T09:36:00Z,Iron Law of Processor Performance,https://i.ytimg.com/vi/eleRJ-y9qo0/hqdefault.jpg,RGMCET Nandyal,PT56M6S,false,141,2,0,0,0,you hello and welcome to today's lecture on performance and obviously by that I mean performance of the computer system and here is the outline of today's lecture after giving a brief introduction I shall define what do you really mean by performance then I shall discuss about the iron law processor performance the various factors on which the performance depends and then we shall discuss what do you really mean by processor performance enhancement and then I shall discuss about performance evaluation approaches after the performance is defined how can you evaluate the performance and the various approaches that can be used and another very important aspect is performance reporting how you can report the performance by using a single number and finally I shall conclude my lecture by discussing Amdahl's law which is related to performance measurement performance measurement is very important because it helps us to define one processor to deter if one processor works faster than the other that means we are discussing about the high-performance computer architecture obviously we shall be discussing various techniques by which the performance of the processor can be improved or enhanced and in that context of the performance plays a very key role and we have to see how really you can measure performance and see it so also it helps us to know how much performance improvement has taken place after incorporating some performance enhancement feature so you will see that we shall be incorporating various performance enhancement features whether in a compiler or in the organization of the computer like pipelining and various other things and by doing so how the performance is improving that we shall know from this performance measurement and it also helps to see through the marketing hype you know there is a marketing hype that whenever a new processor is introduced people say this processor is so much better than other processors so this how true is that hype that can be evaluated with the help of hop with the help of performance measurement it will also provide answers to the following questions number one is white is some hardware better than others for different programs so it will be measuring performance obviously by running programs and how a particular processor is performing better for a particular program than other program so that particular analysis is very important and that answer you will get from this particular topic then what factors affect system performance you will find that there are various factors which affects the performance obviously the first thing is the hardware hardware means the processor by with the way it is implemented and incorporating arithmetic and logic unit then register file control unit and so on so the hardware him which implements the processor that will definitely play a very important role second important parameter is operating system operating system will schedule various talks to the processor and obviously operating system will also play a very important role in deciding the performance of a processor and also you know you will be using whenever you will be writing program in a high level language it is like see Fortran and other programming languages it has to be converted into machine language before you can run on a processor and obviously the the efficiency of the compiler the performance of the of the compiler will also affect the overall performance of the system so these are the various factors which will affect the performance and last but not the least how does machines instruction set affects performance you know a particular processor is characterized with the help of a instruction set as I have briefly discussed in my last lecture now then that instruction set can be different for example it can be a for a risc processor there can be one type of instruction set for sis processors the instructions said is different and as the depending on the complexity and nature of instruction which we shall call as instruction set architecture of the how the performance depends on that so that that those things those aspects are to be starting to question is how do you really measure performance obviously it is time time time and time time is the ultimate measure of performance and for what kind of time you will find that a computer exhibits higher performance if it executes program faster obviously whenever you are trying to measure a performance you will be measuring the execution time faster is the execution time the performance is better so but whenever you measure the time there are various ways you can do it for example if you think from from use individual perspective then response time or elapsed time is the factor which is important to an individual user an individual user will submit a job submit a task and after the task is submitted when he gets the result that is the elapsed time that time is important to an user because that is the time for which one has to wait for getting the result on the other hand for a system manager the perspective is different they are from system managers perspective throughput is the most important parameter because he is more interested in a in how many jobs can be machine run at once that means in a multi-user multiprogramming environment a computer is executing many tasks of many users and obviously how many tasks per unit time is executed by the processor that is important to the to the to the to the system manager and what is the average execution time and how much work is getting done by the computer so the you can see these two are not really same response time and throughput these two are not same and from individual individual point of view people will be interested in response time for system managers point of view he will be more interested in throughput let us see what you really really mean by elapsed time so it counts everything disk and memory accesses waiting for i/o running other program etcetera from start to finish that means a job will be submitted and a fraction of CPU time a particular user will get during the running of his task and obviously there will be other times like the time required to switch from one task to another task waiting time for i 0 and the operating systems I am these are the various times that will come so you can state it in terms of a number and that that number elapsed time is CPU time plus wait time wait time can be for waiting for i 0 it can be waiting for other programs running etcetera it can be for because of page fault later on we shall about all these things then comes the CPU time so the CPU time does not count waiting for i 0 or time spent running other programs so it simply kind finds out the time required to perform a particular task so it can be divided into CPU time user CPU time plus system CPU time obviously operating system calls will be involved when a particular user is running a program so CPU time is equal is equal to user CPU time plus system CPU time then elapsed time is equal to user CPU time plus this system CPU time plus wait time so that means elapsed time which a particular user encounters can be divided into three component user CPU time the system CPU time plus wait time but for this particular course our focus is on user CPU time we shall not be bothered about system CPU time or the wait time when the processor is running somebody else's job so we shall be primarily concerned about user CPU time that is the CPU execution time or simply we shall call it execution time so that is the time spent executing the lines of code that are in our program so a particular user is running a program and how much time is required to run that particular program that is the time that will be used as the as a measure of performance now whenever you try to measure a performance you know performance is a relative thing for some program running on machine x is equal to performance x is equal to 1 by execution time so execution time and performance is inversely related because larger the execution time performance is worse smaller is the execution time performance is better so that means a larger execution time will lead to inferior performance now whenever you compare x with y x is n times faster than y by that we mean performance x by performance y is equal to n so this is how you we shall try to measure performance now before I go into this topics another very important aspect I should tell you know you may be knowing that a computer is nothing but a sequential circuit a sequential circuit or finite state machine will require a clock so we are interested about time how do you relate time actually it is related to clock a computer is controlled by a clock the processor so a clock is nothing but a repetitive waveform you know and this is called the time period tan MP time period of the clock usually it is stated in terms of second or maybe whenever it is very small it is stated in terms of micro second or sometimes it is nanosecond as the speed is increasing so it is some form of time in terms of second maybe micro second nanosecond so this is this is this may be called 11 clock period or time period so if tau is the time period then the frequency of the clock is related to time period in this manner is 1 1 by tau so sometimes you will see we shall try to express the execution time in terms of number of clocks because ultimately the processor is controlled by a clock number of clocks is related to the execution time so let us see how we are going to express performance in terms of these various this clock frequency or clock time period so processor performance is equal to time required to execute a program and it comprises three components so we can say that you know a program is a set of instructions then an instruction requires few cycles maybe one few cycles for few clock cycles to execute and instructions then one cycle will require some time so you can see we have got three important parameters a program can be decomposed into a number of instructions which we may call instruction count then each instruction instruction will require one or few cycles and then each cycle will take some time so this is what is shown here so that means the processor performance is nothing but the instructions number of instructions per program you may call it instruction count so it depends on the size of the code and then the number of cycles per instruction that is known as CPI of cycles per instruction so that the CPI that cycles per instruction can be one can be more than one and later on we shall see it can be less than one as well so and then cycle time clock cycle time I have already explained that is the time period this is the clock cycle time that time period is can also change and so it's a you have to consider all these three parameters and you will see that these three parameters is affected by three important aspects one is your architecture architecture is represented by the instruction set architecture I have briefly discussed in my last lecture and in the next lecture I shall elaborate it in more details then the the processor is implemented for a given instruction set architecture so processor instruction set processor is essentially is represented by the implementation so then comes the realization realization is implementing the a particular processor instruction set processor with the help of some electronic circuit maybe transistors or integrated circuits or VLSI chips so you can see there are several designers are involved in the end up main in executing a program or in the design of the system compiler design at processor designer and chip designer and their design will affect the overall processor performance so these three factors are to be considered when we are considering processor performance so actually this is this is known as iron law processor performance is this but the product of these theorems code size that is instruction count CPI and cycle time now that number of instructions per program or the instruction count will depend on obviously you may be asking what do you really mean by number of instructions per program say a program that source code will consist of several instructions now that is static in nature but whenever you execute then we call it that number of instructions that is executed by the processor so that means the size of the source code has got nothing to do with the or is really independent on the dynamic size of the code that is executed by the processor so we are more interested on the dynamic size not the static size of the code so static size of the code can be very small say you are you have written a program where it is looping so we are looking within this so size is this but it may be looping thousand times so that means this code will be refitted thousand times so you have to take into consideration that the dynamic size that means total number of instructions that is getting executed so do not get confused with the source code size so it is essentially the instructions executed not the static code size so this particular factor instruction cows count is dependent on three important parameters it is first of all it depends on the algorithm you are implementing a particular algorithm and that L the way the algorithm is implemented you can devise some better algorithm to reduce the size of the code it also depends on the compiler compiler can perform a number of optimizations to reduce the size of the code so those who have attended a course on compiler of compiler they must have studied various compiler optimization techniques so the size of the dynamic code will dependent on the compiler then last but not the least it will depend on the instruction set architecture that means the instructions which are which can be executed by the processor so you can see the instruction count is dependent on several factors then cycles per instructions is dependent on the ISA and the CPU organization that means that instruction set architecture as I said it can be sisk complex instruction set architecture it can be reduced instruction set architecture so depending on the complexity of the instructions the cycles that is that will require that means the average number of cycles that that is required by our instructions that means how do you measure CPI cpru you measure CPI is equal to total number of instructions executed or you can say time to execute number of cycles so not this total number of instruction cycles total number of not instruction number of I should say clock cycles by total number of instructions so these are the this the ratio we will decide the CPU so total number of clock cycles by total number of instructions for a particular program it so it is the dynamic and that number is dynamic dynamic in the sense while executing what is the total number of instructions so it is departed determined by the ice is a and the CPU organization the way the CPU is organized later on we shall discuss about in detail about this and overlap among instructions reduces this term so we shall discuss about different techniques for reducing the CPI like techniques like pipelining and other things by which this CPI can be reduced then the last term the cycle time it is determined by the technology organization and clever circuit design so first factor is determined by the technology technology means the VLSI technology as you know the VLSI technology is improving over time and you may have heard of Moore's Law I briefly mentioned in my last lecture about it and as per most law as you know the size of the devices is reducing every 18 months the size is becoming hop and as the size becoming half the capacitance is getting reduced and it is becoming faster so the technology is determining the cycle time cycle time in the early years it was it was micro second now it has become nanosecond because clock rate of the processor as you know earlier the first the clock frequency was a few megahertz now the clock frequency of the modern processors are stated in terms of gigahertz how it has happened that is primarily because of the invite management of technology and of course it is also dependent on the organization particularly pipelining and clever circuit design so you can see the three factors together decides the processor performance and obviously whenever you you decide about improving performance to improve the performance of the processor you will see that all processor performance enhancement technique boils down to reducing one or more of these three terms so in the here we have discussed about three terms you will see that whenever we try to improve the processor performance either we try to reduce the instruction count or we try to reduce the CPI or we try to reduce the cycle time or sometimes more than one of these two terms together we try to reduce so now the question is some techniques can f can be used to reduce one term without affecting others in other words what I am trying to tell there exists some techniques which does not affect others for example whenever the technology is enhanced when we are going from one technology generation to next technology generation circuit is becoming faster so other things are remaining same so the cycle time is getting reduced but CP CP I memory will be remaining will remain unaffected and also the instruction count also will remain unaffected if only the technology enhancement is considered so the one is improved hardware technology similarly whenever you go for compiler optimization techniques you you remove some dead code and various other compiler optimization techniques you use what happens by using these techniques the instruction count reduces without affecting CPI or cycle time so such type of performance optimization techniques are preferred because these techniques does not affect other parameter other parameters only it affects only one of the three parameters so these are however there are there exists other techniques which are interrelated some techniques can reduce one of the terms but may increase other terms so let me explain with the help of X example let us consider Sisk and risk so in whenever you go for complex instruction set architecture we know that instruction count reduces so instead instruction count reduces whenever you go for Sisk so on the other hand in case of risk this instruction count increases because number of instructions required for a cyst RISC processor is roughly three times maybe three times more than six processors so we find that one particular parameter is decreasing for Sisk and increasing for risk on the other hand if you consider CPI cycles per instruction then you will find a whenever you are executing a complex instruction obviously the number of cycles required to execute a complex instructions number of cycles would be more so it will increase the CPI on the other hand for a risc processor since the instructions are simple and you know the the number of cycles required to execute a simple instruction obviously the CPI will reduce so you can see whenever we we go for we go from I mean decide compare between sis can risk we can see for one but one particular parameter is getting reduced on the other hand other particular parameter is increasing so as a consequence we cannot really say that this is better this is inferior because they are interred if there is some kind of inter relationship so sis greedy is a reduces instruction count but increases CPI similarly another technique we shall discuss later on which is known as loop unrolling so with the help of loop unrolling what we do a particular loop is unrolled say for example a loop without unrolling may require the lesser memory in your program and obviously the whenever you do loop unrolling what happens the the number of instructions increases the the code size increases code size increases no sorry the dynamic code side will reduce because whenever you do loop unrolling the this the static code size reduces but increases but dynamic code size reduces because many loop unrolling those are decisions wherever you take if this if there will be decision making things those decision makings will be reduced whenever you go for loop unrolling so you will see the code size will reduce that dynamic code size I should write dynamic code size dynamic code size will ready we'll reduce in a loop unrolling technique that means we can say that instruction count will reduce however what what will happen whenever you are executing a program because of the you know the the static size of the code that you have to load in a program what can happen the it will lead to increase in CPI because of because of you know hazards whenever you r XZ your side the the static code size increases then the hazards increases as a result the cycles per instruction increases so later on we shall discuss in more detail about loop unrolling and we shall see how dynamic code size reduces but CPU can increase because of various hazards that may occur whenever you try to execute the code so the loop unrolling reduces instruction count but increases CPI so we can see there are many factors which are interrelated and in such cases we have to be very careful whenever you try to measure the performance by using these techniques earlier one very important parameter was meeps or mega mflops myth stands for million instructions per second and improv stands for million floating-point operations per second so these two were extensively used 30 years back as a measure of processor performance that means higher the MIPS rating the one used to consider by processor used to be considered faster similarly have the mflops rating a processor used to be considered faster but it has some drawbacks we shall see what you really mean by MIPS MIPS is instruction count by execution time into 10 to the power 6 or we can say that clock rate by CPI cycles per instruction into 10 to power 6 so mix mix can be calculated by executing a program and it can be found used for comparison but let us see what kind of problem we face whenever we use meeps as a measure for performance we encounter three significant problems when we use MIPS and these problems are so severe that somebody commented meaningless information about processing speed so although for many years MIPS were used as a metric for performance measurement for a long time one important factor is MIPS instruction set independent as I have already told that instruction set architecture plays a very important role in deciding the performance of a processor but it can be shown that MIPS is independent of the instruction set architecture the reason for that is you know it is dependent on the technology of the processor we have seen that clock rate by CPI into 10 to the power 6 so it can be it can be instruction set meets is instruction set dependent that means the instruction set dependency is occurring because you can see CPI is there CPI is instruction set dependent and as a consequence the meeps is dependent on the instruction set simply we cannot really tell in terms of MIPS we have to also take into consideration the instruction set architecture second days MIPS varies between programs on the same computer that means whenever you take different programs different programs and run on a single processor then we will find that MIPS rating is different for different programs that means you cannot really tell that MIPS rating when they may hire the mutating means this is better because for a particular program the value of nips may be better and for another program value of you may be inferior which I shall illustrate with example third problem is nips can vary inversely to performance I mean that is the reason why somebody commented meaningless information about processing speed so let us illustrate with the help of example and why myths does not work so let us consider the following computer and we are using two compilers compiler one and compile at two which are designed for the same processor same computer and we have got three types of instructions a category a category B and category c category a instructions require one cycles category B instructions require two cycles category C requires three cycles now a compiler one generates category a instructions which is five and category B generates one instruction of category B and category see another one instruction on the other hand compiler to generates ten instructions of category 1 1 instruction of category B and one instruction of category C so your CPI will be equal to cpu clock cycles by instruction count that is your CPI and now what you can do we can measure the total number of CPI into n take the summation by the instruction count for the for all instructions and then you can found out the CPI for process for the compiler 1 and CPI for compiler 2 so 4 CP CP I 14 compiler one is we have seen that the type of instructions is five that means I mean 55 is the number of instructions so it requires one cycle then one instruction of two cycles another one instruction for three cycles into 10 to power 6 and CPI and total number of instructions is 5 plus 1 plus 1 into 10 to power 6 so we get a CP I cycles for instruction is 1.43 that is for compiler one and so MIPS rating will be 100 megahertz by 1.43 that means so it is operating at 100 megahertz so 100 megahertz by 1.43 we get MIPS rating of 69.9 so that is the MIPS rating for process for compiler one on the same processor now CPI for compile at two can be calculated in the similar way 10 into 1 plus 1 into 2 plus 1 into 3 summation of that into 10 to power 6 x 10 plus 1 plus 1 into 10 to power 6 that is the total number of instructions that is your instruction count and this gives you a CPI of 1.25 so we find that leaves rating for compile at two on the same processor is 1800 message x 1.25 so we find that compile at two has higher MIPS rating so d so it should be faster because MIPS rating for compiler one for the processor is 69.1 and here it is 80.0 so the compiler to has a higher mix rating and it should be faster but now let us see whenever we translate it in terms of CPU time that is instruction count into CPI by clock rate for for for the compiler one we get that is equal to point 10 second that is the execution time and for the compile at the code that is generated by compiler to that execution time is 0 point 15 second so we find in this case and earlier we found that in terms of MIPS rating that compiler to was giving you better performance but on the other hand you know this is the CPU time corresponding to the code generated by compiler one is lesser so therefore program one is first a despite lower MIPS rating so we can say mix rating is not really reflecting the processor performance and what you can do you can calculate overall CPI in this way this is the instruction set architecture different types of instructions ALU operations fifty percent load instructions twenty percent store instructions transverse and branch instruction twenty percent and these are the corresponding CPI and these are the frequency of appearance in a particular program so for a particular instruction for a particular instruction for the particular instruction mix and you can find out the overall CPI in this way one in 20 point 4 plus 2 into 0 point 27 plus 2 into 0 point 13 plus 5 in 2.2 so this gives you overall CPI of 2.2 so for a particular program so we find this is how you can calculate CPI now whenever we try to measure performance you have to use some program what kind of program and these programs are known as benchmark programs so you can see the benchmark programs can be can have five different levels number one is real applications real applications that will be running in your computer in your day-to-day applications like compilers editors various scientific programs graphics pro at the fix applications and so on and unfortunately for these real applications there is a problem of portability because these applications will be dependent on the operating system as well as on the compiler for different computers the operating system can be different compiler can be different as a result portability is a problem whenever you try to compare performance with the help of real applications so instead of that one can consider modified applications that means you a you you you take you consider a particular application then you modify a particular application and tailor it and improve the portability so that the portability is improved or it can test specific features of the CPU so it specific features of the CPU I that means graphics feature or digital signal application DSP features that may be present those particular aspects can be especially tested by modifying the applications then the third level of benchmarks are known as Colonel's Colonels are very small and key pieces of real applications and since these programs are very simple can be 10 200 lines of code and examples of their those are livermore loops 24 loop kernels and linpack linear algebra package this can be used as for the measure of performance so these are known as kernels then the fourth category are toy benchmarks which are also simple programs maybe 10 200 lines of codes and which can which are easy to type and run on almost all computers and these are the applications which are typically given as assignment in your to the students maybe in the first year like quick sort merge sort these programs can be used can be considered as toy benchmarks which can be used for the purpose of testing however there is another level which are known as synthetic benchmarks so synthetic benchmark means you have created some benchmark to analyze the distribution of instructions over a large number of practical programs that means you have a instruction set architecture you want to test how different types of instructions are executed by the processors so some synthetic benchmarks are created and synthetic pro synthesize a program that has the same instruction distribution as a typical program however these programs have no real meaning to an user because these are they do not give you any meaningful result and examples of this synthetic benchmarks are dry stone corner kronor stone linpack these are the some bolder management problems nowadays however people depend on spec spec is system performance evaluation cooperative so recently this is the recently used popular approach where a collection of benchmarks are put together to measure the performance of a variety of applications so here we are not dependent on a particular applications we have chosen applications from different fields which are used to measure the performance so spec is a nonprofit organization this is their website www.sec.gov elip benchmark programs which which can be CPU intensive benchmark for evaluating processor performance for workstation so different benchmark programs have been developed for the measurement of performance of workstations servers and so on so this is the history of spec first round was spec CPU 89 10 programs yielding a single number then second round spec CPU ninth a 92 where six integer programs and 14 floating foreign programs were used then of course compiler flags can be said differently for different programs in this particular case third round was spec CPU 95 so you can see they have been enhanced and the benjamin programs have been changed at the processor technology improved so here eight interior programs and ten floating-point programs are used and in this particular case single flag setting is allowed for all programs we have seen in the previous case the compiler flags settings can be different for current programs then the fourth round is back CPU 2000 which is presently used spec int 2000 has got 12 integer programs and spec cfp 2000 as floating-point programs and single flag setting for all the programs and these programs are written either see either in C or C++ or 1477 or 1490 so here is the list for interior component of spec CPU 2000 so you can see there are 4 12 programs written in C or C++ and performing different functions like compression FPGA circuit placement and routing C programming language compiler and so on then you have got the floating-point component of spec CPU 2000 where you have got 14 different programs written in C or 1477 or 1490 and various functions which is performed are given here so you can see physics quantum chromodynamics shallow water modeling so various applications from different fields of scientific computing image recognition system equal propagation image processing computational chemistry number theory so instead of putting I mean considering a one particular application so different applications of different fields have been taken to evaluate the performance then comes the question of reporting how do you really report with the help of a single number so we have run maybe 14 proteins of 12 integer programs and 14 floating-point programs and those and they are to be compiled and a single number has to be used to give the measure of performance how can it be done so obviously whenever you want to do that you can visit this website for more details and documentation and whatever measure we use it should reflect the execution time so the single number result can be either arithmetic mean or it can be geometric mean of normalized ratios for each code in the suit it has been found that that arithmetic mean although it gives you some measure of execution time it is not very good because you know for differ if you take one computer is reference you get one value if you take another computer as reference you get different value so aretha meeting is mean has some lacuna or pitfall similarly geometric mean is all is although it is good because it gives a single number but unfortunately it does not give you the measure of execution time another another term that is used is weighted arithmetic mean which summarizes performance while tracking execution time so this has been found to be quite good so in addition to using a benchmark suite what you have to do you have to report besides description of the machine because platform is plays a very important role what cpu easy you are using what processor you are using what is the on-chip gasps what is the object cache what is the main memory size these these all these parameters will affect the execution time so this particular whenever you report you have to give precise description of the machine because if you change some parameter of the machine if you increase the cache memory size execution time can be different so platform information is provided has to be provided whenever you report the performance then comes the compiler flag setting so a report compiler what compiler flags setting has been used whenever you use different compilers for different programs like C C++ 1477 or 1490 okay our discussion will not be complete without considering Amdahl's law so it quantifiers overall performance can do to improve in a part of computation normally you know we cannot really improve performance of all the aspects for example we may improve floating-point processing by adding a coprocessor so only performance on floating-point program execution will improve but not they are not the other type of programs so in that context Amdahl's law states that performance improvement gained from using some faster mode of execution is limited by the amount of time the enhancement is actually used that means you have to consider the gain that has taken place for for that part only so you can say that speed up that is achieved by improving performance of a particular aspect is called is equal to the ratio of execution time for the task without enhancement by execution time for a task using enhancement that enhancement can come in different forms may be in the form of the CPU the the floating point processor and or the compiler or various other aspects so speed up tells us how much faster a machine will run due to enhancement and whenever you use em Dells law two things you should be issue one should consider number one is fraction of the computation time in the original machine that can use the enhancement so if a program executes 30 seconds and 15 seconds of execution uses enhancement that that fraction is half that means that entire program may not be using that enhancement that has been incorporated in the system so that that that that is what is being stated in this second is improvement gained by the enhancement you have to consider overall enhancement if enhancement task as 3.5 second and original task took seventh and seven seconds we say speed up is too now let us see the formula we can use for this speed up so you can say that execution time for the new system of the rain dance mat is equal to execution time hold into 1 minus fraction that is enhanced plus fraction that is enhanced by speed up that is enhanced so fraction of enhancement that takes place is taken into consideration in this formula and you can can find out the new execution time using this formula and then you can find out the overall speed up that is called that is equal to execution time by the old system and by the execution time by the new system and that you can find out from this that is equal to 1 by 1 minus fraction enhancement this one plus fraction enhanced by speed up enhanced so this formula there is do not do not try to just memorize it these equations and and these equations are and plug numbers into them so what you should do it is always important to think about the problems to you have to consider what problem you are testing for what application those problems are decided and accordingly you have to choose you have to see on what aspect the improvement has to be done and that will lead to fine enhancement in performance so we can summarize our lectures by mentioning the points to remember of this lecture first of all we have seen that processor performance is dependent on three factors the code size in that is instruction count then into CPI cycles per instruction and cycle time so you have to consider all these three together whenever you try to compare processor performance and particularly we have seen these terms are interrelated so you have to minimize time which is the product not the isolated terms so you may you may reduce cycle time but it may affect others as we have already seen so you have to consider all these three factors together so in the use of benchmark suit to measure performance I have already told the use of spec and you have to report with the help of a single number and to do that we have seen you can use different techniques but we have seen that weighted added metric main gives you good result because it ultimately tracks the execution time so with this we have come to the end of today's lecture on processor performance in the next lecture we shall discuss about the instruction set architecture thank you 
2FNcxAA_kBA,27,"This is the University of Utah's undergraduate course on Computer Organization.  Instructor: Rajeev Balasubramonian.

This video discusses the basics of the main memory system -- memory channel, memory controller, DIMM, DRAM/SRAM, bank, row buffer.",2015-11-28T22:30:04Z,"Video 70: Main Memory System Basics, CS/ECE 3810 Computer Organization",https://i.ytimg.com/vi/2FNcxAA_kBA/hqdefault.jpg,Rajeev Balasubramonian,PT7M53S,false,3653,11,0,0,0,in the last few videos we've talked about the on-chip cache hierarchy I'll now spend some time discussing the off-chip main memory system so we have a processor chip you have the processor core that we've designed in the past and then you have an l1 and an l2 hierarchy and if you don't find data in your own chip cache hierarchy you're going to go off chip this is usually done with a memory channel that comes out of the processor and connected to that processor is a memory module also known as a dim which is a dual inline memory module and this dim has memory chips you know both in the front as well as on the back side of this board and this collection of memory chips is is together referred to as the main memory system and you can have multiple channels and you can have multiple dims connected to each one of these channels now if you look at the on-chip caches they are designed with cells that are referred to as SRAM cells or static Ram cells each of these cells is nothing but a series of back-to-back inverters and because of this feedback loop once you put data into one of these cells that data kind of stays there as long as you have power now SRAM cells or caches built out of SRAM cells have relatively low latency but they also don't have very good density that means you can't pack a whole bunch of data into these on chip SRAM caches but you can access them at either one or two cycles if it's relatively small or tens of cycles if it's a larger l2 or l3 cache once you go off chip you know that you're going to spend hundreds of cycles getting the data right so once you go off ship the focus is not so much on latency the focus is instead on density that means you're trying to pack as much data as possible into a few chips right or in other words you're trying to minimize the cost per bit so given a certain budget you're trying to maximize the memory capacity in your main memory system and the reason to do that is you know once you run out of memory once your data does not fit in memory you are going to have to place your disk at the next level of the hierarchy which is typically the hard disk drive which has latency that is in a thousand times higher than main memory latency okay so to avoid having to go to disk you try to maximize as much capacity as you can in your memory system and you're trying to minimize the cost per bit right so that's why these memory chips are designed with the technology that's referred to as dynamic random access memory so each cell is actually nothing but a capacitor and what you're trying to do is put charge into this capacitor and depending on how much charge you put it's either going to be a 1 or a 0 so these cells are very dense because all it takes is a single capacitor and an access transistor for every bit of data and so you can pack a lot more data into every single chip the latency for access is going to be higher and the other problem with this memory is that once you put charge on the capacitor that charge tends to leak away over time so periodically you have to read the data in these cells and then write it back again so that you don't forget what data have been placed in each one of these capacitors and because of this refresh process this memory is referred to as dynamic random access memory right so in static random-access memory once you place data it does not have to be refreshed anymore but for the off chips main memory system because you have to periodically do this refresh process is referred to as dynamic random access memory now you could have you know 32 64 128 chips all coming together to provide a really large high-capacity memory system so like I said before you could have multiple channels coming out of the CPU each channel could accommodate you know multiple dims and each dim has many memory chips both on the front side as well as on the back side so all of these chips collected together give you a very high capacity memory system so now let's look at a few more details about exactly how memory is organized and so here's another figure that shows the processor on the processor you have a memory controller which handles all of your requests to the memory system and then you have this channel coming out it's broken up as address command wires or address commands signals and then data bus signals and on that channel you have these DIMMs that are plugged in so I'm showing you one side of atom over here and you know on the back side of the DIMM as well you have more memory chips so each one of these orange rectangles here is one memory chip and so when you make a request the memory controller essentially issues the request on this address command bus saying that here's the address that I'm interested in and here's the corresponding command either a read or write let's say now the memory is nothing but rows and rows and rows of data right so each one of these chips has just rows and rows of data there's really not much logic or intelligence on these memory chips right so these memory chips are pretty much dumb for the most part and all the intelligence required in retrieving data from the memory system is localized in this memory controller so the memory controller knows where all the data is placed it knows the state of all of these structures it knows if there are any structural hazards if there are any conflicts for certain resources and it makes sure that it staggers the request appropriately so that there is never any conflict for these shared resources so the memory controller issues a request and then gets the data back because this data access takes so long you are trying to hide some of that memory latency you can't really reduce that memory latency all you can do is perhaps hide some of that latency and the way to hide that latency is you know while a certain portion of memory is busy retrieving a block of data you can issue requests to other portions of memory as well so the memory system is broken up into many sub regions and you have something called a rank I won't get into details of that but essentially even a rank is broken up into many banks so you can view the memory system as a large collection of banks and each Bank can be involved in one memory transfer and all of these banks could be busy all at the same time working on different cache line requests so by having this high level of parallelism you can hide the fact that each access itself takes you know many hundreds of cycles so what I'm showing you in this example here is one bank and you'll see that the bank itself is strived across a whole bunch of memory chips so when you make a request for data you're going to read portions of the data from all of these many banks and each back brings in a bunch of data into a structure which is referred to as the row buffer right so bank reads out some data and puts it into this row before structure and then from here the data gets sent back on the data bus back to the CPU now the row buffer is actually a few kilobytes in size so when you make a 64 byte request you're essentially prefetching about a few kilobytes of data into this row buffer because a spatial locality if your next accesses are two neighboring cache lines that have already been brought into the row buffer that next cache line request can be serviced faster right so if your data is sitting in the bank it's going to be a fairly high latency if the data has already been brought from the bank into the row buffer and you're accessing something in the row buffer that's referred to as a row buffer hit and that takes a little less time to access so the row buffer is a little bit like a cache inside my memory system and so if you have spatial locality you can exploit that row buffer cache that is sitting inside the memory so there's a lot more about the memory system that I can get to but for the purpose of this class I think this high-level overview is enough this kind of conveys the very basics of the memory system and you learn much more about the memory system in a graduate level class 
MIWTxHbPBA0,22,"Computer Organization and Architecture
Prof. Kamakoti",2017-05-31T11:08:30Z,Computer Organization and Architecture - Introduction,https://i.ytimg.com/vi/MIWTxHbPBA0/hqdefault.jpg,Computer Organization and Architecture,PT4M11S,false,10269,42,2,0,2,[Music] welcome to this course on computer organization and architecture if you take the computer science and engineering curriculum the undergraduate curriculum there are three important core courses which are mandatory for any computer science and engineering graduate to a fully understood one of the course is the computer organization and architecture the other courses are the operating system and compilers now what we do in this course is to first distinguish between what we mean by computer organization and what we mean by computer architecture computer architecture is the basic digital hardware that we built the computer organization basically talks of the interface this Hardware gives to the operating system and compilers the operating system and the compilers which are basically driven by customer demands or the user demands one certain facilities for example a programming language needs to support pointers an operating system will need to do something called garbage collection a compiler actually want to do a position independent compilation now for doing all these things there is some support which the operating system and the compiler would require from the hardware so what are these things that are demanded by the compiler and the operating system and how can they actually use the hardware to get these things done basically study of that interface becomes the subject matter of computer organization now this interface typically we call it as application binary interface allows the compiler and the operating system to get certain functionalities done out of the hardware get some support from the hardware now how are these functionalities actually implemented in the hardware that forms the subject matter of computer architecture so we will deal both with the computer organization and the architecture but importantly organization is like swimming so that you cannot just take a theory class on swimming and you know produce the Olympic medal winner swimmer right so we actually to throw you in the pond and you have to start learning swimming there so computer organization education of computer organization is like learning swimming so we tend to have more labs lab oriented classes where we teach the computer organization aspects well the computer architecture we basically cover with more theory so this course would be for around 45 to 50 lectures and we will cover as much as possible and take you to ask deep as possible both in the area of computer organization and also in the area of computer architecture I hope you'll enjoy this and I also want to say that some of the contents of this course especially the computer organization part has been already covered in a MOOC course which is information security 2 which is hardware aid for information security I also suggest that you take that as a reading material as an advanced reading materials which can be a good complement to this course thank you very much [Music] [Music] [Music] 
QJhLQaP6gOg,27,"This video describes the overview of computer organization.
Processor.
Main memory.
cache memory.
I/O
Interrupts
DMA
Von Neumann Architecture",2016-09-13T10:11:42Z,Gate Computer Organization-1 | Introduction to Computer Organization,https://i.ytimg.com/vi/QJhLQaP6gOg/hqdefault.jpg,Learning Simplified,PT13M46S,false,17577,106,1,0,8,hello everyone this is Rahul from learning simplified in this series of tutorials we will learn computer organization so let's get started the word computer was derived from compute early mathematicians weren't having any computing devices as we have today we have calculators and computers to perform the automatic calculations but in those days the mathematicians used pen and paper to compute all the necessary calculations such as if you have a large number nine three four seven two nine one times four seven three nine one four if we want to calculate the result of this they used pen and paper to calculate so there was a necessity to invent something which can calculate these numbers on itself as the Greek philosopher said necessity is the mother of invention they have devised an electronic they have made an electronic device and called it as a computer it was a processor processor they came up with the processor which can process all this information they used to give input as the numbers and they used to get the result as the output from this processor but there was something lacking in this they wanted it to performs a task such that first it has to perform one task say t1 then it has to perform t2 then t3 without the user intervention that is it has to perform in this order let us say task one is addition task two is division and ask three is multiplication but the processor wasn't capable of doing that things because it had it did not have any memory it yes it has been in the form of registers but it is a temporary and very less memory to store this much huge information so they came up with something called as a memory to integrate with the processor memory and this memory consists of various cells these are cells where the memory is stored each cell can store either a data or an instruction that is addition division multiplication and so on we will see in the next future tutorials so this memory is used to store the information but there wasn't the satisfaction with the among the scientists so in where they invented the further went forward and invented something known as IO because they want to interact directly with the processor and get the result directly to them instead of storing first in the memory and the processor processes it and stores the memory into the memory again and looking into the memory for the result that was also a tedious task for us so this quotation perfectly suits our everyou evolution of processor and computer in this form what it does is processor requests data from the memory and memory accepts the request and gives the data to the processor what IO does is from the real where humans can interact with the processor it can directly give the input to the processor and we get output directly from the processor and we have memory as well so this is the big picture of a computer in the next upcoming tutorials we will see more about processor memory and IO in processor we see what are the types of instructions that a processor can handle everyone has their own language likewise a computer also allow or has a language with which it understands if we want to say a processor to add some add two numbers then we have to give the instruction in the format which the processor can understand so we will deal with instructions instructions and the processor has to know where the data is to process suppose it is at this cell then the processor has to bring that data from this cell so we will also deal with the addressing addressing of data in the processor itself and then then comes to the memory we will know what is a memory how the data is organized into a memory I want to tell you something here is the registers in the processor are fast very fast compared to this memory outside the process outside the processor so in order to that the processor calculates the information very fastly but because this memory is very slower the processor has to wait until the data from here is fetched into the processor so we have come up with another idea again there is a necessity here and necessity is the mother of invention we came up with something known as cache memory see ACH II cache memory which lies in between a processor and main memory what it does is it stores some block of data and when processor needs it it it gives the data to the processor and processor processes the data and stores back into the cash at some point we will see that this data is updated into the main memory again at some point of time this is the need for cash this is a bi-directional this means this is the flow of data and we can we also have a connection from main memory to the processor as well direct connection so we will we have to look at cache memory and this memory what in cat what to deal in cache memory is a processor generates the address with respect to the main memory but we have a cache in between these two so we have to map that address like this a processor processor generates an address an address which refers to the main memory but we have to map this address we have to map this address to cache memory so that it can refer to the correct data in the cache and process it so we have three address mapping techniques one is let me change the color one is associative mapping associate their next is set associative associative and the third is direct mapping direct mapping and next after we have finished the memory part we will deal with IO and IO is any physical device that is visible to the user suppose such as a keyboard or a printer or an LCD screen what how do we interact with the process directly is when you press a key a key a on the keyboard while you are on a word document or some notepad what it does is it check it takes the input and changes this a into ASCII format ascii American Standard Code for information interchange so that the processor can understand it processes this this data and renders it on the LCD screen if we have an LCD screen what it does is when we press na it goes into the processor an LCD screen has a large number of dots or points where it can color each and every point such that na can appear here it colors like this something like that to make an a appear on this LCD screen it does it in that way but in this course we are not going to look at the memory I mean IO devices we will deal with two important things one is interrupt what is an interrupt is as said here the processor performs different tasks such as addition multiplication it keeps on performing the tasks when you press an a it has to interrupt the processor to render that a on to the LCD monitor so that is an interrupt we are interrupting the processor while it is doing some tasks such as in a there are two to two types of lecturers who give the lecture for first 50 minutes they give the lecture boring lecture for 50 minutes followed by a 10 minutes doubt session if a student gets a doubt here or here he has to wait until the lecture gets finished and he has to ask the doubts here but there are other lecturers who make the session more interactive this is an interactive session if a student gets heard out here he interacts the teacher and teacher stops teaching what he is teaching it is the doubt of the student and he gets back to his teaching again another student gets it out here he asks again and Heather gets here he asks again so let us see in this topic how a processor handles the interrupts and we will see one more thing which is known as d-m a this stands for direct memory access which means a process this is to aid the processor a processor is a very fast device in all of these devices so when when a large number set of data is to be transferred from one location to other location a processor is always involved in the transfer which is a wastage of valuable processor time in that time it can compute other tasks so to overcome that we have a DMA control what the what what this does is the processor initiates the DMA to take charge of data transfer when the DMA takes charge of data transfer the processor performs other tasks which it has to perform when the data transfer is complete the DMA gives back the control to the processor and processor becomes the king again so we in short we are going to deal with all of these in this subject I find many people having difficult with the subject but I promise I make it very easy by the end of this course you will be very happy with this course now how do we integrate all these into a computer this was proposed by John von Neumann which is family known which is famously known as one Neumann architecture it has a memory as you have said this whole thing is a processor and these are the i/o devices these are connected with the help of bus this thing is a bus which bus is nothing but a set of wires that is used to connect all the devices we will look in more detail on about all these component components in the future tutorials I hope you have enjoyed the video see you next time until then happy learning 
lUcTMF_-pBk,27,"Computer organization lectures for GATE, Complete Computer Organization lecture series. Computer Architecture and Organization for GATE, Computer Organization tutorial. 

. _______________________________________

1. Digital logic design tutorial (DLD Tutorial):

https://www.youtube.com/watch?v=baF-cxSl8TA&list=PL4hV_Krcqz_J4K8dEFsqzm3zJIgzFF9MQ&index=2&t=16s

2. Computer Organization Tutorial: 

https://www.youtube.com/watch?v=ayJBTJLt4cQ&list=PL4hV_Krcqz_JaY3JmbrDgy5tipHrOmGBW&index=2&t=6s

3. Computer Networks Tutorial:

https://www.youtube.com/watch?v=yTnAB4IMU8g&list=PL4hV_Krcqz_KLIzfuShdbDiAdyrhJbwF6&index=2&t=1s

4. Operating Systems Tutorial:

https://www.youtube.com/watch?v=Fd9ucp6_hho&list=PL4hV_Krcqz_KyOBQEm6825QQJ6m2c1JRY&index=2&t=1s

5. Database Tutorial (DBMS Tutorial) | SQL Tutorial

https://www.youtube.com/watch?v=mwlKkUmhLeU&list=PL4hV_Krcqz_IbPUf3mAJbje5XQdPORrYi&index=2&t=0s

6. C programming Tutorial:

https://www.youtube.com/watch?v=zmLv-IjU000&list=PL4hV_Krcqz_JhUAojsTolbrTarJPrtzvM&index=2&t=1s

7. Algorithms Tutorial (DAA Tutorial):

https://www.youtube.com/watch?v=l51gzYCnA8k&list=PL4hV_Krcqz_L_qeClFzxcr9sJCIF5MUGe&index=2&t=10s

8. Data Structures Tutorial | DS Tutorial:

https://www.youtube.com/watch?v=56OA2C9Uxmc&list=PL4hV_Krcqz_KzWhCr3zJj3_z4wkSLSP7v&index=2&t=0s

9. Problem solving using Data structures and Algorithms Tutorial:

https://www.youtube.com/watch?v=wwWGOkYk500&list=PL4hV_Krcqz_LqMkNHswMN868hL1Klj2Li&index=2&t=1s

10. Probability Tutorial | Permutation and Combination Tutorial:

https://www.youtube.com/watch?v=6DeqpQFUPpI&list=PL4hV_Krcqz_Kp449S66_fmbaaVakWAAjc&index=2&t=7s

11. Interview Puzzles Tutorial:

https://www.youtube.com/watch?v=eOUYaaSkwq4&list=PL4hV_Krcqz_JyjXz-8DDzz3XgdkZiMF3F&index=2&t=258s .
.
12. Aptitude Video lectures for Placements | GATE | SSC | Bank PO | Quantitative Aptitude lectures

https://www.youtube.com/watch?v=oiPb-qAWME0&list=PL4hV_Krcqz_KjZl0UzQGnXTPC4xsbA0fS",2018-12-16T11:45:44Z,Pipelining-10 | Pipelining example | GATE 2011,https://i.ytimg.com/vi/lUcTMF_-pBk/hqdefault.jpg,GATE Video Lectures - Success GATEway,PT5M21S,false,2465,36,2,0,2,now come to one more example on pipeline it is gate 2011 question consider an instruction pipeline with four stages s1 s2 s3 s4 each with combinational circuit the pipeline registers are required between each stage and at the end of last stage delays for these stages and for the pipeline registers are as given in the figure that are the four stages s1 s2 s3 s4 Jessica delay 5 6 11 and 8 nanosecond and register Saint Gingka delay 1 nanosecond what is the approximate speed of of the pipeline in steady state under ideal condition when compared to the corresponding non pipeline option aboard I answered economy s is time in non pipeline upon time in pipeline a day on Dana what is the meaning of steady state it steady state kamini hair agar a pipeline may millions of instructions saying we will consider the middle situation not this nothing one not the ending one Jo pipeline open a steady statement Matra be smooth valley stage maharaja a whose a stageful what is average CPA that is one one clock per instruction time lotta Ayane and Jo inter state register st. for non pipeline we use nahi butit pipelining may use Orton to handle the data dependency that is for operating forwarding only clear it know how much time in non pipeline that is some of these stages that is 5 plus 6 plus 11 plus 8 nanoseconds and what is the time in pipeline system pipeline meant match of match of 5 6 11 8 + 1 nanosecond is your overhead no 1 5 plus 6 eleven eleven twenty to thirty upon eleven plus 112 it is 2.5 is your speed of factor for this question these are the option B is correct but so much my option B is the answer for this question a bit chiefs imagina if pure system main since a instruction hota for one instruction agar maybach guru delay key to each question may for one instruction what is the delay in non pipeline 5 + 6 + 11 + 8 Chiaki sorry stages a construction a TOEFL or oregano non pipeline 5 + 6 + 11 + 8 but in pipeline a courageous turcica delay be include karna padega Montejo toe in pipeline what is the delay 5 + 1 + 6 + 1 + 11 + 1 + 8 + 1 got a point therefore previous year gateways and heads for one instruction non pipeline is better than pipeline codifying because of inter stage register dealing buffer dealing got a point now pipelining of 5.2 milta hai jab number of instructions gem number of instructions bohot raha hoon a lot a - every CPI conical cap f-14 Tomcat you skirt a maximum stage dealing got the point plus overhead if any clearance to that is all about one level of pipeline equation Nexus K bottom or detail may spare engage then after completion of the concepts from the previous year gateway ssin jitna be possible Hong gave the timecode yamir active away theorem assess all currently got the point is there any doubt you 
DDmJtHFCNos,22,,2017-06-04T15:17:01Z,Computer Architecture and Organization,https://i.ytimg.com/vi/DDmJtHFCNos/hqdefault.jpg,Computer Architecture and Organization,PT6M47S,false,98686,594,8,0,22,[Music] we welcome you all to the course on computer architecture and organisation this course will be taken by me along with Professor interrelation Gupta so this course basically will give you an idea of the course computer architecture and organization so in this course we will be talking about the about the evolution of computers how computers have evolved over the years and where we stand today how the improvements have taken place over the years you will see into that how a computer actually works inside when we say that computer is performing so many tasks how exactly it is performing that task we will be looking into all those details in this particular course we will also look into the various design alternatives we will be evaluating various design alternative and specifically we will be taking the case study of a contemporary RISC architecture that is MIPS so which will be really helpful for many of the engineering science students specifically the students from computer science and engineering Electrical Engineering electronics engineering and as well as from some of the industry professionals so in this course we shall be covering almost all the aspects of computer architecture and organization that is covered in a standard curriculum the courses which are available in the various universities and institutions for instance we shall be starting with the instruction set architectures of computer systems we shall be looking at the data path and the control unit designs we shall be looking at the design of memory systems design of arithmetic and logic circuits both for integer unit and the floating-point units then we shall be moving towards the input output or the i/o systems and there are so many standard buses so many stand new standards are coming up we shall also be talking talking and discussing on them and towards the end of the course we shall be looking at some of the advanced topics like we shall be looking at pipelining parallel processing which at one point in time were considered to be very advanced topics but today they have become quite part and parcel of the very standard processors we use in our daily lives now in this course one thing is definitely true that learning computer architecture and organization is important not only to the hardware engineers who are into design but in today's scenario even a software programmer a person who is writing computer programs a person who is writing compilers for example they can be a better programmer if they know how computers work internally ok so learning this subject is very important not only for the students and as dr. death has mentioned also for the practicing engineers who possibly are into the software industry they are into programming so this course will also be useful for them so overall what we shall be trying consciously is to start from the very basics without assuming too much background on part of the audience or the participants but we shall be trying to take them into a journey where we shall be moving towards the more advanced topics and advanced aspects on computing and wherever possible we shall be trying to give illustrative examples trying to talk about case studies so that they can actually relate or they can actually alleviate whatever they're learning and they can also visualize the impact of whatever they are learning and one thing I would like to also emphasize the way we shall be handling this course when such will be following some kind of a quantitative design philosophy that we shall be trying to justify something why we are doing this what is the purpose and fertility justification of going this that is a very modern way of looking into design for looking into some kind of you can say implementing a system so as to meet some design goals or performance guidelines so I think you can finish by saying couple of words again a structure shandricka already discussed about one of the various aspects of this course what we will be including here I hope this course will be useful for all those students who are into this engineering science and starting from really very basic we will be going into slowly slowly the advance things which will really make you understand where we stand today where our computer architecture has gone into so I guess for this course will be really useful and come and join this course and I believe you can also tell them about the main text books that would be following other than these standard textbooks which most of the college curriculums follow like the books by Hammacher Vanessa Kanzaki the books by Stallings John Hayes we shall be also dwelling into the more advanced text books like the ones by Hennessey and Patterson so the list of all the textbooks and other study materials will be uploaded on our course website we would be providing you with some assignments on a weekly basis and we are sure that this would be a very rewarding experience not only for you but with your feedback it will also help us in improving the quality of discourse for future ends thank you so much thank you 
zvSWFqbhaVc,27,"This video is a lecture for Chapter 5 Part 3 Assembly Language for DCN1013/BCN1043 Computer Architecture and Organization. You can gain access to more information in our google classroom account.


For Universiti Malaysia Pahang Students Enrolled in this class (BCN1043/DCN1013). Please complete the google form in the link below for confirmation of your attendance:
https://forms.gle/guTZFLhXwfGmVQhXA

 You can download the google classroom app at: 
https://play.google.com/store/apps/de...
or
Open it directly in your browser at: 
https://edu.google.com/products/class...",2020-12-22T10:01:54Z,DCN1013/BCN1043-Chap.5-Lab1 Computer Arch.& Org.: Computer Memory,https://i.ytimg.com/vi/zvSWFqbhaVc/hqdefault.jpg,Syafiq F.K. Academia & Solutions,PT22M20S,false,552,25,0,0,0,very good morning again i am dr sherry fawzi camrosa present to you for another chapter for computer architecture organization so previously we have been looking into chapter 5 under chapter memory so today i will discuss about the lab section under this chapter so let's take a look at the lab chapter so before i start okay i would like to thank everybody for subscribing and giving the likes on my video so hopefully this will be this will continue to be beneficial for you guys in the in the future okay so okay let's go and take a look so supposedly okay this is a lab chapter for chapter five and so let's start to take a look at the laptops so now okay so in our first question is to define the memory technology that we have uh listed in here so based on this so let's take a look at what memory technology and what description that we have so first one is the register so what exactly a register so you can define register as a okay a small memory that is located inside small memory that is located inside a cpu okay it is fast however less in capacity okay then we have the example is cpu like cpu general purpose register so this one will have already discussed in chapter four okay in assembly languages next okay we have the random access memory serve as main memory inside the computer so this one is ready and only memory so in this one the answer is hard drive and then this one is another version of only memory better say the portable version of it only memory so example you see the rom magnetic tab is a old version of read only memory so example is concepted okay so here you have all the description and example provided for you so let's take a look at the next question so memory hierarchy and memory technology so let's take a look which memory technology located in this in which location inside the hierarchy so basically the top one supposedly is the register next is the cache memory because this is also located inside the cpu but not as fast as register however faster than the ram next is the ram okay and then the hard drives and finally the external drive okay so this is the answer for question number two and then we go to question number three so they asked us to find out the coded word for this code that is data so in this case it doesn't mention whether it's a big indian or little india so just assume it's a big endian so a big ndn usually we have okay let's write some using a different color oops so i write in a different color code red okay so put a b okay in the first position c and also in the 8th position d so this was supposed to be red as well and this rig as well so the parity bit is a a b c d so the parity bit is the abcd and the query data is basically the answer for this question so how to find the abcd so let's take a look at the code data so for a okay so let's change the color first okay a is where you have skip one uh take one and skip one so then just okay is the blue color oops okay so this one is the blue color one okay so here you go the total for a is okay so a is total for is one plus zero plus zero plus one one is three so this one supposed to be and odd so it's a one so a here the answer will be 1 so next we go for b so since we already got a which is 8 1 so b now okay let's take a look at b so we change the color for b so b now we have okay instead of using this is better use highlight it's easier b is 2 2 skip two okay so you see that inside this green section the total of one and zero is three also three which is odd okay this one is one and then we go towards c so we have for c so this one is one and b is one so let's change color back to black so we will not confuse later on okay so so now we want to have c so how do you get c c is take four skip four so take four and skip so here the total is one which is an odd it's also one and finally c is also one and finally we have d so for highlighter we don't need highlight so d here in this case we don't need this anymore okay we can change color back to black then d is take it skip it so this one change color to ray so the total here is this 2 so here the answer is 0 even so this one is even so the answer is 0. so here we have c is 1 and the last one is 0. so the answer for this is 1 1 and this one is 1 and zero okay so finish your third task so next we're going to do is for hamming code and the hamming code again okay you have a coded data and you want to correct them into the correct answer so let's take a look at the parity bits first so which one is the parity bit so basically this kind for this time is just we are going to change the are we going to do it backwards basically okay we're going to do it backwards okay so you have this a equal 1 b equal 1 c equal 1 and d and d equal zero okay so what is the actual data so now okay we think that the actual data is one one zero zero zero zero one zero okay so this might be our actual data okay we remove this this might be might be our actual data however we are not sure whether it's the correct data or not okay so what we have to do now is for us to calculate again okay whether the a is a is correct abcd is correct or wrong so let's take a look at the first a so a is big one skip one okay so this one you have to exclude okay so a so what exactly is a so let's take a look so a is total is okay 1 plus 1 plus 0 plus 0 plus 1 so total is 3 which is odd meaning that answer is one so you say that a and the calculator a and the current a is okay so we can put icon okay here so you can put it right here so a is correct then next we go towards b so b here so we remove all the highlight first so this one is b so b here we use a different color use yellow okay so two and two take two skip two okay so when you do take two skip two here in b okay let's check again are you doing this correctly or not okay okay this one is b so b is one plus four zero zero zero one so this is supposed to be two so even and it's zero and now this one is wrong okay cross okay so we have this one for the second one then we check for the third one so this one is c okay stake for skip four skip four now just just in case i do d as well okay so this one eight so what is the best color here gray so here is c so when cc the answer is one even odd one and d is is one odd so if you look at here okay you can see that b and d is wrong you see b is supposed to be 1 here and d is supposed to be zero however parity bit usually they are right okay usually they are right so the problem might be left in the data space here so somewhere within the black uh the black area part which is the data bits there are damages so now how do you find this kind of damage okay you look back at the data okay start with the bottom so d d is wrong so the data range that the one is wrong is within this range okay so and then b is wrong so since d is in this range so b also might be in this range okay so the data should be either this one or this one okay however if you look at a okay one of the data is in a and a is correct so this part okay this part is correct so means that the overlapping between b and d only occurs at this data so originally this one is wrong so this data here is okay this data here is wrong so supposedly the third data from the right okay is wrong so this one is wrong so let's change it into the correct data so the correct data is the current data is one one zero zero zero 1 1 0 okay so this one is wrong so now you got the correct data okay so finish with the fourth question now the last question so this one example of using big endian and also a little engine so let's take a look back okay how does big endian and little endian start okay so big endian start from the left side okay so the left side start at zero and but little and then the right side start at zero so basically you just put in okay so here three a goes here and then four eight then three b and ac okay so for little ndm same thing here 3a 48 3b or ac if we look at so for big n we start with address zero zero however little india start with address zero three okay so because of uh the word address start from the left start from the right we can then the word address start from the left okay so for the for the block address here okay it remains the same however the word address is different the arrangement is different okay so that's all for our lab session for today so hopefully you can finish this task okay by the time we finish the deadline so i see you guys later next time during our class so i'm dr chef aussie camaro zaman thank you very much for supporting me i'll see you guys later on bye bye 
alwPE8RMoWM,27,"This video is a lecture for Chapter 4 Part 1 Assembly Language for DCN1013/BCN1043 Computer Architecture and Organization. You can gain access to more information in our google classroom account.


For Universiti Malaysia Pahang Students Enrolled in this class (BCN1043/DCN1013). Please complete the google form in the link below for confirmation of your attendance:
https://forms.gle/VDSJdpiHeb5266AH6

 You can download the google classroom app at: 
https://play.google.com/store/apps/de...
or
Open it directly in your browser at: 
https://edu.google.com/products/class...",2020-11-26T08:26:22Z,DCN1013/BCN1043-Chap.4-Part1 Computer Arch.& Org.: Assembly Language,https://i.ytimg.com/vi/alwPE8RMoWM/hqdefault.jpg,Syafiq F.K. Academia & Solutions,PT16M34S,false,585,26,0,0,1,good morning so we meet here again for another chapter for computer architecture and organization i myself love to share it with my presenting from the university of michigan so today okay we are going to look at into chapter 4 which is assembly language so in assembly language okay we we usually heard okay instead of examination you usually heard about programming language so in case you are familiar with programming language we have like c c plus plus c sharp java python okay recently we have more like r okay so those are the kind of high level language what we call here high level language so what exactly an assembly language so assembly language is uh actually uh what's that uh lower than the high level language before the machine language so previously okay uh in when the first they first developed the computer so usually the coding of a machine okay the programming of a machine usually be conducted through a sheet of a sheet of paper with holes on it okay where they have to punch holes so that they can do some kind of programming for the pc so later on because of the development of a lot more binaries inside a computer for processing and for storing so those kind of methods become very very tedious and very very difficult and time consuming even costly for them to do so at one point they decide to simplify the commands being entered into a computer so they simplify those commands of those binary in form of a readable language which is what we call as assembly language so the concept of assembly language and the original machine language is not much different where the original machine language they have relation with what kind of instruction what kind of address that you going to operate on what kind of data you're going to operate on all those kind of things okay instead of in binary they simplify in assembly language and for some of the machine for some of the high level languages okay they tend to even simplify the semi language even more so that it becomes a more readable language like readable equation for you when you you you are doing your programming so and even some high-level language they bypass the assembly language they go directly to the machine language okay so no matter which kind of assembly language that you have uh which kind of programming language that you use okay so a second language is basically the the base of those high level language uh programming methods okay so this one okay some language they represent okay binary in simple way okay so what kind of binary simple way okay example of machine language okay so here's what a program fragment looks like so this one is in binary okay this means z equal x plus y so it's difficult for you to understand based on the binary itself so basically okay assembly and machine language so machine language they are native to processor executed directly by the hardware okay and the instruction consists of binary code one and zero however assembly language a programming language that use symbolic names to represent operation registers and memory locations okay so this is slightly higher than slightly higher level languages but not up until higher level languages so readability of this instruction is better than you read for machine language okay so one to one correspond once towards correspondence with machine language instruction so it's exactly like a direct translation of the machine language into a readable language okay so in terms of you to convert the assembly language to a machine code you need what we call assembler okay assembler so assembler is work like it works like a compiler for high-level language okay so high-level language they translate the high-level programs to machine code okay so either directly to machine code or sometimes they translate it first into an sm language through an assembler and later on into a machine code so for compiler and assemblers so usually if you are familiar with programming language you might heard about this compiler okay for example c compiler okay and a lot of things gcc compiler so for high level languages sometimes the compiler will take the languages directly into a machine language and sometimes the compiler will convert it into assembly language first and from assembly language they will convert it into the assembler so for example this on the this figure okay this is looks the program that i show here okay this one represent a high-level language program in c okay so if you're familiar with c this is a program from c so through a compiler c program will compile the c code into a an assembly language program okay so based on this assembly language program an assembler will convert it into a binary program okay binary machine language program so this one okay this is what with the computer will run with okay so basically assembler is a software tools that are needed for editing assembling linking and debugging the sme language program okay they convert the source code program written in smb language into object files in machine language okay so popular assembler have a merge over the years for the entire family of processor that include like for example we have turbo assembler and we have network assembler and we have new assembler distributed by the free software foundation in case you are familiar with open source program you use you can look for an open source program online where you have a gnu assembler available for you to download into your linux mix machine so for sme language okay the first things that you need to remember is that it has what we call a mnemonics okay mnemonics so in assembly language mnemonics are used to specify an op code they represent a complete operational machine language instruction so what does it mean by opcode so opcode is basically mathematical process like for example addition subtraction moving store okay moving something mathematical or loading process okay so multiplication division so those kind of instruction okay in assembly language we simplify it we represent it by what we call mnemonics okay in the case of inside the slide we have mov so the mnemonics mov is used in assembly language for copy and moving the data between register and the memory location so this is one example so other example we have to add the value of one register to another register sub sub sub so this one for you to subtract value from one register to another register so the mnemonics is what represents instruction so each command of a program okay they are called instruction basically so it is because it instruct the computer what to do so computers only deal with the binary data has the instruction must be in the binary form 0 and 1. so the set of all this instruction makes up the computer machine language so all of this set of instruction this is what we call instruction set so basically what they what they call a program is a set of instruction okay a set of instructions for the pc to conduct so inside the instruction fields okay they have two parts involved okay two major parts and there are a lot more parts but basically two parts that you need to know for now so the first one is the op code okay which represent operation what kind of operation you are telling the computer to do and then the second one is what we call operands operand is what kind of items inside your pc that you want the operation to conduct at okay so like for example you want to add value in register a to value in register b and op code is represent the op code of that instruction is addition and the operands for that that instruction is the memory location of register a and the memory location of register b okay so this specified the operas basically specify where to get the information the items that it needs to operate the instruction so basically it's a source or destination of register so this is example okay so you have n r one r three three okay so this one okay and r one r three three so means that r three you will add with three and the answer will be stored in r1 okay so the op code here is f what to do with the data the operands here where to get the data and put the result so the data will get from register three and this one is just uh a value okay that you inserted from your keyboard okay to be added to register three okay and then the answer will be stored in register one so the register three items will not be changed so there are a lot of types of opcodes and it differs according to the processing maker okay so the processor maker they make their own they make their own system they make they have their own architecture they have their own organization and sometimes they even provide the basic operating operating system for the components that they provide so in this case they need their own assembler okay for assembling the the program for their components so in this case the programming language also need to be compliance to the assembly cell so some of them are common and some of them might be different different according to the maker itself so some of the common one is like for example at some mult okay sometimes for different maker they use mpy for multiplication this one mult example okay and or this one is a binary instruction for example you combine one and one you got one if it's n okay so compare cmp so memory load and store you have ld or st or you have load okay or you have a store okay control transfer gmp jmp means jump okay so you have b and e i'm not familiar with okay and then you have like for example complex instruction like movs okay this one also i'm not familiar with so we have uh operands like for example register okay and then you have register and then you have immediate instruction okay like for example you put in from your keyboard and then you have indirect like for example you have to take the value from another location okay and then you have offset you have uh offset means that you have to change the value in the certain location first and you have a pc relative uh control operands so each operands okay they have their own particular addressing mode okay for any instruction that you are going to use so they reflect the processor data pathways so basically if the professor want to do the instruction so it will create a pathway from the processor to the operand itself take the data from the operas and process it inside the cpu register itself so example of assembly and assembly address instruction okay assembly language instruction and machine code okay this one is one of the example table from one of the makers online so you have like for example move ax one move ax2 okay this one is example of uh register okay register register this one is direct instruction so you have the from the assembly installation converted into the machine code okay and then this one is the instruction address where you can get the instruction from so when you translate a language okay same thing okay a high-level language translate to assembly language so for example in english you have these assigned the sum of a times b plus 10 in this case so higher level language they will conduct in programming for example d is equal a times b plus tangent in equation form and when you compile it it goes into assembly language like this move e a x a okay and then multiply with b okay so this is a register inside this register put a and then multiply it with b and then add okay 10 to the e a x so in the e a x we have a multiplied with b and then you add 10 to the e x and then from that move d e x transfer the value from eax into d okay so this is an assembly language instruction and from this we convert it into a machine language like this so usually okay you can familiar with assembly language okay we are using it based on the ascii itself so they ascii is basically okay as we know is a scheme to use for assigning number of values to punctuation marks spaces numbers and other characters so ascii they use seven bits to represent characters the value from zero zero zero zero zero zero zero to one one one one one or zero zero to seven f okay which are used by giving ascii the ability to represent 128 different characters we can extend the version of ascii a science character from 80 through ff so we have an estimation as well so this one is what is we are going to use to represent the assembly language here okay ascii is what we use to represent the assembly language when you convert them into a machine language okay so that's all for our chapter for today so we'll continue again later on for the second part of the chapter so thank you for subscription and liking my videos i hope i can see you again next time in my other videos so stay safe bye bye salaam alaikum 
6-2qBShDHCo,27,"Instruction Codes i.e
memory reference,register reference and I/O reference
immediate, direct and indirect address",2017-12-03T14:29:03Z,instruction codes in computer architecture | COA,https://i.ytimg.com/vi/6-2qBShDHCo/hqdefault.jpg,Education 4u,PT10M12S,false,83505,985,67,0,23,hi students coming to our next topic that is instruction code so what do you mean by this instruction code instruction code is nothing but it is a group of bits a group of bits you call it as instruction code means they're in the form of ones and zeros like that so instruction code is a group of bits that tells you the process to perform some specific task it it says to the computer it says to the computer to perform some specific task so whatever the task and say it just saying to the computer to do some work so here every computer has its own particular instruction code format so whatever the computer you are taking they are having their own instruction format the simple way to organize the instruction code by using two with two parts the instruction code can be organized into two parts that is one of code and mix one is the address so the complete instruction code will be divided into two parts one is opcode and another is address so actually from where we are getting this instruction code in the memory the instruction will be present so from the memory we are getting the instruction code and that we are going to be converted in to group of bits and that group of bits will be read by the processor so actually this memory will be the size of 4 0 9 6 into 16 so this is the size of the memory for the 16-bit instructions so this memory will be divided into two parts so one holds the instruction program instruction program and another will holds the operand that is the data so memory is having instruction program and then is operon data so here actually this instruction code is of size 16 B because it is having 4 0 9 16 to 16 memory location so this instruction code format will be divided that is a 16-bit address from 0 to 15 so in this 0 to 15 4 0 9 6 means this is a to power 12 and 16 means to power 4 means here the 12 bits specifies the this trolly bit specifies the address part and this 4 bits specifies the opcode so the instruction code will be divided into two parts one is address part and another is opcode so here the 4 bits will specifies the opcode 0 to 4 sorry 15 to 12 and the remaining 12 bits 0 to 11 well specifies the address part so this is the instruction format this is the instruction format ok so actually this instruction is of three types it's a memory reference instruction register reference instruction or I will reference instruction so this instruction which we are getting from memory or we can get from a register or it is or I boo so whatever the instruction that will be either of is of three types memory reference instruction or it is a register reference instruction or it is a IVA reference instruction so based on this instruction the processor will understand what type of instruction we are getting so this instruction is coming from memory reference or it is from register reference or it is from either inference okay so actually this instruction code can be of three types means it can be written the format of like immediate address direct address indirect tetras so this instruction holds opcode and address so this opcode and address will be of immediate or direct or indirect so based on the instruction the processor can understand the instruction which it gets is whether there it is immediate data or it is a direct data or it is an indirect detail Indian means it specifies actual operand so actual operand will be present in the immediate address means opcode of good and direct operand so this is 0 to 11 12 to 15 so this is the immediate address or code and operand that operand will be present so then the browser understand this is an immediate address so why we are just getting the operand and it calculates the operation now coming to the direct address so what does direct address specifies director to specifies the address so for brand address of oprand so we call it as an effective address effective address so direct address holds the address of the operand here in this place zero to 11 address or if address of the operand is present then we call it as a direct address means just suppose this is a memory location so which I said here this is an instruction program instruction will be present so this instruction may be suppose add so was four three zero this is some memory location so in the particular four three zero you are having the actual operand actual data will be present so this is the memory location so in that memory location the instruction will be like this zero at 4:30 4:30 here indicates the address of our brand so this is the address of operant so where this address in this 4:30 location you are having the operand so here the zero indicates it is a direct interest so this is Derek so in that particular address of our brand in this 430 is address in that particular address location you are having the operand this is the effective address you call this as a direct addressing now coming to indirect addressing indirect addressing specifies the address of memory word so it specifies the address of memory word in which address of operand is found so first it holds the address of memory word in that particular address of memory word address of operand is present again you have to go to that particular open so it's looked like this so this is the memory location so but if it is an indirect addressing you have to keep it as 15th bit is zi1 at 4:30 20 so in that particular 4:30 location you are having the address of the operand so again you have to go to that particular address there you will find the actual op prime so this is the indirect addressing so this is the instruction in this instruction indirect address means address of memory word is a 430 in that address of memory word in which there is address of a prank will be found so here the address of operand is 1 2 5 0 so in the particular 1 2 5 0 location you will find the apply so this is the indirect addressing so these are the different formats of the instruction code thank you 
PDG_2Mll-_c,22,,2018-09-25T07:24:13Z,"Multiprocessor Computer Architecture: Why, How and What's next ?",https://i.ytimg.com/vi/PDG_2Mll-_c/hqdefault.jpg,NPTEL IIT Guwahati,PT19M54S,false,4365,48,5,0,1,[Music] you hello everybody I am homonka Kapoor from the department of computer science iit guwahati and first of all i would like to congratulate all of you for having taken such a wonderful course on advances in computer architecture this being a very exciting field and I'm sure you enjoyed the course as we are going towards the end of this course I thought I would come and give you a guest lecture on summarizing the contents and then seeing what lies beyond this course and was this course useful how it fits in the complete context and so on therefore the title of my talk is multiprocessor computer architecture and we are going to see answers to three questions why how and what's next so let's first answer the question by why would we want to study a course like this and what does multiprocessor architecture we are all aware of Moore's Law as you can see in the slides I've put a picture of Gordon who who is the co-founder of Intel in 1965 he predicted that the transistor density would double every 18 months and what did we do with these extra transistors we could add more processing logic we added floating-point units we added more memory elements we added caches and so on and so forth and this added to the processing power of the micro processors making them more dense and powerful as you can see in the graph on the Left the trend of the micro processors increased linearly with the frequency and with the number of transistors which got added so we got double the number of transistors every 1.5 years following the Moore's law next what did we do with these transistors again I would like to draw your attention to the graph here this graph shows the green part shows the number of transistors increasing every two years so as we got these transistors we utilize them to improve the processing power of the processors by increasing the frequency and also exploiting the inst and level parallelism increased frequency and increased craft rate increased the power consumption of the ices however we were able to manage this until early 2000s but beyond that beyond the three guards processor if we went beyond that the power consumption was so huge that the chip temperatures went beyond control the next slide shows the chip temperature profile as we scan the clock so currently till early 2000 the chips temperature reached as hot as a hot plate but if we had continued to increase the frequency beyond the curve frequencies at nollie 2000 the chip temperatures would go to a nuclear reactor or a Sun surface and so on so this is very impractical because the chip would burn itself instead of functioning so what was the solution the clock scaling was not working so what do we do coming back to this the chip density was increasing the clock speed was not allowed to increase and what would I do with such a high clock speed the processor has frequency but I don't have enough work to give instruction level parallelism which was exploited till early 2000 was coming to a standstill because there was not much parallelism available in the applications right so the solutions people thought who was that instead of having a very powerful processor can I have multiple processors which can do really different applications instead of digging out more parallelism within the single application hence a revolution started the processor designers were forced to go multi-core because of the heat density faster clock meat meant hotter chips so people or designers thought that we should have more cores with lower clock rates that would burn lesser power they were declining benefits of the instruction level parallelism because it was not found in the applications and we were wasting the logic power in only digging out the ILP which was never found in the applications beyond a point the other reason for going multi-core was the yield yield is defined as the number of good ICS you manufacture among the total manufacturing capability of your unit when an IC is manufactured it passes through a test process so if the test is passed then this IC goes into a commercial utilization otherwise it reaches the dustbin so if I have a single core processor the chances of throwing away the ICS would be very huge but if I have a multi processor chip and I have manufactured a chip with eight cores and suppose to cause do not function I can still sell a six core chip at a cheaper price so the yield problems could be sorted out by going multi-core hence now we are going to reinterpret Moore's law instead of having double the number of transistors every 1.5 years I would have double the number of course every two years this all these cores would run at a slower clock frequency which would consume less power however now the designers would have to look at concurrent execution parallel execution among the system and dig out parallelism among the chips as well as interactive parallelism had to be exploited when parallelism comes into picture you will have multiple applications running and these applications would need data so we would also need to provide larger caches to commensurate with these data demands right so the slide shows you the new Moore's Law where some practical application ICS are shown see if you can see in after early 2000 we started getting multi-core chips from power for ultrasparc 4 and all the way to tile 64 which are 64 chips and teraflops which has over 100 chips right so this is the new Bors law which is giving us more pair processing capability in terms of more cores and so on this slide shows some commercial examples from AMD arm Intel so I've listed a few however this gives you an idea that the industry is going multi-code so we've seen our answer the question why multi-core architectures came into picture and the how do we now the next question is how do I address the design challenges of this and am i capable of doing it so the next question is how and I think we have answered or rather you have an answer to this how because you've done the course in advanced architecture look at the slide here I've just put a big picture of what a multi-core architecture looks like you have multiple cores you have one or more caches the main memory and all of these are connected through a cloud of interconnect infrastructure and I think you are all familiar with all these terms by now we will quickly do a recap of each of these the processors or the cores are mainly pipelined because pipeline processors give you a best throughput and once you go pipeline it doesn't come for free you have lots of design challenges and I think you've gone through several of those during the part of this course then these pipeline processors as well as multiple processors need caches you need multi levels of caches you need larger cache sizes larger the cache on multiple levels would need optimization techniques so that the caches are don't become a bottleneck for the processing and so on when you have multiple caches and multiple cores you would need to address cache coherence issues so these also terms you have gone through during your course so multi-level caches larger caches optimizations and cache coherence methods are the challenges related to caches for multi cores and these were very nicely covered during the advanced architecture course then comes the main memory you have to design efficient main memory systems for example DRAM has been used then what is the structure of D Ram what are the different controllers and address mapping one is to understand this then we need to understand the arrangement of the cores the caches the interconnection interest the course required multiple levels of cash then these more cashes required bigger size memories so we need to understand the main memory system how the they are controlled what are the application or other edges making mechanisms and so on then multiple processors one needs to understand their layout how they are connected that interconnection topologies the infrastructure of the connections the protocols and the layouts so these are covered during the NOC part of your course so essentially these are the points which we covered during the advanced architecture course you have studied pipeline processors out of order execution scene the memory hierarchy the cache memories the RAM memory architectural topologies and the layout as well as interconnection infrastructure so these are familiar to you and these are the foundation of designing multi-core architectures so having seen all this now what lies beyond right what's next so the last question to answer is what next after this course what do I do what do I learn what was the way ahead in terms of study research and well as the industry going again as this slide shows that we started with a multi-core setup where we had a few large course however now the industry is going towards some large course followed by a mixture of smaller course so we will have a heterogeneous system we could have systems with several small cores and many small cores we could have systems with some force some accelerators some purely designed floating-point course and all of these would need memory so memory can come from the 2d layout or a 3d layout and then we would have chips which do specific tasks related to home management games graphics business and scientific applications so the question is not whether this is going to happen the question is whether we are ready for it and am i interested to build up my capable is to go for such systems so in the what next scenario I have listed down here some topics which would be of interest of course this list is long however you can choose where you want to specialize depending on your interest so we have listed several things here and we will do one of one by one all of them so the one topic which you can take up is parallel computer architecture here we would study arrangement and communication among the processors how is the interconnect infrastructure what are the cache coherence protocols because well there are there are multiple cores each code would want to talk with another core and that communication happens through shared memory so this shared memory protocols would depend on the architecture and management of these multi-level caches how is the connection infrastructure and so on so one more point is the memory consistency models because of the the first topic we will cover is parallel computer architecture where we are going to speak about having multiple cores the arrangement of the course how these cores would communicate with each other generally the communication happens using message passing infrastructures or or shared memory in suppose we are designing architectures they go they communicate over shared memory which is through the caches so these caches have to talk with each other so they need a protocol of speaking and they need a communication infrastructure to send and receive messages so what was the infrastructure what is the protocol that needs to be understood when we deal with parallel computer architecture another challenging issue is the memory consistency models which say that because of parallel access is done by several processors there could be inconsistency in the data that is a write by a processor could be overridden by a ride by another processor so how do we order or serialized these read write accesses comes as part of the memory consistency model which also needs to be understood the next topic could be are of interest as high performance or general-purpose GPU superscalar designs so this is mainly a parallel computing paradigm where you need to understand parallel computing paradigms parallel algorithms parallel data structures and so on if you are interested in the interconnect aspect one could do advanced courses in network on chip NOC comes in two variety of flavors as I've listed here it is wired interconnect which we have studied as part of the current course one could have optical interconnects photonic interconnects wireless interconnects interconnects is in transmission lines etcetera so once you've we have this interconnects it being a backbone of communication it has to always run so one is to understand the fault tolerant aspects if the network fails what are the fault tolerant routing mechanisms can I'll configure the network so that the connectivity is maintained then scalability is another issue we have multiple cores can I have a network which scales the number of processors if scalability in a particular technology is not possible can I have clustering or hierarchical interconnections possible can I use different technologies wired photonic a mixture of these to connect heterogeneous interconnect so that I can connect 100 mm cores and so on so these are the advances related to interconnect technologies then looking at memory systems advances in DRAM we can have 3d stacked theorem the reliability of the rams is important how are the different compression techniques implemented so these are the areas you can study related to memory systems the next aspect is emerging technologies cash shells and DRAM sodium is made up of DRAM that caches are made up of SRAM so Aslam is a very power consuming technology so can I replace this power-hungry aslam with another technology so that my power consumption and temperature of the chip is under control so people are looking at non-volatile memories as a replacement of DRAM so these nvm so-called in VMs are coming into the memory hierarchy some examples of in VMs are spin torque transfer SD I'm registered and phase change random access memories and so on so why are people interested in non-volatile memories because of their low leakage higher density and they are very power efficient but of course these good things don't come for free they are coming with challenges like they wear out very quickly the rights are slow and we one needs to do retention aware usage of these MBMS so replacing aslam with nvm s one would need to address these challenges before moving ahead as we were talking about power when we replaced SRAM with nvm we need to also be alert about the power consumption and power aware computing so power in a system consists of dynamic and Static so dynamic power is consumed when you do any access of computation static power gets consumed as long as your devices on with the scaling of Technology the static power component is significantly increasing increase static power increases heat dissipation the temperature and increase temperature in turn increases static power so there is a circular dependency between the two so we need to control static power or rather to control the whole power as such people are looking at techniques called dynamic voltage and frequency scaling at the core level as well as the uncor level that is how can we have voltage frequency Islands be among the caches the memory components and the interconnect infrastructure what also needs to think about the thermal aspects of the chip that was called the thermal design power or the TDP so every chip comes with a maximum temperature or maximum allowable temperature within which the chip has to function so if we are about to cross this limit one needs to implement techniques to have a temperature aware functioning of the chip one solutions for this could be to turn off some components run the components at a lower of frequency and then the other attractive option is to use technologies consume less leakage power so that is what was next available to us if you're still interested to look beyond so we are still in the computer science area or the rather computer engineering aspect looking beyond is an interface to micro electronics or physics these emerging technologies if you're keen one can look at the designing of these devices how these emerging devices can be incorporated into the memory which does inbuilt logic computation and designing even for the low-power devices so understanding micro electronics and physics can give you insights into these methods the other aspect is more computing aspect that is the interface with software for example how does operating system-level data mapping happen application mapping on the course when you have a multi-core set up you need to map the applications such that the calls go through less communication costs among themselves then scheduling algorithms so that again these could be depending on your quality of service they could be real-time they could be power aware and so on and so forth so looking beyond you have scope beyond computer science into physics and micro electronics and into the software so to conclude there are lots of scope in the subject it's all the subcategories it's relationship below and above the system hierarchy so let's learn and have fun so that eventually you will design a manufacture towards lending a helping hand to make in India so life in architecture is very exciting best wishes for your future thank you [Music] 
kl0I2Ocy16o,22,,2015-03-02T07:15:50Z,01 intro to comp arch,https://i.ytimg.com/vi/kl0I2Ocy16o/hqdefault.jpg,Kishore Kumar,PT12M6S,false,127,2,0,0,0,"hello and welcome to this course on computer architectures my name is Baskaran Rahman and I am with the Department of Computer Science at iit bombay in this introductory video we will first look at what computer architecture is and why the study of this subject is important so let us start with what is computer architecture what does the term architecture mean to you the term architecture usually refers to a way of designing or making things typically it refers to buildings so what is the term computer architecture mean then computer architecture refers to the way of building computers the way the hardware and software are put together the way they interact with one another and the internal structure of the microprocessor before we can learn various aspects of design of computer systems there are some prerequisites you need to know some basic soft data structures such as arrays pointers stacks and queues and algorithms involving these data structures you also need to have done course on logic design which covers basics of switching Theory boolean algebra representation of numbers in computers and arithmetic involving these representations building logic circuits using gates combinatorial or combinational logic karnaugh maps for optimizing these combinational logic more machines and milling machines finite state machines constructing them in Hardware using them to make control units and the design of computer hardware for doing computer arithmetic these are the contents of this course we will learn about computer organization the basic parts of a computer the one moment architecture we will then look at the design of the instruction set which defines a computer we will learn about ways of measuring performance so that we can answer design questions in a quantitative fashion the microprocessor which implements the instruction set has to be implemented in hardware and this involves the design of data path and control part we will learn about these pipelining is a mechanism to make the implementation of the processor fast we will look at pipelining methods and the problems we run into in terms of hazards when we try to implement pipelining and we will also learn ways of getting around these hazards the next important part of a computer is the memory system we will look at the memory hierarchy the design of cash systems and how the various design choices affect the performance of the cash we will then look at secondary storage or disk storage and raid systems which use redundancy to achieve fault tolerance we will look at Hamming codes as a way of error correction codes in stored data then we will look at input output systems and buses it's useful to know how the contents of this course relates to the contents of other courses here in this diagram I have shown a hierarchy at the lowest level of the hierarchy we have digital gates and circuits using these gates these are used to implement the various components of a computer such as the central processing unit or the microprocessor its memory system and i/o systems the instruction set of the processor is what is implemented by the components of the computer system using the instruction set the operating system runs and also the application programs via the operating system use the instruction set to run their applications if you look at the hierarchy to the left here the application programs and operating systems are usually written in higher-level language such as C or C++ or Java this higher-level language gets converted into assembly language which essentially corresponds to the instruction set of the computer the assembly language gets converted to machine language which is nothing but a binary version of the assembly language and the Machine language is in turn implemented using the digital logic the lowest level in the hierarchy is typically covered in a separate course which may go by the title of digital logic design there is typically a separate course to deal with operating system design and application programs are usually dealt with in algorithms related courses there is typically a separate course which teaches techniques for converting the higher-level language into assembly language that would be in a compilers course this course computer architecture deals with these two parts of this hierarchy the computer system design and the instruction set design we deal with assembly language and machine language in this hierarchy until here is the hardware and above this is the software and this course will deal with the interface between the hardware and the software these are the textbook references for this course I will mostly be following the computer organization and design book by Patterson and Hennessy I will closely follow the third edition although it is also okay if you have the fifth edition with you note that the fourth edition uses the ARM processor for various examples the third and fifth editions use the MIPS processor which is what i will be using so try to use either the 3rd edition or the 5th edition for this particular course in additional reference is this book by John Hayes computer architecture and organization note that both these books there are low price editions available as well as ebooks please buy them if you need and don't use pirated copies i will also be using some nodes from other computer architecture courses so having looked at what computer architecture is and what we are going to learn in this course before actually learning that let's first look at why we need to learn these aspects about computer architecture so take a pause and think why this subject is important or maybe you think it's unimportant let us identify the various aspects of computer architecture around you if you are watching this video you are already using computer architecture quite heavily you probably watching this on your personal device like laptop or tablet there are many more components involved as contents of this video are streaming towards you the server which is hosting the content and the network routers and switches in between all of these are computing devices in one way shape or form you would have likely used a whole range of personal devices may be non smart phones smart phones tablets laptops is actually a quite a continuum of devices from mobile phones to desktops representing a range of personal computing devices and then there is a whole set of computing and data storage platforms which you actually use but don't interact with directly these are the servers and data centers or cloud computing platforms which host various internet services popular ones such as Google or Facebook or flipkart Amazon Yahoo and so on these form the backbone of these major Internet services a large set of high-power computing devices computing servers put together in a system called a supercomputer is often used in very specialized but very important applications weather prediction being one of them or research applications such as the Human Genome Project mapping the genes in the human DNA or other things such as simulation studies of subatomic particles in physics or research related to climate change involving huge amount of computations of geological models supercomputers are used for such research applications and are often points of pride involving national and international reputation and then we have computer architecture playing a role in what are known as embedded computers these are very large in number and they are very small that we don't often notice them but they play very critical roles microprocessors are used extensively in home electronic products and in vehicles and also very extensively in industry automation so that is with respect to computing devices in various contexts what about the number of personal computing devices this graph here shows the number of personal computing devices of various kinds which enter the market per year this is based on a study done by Gartner in april 2013 so what you can see is that per year there are over two billion devices which enter the market getting close to three billion in 2017 is a prediction the other thing you can see in this graph is that traditional personal computers in terms of desktops and notebooks are actually reducing in number while its place is taken up by tablets the range of things for which we are using computing has been growing in large part due to the growth in the raw processing power in the textbook there is this graph which shows the processing power in the y-axis and in the x axis is time in years and what it shows the details are in the textbook what it shows is that there has been an exponential growth so the y-axis is logarithmic so a linear representation in this graph shows exponential growth in processing power with time it shows that between about 1986 till about 2,000 to the growth has been about fifty percent per year this roughly translates to doubling of processing power every two years and after 2002 it has sort of the rate of growth has slowed down but still it is exponential it has come down to about twenty percent per year so in this course we will learn some of the architectural techniques which has driven this kind of growth in performance over this duration and also reasons for why this is sort of slowed down in the recent past the exponential growth in processing power has been driven by exponential growth in another metric and this phenomenon goes by the name of Moore's Law so you can look up graph for Moore's law on Wikipedia and it will show again a y-axis which is logarithmic and here it shows the number of transistors per chip and again x axis over time and you will see a linear graph which represents an exponential growth in the number of transistors or the number of gates per chip which has possible this exponential growth has also been roughly of the order of doubling every two years so the more transistors we've been able to pack we've been able to grow the processing power roughly corresponding to that so hopefully now the answer to the question why study computer architecture is self-evident computing is central to today's information age computer systems range all the way from very small in devices surrounding us which we use but we don't realize to personal computing devices which we actually use an experience and then at the other end we have these high-end servers and supercomputers which also perform important functionalities new computing devices have been coming up new end-user devices have been coming up in this course we will understand how these various computing platforms are designed what affects the performance how we can go about designing these computer systems to optimize performance along various optimization matrix "
2MvbKCSla-g,27,"These videos are helpful for the following Examinations - GATE Computer Science, GATE Electronics and Communication, NTA UGC NET Computer Science & Applications, ISRO, DRDO, NIELIT, Placement Preparation, etc. 
If you want to enroll for our courses please visit our website https://www.Digiimento.com or call 9821876104
You can also add me as a friend on https://www.facebook.com/HimanshuKaushikOfficial

#Call_9821876104 #GATE #NTAUGCNET",2017-09-29T13:12:33Z,Computer Organization and Architecture for ECE ESE/IES COA 2018 electronics GATE ECE lectures,https://i.ytimg.com/vi/2MvbKCSla-g/hqdefault.jpg,"DigiiMento: GATE, NTA NET & Other CSE Exam Prep",PT7M8S,false,4018,36,6,0,9,"hello everyone so in this video we are discussing about the announcement of the package that is for electronics and communication students for electronics and communication es si that is engineering service examination or you can say Indian engineering services this is the package which is common with subject which is common and this is a computer science subject it is a mixture of three subjects which is computer architecture organization operating system database management system and some basic from the programming fundamentals or programming concepts now the problem with this subject is it is not a electronics and communication core subject so that is why the students were from electronics in communication they tend to leave the subject in their AAC preparation and let me tell you one thing while you are preparing for engineering services our mission or gate it is important to prepare all the subjects thoroughly because if you leave that any single subject and there must be some other students who is studying that subject thoroughly see all the topics of the exemptions I have seen people that they say that don't do all the subjects just focus on if there are 12 subjects focus on eight to nine subject to do them thoroughly you can leave three or four subject but that is not true because there's so much competition in the market there are people who are preparing from last three to four years or five years just for annealing for its examination gate examination so you are competing with those students who are preparing from a long time and they have covered each survey subject so thoroughly very good okay so I want to say that if you want to prepare or esea then do not leave any subject even if it is forms of the sense background so here we are starting a package there is only for electronics and communication students okay we are not from ACC background okay but we have seen the slavers of this subject I've taught this subject for EAC students in the coaching institutions now I know that even I am from commit science background I've never given the examination you have never given the EAC or IES examination but still I have taught this subject and students got very good marks in this subject for EAC examination okay even though it is a very small part in that case okay but still if you want to prepare for this subject we are starting a package for this subject in this research papers actually this leavers is divided into three and parts here you can see the first part is basic architecture which is CPU I organization memory organization peripheral devices now this complete part is a part of the computer architecture and organization okay now here if you see operating system processes characteristic applications this again there is a memory management here this is memory management here there's a memory organization we have a memory file system and security this specific part is a part of operating system there is OS okay now there are some part which is there in computer science that is process synchronization and process scheduling that particular part is not here in this operating system subject now we have the third part is databases different types characteristics design transaction conferencing until this particular part is one subject okay so this part from here to here this is the DBMS part in computer science apart of this we have index management we have structured query language SQL and we have topics related to the normalization so for electronics and communications for me you are just discussing about databases different types of databases their characteristics and design transaction concurrency control a last part is the elements of programming languages okay now the students who are from electronics and communication background for you it is a mixture of all the subject it is like you know it sometimes you get very confused in this subject now we have created out videos in a very thorough manner thorough in the sense that we have already created these kind of videos for computer science background we have taken some some of those videos from the company science part and we have embedded those videos for this particular package and a part of this you are getting the previous or partial solutions from this particular parts from AAC from gate gate computer science and gate electronics and gate compress I said DAC that products and communication and delayed examination barque examination and the idea examinations we have taken there taken questions from all these examinations and we have embedded them in their package so if you are preparing for this subject for electronics and communication engineering services then you can take this package okay this will be very helpful for you this fourth participant some programming languages now how to watch the demo lectures see we have already uploaded demo lectures on YouTube so there are a lot of students after watching these many video lectures if you see our YouTube channel we have uploaded more than 900 videos which is double or triple than any other person who have uploaded media on YouTube right we have considered more than 3000 video still now and after those 3,000 videos we have uploaded 900 videos on YouTube so which is lot more than anyone else who has uploaded the videos for AAC indicated on YouTube okay next is you can watch the demo lectures on YouTube a part of this what is the extra that you are going to get we are currently recording the previous question solutions for AAC electronics and communication ESC background so you are going to get this previously partial solutions and this is these solutions videos will be available by the end of October month we have already started recording them but all these videos the more than 200 videos that are already available so we have already created two hundred plus videos for this particular package the fees of these packages are paise 3000 which i think is a very very nominal fees considering that it is a combination of four subjects okay and currently we have more than 200 videos for this particular package and by the end of October or November we are going to add 100 more videos in this package so it will be around 300 plus videos that will be there and duration of every video is six to eight minutes only how to restore this packet just email us at admin at the Red Gate lectures dot-com and for the subject just write that AAC electronics and communication ESC complete octet organization and you can whatsapp me online eight two one eight seven six one zero two so my team is there and my team is going to reply for this whatsapp messages and this email and you can pay by a net banking ATM debit card credit card everything is available okay so if you want to register for this package just let us know and they are going to there we are will be there to help you out with it okay and the biggest question is how to resolve your doubts or queries so if you find any doubts in CODIS you can just email your doubt app doubts at the Red Gate lectures comm so all the registered student they have a proper doubt solving mechanism we can resolve your doubt by doing a Skype call or we can resolve your doubts by doing phone calls email solutions etc so top it out solving mechanism is there okay I hope this is helpful for you all the best for your preparation "
blGiqunHfn4,27,,2012-03-22T09:17:09Z,"Computer Organization & Architecture,Module 1, Part 1 by Dr  Ahmad Azzazi from ASU",https://i.ytimg.com/vi/blGiqunHfn4/hqdefault.jpg,TEST,PT17M34S,false,4657,9,4,0,3,miller of mana rahim my name is dr. Hamid Ozzy Ozzy at the Applied Science University and the Faculty of affirmation technology an hour ASU urban core system I will introduce the lecture computer organization and architecture this is the part 108 the introduction computer organization and architecture is a major course of the computer science courses and you have to have the knowledge of the course digital logic in this course we will provide the student the knowledge necessary to understand organization of the computer and the operation of the hardware components of a digital computer system and description of the relevant software aspects earn and an introduction to the parallel processing concepts we have used the text box textbook computer organization and architecture designing for performance the 8th edn from william stallings oh and this is the reference of the sport you can use this reference to get more information about the content of this book ok let begin with architecture and organization we have two things we have festival architecture and then organization or otherwise we could have organization than architecture so architecture is those attributes visible to programmer instruction set like the instruction set number of bits used for data representation I own mechanism addressing techniques if we want to put any hardware we have to decide about all of these attributes question is there a multiply instruction this is an architectural question because it is about the attribute to multiply something with other thing there were we could answer this question from an architectural point of view with a yes or no if we decide to have a multiply instruction and we have to decide how to organize it or how to implemented therefore organization is how features are implemented like control signals we want to implement them interfaces how to implement interfaces connections which memory technology would like to use for the question asked in the previous paragraph if we decide to have a multiply instruction as an architectural decision we could answer or we could organize it in two ways this is an organizational decision should we use Hardware multiply on it or should we use it or should we implement it as a repeated addition this question is answered in two ways therefore the organization is definitely by each answer therefore Y to the side for each is due to some reasons like the cost the speed women with speed performance trolling the frequency of usage of this unit therefore that designer should decide about which organization is better to implement the multiply instruction all Intel XXL the same basic architecture this is a general thing that they use the same instruction set they'd use the same properties or visible properties the IBM system 370 family share other basic architecture than the x86 family therefore they have their own visible property than the x86 family and they have also different different instruction set therefore this gives quad compatibility at least backwards in the same family not in different families okay the organization and each family the x86 family or the 370 family differs between different version we have the machines and the x86 family they are differently organized and the machines and the IBM system 370 are also differently organized we could say they have different implementation of the memory different implementation of the buses and other things okay we have also two different walls the word structure and the word function structure is the way in which components relate to each other but function is the operation of individual components as part of the structure function could be as follow link computer functions are data processing data storage data movement and control of everything these are four functions of all computers in general therefore we have a functional view of the computer this is the computer within its environment this is our computer this is input and these are the outputs processed through the computer system computer system this is the operating environment operating rating environment the computer is a source and destination of data the data the data movement through and apparatus to the control unit or control mechanism where it could be sent to our storage or received via a storage facility or a data processing facility these are the four basic functions of any computer we could look deeper into each of these functionalities by looking at each movement of the data the data movement itself the data movement is done between the source and the nation's or from the operating environment to the control unit therefore the there is a movement from outside to the control unit to the outside again the storage operation through the movement functionality over the control functionality we could store the data and and the other direction we could throw the control unit move the data to the operating environment the other scenario is the processing from to the storage from to the storage from the storage the data could be moved over the control functionality to be processed then it could be after the processing moved to the storage over the control and the other direction finally we could do processing from storage to i/o from the storage over the control we process data then the process data is moved through the control to the outside environment a structure top-level view of the computer and its organization we have the component under relation between each component this is the computer system we have here some different communication lines to communicate with this computer system and other brief irials to get input and to write out with to them if we look T bar as a top-level view we could find a processing a central processing unit the CBU we have a main memory we have input output units and all of them are connected with each other through systems interconnection we could then have a t-bar look as a top level view of the CPU itself the CPU contains the following components we have registers mainly for our temporary Sturridge way they are very very high speed memory units with high associated with a higher cost but they are very effective because they are within the CPU we have done the arithmetic and logic unit we have them the control in it to control everything done and the CPU and the arithmetic and logic unit to perform the arithmetic and logical operations needed for data processing and we have done also internal CPU connections or interconnections to connect all of these components we could then have within the CPU itself a G bar look on each component they'll use the internal pass and registers will see that and the next lectures the control unit itself could have the following in your unit or component sequencing logic to get the sequence of operations or instructions or data a control memory the control unit itself needs some memory to do some buffering or some temporary storage and we have then control unit registers and decoders the control sister's the control unit registers are used to store also memory of monitor content temporary and decoder stick to decode their instructions them selves this is our top a first look of the computer organization under picture the outline of this lecture we will have then computer evaluation and performance as next there are computer interconnection structures internal memory structure external memory structure input/output operating system support computer automatic instruction sets CBU structure and function reduced instruction set in the computer superscalar processors control unit operation microprogrammed control multiprocessor and vector processing we have then if we need we need a a pre look to the digital logic and digital logic design we have them to look within the web for the following we could use the following subjects to search for them computer architecture homepage on the www CPU and for center and other things like manufacture of sides of different CPUs like I p.m. and entered I strongly want from you to search for all of these resources on the net you could use usenet newsgroups also the following are related to our course ok thank you for the first time we will see you in the next part of this rest on your home goodbye you 
UmYBlFFvRj0,17,Operating System : Processor Structure and Functions,2016-11-13T02:30:46Z,Processor Structure and Functions,https://i.ytimg.com/vi/UmYBlFFvRj0/hqdefault.jpg,venkatesan ramachandran,PT4M16S,false,10535,87,10,0,N/A,hi friends so in this video we are going to start the topic processor structure and the functions so in here we are going to discuss about what is CPU structure and what is all about the components inside the CPU okay so if the programmer or the user point of view the CPU need to do the following functions example if you are taking a program may have the number of instructions so the processor will execute instruction by instruction so during the execution so the processor need to fetch the instruction then it needs interpret the particular instruction fetch the instruction need to be interpreted so after that fits the data then the process they need to be done by the processor so after that the result need to be written in the exact location so here it needs number of registers for these functions provided by the processor and operating system so first one that is a fetch instruction so a fetch instruction means it need to be from the secondary memory the instruction or the whole program need to be copied into the main memory softer that from the main memory the instruction need to be transferred to the processor so where the instruction will be stored in the processor means it will be stored in the instruction register so like this number of registers may be required for the whole processing of a particular instruction or the execution so this is how the CPU will execute the instruction step by step so for this step by step inch execution so these are the components required for the particular CPU one is a ALU arithmetic and logical unit so after that registers are required software control unit is required so alias arithmetic and logical unit whatever operation is required for the bodily instruction that can be provided by the arithmetic and logical unit so then data transfer from the memory to processor then processor to memory so that can be provided through registers so then control unit so when the operation need to done so when the opera you need to carry down example of the data when the data need to be transferred to the register from the completing the process when the data need to be transferred to the transferred back to the memory so all these things will be carried out by the community so control unit will provide the basic functionality that is it will control the basic functionality of the processor or the CPU okay so then with what are the other things may be required for the process that is a system bus so system bus records the the request for a data transfer or the instruction transfer or control signal transfer between the ALU or the CPU and the memory so the system bus contains three type of three different buses will be there one is the control bus will be there and second one is the data bus will be there and third one is the address bus so address bus will provide the where the address where the instruction need to be transferred from from the memory location so memory location address will be provided by through address bus then then neither bus may be used to transfer the data between CPU and the memory then the control bus may be required for transfer the control signal between the CPU and the memory so this is how the process structure is there so this is the processor organization okay so processor organization means so what are the components inside the processor that is a CPU so CPU organization so what are the components there inside the CPU then what are the other components required to support the CPU operations so this is how your processor structure will be there then functions also it will be provided like this for the user point of view thank you 
kS2yiC0DYRM,22,online class videos lecture series on the subject Computer Architecture and organization,2020-06-08T07:49:35Z,Lecture- Computer architecture and organization : introduction,https://i.ytimg.com/vi/kS2yiC0DYRM/hqdefault.jpg,Anwar Ali Sathio PowerShell Academy,PT4M1S,false,43,6,0,0,1,hello everybody welcome to their online classes and computing science crotchy I am your host today we discuss about the topics which go through from the introduction a little bit and after that we start from the left in the class previously UST so today we are going to learn about computer architecture and so let me start from the course offline then after that we refer a book which we discuss in the class in detail and groups are formed as prius piracy we have already formed the groups according to the topics and the presents in the class it is already and the Google glass material posted on the Google glass and you have to view that material at the class so we have to share today the contents of these are the contents systems and components process design instruction set design and addressing boots control structure and micro program memory management caches and memory hierarchies and interrupts and structures pipelining of process processor issues and Harder's exception handling realism multiprocessor systems these are the key topics we had to cover half we have already covered in the class today we start from the introduction so that we review the topics after that we catch during many topics now then the joining of little disturbance of network so see their books this is the book we referred in the class obviously also thank you for joining thank you for joining this is what we have this book I will have to discuss in the class topics lot of topics have been covered 
lllBFn3n4Bc,27,"Computer Architecture
About this course: In this course, you will learn to design the computer architecture of complex modern microprocessors.

Subscribe at: https://www.coursera.org
https://www.coursera.org/learn/comparch",2017-12-05T14:24:45Z,Computer Architecture - Machine Models,https://i.ytimg.com/vi/lllBFn3n4Bc/hqdefault.jpg,intrigano,PT16M4S,false,463,7,1,0,0,so now we're going to start talking about the fundamental things inside of an instruction set architecture and the things you need to build inside of a instruction set architecture and we're gonna start off by talking about machine models it's not a question here where do operands come from and where do operands and where the results go to so when I say operands these are the data values that your to operate on with a single instruction on a processor and the results are where they and what the data that gets calculated and where does it go and this is actually an instruction set architecture or Big Eight architecture concern fundamentally you're going to have some form of arithmetic logic unit or some sort of calculation unit and you're gonna have some type of memory storage and in a processor you're probably gonna want to take stuff from the storage move it to the ALU compute on it and then put it back into storage somehow and this is the processor here that we wrap around the ALU and the machine model is what is the implementation what is the semantics or something not the impression what is the organization of the registers how do you go to access memory what sort of instructions and operations are allowed and we're going to talk a little bit about where do the operands come from and where the results go and different machine models that people have built so this is different instruction set architectures and and even more than instruction set architectures because the circuit set architectures talk about how do you encode instructions even this is even a little bit more fundamental of how do you go about reasoning about how to move data from memory to the ALU and back and how do you go and store the data close to the ALU so let's start off with a very simple example here a very simplistic processor that people have actually built believe it or not you don't have to have registers in your processor or you don't have to have named registers in your instruction set architecture instead you can have a stack a stack is just a storage element where you put things onto the stack and then they come off in the order that was the last one that was put on it was taken that gets taken off first instead in a very basic sense we just take the top two things on the stack we fetch both of them operate on them and put them on what would be the top of the stack building building up from there we can think about something like a accumulator architecture so typically an accumulator architecture if you have one accumulator so there's only one register one of the operands for every operation you do is always implicit it has to come from the accumulator the other one let's say can come from from memory so you have to name one of the operands in architecture like that building up from there we can start thinking about maybe register to memory operations where you'll name let's say the upper operand that is the source let's say coming from memory and you'll name let's say an operand coming from your register file and optionally you may even name the destination so these are all these are all options and we'll call this a register memory architecture finally we have something like a register register architecture so lead your guests here how many named operands do we have here three okay yeah and so this picture here we're gonna take something from a register something from another register and we have to name where to go put it so that's that's a I wanted to point out here actually the number of explicitly named operands for some of these is a little bit more questionable than others so as you can see here we have zero we have a little more complicated something where we have one two or three now how do why do I say two or three here so there some of these architectures our architectural models don't necessarily name the resultant or the result location some of them will implicitly name the result so something like x86 for instance the first source operand is always the destination so it only has to name two things something like MIPS or most RISC architectures will actually name all three and you can think about that happening with Oh both memory and a register register architecture so a good example of a three operand memory memory which I don't have drawn here this is just I just have register memory drawn here but a memory memory which basically says all of your operands come from memory and all your destinations are on memory is something like the VAX architecture which was popular in the 70s you could actually have all of your operands come from memory do some operation on them and store it back into memory so one interesting trade-off here is as you start to have more operands that are explicitly named you need more encoding space for that this is one of the important trade-offs ok so let's let's go into a little bit of depth about a stack based architecture so some examples of this are actually the Burroughs 5000 the symbolics machines were stacks based and these are sort of machines from the past there's a few modern or more modern examples of this for instance the java virtual machine is actually a stack architecture and then also intel's old floating-point system x87 they've since sort of deprecated this but it's still relatively modern is a stack based instruction that architecture let's take a stack-based instruction set architecture and look at how you can go about evaluating an expression so here we have an expression A plus B times C all in parenthesis that whole quantity divided by a plus D times C minus e so it's some complex math an instruction or math operation that we need to do here we actually break down this into a parse tree of this so you can see this is preserving orders of operations well we have a plus this sub quantity here of B times C and you take all of this and divide it by this sub expression here and a little bit of a throw back here this is a way that we're going to take the operations and if we do these operations on a stack machine model we're going to get the right result so what this means is put a on the stack put be on the stack put C in the stack and then multiply B times C and then add the results times a and you can see that we're going to build this expression on on a stack so sort of a different way to sort of rethink about problems and we'll walk through an example here so here we have an evaluation stack and the top of the stack is going to be whatever's on the top here in this picture so first thing we you're going to do is we're going to say push a so a shows up on the stack then we're going to push B we're gonna push C and then we're do a multiply so we're going to take the two entries there on the top of the stack multiply them and put them into this entry in the stack and then we're going to say add and it's gonna add the two top things on the stack here and put the result here now you can see that if you run something like this you can actually do full computations on a very small stack and what's also nice here is you don't have to explicitly name any of the operands so this machine model allows you to run real programs and one of these is going to get across here is that the stack is part of the processor state and it's usually so that's the Big Gay instruction or Big Eight architecture or the instruction set architecture you have a stack and many times it's unbounded in the big a architecture but in real life it needs to be bounded somehow because you can't have infinitely long physical stacks in your machine so it's conceptually unbounded but you probably want to have it overflow to main memory if you put too many things on the stack and this is this is an important characteristic because all the time otherwise there's certain computations you can't do if for instance the parse tree is too long here or the the depth of the tree is too long your stack might microbes long okay so let's say we have a microarchitecture implementation of a instruction set architecture which is a stack based architecture and the top two elements of the stack are kept in registers and the rest is in main memory and it spills and fills well each push has a memory reference so when you put something on the stack it has a memory reference and each pop has a memory reference you just put something back into main memory but more so than that you might have extra memory references here as the stack spills over in the main memory and you have to pull something back in from a memory so that's not not very good because you might end up with a push having more than one memory reference so one optimization from a microarchitecture perspective is you can think about having n elements of your stack in registers very close to the processor and only have to go access main memory when the register stack overflow under flows so instead of having to do a memory reference basically every single time you do an operation or every second time we do a push or pop you do it only when the stack depth gets too large that you can't fit everything on your stack so let's let's walk through a brief example here we have the same expression calculation that we were doing before and to see we can do push push push multiply add push push push multiply add push ease subtract and do all the divides at the end well if we have a stack size of two what's gonna happen here is at the beginning rolling in a you know we do a push we're going to a load from a push we do a load from B now we do a push of load of C but our staff already has two things on it so which one we try to push the third thing we have to overflow the bottom of the stack somewhere so we're actually gonna do a stack save of a out to main memory here we do this multiply and we actually do a stack fetch of a and get it back from main memory so it's a lot of extra memory operations so you might want to think of a different microarchitecture and if you sort of walk through this whole example here we're gonna see that we have four stores and four fetches from main memory which are all implicit plus the explicit ones that we're trying to do if the pushes in the pops mmm that's eight extra memory operations should we think about how to build a micro architecture which has less but has the exact same instruction set architecture well let's say we have a machine which has a stack size of four well if the stack size of four at the worst case here we only ever have to fit four things on our stack so we never have to spill out to main memory our stack pushes and pops still have two access memory but that's what they're trying to do they're actually trying to access memory so to sum up here about stack based machine models they look they look pretty cool but not not all things are are great at all times one of the interesting things here to see is we actually push a and we push a again so we're doing redundant work and we push C and we push C again C and a had the same value both times we push them so all of a sudden we're doing extra work and maybe we could have done less work if we had an architecture or a machine model a big a architecture which allows you to store multiple things and name different operands so what we want to get across here is while a stack based architecture is very simple the instructions are very dense the it may not be good for performance because you might end up having to reload values multiple times versus if you had a instruction set architecture or something like MIPS where we actually have 32 general-purpose registers you can name any register for any instruction you could have loaded a b c d and e all into the register space once and then done all your operations and never have had to reload and this is actually an instruction setting issue and not our fundamental machine model issue and not a microarchitecture issue okay so to summarize different machine models we have our stack accumulator register and memory and register register or sometimes called load store architecture if you want to take some expression here like c equals a plus b we can look at the instructions that would have to be generated on an abstract architecture so here we're gonna have push a push b ad pop c as we add more naming we actually can potentially have fewer registers or fewer instruction excuse me so we load a add b note here we don't have to name what we're adding with because we're adding with the accumulator and then store c start to get a little bit more compact if we have register memory we can load one of the values ad store it's pretty simple we the load store architecture we actually do a little bit more work it's a load load add store but which which seems inefficient but if you have to reuse any of these values a B but you don't have to go reload them versus and these two architectures you have to go reload the value you 
BKUkIGc9JsE,27,"different types of instructions used in COA i.e data transfer, data manipulation and program control instruction",2018-02-26T15:17:15Z,instruction types in computer architecture,https://i.ytimg.com/vi/BKUkIGc9JsE/hqdefault.jpg,Education 4u,PT11M41S,false,63009,567,33,0,22,students welcome back coming to the next topic in the subject computer organization and architecture is types of instructions so let us see what are the different types of instructions that are used in a computer so most computer instructions can be classified into mainly three categories let me write this point most computer instructions can be classified into three categories so mainly it is classified into three categories so one is data transfer instruction data transfer instruction second one is data manipulation instruction data manipulation instruction whereas third one is programmed control instruction program control instruction so any computer most computer instruction or all classified into these three categories so it requires some data transfer instruction to transfer the data from one device to another device and data manipulation instructions to perform the operations by the ALU whereas a program control instruction is going to be control the system so with the help of these three categories most computer system can perform the operations so let us see what is this data transfer instruction okay so a data transfer instruction is nothing but it is an instruction cause of data from data transfer instruction causes causes of data from one location to another means it is a cause of transferring data from one location to another without changing the binary information contents so that you have to be noted the data transfer is used only to transfer the instruction it doesn't change any content that is present in the data okay so it costs of data from one location to another without changing without changing the binary information content binary information content so whatever the data that is present inside the registers or whatever it may be so the data transfer only transfers the information it doesn't change any binary information content now coming to data manipulation so what is this data manipulation so data manipulation means here the data manipulation instructions or those or those that perform automatic logic shift operations so manipulation it's saying something is going to be changed in the content of the data so how it is going to be changed with the help of some performing some automatic cooperations logical operations or shift operations so these you call it as data manipulation instruction now let us see what assist program the control instructions programmed control instructions program control means we are controlling the program so this provides this instruction provide decision that is a decision-making capability it is having decision-making capability and change thee and change the path taken by the program when executed in the computer then executed in the computer so that's about this program control instructions though whatever the instructions that are used for programming control those instructions provide decision-making capabilities and also it has the capability of changing the path taken by the program when executed in computer now let us see different datatype transfer instructions data manipulation instructions and programs front roll instructions so first let me write thee what are the instructions that represents the data transfer that performs the data transfer operations data transfer instructions so the name mnemonic so load is one instruction so the mnemonic for load is LD means well you are writing the assembly language program you just represent LD in the program LD suppose LD X comma B X okay just moving them we are loading the data from B X - X so LD represents load in the same way stored this is one data transfer instruction the mnemonic used is s T in the program move MOV exchange x CH input I n output o UT push push mnemonic pop poke Nnamani so these are the different data transfer instructions names and their mnemonics so with the help of these mnemonics we are just mnemonic the instruction there's the whatever the assembly language instruction consisting of mnemonic and op codes so mnemonics as the names whatever we have written here and the outputs are nothing but the registers or data registers or variables or direct data okay so this is about the data transfer instructions now let us see what is the data manipulation instructions what are the data data manipulation instructions so data manipulation instructions I said these are three types of data manipulation instructions one is automatic automatic instructions and next is logical and bit manipulation instruction and the next one is shift instructions shift instruction so these three you call it as a data manipulation instructions now let us see the first one what is this automatic instructions so automatic instructions are name and the mnemonic that is used for the automatic instructions increment I&C decrement DZ add a DD subtract su B so these are the automatic instructions multiplying mu L divided div so in the same way advic carries subtract with borrow negates negates two's complement and Ageing so these are all the different automatic instructions now let us see what are the logical and bit manipulation instructions named mnemonic so the bit manipulation instructions are clear CLR compliment for complement we use c om c ym is nothing but a complement and a and d it is a bit manipulation instruction or o our logical exclusive or means X whoa are clear carrying clear carries the LRC so enable interrupts disable interrupts so these are all the different logical and bit manipulation instruction now let us see the shift instructions so shift instructions name mnemonic so what is the name of the shift instruction that is used in the data manipulation and what is a mnemonic for that logical shift right and the mnemonic it is used is SH our logical shift left sh l add the matic shift right as hitch are II so whenever you're add a it is nothing but a dramatic shift read a dramatic shift left H R L rotate right rol Argo are rotate left Argo n rotate right with carry rotate left with carry so these are the different shift instructions so these are all the data manipulation instructions automatic instruction logical and bit manipulation instructions and shift instruction now see the third one the third type of instruction is programmed control instruction so it controls the program's how it controls the program with the help of these mnemonics branch it is controlling the program means it is jumping from one location to the another two minutes the skipping it just the branch conditional or unconditional jump it is to jump from one statement to another statement or from one function to another function skipper escapee and call CA ll return R et test T TS t so these are the different program control instructions so this is about the different types of instructions data transfer instruction data manipulation instruction and they program the control instructions thank you 
zvq9KfDda0Y,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2017 (https://safari.ethz.ch/architecture/fall2017)
Lecture 19: Multiprocessors, Consistency, Coherence
Lecturer: Professor Onur Mutlu (http://people.inf.ethz.ch/omutlu)
Date: November 29, 2017
Slides (ppt): https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=onur-comparch-fall2017-lecture19-multiprocessorscoherenceconsistency-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=onur-comparch-fall2017-lecture19-multiprocessorscoherenceconsistency-afterlecture.pdf",2017-11-30T12:28:15Z,"Computer Architecture - Lecture 19: Multiprocessors, Consistency, Coherence (ETH ZÃ¼rich, Fall 2017)",https://i.ytimg.com/vi/zvq9KfDda0Y/hqdefault.jpg,Onur Mutlu Lectures,PT2H33M46S,false,1394,14,0,0,0,"okay I guess we can start know people are very distributed right now it's very sparse hopefully you can you can still here are the lights okay you won't fall asleep with these okay that's good so we're gonna cover another really exciting topic I mean there's no topic that's not exciting in this class hopefully but this is something also very fundamental we get to it right now just because of the way things are ordered but we're going to talk about hopefully all of these today if we talk about all of these that'll give you a full picture of what a multiprocessor design is about today although we we really touched on a lot of aspects of multiprocessors so far this lecture kind of brings them together and formalizes some of the concepts and hopefully we'll fight we'll have fun with some problems also okay just a summary of what you learned last week we talked about memory latency tolerance and prefetching these won't be on the exam next week as you know but you should know them they're fun today we're going to talk about multi processor is memory consistency and cache coherence and tomorrow is supposed to a review session but at the end of lecture we can discuss what's going to be covered okay we're going to jump to this they're a bunch of readings that I recommend that you do and those seminal paper is a required reading it's only three pages I think everybody who is getting a computer science education should read that paper in this day and age so hopefully you'll have fun with it it's a very easy to eat and they're a bunch of recommended readings these are these two are relatively overview readings they're a bit old and outdated but they still apply I think okay and memory consistency this is lamport's seminal paper on sequential consistency that's also two pages only and again you'll have fun with it and cache coherence and this is the as I mentioned earlier this is the Illinois protocol which is the meze protocol paper that was proposed in 1984 and if you are really interested in learning more about cache coherence these are some old all chapters but they cover some of the basics well I think lecture notes are really the basics but if you want to learn more you can read these ok before I move on I will encourage you to attend two distinguished lectures that are coming up I'll talk about one of them today but these distinguished lectures or distinguished colloquia as they're called here there are very good opportunities to learn about state of the art in many different topics and two of them that are upcoming are very relevant to what we're covering in this course this is more at the software level Michael Scott who has done a lot of work on memory synchronization and multiprocessors will actually touch upon that very little today is going to come on December 4th it's at cap G 61 at the regular time for these distinguished lectures and he's going to talk about data structures for persistent memory does this sound interesting hopefully you guys are all done this is a really good opportunity I think because these are special invited speakers that come and that's give a talk so that you can learn about cutting edge research that they're doing at that point in time and this topic I'll very briefly derail the lecture to talk about what this topic is about because I think it's really important as you can see over here we've covered emerging memory technologies right non-volatile memory they they're bytes addressable memory technologies and they enable the opportunity to directly manipulate data persistent data inside memory through the programming language so you don't need to go through the file system if you remember that lecture where I talked about it you can actually get significant benefits by directly manipulating an array in memory in a persistent manner and that's good because that way you get rid of a lot of the software overheads to actually do the file open and underneath the memory underneath the memory system or the operating system actually copies the file to memory such that you can manipulate it and then copies it back to the disk you get rid of all of that overhead now the problem is it opens up some issues also for example special care must be taken to assure that the values in memory will be mutually consistent in the wake of the crash given that caches write values back in arbitrary order so what up what happens if you have many played at some data in a linked list for example and your system crashes and this data was persistent in memory so that's actually a real problem if you if you actually expose persistence directly to the programming language now you have to deal with this issue and he's going to talk about his solutions to this issue and I think this is really important to enable something like non-volatile memory directly accessible to the programmer and I'll give you a very brief overview of this view this is a topic actually my research group has been working on for a while as well I think Michael Scott is looking at it from the more software side and we've been looking at from the more hardware software cooperative site but I think in the end the solution will be both together but what is the problem basically you have a persistent memory that you directly store into let's say you store into a persistent array you update it or persistent linked lists but you get a system crash this could be a program crash this could be a hardware failure it could be any type of crash and now you have a problem your data may be corrupted because you've already updated some part of your data structure but other part may not be updated so your pointers may be inconsistent for example let's see how this might happen for example you're trying to add a node to a linked list you first link to the next and then you link to the previous but while you're doing this some something happens and you get inconsistent linked lists and your when you restart your system your program will crash again because your pointers are not connected well but you can imagine many many errors happening here right so this should not happen clearly and the programmer should not have to reason about it otherwise it will go crazy this lecture is going to be all about programming actually later when we talk about multi processors also so how do you actually ensure that you don't get into this inconsistent memory state basically this is a key question how do you ensure this is called crash consistency this is called the crash consistency problem it's very well-known actually in storage systems because a similar issue exists in storage systems but storage systems it's easier to handle because the latencies are very large right you're writing to an SSD you're writing to a hard disk and storage system needs to guarantee that whenever you crash for example when you when you take out your USB for example while you're writing data to it your USB needs to be in a consistent state right that's essentially a filesystem you don't want to see an inconsistent state and sometimes you do actually because your USB drivers terrible probably that happens but that's exactly the same problem here you take out your USB and you get an inconsistent file but it's much worse over here because your memory can be corrupt completely okay there are two extremes as usual you can do everything in software and everything in hardware when it's programmer transparent let the system handle it system give the illusion of consistency without the programmer dealing with it or the others let the programmer handle it and go crazy well the system designer does nothing right and clearly there are many alternatives in between and in between alternatives are the most interesting ones I think so what has other works done I believe Michael Scott will talk about some approaches that look like this so some solutions are basically if you're writing to persistent memory you write it in terms of transactions we haven't talked about transactions we may talk about it later on but the transaction it guarantees atomic execution like this basically it's an atomic piece of code which you update the linked list for example over here insert a new note and either all of this code is executed or none of it is executed and somebody guarantees that let's assume the system guarantees that software can guarantee it or Hardware can guarantee it now if this happens that's good but now the problem is the programmer needs to write the code such that they insert these transaction boundaries or atomic execution boundaries and this could potentially limit the adoption of non-volatile memory because now you have to rewrite the code with clear partition between wall tool and non-volatile data does that make sense so somebody guarantees that this code is either completely executed or not executed at all regardless of what happens to the system ragazzo have a system crash for example if the system crashes in the middle of it you're over here the system gives the illusion that or you haven't even started this atomic code block and you could do this in software you could do this in hardware there are many many proposals for it which we're not going to go into all must be covered transactional memory so the downside of this is somebody needs to rewrite the code now you can say you can always say the code whenever you start the program atomic begin and when you end the program atomic end so the entire program is transactional that works clearly but that may be terrible right that actually has a lot of issues now the program may need to be executing alone but it may also actually lead to starvation problems right what if you keep getting a system crash you may never progress so this is actually tough so it puts burden on the programmers and also it's like it's always a good good thing to think about what happens with legacy codes right how do you make legacy code work there's a lot of code written assuming that you're operating on Walsall structures volatile data and if system crashes while you're actually writing to a persistent piece of memory non-volatile memory how do you actually make sure this works there are a lot of open questions here so I'm not going to answer all of these but I'll give you an example of one of the works very very briefly that has come out of my group I think this is the other extreme if you will the programmer does nothing I don't think the solution is really that's either extreme the solution is somewhere in between but it's always good to explore the extremes basically provide software transparent crash consistency and the idea here is relatively simple actually if you think about it you periodically checkpoint the entire state that you have in the system and some systems actually do that for fault tolerance reasons you checkpoint the state and then you around for a while and then you take another checkpoint now if your system crashes in between you go back to the previous checkpoint that's the basic idea how do you do that then you have to read the paper for details because if you want to actually make this work it really needs to be efficient for it to be efficient you need to consider different granularities you cannot checkpoint for example at a page granularity that's a lot of state and you need to overlap checkpointing at execution you cannot say I execute for a while and then I spend time for checkpointing that's a big no-no because that checkpointing takes a long time and if you look at systems that try to provide fault tolerance oh if you think about a Fault in a system it's relatively similar you're running this huge simulation in your super computing cluster you get a fault and that fault actually affects all most of your computation if your check pointing at regular intervals you can go back to the previous checkpoint right and that previous checkpoint enables you to redo the execution and hopefully overcome the fault but if you don't do it carefully you can actually destroy the performance of regular execution also you need to do execution checkpointing but while you're doing checkpointing you should really start the next execution if you will yes yeah exactly that's the trick basically how do you ensure that while getting high performance meaning overlapping checkpointing and execution you ensure that things are consistent so you really need to do multiple buffering so the next execution will start from a separate version of memory if you will does that make sense yeah but you should read the papers for paper that I'm going to reference later on yes so you have the right idea basically that's that's one way of doing it basically you have separate states one is operating on the current checkpoint one if the other is operating on the new checkpoint but you don't necessarily want Hart partitioning your memory you want to create these new versions as you go along but essentially it's a monster having partition or new versions of you well it's it's a version type of memory in fact you don't need you should really have three versions not two versions if you want to operate on two copies because you want to have the copy that is really consistent that you should never touch and the copy you are currently operating on and the copy the next execution is really operating on so there's all our overheads and clearly how do you manage the overhead is important and of course you need to have here I'm in the system so you need to somehow adapt to the characteristics of these things as well well this was the advertisement for the talk so you can so that people can come to the talk but you can read if you're interested there's a there's interesting literature and the sir the open research topic at the moment this is one of the papers but the bigger issue is really how do you exploit persistence in the memory directly exposed to the programming language while not making the programmers go crazy so this is one example problem but there is also other examples like how do you ensure people actually start using this in a manner that doesn't really examine other-- example is if all of your memory is persistent a pointer that's corrupt can corrupt some random persistent data for some other application right how do you ensure that doesn't happen now today if you this is a lot harder to do with the file system because file system is has a lot of guarantees or it has a lot of layers if you will all of that code that's executed ensures that you actually have the permissions to do that but if your pointers corrupt you can actually corrupt any state for which you have the permissions for in the virtual memory so one of the things that is interesting is developing libraries to help the programmers and this is one approach that we've been looking at basically how do you actually develop a library that makes life easier for the programmer to figure out what objects in your program should be protected or should be made persistent because not everything should be made persistent if you actually move to a persistent memory based system some some data is temporary and they don't need to be written to a persistent part of memory rights how do you do that classification turns out to be not an easy problem actually okay any questions I just want to give you a brief overview of what the subject of the distinguished colloquium would be while touching on some related research topics yes well that's a that's a great question and that's also a research topic so how do you actually ensure that that's another problem actually if your memory is non-volatile how do you ensure that somebody doesn't observe the private data that you may have over there people have proposed for example encrypting the the non-volatile main memory a lot of disks are encrypted today for this for the very same reason that you're saying another solution could be keeping things volatile but that may have issues also because you want some persistence I see but what if for example let's say your memory control is always encrypting yeah it's like an atomic operation so that's one solution to that problem for example but yes if you're doing the encryption in bulk maybe when you're flushing stuff into memory or maybe over time then yeah you may have you may run into the problem that you're saying that's right yeah memory is encrypted and Intel SGX for example similar solutions can happen for non-volatile memory I think the downside with a lot of those solutions the encryption overhead both in terms of performance and energy okay that's all that's always a good advertisement right from a researchers perspective that may not be true from Intel's perspective or selling products up may be true but from a researchers perspective I mean it's very fundamental right whenever you write to memory you need to encrypt it and that's certainly at some operation over there if you can hide the performance penalty maybe that's possible but the energy penalty is still there so actually a lot of mobile devices are so constrained in terms of energy that they may not want to adopt some of those solutions you may actually want to look for some other solutions okay it also depends on your design point basically maybe it's it's a great solution if you have a lot of processing power and a server but maybe it's a tough solution if you're operating I don't know out in the field and you're a sensor collecting important data and you need to write it to your persistent memory but someone is out there easily attacking you right it's not as high a sci-fi scenario anymore I think this will exist this already exists actually ok now let's move on to multiprocessor so that's an extreme form of multi processing these sensors all over the place and they're coordinating but really going to be more fundamental and we're start with an old paper that doesn't talk about sensors or anything so this is a paper that we briefly discussed we've certainly discussed in digital circuits but we've discussed it also let me talk about the GPUs and this is Mike Flynn's classification of computing systems as to how how they operate basically there's sysd systems single instruction operates on a single data element there's sim D system single instruction operates on multiple data elements that's data parallelism M you covered array and vector processors there is a miss D type of processors multiple instructions operate on a single data elements and single data element gets transformed as the instructions operate on them it's not a perfect thing but the closest analogy for this is systolic erase which we've covered in digital circuits but we've not covered in this lecture and I recommend that you take a look at the digital circus lecture if you're interested in this because systolic ra is is for example Google's tensley processing unit is essentially a systolic race it's a very classic sister okay if you will okay but we're not going to talk about that today we're going to talk about this last piece over here multiple instructions operate on multiple elements you have multiple instruction streams and they're completely independent well they can be coordinating but they're operating independently on different data elements so a multiprocessor is an example of this or a multi-threaded processor like a fine grade multi-threaded processors and another example of this multiple instruction multiple data if you hear the term MD that's what it refers to multiple threads essentially okay but before we basically it's a parallel computer this is also called a parallel computer but actually a parallel computer is kind of a misnomer because all computers today exploit some part of parallelism if you think about pipelining it's really a form of parallelism right you're really exploiting very fine-grained instruction level parallelism it's the parallelism in the instruction cycle that you're exploiting and every instruction is going through a different part of the instruction cycle in parallel but usually when people refer to parallel computers it's usually sim D or Mindy and Mindy more general but let's talk about parallelism in general a little bit parallelism at its broadest form means doing multiple things at a time and anything can be parallel you guys are parallel actually you're doing multiple things at a time right things could be instructions operations and tasks from the viewpoint of a computer so the main goal main reason why this has been developed is really performance right improving performance you want them for execution time or test throughput depending on what kind of jobs you have if you have a single task you want improve the execution time of it or if you have a bunch of jobs let's say simulations that you're running that have nothing to do with each other except you want to finish all of them then the test throughput is your performance metric in that case and we'll see that the execution time is governed by an dolls law as they're going to be reading but there are many other goals maybe we should discuss some of them any thoughts why you would like to do things in parallel other than improving performance I know you guys know yes sure so that's true I would consider that improving performance tell I'm thinking about metrics like what else can you improve in terms of metrics yeah energy efficiency right that's certainly one you can reduce power consumption right energy consumption this way for example if you have parallel units if you have n units operating at frequency F as opposed to that you could have four n units if you can paralyze your program four by four X and operate each of them at frequency F divided by four or performance F divided four by four for each unit you will still get the same performance but the energy efficiency of this will be much better right wife remember the power equation power is equal to capacitance times voltage square times frequency you reduce the frequency by four but now if you reduce the frequency by four you could reduce your voltage maybe not by four but let's assume in a perfect scaling world by four you can actually reduce your power consumption by 4 cube right that's 64 X while keeping your performance constant and that's very powerful and that's a very big motivation for parallelism actually a multi-core was motivated partly because of this not fully but partly if you can have a huge single traded program one units operating at performance F why not have 4,000 units operating at performance F divided by 4000 now you get an energy efficiency of 4000 cube increase that's very compelling of course this assumes that you can perfectly paralyze your program right such that your performance will stay the same and we've see we've already seen that that's not easy to do and we're gonna see more and more today okay well I guess I've given you one more over here this is a freebie basically you can improve cost efficiency and scalability and reduce complexity also right maybe it's easier to design for and units that are operating at lower performance as opposed to one single unit that's operating at very high performance and that's this is really the reason for sorry moving to multi core multi it was it turned out it was very difficult to design a single threaded processor that's operating at extremely high performance for that single threat it was a lot easier to design a thousand cores that are operating each operating at 1,000 the performance one one thousandth of performance of that big core that's improves your reduces your complexity clearly compared designing that's very hard to design units improves your cost efficiency and scalability also but again it assumes that your thing needs to be parallel completely parallel right what about the third one or those people who've seen it they can stay sound there's a third one actually there's a third reason for parallel computation all right I'll give it to you because you may have seen this but basically it's really improving dependability and this was actually one of the old reasons for for this also if you really want to dependable execution you run the same program in ten different units and these good you're voting in the output and declare the output to be provided by most of the units that agree on the output the majority of the of all of the units that provide the execution so this is an modular redundancy if you will so if you have two units for example it's dual modular you dunces redundancy you execute the program on two processors and either they disagree or agree if they agree that's good you assume the outputs is good if they disagree you assume there is an error now if you have triple modular redundancy if at least two of them agree you assume that that's the output right and this is a very old reason or building parallel computers and the old days tandem computer was the first one actually who did this sort of execution in late 1970s early 1980s and still people do that does that make sense okay so let's talk about some types of perils amande how to exploit them we've talked about the instruction level parallelism quite a bit and as we've said different instructions within a stream can be executed in parallel pipelining is an example of this Auto word executions speculative execution very low in very long instruction weird engines and data flow are all examples of this rate data parallelism is a more regular form of parallelism different pieces of data can be operated on in parallel perhaps why the same instruction right sim D is an example of this systolic aries and streaming processors are examples of this also and there's also testable parallelism in this case different tasks or threads can be executed in parallel and this is really multi-threading and multi processing an existing GPUs combined both in an interesting way as we've discussed as aresult they called it single instruction multiple thread right sim tea engines so you see in parts of this before but how do you create these tasks because the task can be independent or dependent so you can partition a single problem into multiple related tasks let's call these threads but they don't have to be threads as we've discussed earlier they could in general their tasks so you could do this explicitly by parallel programming and this is relatively easy when tests are natural in the problem for example queries but this is difficult when natural task boundaries are unclear right in one example I've given before is you have this huge book and you're really trying to do a histogram of the characters that appear in the book right you can divide the book into I don't know 32 pieces and give 1 30 30 seconds of of the book to each of the 32 threats that you've created right and each thread counts on its own creates a local histogram and then you merge those histograms with multiple threads or a single thread to get the full thing so that's parallel programming essentially and you're creating the task boundaries somehow right okay or you could do this transparently implicitly maybe a single thread can be partitioned speculatively think about the book example you don't do it explicitly but you write a single-threaded program that has a for loop that goes through every page but somehow somebody either the operating system or the runtime system or or the hardware decides oh I'm going to start a new thread assuming that there's going to be a huge loop iteration going through every single page maybe I have a thousand pages let's say I'm going to start the new threads for every thirty second iteration you could imagine someone doing that automatically right and that's essentially transparent implicit somebody didn't program it to be operate on multiple threads but somebody's creating these threads automatically and of course somebody needs to ensure that those threads work correctly also runtime system could be doing that hardware it could be doing that and then it eventually because the program is written in a single threaded manner somebody needs to stitch everything back together such that the outputs are as expected by the programmer like a single thread output right this is you can think of this as a task level out of order execution right testable parallel execution that's transparent and people have actually strive for this a lot it's not an easy thing to do and the gains are not have not been very impressive so far for this one but here gains can be impressive of course as we've seen before also ok if we are time later we may cover some of these thread level speculation approaches so this is all trying to partition a single problem into multiple related tasks because the tests are cooperating in the end they're trying to come up with a single result where all of them contribute to but you can also have many independent tasks together like parallel job simulations right you're you're designing the greatest processor of the future and you want to simulate like the simulations you're doing for your lab lab assignment and you want to run many different versions your memory controller let's say or data cache and you want to figure out the best configuration that's a very parallel test and they have nothing to do with each other so it can actually paralyze that across many different processors batch simulations and example different users cloud computing workloads that have nothing to do with each other is another one right and this is relatively easy when you have a lot of these tasks but this doesn't improve the performance of a single task clearly right because you're assuming that there are multiple tasks so this they're more easier tasks we're going to we're going to talk more about this part today and less so about this part we're going to assume somebody has somebody has provided you threads okay but even this slide actually has a lot into it like some of the issues we've looked at interference and resource contention plays into all of these ok let's talk about some fundamentals first so there are two types of multiprocessors really loosely coupled and tightly coupled and the key difference between them is whether you have a shared global memory that's visible to the programmer so loosely coupled there is no shared global memory address space this is like a network essentially where as tightly coupled shared global memory address space this network based multiprocessor is here this is what traditional multi processing is about for symmetric multiprocessing existing multi-core processors multi-threaded processors all have this and these are usually programmed via message passing there are multi processors also we've seen one example of this the tesseract graph processing engine in memory that was based on message passing right you explicitly send calls send and receive calls for communication remote function calls for example if you want to distributed systems work this way it's essentially a distributed system where the memory is separate for each processor but if you want to operate together you send a message to this processor and that processor can execute a function on that message and then it can reply back with another message this essentially how distributed systems are programmed today we're going to cover a lot of the shared global memory address space today although this is very interesting to you but we don't have time in this lecture in this case this is very interesting the second one is very interesting because the programming model is very similar to unique processors right you're assuming really a single global memory address space and all processors can load from that address space and stores that address space and communicate by doing loads and stores to a shared memory location for example multiple shared memory locations so it's really like a multitasking uniprocessor except operations on shared data requires some sort of synchronization and we're going to talk about that okay there are many design issues and tightly coupled multiprocessors how do you do the synchronization how do you handle locks atomic operations we've talked about this briefly we're not going to talk about higher-level synchronization primitives that much if you take a parallel programming course you will see that a lot in the parallel programming course I'm here we have taken any parallel programming course so you've seen synchronization primitives like test and set test and test and set those things sound good ok good you've seen different ways of doing the locks locking such that you don't run into starvation fairness dot dot ok you should definitely attend Michael Scott's lecture you know about MCS locks yeah that's Miller coming and Scott and Scott is Michael Scott yeah basically that's it's a way of doing locking such that you reduce the overhead of locking significantly ok we're not going to talk about that we're going to assume that that's done correctly actually but we're going to talk about lower level issues in supporting that because if the lower level issues are not done correctly none of this really matters at the end because you'll get incorrect execution ok we're going to talk about cache coherence or the ensure correct operation in the presence of private caches memory consistency ordering of memory operations shared resource management we've talked a lot about and communication interconnects that's going to be one of the next lectures ok load imbalance is a problem as we've discussed in the past how do you partition a single task into multiple threads that clearly affects how much parallel speed-up you gets right you remember the does law that I showed you earlier we're going to see that again but if you're not in bail you're not balanced in terms of your load I'll give you the example again you partition your book into 32 threats and one of the threads get all gets all the empty pages let's say that's thread clearly has a very low load so it's going to finish early so you're going to waste the processor that's executing that thread right every other thread will have much more load so that's a clear example of load imbalance so you're partitioning mechanism matters a lot in terms of your load if you blindly partition you may run into the situation I just said a thread gets all empty pages or all pages with pictures whereas though there's no character to count right synchronization how do you synchronize efficiently between tasks how do you communicate between them and we've seen some of these things as we've discussed when we talked about heterogeneous multi-core again we're not going to talk about the higher level primitives over here but we're going to talk about the lower level support like cache coherence and memory consistency in how to implement this and ensuring correct operation while optimizing performance this is really what's parallel processing is all about actually how do you maintain correctness while maximizing your performance we're not even talking about energy energy actually complicates things even more but a lot of the design of parallel computers both in the hardware as well as a software and the libraries and everything is really about this okay and we're going to get back to that so just one a site hardware based multi-threading is really an even more tightly coupled form of multi processing if you will basically a tightly coupled in the sense that resource sharing is even higher between the threads right basically you have multiple threads that are concurrently running in a single pipeline in this case in a single core and this could be coarse grain fine grain simultaneous coarse grain is essentially you maybe quantum based every quantum you switch to another thread it could be event based for example Itanium to intel's ia-64 processors used to switch on l3 whenever you get an Elfie miss you switch to another threat and you have a multiple thread context to switch between it's relatively coarse grained so that you can overlap the latency of the l3 missed right you could also do run ahead fine-grained is another one and we've seen this a lot you every cycle you switch to some other thread and that's the default mode of operation and there are a lot of benefits to it you will remember and simultaneous is even more finer grained basically you don't switch every cycle but there are always instructions from different threads in the pipeline at the same time so when one ALU is executing an instruction from thread a another ALU maybe executing an instruction from thread be another ALU maybe executing an instruction from thread see another from thread D dot dot this is what Intel calls hyper threading really it's really a simultaneous multi-threading it's even more fine-grained than here but they can be combined clearly so simultaneous multi-threading doesn't assume that you switch every cycle to some other thread you can be executing from this thread but you can also be feeding instructions from this other thread and they're both executing in an out-of-order manner the out of order execution circuitry is operating on both of them so whenever any thread has an instruction ready that instruction gets scheduled into the functional unit as a result you may have actually multiple instructions from different threads scheduled to different functional units at the same time so if we get a chance to cover multi-threading we'll go into more detail in terms of simultaneous multi-threading actually how it's implemented in existing processors so the good thing over here is you can improve execution unit utilization if you have sixteen different execution units all of them will be operating on sixteen different threads at the same time whereas in terms of coarse grained clearly coarse grained you don't have multiple threads active at the same time fine-grained you have only one thread that's going through a single pipeline stage at a given time so simultaneous is much more finer grained okay that's an aside but that's really this is again multi processing all of the issues that we're going to discuss happen in multi server processors also okay before we move on let's talk about M dollars or a little bit more because this is the should always be on the back of everyone sets they're always going to be will always be limited by the parallel speed-up and the paper that you're reading is going to talk about that also three pages remember I said that M doll was concerned about the power of parallel processing and this paper is titled the validity of the single processor approach in improving processor performance right he was arguing basically all actually you should really improve single processors but before we go into M does well let's have some fun so this is a polynomial evaluation right this is a assume a I where I is zero through four our constants X is an input this is a polynomial that we want to evaluate now assume we are given the inputs X and each a I I'm going to ask you some questions assume each operation takes one cycle for example this addition takes one cycle and a multiplication takes one cycle and you can see that there are many additions and multiplications when you will want to evaluate this polynomial and assume there is no communication cost and each operation can be executed a different processor so you're really limited by one cycle add one cycle multiply the first question is how fast is this with a single processor single processor basically a single processor can do one single operation at a given time either an ADD or a multiply no parallel functional units here and there's no pipelining or anything assume you're one cycle you do the add one cycle you do the multiply 14 okay any other takers you can use pen and pit and then paper also although you guys are smart you could compute this in your head I cannot I have my cheat sheets over there anybody do better than 14 or worse than 14 maybe 14 is an incorrect program think about it a bit you would say 12 okay what else 17 okay c11 okay that's good I have 11 arrange or 11 to 17 that's good anything else know anybody doing better than 11 whoo-hoo everything's it's 11 okay multiple people who works for 12 I heard 12 also okay who votes for 14 Wow 17 longwood any other takers okay keep that in mind we're going to get back to this what about well I said this already what about with three processors that's a fair question maybe or easier question three processors meaning each processor is the same again it can be only one addition or multiplication but now you have three of them and when you need to communicate the results there is no latency we're going to ignore even that latency right now so you can have three processors doing three multiplications at the same time or any combinational multiplication or addition so again four cycles that's right yes five cycles okay four you're not sure about four you're sure about five sounds like pretty sure that's good I like the pretty shirt what else remember there's a correctness performance trade-off here you can get four but it may be incorrect that's exactly what what this is about actually well this is not exactly about that but that's one of the examples that could come about I could actually do all of this in zero cycles except I'll give you a random result any other takers somebody says five who says five who agrees with five okay one more why is it related to the previous answer okay maybe there's some relation that I don't quite understand at the moment okay should I move on or anybody else attempting to do this okay I'll I'll move on I think so the first answer those of you said eleven were right and I'm sure everybody else is right by definition because you can add delays arbitrary delays and make it 17 and we will see why is it 11 and the second one five is actually the minimum I believe while maintaining correctness so I think you got the five right so how would you do this basically I would if I were first thinking about it if I didn't know better I would probably take out the XS right and then we use them somehow so this is one example basically single-processor you have eleven operations because it's a single processor it takes 11 cycles right and this is a data flow graph so I would have an X 3 over here X square X cube X to the 4 I would generate them and I would basically multiply that with affective ace with whatever I generate it this is a 1x1 sorry a 1x a 2x square a 3 X cube and then a 4x4 and then I would add and then there's an addition tree right and this is a logical way of doing it that sounds good right 11 cycles and the multiprocessor version you'll need to think a little bit more about it this is a parallel version basically you could take the same thing and paralyze this essentially and that's what you get basically which ones are parallel x squared a 3x a 1x over here and then add a 1 X plus a 0 and you can go through this basically this is this is what determines this is the critical path if you will because you need to go through a 4x to the fourth you need to compute X to the 4th and then you need to do the addition in between somehow ok I don't think there's a better version than this if someone comes up with a really false 4 cycles let me know ok so I'll basically speed up with 3 processors in this case is 11/5 right that's 2.2 clearly we didn't get 3x speed up on this workload which is unfortunate but the first question that you should really ask yourself when you're doing this sort of parallel speed-up comparison is is this a fair comparison so what is really speed up speed up really should be defined as the best algorithm on a single processor divided by the best known elk so best known algorithm to do the computation on a single processor divided by the best known algorithm on the multiple multiple processor system and the time taken for those algorithms so the question is have we used the best algorithms anybody else to challenge the 11 or the 5 well I cannot challenge the five because I don't know a better algorithm but I'll actually challenge the eleven which will probably draw our speed ups to a lower thing you can actually do better than the single processor algorithm that we looked at before what we looked at was this thing over here and it actually had a lot of operations but somebody smarts called Horner developed a method to for polynomial evaluation a long time ago as you can see almost 200 years ago now I think yeah and maybe somebody else did that before him but he is definitely for the first one who wrote the paper about it but have you got steady foreigners method for polynomial evaluation in high school probably okay you remember okay good excellent now you're going back to all that high school knowledge right so that's where your high school knowledge matters in developing algorithms so if you actually go back and look at foreigners methods and you can actually have nice ways of doing it I'm not going to go through it and this is a nice paper that talks about it but basically you take out progressively the X's over here and you have the minimal number of our patients in that case and what what what it looks like is this and it's only eight operations basically you get eight cycles on a single processor if you do the comparison of the speed-up you're now down to one point six so that's one caveat or parallelism if you really want to get the benefits you really need to show the benefits compared to the best algorithms with a single processor version so if the single processor version can be optimized even more you may decide to build the big parallel processor but you may not really get a lot of performance because somebody else optimized their single processor code that's always true but I think it's especially true for this case okay so this actually brings me to another point over here can speed up be greater than P with P processing elements in fact if you don't pick the best algorithm you could easily get speed-up that's greater than P you have a terrible single processor algorithm I don't want to put you on the about 17 cycles divided by five is greater than 3.43 right that's 3.4 so you would get super linear speed-up and whenever you see super linear speed-up you should really think why is it happening one of the reasons maybe the comparison is not fair and that's you that's that's a very valid reason so what is super linear speed-up basically you plot the process we've seen this graph before right the scalable T curves this is called a scalable T curve also you have the number of processors or a number of threads this is speed-up compared to the single threaded version this is the linear regime and typical success looks like this as we've seen in fact it actually drops later on as we've seen in the heterogeneous multi-core lecture and a typical success is super linear but there are usually reasons for it unfair comparisons could be one reason you can compare the best parallel algorithm to a wimpy serial algorithm as a result that's unfair and the other thing could be you may be adding some things other than processors into the system now this may be a valid reason but it's not necessarily just because of processing power you're adding but it's really because of cache you're adding for example for example if you have more processors you may have more cache and memory and your working set magically starts fitting into the cache now you don't get misses anymore as a result your speed-up shoots up at some point when you're working set starts fitting into your cache or memory and that's a reason to get super linear speed-up that's a valid reason but it's not because coming because of the process it's coming because of something else in the system that may be true for disks or network also right if you're adding more network connections for example getting more network bandwidth because you're adding more processors you may be getting super linear speed-up okay but this is a cautionary tale so that you should be careful when you see this sort of super linear speed-up okay let's define some other metrics we had some fun let's have some more fun so there are some traditional metrics that are used to talk about processors these are old but I think they're they're very instructive utilization is one we dominance is another an efficiency that you is basically how much processing capability are using compared to how much you're tying up so basically this is the number of our patients in the parallel version divided by how many processors you're tying up for how long of a time we'll see this with a pictorial example redundancy is how much extra work you're causing because of additional parallelism and usually your redundancy is more than one hire this is a number of operations the parallel version divided by number of our patients and the best single processor algorithm immersion and efficiency is basically a combination of both time with one processor divided by processors times time with P processors how much efficiency you have compared to the best single processor version basically efficiency is utilization divided by redundancy so let's take a look at these based on the example I've given remember utilization is how much processing capability we're using we're assuming we're tying up three processors that's the three processors that we have four five time units that's the best algorithm we had for the three processors right we're really tying up five of them for three cycles so we're really tying them up for 15 time units we're assuming that all processors are tied up on full parallel computation finishes now of course with multi-threading this assumption changes but this is a pure metric but we're not doing operations on all of them as you can see there are some load imbalance in the processors over here this processor executes five operations as processor three operations press through two operations so if you go back to this picture that's how I got this picture this one five operations on this processor three on this and two on this so our utilization is 10 out of 15 basically okay redundancy is how many operations that we have with the three processor version ten how many operations do we have with the best single processor version eight so our redundancy is greater than one this is actually a good way of checking whether you have the best single processor version or not if your redundancy is less than 1 or equal to 1 you should have a question maybe I don't have the best single processor version because parallel adding I usually add redundancy to your program you need to do some more work to get the benefits of parallelism in this case clearly we did some more works more operations basically and in fact I actually didn't count some of the operations like communication right we didn't even count the communication between the processor so that adds even more operations and if you have some other data set some other problem you may actually need to copy the data such that different processors need to operate on different copies of the data and that adds even more redundancy into the system in this case we looked at a very simple version we don't need to deal with communication or copying of the data so usually this redundancy is much higher than one if you're getting really close to one you should always question yourself maybe I really don't have the best single processor version so efficiency is how much resource we use compared to how much resource we can really get away with basically tying up one processor for the best execution time units divided by tying up P processors for the time units that we've seen we if you had the best algorithm for a single processor you would tie it up for eight time units it with the multi processor version we tied up free process for five time units so our efficiency is actually a little bit over 50% in this case we're not very efficient if you think about it and this correlates with energy efficiency certainly but energy efficiency is much broader than this because you need to take into account frequency voltage that okay hopefully these are simple but they actually give insight into how well you're operating how well your parallel processor is really operating sim D efficiency simply utilization that we've seen in GPUs is another example of this actually a very GPU specific version of it how many of these thread slots that you're tying up versus how many of those that you're really using right here essentially that's what we have except not in that context okay so let's talk about MDOT law you've seen this before that's the parallel speed-up curve so why do we have this reality over here clearly we have diminishing returns and at some point things go down over here although it's not clear if M does law see that things go down but we're going to talk about those things going down later on so why do we have this reality clearly we have diminishing returns we have a parallelizable portion of the program and we can speed that up perfectly assuming and we have an unparallel eyes over a portion of the single-threaded program that's nice this is where redundancy helps I think I'll bring my other processor yeah or an extra source of battery would help also but less will have to make do with this okay that's good I like the green better it okay so basically this is the time it's take it takes to execute with key processors it's the parallelizable fraction times time it takes to execute with one processor divided by PP assuming this perfectly parallelizable this fraction alpha is perfectly parallelizable and this is a non paralyzed Abell part basically non parallelizable fraction times the execution time with single processor right it makes sense and then you can calculate the speed up with P processors as t1 divided by PP and this is what you get and as speed-up as P goes to infinity you have an infinite number of processors this is what this equation boils down to so your ball neck for parallel speed-up is really this parallelizable fraction in the end right and we can draw a nice curse let's do that this is my handwriting sorry I'm it takes a lot of time to draw these actually with Excel so I'll make you read this but I think the key the key is very simple I have one graph with Excel later on but basically adding more and more processors gives less and less benefits if alpha is less than 1 this is alpha 4 point 9 alpha 4 point 9 5 alpha 4 point 9 8 and if if alpha is 1 then you'll get linear speed-up actually assume parallel portion is perfect apparel but this is why you get the diminishing returns and I'll see I'll show you a similar result so that's one illustration of Amdahl's law now I can turn it aside and you can put the Alpha over here and you can put the speed up over here and you'll see that the benefit I e the speed up from parallel processing is small until you get alpha that's really close to 1 and this is also interesting because you may not always so this is the perspective of all you have some program and you want to paralyze it how many processors should you use and this is a perspective of maybe I have some number of processors P 1 P 2 P 3 and I'm going to develop my software to become better right and how do you develop your software to become better you're you change your alpha right you try to find the opportunities for reducing the serial part and as your alpha grows becomes more parallel your program becomes more and more parallel you get closer to this peaking part of the curve but it takes a long time to get there and I'll show you an example of this in a little bit basically your benefit is small until alpha is really close to one and this is obvious right your alpha is 50% the maximum speed-up is 2x your alpha is 99% your maximum speed-up is 100x your alpha is 100% your maximum speed-up is infinite assuming you have infinite number of processors I guess and mathematicians would say it's technically undefined but yeah we're not mathematicians ok so let's talk about TV outs of parallels I'm a little bit and then I'll finish it after we were done with the parallelism of kkb that's a problem I've shown you the slide before actually this is the same view over here so clearly maximum speed-up is limited by the CL portion you have the serial bottleneck but what M does law does not even consider is this part this is the parallel part right parallel portion is not perfectly parallel usually and remember I said that this is a perfect memorization question for an exam why there are three reasons one is synchronization overhead because you have updates to share data and there's some communication overhead which we ignored in the previous example there's load imbalance as we've seen imperfect parallelization 3 processors doesn't mean that all of them are operating in sync some of them may have a bigger load and there's resource sharing overhead which you really really didn't consider in the previous example either right when one processor is accessing memory the other will be not to be accessing memory because there's a resource conflict right and we've seen this a lot earlier so all of these actually reduce this equation and make it much more complicated than we can actually model today but that's actually a really interesting direction how do you model put these things into mmm does law such that you can actually have a better model for the parallel speed-up not easy but you should always be thinking about these things you're not always dividing by n as n actually gets this gets divided by very little if you're completely serialized because of this then you're back to square one you're not even dividing by n right ok so this is the sequential bottleneck I said I promised one Excel figure and this is my excel figure it's ugly I think my handwriting is nicer it I'm just kidding at least I can make my point better with my handwriting but this is a parallel fraction going from 0 to 1 this is a speed-up and as you can see with 10 processors we get to 10 over here but we start getting to 10 very late now if you have 100 processors we start getting 200 really late close 200 the cure shoots up really late over here after 80% and if you have a thousand processors in order to get to a thousand you really need to be more than 96 percent parallel so that's the curse of the sequential bottleneck it's essentially exactly why the cray-1 machine that we've discussed was the fastest scalar machine of its time Cray was a smart guy all of those architects were smart people they said we have this parallel machine we can do great in these parts of the program but in the end if we want to get speed-up we got to do really really well in these parts of the program so that we're not bottleneck by it and that's the same let's exactly why they designed the scaler unit to be really fast so there are many reasons for the sequential bottleneck for example non parallelizable operations on data and this is clearly an example that's looking at the extremes right there are also sequential bottlenecks that are relatively less more sequential than this one maybe you have two threats reduction operations for example tend to be sequential at some point you start with many many a lot of parallelism but you're reducing the data for example you're doing an addition across many number of elements you can think of that as a tree for example right and as you go down the tree your parallels and reduces and you have a really sequential portion and tiered sequential portions but basically you have non parallelizable operations on data non paralyzed double loops is a really good example of the sequential bottleneck if you cannot paralyze it because there are a lot of dependencies between different loop iterations your bottleneck one example could be this but you can of course try to paralyze it just don't do it right now and there are other causes as well for example single thread prepares data and spawns parallel tasks it's usually sequential to do that actually because you you touch a lot of shared data and you may want to keep it sequential actually okay so this is one example from a paper that you're reading this is a critical section execution acceleration paper and we have this example in that this essentially one thread is spawning threads over here for example you have a priority queue of tasks and the threads that's executing this part a is actually spawning a bunch of threads and for each thread you're doing some stuff and this is a critical section it's removing a task from this priority queue that was prepared by this initial thread and removing the task is clearly accessing the shared priority queue so you need to lock it and then after you remove it you have the parallel portion everybody can do this in parallel because you already copied the problem and solving it on your own and then when you actually create new problems based on that you insert it to the priority queue so this is an example of a dynamic tasking program a lot of programs can be programmed using this dynamic task double parallelism and each thread does this many times eventually they figure out that or we've sold the problem and somebody is to print the solution and printing the solution could be a sequential task also right assuming you have one communicating thread with your print driver right now that's these a and E are your sequential bottlenecks C C 1 and C 2 are your critical sections D 1 and D 2 are paralyzed about portions right and this is an example timeline from this paper a may be long you have a long sequential portion and you have a parallel portion over here and you have at the end may be a long sequential portion also so you're really bottleneck by these two and now in the parallel portion you have idle times because of critical sections for example over here and this shows only the critical sections it doesn't show the load imbalance well I guess this is the load imbalance over here if you think about right that's this is done over here and basically the other threads reach the barrier over here at the very end or at the end of the end of the loop at the very end so there is some load amounts over here but this doesn't show the contention resource contention issues in the parallel section so parallel programs every one of them looks like this actually if you look at the execution okay so about Lakes in parallel portion clearly synchronization is one bottleneck right basically operations manipulating shared data cannot be parallelized you can try to paralyze them but you run into other overheads we've covered the slocks mutual exclusion barrier synchronization this is also a communication problem basically whenever you're synchronizing you need to be communicating tasks may need values from each other but essentially I'll use these interchangeably communication if you're doing it right you need to be communicating only when you're synchronizing on shared data okay the downside is this causes thread serialization when shared data is contended load imbalance we've seen parallel tests may have different lengths and this could be due to imperfect parallelization this could also be due to micro architectural effects for example you have an unfair memory scheduler and you delighted your program and memory scheduler prioritize the streaming threat as we've seen very early on right if it prioritize the streaming threat and slows down the random access threat these may be cooperating together on different portions of the data as a result the random access threat reaches the barrier the end of the parallel part late and you have a load imbalance because of a purely micro architectural effect this could also happen in the cache so resource contention can lead to a load imbalance also or for example one of the process one of the threats can be executing on a processor that's not as fast as the other one that's could also happen maybe it's that processor is currently throttling for some reason because it's closer to a heat source right these things happen actually these are real-life issues that may lead to low the amounts even though you may have perfectly paralyze your program across different threats ok this clearly reduces speed-up in the parallel portion and resource contention as we've seen parallel tasks can share hardware resources delaying each other right so clearly replicating all the resources is expensive over here and this causes additional latency not present when each test run runs alone for example you get robot for thrashing right or you get thrashing in the cache so your performance actually can be great for all tasks compared to mine your run alone this this is not handled well that's why we dedicate a lot of time to this one and we'll talk about how to support synchronization at the architecture level today so another view of this threads in a multi-threaded application can actually be interdependence as opposed to threats from different applications and they synchronize with each other with these things and we've actually seen some of these some threads can be on the critical path of execution where some threads are not and we've seen this before that's why I'm going to repeat this relatively quickly but even within a theory at some code segments maybe on the critical path of execution some are not so it's always good to think about that and we've tried to accelerate these quote segments if you remember with the state execution model with heterogeneous multi-core processors we were going toward that direction but I want to jog your memory basically critical sections are there to enforce mutually exclusive access to share data we've seen this in the previous example and only one thread can be executing at a time and contended critical sections make other threads wait and thread causing civilization can be on the critical path we've shown you these slides that's what I'm going to go through these relatively quickly and remember because we're going to deal with issues like this well deal with the hardware support to actually make these things work so barriers they lead to a synchronization point and the thread that's leeching the very there year the last determines your execution time and threads that are reaching the barrier early need to wait for all threads that reach the barrier right and we've discussed how to optimize this before so if you have a heterogeneous multi-core processor you ship the critical path threat to that if there's a large core that's one example or you could say power in this thread by reducing the frequency and voltage at this point right okay and we've seen the pipeline parallel programs as well right you can have a loop iteration that's divided into code segments called stages and you may divide stages such that for example Part A in each iterations operating on some data set that tends to state constant and this Part B is operating on some other data set and this Part C is operating on some other data set if you actually partition those data sets and if you partition the loop into different threads or stages ABC you can actually execute those ABC in different processors where the data sets reside and as a result you can exploit locality and maybe you can actually customize those processors for these computations as well so that's what this thing looks like these gets this gets different instances of a this gets different instances of B and this gets different instances of C and the way you communicate between a B and C in a single iteration is through these queues which could be software based or Hardware base again but again your bottleneck becomes the one that's lagging the slowest stage determines your throughput and parallel performance and yet in this case the slowest stages stage B it's always taking long as a result it's determining your speed-up okay so I think this one is my last slide before we take a break basically parallel programming is not easy but it's easy if parallelism is really natural so if you have embarassingly parallel applications those are actually relatively easier cases a lot of multimedia error codes for example graphics physical simulation they actually have a lot of embarassingly parallel parts those are the easy parts and maybe larger web servers on databases the throughput oriented parts are actually easy parts but whenever you're operating on shared data in any of these it becomes hard also so the shared parts of all of these applications for example how do you do the locking in a database when you have a billion requests coming into your database that becomes actually really tough because you have a bottleneck shared data portion that could be true for a web server also so difficulty is really optimizing those parts where you have these bottlenecks as we've seen I'll call all of them bottlenecks in general synchronization load imbalance and resource contention they all lead to bottlenecks so difficulty is really getting parallel programs to work correctly while optimizing performance in the presence of these bottlenecks so it could give either of them up and you could get a really good parallel computer you could give correctness up well I don't know what you get out of that maybe this approximate computing assuming it works maybe that's not a good thing to give up but you can give performance up and you can easily get correctness right but the real interesting thing is much and that's what much of parallel computer architecture much of what we will talk about will be about is designing machines machines meaning complete platforms hardware and software that overcome the sequential and parallel bottlenecks to achieve higher performance and efficiency while ensuring that the programmers don't go crazy basically making programmers job easier in writing correct and high-performance parallel programs again you can give this up this becomes really easy and you can give this up and this becomes really hard to write okay so that's why we're going to cover memory ordering this is a perfect example of this trade-off actually but we will do that after we take some number of minutes or break about nine minutes so that we're back here at 1440 okay let's get started are the lights still good nobody's falling asleep yet but the topics are so exciting that it cannot fall asleep yeah it's going to get even more interesting anything like every every other lecture okay this where we left off basically much of parallel computer architecture is about designing machines that overcome the sequential and parallel next to achieve higher performance and efficiency while ensuring that the programmer is staying sane and productive say maybe easy but productive is hard actually okay so I will look into memory ordering in multi processors this is actually the seminal paper that I mentioned earlier and these are some other papers that are very interesting to eat first I will this there are two kinds of orderings actually we will cover the first one is a global ordering that's called memory consistency unfortunately naming is not that great because they are both consistency issues but memory consistency usually we force two ordering of all memory operations from different processors to different memory locations very easily we are concerned ourselves with we're concerning ourselves all memory operations cache coherence which you will cover later basically the global ordering of all accesses of access to all memory locations the other ones coherence which is about the ordering of our patients from different processors to the same memory location it's really about the consistency for a single location they're related and we may talk about the relationship later on but they're very different necessarily a local ordering of accesses to each cache block it's really two each word of memory but we're going to generalize it to each cache block because things operate in their cache block grant legs today but they don't have to all the time okay we should really distinguish between the two we're going to talk about consistency first global ordering of x2 all memory locations from different processors as I said much of parallel computer architecture is about designing these machines that overcome the sequential imperil bottlenecks while making programmers job easier in writing correct and high-performance parallel programs so that's where the ordering comes into play basically whether you make the programmer job easier let me give you an overview of this ordering across different processors we've actually seen this before in the single core as well as dataflow case so let's assume we have operations ABCD you can assume that there are memory operations they don't have to be let's take memory for now in what order should the hardware execute and report the results of these operations that's the key question that we're asking and we've asked this question before there are two issues here there's a contract between the programmer and the architect or micro architect that's specified by the ISA that is the order that's specified by that's exposed to the programmer right and preserving an expected order or accurately an agreed-upon order simplifies programmers life that's the is a specified order that's the architecture what is visible to the programmer right underneath the architecture you can do anything again but what is visible to the programmer is what matters for the programmer to not go crazy right and we stay productive because it may it changes how easy debugging years of easy state recovery is how easy exception handling how easy reasoning about your program is and clearly there's a trade-off here because preserving an expected order usually makes the hardware designers life difficult and especially if the goal is to design a high-performance processor remember we had the reorder buffer that's added in an auto board execution processor in other word execution processor had a sequential instruction stream it fetched in sequence and gamm order and it executed things that out of order in the dataflow order in a restricted manner and it had to reorder all of those instructions when it's updated the architectural state including registers and memory and this was difficult because you need to keep a lot of state you need to ensure load and store stores get correctly ordered and we've discussed how do you ensure order right whenever you get a load instruction it needs to check whether all of the store instructions if it's depend on any of the store instructions that are older right so we need to actually do an content-addressable memory comparison to all of the addresses actually it's a range of addresses and in an ordered manner so actually the hardware becomes very very complex even with a single core processor in this case so clearly there's a trade-off between the programmer and the micro architect and the reason all the world executions successful as we've discussed is because we were obeyed that sequential execution semantics and all of those machines that did not obey that sequential execution semantics disappeared IBM 360 91 CDC 6600 those were the first machines that implement out of order execution 40 50 years ago and they were not successful our word execution became successful then people said oh we need to reorder the instructions when we make it visible to the architecture in 1984 1985 and later first incarnated in motorola 88000 and also i intel pentium pro ok so in a single processor we had this issue basically the memory ordering visible to the program is specified by the von Neumann model and it's a sequential order Hardware execute the load and store operations and the order specified by the sequential program it's very simple all the word execution does not change the semantics it changed the implementation hardware execute the operations in any order to Isha's as long as the three dryers or reports to the software the results of the load and store operations in the order specified by the sequential program so it opens the contract the advantages for the programmer architectural States is precise within an execution and architectural state is consistent across different runs of the program right so if you have a bug in your program you'll consistently at the same place so whenever you're debugging it's easy you can reproduce the bug right because if you have a bug the bug will appear at some point and when when the program crashes or does something weird it gets an access protection for example you know that it stopped at that instruction that caused that problem or and the last instruction that was retired was the instruction just before it and no other instruction that came after this instruction affected any of the state that's the beauty of one normal model now I can debug programs really really easily disadvantage preserving order adds overheads reduces performance increasing complexity reduces scalability and this is all the baggage of implementing automatic execution you know in a manner that's hidden from the programmer yes well executing out of order and preserving order because of for example this load store queues right that it's actually pretty complex yeah basically all of that machinery that you need to add to ensure that you need to report the instructions in the correct order adds complexity and as a result it reduces scalability because now you're more complex engine and you can put only a few of those into your area budget rate okay and we've seen that trying to reduce these overheads is an open research problem okay so let's memory ordering in a single processor we've also seen to some extent memory ordering in a dataflow process this is completely opposite end right basically a memory operation executes some into operations already and ordering is specified only by data dependencies that's the contract so the programmer should not expect anything in this case to operations can be executed and retired in any order if they have no dependency the advantage is lots of parallelism so you get high performance because you're not constrained to a sequential order right now the disadvantage is are many precise data is very hard to maintain as we've discussed because see may execute in any order right and it's very hard to debug whenever your program crashes or whenever you put a breakpoint you don't know what's executed right and order actually can change across runs of the same program also so this one is even if you don't execute the same program twice it's very hard to debug because you don't know where things stopped so if you take a memory dump for example it's very hard to examine because you don't know what if operations execute it unless you dump everything including all this operations in this stage this operation is in this stage dot which becomes unwieldy actually that was one way of debugging programs if you don't if you do inter precise state there were machines that dump dump the entire pipeline stage state to memory and it was the programmers job to figure out which operations execute them which operations didn't update the memory State now that's terrible right you don't want to do that it's already difficult to debug programs with the bugs we have you don't want to deal with the pipeline State so this is heart within a within a single execution you don't know the precise state but it costs executions you also don't know you can get different results because things happen based on the dynamic order of events right so data flow processor is very difficult to program because there is no guarantee in terms of ordering at all and as a result nobody really implements dataflow processors today our word execution is the best approximation but it's really not dataflow at the is a level it's really dataflow that the microarchitecture level right so we're going to see similar issues in multi-process multiprocessor actually between these two extremes and we're going to see the issue is related to it we're gonna be out going to solve all of the issues but we're going to look at one of the issues so for example we're not going to solve the issue of order changing across the rounds of the same program okay so let's take a look at the issues over here so each processor is memory our patients are in sequential order with respect to the thread running on that processor so we're going to assume that each processor obeys the von Neumann model because that's a good model multiple processors execute memory operations concurrently the key question is how does the memory see the order of operations from all processors in other words what the ordering of operations across different processors and the first question you should ask is why does it matter well it matters because of the things that we've discussed actually ease of debugging correctness and performance and already ease of debugging basically it's nice to have the same execution done at different times to have the same order of execution repeatability we're not going to solve that problem but they're issues related to that also we're going to focus more on this one mainly for correctness basically the key question I'm going to ask is can we have incorrect execution in the order of memory up if the order of memory operations is different from the point of view of different processors if this processor observes memory operations in some order versus this other processor observes memory operations in some other org will that lead to correctness issues and the answer will be yes that's why it's interesting even when each processor obeys the one moment model they're ghosts coming in I think ok and we're going to talk about performance and overhead a little bit although not a lot because we just don't have time because enforcing a strict sequential ordering for example you can say that every operation is ordered sequentially inside memory this can make life harder for the hardware designer in implementing performance enhancement techniques like autumn or the execution and caches and we're going to talk about that briefly but let's let's jump into this correctness problem and these are all actually resource research problems that are not that easy that are relevant to some people ok when could order affect correctness and one of the key T times is really when you're protecting shared data so what I'm going to describe really matters for people who are writing libraries for example for manipulating shared data so if you're programming nicely with a nicely written library this may not affect you that much but if you're programming with a based on a library that's not written well you may actually have problems but we're going to talk about really the hardware support that's needed to write a good library that actually works so threads are I mean we've discussed shared data before threads are not allowed to update share data collect currently for correctness purposes this is mutual exclusion principle right access to share data are encapsulated inside critical sections and they're protected via synchronisation contracts of which there are many locks semaphores condition variables which we're not going to talk about again but the key is only one thread can execute a critical section at a given time that's the mutual exclusion principle and this is what the multiprocessor should really obey writes the multiprocessor should provide the correct execution of synchronization primitives to enable the programmer to protect the shared data basically it should support this mutual exclusion principle so how do you support mutual exclusion there is a software part of it and there's a hardware part of it the programmer first needs to make sure mutual exclusion is correctly implemented we will assume this the synchronization perimeters don't have any bugs but of course it's a critically important topic also and this is an old topic also and if you haven't read this I would recommend reading Dijkstra's seminal work on cooperating sequential processes which actually talked about how you should synchronize between different threads and he talked about Decker's algorithm this algorithm we developed with one of his students Dekker for mutual exclusion we're going to look at a very simplified form of Decker's algorithm I'm not going to claim that it's Decker's algorithm but I have to simplify it so that we can reason about it so basically but the programmer relies on hardware primitives to support correct synchronization but if the hardware primitives are not correct or if they're unpredictable for some reason programmers life is tough if the hardware primitives are correct but not easy to reason about programmers life is still tough so that's the key over here and there's a huge amount of research that we can cover here but we're going to start with the basics let's assume this basics and we're going to assume this program is correct because I'm actually omitting a lot of stuff over here basically we have these two processes that are executing there are communicating with each other this processor is setting a bit f1 equals to 1 over here indicating that it's going to enter the critical section or it has entered the critical section and it's checking if the other processor is in the critical section if it's not then it enters the critical section at the end of the critical section it says this bit to zero indicating that it's out of the de Gaulle section and the other processor does the opposite it sets this other bits F 2 equal to 1 indicating that it is in the critical section it checks if the other processor is in the critical section if not it enters the critical section at the end of the critical section it sets its bits to zero saying that oh I'm not in the critical section anymore dot dot and this else loop ensures that you go back somehow and ensure that you retry and that's the part I'm gonna ignore over here because there are a lot of algorithms that have been developed to make sure that that retry is efficient dot that's right that's the synchronization part of it ok basically we have two operations here in this processor the operation a and the operation B that we're going to concern ourselves with this processor needs to set f1 and check f2 this processor needs to set F 2 and check F 1 and for what the hardware needs to provide in the end is only one processor should be in the critical section at any given time right not both so assume p1 is in the critical section p1 is this processor intuitively it must have executed days right because it's here which means f1 must be one because a happens before B in the sequential one moment order which we're assuming for each processor which means that p2 should not enter the critical section right now if it does enter then that's a problem so the question is can the two processors be in the critical section at the same time given that they both obey the von Neumann model and we don't put any other constraint into the system and the answer is yes well I'll give you an example you need to construct an example basically these two processors are connected to the memory somehow well let's give you this example over here basically we have these two processors p1 and p2 and p1 is trying to set f1 so that it enters the critical section p2 is trying to set f2 so that it enters the critical section and assume that it takes time for these guys to read memory for these different processors to read memory let's say at time 0 p1 execute this operation a remember operation a is setting the bit that says I am in the critical section operation B is checking whether the other processor is in the critical section from the processor to point of view operation X is setting the bits that says I am in the critical section operation y is checking whether the other processor is in the critical section ok I'm going to concoct an example but it does happen in existing systems actually but p1 executes a sets f1 to one and sends that update to memory and assumes that it's done basically p1 at this point at time zero says I'm going to set F one to one and I'm going to assume that this is complete done p2 does the same thing it executes X which is setting F 2 to 1 and assumes that it's done from its point of view but it sends the update to memory so a P one's update is going through memory it's going to update F 1 at some points set it to 1 P 2 is update is going through memory it's going to update after at some point set this to 1 let's assume that this is closer to p2 and this is closer to P 1 so P 1 actually can read F 2 much faster then it can actually write to F 1 and P 2 can actually read F 1 much faster than it can write F 2 so let's assume that he wants read of F 2 takes 50 cycles but P one's right 2 F 1 takes hundred cycles and vice versa Peters read of F 1 takes 50 cycles Peters right to have 2 takes hundred cycles let's look at what happens from the execution so bear with me this is what I said just now earlier so at time one processor 1 executes B basically checks it tries to check if F 2 is equal to zero it starts the load of F 2 right remember F 2 is close to it it sends a load request to here processors to a store is going to F 2 in the previous cycle but it's going to take longer it's going to each F 2 much earlier similarly processor 2 is executed executes swai at time 1 it which is testing whether the other processor is in the critical section testing F 1 is equal to 0 it starts a load of f1 and it's going to take some time 50 cycles it takes 50 cycles to access this memory let's say okay and processor is one update of F 1 is still propagating over here so basically memory sends back to processor 1 f2 saying o f2 0 right so F 2 is 0 because processor 2 even though it said F - it didn't propagate over here yet but memory already sent to processor 1 0 at that point similarly processor 2 gets F 1 equals to 0 because processor two accesses F 1 in 50 cycles and the update that was made by processor 1 to F 2 didn't propagate so it takes hundred cycles to do this update but this processor assumed that it was complete ok so basically this processor p1 loaded F 2 that's equal to 0 and P 2 load F 1 which is equal to 0 and they both know think that neither of the processors are in the critical section so they both enter into the critical section at that point make sense so if you go back to this code what happened was this processor executes that one assume that it's done this prospects agree to assume that it's done it load this verse are loaded F 2 and it's God F 2 before the update of this processor propagated into memory this parts are loaded F 1 it's got a fun equal 0 before this update propagated into the memory as a result both of them think this this processor thinks f2 is equal equal to 0 and this process things F 1 is equal to 0 even though individually they both think or F 2 is equal to 1 over here and F 1 is equal to 1 but they both entered the critical section at that point in time and at that point that thing matters actually both of them violated the mutual exclusion principle at this point and at time 100 memory completes the operation a that was sent F 1 becomes 1 but it's too late and F 2 becomes 1 but it's also too late propagation happened much later so this is an example that could happen perfectly in a system that looks like this there may be other reasons contention for example in the network if your network doesn't preserve ward ring all of those things happen actually but in this case if you look at this both of these processors execute in one moment order right there was no other word execution within each processor they both did the operations in one moment order one no I mean order here what was different is the memory actually saw a different orders so let's see what happens over here basically processor ones view of memory operations look like looks like this it executes a which is setting f1 to one it executes B which is testing f2 whether it's equal to 0 and then it sees x f2 is set to 1 right because this propagated to memory at cycle 100 right memory completes X at this point assuming it read the X at that point right trustus truth view is very different it's assumed that X is completed at the time at rotor f2 and then it tested f1 equals to 0 Y and then later only at 100 cycles if it did I read at that point it's all a right basically from this point of view from this process point of view a happened before X or a appeared to happen before X from this process point of view X appeared to happen before a and clearly that's a logical inconsistency you cannot while it cannot happen at the same time right it has to be only one way so basically these two processors did not see the same order of operations in memory they assume the different order and from each processors point of view it was a correct order if you only assume one normal model of execution so you need something else to ensure that these processors operate correctly in the order in the presence of this sort of synchronization so this is there from the processor did not see the same order of operations to memory and as a result that happened before relationship between multiple updates to memory was inconsistent between the two processors point of view as I just said as a result each processor thought the other was not in the critical section and as a result you're getting correct results and this should not happen because the programmer relies on corrects a mutual exclusion support right any kind of synchronization primitive is broken if this is the case so how can we solve the problem basically that's the key idea of sequential consistency the paper that you're eating two pages that describes the exact same problem that I described make sure actually the examples taken from that paper also the idea is very simple all processors see the same order of operations to memory a single global order and everybody is on the same page as a result you don't get this inconsistency to happen before relationship from different points of view because everybody has the same point of view in other words all memory operations happen in order or are reported to happen in the same order right this is called the global total order that is consistent across all processors the assumption is that within this global order each process operations appear in sequential order with respect to its own operations so that's the one moment part of it okay that's the paper that you're eating basically the paper formally defines sequential consistency as a multiprocessor system is sequentially consistent if the result of any execution is the same as if the operations of all the processors were executed in some sequential order and the operations of in each individual processor appear in the sequence and the order specified by its program this is the one moment part and this is the global total order parts everybody sees a single global total order it could be any global total order as long as it's the same order so there are many many acceptable orders actually and we will see that and this is called a memory ordering model it's also called a memory model but memory models too general it's really a memory ordering model and specified by the ISA so x86 for example has some model which has changed over the years alpha has some model different different processors have the model also actually this is even bigger than this like a lot of things we've been seeing this model really effects the programming model rights and programming models everywhere so if you're doing distributed systems programming you run into a very similar issue your answer consistency in the global scale and the same issue exists over there at this data center is executing something this other data center is executing something how do you ensure consistency exactly the same issue happens and people have developed consistency models some people called eventual consistency eventually things will be consistent for example but we're not going to go into that similar issue again programmer interface programmer programming language interface languages also have this model languages need to provide some thing to the programmer such that the programmer can reason about how the operations will be executed by the compiler for example we're not covering that but very similar issues exist they're very working at the bottom of the stack the hardware what is the hardware provide but similar consistency issues arise at different levels of the stack okay so this is a program is abstraction basically you can think of memory as a switch that services one load or store at a time from any processor only one all processors see the currently service load or store at the same time and each processors operations are serviced in program order now if you satisfy this you satisfied sequential consistency and you don't get to the problem that we've discussed in correctness problem that we've discussed earlier now clearly if you implement it this way you get rid of all the parallelism in memory right although the bank doubled parallelism is gone although the channel level parallelism is gone your basic goal of the optimizations that you talked about are gone but clearly people are not implementing this way this is really an abstraction to the provided to the programmer programmers see this order but underneath things are executed in very very different orders okay but let's take a look at this sequentially consistent operation orders so in the example that we've shown there a bunch of potentially correct global orders and all are correct these are the different operations remember a is setting F 1 to 1 B is checking whether F 2 Z 0xs setting f-22 1xy is checking f2f1 is equal to zero execute by processor 1 processor 2 and they're all sequentially consistent orders as long as all processors see the same order and remember the 1 norm an order within each processor needs to be maintained they can actually enumerate all possible orders in this case I think there are only 6 if you find one more like you know I don't think there are ok so which order this also called an interleaving of memory operations which interleaving is observed depends on the implementation and the dynamic latencies so there are two corollaries to this first within the same execution when you're executing all processors see the same global order of operations to memory so correctness is preserved basically we've solved the correctness problem which is good another programmer can write correct parallel programs and because it satisfies it happen before intuition so it's very intuitive well if you break this I don't know what happens actually a lot of these issues arise in distributed systems this happened before relationships were developed for distributed system LAN port later actually developed LAN port clocks for example to ensure that happened before relationship holds in a distributed system but we're not going to go into that if you if you take distributed systems classes you will see this happen before relationship a lot has anybody taken distributed systems that's well you know about this probably ok good so one problem we have not solved is across different executions different global orders can be observed and each of them are sequentially consistent they're all correct orders but debugging is still difficult while debugging of this one may be easy you can dump the state and you can see what happens in this particular execution but if you want to replicate the problem you may not be able to because you may get a different order the next time you run the program right so a part of the debugging problem is alleviated because of this also but part of the debugging problem still remains and this is one reason some of the bugs happen some of the bug bugs become observable some of the bugs don't become observable right because once you're on the program you get a crash many many time around the program you get different interleavings and the program doesn't crash right if you have multi-threaded application that's very common that's why it's hard to replicate bugs that you have in multi-threaded programs because of the second one today the first one is hopefully a given today okay so how do you solve the second problem is actually tougher people have proposed deterministic replay mechanisms for example if you actually have a given interleaving you record that interleaving during an execution and if you're debugging the program with the same input next time you replay deterministically with the same interleaving and good debuggers would provide that support that's a lot of overhead to provide that support because you need to do record the interleaving that you see right or um force a given interleaving during an execution deterministic order but that's also very tough because now how do you start with that deterministic order I told you actually start with defining that so you can actually punt to the programming language and programming language says it should be executed in this order but that could lead to a lot of inefficiency in execution because maybe the dynamic order should be much more efficient right you should not obey the order that's specified by the programming language or the compiler okay we're not going to talk about this as much but you can think about it so there a bunch of issues with sequential it's really a nice abstraction for programming that's why the lamport's 1979 paper is actually a beautiful paper that's why you're reading it but there are two issues one issue is just too conservative ordering requirements and people are try to attack this over the course of decades which means that because it's too concerted limits the aggressiveness of performance enhancement techniques so the first question I'll ask is the total global order requirement too strong so every processor sees the same load store order every time and the answer is anybody depends that's a good yeah actually it depends it depends on who where you're applying this order that's right yes but if it's actually applied across all of the loads and stores in a program it's actually too strong basically do we need a global order across all operations and all processors how about a global order only across all stores for example why do we care about loads in fact it turns out this is a much simpler way of designing and this is closer to x86 is model where there's a model called total store ordering model you get the total store order across the process but not load order you need to make it work of course in the presence of Lord reordering inside the processor but we're not going to go into that basically yeah unique store order memory model spark you status also or how about I'm forcing that doesn't look right to it how about am forcing about I'm forcing a global order only at the synchronization boundaries okay I can't see this okay that's better how about doing it only at the synchronization boundaries because does it really matter when the processes are not communicating it doesn't matter right and that actually makes sense because if you're touching some private data who cares who else is observing that so that's a big realization also over here why not enforce order only at the boundaries of synchronization whenever you're touching shared locks share data and this leads to relaxed memory models they're called relaxed memory models because they're relaxing this global total global order to only a local order if you will and this is actually for example acquire release consistency model as an example of this one of the papers crush garrulous paper introduces that but I'm not going to go into that except for one slide maybe okay so performance enhancement techniques that could make they're actually some performs the enhanced techniques that make sequential consistency implementation difficult so how do you do auto word execution in the presence of this for example for example loads happen out of order with respect to each other and with respect to independent stores if this happens in a processor this makes it difficult for all processors to see the same global order of all memory operations because you're actually sending the loads and stores in a very different order so think about this I mean we're not going to go into the details of this there are solutions to it but solutions actually boil down to reordering things potentially caching a memory location is now present in multiple places and this prevents the effect of a store to be seen by other processors right you've cashed this location and you're storing to it that's not even going to some other bus for example if you go back to this abstraction abstraction is not how things are implemented but this is a really good way of thinking about it you catch this location you're operating on it the store is not even visible over here and that's one of the benefits of caching right you don't expose your loads and stores to somebody else you say bandwidth now do you have to expose everything over here so you're back to square one maybe not square one you can exploit locality but then bandwidth is increasing basically this makes it difficult for all processors to see the same global order of all memory operations unless you expose everything to the memory or being that abstraction but then that that actually gets rid of allows these optimization so there is clearly a tension between these two optimizations and memory seeing all of the operations from all processors and ensuring a consistent order for all processors but the existing systems actually that's why sequential consistency is hard to implement because at some point you will need to ensure the same order that's why people went to these different models over here maybe we get rid of the loads we have a total store or maybe we don't do it for all operations we do it just for the synchronization operations and we can keep the benefits of order or the execution and caching as much as possible so a weaker memory consistency is essentially what I just said don't do it for all the operations but realization is that the ordering of operations is important only when the order affects the operations only shared data basically when this processor is syncing needs to synchronize to execute a program region so weak consistency says programmer specifies the regions in which memory operations do not need to be ordered or vice versa or the compiler does this if the programmer is programming nicely with the libraries then the compiler can figure this out right if the programmer is not programming nicely it's doing it's his or her own synchronization then it's his or her job to do this right that's why I said programmer it's some programmer it's either the programmer who's doing the synchronization by themselves or the library programmer who needs to get this right for everybody who's using the right library so how do you actually delineate those regions this is the reason for the memory fence instructions that we have in a lot of the ISAs today exceeded all of the ISAs have some sort of memory fence or memory barrier basically when you get to a memory barrier all memory are patients before sense must complete before the fence is executed and they become miserable and all memory operations are the fence must waste for defense to complete so if we insert the fence after every operation you ensure that that operation becomes visible to everyone else that's one way of doing it that's the simplest way people add it to the ISAs today to support weaker memory consistency models but there could be other ways actually and France is complete and program order of course and all synchronization operations act like a fence or you insert explicit fences after each operation so that's one way of implementing weak consistency clearly this puts back some burden on the programmer now write sequential consistency is nice because the there is no burden on the programmer other than getting the synchronization itself correct but the burden is on the hardware designer and hardware designer now has a problem making sure that you get high performance and these are very widely known techniques they go against sequential consistency but now maybe you can implement some of those techniques easily but now you pump back a little bit on the programmer and the the paper by Corus garish or law that talks about weak consistency models beautifully outlines this actually they design a compiler to actually insert these fence operations I believe they actually inserted by hand at that point in time ok any questions yes so that's true actually I see it it's it's it's as as long as the processors see the same order that's fine yes may be complete is too strong over here as long as the yeah they don't underneath you guarantee that it completes at some point of course but you may report it to have completed as long as you know that that guarantee is done so I can actually still play games underneath but you need to be very careful of course okay okay so let's talk about trade-offs over here weaker consistency compared to sequential consistency clearly there is no need to guarantee array strict order of memory operations anymore with weaker consistency models so that's nice this enables a hardware implementation of performance enhancement techniques to be simpler I'm not saying it's possible because it's really possible with sequential consistency also but with overheads and this can be higher performance and stricter ordering because you do this ordering only when only in these critical sections if you will the disadvantage is there is more burden on the programmer or software so you need to get defenses or these synchronization points correct so if you miss one then you're back to the problem that we've discussed right two processors can be in the same critical section again and this is another example of the programmer micro architect trade-off that we've been covering in this course okay there's an example question which I'm not going to solve but I'm going to show you what an example question might be looking like this was a question from a past exam as you can see spring 2013 you can find the solutions also and this will be on a homework assignment so for example one question related to this could be and this also gives you an idea of what I expect you to know for example you should know sequential consistency that's I don't consider that memorization because that's a concept that's really fundamental I'm not going to define sequential consistency for example in exam but I can ask two threads are concurrently running on a dual-core processor that implements a sequentially consistent memory model assume that the value at address 1,000 is initialized to zero and trade a is executing this thread B is executing this stores and loads a bunch of them list all possible values that can be stored in r3 after both threads have finished executing that's pretty simple actually right so if you go through this relatively used to do and then after both threads have finished executing you find that the values of r1 through our for our this how many different instruction interleavings of the two threads produce this result now you have to do a some reverse engineering to figure this out not that hard though what is the number of total number of all possible infrastructure interleavings you need to think a little bit over here this is essentially what we did but you need to do it with more number of things over here and I'm not really interested in number crunching really you can just write in the open form and on a non sequentially consistent processor is the total number of all possible instruction into your leavings less than equal to or greater than your answer to Question C I mean you think about non sequentially consistent processors any way you can this is going to be a homework question so you'll have fun thinking about this has anybody sold it now that's okay yeah this requires some thinking okay any questions so I mean there could be more that could be talked about over here actually one of the things that simplifies programmers life is an actual concept that's what you discussed earlier transactional memory or why are we dealing with all this why doesn't the programmer provide these critical sections in terms of transactions the programmer says this part of the code is transactional this is the beginning of the transaction that's the end of the transaction and that's all I'm going to provide somebody deal with the synchronization for me that's one way of actually making programmers life easier but how do you do that transaction internally so transaction essentially is either you execute all of the transactional parts or none of it right atomic as we earlier that's that's one of the reasons I showed that earlier to programming persistent memory actually has similarities to programming synchronization there are some differences which we're not going to go into but you can have a transactional programming model for both of them and somebody needs to provide that illusion of atomicity to the programmer and that comes after these consistency coherence models but we're not going to talk about that but keep that in mind no questions otherwise I'm going to jump into caching and consistency here sorry yeah okay let me actually start this a little bit and then we can take a break so caching not only complicates ordering of all our patients as we've seen caching prevents some of the operations to be seen by other by the memory right as a result it complicates ordering of all our patients a memory allocation can be present in multiple caches also and this prevents the effect of a store or load to be seen by other processors this makes it difficult for all processors to see the same global order of all memory operations so that part we've seen but it also complicates ordering of operations on a single memory location and we're going to concern ourselves with the single memory location for the rest of this lecture a single memory location can be present in multiple caches and this makes it difficult for processors that have cached the same location to have the correct value of that location this is different from this global ordering it's really about the updates that you have for this particular location you may get this correct but the school ordering incorrect and vice versa ok so we're going to talk about cache coherence but I think before we move into cache coherence we should probably take a six minute break okay maybe it's time to restart not sure if you're gonna be able to finish cache coherence that's such a such an extensive topic but let's see how much dent we can make hopefully this is still fun you're all alert yeah I see a lot of alert faces that's good okay we're going to talk about another fascinating topic there's been a lot of research done in all of the areas that we've been talking about but there's all I think there's a big need for new fresh ideas so Outsiders who are coming in can provide the fresh ideas I think you don't want to be doing the same old same old okay some readings well actually this one is perhaps more required one over here but if you're interested in brushing up on or learning more you can read these these are the two seminal fate papers I would say maybe this third one actually the three of these this talks about the directory-based coherence which was developed earlier than the meze core has protocols which are really orthogonal to each other actually but this is assuming a bus based snoopy coherence protocol and this another bus based snoopy client's protocol and there a bunch of other coherence papers over here that are interesting okay basically we're going to deal with the shared memory model many parallel programs communicate through shared memory processor 0 writes to an address followed by processor 1 reading and they communicate with each other somehow and each should receive the value written by last value written by anyone and that requires synchronization clearly synchronization what does last written mean right what if my memory location a is cached at either end you should be printing the correct value right if this is updating its cache you should not be printing the value that's not updated over here and we've seen this before as I said if multiple processors cache the same block how do they ensure they all see a consistent state should crash we should change that to coherent States because we've already seen consistency is abused or used for something else as we've seen write memory consistency is really about all global ordained or memory ordering for all locations coherence is about clients of a single location as we've looked at before if this processor both processors load X into their caches when one processor writes to X and this processor again loads X into a register it should not get the stale value right it should get the new value and we've discussed whose responsibility should it be I'm not going to go over this in detail can the programmer ensure coherence if guys are invisible to software it's possible but it comes with a lot of overhead and we've discussed some of these instructions what if the I say provides a flash local flash global flash cache instruction dot dot there's actually another way you could actually punt to the operating system and you can say whenever I'm modifying a location I'm going to protect this page right and no other processor process can touch that location or no other thread can touch that location that's very high overhead right whenever some other processor needs that location now they need to get the access permissions and a lot of overhead comes into play that way you can actually ensure coherence at the software level but at what cost is a question so as we discussed Hardware coherence simplifies software jobs one idea is to invalidate all other copies of block a when the when a processor writes to it right and we've seen this simple coherent scheme I didn't called VI at that time but I call this protocol VI valid invalid coherence protocol and it's makes some assumptions you have a right through nor write allocate cache and whenever you're right a location you send a bus write signal which means that whenever a processor receives the bus write signal for that cache block it goes to the invalid state right and you stay in the invalid state because you're right through cache and you don't allocate on a right as you can see if you earn the invalid state you stay in the amount state but if you're in the valid state and if you read from that location that's fine but if you write to that location you send an invalidate request bus right request on the bus such that another processor that sees the bus right to that cache block it's go from reality invalid write of course if a processor gets this state machine is not complete as you can see if a processor gets oh no it's not complete I think oh yeah it is complete if a processor gets bus ride signal over here it's it stays in the invalid state no it's not complete it's not specified well anyway basically if you get a bus ride signal anyway in the invalid state you stay in the Amell state clearly right because you don't have the block in your cache but basically this is a Snoopy cache coherence protocol caches snoop or observe each other's write and read operations if a processor writes to block all others invalidate the block and this is one specification of this protocol so there are actions of the local processor on the cache block processor read processor writes and they trigger actions that are broadcast on the bus for the blockbuster read and bus right so we're going to see more complicated protocols soon so let's talk about non solutions to the cache coherence first because these non solutions exists basically first is now hardware-based coherence keeping caches coherence is software's responsibility this makes micro architects life easier clearly but makes average programmers life much harder and again processor that did not provide cache coherence didn't fare well in the market IBM Cell processor is one example as we've discussed even though I did a lot of innovative ideas it was a powerful processor so you need to Mary about hardware cache is to maintain program correctness not a good idea and there's also overhead in ensuring correct coherence in software for example what I discussed earlier you protect the page and you have page based software coherence that is a lot of overheads you chap into the operating system whenever you try to modify a page the other non non solution is all caches are shared between all processors it's means that the data is not replicated so there's no coherence problem to begin with clearly this gets rid of the coherence problem that's a non solution because it's not a problem to begin with so shared cache becomes a bandwidth bottleneck in this case or so the latency bottleneck and it's very hard to design a scalable system with low latency cache access this way right you want to have many many million processors in the system how do you share all the caches that's not going to work okay so basically if you want to maintain clearance you need to guarantee that all processors see a consistent value ie consistent updates for the same memory location rights to location a by processor one should be seen by a p1 and all rights they should appear in some order so it requires two things it requires write propagation you need to guarantee that updates will propagate and you need to serialize the rights to a given location you need to provide a consistent order seen by all process for the same memory location and all clearance protocols need to guarantee this and for this you need a global point of serialization for the store ordering these are the ordering for the updates basically but this is again for a given location not for all locations all updates this is just for a given location and you need to serialize those so let's see different clearance protocols how they do it so these guys your hardware cache clearances a processor or cache I'm going to use these interchangeably because we're really talking about a private cache over here it broadcasts its rights and updates to a memory location to all other processors that's a broadcast based protocol another cache that has the location either updates or invalidates its copy so it can certainly send the data along with the address if you want other processors to update or you can have an invalidation based protocol that just sends the address saying I'm going to write to this location invalidate all of the other copies in the system so clearly the first trade off is what I just discussed write the update or the invalidate whenever you write to a location do other processors update their values or do they invalidate their locally cached values so the first one is an update protocol you push an update to all copies in the system and the second one is invalidation and sure there is only one copy local and update it so let's look at both so when I read if local copies invalid you put out the requests if another node has a copy it returns the copy otherwise the memory does and on our right you read the block into the cache as before if you are an update protocol you write to the block and simultaneously broadcast which data and address to all of the other notes and the ones that are that have the block cashed in their caches they take the value and place it into their caches yeah that's what I just said if the block is present other nodes update the data and their caches now it helps if you have all the process connected to a single shared interconnect right bus for example invalidate protocol you write to the block and simultaneously broadcast invalidation of the address only the address to all of the other processors and the ones other nodes that have the block cashed in their caches invalidate the block this way only a single processor in the system has ensured that it has the block now it can write to it yes yes certainly I mean you need to have some sort of interconnect between them to be able to do this there's no other way well you you need to have some data path anyways right so the question is do you broadcast the data to everyone or not so the past should already exist somewhere because you need to update the data out to memory and all those needs all those caches need to get the data from somewhere yeah there are there a lot of issues we're gonna cover it up we're gonna succumb er some of them certainly so if you have a simultaneous update sign if you have a single shared medium you have a serialization point that was one of the requirements right you need to serialize the updates to the same block well if you have simultaneous updates to two different blocks yes now how do you do that in parallel now that's limited by your interconnect that's why the next lecture is going to be on interconnects because there is a very tight coupling between how good your interconnect is and how much performance you can get from a multiprocessor so you're touching very good issues over here okay so which one do you want update where's the MLD clear there are some trade-offs over here right and it depends on the program behavior where it depends a good answer right frequency and sharing behavior are critical for example if if you have a share set that's constant and updates are very infrequent if you have an update based protocol you write to the location and you also send it to all of the other shares and if all of them are going to read that location for example if you have producer-consumer communication between them you may want an update protocol that automatically updates all of the shares right and that's that you broadcast to everyone so this basically gets rid of the invalidations so you're writing for this location and another processor actually go is going to read from that location later on if you actually when you write it if you invalidate that location this processor needs to send a request to get it afterwards that takes time but when you write to it if your updates the cache of the other processor if the quarians protocol automatically does it when this processor needs to read it it already has it since cache so you reduce the access latency for other processors depending on the sharing patterns but if you actually update the data many many times without any interfering reads by other course then all of the updates that you made for the other caches are useless so it really depends on read and write patterns that you have in the program this processor a may be doing 1,000 writes to the same cache block do you really want to do all of the updates to all of the other caches in the system maybe you just do one write invalidate everyone else and then do 999 local writes to that block and no one gets informed about those right that's a place where you really want invalidate based protocol ok so one of the other issues with an update protocol is it's essentially a write through cache at that point right and your bus can become the bottleneck okay so invalidate based protocol after you broadcast an invalidation to all of the other course the core actually can have exclusive access rights to that block because assuming the protocol is correctly implemented you know that this core is the only one that has the block in its cache everybody else is invalidated which means that it doesn't need to inform anyone about what it does to that block that's essentially what I said with the 999 updates in the previous example only chords that keep reading after each write retain a copy this could be good for caching also actually because you've invalidated the copies so if a core is not going to read the block it doesn't get the update which makes us but if contention is high this invalidation can lead to a lot of ping-ponging basically a lot of invalidation and we require write for example if you have a really contended lock for example if you may actually be invalidating from all of the other processors but if you actually updated them after you wrote to the lock the other processor would have gotten the latest value of the lock and it would quickly check what the values and it could actually enter the critical section right away right as opposed to doing another reads and when it does the writes it says another invalidate dot dot so ping-pong ax is actually at map ins at multiple levels but this is one example of it so this may actually cause a lot of in validations if you have producer-consumer type of perils and you're going to write to one location and somebody else is going to eat from it of course you can try to optimize this even more right as we've discussed earlier people have tried to whenever you do an update try share data or shared locks they've tried to predict which other processor should get that update right you can only update the processor that needs that data you could push that over there and there have been a lot of optimizations but this is really two ends of the continuum over here okay so we'll talk about two cache coherence methods and not necessarily protocols but these are methods how do you ensure that proper caches are updated when is it Snoopy bus as we've discussed you have a single shared bus across all processors and that provides a single point of serialization for all memory requests not all not a single location but locations over here so this is actually good for consistency as well actually if you want to do memory consistence but we're going to ignore consistency here and processors observe other processors actions on this bus for example if processor one makes a read exclusive request I'm gonna call the write request as read exclusive I'm gonna read this block such that it's the exclusive copy that I will have and I can do whatever to it after that if processor one makes that request for a block a on the bus process zero sees and invalidates assume own copy of a or updates so the protocol is really different from the method method can be Snoopy bus and then you can have a valid invalid protocol or some other protocol that we're going to discuss or an update based protocol or invalidate based protocol dot dot the directory-based protocol is inherently a bit more scalable but you really have a single point of serialization per block distributed among nodes so you have a directory let's assume that it's not distributed it's assumed that you have a single shared place that's keeps track of all the blocks in the system and a processor when it wants to do something to a block it sends a request to the directory directory please give me this block I want to eat it and the directory keeps track of which caches have each block and it coordinates invalidation and updates if you think about it this middleman that's distributes two blocks to the different course whereas here Snoopy bus implicitly serve as a serialization point but if you have this directory let's assume you have a single shared directory at a single place all of the requests go through the directory so you have a point of synchronization but now you can actually scale this you can actually have a partitioned directory across a thousand processors right memory requests that are going to address space portion 0 through n goes to this directory memory to get the permissions core has permissions memory requests going to addresses end through to 2n n plus 1 to 2 n go to this node dot dot basic we can partition the directory whereas this one how do you do that well this is assuming a single point of realizations assuming a bus so inherently a directory-based protocol is more scalable so we're going to look at the trade-offs between them toward the end after we cover examples so for example in a directory-based protocol processor one asks the directory for an exclusive copy as opposed to sending a read exclusive request on the bus that everybody else sees the directory asks processor 0 which has that block a to invalidate its copy because it's going to grant that copy to processor 1 the directory waits for an acknowledgment ensuring that processor 0 validated the copy and then gives the permission to a processor 1 make sense right ok so let's look at directory based cache coherence in a bit more detail it's a very simple idea basically you have let's assume that you have a logically central directory that keeps track of where the copies of each cache block reside and caches or processors consult this directory to ensure coherence one example mechanism people have optimized this a lot also but assume that you have P processors for each cache block in memory you can store P plus 1 bits in the directory one bit for each cache indicating whether the block is in that particular processors cache and one other bits saying exclusive bit indicates that a cache has the only copy of the block and can update it without notifying others so if the exclusive bit the plus one over here is set only one of the pbut s-- or exactly one of the P but should be set right exclusive bit may not be set but again exactly one of the peoples may be set that means that the processor has access but not exclusive access that block ok so on every each you set the cache as bits and arrange the supply of data somebody needs to supply the data to the cache and we're going to see methods of doing that on your writes the directory invalidates all the caches that have the block and we set their bits and we have an exclusive bit associated with each block in the cache so you don't you should she has people as one bits that's the plus one bit but the cash itself also needs to know that it has exclusive access to that data right it's not a stream exclusive access somebody needs to grant that exclusive access but once it has the exclusive access needs to mark the blocks especially saying that oh I have exclusive access to this block I can do whatever I want without consulting the directory until the directory asks I want your block so you don't have exclusive access anymore okay so directory is really the coordinator basically if the cache has exclusive access to that block it can update the exclusive that block silently without informing the directory and this is actually very important because of the sharing patterns you can actually update the block many many times without anyone requesting the block right okay listen again my pictorial example over here P plus 1 P equals 4 in this case this is an example dear cubase scheme basically for a given block or block a for example you have four bits one bit for each processor and you have an exclusive bit and this sits at the memory controller let's assume that it's in the memory controller in a centralized place in this case you know that no cash has block ace right let's assume p1 takes a read miss it sends a read request to the directory for block a and the directory takes the data from the memory controller sends it to the p1 and marks key ones bit as one saying p1 has the Block no one else has it p1 doesn't have it in exclusive state so if he one wants to write to that block it requires another request to the directory saying I want to write to this block so a p3 let's assume that after this the next action is p3 processor 3 takes a read miss to the same block the directory consults the bit vector it says oh I have the copy of the Block in the memory processor 1 also has the copy but they're consistent they're coherent basically so I can take my copy and sent it to processor 3 and Marc prostrate fees bit to one now processor 3 and processor 1 both have the block and it's the same as the block in the memory because no one has exclusive access now I just described one potential implementation right the directory could say oh I'm not going to take the copy from memory it takes too long to access memory I'm going to tell processor want to send processor te this block because only now I know that processor one is really close to processor 3 and that cache to cache communication is very quick and I'm going to coordinate that communication so that's certainly possible that's another implementation of how do you communicate the data ok let's keep going oh ok so the next action is processor 2 takes the right miss right miss meaning basically processor 2 wants to write to this block block a the it sends a request to the directory saying I want to write to this block that's essentially really exclusive request I want to read this block exclusively so that I can write to it you could call the write request also but it's usually called the read exclusive request you want to read the block and exclusive manners I said you can write to it so directory looks at the state or it sees processor 1 and processor 3 have the block so what the directory does first is it invalidates processor 1 and processor these caches it sends invalidation signals to both of them it waits for acknowledgement once it gets technology it knows that no other processor cache has the block all of them become really zeros but it may transition of course quickly and then basically it says processor two's bit to one and it sets the exclusive bit to one saying processor two is going to write to this block and sends a grant request saying processor two now you can write to this block and by the way here's the block right so it sends the block as well as the grant to processor two now the processor two can update the block without notifying any other processor or the directory because it has the block exclusively set inside its cache it marks the block as a closer and can keep writing to that block and reading from that block it doesn't need to inform anyone and processor two I guess I already said this you have a private it's also called the private bits or exclusive bit for cache block now let's say processor three wants to write to this block same block and the block is in this state you have the same issue basically in this case it's a bit different actually because here the Prada the directory had the copy of the data so it could supply it to processor two because it reactor he had the up to date copy here in this case when processor three wants to write to this block and the block is in this state the directory the memory controller doesn't have that up-to-date copy so what the directory needs to first do is to get the update copy well their optimization points over here but basically the directory sends a message to processor two saying somebody else wants to write that block so invalidate that block in your cache and give me the block by the way but there's an optimization here maybe it doesn't really need the block right maybe it can tell the the processor invalidates your block but I by the way send the block to this other processor who actually wants the block that's an optimization that you should really carefully do because there are acknowledgments maybe you get you get into an inconsistent state guide okay but let's assume that you get the block also for whatever reason and then once it gets the block and once it ensures that the block is invalidated in processors twos cache then it's it grants processor three exclusive access because processor three wanted an exclusive access and its supplies the block to processor three okay now this is update now processor two takes a read miss at this point what the directory does is gets that read request and then it's you need to think about what it's not needs to be done I didn't write it over here but at this point you could do many things one thing you could potentially do is you could say processor you could send a message to processor three you know that processor three is an exclusive mode send a message to the processor three saying that oh don't be in exclusive mode anymore you can keep the data don't invalidate it somebody's going to read it give me the data you get the data and then the directory sends the data to processor two who wants to read it so it sets the bit but it's not an exclusive in any of the caches so it's not shared across different processors so the directory is really the coordinator and a very simple protocol as you've seen over here but even this very simple protocol enables or requires perhaps many many optimizations okay and this is probably a good place to stop any questions yes yeah yeah so those are so you're thinking about a Snoopy protocol right bus based protocol not a directory base protocol because if you have a directory base protocol that becomes a bit simpler because you have a single point over here but in a bus based protocol yes you need to take care of all those race conditions it's essentially a race condition and you need to guarantee coherence in the presence of these actions while you're propagating your information on the bus somebody else might be doing something and you need to ensure that those things are consistent well that's that's that's the difficulty of it essentially that's the difficulty of designing a cache coherence protocol no no these are these are fully implemented in existing systems but people need you do need to handle in the hardware design all of those potential erase conditions and that's why it's a difficult part of the design yes well yeah I mean you can you can try to optimize this protocol in many ways sure try to customize to the access pattern that you may potentially see but that complicates the protocol of course right sure exactly I certainly yeah so those are all optimizations right you could potentially predict these things again so if you get it exclusive you don't need to inform the directory right I don't know what you are talking about over there but I think the example was if two processors are actually keep on writing right and the directory is somehow getting right requests exactly but but then if it keeps getting a really exclusive request or right requests from someone else well I think what he was what he is alluding to is what if this keeps happening all the time like I'm in a ping ponging manner maybe you you waits for giving the block to this other processor you give this processor for some time I don't know for a minute let's say and then you give it to the other processor for another minute so that's a fairness issue basically depending on the access pattern as opposed to a switching between the processors all the time you give some time yeah well again that's an implementation decision also right like how do you decide so that these are all implementation this is in the protocol but in the basic vanilla protocol that I said the when the when the directory says oh I want this block the CPU gives up the exclusive access right but you can have many many issues related to this certainly okay well I think it there are issues here there is a performance issue and there's a correctness issue right first of all getting it correct is the first step and what I described here is correct but performance now if you try to optimize for performance sure there are many many issues and we have not started that that's that's part of what we're going to discuss I guess we'll start with this tomorrow and we'll finish it and then we'll see what we can do okay thanks "
Ol8D69VKX2k,27,"COA: Computer Organization & Architecture (Introduction)
Topics discussed:
1. Example from MARVEL to understand COA.
2. Basic overview of Computer Architecture & Organization.
3. Typical Structure of a Computer.
4. Course Outline.
5. Prerequisite & Target Audience.

Follow Neso Academy on Instagram: @nesoacademy (https://bit.ly/2XP63OE)

Contribute: http://www.nesoacademy.org/donate

Memberships: https://bit.ly/2U7YSPI

Books: http://www.nesoacademy.org/recommended-books

Website â–º http://www.nesoacademy.org/
Forum â–º https://forum.nesoacademy.org/
Facebook â–º https://goo.gl/Nt0PmB
Twitter      â–º https://twitter.com/nesoacademy

Music:
Axol x Alex Skrindo - You [NCS Release]

#COAByNeso #ComputerOrganizationAndArchitecture #COA",2021-04-18T13:30:03Z,Introduction to Computer Organization and Architecture (COA),https://i.ytimg.com/vi/Ol8D69VKX2k/hqdefault.jpg,Neso Academy,PT7M1S,false,11761,394,4,0,66,hello everyone welcome to the very first lecture of computer organization and architecture it is also popularly known as coa so in this course we are going to learn about computer organization and architecture that is coa in a very detailed manner and in this particular lecture we will learn about the basics of the same all right then let's get to learning now we all know about the great tony stark in the mcu when he was attacked and abducted by a group of terrorists in order to escape from captivity he using his great knowledge designed a suit of armor organizing all the required components he finally built one of his greatest inventions the iron man suit so basically determining the objective first mr stark designed the architecture then with the help of proper organization he became iron man similarly when any machine is devised it is built to achieve some goal first we build the structure of it and once that is decided upon thereafter we try to organize our resources to achieve the final product for instance if we had to build a circuit which can add two bits and produce some as well as carry i mean when both the inputs are zeros the sum and the carry will be zeros again with two inputs and having one of the inputs as one there can only be two cases and in both the cases the sum is one and the carry is zero now if both the inputs are ones the result is two but since we don't have the symbol two in binary rather it is represented as one zero so the sum is zero and the carry is 1. anyway we need not worry about the details right now because interesting discussions are awaiting so in order to devise such a machine first we need to determine the structure then organizing the available resources supposedly the universal nand gates in this specific scenario we can build the half adder circuit which is nothing but the machine we intended to build to solve our two-bit edition problem which means architecture and organization our systematic approach of deriving the solution of any problem now let's move on to the technicalities so computer architecture basically deals with the functional behavior of computer systems and it is also all about the design implementation for various parts of computer now coming to computer organization it deals with structural relationship basically the functional units collectively work together in order to execute computer instructions so operational attributes are linked together and contribute to realize the architectural specification so to sum it up the designing is the attribute of computer architecture whereas utilization happens to be the attribute of organization now a generic computer is comprised of various functional units amongst them the processor is the most important one the processor itself is a collection of register section and arithmetic and logic unit also known as alu and a timing and control unit along with these it also has another section called interface we can think of the processor as the brain of the system next we have the memory it stores all the instructions reading ways the processor works accordingly also we can store data inside the memory another functional unit is the input output peripheral now the term peripheral is just a fancy version of device all of these are very essential when computation involving computers are concerned so the programs or the set of instructions are stored into the memory using input devices so that the processor can execute them during execution the required inputs can either be fed into the system using the same input peripherals or stored into the memory beforehand finally the outputs can be generated onto the output devices or else can be stored into the memory for later extraction the intercommunication of all these functional components is carried out with the help of system bus we will get to know about all these in a much more elaborated manner in our due course now coming to the outline of the course that is the syllabus that we are going to follow is as follows first we will begin with the basics of computer architecture also we will learn about the various classifications of it next we will learn about memory interfacing and memory hierarchy now in this particular section we are going to learn how the memory devices are connected to the processor also the way the intercommunication takes place and by the end of this we will have a pretty detailed knowledge of all sorts of memory mapping techniques and a decent idea about the secondary memory storages now comes the big part the computer organization starting from machine instructions addressing modes then the detailed organization of algu the data path and finally different types of control units all of these we will learn in an eventual manner with the help of a constructive illustration next up is io interfacing here we are going to be introduced with the ways io peripherals can be interfaced with the processor also we will learn about various data transmission modes then we will learn about the instruction pipelining which is sort of optimization basically we will see how to increase the efficiency of a single processor and finally a bonus section number systems now many of us may argue that it is a very basic thing and very commonly known yet i insist on learning it because this specific section has specially been designed keeping cos application in mind by the end of it we will have a detailed knowledge so it is my promise to deliver a new perspective for the learners to have therefore the entire course can be subdivided into six major sections all right now to be very honest coa is not really a basic course rather it's an advanced course so having a primitive knowledge of digital logic design would be very helpful in order to acquire a quick grasp over the subject however i promise to keep the lectures as detailed as possible also i would mention the topics from our own digital electronics course in respective lectures whenever it's necessary now coming to the audience whom keeping in mind the course has been designed all the college and university students are going to get benefited due to the detailed explanations of each and every aspect of coa next any aspirant willing to compete in any competitive exams such as gate net n-i-e-l-i-t etc will be able to understand many numerical problems along with the most apt illustration of the theory involved in them finally any computer science admirer who just wants a quick detailed recall about any topic of coa or anyone who wants to get wise in this particular subject can refer to the lectures so that was all for this lecture hope to see you in the next one thank you all for watching 
sUZKatd_g70,27,"Computer Architecture
About this course: In this course, you will learn to design the computer architecture of complex modern microprocessors.

Subscribe at: https://www.coursera.org
https://www.coursera.org/learn/comparch",2017-12-05T14:24:41Z,Computer Architecture - Architecture and Microarchitecture,https://i.ytimg.com/vi/sUZKatd_g70/hqdefault.jpg,intrigano,PT23M38S,false,660,7,1,0,0,ok so now we're going to change topics and start talking about our first technical subject of this course and as an introduction to computer architecture we're going to be talking about what is architecture versus microarchitecture and I wanted to just briefly say that as you take this class the first three lectures or so should be review so if you sing in the class and you're saying oh I've seen all this before don't get up wait to the fourth or fifth lecture and then the content will become new and this is because I want to teach everything from first principles and get everyone up to speed but it's that those first few lectures are going to go very fast so if you're lost in the first three lectures which should be review then that's probably a bad in indicator so we'll start off by talking about architecture versus microarchitecture and I wanted to say briefly what I mean by architecture and I have in the slide here a very large a4 what all sometimes call big a architecture so your Patterson has he calls this instruction set architecture and when I contrast this with micro architecture or paracin hennessy calls organization so big a architecture is an abstraction layer provided to software or instruction set architectures our abstraction layer provided to software which is designed to not change very much and it doesn't say it says how a theoretical fundamental sort of machine execute programs it does not say exactly the size of different structures how fast those things will run the exact implementation issues that falls into organization and one of the things I wanted to emphasize is that computer architecture is all about trade-offs so when I say it's all about trade-offs you can make different design decisions up here in the big a architecture or the instruction set architecture that'll influence the application influence the microarchitecture but also you can make different design decisions down here and make a lot of different trade-offs on how to go about implementing a particular instruction set architecture and largely when you go to look at computer architecture and computer architecture implementation the design space is relatively flat there's sort of an optimum point where you you want to be but the other points around it are many times not horribly horribly bad though there are you know at the at extremes probably horribly bad design decisions but you know a lot of different design points are equally good or close to the optimal and the job of computer architect is to make the very subtle design decisions around how do you move around this point to make it both easier to program lives on for many years it's low power and the sort of other a little bit of aesthetic characteristics mixed together with just making your computer and processor go fast will say and these trade-offs I will reiterate this over and over again in this class that because there's multiple different metrics so for instance speed energy cost and they trade off against each other many times and there is no necessarily optimal point it depends on you know if are you more cost driven or energy driven or speed driven and within that point there's sort of sometimes Pareto optimal curves where all the points are are equally good if you're trying to trade-off these different things for different cost models okay so let's let's talk about what is a instruction set architecture and what is a microarchitecture so a instruction set architecture or big a architecture is trying to provide the programmer some abstract machine model and many times what it really boils down to is it's all the programmer visible state so for instance how does the machine have memory doesn't have registers so that's the that's the programmer visible state it also encompasses the fundamental operation that the computer can run so these are called instructions and it defines the instructions and how they operate so for instance ad ad might be a fundamental instruction or fundamental operation in your instruction set architecture and it says the exact semantics on how to take one word in a register and add it to another word register and where it ends ends up then there's more complicated execution semantics so what are we my execution semantics well if you just say ads take two numbers and add them together and put them in another register that many times does not encompass all of the instruction set architecture you'll have other things going on for instance IO interrupts and you have to define in your instruction set architecture or your big a computer architecture what is these X semantics of an interrupt or a instruction or a piece of data coming in on Io how does that interact with the rest of processor so many times the instruction execution semantics is only half of it and we have to worry about is the the rest of the machine execution semantics Big Eight architecture has to define how the inputs and the outputs work and finally we has to define the data types and the sizes of the fun the fundamental data words that you operate on so for instance do you operate on a bytes at a time four bytes at a time two bytes at a time how big is a byte do you actually have bytes so this gets into sizes and then data types here might mean that you have other types of fundamental data so for instance the most basic one is you have just some bits sitting on on you know in a register in your processor but it could be much more complex so you could have for instance something like floating-point numbers where it's not just a bunch of bits its bits formatted in a particular way and has very specific meaning it's a floating-point number that can range over let's say most of the the real numbers okay so in today's lecture we're going to step through all these different characteristics and requirements of building an instruction set architecture and I wanted to and we'll talk about how it's different than microarchitecture or organization so let's think about some examples of microarchitecture and organization so what microarchitecture and organization is really thinking about here is the trade-offs as you're going to implement a fixed instruction set architecture so for instance something like Intel's x86 is an instruction set architecture and there's many different micro architectures or implementations there is the AMD versions of the chips and then there's the Intel versions of the chips and even inside of let's say the Intel versions of the chips they have their high performance version for the laptop which looks one way or high performance version one for out say a server or or a high-end laptop which looks in one way and there's another chip for tablets Intel's trying to make chips for tablets these days and they have their Atom processors and internally they look very different because they have very different speed energy cost trade-offs but they will all execute the same code and they all implement the same instruction set architecture so let's look at some examples of things that you might trade off in a microarchitecture so you might have different pipeline depth numbers of pipelines so you might have one processor pipeline or you might have six like something like the core i7 today cache sizes how big the chip is the silicon area how what's your peak power execution ordering well does the code run in order or can you execute the code out of order that's right it is possible to take a sequential program and actually execute later portions of the program before earlier portions of the program and that's kind of mind-boggling but it's a way to go about getting parallelism and if you keep your ordering correct things things work out bus widths ALU whiffs you if you ever say a 64-bit machine you can actually go and implement that as a bunch of one bit adders for instance and people have done things like that in the microarchitecture and this allows you to build more expensive or less expensive versions of the same processor so let's talk about the history of why we came up with these two differentiations between architecture and microarchitecture and it came about because software sort of pushed it on us and ended up being a nice abstraction layer so back in the early 50s late 40s you had software that people mostly programmed either in assembly language or machine code language so yeah the right ones and zeroes or you had to write assembly code and sometime in the the mid 50s we start to see libraries show up so these are sort of floating point operations were made easier we had transcendentals as the sine cosine libraries you had some matrix and equation solvers and you started to see some libraries that people could call but people were not necessarily writing code by themselves or any large bodies of code in assembly programming because it was pretty painful and then at some point there was the invention of higher-level languages so a good example this was Fortran came out in 1956 and a lot of things came along with this we had assemblers loaders linkers compilers a bunch of other software to track how your software is being used even and because we started to see these higher-level languages this started to give some portability to programming it wasn't that you had to write your program and have it own map-21 Pro one processor ever and back in the 50s even 60s time frame here machines required experienced operators who could write the programs and you know you you've got these machines and they had to be sold with a lot of software along with them so you had to basically run only the software that was given because it was yet to be a master programmer or someone who worked for the company too even that built the machine to even build a program these machines back in the day and the idea of instruction set architectures and these breaking the microarchitecture from the architecture didn't really exist back then and back in the early 60s IBM had four different product lines and they were all incompatible so you couldn't run code that you ran on one on the other so to give you an example here the IBM 701 was for scientific computing the 1401 was mostly for business computation and I think they even had a second one that was sort of for business but different types of business computation and people sort of bought into a line and then as you as the line matured and developed they had either rewrite their code or they had to stick into one line but IBM had some and some crazy insights here is that they didn't want to have to when they went to the next generation of processor they wouldn't want to propagate these four lines they wanted to try to unify the four lines but one of the problems was these different lines had very different implementations and different cost points so the thing you were building for scientific computing wasn't necessarily the thing you want to build for business computing and the one that you built for business computing let's say didn't you wanted to not have it have very good floating-point performance so how do how do they go about solving this and their solution was they came up with something called the IBM 360 and the IBM 360 is probably the first true instruction set architecture that was implemented to be an instruction set architecture and the idea here is they wanted to unify all these products into one platform but then implement different versions that were specialized for the different market niches so they could build they could unify a lot of their software systems unify a lot of what they built but still build different versions so let's let's take a look at the IBM 360 instruction set architecture and then talk about different micro architectures that have been built of the IBM 360 so the IBM 360 is a general purpose register machine and we'll talk more about that later in this lecture but to give you an idea this is what the programmer saw or what the software system saw this isn't what was actually built in the hardware because that would be a microarchitecture constraint but the processor state had 16 general-purpose 32-bit registers it had four floating-point registers had control flags as if you will had a condition codes and control flags and it was a 24-bit address machine at the time that was huge so to the 24 was a very large number nowadays it's not so large and they've since expanded that on the IBM 360 successors but they thought it was good for many many years and it was good for many many years and they defined a bunch of different data formats so there was a bit bytes 16-bit half words 32-bit words 64-bit double words and these were the fundamental data types that you could work on and you can name these different from old data types and it was actually the IBM 360 that came up with this idea that bytes should be 8 bits long and that's lived on on for today because before that we had lots of different choices there was binary-coded decimal systems where the you actually would encode a number between 0 and 9 and then you had the each digits and this is sometimes good for sort of spreadsheet calculations of business calculations we want to be very precise on your rounding to the penny and sometimes bit based things don't actually round appropriately oh they'll do the you lose pennies off the end and so you had these binary coded decimal systems and well the IBM 360 they they unified it all and said well no we're gonna throw out certain things and make make choices now they of course because it's the IBM 360 and they did have business applications they still support a binary coded decimal in a certain way and let's look at the microarchitecture implementations of this first instruction set architecture so at in this is in the same time frame the same generation here there is the model 30 in the model 70 and this was very very different performance characteristics so if we we look at the machine let's start off by looking up a storage the the low-end model here had between 8 and 64 kilobytes and the high-end model had between 256 and 512 kilobytes so very very different sizes and this is what I'm trying to get across here is that micro architecture can actually change quite a bit even though the architecture supports 64-bit ads and additions you can actually implement different size data paths so in the low-end machine they had an 8-bit data path and for one to do a 64-bit operation it had to do eight sapin operations to make up a 64-bit operation and probably actually even had to do more than that to handle all that carries correctly versus the high-end implementation had a full adder there and it can actually do a 64-bit ad by itself without having to do lots of micro sequence operations and oh yes with minor modifications it lives on today so this was designed in the 60s and even today we still have system/360 derivative machines and a piece of code ran or you wrote back in 1965 will still run on these machine today which is pretty pretty amazing natively so how does this survive on today so here's actually the IBM 360 47 years later as in the Z 11 microprocessor so the IBM 360 has since been renamed to the IBM 370 and then it has been renamed to the IBM 370 e^x which was in the 80s there was never a IBM 380 strangely enough and then later on they just changed the name to the Z series so have a cooler modeling model numbers here so we had the IBM z series processors and this lives on today so going back to that 8-bit processor which had a 1 microsecond control store read which is forever we now have the Z 11 which is running at 5.2 gigahertz has 1.4 billion transistors they they have updated the addressing so it's no longer 24-bit addressing but it still supports the original 360 addressing has four cores out of order issue out of order memory system big caches on on chip 24 megabytes of your l3 cache and you can even put multiple these together to build a multiprocessor system out of lots and lots of multi course and what I'm trying to get across here is that if you go forward over time and you build your instruction set architecture correct it can live on and you have many different microarchitecture implementations and still leverage the same software and a few few more examples just to reinforce this a little bit more let's take a look at an example of something where you have the same architecture but different micro architectures so here we have the AMD phenom x4 and here we have the Adam Intel Atom processor the first Intel Atom processor and what you'll notice actually is that they have the exact same instruction set architecture they both run x86 code and that is science and this is just to point out here these are the same time frames so this is modern modern roughly modern-day processors this one has four cores 125 watts here we have single core two watts so there's design trade-offs so you want to build different processors in the same design a technology will say but with very different cost power performance trade-offs this one can decode three instructions this going to decode two instructions so it's a different microarchitecture difference this one has 64 kilobyte cache l1 this one is a 32 kilobytes l1 I cache very different cache sizes even though they're employing the same architecture or big a architecture strangely enough they have the same l2 size you know things happen this one's out of order versus in order and clock speeds are very different and I want to contrast this with different architecture or different big a architecture and different microarchitecture so if we think about some different examples of instruction set architectures there's x86 there's PowerPC there's IBM 360 there's alpha there's arm you've probably heard all these different names and these are different instruction set architectures so you can't run the same software on those two different instruction set architectures so here we have an example of two different instruction set architectures with two different microarchitectures so we have the phenom x4 here versus the IBM power 7 and we already talked about the x4 here but the power 7 has a our instruction set which is different than the x86 instruction set so you can't run one piece of code that's compiled for this over here and vice versa and the microarchitecture is are different so here we have eight core 200 watts can decode six instructions per cycle while this is a pretty beefy processor it's also out of order and has the same clock frequency something that I that can also happen is you can end up with architectures where you have different instruction set architecture a different big a architecture but almost the same microarchitecture and this this does this does happen so you end up with let's say two processors that are both three wide issue same calf sizes but let's say one of them implements PowerPC and the other one implements x86 and things things like that do happen that's more of a coincidence but I'm trying to get across the idea that many times the that the microarchitecture could be the same and those are more trade-off considerations versus the instruction set architecture which is more of a software programming design constraint you 
Vg5onWKah4o,22,"#GTU #MU #COA #Computerorganization #computer #architecture
In this tutorial there will be discussion on how CPU performs input output operation using interrupt. There will be discussion on how CPU stores current execution status.",2020-04-19T09:32:33Z,Ch - 5 | Tutorial - 12 | Interrupt Cycle | Input Output Organization | CO / COA,https://i.ytimg.com/vi/Vg5onWKah4o/hqdefault.jpg,Technology Gardening,PT18M4S,false,489,4,1,0,0,"hello student so in this tutorial we will try to understand what is interrupt cycle and what is need of interrupt so in previous video we have seen that what is input-output cycle means how input and how output operation will be performed into the computer while we will discuss while we were discussing that what is input and output cycle at that time we have learned that clock frequency of CPU is very high means number of instructions executed by CPU in a second is very high while the clock frequency of input device is very low so there is a huge clock frequency gap between input device and CPU input output device and CPU so if we go with the conventional way which we have discussed in previous video then what will happen CPU will waste its most of time in checking the input flag and output flag so let me just give you an example so let us consider a value let's say CPU take 1 microsecond to execute one instruction next so in one second we can have 10 raise to 6 instruction and we are hearing an input device which has maximum data transfer rate let's say 10 character per second clear with this so what we can say we will have 100,000 microsecond for one character okay so what will happen in this case so if we will execute two instruction to check the plug bit of input device and output device so to check whether the device are ready for data transfer or data transfer or not okay so if we calculate the value then CPU will see we will check 15,000 time CPU will take 15,000 or 50,000 times whether the input is available or not okay so now this is a significant amount of time will be wasted in checking the input and output flick so how computer will deal with this issue or how we can improve the utilization time of computer so to efficiently or to effectively utilize the CPU we will go with the interrupt cycle now as you can see in the figure that we are having interrupt cycle and interrupts I curl is having various component like this is the inch instruction cycle this is the interpreting mechanism means we can check whether the input or output is ready or not and this is you can say the interrupts subroutine so let us start our discussion how it is implemented now as you can see in end of cycle we will have a flag called R this R represent the interrupt okay so this type of execution will be there it will be decided based on the value of R so here R represent the interim so on how normal execution will be there so by default if there is no any input sorry interrupt okay then it will go to the left side which is instruction cycle now instruction cycle here first message is fetch decode and fetch and decode instruction okay so if we discuss about timing signal then T 0 T 1 and T 2 this timing signal will be therefore fetch and decoding the instruction while in T 3 CPU will execute the instruction while CPU is executing the instruction it will also check whether the interrupt is enabled or not so in parallel to execution of this instruction CPU will go with this part now what is this part so for so called CPU will check whether interrupt e is enable or not now what is meaning of interrupt enable so if it is 1 then CPU is giving permission to external device to generate and interrupt interrupt means disturbance so whatever normal execution will be there in CP okay it will be posed temporarily and CPU will handle the input-output requests so whether we want to manage the input-output request or not it will be decided by value of interrupt enable so if interrupt enable is one and then it will be allowed to manage the people be allowed to manage the external disturbance means external requests and if it is zero then whatever current execution is there it won't be stopped or opposed at any cost okay so CPU will continue with the normal execution now let us assume that value of interrupt enable is 1 so we have allowed the interrupt then CPU will check whether the request or the interrupt is generated by input device or not so if it is generated by input device then it will set the value or heart to 1 dear learners please make sure that if this is not a single standalone process in parallel to this process there is a execution of instruction so while CPU is executing the instruction it will also check all this thing so CP will check whether the input flag is set or not let's assume that interrupts is generated because of input device so a value of R will be set to 1 and as soon as value of 4r will be set to 1 it will go with the next cycle but as you can see in the figure that here if the value of R is 1 then it will not go with the instruction cycle or it will not go with the other interrupt request it will go with the interrupt routine let me try to draw a figure for T people so you can understand the table how it will be implemented or how interrupt cycle will be implemented so let's assume this is memory map first location of memory map or memory will be kept empty on next location let us assume that it it stored the subroutine address of a particular interrupt so let's say zero branch zero four zero here we are having some main program is currently executing program program which is executed by CPU so here let's say currently we are executing the instruction which is on memory location 25 and so well your program counter will be 26 so this is our main program or you can say user program set of instruction which is supposed to be executed as a process here we are having the for your program okay program which is which will deal with the input/output device okay means which is used to handle the input-output requests and last instruction in this ayah program is one branch zero yes so how interrupt cycle will be executed now as you can see whenever there will be intra progressed there is a input/output operation request because of interrupt then CPU will halt or it will pose the current execution it will store the current status into the memory location zero so the first location will be used to store the value of program counter so right now we are executing the instruction which is on location 25 so the next instruction will be 26 so 26 will be stored on location 0 clear with this next next is let's say this location specify the subroutine address which will deal with the input/output request ok so what is the instruction 0 branch 0 4 0 so we will branch to location 1 and then we will disable the interrupt so no other device can generate the request or we will not manage that request and we will say it r2 0 now meaning of setting our to 0 that means interrupts is handled so next time after doing this it will again if it will go to the top of this product then it will not go right side but it will go website now then it will start with the instruction which is on location 1 what is our instruction on location 1 0 branch 40 so after executing of this instruction in first phase ok counter or control of our execution will be transferred to location zero four zero then the first instruction will be fetch it will be executed next instruction will be fetched it will be executed next instruction will be fetched it will be executed next instruction will be fetched it will be executed while CPU is managing this interrupt it will never go this side y because value of interrupt enable is zero clear with this so what will happen whenever CPU will go to the last instruction what is last instruction one branch zero okay so this is not direct addressing but indirect addressing so at the execution of this instruction it will go to this rather than 1326 as in zero as an actual address to jump we will use we will treat 26 is an effective it does to jump so from here it will go to location number 26 using indirect addressing mode okay so this is how an interrupt cycle will be managed while it will execute this instruction again value of interrupt enable flag will be set to what clear with this okay so this is how CPU will manage the input/output operation using interrupts okay so this this will be the efficient to avoid because if you will not waste it its time in checking the input flag and output flag now let us continue our discussion we will try to understand then when interrupt requests will be handled so as we can see when we will check for the interrupt except timing signal T 0 T 1 and T 2 okay so p0 - t1 - t2 - and what will be the second and required condition interrupt value of interrupt enable should be 1 excuse me and then what will be the third condition either value of input flag or output flag should be 1 okay so if this condition is true then value of all will be set to 1 we can say this will work as a control function for setting the value of interrupt flag R now let us just try to convert that interrupt execution process into timing signal so whenever interrupts cycle will it will be executed okay it will go this side then at first timing signal RT 0 okay 0 will be stored into address register and program counter will be stored into temporary register so till now we have not discussed the use of program counter but now sorry till till now we have not discussed the use of temporary register but here you can see how temporary register will be helpful in storing the intermediate result then what will happen next clock cycle rt1 we will store the value of program counter on location memory location 0 okay which is already stored into temporary register and we will initialize program counter to 0 then what will happen in next clock cycle the currently program counter is pointing at 0 location actually our first location specify the program our location of input/output subroutine so PC will be incremented PC is equal to PC plus 1 and then interrupt will be disabled so no other device can generate any interrupt io io request R will be set to 0 that means interrupt is managed and sequence counter will be initialized to 0 so after doing this from next instruction onwards whatever instruction will be executed it will belongs to subroutine of IO program this subroutine is same as higher level programming language user-defined function okay so the people you people are writing I use a different function in a high-level programming language like C C++ or Java so dotnet so this is same as subroutine okay so subroutine is nothing but a function for handling the request of a particular hardware device or whatever task you are performing frequently it will be stored in form of subroutine so in this tutorial in this would be in this video we will keep up to this thank you "
r70NFuEHEt0,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2019 (https://safari.ethz.ch/architecture/fall2019/doku.php)

Lecture 14: SIMD Processors and GPUs
Lecturer: Dr. Juan Gomez Luna
Date: November 8, 2019

Slides (pptx): https://safari.ethz.ch/architecture/fall2019/lib/exe/fetch.php?media=comparch-fall2019-lecture14-simdandgpu-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2019/lib/exe/fetch.php?media=comparch-fall2019-lecture14-simdandgpu-afterlecture.pdf",2019-11-16T12:14:00Z,"Computer Architecture - Lecture 14: SIMD Processors and GPUs (ETH ZÃ¼rich, Fall 2019)",https://i.ytimg.com/vi/r70NFuEHEt0/hqdefault.jpg,Onur Mutlu Lectures,PT2H31M9S,false,2329,37,0,0,1,"okay hello everyone how are you good professor Malou cannot be here today so I'm going to teach these things seem the processors and GPUs which is my favorite lecture how was the exam yesterday was it ok good ok we'll see when we when we read it um yeah ok so what's different today with respect to previous lectures it's different the fact that we are not focusing today so much in memory even though we would talk about memory you know that most of the previous lectures have been around DRAM how different worlds different reliability security and so on and now we are going to start let's see a new at least today we work with a new block of contents of the course which is processing paradigms and today we will gonna talk about to processing paradigms that are very related or at least one of them is based on the other one GPUs are based on Cindy processors and that's what we are going to talk about today so first part of the lecture will be about Cindy processing and here we will discuss or we will introduce vector and high rate processors and in the second part of the lecture we will talk about graphics processing units pretty sure that many of you must already be familiar with these two topics maybe if someone has already taken the computer the design of data circuits course with Professor model you might have seen already nice lights but yeah because probably is not the case for everyone I think it's it's good and also this lecture is going to be also introductory of possible later lecture that we will have a GPU and their genius programming which is a topic that is quite useful and and also very much related with some of the research lines that we have in the group so over the course of the lecture please don't hesitate to stop to ask questions and if we can have like some sort of discussion that also could be really nice okay so essentially what are seeing the processors and GPUs in essentially what they are is parallel machines they are data parallel machines they're machines that are able to execute computation on multiple data instances at the same time so what we exploit here is what is called the data parallelism but as you will see this is not the only thing that we are going to be able to execute for example in GPUs where we can also have some sort of task parallelism we will talk about that later but in principle this is what it is is data parallelism and especially I mean they will be specially good when these data parallelism is regular okay so before we start it is good we take a look or we somehow review something that I'm pretty sure that you already know is the Flyn taxonomy of computers my clean in 1966 classified computing systems in four main categories based on how do they execute instructions to operate on data the simplest of this category is C's D is the first one single instruction single data this is what we understand by sequential matching okay or as color matching where we operate one single instruction on one single data element the next one is Cindy single instruction operates on multiple data elements we have a lot of elements may be hundred thousands or millions of elements and the instruction that we to execute on these individual elements is the same for all of them so that's what we call the Cindy paradigm in this paradigm we have the two main types of processors that we will talk about in the first part of the lecture is our D array processors and the vector processors then we have multiple instructions operate on single data element this is not so something that you will find so frequently in real-world machines but there are some of these like for at least similar ones like for example systolic array processors streaming processors or you might have heard also about I think professor mu to talk here about microns automata processor could be seeing as as some sort of miss D and the last one is me the multiple instructions operate on multiple data elements we have multiple instruction streams examples of these or let's say the the most straightforward examples of these are multi processors and multi thread processors like for example the multi-core processors that we have these days in every computer or even two cell phones okay where are we today in this Cindy right single instruction operates on multiple data elements let's talk about data parallelism how are these seen the processors going to exploit data parallelism they are going to carry out they are going to perform concurrent execution because by executing the same operation on different pieces of data this is what we call single instruction multiple data and there are many examples of possible single single instruction multiple data execution in the real in real world applications for example the vector the dot product of two vectors right when you're doing the dot product what you do first thing that you do is multiplying one element of one vector with another of the other vector that let's say element zero of the first vector multiplies by element zero of the second vector 11 1 by element 1 and so on right so we have a data parallelism there because we are we have multiple data that we have to operate on and is Cindy because we can execute the same instruction on all these instances of data here just very briefly what's the what's different with respect to dataflow machines or dataflow execution it's different because in in data parallelism the concurrency arises from the fact that we need to perform the same computation on multiple data elements in dataflow machines the concurrency arises from the fact that we can execute multiple different operations in parallel and we say in a data-driven manner because these operations only are only fired when the data is ready ok and also the contrast with threat or control parallelism which would be something similar to programming with multiple threads like P threads opening P and so on which is what we usually doing multi processors or in multi-core machines there we have different threads that can execute completely different programs right in Cindy what we are doing is exploiting operation level parallelism on multiple data and this can be seen as a form of instruction level parallelism meaning that we have multiple instructions that can be executed in parallel but it turns out that this instruction level parallelism the instructions are always the same ones right it's the same instruction that is applied on multiple data elements so in simply processor processing single instruction operates multiple data elements and this is something when we actually around the computation when we actually have this concurrent execution that we were saying the previous slide this is something that can happen in time or in space we will typically have multiple processor processing elements not always you will see some examples later but we will typically have multiple processing elements and and we have this time space duality that differentiates between what we call the array processor and what we call the vector processor in order to see this very clearly with a very clear example we have the next slide and and and here you can observe already the main differences between the array processor on the left hand side and the vector processor on the right side as you can see in this array processor we have four processing elements and the four processing elements are exactly the same the only thing that is different is that they have a different index right from preceding element zero to processing element three in the case of the vector processor we have different we have several processing elements as well but these are specialized they are not able to execute every instruction as is the case in the processing elements of the array processor here what we have is processing elements each of which is specialized on a certain type of instruction and you can already see that here we have one of them for load instructions add instructions multiply and store okay and how do we execute a program on these processing elements in the array or the vector processor okay let's assume that this is a program very simple one what you see that first thing that we do is a load instruction so here we go to memory to this array a and we read some elements in this case four elements right from a0 to a3 we read these four elements and we place them we keep them in this vector register next we operate on the vector register we add one to every single element to the four elements that we just loaded into the vector register after that we multiply by two and finally we store the result in the same array that we had in memory right how do we execute this run an array processor so because we have four elements we want to read the four elements at the same time so first thing to do is this load instruction is executed on the four processing elements in the first cycle or at least we issue the instruction in the first cycle it might take more than one cycle to execute right but we can we can issue the four loads at the same time in the same cycle when we are done with the load instruction we schedule this add instruction and because these processing elements are somehow flexible or general-purpose because they can execute many different instructions so we are executing this at 0 1 2 & 3 using the same processing elements and then we have the multiplication and then we have the store operation how do we do that in a vector processor the first thing that we have to do is issue one load this load 0 will load element 0 of array a one cycle later we can issue load 1 and if we already have the value that we have to read from memory zero then we can start operating on it right let's assume that we can do all these instructions in one in one single cycle right so right after we get the value 0 from memory we can start adding 1 in the corresponding processing element and in the next cycle load to add 1 and multiply 0 and then load 3 to multiply 1 and store 0 and so on ok so observe the difference in the case of the array processor we have the same operation at the same time in the case of the vector processor we have different operations at the same time in the same cycle if we look at the space recall the time space duality that we were talking we were mentioning in the previous slide if we talk about the space then we have different operations in the same space in the same processing element in the case of the vector processor we have the same operation on the same back processing element ok ok we were comparing before with dataflow machines and with multi-threaded machines one more comparison that is worth doing you might also be familiar with the VL IW paradigm very long instructional words this is sort of machines that helped by a very wonderful compiler that is able to track instruction level parallelism from the programs they generate they create a program where every single instruction is a very long instruction that actually has like multiple instructions you need multiple instructions that are independent and can be issued at the same time so they can be executed on the processing elements at the same time because they are independent and we can exploit instruction level parallelism in this way why is this different from sim giving well it's different I think it must be clear that here we have multiple processing elements as well but on each of them we are going to execute a different instruction that will be likely working on different data elements as well while in seeing the processor and a sim theory processor we have one single instruction and this instruction is executed on the four elements okay operating on different data instances okay so yeah let's continue with vector processors vector processors I mean received this name because they operate on vectors right and you know already what's a vector you know that this is and in principle one dimensional array of numbers it can also be like two dimensional or three dimensional etc but in principle we will only talk about one dimensional arrays many scientific commercial programs use vectors this is something that you already know and and here we have our first example operating on two vectors a and B and we are calculating the average of element wise average of these vectors and storing the result in array C so this program is like the typical vectorizable program that we want to execute on a sim deep roster so here we have multiple instructions also we have instructions that operate on vectors not on scholars and there are some basic requirements or basic things that we are going to find in vector processor these are first of all we need to be able to load and store vectors and for that what we use is these vector registers recall the example that we had before we execute the load instruction we have to go to memory and the in that example we were reading four elements at a time and when we read the four elements from memory where do we put them we have to put them in our register right so these registers are called vector registers and each of them will contain multiple day data instances instances or elements of the vectors but these vectors and also these vector registers have a certain length right so that they'd I mean they will have the size that that they have right for example 50 and we can see we can fit 50 elements in them and this is what we call the vector length usually this vector length is going to be stored in a specific register in a special register that will contain the vector length and then there is another register that is important for the data access for the memory access is the vector stripe in the example that we saw before or in this example as well what do you have what you can see is that we always access consecutive elements right we would start from element 0 until element 49 and we are going to read them all so here what's the stride among consecutive accesses this try this one right no matter I mean we are not talking about the vector length here but assume that the vector length is 50 so we can read the 50 elements at the same time and we could do that but they stride among this element is just 1 in some cases it won't be that way right in some cases we are going to have a stride at access we might be for example reading only even elements or only odd elements right and in that case or stride would be 2 so when we are writing a vector program a vector processing program and and we need to access memory with a certain stride we will have to enter that stride into this register so vector instruction performs an operation on each element in consecutive cycles this is what we saw if we go back to the vector I mean the disk comparison here we operate on different elements in different cycles this is the case in vector processors right so that's what we have here the good thing of doing that is that it is possible I mean because these execution units this processing elements are will be typically pipeline the good thing of using them for vector processing is that they can be deeper pipelines we can have more pipeline stages and I'm pretty sure that you're familiar with pipelining right you might remember that in a pipeline in a pipeline we have multiple stages the more things or the more computation that we have in each of the stages the longer the cycle will be right because we have more combinational components there so we need more time to traverse them also the the the cycle duration of the machine depends on how many operations how many things we have to do in each of these pipeline stages that's what is called the critical path this critical path in the pipeline is what defines what's the the frequency that the machine can operate at right so if we have deeper pipelines we can probably have less things to do in each of the pipeline stages and this way we can have a higher frequency I mean that's more or less the reasoning and why is that possible why is it possible to have deeper pipelines when we are executing vector instructions it is possible because there there are no in trabecular dependencies recall how these vector instructions are executed in the vector processor it's we start the operation on every single single element in consecutive cycles because in each processing element we operate on the entire vector and because the computation that we apply to the different vector elements is completely independent of each other whatever I do with element a zero or element B zero is independent of what I do with element a 15 right so there is no in trabecular dependencies so that's what this means and there's also no control flow withing a vector recall in a conventional pipeline in a conventional pipeline you execute load instructions add instructions instructions that you also execute branches right because you want to have control flow in your program and the problem with these branches is that sometimes you need to wait a few cycles until the branch are the branches are resolved right this is something that that's doesn't happening in a pipeline processor and one of these pipeline processing elements if we execute vector instructions why is that because there is no I mean there is no that the execution of the operation on element a1 doesn't depend at all on the execution of element a zero so that's what this means we don't have this control flow within a vector so there is no there's never a need to for example install the pipeline because we have to wait until a branch is resolved or these kind of things that happening in in conventional pipelines and also we were talking before about the strided accesses here this tri is known right we I was saying if you want to access only the even elements of the array you will have to enter in these in this stride register you will have to enter number two so that the hardware knows that the accesses have a stride of two right and because this is something that the hardware knows in advance then the memory access can be very efficient right for example it is possible to prefetch data you might be it might turn out that at this point in time I'm operating on elements 0 to 49 and only operating on 15 elements because the vector length is 50 but the size of the array is much larger right for example 200 the thing is that if I know the stride I know that I will be accessing all the elements that are a certain stride for example with stride - I could be reading vector 0 or array a zero two four six and while I'm operating on these ones I can be accessing memory and for example bringing from memory prefetching element a twenty twenty two twenty four and so on and this is something that is doable that is possible because we know in advance what's going to be the stride of the memory accesses so that's why we can also have efficient memory access here as well okay advantages I mean we are already talking about the advantages okay right but let's summarize them here there are no dependencies within a vector so these allows pipelining and parallelization to work very well so we can have deep pipelines because there are no dependencies each instruction generates a lot of work why is that good because I only need to read from memory one instruction to operate on for example 50 elements right so in a conventional sequential machine you need to go to memory to read instructions for I mean operating on every single data element and in this case we are just fetching one instruction from memory to operate on multiple data elements so from that point of view this is more efficient highly regular memory access patterns it's what I was just mentioning this can also have some benefits in terms of exploiting the memory bandwidth better and no need to explicitly code loops what does this mean I mean just recall this for loop that we have there from 0 to 49 there is a for loop there right so every time that we want to execute these of these each of these instructions that is computing the average of the elements of a and elements of V we need to compute and then check a condition and then jump right so these are the branch or the control flow instructions we don't need them here we don't coming in principal when we are operating inside a vector we don't need them right because we are operating on the 50 elements at the same time now there are also disadvantages of course very easy to understand disadvantage is the fact that we need a parallelism to be regular what happens if the parallelism is irregular can you use the cinema Ching well maybe you can use it as well but for sure it won't be so efficient we are going to talk about this again later when we talk about GPUs because in GPUs is possible to to execute irregular parallelism it is possible to do it right but for sure it won't be so efficient you cannot achieve achieve the same throughput as if you are executing regular computation but at least you can do and then another disadvantage of this vector processors is that they're also typically they are difficult to program and this is something that already Fisher in 1993 pointed out you can you can read this extract from the paper to program a vector machine the compiler or hand holder must make the data structures in the cold feet nearly exactly the regular structure built into the hardware that's hard to do in first place and just as hard to change so this means programming is difficult why is that because when we are writing a vectorized program or where or even when we are writing a program for a GPU we need to try to make things regular as much as possible or at least make sure that things are going to be executed in a in a regular manner or at least as much as possible okay based on this and this challenging thing that we have here programming Sindhi processors we might have our GPU and the unit's programming like later another day we will probably take a look at this slide again by then and another disadvantage is that the memory bandwidth can easily become a bottleneck especially if memory and compute operation balance is not maintained what does this mean this means that simply processors can be very efficient right you can have multiple processing elements working in parallel and they can be operating on many input elements at the same time also in this vector processors and as we have just discussed we can have deeper pipelines because we can have different deeper pipelines it's likely that we can operate at a higher frequency so what this means essentially is that we have a lot of compute power we can execute operations very very fast actually let's say arithmetic operations for example very very fast right but in order to be able to execute these instructions we need to be able to gather data from memory very quickly as well right and this is this is why this is going to be typically the bandwidth here because as you already know because this is something that Professor mundo has explained and and that's actually what has motivated all these research that we are doing in processing in memory accessing data from memory is much more costly than operating on this data and this is something that for sure is clear in in vector processors and in GPUs these days even though they are incorporating these new memory technologies like for example hvm they are still bound by the memory bandwidth okay actually I mean you can even compare what's the ratio between the number so between the peak flops with peak floating-point operations that you can execute on a GPU these days with respect to the bandwidth that these GPU provides and if you compare these to a GPU that was built 10 years ago for example you will see that with this ratio is even worse this means that the in ten years GPUs have changed have evolved a lot they have much more computer output they have much more bandwidth but the distance between these two is larger now than it was 10 years of also the memory bottleneck problem exists and it's even worse these days we were talking about this computer memory operation balance but also the fact that we need to make sure that the access to memory at this we do or part let's say with the word share is like let's try to make the memory access efficient by mapping data appropriately to the memory to the memory banks we will talk about the memory banks right now okay vector processing in more depth any questions so far is everything clear okay good so we have already talked about the vector registers each vector register is going to be able to hold an M bit values okay so this is one of four vector registers as you can see here we can store up to n elements and the size of these elements is M bits okay and we have in this case like three different vector registers then we also have the vector control registers we have already talked about two of them vector length and vectors tried but there is a third one which is super important as well is the vector mask vector length is actually the maximum vector length in the previous example I was saying oh let's assume that our vector length is 50 what does this mean this means that we can be operating on 50 at the same time right so we can be executing like yeah computation on 50 operands at the same time but this vector length is the maximum vector length that as you can guess it's related to the hardware it's related to the number of processing elements that we actually have in your system what happens if instead I mean the maximum vector length is Eng the maximum vector length is 50 but we only want to operate on 15 elements not 15 what would we do so what we have to do is in your program we will have to say that this V length is equal 15 not equal 50 okay and then we have there well we talked before about the vector stride and here we have the back term mask register this is going to be very useful especially when we when your computation is not so regular right so we were saying before simply processors or vector processors are very good when the parallelism is regular and when we use all the vector lanes when we use all the elements in the vector registers and we operate on them on them in parallel right but what happens if that is not the case what happens is for whatever reason we don't want to apply the same computation the same instruction on this element but we want to do it on this one and this one and this one and this one well in that case we can use the vector mask essentially the vector mask will contain a value 0 or 1 depending on the fact that we want to operate or not on a particular element ok for example we say ok if the element is 0 we don't want to operate on it if it's 0 the value of vector mask I will be 0 and then we don't do anything so that's the reason why we need this vector mask anyway we will see a better example later in a few slides okay vector functional units what's what we call the vector functional unit is the collection of functional units that operate on a vector right we already know that they can have the pipeline which means that we can have a faster clock cycle and we don't have to worry that much about control of the deep pipeline because the vector I mean the the operations inside the vector are independent this is something that we have mentioned before let's assume that this is our pipeline is a pipeline with six stages recall what I was saying before recall so think for example that these were conventional scalar pipeline right like in the MIPS pipeline that the Britisher that you have studied and it turns out that here for example we have two access memory and the instruction after the load instruction that is accessing memory is dependent on this load instruction so what would happen is that if we are accessing we are going to memory to read something and this takes a few cycles let's say hundred cycles or so the next instructions that are dependent on this one will has will have to be stalled right until the the load is resolved until we get the value from memory and then we can use the latest subsequent instructions can use the value read from memory this is this doesn't happen in vector computing why is that in vector processing why is that because we have in this case two input vector registers v1 and v2 they contain multiple operands and what we do in this pipeline is that we first start the computation for element 0 in these vector registers then element 1 then element 2 and when we fill the entire pipeline we will we would be operating on six different elements of these vector registers but the computation on them is completely independent so control of this pipeline is pretty simple as you can see okay one example of vector matching pretty old one is gray one yeah you can take a look at that technique technical board or data sheet from 1978 if you want and this is like a somatic off of these the internal organization of this vector machine as you can see here we have vector units but also scholar units for example here you can see a scholar compute units there floating-point units here they're vector units there and here we have scholar registers and here we have vector registers you can see up to eight vector registers right so this is like a very first example of real-world vector machine actually we have a very similar one at C AV you might have seen it this creates mp-28 which is beautiful these days is no longer a supercomputer now it's a coach you can sit here it's pretty comfortable and because if you really want to run some computation here you better go and buy one Raspberry Pi you will see there as well they both are have similar peak throat put you have 400 mega flops so yeah you can take a look at this it's a really beautiful one and here in the next slice you have a little bit more information about the internal organization this particular model has four CPUs and inside each of the CPUs you have what we have seen the in the previous slide as well like you know a skull are functional units floating point vector functional units and so on a little bit more information about the different models of these Cray XMP different number of CPUs memory size number of banks etc a few more details here about the functional units address functional units color vector floating-point and about the system configuration here in the different models like you know formation about type of of memory storage units etc and this is the person who was responsible for these these computers write the same or Cray this is so called the father of software computers and and I really like this sentence if you were plowing a field which would you rather use to to strong oxen or 1,024 chickens and here you have you have them so what would you use what they do it will depend right it will depend on what you really have to do it depends on I mean all these these different processing paradigms you will choose the one that or in principle you have to think about choosing the one that that's better to your workload right the thing is that in the end we cannot build the perfect machine for every single workload and usually we will have to you know computer architects have to make to take many trade-offs and and decide ok let's have something you know like a few let's say vector units for example but let's also have some scholar units as well because they are useful for many other workloads and this is actually something that Cray already knew and that's why even though this gray one was the first vector machine this doesn't mean that it was also it wasn't also the fastest color machine of its time so not only a vector machine but also the fastest color machine ok and much of the success of this vector processing is based on the fact that we can access memory pretty efficiently and why is that because we can use multiple banks in memory in this particular machine for example 16 memory banks and let's let's see what we mean about the memory banks and about accessing memory in an efficient manner for for vector processors so we already know that we need to load and store multiple elements at the same time or more or less at the same time we will see how we do it because we want to operate on them because we have multiple processing elements that can run in parallel so that's why we need to read and write multiple elements at the same time from memory this we already know that these elements are separated from each other by a constant stride for now we are going to assume stride equal to 1 and we as you will see what we are going to do is that we are going to be able to load elements in consecutive cycles and such that we can start a load of one element per cycle that this is something that we are going to be able to do by using multiple banks inside memory ok so yeah so the question is also how can you do that if the memory access a single memory access takes more than one cycle recall the very first example that we have for array and vector processors and I told you ok let's assume for now that every memory access every instruction every operation here takes one single cycle but unfortunately that's not the truth right you know that processing elements are usually pretty fast you can execute for example an addition or multiplication in just a few cycles but if you have to go to memory you have to go to the RAM then you start you know talking about the order of magnitude of 100 or 200 cycles or something like that so yeah so that what we can do if our memory access is take longer than one single cycle what we have to do is Bank the memory so what we are going to do is take the whole memory and we are going to divide it in multiple pieces that we are going to call banks and that's what is called the memory banking in this example we have 16 banks instead of having one single monolithic big memory what we do is that the whole memory capacity is divided in sixteen banks in this case as you can see each of the banks have its own MDR and ma are I would like to remember to remind you what this means this is memory data register memory address register so how do we typically operate on this we have an address bus here so what we do is from the CPU we use the address bus to bring the address that we want to access in memory to this M IR and this aim IR is connected in this case to Bank 0 and this allows us to take the contents of this address and put them here to the end MDR and then by using the data bus we are able to bring these to the CPU so this is how each of the banks individually works right but now what we want to do is operate on the 16 banks in parallel at the same time and how do we do that well what we can do is that we can issue one request every cycle and because reading or writing from or into each of these banks takes more than one cycle we are able to start issuing requests to the multiple banks and a few cycles later we will start getting the values that we want to read from memory right so in this particular case we can sustain up to 16 accesses in parallel or in accesses parallely if we have egg banks this is one more nicest slide to show you how the address is generated think about something like this recall that we have an astride right we have a base address which is going to be let's say they they start the starting address of the array and then the elements that we are going to be reading or writing are going to be determined defined by this strike so we need some sort of address generator like this and this is the address that we will place in each of these ma yar ok so yeah so the way that we calculate the next address is the previous address processed right and and and this is just one example right if the straightest one and consecutive elements are interleaved across banks and the number of banks is greater or equal than the Bank latency then we can sustain one element cycle throughput you understand that what does it mean it means that here we have 16 banks and we are going to be reading one element from each Bank why is that because it Stratis one and because the consecutive elements are stored in consecutive banks that's what these if says and the last part of the says the number of banks is greater or equal to the bank latency why is that why is that important because we want to receive one element every cycle so for example here imagine that the number of cycles that we need to read one element from each of these banks is 11 if it's 11 we will have to wait 11 cycles to read element 0 from this bank 0 right so because we don't want to just simply wait what we do is put in more banks next to this one and you read these 11 cycles what we are doing is that the first cycle we start the request or start the read in bank 0 in the next cycle we start the next read from bangkwang then from bank to then from bank 3 and so on and in every 11 cycles later we are still you know issuing requests to the rest of banks 11 cycles later we can start reading the element that we needed from bank 0 you see now imagine that instead of being 11 cycles is 32 cycles okay what would what would happen during the first 16 cycles we are issuing requests but then after that we will have to wait sixteen more cycles until we can actually get the first value from memory right so that's why if we want to really you know like have all memory dimension in a nice way we want to have more banks than cycles we need to access memory okay you have a question we need like four data but Stella back 11 seems like five then is not really meet Greg in back instead of the we can prefetch then for data from four different banks began just over at the data so it should not it be linked back let's see in [Applause] you're asking if the bank latency is related to the number of processing elements that we have or o the number of blanks related to the number of processing elements yeah I mean that that might make sense I mean it will depend on their on the specific architecture for example if you are thinking about an array processor and these are in an array processor where you have let's say 16 processing elements and each of them can execute any every instruction like addition multiplication and so on so you compute something using additions and right after that the next instruction is instruction is allowed right so yeah in this case it could make sense to say okay if I have 16 processing elements let's have 16 banks as well and so that we can issue the request for all of them at the same time okay that's let's say intuitive but doesn't really need to be like that you know because you could have more banks or you could have less banks as well but how efficient this is actually going to work will depend on how how many cycles you really need to read from memory okay we can maybe discuss later a specific examples and and we will likely have questions in the homework where you will have to think about about this dimensioning okay okay let's take a look at a good example here it's a element-wise average right you remember that the code that we have seen before from I equals 0 to 32 to 41 we are going to calculate the element-wise average of the elements of two arrays a and B right and we start the resulting array C so this is an example of s color code C let's think about the sequential machine CPU for example and in the CPU what we are going to run to compute that element-wise average that we have there is something like this this would be like or assembly or or matching code right so take a look at what we have here first thing is we load 50 in register r0 why is that because we need to perform 50 iterations to compute over the 50 elements that each of the arrays have then we have the address of a and the others of B respectively in registers r1 and r2 and then the address of seeing r3 next thing that we have is a load instruction to load here into our for something that we are reading from memory and in particular we are reading r1 which is a and then we have here some sort of auto increment addressing we do the same for r2 which in this case is array B and now we add these two registers the consume the contents of this register r4 and r5 stored the result in our six and then we shift right because we want to divide them by two right and then the result in r7 is what we write into memory we store into memory in whatever address is determined by this register 3 and finally the only thing that we do here is we decrement in this case register 0 and compare the value to 0 and if it's not 0 then we jump or branch and go here and start and we do this like 50 times right so if you count how many dynamic instructions we have here after you know like running the 50 iterations of this loop this means three hundred four dynamic instructions okay now recall that we have to read this 300 for dynamic sense drawing dynamic instructions from memory hopefully not from the memory hopefully from the instruction cache but we still have to fetch them from memory ok the problem with this color code is that I mean it's it's quite inefficient right why is that well first of all let's assume that okay yeah first of all let's think about a memory with one single bank if we have a memory with with one single bank and we compute how many cycles in total do we need to execute this 300 for dynamic instructions what we will have to do is adding up all these latencies that we have here I didn't mention that but here in this column what we have is the latency out of each of these individual instructions so for example load instructions take 11 cycles so the calculation in the next slide is say ok I have 50 iterations so this is 11 plus 11 plus 4 plus 1 plus 11 plus 2 is a number of cycles for a single iteration so then 50 of them this is the total number of cycles here two thousand four cycles this is in the case that we only have one single Bank in memory that because we don't want to be unfair and we are going to compare two vector processors now we are going to consider that this inorder processor this very naive CPU or can already use like a memory with sixteen banks so what this means is that unless these two are mapped into the same Bank we're going to be able to read them at the same time so let's assume that these for example a zero is in bank 0 and B 0 is in Bank 1 they are in different banks so we can be reading both at the same time right so the good thing is that these 11 cycles can be overlapped right you already know that right so in the first cycle we issue this request in the next cycle we issued this weakest 11 cycles later we get this value and one cycle later we get this value so if we do that then we can reduce the total number of cycles to 1500 4 cycles ok ok yeah here you have one why 16 bands I already explained this right it's dependent depends on what's the memory latency but now let's assume that I mean because you know all the computation that we have in that for loop to calculate the element wise average is very regular we come back towards this loop meaning that we can execute that computation on a Sindhi processor or on a vector processor so if we vectorize with with this loop and we write this using vector instructions we can have something like this first thing to do is set vector length equal to 50 okay which is the number of iterations that we had in the loop next thing is vectors try equal to 1 why is that because we are all accessing all consecutive elements and then vector load to read the 15 elements from array a from vector a and then another vector load to read the 50 elements from B and then we add element plus element all of them in these two vector registers b0 and b1 then we shift right and finally we store so now latency of these the latency of these is here each of the individual operations take exactly the same amount of cycles that the good thing is that because we are using a vector machine as soon as we issue one instruction one load or one add or one sheet right in this in the next cycle we can start with the next operation so that's why for example for this vector load in the first cycle we issue the load for a zero and in the next cycle for L for a 1 the next I'll call a 2 and so on so after 11 + wheeling 50 minus 1 cycles we will have all the 15 elements read from memory make sense so here drastic reduction of the number of dynamic instructions is only seven not 304 so recall what I was saying we don't need to fetch so many instructions from memory and now for now we are gonna assume chaining we will see what's chaining chaining means that we can you know like forward data from one vector unit to another from one vector processing element to another vector processing element for now we ignore this possibility so we assume that we cannot start the execution of an instruction until the entire input operand back to register is ready we consider that we have one memory port meaning that we have one port per Bank so we can be only so if we are using one memory bank we can only be reading or writing one element at a time and we have 16 banks and the mapping that we assume is this one is the world interleaf meaning that consecutive words consecutive elements go to consecutive banks and now we can see this timeline how the this code that we have here is executed right I already mentioned this but yeah so these two is just one single cycle is storing the vector length register and the vectors right register and right after that we have the vector load so in the first cycle we issued a request for element 0 1 cycle later the issue for the request for element 1 element 2 and so on after 11 cycles because the latency of a memory access is 11 after 11 cycles we have a 0 1 cycle later a 1 1 cycle later a 2 so in total after 11 plus 49 cycles we have this vector load executed now observe we have one single memory port so we can only start the next back to back to load when this first one is completely done and again is exactly the same 11 cycles to read from memory be zero and then one cycle later we will have the remaining 49 elements of vector B for array B and then we start the addition picking for the first addition we need four cycles and then we will be obtaining the addition results in the next forty nine cycles for shift and for a store so this in total is 285 cycles which is already a strong reduction with respect it's like five times faster or so than the thing for the sequential machine right now let's assume that we can use vector chaining which essentially means data forwarding essentially means that if we have if we are reading these elements from me and from B observe that as soon as we get here B zero we could start this addition right why is that because we obtain we read a zero from memory and in this point and this here from memory in this point so right here we can already start the addition of T 0 and B 0 and this is actually something that makes a lot of sense right why is that because we not only have load and store units in your system we also have additional units we have multiplication units and maybe many more right so it doesn't really make sense to say oh okay you can start the computation of the addition here but let's wait until here so during all this time we could be using this addition unit but we are not doing it right so we are wasting the resources so vector changing makes a lot of sense and that's essentially what it is about right there's like ok I apply a partial result from the load unit and partial result meaning you know what one of these individual elements so as soon as I get it I can feed it into the multiply machine in the multiply unit for example here and as soon as I obtain the result of this multiplication I can change the result and send it to the or forward it to the addition unit right so this is chaining so if we apply chaining or to our element wise average program what we could do is exactly what I was telling you right after obtaining from memory b0 we can start the computation of the addition for a zero plus V zero right and as soon as we are we have this addition a zero plus B zero so we can start the shift right right so observe that we are changing from here to here from here to here and you know there is no chaining here simply because we have to wait for the store until the two loads have finished and why is that okay this is 182 cycles so like 100 cycles less than the previous implementation but still we have some problems here and the provenance is that we cannot paralyze memory accesses that you see that these two vector loads cannot be pipeline cannot be in parallel and this vector store is exactly the same right we have to wait in order to start the vector store we have to wait until the last the second vector load is completely finished right but observe that we we could have a start at the store here right because here is where we have a have already a zero plus b0 divided by two so we have it here and we still have to wait a few cycles and the reason why this happens is because we have one single port in each memory bank so possibility here is two of course at the expense of having a more expensive hardware we could have to load ports and one store poor in each Bank so if that's the case this means that for example we have to load ports so this means that we can be reading to elements from two different arrays that are mapped to the same Bank right so we could be reading at the same time a 0 and B 0 assuming that they both are placed into the same Bank in banks 0 for example right so this is what we have here we would only need to wait one cycle why is that because I mean the address bus and the data bus is sure right but right after this cycle we can start the load vector load for B and and for the store operation because the store port is also independent of the load ports we cannot start the store operation right here we don't have to wait until at until this second vector load is done so in total like 19 times faster than the sequential machine so it's a quite good ok questions yes you mean yes why is there you mean this so so the thing is that you generate 100 every cycle that's clear right so what you have here is that it refers as a let's say cycle zero or cycle one you generate the address for the first memory access and one cycle later you have the address for the next memory access oh you're generating addresses for a and B yeah that's that's the that let me write we have to load ports so now you can assume that you can generate the addresses at the at the same time because of the data bus or than they and the address bus I mean this okay also take into account that this is a simplification right so we don't go into all the details how this is this is let's say I add an abstraction of how of how this really operates but if you check the let me go back to the slide it's here right this is life so what you can assume is that this address bus is shared so yeah you might even have like several ports here but still for you know moving the address from the CPU to the individual banks you need to share this this is you can assume that this is why you have to wait one cycle but as I said this is bigger you know simplify this an abstraction that because that's what we have to do to do to to explain the concepts right but that's more or less like that okay yeah so next thing what if the number of data elements is greater than the number of elements in a vector register okay that's a important question why is that because I mean in the previous example we were assuming that vector length was 50 and we have 50 processing elements there or somehow I mean we can be operating on the or vector registers at least can hold these 50 elements at the same time but that won't be always the case right what happens if our vector length is 16 or for example in this example what happens if for vector length is 64 meaning that every vector register can only hold 64 different values right but now instead of operating on an array or a vector of 64 elements we operate on as 527 elements so what do we need to do in this particular case we will first set the vector length register to the maximum possible which is 64 and we carry out like in this particular case eight iterations where the vector length is 64 so after the eight iterations we could have operated on 512 elements but then we have 15 more elements right because we have 527 in total so what we will need to do is changing the content of the vector length register to 15 and then operate on the 15 elements ok yeah so this is called vector strip-mining and the name is coming from from surface mining or strip mining which essentially is a mining technique that that consists of getting rid of the that you don't need and simply take what you really want like the gold or silver ore or whatever right so in our case in the case of the vector registers the goal is the data that you are really going to operate on and they and the shitty part is the you know the this tail that you don't really need in the back toe right okay next question is what if vector data is not a store in a straight fashion in memory so this is interesting right because as of now we were saying oh let's consider that the straight is one meaning that we are going to read all consecutive elements of the of the two input arrays or the two input vectors or now let's consider that the stride is two and we are reading a 0 a 1 a 2 a 4 and so on and and this is fine and actually we have already said talked about the you know that the efficient memory access that we can have we can prefetch data because we know what's a next data element that is going to be needed and so on but unfortunately this is not going to be always the case right in some cases we will need to have the regular memory accesses and for example you can think about a sparse matrix computation we had the other day constantinos right presenting his work where he figure out sways mechanisms techniques to improve the operation on sparse matrices or or vectors yeah so this is something that also applies here right if we are operating on Deng's mattresses ding that's perfect because you know the access is for sure are going to be regular we will actually see an example soon but if we are operating in sparse matrices then is not only going to be like that you might even remember that constantinos was talking about indirect address accesses right he talked about the CSR compression format and and he said oh you're gonna have indirect memory accesses so if you have to operate on sparse matrices or sparse data structures you will likely have memory accesses like this one where you first need to access to read one array D and this array D contains the address that you need to read from array C right so the problem here is that the accesses to memory are not any more regular so they are probably going to be costlier yeah but anyway Cindy machines in the processors usually have this sort of load indirect that can starting from base address in this case our C is the base address contains the base address of array C and having some indices stored in in this case vector D which is something that we just read with the previous instruction so we can generate the addresses from these two from the base address plus the index inside the array C and then go to memory and read some values and store them in vector C in this in this case this is what is called a gather instruction why is that because we are gathering data from memory and we are putting these data elements inside a vector register and the opposite operation is called the scatter operation right and and and here you have an example of that this is the data that we need to store four elements and these are the addresses in array EC for example where or the indices in array C where we have to store these values so this is what we are going to store in memory okay so is it for example these 314 goes to index 0 these 6.5 goes to index 2 and so on and here we talk about a scatter operation a scatter access right so we have the values that we need to store in a register and then we scatter them across the whole memory or they the whole memory space occupied by the output array ok yeah so as you can guess also I mean is this more less efficient than regular accesses well it depends on the actual implementation but as you can guess this is typically going to be less efficient right and let me just give you an example so these elements might be in different cache lines right and you know that memory success at the granularity of a cache line so what happens if for cache line size is 4 for example I mean 4 elements right so these 4 are in 1 cache line these 4 are in another cache line and now in the same vector store we are writing these 4 elements and these four elements are scattered across across 2 cache lines meaning that we need to write into 2 cache lines if we are reading it's exactly the same so we are reducing or throughput by half right ok okay next thing is conditional operations in loop and this is where we need to talking about mask operations we already mentioned that before but now we are going to see a more concrete example recall that we have this V mask register which is a bit mask that determines which data element should be act upon or not so in summary this vector mask is a register and this register contains as many bits as vector or vector length so we have one bit per vector for element inside a vector array and if the value of the vector mask of the value of this bit is 1 we want to execute the operation if it's not then we don't want to do that right and this is very useful whenever we have conditional operations so for example here we only want to perform this computation in case that a AI is is not 0 so what we do here is we first read few num a few elements from vector array a from array B and now we compare all the elements that we have in v-0 we compared them to 0 if they are 0 then the result of this comparison is is 0 so that's what we will have in the corresponding bit in the bit mask and in the vector mask if these two are different then the result is 1 and this one is the corresponding bit in the vector mask and then the next operations are going to be conditioned or predicated to the contents of this vector mask so for each of the elements in V zero and one these two vector registers we will only execute this multiplication if the corresponding bit in the best vector mask is one okay and this is how we can implement conditional execution in vector machine right so now good thing of doing this the good thing of doing this is that we can exploit irregular parallelism right by using this so for example here we have n elements and maybe half of these elements are equal to 0 and the other half are not right so we need to operate on only n over 2 elements so we still have parallelism irregular parallelism because we are not applying the same computation on the same elements good thing is that we can really use the matching but thing is that we are not using all the lanes right all day all the vector length we are only using half of it ok as I said this is what it's called a predicated execution so the execution is predicated on mass bit ok yeah another example with masking you have a different example here if a is greater or equal Dam Bi then you do this you write a into see if not you write B into C so how could we program this so first thing to do is comparing amb comparing the elements in amv to get the V mask after that we store in to see those elements of a for which the mask is equal to 1 then we complement the vector mask so that now the elements so yeah we are complementing it that's clear so now we store only those elements of B in to see those elements of V for which the mass is now equal to 1 right so here you have the first I'll be the example the original value the first value of the vector mask if we compare a to be a 0 is not greater or equal than B 0 so that's why this is zero these two are equal that's why this is 1 okay yeah and now just to finish with these mass vector instructions so we would typically do something like this right so this is our or vector pipeline and we start computing on C 0 I mean we start calculating C 0 C 1 C 2 and so on so C 0 is whatever a 0 plus B 0 for example C 1 is a 1 plus B 1 and so on and for each of these operations the mask has a value 0 R 1 and so observe that all the computations all the operations go through the pipeline and only when we have the actual value we check the vector mask and we decide if we write or not so in this particular case for C 0 the mask is 0 so we don't enable the writing it's kind of wasting cycles right so because that's the case it's better to use this or might be better to use this density time implementation if the density time implementation what we do is that we don't start the computation for some particular elements if the mask is 0 so this is why you can see that we are only issuing the computation for element 1 element 4 5 & 7 and that's what we have here in the pipeline right so what is which one is better and what are the trade-offs so in principle this is more efficient right the problem with this one is that we need to scan the mask before we start so we are already adding some overhead before we actually start right so this makes the hardware more complicated more expensive but the good thing is that it will be more efficient okay okay something similar to this is what current GPUs do to to be able to execute irregular computations when we don't have all the same operation on all the elements of the vector then that's what we have to do I don't know if I okay I might be doing something wrong with this yeah maybe we are going to have a break song so we can check okay okay do you want to have a break now yes okay let's have a break for ten minutes okay I think it's time to continue okay so we were here some issues to strike and banking this is something that I might have mentioned already but now we are going to discuss a little bit more is what should be the relation between the stride that we have in the memory accesses and the number of banks or the yeah and the anti banking well we want them to be prime relative or relative prime what does it mean this means this means that they strike itself this number is prime relative to the number of banks if that's not the case then we will have Bank conflicts and let me let me give you an example let's think about matrices right you know that there are two possible ways that well at least two possible ways that we can map a matrix into memory memories linear array addresses are linear right and and matrices are not linear they are two-dimensional so how do we store the rows and the columns of the matrix into the into this linear memory there are two possibilities at least first possibilities we store all the elements of row 0 dang on the elements of row bang all the elements of Row 2 and so on and the other possibility is all the elements of column 0 all the elements of column 1 and so on ok well let's assume that it's row major which actually is the most typical one in programming languages like C for example let's assume that we have these two matrices a and B and we want to multiply them so as you can see this matrix a is 4 times 6 and this matrix B is 6 times 10 so we can multiply here and to perform the multiplication what we do is the dot product of a row of a and a column of B both are stored in the same memory in the same system so both are stored in row major order this means that in memory they are stored like 0 1 2 3 4 5 6 7 8 9 10 and so on and in this case is 0 up to a 9 and after 9 we have 10 and after 19 20 and so on ok so this is how they are mapped in memory so how they are stored in memory but now all memory is multi-bank it has multiple banks so I don't know how many banks for now but let's assume that the interleaving is at the granularity of a world meaning that a 0 is in bank 0 a 1 is in bang bang a 2 is in bank 2 and in this case B 0 is in bank 0 one is in bangbang and what happens with the ten is hitting Bank tang well if we have ten banks it will be impacting I mean if we have eleven banks it will be in Bank thing right but if we have eight banks for example where is this element zero it will be in Bank three right okay so this is how they are going to be mapped and now what's the problem here because as long as we access consecutive addresses perfectly fine right if we have a vector load that reads these six elements of the first row row zero of a then they are going to being consecutive banks so we can read them the six of them at the same time and if we do the same for me it's perfectly fine as well but what happens if we want to perform the dot product of this row and this column and these guys are not in consecutive banks then it will depend on what's the relation between the stride which in this case is tang as you see and the number of banks so if the total number of banks let's say is Tang then we are probably so no okay if the total number of banks is ten we are perfectly trying for these six elements right because from a zero to a five they are going to be mapped into us or stored in two consecutive banks but what happens with these ones we have only ten banks right so if we have ten banks it turns out that element 0 10 20 30 and so on are all mapped into the same Bank in banks here oh and the problem with that is that we cannot start and we cannot perform the accesses in parallel concurrently but we will have to wait in order to read the tank we will have to wait until they read the access to be zero is completely finished right are we always going to have this sort of problem we will always have this problem as long as the stripe and the number of banks are not relatively prime right so if the number of banks that we have is 16 for example if it's 16 then that's fine we have enough banks to be able to access B 0 and B Tang at the same time and and maybe B 20 also maps to a different Bank but at some point there will be some stripe that will be a multiple of 16 and this means that we are not going to be able to access these two in parallel okay yeah so this is essentially what we what this slide is about and and what we are doing here is describing what Bank conflicts are so Bank conflicts are going to happen anytime that we want to access at the same time two data elements that are mapped into the same Bank because we can only sustain one access per Bank in principle of course my depend on whom how many parts we have right but for now let's assume that we can have only one Porter Bank okay is there any way of minimizing the potential backup bank conflicts for sure we will have Bank conflict swing accessing in a conventional matching for sure we will have conflict bank conflicts when accessing this matrix B is there any way of minimizing these banks yeah there are possible ways first first way is more banks okay that should work but for sure is more expensive is not it's not a very very good idea another possibility is to have a better data layout to match the access pattern we can play different tricks and actually if we have a chance to to have a GPU programming lecture I will give you at least one way of reducing the number of bank conflicts when programming GPUs one possibility let me very briefly mention what I mean one possibility is to add some padding padding means that we are adding some you know the scattering some memory positions that we are not really using for example here instead of having a matrix of six times tank what we could do is reserve a little bit more space for this matrix such that instead of having these and assuming that we have ten banks remember that that was the original example here instead of having this guy here we could write this guy here in the next position and the position here this memory memory address will be empty so this is padding so what we are doing is inserting some memory positions that we are not really using to store the actual values such that we are able to shift these values in this case to the right to the next bank and this way we would be avoiding the bank conflict anyway that's one possibility we will we will talk about that one probably in more detail in a later lecture and then another possibility is to do a better mapping of others to Bank and for example our randomized mapping there are for example hash functions that you can use observe that here the way that you obtained a mean and a good knife example like this one the way that you obtain what's the bank where one particular address is mapped to is just by looking at the least significant bits of the address right so if you are element 0 element 1 element 2 are mapped to bank 0 bang-bang-bang 2 and so on but we could randomize this in some way and say ok element 0 is going to be in bank 0 but element 1 is going to be in bank 7 an element saving is going to be in bank 1 why not we can do that right and we can do that using hash functions and this is something that was proposed in first time in this paper by Bob browsing 1991 pseudo randomly interleaf memory it might be good but I mean it also has its own drawbacks right because if you I mean for example from the programmers protect perspective if you know how the data is mapped into the different banks you can't reason about that that if you don't know what the hardware is actually doing thing is much more difficult right if there is a hash function in between that you don't know exactly where is mapping a particular address that you're working with then it might be more difficult for a programmer to to reason about all of these okay yeah so we are already finishing this first part about the array and vector processors we have seen the difference between these two data parallelism in time and or in space and GPUs are a prime example and we are going to talk about GPUs because GPUs as you will see have you know have characteristics of things of or things of both paradigms and not only about our array and vector processors but also about fine grained multi-threading machines we will also talk about that okay one more thing is how to achieve this concurrent execution we are we with Cindy processors we are we operate on multiple data elements at the same time by using one thing an instruction but after saying that we have seen how a vector processor works and I mean a vector processor we have seen that there are some specialized units and each of them computes you know that performs the load the addition the multiplication and so on and and we said also that these these are like pipelines so when we need to for example execute one addition of value meaning a zero plus V zero in reality what we are doing is using this addition unit in a five right so we are not really operating at the same time right but conceptually it's at the same time for sure but that would be indicates that we only have one single functional unit it might turn out that we have more than one functional unit something like this we could have for addition units but we don't operate on only four elements we can operate on for example 28 elements so the way that we are going to schedule the computation here is by taking advantage of both time and space right so observe that in the very first cycle we can issue the addition for calculating C 0 1 2 3 in the next cycle 4 4 5 6 7 and so on so this way in the end the program is exactly the same is V at a plus V store the result in C the way that we really execute this on the hardware depends on the hardware if we have one single functional unit will be something like this if we have multiple it can be something like this GPUs these days are more similar to this thing as we will see and each of these functional units are somehow arranged in this in you know this way you know so when we talk about the functional unit here the functional unit would be these four pipelines because the four of them are for the same type of instruction for example addition or for example multiplication and then each of these columns that we have here is called a lane is called a vector Lane and observed that inside in this vector Lane what we have is registers right these are the elements for example I mean zero an element four are going to be here here we have element 1 and element 5 here we have element 2 and element 6 and this might be for example the multiplication unit that one is the addition unit and whenever we need to use either these or that we would be going to this part of the register file as you can see the entire register file is partition and part of the registers in the register file are assigned to each of the lanes so whenever we need to use this we will go to these registers we will grab the corresponding elements and we will start operating on them but from the point of view of the vector register as we have defined it in the beginning of the lecture the vector register would be the entire thing element 0 of vector register is here element 1 of vector registers here element 2 of the vector register is here I hope that is clear ok how do we schedule the computation in a hardware similar to this one how do we scale your computation we would be doing something like this for example here we have 32 elements in in each vector register but we only have 8 lanes ok so this means that we can only start a computation for every 4 8 operations not for the 32 at the same time so for example if we have a load unit and we want to load 32 elements from memory first thing to do is start in the load for 8 of these elements and in the cycle the next cycle for the next eight elements and in the next cycle for the next eight elements and that's what we are going to do for all these units for example the multiply unit how many lanes do we have we only have eight lanes we only have eight lanes but each of these functional units that we have there are pipeline so every cycle we can start the multiplication for eight elements and in the next cycle for the next eight elements right ok we will see this is light again later yeah as you can see here in one single cycle for example this one we are operating on we are performing 24 operations we are operating on 24 input operands ok yeah to do that we need to vectorize our code I mean pretty but we were saying o vectorizing code writing vector code is it's very difficult right and because it's difficult compilers have been trying to do this automatically for a long time so what a compiler needs to do if we want to vectorize a code automatically is to do some loop dependence analysis and the compiler would be able to vectorize a loop for example the loop that we have there on the top of the slide will be able to vectorize it if there are no loop carry dependence ease meaning that this iteration 2 is completely independent of iteration 1 and if that's the case then we can't generate vector code and we can perform these loads and these loads and these additions and these store operations at the same time you know in a Sindhi fashion yeah this will be particularly possible when we have regular data level parallelism if if it's not regular or if we have loop carry dependence ease these kind of things then the automatic vectorization won't be possible or won't be that easy and in the end we are always going to need to have the possibility of executing scalar operations recall what I was saying before about the cray-1 even though it was a very good vector machine it was also the fastest current matching up its time okay and we are already done work that there are some examples in real systems of seeing the operations and what we are going to do next is very briefly talk about some of these examples or in particular one of these examples which are the MMX instructions that were introduced in the 90s in Intel machines so essentially the idea here is to make use of the registers that are already available in the system in the x86 architecture make also use of the functional units that there are already in that computer and what we are gonna do is using them to operate on let's say soup wort so if these are 32 minutes and these 32 bits originally where this register s0 what we can do is divide in this register into four chunks of eight bits each and in each of these eight bits groups of eight bits or bytes what we are going to do is storing different operands in this case elements a 3 or a 0 2 a 3 and in the other 32-bit register we store elements V 0 2 B 3 and now we can do a pack 8 which means it's packed because the four elements are packed into the same register is 8 because we are operating on 8-bit elements and is the addition right because we are doing this Plus this and the result is a store in these bits of the register as - so that's what MMX instructions do they they use the registers that were already in the system 64-bit registers and in a 64-bit register you can either store one 64-bit element or two 32-bit elements or four 16-bit elements and so on and later what you can do is say ok now I'm going to execute one packed at 8 or 1 pack at 16 and depending what the size of the input we would be using one of these instructions you can take a look at the paper as well I'm it might be required reading I don't know that yet this is that we will decide that later yeah and here you have a very simple example actually I think that you can take a look at yours look yourself about this at this example I mean this MMX instructions were included or were yeah I started in the 90s in these interred CPUs because you know in the 90s many multimedia applications started to be very very popular same as these days we have many for example AI applications and we are start to incorporate all these applications in your computing systems in the 90s and something similar happened to multimedia applications and that's why Intel engineers thought ok it might be a good idea to you know incorporate in your is a specific instructions that can operate on multimedia data right and that's what essentially MMX means is a multimedia execution so that's where the acronym is coming from in the example here as you will see that actually the example is in the paper as well what you will see is a very simple way of doing image overlaying which essentially it's like you know like in when you watch in the TV the weather forecast you you see the the forecast woman there and at the back you you will see the the map of Zurich or Switzerland or or whatever so that what they really have is a chroma and later in the post-processing they replace all the pixels that are green or blue or whatever they replace them with the actual in this case is a this kind blossom background that in the case of the weather forecast it will be like like the you know the map or whatever so essentially what this computation does is going one by one through each of these pixels and say okay if the pixel is blue this means that it belongs to the background and if it's blue what we are going to do is replacing in the output image I'm going to replace that pixel with the pixel from the map or the pixel from this blossom background and if it's not blue it's because it's a woman so if it's a woman then i simply copy the value of the pixel to the output and that's something that can be done in a Sindhi machine right why is that because they're computation that we need to do here it's exactly the same one for every single bit and yeah you can take a look at this by yourself you will see that first it's a kind of mask generated and then by using this mask we are able to remove the pixels from the background and replace them with the actual map or blossom background or whatever you want to put in the background okay so this is the MMX extensions to do the total from from inter machines and now we are going to start with the GPUs first thing here is GPUs are I already mentioned that they are a combination of vector processors array processors plus fine grain multi-threading very quickly remind you about why what fine grain multi-threading means you probably use to you know to see a pipeline like this right this can be a pipeline observe that here you have instruction fetch and and here you have an instruction register and then you have some control logic that reads this instruction and generate some signals or some micro operations to you know to tell all these the rest of the components you need to perform an addition or you need to store this in memory and so on right and and here we have a register file where we store the values the contents that we are operating on and after that we have an ALU and after that we have the memory access and so on right so each of these stages is typically in a different pipeline stage right and within these stages what we have are pipeline registers having between these stages so if you think about very simple inorder CPU what we are gonna have in each of these pipeline stages are different instructions that are at every point in time and every cycle they are in a different phase of their execution you are for every instruction you are either fetching it from memory or either decoding it or either using the ALU and so on so in the pipeline we are able to achieve some sort of parallelism because if we have instruction level parallelism we can start the execution of an instruction before the previous extraction instruction has committed has completely finished its execution right but the problem with one of these pipelines is that there are instructions that are dependent on each other recall that we mentioned that in the beginning of the lecture for example if you are if you have a load if you have an access to memory when you reach to this stage of the memory access if it turns out that the data is not in the cache then you need to go to the RAM and this takes much longer right and because this stage may takes much longer if the instruction that you have after that it's for example an addition that needs to use needs to use the data that you read from memory then you will have to stop you will have to wait until you bring the data from memory right and that's a problem that we have in one of these simple pipelines so one thing that we could do is say okay because you know these days we have a lot of parallel programs we want to have a machine that is able to run multiple threads at the same time maybe these threads belong to the same application or not maybe they are simply different processes in this example for example we have four so if we have four threads and we need to leave the execution of instructions coming from these four threads into this pipeline when we have to stop because for example we have a memory access the next instruction the instruction that is in the ALU belongs to a different thread and if it belongs to a different thread then it doesn't need to stop right and probably the next instruction that will belong to the same thread is not yet issued is not yet executing and when we issue an instruction when we start executing this depending instruction we have already finished with the memory access so that's essentially what the fine grain multi-threading means and this is something that we can find in CPUs but we can also find in GPUs as as we will we are going to see okay let's start with the GPUs they are Sindhi engines underneath that's the first thing to take into account they are a combination of vector and array processors but they are special in the sense that they are not program in the traditional Sindhi programming way with these vector instructions and so on they are program using threads such that we can write code for each of these threads individually but for sure it doesn't make sense to have like very different programs you know for each of these districts we will likely have the same program for all of them and that's what we call single program multiple data single program multiple data is a programming model the name itself itself means right we are gonna have a single program that is going to operate on multiple data elements the way that this program is later executed on the hardware will depend on the hardware for sure I could write a single program multiple data program and execute it on multi-core CPU but what we are going to discuss here is how to execute these on Sindhi machine which is a GPU underneath so that's why first thing to do is to distinguish between what we call the programming model and what we call the execution model because one thing is the hardware so the software and the other thing is the hardware so programming model refers to how the programmer expresses the cold and there are different ways of doing this one way is writing a sequential program in C for example or in Fortran you just write a sequential program another possibility is to write a data parallel program for example using vector instructions or data flow or a multi-threaded program using openmp or P threads that's the programming model and for GPUs we have CUDA and we have OpenGL which are the programming model for GPUs and then the execution model is how do we actually execute this program on the hardware that we have and this will depend on the actual hardware maybe it's an out of order processor may be subject or processor may be summary processor may be so data flow processor etc so the execution model can be different from the programming model for example for Noah model from normal programming model can be implemented on an out-of-order processor you are familiar with out of order processors right in out of order processors we don't need to follow the exact sequence of instructions at this is defined in the sequential program but in the sequential program there is a sequential execution right and the way that these instructions are executed on the hardware depends on the hardware and for example in another for the processor is two instructions that are one after the other are completely independent they can be issued at the same time or almost at the same time and they can be executed in parallel so observe that we are doing concurrent execution parallel execution while our program is completely sequential you understand the difference right and in GPUs what we are going to do is exactly the other way around we are going to write a program for each of these hundreds thousands of millions of threads that we that we want to run on the GPU but the way that this program for each of these threads is going to be executed is as if the GPU was Cindy processor so we write Pro will write code for individual threads and later the hardware is going to collect these threads and put them together in Cindy lanes that are next to each other and these threads in groups of 32 threads for example which is the vector laying vector length in NVIDIA GPUs will execute in parallel we will see more example we will see examples with some detail here ok how can you exploit the parallelism here do we have parallelism in this code yes right we have parallelism in this code we have a for loop where each of the iterations is independent so it's something like this this is colored sequential code that we are writing there that now we can see how we are going to execute these on different machines what happens if for matching is sequential or if it's data parallel like a similar matching or if it's multi-threaded like a minty or SPMD so first of all sequential machine CC so well okay we will execute it like that right but depending on what the matching is indeed in my we ignore their pipeline processor or it might be an out-of-order execution processor right so if it's an out-of-order execution processor we can't have execute we can't have independent instructions being executed as soon as they are ready and we don't really need to follow this sequence of instructions that we have written in your program right so in to some extent what the out of order processor is doing is kind of loop unrolling the out of order processor for example could see that this load is completely independent of those two loads right so it could issue these instructions at the same time or almost at the same time and this load and that law and that law might be might be serviced almost at the same time why is that because the out of order processor operates at that way so it's a it's a way of extracting instruction level parallelism from the code and executing instructions in parallel on the hardware and we could also I mean for example use a superscalar V liw machine as well right for example these instructions here and those instructions here are independent we could pack them in the same very long instruction work okay that's for CSD now for Cindy well the realization here is that each iteration is independent of each other so if they are independent we can execute them in parallel right we can have something like this and we can pack these two loads into this one single instruction which is a vector instruction and then and that's all vector is called and then these two loads is another vector instruction and then these two add is a vector add and then these two is a vector store right and we can do that because even though word program was sequential but we can have for example a compiler that detects that these iterations are independent and and then this compiler generates code for a sim D processor for a Sindhi machine and the third possibility is that because the each iteration is independent we assign each of the iterations to a different thread observe the difference one thing is talking about lanes in a Sindhi matching another thing is talking about threads in a multi-threaded machine right you see the difference right the the important difference here is an EDA finding matching if we want the 16 guys or the 32 guys run really at the same time they really need to be doing exactly the same thing right because it's Cindy single instruction but if it's a multi-threaded machine we have multiple programs right we have multiple threads and each of them could potentially be doing completely different things but in this example know in this example they are doing exactly the same thing and this is what we call single program multiple data meaning that we have multiple threads these multiple threads are going to run the same program on multiple data and this is what we do in GPUs these can be executed on a Minda machine this could be executed in a multi-core matching but we are going to execute this on a single machine in on a GPU okay yeah related concept is this single instruction multiple thread it's more or less like the same thing it's it's more like nvidia terminology so a GPU is a Sindhi or simply machine that is not program using seeing the instructions we just program it using threads and then we will have the hardware later that will take these threads and put them together in a Sindhi unit and execute them all at the same time so these groups this sorry threads that are grouped together are called a work or a way from in AMD terminology at the very end of the presentation you will in kind of table that I prepare for you know this terminology because there there is something like this the official terminology of the of the lectures which is let's say like band or agnostic and then we have the terminology for the different vendors right that's why we included that table at the very end for you to clarify any possible confusion okay so a warp is essentially a seeing the operation form by the hardware so instead of having on a GPU instead of having one vector instruction for these two loads what we have is one warp that is going to execute these two loads or in reality 32 loads at the same time because a warp consists of 32 threads but in the end the way that they execute orders on the hardware is exactly the same as a seemly machine because they share the same PC for all these threads that belong to the same world by PC I mean the program counter right which is the address of the instruction to be executed the address in memory of the instruction to be executed okay then after warp zero is done with the loads then work one goes to the next load and then it goes to the addition and then it goes to the store and this is how the program is going to execute okay so graphics processing units are Sindhi processors not exposed to the programmer cindy versus in the-- i think that we have already you know talked about these you know where it's indecent is essentially the same as single program multiple data as we have already described and two major advantages of this type of machine with respect to conventional Sindhi processors vector or array processors and are these two things that we are going to discuss separately so the first thing is each thread can be treated separately we can execute each thread independently you know so it is possible to write code for each thread individually I told you before okay we are going to have one single program for all the threads to operate on the entire amount of data that we have but if we want to have threads diverging that's possible as well I could say in the very beginning of my program I could write if you are threads Eero do this if you are thread one do that that's possible I can run that program on a GPU and the GPU internally is just like a Sindhi processor of course that's not going to be very efficient right because if you have a Sindhi processor and every time that you launch any instruction only one of the I mean you're operating only on a single data element then you are wasting VLAN minus one lanes right and you don't want to do that but you can do it if you want so yeah let's go back to the to the example here assume that the work consists of 32 threads so if you have 32 thousand iterations and you are going to sign one of these iterations to each thread you're going to need 1,000 1,024 depending on what the k-means warps now we are going to have warps executing these instructions these load instructions how many were one thousand warps right and how are we going to execute these warps do we go first all the instructions for warp zero and then all instructions forward one and then all instructions for work - can we do that that could be one possibility right but the problem if we do that is that these instruction here is depending on these and depending on these so we cannot start the execution of this instruction until these two have completed right so what do we do in the meantime what we can do in the meantime is scheduling instructions from different warps recall that we have 1,000 warps right so we will have to execute each of these four instructions 1000 times for the 1000 warps right so instead of first do all the instructions for warp 0 and then all the instructions for one work 1 and so on what we can do is interleaving instructions from different warps because they are completely independent of each other right there is no dependence across warps there is dependence across the instructions of the same world so that's what we do that's why we call we talked about trying grain multi-threading of words and that's why I briefly reminded you what multi-threading fine grain multi-threading means in those slides that you can take a closer look by yourself so now if we have a you know or Cindy units there or Cindy processor where we want to execute these words we may want to first issue the this instruction for warp one and then this instruction for warp 20 obviously if we do this is because we already executed these and these forward 20 right otherwise we could be doing like crazy scheduling and we really want to schedule things in the right way so and after that we might want to know scheduled this instruction for warp 2 and then this instruction for wire 26 ok ok so yeah so this is essentially how the different words are scheduled on to the Cindy pipeline so for example here you see next one is warp 7 warp 7 but which instruction okay for example instruction 0 so we start the execution of instruction zero for web saving and right after that we say okay what instructions are ready and it turns out that instruction one for work eight is ready because the operands are ready right and if the openings are ready that can be the next instructions to the schedule the next instructions to issue make sense okay and here we have a you know nicer picture of the internals of the GPU and here you can see this is the Cindi execution unit we can't talk about scholar pipelines but each of these is colored pipelines is only a sim delaying a back to lane so this is one lane this is another lane this is another lane and threat zero will execute here thread 1 3 2 3 4 and so on okay this is very simplified this is a little bit more a little bit more detailed and you can see the entire pipeline here this is instruction fetch decode here we have the partition register file we have our vector registers right and then we have the ALU that can execute addition multiplication so long and then we have a data cache and then sometimes we will have a means right if we Pham is what happens we have to go to the next level of cache l2 GPUs typically have two levels of cache l1 and l2 so if it's nothing l1 you will have to go to l2 which is shirt this is one single simply pipeline you will typically have multiple of these so each of them has an individual or private D cache but then they have a shared l2 cache and now you go to l2 and you also mean so you have to go to the RAM and now you know how much time this takes hundreds of cycles maybe 500 cycles which is a lot right so what can you do in the mean time because these memory access was for for example for what 1 what can you do in the meantime are you going to install the pipeline and keep these completely idle while you're waiting for the values for no you can execute other instructions for example arithmetic instructions which belong to the execution for other works and you can be using the pipeline in the meantime so those are the benefits of this fine grained multi-threading with fine grained multi-threading we are able to tolerate long latencies for example like access to memory because while we are accessing to memory for one warp we can be executing instructions for other warps so this is what is called the latency hiding mechanism it works it works well for sure but it also depends on on the workload itself right if the workload has a lot of data accesses and very little amount of computation in the end you will have to wait right and that's actually the main bottleneck I could say in in real GPUs ok you remember this is slide right now we don't talk about Cindy vector or array processors now we talk about GPUs and now this instruction is not about a vector add is the add instruction for one word for example warp zero and it turns out that warp series composed by 32 threads so depending on how many of these functional units do I have depending on how many of these lanes do I have I will schedule them it will schedule the different operations in a different way if I have one single lane then I need to first see zero then compute C 1 then compute C 2 then compute C 3 and this is what I would be doing for the 32 threads that belong to this warp if I have four of these lanes then I can operate on four of them at the same time you also remember this one right now we still call them lanes and on each of these lanes we run different threats you see so we write or code for 1 million threats and on each of these threats has a threat ID and the condom in the values that threat zero needs to operate on will be here in these registers the values for thread 1 will be here and so on and you also recall this a slide right we were talking about vector instructions we don't talk about vector instructions now we talk about warps and now our words have contained or are composed by 32 threads and we want to execute for example a load or a multiply or an addition for these 32 threads but we only have eight lanes so the way we schedule the 32 threads of the warp into the eight lines is by doing it in four cycles but observe that conceptually it's exactly the same as as we have seen for vector processors now instead of talking about vector instructions we talk about instruction issue for warps okay any questions here okay yeah so this yeah so this is like a little bit more of it right so how do we distribute the computation here we have 16 data elements and for these 16 L data elements we are going to I mean to use 16 threads in this case we assume in this example 4 threads per warp so what the hardware does we write our program right we write our program with 16 threads and we tell to each of these threads you need to operate on these on these on that and then what the hardware will do is say ok I have 16 threads and I'm going to you know group them in groups of 4 and each of these 4 is going to be a warp and when we execute the instructions corresponding to these 4 threads are going I'm going to issue I'm going to launch these instructions for for the entire work at the same time right so that's more or less what we have already seen in the past and there in the previous slides so if we want to write GPU code we we will still be using the CPU we will still be writing serial code that that's for sure so GPUs essentially are coprocessors we have GB using or laptops desktop machines in or in your cell phones but you know we still have a CPU and that's because we we still need it right because there is there has a lot of workloads that can be run in parallel in parallel machines like GPUs but but still there are serial or sequential parts of the code that we need to execute on a regular CPU so what we do in this case is we write our program in somehow this way we will have some serial code that runs on the CPU and at some point we reach the parallel part of the execution and what we do is calling the GPU bye-bye this is this is kind of similar to the actual syntax in in CUDA so here what what were essentially doing is calling a function that is going to run on the GPU and at some point in this function we'll finish the execution and then we have more sequential code and then we have might have another function these functions executed in the GPU or device observe that we call it here device are called kernels and for each of these kernels we launch a number of threats and the way that these threats are organized from the software perspective from the programmers perspective is in blocks you see so these are blocks of threads in total we might have 400 block 400 threads but what we are launching here is for example 4 threads of 100 so sorry 4 blocks of 100 threads each write observe one thing this block is a software concept is independent of the warp but we will see what how do they connect to each other we're going to see that in the next in two or three slides and this is a very very simple GPU program CUDA code this is the CPU code here observed at the secure element wise additional you're pretty familiar with that after yesterday and and here is how do we implement this for a GPU so what we write is essentially the same as we have there right the body of the loop is this addition and we also have the addition here right I mean the only difference is that ok we are separating it in some some way no so we this is in memory we go to memory and we store in this variable this this will be stored in a register actually we go to memory store in a register and then we go take the contents of the two registers at them and store in the output position and observe that this memory accesses are indexed by this T ID and this T ID is computed here is the thread index inside the block block index times the dimension of the block you see right remember in the previous slide I was saying okay you are the programmer and you're going to write a program to launch for thread for blocks of 100 thread each right so this is going to be you have 100 threads per block so this is going to be a number between 0 and 99 and then you have 4 blocks right so this is going to be a number between 0 and 3 and the size of the block this block team is 100 and this way I can access the 400 elements that these two arrays a and B have this is how we write the program but observe that the product these are not vector instructions right these are scalar instructions right and here in the very beginning I could write if T ID equals 0 do this if t ID equal 1 do that and I could have completely different programs I mean it's actually the same problem is one single program but completely different execution paths for the different threads you see the difference already right between CUDA programming and GPU programming this is a little bit easier I would say it's a little easier to write it's difficult really difficult to optimize that's for sure ok here you have the same thing but here with a little bit more details you can take a look at this by yourself so this part here is the kernel is a GPU function and this part here is the thing that is wrong on the CPU because the GPUs are coprocessor we start the execution on the CPU and when we reach this kernel call we call the GPU and execute this onto the GPU okay and now we will be finishing soon but let's clarify something that is very important here what's the difference between or how do relate blocks with works because observe I started the explanations talking about warps and talking about how these warps relate to the Sindhi lanes that we have inside the inside the GPU right but after that I started talking about the software and I said oh we are gonna write software for four threads and the programmer organizes or arranges these threads into blocks and now the thing is okay if we're if I'm if I'm writing code for threads and blocks how do these mapped to the warps right well actually it's a it's quite simple essentially what we do when we write or what the hardware does when receives the code that needs to execute for a block of threads this is one so called stream multiprocessor in NVIDIA architecture but essentially for us this is like a Sindhi pipeline okay or app let's call it cindy compute unit if we want so when when this cindy pipeline receives the program for one particular block what it does is dividing the block into works so if the programmer said oh each of these blocks is going to have 100 threads what the hardware does is dividing these hundred threads into groups of 32 threads each of which will be called a warp or assume the unit of 32 threads so now you might be thinking oh how do I divide 100 threats threats in groups of 30 - well obviously 100 is not a multiple of 30 - right so the last warp will have some unused threats or unused lanes right so that's like one of the first hints programming hints but for people who who learned to program GPUs is you always use threats that are multiple you always use blocked with the number of threats that is a multiple of 32 because you really want to use all the lanes for the works for running that are available for each of the warps right and you can take a look at the and at the picture here and see how I mean the different elements that we have inside these streaming multiprocessor and recall what we were saying before GPUs are a mix of vector array processors and finding rendering multi-threading processors right vector processors why is that because each of these lanes is pipeline and we can have I mean yeah so I was saying vector right so in vector in vector processors we have different types of units so for example we have these SPS that execute integral and floating-point operations like additions multiplications and so on and here we have these other units that are specialized for load and stores and this we and then we have this SFU which means a special function unit that can execute other type of operations more complex operations like transcendental functions trigonometric operations for example so you see different types of units that each of these units my might also be able to XR is also able to execute different instructions so to some extent they are like re processors and like vector processors and also they are finding multi-threading because here we have the warp scheduler and the dispatcher unit that are continually continuously every cycle go into the instruction cache and say ok this instruction for warp zero let's start executing it this instruction for work one let's start executing it and so on okay good yeah so work base in D versus traditional Cindy this is just like comparison between traditional Cindy and work base Cindy I think that we have more or less gone over all of this you can read the slide more carefully and also remind you as well what the SPMD means single procedure or program multiple data we are writing one single program that is what all these threads are going to run but to each of them as is are going to operate on on different data elements at some point they they can also synchronize this is something that we will probably talk about when when we continue with GPU programming and yeah essentially multiple instruction streams that execute the same program and operate on different data and the second thing just very very briefly before we finish is another advantage of the Sinti architecture is that we can group or at least potentially we could groups group threads into work flexibly this is something more it's more like a research proposal rather than an actual implementation on real but it's it comes from a very reasonable concern which is that you don't want to I mean you you in principle you want to avoid it you already know why I want to avoid it like you know separate different control flow paths for each of the different threats even though you can write that you can write your program in that way but you you in principle want to avoid that and why is that because in the end they are only like Cindy pipelines and when you reach did you know a divergence point like here this is a branch and some of the threats are going to be through path a others through path B and when this happens what will happen is that you are issuing instructions for the same work twice right instructions for paths a instructions for for path B how do you really implement this it's a simile machine so simply as a vector or or array processor right so it has a mask so in your mask when when going through path a this is one zero one one zero zero zero one and then you complement and execute path B and then this is 0 1 0 0 1 1 1 0 in the vector mask right problem with this I mean the good thing is that it works the bad thing is that we are wasting some of the lanes right so we are not exploiting all the throat put that the GPU can give us ok so what was proposed by yeah some paper you will see the title of the paper somewhere is to dynamically merge threads such that we have threads that were originally going to belong to different words but the hardware says all I have divergence here I have some threads here that are not going to execute but I also have another work that has some traits that can merge with the other ones right could execute at the same time so what you do is that you compose what they Harbor the work schedule it does it that composes like a artificial warp in this case and you see that this warp Z contains threads that originally belong to war Peaks and work W and this way you can have a more efficient execution that you have a nice example here that this is the title of the paper dynamic war formation is scheduling for efficient GPU control flow and and here you can see a very nice example pretty simple as well and this is another example we might assign you this for reading you will you will have a chance to read this in detail but let me very briefly introduce we have already mentioned like two main bottlenecks in GPUs one of these is the divergence that we have just discussed the other thing is they access to memory for example there is no latency operations and in this paper they try to deal with both so with respect to so this is like the typical default scheduling that we have in GPUs is round-robin so you first schedule instruction for warp 0 then for work one then for work two and so on and even though you can achieve some latency hiding not so much if you if you see this timeline here all threads all all warps or start computing arithmetic operations and at some point they need to access memory and this takes a lot of time a lot of cycles right and and you can not resume the computation in inside the pipeline until you have obtained all the data from from DRAM and yeah of course there is some latency hiding here right because we are somehow overlapping a little bit of computation with a little bit of data as access but not that much why is that because there are many accesses to memory here so this is I mean it's made the main bottleneck so what in this paper they proposed is to have like larger warps and then schedule like kind of soup works that are dynamically formed for example this is the entire work this is the mask and instead of you know scheduling these four and then these four what we do is ignoring those for which the mask is zero and we only schedule those for which the mask is one so if we do that I mean this is for this is the way of reducing what no this is for branch divergence is not for the memory access but you see more or less how it works it's kind of similar to the previous technique and for the love latency operations that we do is like a two-level work scheduling they organize the warps into smaller groups such that they you know they first scheduled round-robin inside a group and then schedule run roving across groups and this way as you can see it's quite clear you can achieve more overlapping right than there you can achieve more overlapping and save execution cycles Indian and this is the paper and because it's more than 4 p.m. I think that we are going to finish here there are a few more slides probably if if I have a chance to give you a GPU programming lecture another day we will probably start with this one's very quickly yeah and yeah let me know if there are any other questions or maybe yeah we can finish and as questions in Moodle or or whenever you want okay thank you very much for your attention "
bMKzi6aEBbE,27,"This is a lecture video from the Hardware/Software Interface class, which examines key computational abstraction levels below modern high-level languages. From Java/C to assembly programming, to basic processor and system organization. This course was taught on Coursera by Gaetano Borriello and Luis Ceze from the University of Washington.",2016-09-08T05:36:41Z,"Architecture, Video 1: Overview",https://i.ytimg.com/vi/bMKzi6aEBbE/hqdefault.jpg,Luis Ceze,PT14M52S,false,5385,87,1,0,4,"so we just went from a guy to know how to encode integers and floats and now in this new section we're gonna start looking at machine code okay so by machine code if you recall we saw in the first section are this zeros and ones that tell the computer what to do when executing the program okay and that's gonna we also going to start looking at an introduction to assembly language and what you have to understand in terms of basic machine organization in order to learn about assembly program and learn how a computer word works down a little bit lower level okay so here's what we're gonna see this section we're going to see what an instruction set architecture is which is the hardest software interface it's the contract between hardware and software we're going to look at briefly at the history of Intel processors and architectures and then we're going to see how seeing assembly code machine code relates and it's gonna start looking at x86 basics so we saw this picture in the very first section and if you recall this is the lifetime the lifetime of a program starts with coding time when you write the code compile time when you translate from the high-level language all the way down to machine code the zeroes and ones that compose what the computer understands okay and in the run time which happens when you're actually executing the program okay and also recall that what makes programs fast is well your algorithm how you write your code okay what the compiler does not know is how the compiler translates from C code to the to the hardest software interface to the ISA the instructions that are chosen by the compiler and then they are Center converts that's as yours and once two months of machine codes and then you know the opinion how do harder execute it it's going to make your program run faster okay so just just to repeat what I just said you know the the time required to execute a program depends well in the program itself what algorithm you've used how the compiler translates that's code written by the programmer into assembly instructions it also depends on the instruction set architecture which is a set of basic instructions offered by the processor to the compiler or to whoever's writing assembly code and then ultimately of course the program performance is also a function of the actual Hardware implementation how's the harder' itself how the heart itself is organized to execute those instructions okay so now let's a little bit more time listening exactly what the ISA is okay so the ISA defines three things three important things the first one is a system stage or defines the state of your system okay of your computer system that includes the contents of registers which is as Gaetano told you you know a basic unit of storage inside the CPU itself okay it includes the program counter which tells what instructions being executed at that time in in a processor and also it includes all of the contents of memory everything that's stored in registers memory in including the program counter is part of the system States okay so the ISA also defines the instructions that you execute that the CPU can execute in the most basic level think of that as very very simple instructions things that do like an ad or a subtraction or say a load from memory okay the gets data from memory into a register as another time an example of an instruction okay and finally G is a also defines what exactly happens when these instructions are executed because one of one of these when one of these basic instructions are executed it changes the state of your computer system okay so and how this state changes is function of also function of what the ISA specifies okay so here's our major decisions when designing an is a first or obviously what instructions are available like add subtract multiply cash and floating-point operations and so on okay and what do they do exactly how they change the system state and also how they are encoded because if you recall when say you have an add instruction in assembly it becomes some sequences of zeros and ones that's that's what the computer actually understands and the encoding is exactly this mapping between the instruction in the ISA and the actual base sequence that determines that instruction that that that encodes that instruction so the ISA also tells how many registers your computer has you know how many you know explicit units of storage you have inside your processor ok so how many you have available and also how wide they are right for example are they 32 bits are they 64 bits or they smaller like in case of very simple processors and so on okay and finally the ISA also defined something very important which is how do you specify a location in memory okay so how do you specify an address in your assembly code or how in your in your instructions okay how do you tell instructions what addresses they should be manipulating okay it's also called you know what are the addressing modes supported okay so let me write it here addressing modes the all the ways that you can specify an instruction in an address in your program so now x86 is one type of is a okay so it's very very popular okay so that and in fact it's popular that x86 which is you probably know intel intel corporation makes x86 processors or processors that implement x86 is a and this process was completely dominate the server desktop and laptop markets for now and they actually had a very evolutionary design now in fact if you have code written for daily 86 all the way when he was design in 1978 in fact very close when I was born so code reason forget it for the act the 8086 still runs in the most modern ink towel x86 processors okay so we had very very evolutionary design and more features were added as as time went on in x86 is a is something that we is a type of assay that we call Sisk which stands for complex instruction set computer ok well that said is that the instruction duvets the basic instructions specified by the x86 is a are very rich ok they can do things like you know move us move a string in memory they do things that are much more basic than just a simple add the subtraction remove a little bit of data here and there those things are much richer than that ok so and we call it Sisk and as in complex instruction set computer as opposed to risk which stands for reduced instruction set computers ok a RISC machine has much fewer instructions and instructions are much much simpler ok so in more advanced computer architecture course as you see that this has a lot of implications of how the hardware itself is designed ok but it's a little bit out of the scope of this class to give you the details of why whether and I say being risk or sisk how why and how it affects computer design so let's look at some numbers you just to give an idea of how amazing this evolution was ok so if you go back to 1978 when the 8086 was introduced that processor had 29,000 transistors and it ran on Europe between 5 and 10 megahertz that's the cycle time of the processor known between 5 and 10 million cycles per second ok at that time there was the first 16-bit processor the needed award size was 16 bits or two bytes and there was the basis for the I'm the IBM PC and dollars that really popularized personal computer that's a very very important processor in the total address space the maximum memory you could you could address was one megabyte can imagine that he can't even hold the photograph but and then another major step was that the the 386 that was introduced in 1970 later in 1985 and he had 10 times as many transistors he ran about no three times faster in terms of its clocks frequency but it was significant for a number of things ok it was the first 32-bit microprocessor ok he's referred to as the ia32 he had a flat editing mode meaning that you didn't have to divide the men the memory into regions called segments and address them individually you had the single address that could point anywhere in memory so that made the process of managing memory and accessing memory much much simpler okay so and this machine is also capable of capable of running an operating system called UNIX which is very similar so we call Linux today okay and so it's very some Linux is a type of UNIX operating system okay and out in today 32-bit linux GCC targets targets the 386 by default so 3 seems a very very important architecture as well now let's jump 20 years later and look at what happened now we know the Pentium 4 when when he was introduced in 2005 had 230 million instructions we're talking about at ten thousand ten thousand fold increase in the number of transistors okay and you know a sim in about about a huge increase in frequency as well okay and then the pension for F was the first 64-bit processor was also introduced by at first 64-bit x86 processor introduced by in town it's also referred to act 3 664 and then Lee was actually involved in in defining what the x86 64 looks like as well which we're going to see in a second ok so just a little bit more history of Intel processors the 486 was introduced in 1989 was also very important processor then there was a series of Pentium process they had they had a lot of evolutions including like around this time they were starting to worry about power consumption as well so and then there was a Core 2 Duo you know which was a dual-core processor first I had multiple multiple computer cores in the same chip and then more recently we had the i7 that for example here we have a diet because as the picture the actual silicon of the i7 and this had 4 cores it is essentially you know these are exactly each each one of these boxes here ok are each one of these boxes is a processor and you have four of them on a chip and they all share a bunch of cache memory that we we don't see exactly what that means in the later sections of this class okay so in along this history there's a lot of other things that were introduced along the way one was support for multimedia operations essentially multimedia operations tend to have a lot of fine-grained parallel to the instruction level we call that vector type of parallelism so and you ought to take advantage of that Intel introduced a accession to the in science a called Cindy which stands for single instruction multiple data in that that that instruction actually operates in multiple data items simultaneously so take advantage of parallelism that the application that the application has okay so and also along this way like in this is a evolution not only I see an architecture evolution there instructions that were introduced to do more efficient condition operations okay we'll see later in this class that this is instructions called branches that changes the flow of execution the processor and this instruction can be very very expensive so in part of the innovation happens in this evolution of microprocessor was to make some of that much cheaper and then obviously as I mentioned before there are lots of course introduced along the way ok so if you're interested in learning more about the specifics of of Intel's architecture you can go to the automated relational knowledge base great name that's available in Intel website and also you can look at the list of Intel microprocessors on Wikipedia which is great to get more historic information as well but now let's not forget about AMD a company called that Advanced Micro Devices that made clones of x86 by clones it means that other processors they had a completely different hardware but also implemented the same instruction set architecture that's one example of the beauty of abstractions since had the same abstraction called instruction the instruction set architecture the hard SATA interface was the same the implementation underneath doesn't really affect software as long as the ISA stays the same and so II AMD came up with completely different hardware that implemented x86 is a and it had some advantages at first you know there were a lot cheaper doing the tiny bit slower but it made sense in terms of price performance okay typically just followed be hiding tell but then AMD got really really aggressive in terms of taking advantage of new super design techniques they designed very very very fast processors and you know one great example is the AMD upturn which is really really tough competitor to the Pentium for an AMD was also instrumented it was also instrumental in developing the x86 64 which is the 64 bit extension of of the x86 I say that later on actually became a good part of what x86 64 looks today even for Intel processors okay that's really really interesting you know quote-unquote collaboration between two companies are essentially competitors okay so now to to end this the module the first module of this section just to talk we are going to cover now this section in in Exodus 6 program which would be the following section we're going to look at what with what's called ia32 it's the traditional 32-bit 32-bit is a int 3 here 33 32-bit is a and then we're also going to talk about the 64-bit version of the x86 is a ok and keep in mind that all have assignments actually going to use x86 64 but when teaching you assembly programming it's going to be largely transparent to your week you're going to know when it matters okay thank you and see you soon "
BPXmRZSyLrw,22,,2020-07-07T16:19:03Z,Computer_organization_Ch1_Introduction_part_1,https://i.ytimg.com/vi/BPXmRZSyLrw/hqdefault.jpg,Dr. Bassam Jamil,PT18M1S,false,1567,5,0,0,0,hi this is the introductory chapter for computer organization class and we will review key concepts and before that this is our textbook it is a computer organization and for for the famous authors David Patterson John Hennessy and Morgan Kaufmann the the important things we have reviewed this the introductory class the important things is that we would like to discuss key or let's say include introductory concepts one of them is the the we need to point to the computer revolution and this computer revolution basically computers and digital systems they are everywhere in our lives in the automobiles and the cell phones in the internet the search engine etc etc so to sum up computer computers and computing systems pervasive everywhere in our lives there are different classes of computers and digital systems you can classify them to s tabs servers impaired systems and basically you can say that the servers are the faster machines behind high-performance machines a desktop and laptops kind of the personal usage for students and small organization and embedded systems on the other hands and embedded computers are in the are those digital systems that are in a system with is not called a computer so for example your microwave and washing machine the automobile whatever digital system is there excuse me the point is all these are different classes of computing systems where you can look at the server's as they as the high-end fast machines the the embedded computers or image systems are those in systems that help sort of automate the the the operation of the device at adds to it the automation it might activate some artificial intelligence etc okay needless to say that the market for the processors have been going through an exponential growth and obviously propelled by the need for electronic devices PCs etc so you know you can see the need for understanding the computer system okay and in in this course we discuss how we translate high application programs into some kind of machine language we did this in earlier but we're gonna point to a bit it's one more time we are going to study this hardware software interface with very important concept this is you're basically about that you have the software world below that you have the hardware so that's that's that interfaces is important and some people refer to this as the instruction set architecture and that's the interface between the hardware and the software we will look at key metric for performance and in this course speed or time CPU time is a key metric but I must say that in active systems people now pay attention to other metrics they are really key metrics for examples energy energy consumption and critical for might seas that are sort of implanted in the human body for example or that are placed in hard-to-reach areas like for example outside cities where they kinda kind of do some monitoring or sensing so these for these systems energy is it's very important sometimes a metric is used this is the cost okay you want to have air system computing system with decent performance but with the lowest cost so that it can be volume numbers okay so in this course however we will refer to the performance to just the the CPU time and we will limit that to the performance just to the CPU time okay of course we are going to focus on how hardware designers can improve the performance that is CPU tombow gonna take on some but a but a parallel processing at the end now if not improve performance and by the way this applies to CPU time and power and all that you really need to look at the entire picture here and and and that includes the algorithm that you are running includes the software side the programming language pilots and the processor and the memory system okay while we are going to focus on this right is course implant Pio systems as well but in reality people have to worry about the entire stack which a promise now look at when we write programs those programs typically in c-sharp C++ Java as application software and they are kind of translated by compilers and assemblers - okay what should this machine language it writes the hardware drives the hardware so this is this is our this is our application software and then you have system software's like the compilers in the assemblers which translate to machine language that drives the hardware so this is kind of your the way you look at how things okay in terms of hardware we need to worry about the processor and memory okay and the i/o controllers because they kind of drive the yeah they are our hardware now of course as we mentioned in the earlier slide that you are you have this high level language translate it to assembly and assembly translate to machine language and this is where your hardware understand this language the hardware does not understand C or C++ or Java or even assembly it rather understands the language of ones and zeros and from there okay appear you have to kind of apply this algorithm to compilers and assembler so that you can do one one thing interesting is that in the early years of computers they used computer they used to program computers with ones a zero stuff got very tedious and boring so people invented assemblers and compilers to make it more human friendly okay so this is kind of your level of program code if you will high level assembly language and machine language which very straightforward but the computer system the major components of any computer system you can see the computer system consists of inputs like for example your keyboard okay and output like for example your your screen monitor okay another input by the way is the mouse sir another app is the speaker for example inside the computer system you have you have the processor which consists of the control section and a tap a section and control section sort of orchestrates how the things happen inside your computer system and the data path basically it takes in data crunch the numbers and produces the results okay in in this course our focus will be on the data path and control so so basically this this processor ok side so this is where the processor which consists of data path and controller and we will also at the end of the course visit the memory okay so this is the memory right so the memory basically states the data that comes in the input and saves the results that is going to the output okay so the me the processor and the memory will be the focus of this course okay so here are here more of picture of the anatomy of the computer's so input and output devices okay and then what we will do is that we're going to talk about the CPU both in terms of the data path and control some old processors you can see how they are they are organized we are going to talk about coming lectures about the instruction set architecture which is the interface between the software part and kind of applies the knobs required to drive the machine underneath in the hardware technology trends you can see that how the the its capacity for DRAM technology versus years and this is the growth is exponential notice that the y axis is logarithmic okay and and the point is that that we are we are packing more transistors per per per single year and in fact this is this graph here if I go back here this graph is driven by something known as the Moore's law and Moore's law states that the number of transistors packed in an IC increases or doubles every 1/2 years yes so that's that's one of the fundamental in computer hardware we're going to need to understand the difference between response time and a throughput and basically the response time it is how long test that's that's the response time and while the throughput is it's the total work done per unit time so response time is really very important if you are dealing with real time applications so for example if I am calling somebody in a different country and we are we are having a conversation then the response time is very important so that we can have a meaningful conversation if I if it takes hello me to him a phone system or internet phone system is is not good for in for real-time application so response time it is critical for the real-time applications how about Rupa throughput is really important metric when you're having lots and lots of product to plate so for example if you're having a bakery right and in the bakery you are baking bread you want do as many of Peter processes as you could then you worry about the throughput you know that every pita bread will will will take this almost the same time if you do one piece or you do a million of them okay but in this or not you're not any the response time you are really concerned of how many you finish of the of these pita breads per second or per minute so tasks per minute per hour okay is is what what we care about for throughput so throughput as as I indicated earlier it leads itself it's a metric for processes which kind of deals with large number of items large number of data you're concerned about the volume rather than the response time is concerned about the quickness the speed so response time important for for real-time application throughput is important for if you are doing high performance for crunching lots of instructions lots of data okay so in this class we will define performance as one over the execution time so for example if you if you if you have if you have let's say time to take to run a program 10 second a and 15 seconds and B so the execution time of machine B over the execution time of machine a is 15 over 10 15 seconds over 10 seconds and that gives you a factor and that is really as you can see this is how fast machine a compared to machine its 1.5 times faster than B okay so so that's that's a that's the definition use the performance is 1 over the execution okay we have other definitions of elapsed time which includes all aspects includes processing and iOS an operating system overhead etc but really we're gonna focus in this class of a CPU time how much time it will take CPU to ensure your program and and this is what we are going to discuss but let's just before doing that let's just take a look about what we define as the CPU clocking in the CPU we have a signal called the clock and the keys in the system okay and so you would want to have some signal which kind of synchronizes or happens with this clock signal the clock signal has a period okay one over this period is the frequency of the clock okay as a definition alright and and this clock signal we will try in in this period to finish and execute as many things as we can before we go to the next period okay in general the shorter the clock period or the higher the frequency they mean the same thing the better the machine is and I say in general because the performance of a man of a machine CPU in this case depends on other things other than the clock the clock period or the clock frequency other parameters as we will see shortly okay so here for example it shows that the clock period and how we compute clock period based on a clock frequency so again frequency equals 1 over period and period equals 1 over frequency so our basic ones okay we'll stop here and continue our discussion in the next class or the next part of the introduction thank you very much 
H01UwFG1Fkk,27,by Ms Jisha Mary Jose,2020-03-28T05:03:08Z,CS202 Computer Organisation and Architecture  Module4  Cache Memory,https://i.ytimg.com/vi/H01UwFG1Fkk/hqdefault.jpg,Rajagiri School of Engineering & Technology,PT27M43S,false,655,N/A,N/A,0,0,hello students I am Deshawn Mary Jones assistant professor from the computer science department of Raja Gauri School of Engineering and Technology in this lecture we will be dealing with some of the topics from computer organization and architecture a course under K 2 you for the fourth semester CSE and IT students the topics to be covered in this lecture will be from module 4 cache memory the main contents that have been discussed in this lecture have been listed below in our previous modules we have already discussed the different functional units of a computer and this involved two main components that is a processor and main memory assume that some data transfer operation is to happen between these two components that is I want to send some data from the processor to the main memory or vice versa now what is the main issue that is going to happen here yes it is the speed difference the processor is is of high speed whereas a main memory is comparatively slow so what happens is the processor has to waste its time waiting for the main memory to complete a task this results in wastage of the critical time of the processor now how to solve this problem to solve this problem a very fast memory module was introduced and placed in between the processor and main memory this fast memory module is known as the cache memory the cache memory can thus be defined as an architectural arrangement which makes the main memory to appear faster to the processor than it really is and the main advantage of introducing this extra memory module is that it ensures efficient utilization of the processor time is achieved next we will see what is a working model of this cache memory suppose the processor wants to read some data from the main memory the processor will issue every request when this read request is received the data that is to be read which is available in blocks in the main memory will first be brought into the cache memory and from the cache memory the data will be provided to the processor now what is the advantage of keeping this data in the cache memory the advantage is that when a request for the same data comes in the future then we've known we don't have to go to the main memory to read this data again instead the data can be read from the cache memory which is comparatively nearer to the processor the time involved will be less now what is the concept of working behind cache memory it is based on a concept called locality of reference locality of reference is of two types it can either be a temporal locality of reference or it can be a spatial locality of reference the first one is based on the concept that the most recently executed instructions in a computer are likely to be executed again very soon for example take the case of looping statements the statements that are present inside a loop are very likely to be executed again next one is spatial locality of reference here it is defined as the case where the instructions with close proximity to each other or close proximity to recently executed instructions are very likely to be executed again soon so this is what is known as temporal and spatial locality of reference so how this concept can be applied in the terms of cache that is we can talk about the temporal aspect and the spatial aspect in temporal aspect we can say that whenever an information item is first needed or whenever an information item is acts by the processor first this item can be brought into the cache memory and that item can remain there until it is needed again okay next one is spatial aspect that is instead of fetching just one item from the main memory to the cache it is always useful to fetch several items or items that recite at adjacent locations to the currently accessed either together into the cache so the two main activities that can happen inside a cache memory are hit and miss so whenever one data item is accessed first we will check whether the corresponding item or in technical terms of word is available in the cache or not the word is already available in the cache then the operation will take place on that corresponding data and this is called a hit whereas when I am trying to access a word and if that data is not available in the cache then the block of data involving that word should be first fetched from the main memory and only then I can access it in such a case we call it is a cache miss when the processor wants to perform a read or write operation from the main memory it simply issues a read or write request the processor has no idea about the cache memory which is sitting in between the processor and the main memory so when a read or write operation is performed it is a cache control circuit who is going to perform the read operation on behalf of the main memory for example first let us consider a read operation if the data that is to be dead right by the processor is already available in the cache then it is called a cache hit the data can be directly taken from the cache itself no need to go to the main memory whereas the data that is to be read from the main memory is not available in the cache then it is called a cache miss and that corresponding data contained block should be accessed from the main memory first to the cache then from the cache the data will be read to the processor now third case that can arises suppose I am trying to read some data and that data is not available in the cache but it is accessed from the main memory and now there is no space present in the cache to bring in the incoming main memory block in that case some blocks of the cache memory should be replaced first using some algorithms which we will discuss later and after that the data will be provided to the main memory next is the write operation if the processor wants to write some data into the main memory in that case also the cache will be involved and here also there are two cases that can either be a write hit or a write miss in the case of write hit itself there are two protocols followed it can either be right through or write back methods if I am following the write true method then it means whatever data the processor wants to write into the main memory it will be updated simultaneously in the cache as well as in the main memory and if I am following the write-back method whatever data the processor wants to write into the main memory now will be updated only in the corresponding cache block at this moment and when this egg corresponding cache block has been replaced at the future period from the cache at that moment by making use of a dirty bit which is set during that write process it will be updated in the main memory and in the case of the right miss operation that is when I am trying to write something into the data and if there's if that corresponding block is missing in the cache then also there are two cases we have right around and write allocation policies right around policy is where whatever data is to be updated it will be updated directly in the main memory block no changes or no operations are done in the cache because that block is not there - whereas in the case of Wright allocation method first that main memory block which is to be updated will be brought into the cache and then the updation will be made in the cache and at a future point of time that corresponding data can't be updated in the so these are the different possible cases that can happen during a raid operation and a write operation in the case of read operation I can either have a cache hit cache miss or a cache miss with a block replacement whereas in the case of write operation I have write hit and write miss where there are different protocols followed which are right through and right back in the case of write hit and in the case of write miss I have white around and write a location in the beginning of this module we have already discussed the memory hierarchy and according to the hierarchy we know that the cache memory is much smaller in size compared to the main memory so at a particular point of time only some blocks of the main memory can be held in the cache not all the data or not all the blocks that are there in the main memory can be put inside the cache so which block of data of the main memory can be put into the cache there should be some kind of a mapping and to do this mapping there should be a mapping function thus we have the term mapping functions it is defined as the correspondence between a main memory block and a block in the cache memory so there are three kinds of mapping functions namely direct mapping associative mapping and set associative map now to study in detail about these mapping functions let us consider an example system this example system has got a main memory which is addressed by a 16-bit address line and the total size of the cache that is available in this system is to zero foreign words and these two zero foreign words are actually 128 blocks or 16 words each that is there will be 128 blocks and each block will contain 16 words okay and the main memory of this example system will have 64k words that is 4:09 six blocks or 16 words each that is each block in the main memory will contain 16 words so from this example itself we can understand that the cache is much smaller compared to the main memory so first we see direct mapping it is a simplest of the other three mapping functions here when I want to map a main memory block into the cache the block suppose for example if I am mapping block J of the main memory into the cache it will be only being mapped into the block J modulo 128 the position in the cache for example if I am taking block 0 from the main memory it can be brought to 0 mold 128 that is the 0-2 block in the cache another example if I am taking block 1 then it would be mapped into block 1 modulo 128 or the first block in the cache similarly so if we can generalize it we can say that block numbers 0 128 256 etcetera all these blocks can only be mapped into block 0 the cache but only one block can come in at a time into that corresponding block another example if I am taking block 1 or say block 129 or say block 257 all these can come into block one of them - so now what is the what is our aim how do we find out which block of the main memory is in the cache block now whether it is whether is it not 0 or block 128 or say block 256 which is now currently residing in block 0 of the cache for that we need to look into the memory address so this is a diagrammatic representation of what we just talked about now that is direct mapping here you can see the cache memory and main the cache memory as I have already said consists of 128 blocks numbered from 0 to 127 and each block can store 16 words each similarly the main memory also consists of 4 0 9 6 blocks numbered from 0 to 4 0 9 5 and each block can hold 16 words now how was the direct mapping happening that is block numbers 0 128 256 etcetera can be mapped only into log zero of the cache similarily block 1 129 257 etcetera can be mad only into block one of the cache so this is how the mapping is going to take place now suppose for example block 0 and block 1 128 needs to come to the cache memory at the same time what will happen both of them try to come to the cache block 0 which is not technically possible that is there will be some kind of an issue because they can come only to 0 even if the rest of the 127 blocks are remaining empty in the cache okay so this is how dark mapping works now at a particular point of time how will we know which block whether it is block 0 or block 128 or block 256 of the main memory is currently residing in the cache now for that we can look at the main memory address so as mentioned in the beginning here we are going to consider a 16-bit address line so the memory address will also be a consisting of 16 bits now this 16 bit memory address can be divided into three parts a 4-bit word field a 7-bit cache block field and a 5 bit tag field the least significant four bits in the memory address can be used to select one of the 16 words in a block that is now residing in the cache that is for example if a block is now present in my cache memory then in that block which word I want to access that can be that can be specified by using the four bits for example if my four bits are zero zero zero zero then it means I want to access that first word in my current block if my four bits are say 1 1 1 1 then that means I want to access that last word of my current cache block ok next one is the 7 bit cache block thing it is used to select one of the 128 cash positions in which an incoming block must be stored so we know that our cache memory consists of 128 positions into which of these 128 positions an incoming block from the main memory should be placed that is decided using this 7 bit cache block thing for example if my cache block field value is all the 7 bits are zeros then it means the incoming main memory block should be placed into the block 0 of my cache whereas if all my 7 bits are once then that means if the incoming main memory block should be placed into the 128 that is last position in my cache and the last one is a 5 bit tag field it is used to identify which of the 32 blocks that are mapped into the same gasp cash position is currently resident in the cache for example we know that there are 4 0 9 6 blocks in my main memory out of these 4 0 9 6 blocks 32 blocks each are mapped into one cache block position so which of those 32 blocks are now currently residing in a particular cache block position that can be identified using this 5 bit tag field so the main disadvantage of direct mapping method is contention can occur that is if the main memory blocks 0 1 28 & 2 56 need to be brought into the cache at the same time they cannot come into the cache at the same time because all of them trying to come into the block zero of the cache so there is some kind of a contention occurring this is the disadvantage of direct mapping so a solution to this direct mapping problem was achieved in the next method which is known as associative mapping here an incoming block from the main memory need not be mapped into a particular modular position instead it can be placed anywhere in the cache if that position is free that is so what do we understand from that there is no need of a separate seven bit cache block field in the memory address so we only require some bits to identify which word I want to access and the rest of the bits in the memory address can be used as tag bits so there will be if I am taking a 16-bit memory address only four bits are needed for spare identifying the word whereas the rest of the twelve bits can be used as tag bits okay so and here the search of the complete cache has to be done in order to write defi whether a block is there in the cache or not that would be an issue of this method but compared to the first method there is no issue of contention occurring that is even if there is free space available in the cache some blocks which are mapped into the same cache block cannot commit but that issue is solved here but instead the problem here would be that the if I want to identify a block that is residing in the cache the complete cache memory has to be searched so as I have already mentioned in this in the case of associate mapping there is no one you don't separate seven bits for identifying the cache block field instead we require only two kinds of bits that is four bits for identifying a word from the corresponding block and twelve bits for identifying the position into which the main memory block has been back so twelve bits for dad and four bits for what so what is the disadvantage of associative mapping the main disadvantages the search time that is involved to identify where a particular block is in the cache is much more compared to dark mapping you have to search through the full cache block to find out a corresponding entry now to overcome this advant this disadvantage we go for the third method which is known as set associative mapping it is actually a combination of the first two methods that is a combination of the concepts used in direct mapping and the concepts using associative mapping here the new thing that is introduced is sets so here the cache blocks are grouped into sets set means the set size may be 2 or set size may be 4 or it can be any size it can be of size say K so if I am having a set of size K then that means that set will contain K number of blocks and when a memory block is coming from the main memory into the cache it will the incoming block can be mapped into a particular set and inside that set it can go to any of the freely available blocks so it is combining the concepts of direct mapping as well as associative mapping now what is the advantage of this method it uses fewer tags compared to associate mapping but it is found to be more flexible than because we are not restricting the main memory blocks into one particular block instead if we are only restricting them to a particular set now inside that set itself they can go in suppose if a particular block is filled it can go into another block which is free this is what is known as a set associative mapping so this is a diagrammatic representation of set associative mapping in this we have two way set associative mapping that is the number of blocks that are involved in a particular sector two so here the main memory blocks 0 'clock 64 block 128 etcetera all these kind of blocks will be mapped into sets wrong cache of the cache memory similarly block 1 block 65 block 129 etcetera will be mapped into set one of the cache now inside set 0 itself there will be two block spaces available so suppose for example block 0 and block 64 wants to come in at the same time into the cache they can come in and they can occupy either block 0 or block 1 of the set 0 of the cache now if again block 128 also wants to come in and if block 0 and block 1 are filled then there is a issue of contention so compared to direct mapping it is more flexible that is the advantage here and also if we want to search for a block you don't have to search through the for cache instead you can search inside their given set and to identify which is the given set the memory address can be split into 3 parts the least significant 4 bits can be used to identify which word you want to read from a particular block and since there are 64 sets possible in the cache there are 6 bits set aside to identify the sets to which the corresponding cache block can the main memory block can be mad and the last 6 bits the most significant 6 bits can be used as a tag to identify the cache block into which the main memory block has been so with that we have completed the three different mapping functions now we just need to make some comparisons that is in direct mapping the position of each block is already predetermined based on a function based on a formula that is for a block J that is coming from the main memory it is always mapped into J modulo 120 so there is no need of any replacement strategies if the corresponding cache block is full then it will either have to be overwritten or some waiting procedures have to be followed whereas an associative and set associative methods the block position is not predetermined we have to place the block into a particular location that is when the cache is full and if the new blocks are brought into cache and the cache controller must decide which of the old blocks have to be replaced so some mechanisms must be in place so that I can replace and already existing block in the cache so whenever a block is to be overwritten it is always better to override the one which has been which has not been used for the longest amount of time so if we can find out which is the oldest block in the cache and if we can remove it then we can maintain the temporal aspect in spatial aspects of cache memory concept as well and this procedure is called the least recently used technique and the block which is here removed based on this concept is called the LRU block or at least least recently used block and the technique is called the LRU algorithm the cache controller will track the references to all the blocks that have been made in the passed with the help of a block counter and based on that he will find out which is the oldest reffered block and based on that that corresponding block can be removed from the cache and the new incoming blocks from the main memory can be placed into the cache now two main factors that can be considered for analyzing the performance of the cache memory can be the best possible performance at low cost and this is called the price performance ratio the performance of the cache memory will always depend on how fast the Machine instructions are brought into the processor and how fast they can be executed okay so this is what is known as price performance ratio two other values which can be used to determine the performance of a cache memory are he trade and miss penalty hit rate is defined as the number of hits which was stated as a fraction of all that tilted axis that is when I am trying to access a word from the cache memory what is the chance that I will be able to access the word from the cache at the first axis itself that is what is known as hit rate whereas Miss penalty can be defined as take extra time that's taken to bring the desired information into the cache if it is currently not present in the cache that's what's known as Miss penalty and always the goal will be to increase the hit rate and decrease the missed penalty so with that week to the end of this online lecture video which was basically about cache memory so initially we discussed about the need for a cache memory inside the computer then we saw what are the two main concepts based on which cache memory works that is the temporal locality of reference and spatial locality of reference then we saw the working model of the cache memory then we saw how the different mapping functions that is direct mapping associative mapping and set associative mapping can be used to map the main memory block into the cache memory block and also we saw two different performance consideration terms like the price performance ratio hit rate and miss penalty I hope all of you where will benefit from this presentation thank you 
4B4X--4kRJc,27,"ISRO 2013 Exam, Computer Architecture and Organization, Logical Memory Address Representation, Physical Address Space Size, and Logical Address Space Size",2018-04-19T09:17:25Z,ISRO 2013 Exam: Computer Architecture and Organization (Logical Memory Address Representation),https://i.ytimg.com/vi/4B4X--4kRJc/hqdefault.jpg,Ritu Kapur Classes,PT1M55S,false,2446,20,0,0,2,hello friends I'm here with a very important question from computer architecture an organization so in this question basically they have asked about the bits required to represent logical address space and physical address space a very simple but a tricky looking question let's go through it once it says that consider a logical address space of eight pages of one zero two four yards each mapped on to a physical memory of 32 frames how many bits are there in the physical address and logical address respectively so you've been given with some push of options first step is to write solve it yourself and then finally verify with the answer right so for the logical address space it said that it has eight pages of these many words each so that means the size is 8 into 1 0 2 4 I have just converted it into powers of 2 to check out for the bits required so the final answer from this is 2 to power 13 so that means 13 bits are required to represent your logical address space now similarly what I did was for physical address space it says that physical address space has 32 frames and since the frame size is not given whenever it does not given we take it equivalent to your page size so that means 32 into 1 0 to 4 which is 2 to power 5 into 2 to power 10 which is 2 to power 15 so that means you require 15 bits for your physical address space so the question was for how many bits are required for physical address and logical address respectively answer would be 15 and 13 respectively which is your C option in this question that's what for this question stay tuned for more questions coming up thank you 
mDk6_dfm8a8,22,"Ù…Ù† Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø³Ø§Ù‚ ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" Ù„Ù„Ø¯ÙƒØªÙˆØ± ÙˆØ³Ø§Ù… Ø¹Ø§Ø´ÙˆØ± Ø§Ù„Ù…Ø­Ø§Ø¶Ø± ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¨ØºØ²Ø© .

Computer Architecture and Organization course by Dr. Wesam Ashour, lecturer at the Islamic university of Gaza - Palestine.

Course language: Arabic",2016-11-21T18:43:13Z,"17- Computer Architecture ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" - Chapter 5",https://i.ytimg.com/vi/mDk6_dfm8a8/hqdefault.jpg,Abdallah Safi,PT40M4S,false,1981,9,0,0,4,in not like a us-led coalition where Galia cook on a shelf of my ish women can have a Max Biaggi that cancer obesity just right over that is file extension over the our mental worry max has a narrow Yemen fine either Dana ha ha the way if she stiffness and let me tell exam it's too far Imogen - it's uh come again we shall bathe in some hair shampoo contribution that is why act phase signal jump explain I see I see you funky didn't have to die shoo yeah yeah which might like you do you handsome it is what is when I die which guarantee go with equal and so we'll jump in campaign when Hey chan was a very big massage you know big American operation then do it yeah max with the head with the guard and wrestling Bama Delta and for fragments in general the German code but Heavenly Father she wanna sit there we should be thankful in shallow inputs and whether it be a demon but my to community and some statistical knowledge actions but the ugliness in a West Virginia had secretary is well welcome would you have become alpha 
BWN-nL1KPNA,22,,2017-01-25T14:44:21Z,stored program organization and addressing modes,https://i.ytimg.com/vi/BWN-nL1KPNA/hqdefault.jpg,harsha mangipudi,PT12M13S,false,5735,84,15,0,10,I heard in this video we won one about what is indecision code what is the operation code and how we address the memory and types of addressing indirect and denied addressing and stored it stored program organization so first thing is session so what is the expression code we basically a new session code is group of bits there is a set of base that perform certain operations on give instructions okay now if a section code is divided into five insertion code is divided into parts in this circle below wood to get yourself insertion code this many important beliefs of coal what is NOC code of code ez is called an operation code that means it is a group of it does perform certain instructions which defined by a multiply divide increment etcetera that means in define what type of operation it should do the computer should do it should perform addition or multiplication or subtraction or incrementation whatever you should do that will be defined we're off course suppose my computer should perform 64 operations 64 operation and I need to give that instruction format for performing the system operations then in my instruction format I need to have a taste 6 whispers please 6 bits per output that means to position 64 as you know so to go from 64 operations I need to have 6 bits first awkward and remaining bricks first some other string we'll see what it does ok these people are not 64 micro operations we - secret first output ok so that suppose as an example operation and one one 1 1 0 this is my 6 bit opcode which takes us which tells the computer to cause of engine operation ok whenever this opcode is decoded whenever is all fully coated computer they send their control signals saying that I have received us a score for addition now you start searching a further operate from that memory okay now we restore the initial operation in some memory and we add that memory to this register so this patient operation complete now this is the awkward to infer engine the same way they can be one of course for attraction they can be one of comfort for it multiplication this in that way I can have now code for 64 different operation I think it is clear it till now what is awkward right now ok I have this come and give our output have decoded it ok is the formation but from where am I getting my open ok that opponent will be given merger and trust there that operand is stored ok so the next part of my instruction form of the winner okay so as my operand will be in some memory location of memory address okay wait I decide to inspire that location my controller blue transferred to that location finds all friends as can exist in addition they become pretty much addition operation right so basically when computers we have four zeros and every state before nine six world over zero matrix world where social links it can be integrated Porter tuple of 20 right so two powerful I was arrangement in to power 16 bit words two powerful 16-bit words so one 16-bit words days to go through okay two powerful this work will be mine of course opposite so I have four bits per awkward and remaining 12 big burn and this is also operate this carry have managed players in a computer for for genetic votes for the next escape depicted by 2 plus one okay two powerful 16-bit words 16 business to power 4 4 so whatever insider mitigate these useless accorded whatever remaining colleges use was attached flow to Delhi this is my instruction on that okay not of these this will be from never to 0 will be my address under from 20 to 15 12 animal in 54 output general instruction format which will be illustrated right this is the web which told the program is a program using the instruction format now the more - little extension for this instruction format okay and I saw it [Music] and so underneath we discussed it in a comparative way we're easy to remember one thing is direct addressing next thing is in their relative there is no continuum and we all know the data format of instruction format e this is awkward this is address is awkward now here what happens is whenever whenever the opcode is addition or subtraction and if the across is specifying the operon then it is called negative if the address is specifying the operand going to the Delta okay whenever the address is specifying the entrance of memory word of the Auckland then it is called indirect addressing see the difference it shows this is abhrush shows directly the operand address then it is called diagonally then that will show the number E for of the operand then it is called innate addressing so we will receive notice from the system now here you can see I have given this for this block right this shows the disk use an idea of whether the given hunters is a direct address for editors from Eric addressing this customers will be zero we invade addressing it it will be one okay so one good for here will be the recording process and put into plus plus rock code occasionally I do oculus and at the fan and from rebel to 0 so Delta G will reminder so a bit address it is eligible for cooperation so forfeiture will be back operators generally this will be at 22 and casually so 457 will be here when operator face hearing now pair right I am following a high operation so it uses this danby's would super special here I'll add and here some 300 out of it so they intended another 75 euro t-150 I have my now you clearly observe what are the differences between Belgium and see here the first bit is below issuers it is a dieter the first one is one it is an imaginary and 457 is given an service in issuing the operand value so directly whatever the registers are tumors these both ACS for a curator register or it is a single processor registers okay this is an identity to the given event and we get in the Sun here we have subscribe the enters of the operand on Google Earth on top 6050 memory mode of the operand so in the end of we are one two five zero so the custom chips to want to say ok we can find operand so where they started collecting data to give us some right she's addressing this is indirect under the next video we look into the system using register thank you 
wRJrhipHVlM,27,"Arithmetic and Logic Unit : 
Operations are executed in the Arithmetic and Logic Unit (ALU).
Arithmetic operations such as addition, subtraction.
Logic operations such as comparison of numbers.
In order to execute an instruction, operands need to be brought into the ALU from the memory. 

Operands are stored in general purpose registers available in the ALU.

Access times of general purpose registers are faster than the cache.  
Results of the operations are stored back in the memory or retained in the processor for immediate use. 

Output Unit : 
Computers represent information in a specific binary form. Output units:
   - Interface with output devices.
   - Accept processed results provided by the computer in specific binary form.
   - Convert the information in binary form to a form understood by an
      output device.

Control Unit: 
Operation of a computer can be summarized as:
Accepts information from the input units (Input unit).
Stores the information (Memory).
Processes the information (ALU).
Provides processed results through the output units (Output unit).
Operations of Input unit, Memory, ALU and Output unit are coordinated by Control unit.
Instructions control â€œwhatâ€ operations take place (e.g. data transfer, processing).
Control unit generates timing signals which determines â€œwhenâ€ a particular operation takes place.",2020-10-03T13:24:33Z,Lecture 03 CSE 231 Computer Architecture and Organization Spring-2021,https://i.ytimg.com/vi/wRJrhipHVlM/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT17M57S,false,144,9,0,0,19,[Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] logic unit output unit control operations are executed in the arithmetic and logic unit cheshire cool operations school of jamun people are the hundred arithmetic operation addition subtraction multiplication division [Music] arithmetic logic unity in order to execute an instruction operands need to be brought into the arithmetic logic unit from the memory judy umbra construction executed are stored in general purpose registers arithmetic logic unique to general of general purpose registers general purpose registered access time city money faster than the cash memory cash binary key interfacing or a interface with output devices shake a stick or output units so computer represents information in a specific binary form computer computer and header information represent for a specific binary form our output units just say output devices shutter interface car interface with output devices accept processed results provided by the computer in a specific binary form convert the information memory arithmetic logic unit and output unit angular coordination acoustic a control unit so here control unit is nothing but the coordinator of the operations using input unit memory arithmetic logic unit and output unit so instructions control what operations take place control unit generates timing signals which determines when a particular operation takes place so the hub so control unit generates control signals or timing signals which determines when a particular operation takes place so keep having a functional unit glue the clamp the input you need output unit memory unit upon processing unit hrt unit functional units of a computer hrt functional units keep happy accommodation see how are the functional units connected for a computer to achieve its operation the functional units need to be communicate with each other functional operation communications connected [Music] we must have to use 32 of 64 number of oil single for one bit of information so number of parallel oil in a bus is equal to the word length of a computer computer world link parallel to the google authorized foreign time of the cash memory less than the access time of the main memory main memory data access already because cash memory is foreign engineering 
2XZ3ik6xSzM,27,"Digital Design and Computer Architecture, ETH ZÃ¼rich, Spring 2020 (https://safari.ethz.ch/digitaltechnik/spring2020/doku.php?id=start)

Lecture 19: SIMD Processors
Lecturer: Dr. Juan GÃ³mez Luna
Date: May 7, 2020

Slides (pptx): https://safari.ethz.ch/digitaltechnik/spring2020/lib/exe/fetch.php?media=onur-digitaldesign-2020-lecture19-simd-beforelecture.pptx
Slides (pdf): https://safari.ethz.ch/digitaltechnik/spring2020/lib/exe/fetch.php?media=onur-digitaldesign-2020-lecture19-simd-beforelecture.pdf",2020-05-06T05:32:03Z,"Digital Design & Computer Architecture - Lecture 19: SIMD Processors (ETH ZÃ¼rich, Spring 2020)",https://i.ytimg.com/vi/2XZ3ik6xSzM/hqdefault.jpg,Onur Mutlu Lectures,PT1H33M23S,false,2929,44,6,0,0,okay hello everyone I hope you are ready for a very interesting talk unfortunately I have to give the talk today professor moody Lewis not here not here today neither tomorrow and these two days we are going to cover a very important topic of the course or two in important topics of the course today we will talk about simply processors tomorrow about GPUs which are let's say like the special form of Sindhi processors well why are they important these two lectures well the reason is that first of all we are going to complete this last part of other execution paradigms recall that everything started with a single cycle and the multi cycle machine and after that we started studying how to improve the performance how to achieve higher throughput in the computing systems and different ways of doing these where pipelining out of order execution in the last lecture professor mutlu talked about very long instruction word architectures about systolic arrays these are let's say in this category of order execution paradigms and now we are going to cover some of the most important execution paradigms these days that are Sindhi and GPU that's one reason why these two lectures are important the other reason is that very likely you will have one or two questions related to these two lectures in the final exam you can actually check last year exam spring 2018 which is already in the website okay so this is more or less what I already said and these are the readings for this week the required one is the first one nvidia tesla this is an introduction to the architecture of nvidia gpus it's quite accessible paper I go quite easy to understand and especially if you read it after today's and tomorrow's lecture and the other one it's also very interesting one it's the but it's recommended it's about the MMX extensions these multimedia extensions that we have an Intel processor since the end of the last century or the beginning of the 2000s and and actually we are going to talk about the album probably at the end of today's lecture with the lectures in D processors and GPU it's what we are going to cover today and tomorrow and we start with Cindy processing what is Cindy processing good for is good for exploiting data parallelism and in particular regular data parallelism we are going to talk about this during the whole lecture but just to give you like a very very quick and easy to understand example usually when you are writing computer I mean programming right in your C code or whatever you will see that many times you write a for loop and this for loop you are going over an array and doing many times the same thing right so for example the most easy example to understand is you have two vectors and you want to add them LM element wise you want to add a zero with width to be zero and store the result in C 0 a 1 plus B 1 and store the result in C 1 and so on so there is a lot of parallelism there let's assume that or erase or vectors have 1 million elements so we could potentially be operating on these million elements at the same time and why is that because the computation that we need to apply to each of these elements a 0 v 0 c 0 a 1 B 1 C 1 and so on is completely independent right so why don't we design a machine that can operate on all these elements at the same time unfortunately it won't be possible to design a machine with 1 million functional units but maybe we can create a machine with 128 functional units and what we will do is iterating 128 after 128 right that's the essentially the idea that Cindy processors use so this Cindy processors actually are one of the four main categories of computer computers that Mike Flynn introduced or discussed in his paper in 1966 the first type is the first category are the sis D machines this means single instruction single data this means that I have one single instruction and I go to memory and get one data element and operate on this data element I do whatever this instruction says that I have to do that's what a sequential machine does like what if you if you go to your laptop or to your cell phone and you look at the CPU that you have there probably that the task CPU will be multi-core CPU but if you focus on just one single core of this this will be a sequential machine has seemed a machine that can only execute one instruction on one input data element why maybe one or two right because we if we are adding two elements we need to input operands and we will get one output the second group are they simply single instruction multiple data elements and these are the ones that we are going to talk about and as you will see very soon there are two main types are a processors and vector processors they are very similar they have a lot of things in common that's why we explain both together what's the main idea here the main idea is that if I want to add these two vectors a and B and store the result in C I just need one single instruction add ad and store right just need one single instruction for the million elements so that's why is single instruction multiple data another the third category is Miss D is multiple instruction single data it's not easy to find computers or machine machines which follow this paradigm these days probably the closest thing is the systolic array are the systolic arrays that Professor model explained in the last week another example that you can also find may be search your server microns automata processors they are also kind of similar thing and the last category are the mean the multiple instruction on multiple data and for example mean I could say that the easiest example that we can talk about here are multi processors multi-core processors or multi-threaded processors whatever you want to call them and this is when we have different threats of execution and each thread can be doing a completely different thing tomorrow when we talk about GPUs we will also relate the acting the way that GPUs processing instructions with Mindy because even though they are mostly Cindy they can they also have some features that are related to to Mindy okay anyway let's start talking about this simile yeah go ahead could you repeat yeah it's a scalar value essentially hmm if it's an array if we operate on an array then it's a seemly machine okay okay data parallelism let's define what we understand by data parallelism I think that I somehow already defined that but data parallelism in data parallelism the concurrency that is the ability of execute several things several instructions at the same time on several operations at the same time arises for from performing the same operation on different pieces of data that's Cindy and the best example is the vector addition that I mentioned before another example can be dot product of two vectors write in dot product I can also have some sort of independent computation on the different elements at least for the multiplication part for the accumulation yeah we could discuss about that we are not going to talk about that right now but it's it's another possible example of Cindy computation you have to contrast these regular data parallelism with something that has already been covered in the course as well it's a data flow in dataflow concurrency arises from executing different operations in parallel and the computation can be completely irregular right this is computation in a data-driven manner and also contrasts with thread or control parallelism it's the example that I gave you before about the multi-core processors in multi-core processors we can have individual threads running on each of the cores and they can be doing completely different things maybe they these different threads might belong to the same program to the same process or maybe even completely different processes completely different applications so here in seam D we exploit operation level parallelism we could say that it's a form of ILP instruction level parallelism but not in the way that ILP is exploited for example in a very very long instruction word architectures right here it's a much more simpler simple this data level parallelism because the multiple instructions are essentially the same instruction on different data elements okay a little bit more single instruction operates on multiple data elements in time or in space we are going to have multiple processing elements you're going to see that in the next slide and we say that we have a time space duality depending on if we operate on let's say time or let's say we exploit this data regular data parallelism in time or in space we will be talking about our rate processors or vector processors you have the definition here but the best way to understand it is going directly to this slide you can already observe the difference between these two array processors here on the Left they have let's assume for now four functional units or for processing elements I will probably use these terms as synonyms right processing element computing element was a functional unit I hope it's always clear so you have here for processing elements that are exactly the same each of them has a different number index of course and here in the vector processor you also have for processing elements for functional units but each of them is specialized let's take a look at how they execute the code but first of all let's take a look at the code so here we have instruction string for instructions the first instruction is a load instruction and we call it a back to load why is that because in the same access when we execute the same one single instruction we are reading loading from memory more than one element in this case for okay as you can see there we are going to wear whatever array a is in memory and we are getting four elements at the same time a 0 a 1 a 2 and a 3 and we are storing them in a vector register this vector register you can think about it like a very very long register which is in this case divided into four slots and in each of these slots we are going to place a a 1 a 2 or a 3 the length of each of these Lots will be typically 32 bits or 64 bits right as usual in I mean there's color registers that we have studied in the lc3 or in MIPS the second instruction is the vector add in this vector at what we are doing is for two each of these four elements that we have in the vector array we are adding one next instruction is vector multiply so we multiplied by two each of these four elements and finally after having done all the computation we store the four elements at the same time in memory so how is this code executed in the array processor in the vector processor so in the array processor each of the processing elements can execute any type of instruction first instruction is low so therefore processing elements are loading from memory one cycle later let's say one cycle for now probably this will take longer right accessing memory unfortunately it takes more than one cycle but let's consider that this one cycle for now we execute the add instruction and we do the add operation on the four elements that we previously loaded after that multiplication and after that store operation in the vector processor because we have specialized functional units we need to execute all the load operations in the same unit so in the very first cycle we issue load 0 is reading element a 0 after that a 1 and after that a 2 but observe that as soon as we have read element a 0 we can start executing this addition right that's why we see this at 0 here in functional unit so and in the second cycle while we are issuing load 2 in functional you need LD and the addition one in the functional unit addition we can start with a multiplication in this third functional unit so that's more or less how it works and we will complete after some cycles we will complete the execution of the entire code for the four input input elements observe that here in the array processor on the Left we have the same operation at the same time so in any given cycle in all the processing elements we are executing the same operation while in the vector processor in the same cycle we are executing different operations that's with regards to time recall the time/space duality with regards to space in the array processor we have different operations in the same space well we have the same operation in the same space for the vector processor so these are essentially the most significant differences between array processors and vector processors the interesting thing here is that let's say that this is like kind of purist distinction so real world simile processors are a combination of both and actually that's the casing in GPUs as we will continue discussing today and tomorrow let's very briefly compare and recall what's very long instruction word architecture also to understand what's the main difference between Cindy in this case the example will be with array processor and very long instruction word architecture recall that in real iw what we have is a very smart compiler that is able to strat parallelism typically regular parallelism from the code and generate some pack instructions that contain several instructions right in this case for example in the same instruction we have ADD load move and multiply and these four instructions will be executed in parallel in these four processing elements that you can see in the slide so the parallel I mean the concurrency here is coming from the execution of different instructions different processing elements while in the array processors because they are simply processors we have one single instruction in this case is a vector addition and what we do is executing the addition for different elements in each of the four processing elements is that clear okay let's continue let's go a little bit deeper our first example and this is actually it's still sequential code right so but it's a very good example of what is called a vectorizable loop why do we call this a vectorizable loop because we call it a vectorizable loop because the all the iterations of the loop are independent there are no loop curry dependencies so I can operate in parallel on a0 and b0 and on a1 and b1 right that's why it's vectorizable because I can easily convert this into something some code that will be executed on a Sindhi processor so in the vector processor we operate on vectors not on scalar data related to what he asked before and there are some basic requirements there there is a need to be able to store and etsu load and store vectors and we do that in the vector registers actually we are going to use this nice thing today and we are going to see the first example very right now so we have vector registers I already mentioned or define what the vector registers are we are going to contain different elements coming probably from the same array in these vector registers and this vector registers are going to have one certain vector length and what's the vector length recall the example that I gave you in the beginning let's consider vectors of 1 million elements unfortunately we cannot have machines with 1 million processing elements but we can have short-term machines and we will I mean short term acheive machines with certain number let's say 128 256 processing elements and what we will do is using them at the same time this is what we are going to call the vector length ok and the next the next thing is that we will need to access memory and we are going to access this memory with a certain stride in principle you can assume that this stride is 1 like as in the example that we saw before where we access a 0 a 1 a 2 and a 3 they stride between these elements that are accessed is 1 right but we could be accessing a 0 a 2 a 4 for example and in this case or stride will be 2 you had a question that's a very good question actually the way that we are going to do this will depend on the machine for example we will see they they they actually at the very end of this presentation and and we are also going to cover these tomorrow but let me very briefly explain you some GPUs for example the Nvidia Tesla GPUs that you're going to read about in the Indies required paper for this week they actually have let's say kind of software vector length which is 32 but the truth is that you only have 8 functional units so if your array is 1 million elements you will go 32 by 32 but the hardware will be actually executing 8 by 8 but you will see an example later ok let me very briefly explain a little bit better what this vector length and vector well at least vectors tried with this example what yes okay so this is our memory this is address 0 this is address M minus 1 and we will use a certain vector length okay let's say 50 for example and we have to set certain vector stripe this vector stride is going to be a register where we put one number and this number means the distance between elements that are going to be accessed from memory in the easiest example as I said before you can consider that this vector stride is 1 so if this vector stride is 1 and let's say or array a starts here we are going to access this element 0 element 1 element 2 and so on right if we execute one vector load a what we will be doing this is our vector register and this vector register is divided into several slots for now let's assume that the number of a slot is billing okay so if all stripe is 1 we will access address a which is sit this one a plus 0 and we will store it here and then a plus 1 so we will read these and store it here and then a plus 2 and so on right if we set the stripe to let's say 8 we will access a 0 but then a 8 and then a 16 so the address is that or load functional unit is going to generate our address AIDS a plus zero A plus 8a plus 16 and so on so we will be reading this thing and put this here this thing could this here this thing could this here so long okay so this thing is the stripe which defines what's the distance between consecutively accessed elements okay we will continue talking about the stride later okay there we are okay so let's continue our vector instruction performs an operation on each element in consecutive cycles that's what we have seen in the in the example before but there is something good here recall the operation that we execute on every element store in a vector register is independent from the other operations on the same vector register so that's why we can allow deeper pipelines recall that we have studied here the MIPS pipeline for example right it has five stages we can have even deeper pipelines and that's going to be very good to increasing the throughput of of the machine and why can we have these deeper pipelines because there is no intra Becht or dependence so if I operate on a zero and a one I can do it at the same time because they are not dependent instructions they are not dependent operation so I won't have to worry about things like hardware interlocking recalling the pipeline and this lecture about pipeline issues if you have two instructions that are depending on each other and they are executing in the pipeline at some point you will have to stop the pipeline right we will do what is called a stall and so we don't have to worry about this here we don't have to worry about control flow because even though so this is pipeline operating on different elements and it's one single instruction there is no there are no branches there so we don't have to worry about them we will use branches as well in simply programs but the number of branches will be much smaller they will be much more unfrequent and also the thing I already mentioned that calculating the address is very easy why is that because we are defining a stride and using this stride we calculate a a + 1 a plus 2 or a a plus 8 a plus 16 and so on right the good thing is that because we know exactly what are the others is that we are going to access we could be doing something that like prefetching and what does prefetching mean have you ever heard about prefetching this essentially means that if I know early you some of you might already have used in when programming in C the thing is that if you know exactly what you are going to access in memory in from memory a certain number of cycles you can you could start the access already right so here and going back to the example that we were discussing I have 1 million elements I want to operate on and I know that right now I can operate on 32 but after that I will not be operating on the next 32 so while I'm doing the computation for these 32 elements I can already be accessing the next 32 elements from memory that's what is called prefetching and there is a lot of research on these and and and this is something that Noah days all computers are all CPUs include hardware prefetcher but this is let's say like more advanced concept in computer architecture so if you are interested you can you can come to the computer architecture master's course and the bachelor course that Professor the bachelors seminar that Professor would loo teaches as well yes so either way you can have software prefetching your compiler can I can detect that and can generate these instructions these load instructions to go to memory and bring the data or even easier if the hardware does this for you and for the compiler that's a actually the way it is in current CPUs ok advantages of vector processors I think that this is something that we are already talking about there are no dependencies within a vector so pipelining and parallelization work really well deep pipelines we were already mentioned in that each instruction generates a lot of work one single instruction can be useful to operate on let's say 32 elements or let's say 128 elements depending on what's your vector length right and this is definitely useful because remember that in this how was it called a in how was it called in in the lecture the the cycle the exec the instruction execution cycle right recall that the very first thing was instruction fetch go into memory and bringing the instruction from memory so this is something that here it's going to be much more efficient right we need to read so the number of instructions that we need to read from memory is much smaller if we have a scholar machine and we have to operate on 50 elements we need to go 50 times to memory to read the instructions to operate on these 50 elements but if we have a vector matching we only need to go once right so that's why here we say that we reduce the instruction fetch bandwidth also regular memory access patterns this is what it will be typically we will define a stripe but after that the accesses are always regular and even though we don't really need to cover the reasons why but it's much more efficient memory access and then there is no need to explicitly code loops we will use loops and actually we are going to see an example at the end of the lecture but we will have fewer branches in the end think that branches are not productive right they are of course necessary because we need them to control the flow of the program but they are not really necessary I mean necessary they are but they are not really productive right productive is execution doing additions or executing multiplications that is really productive but they also have some disadvantages disadvantages is that they are not good for exploiting irregular parallelism and actually GPUs for example are not really very good at irregular parallelism you can if you're a smart programmer you can figure out a way of instructing the parallelism and still getting good good speed-up as compared to a CPU but it takes obviously more effort and that's actually something that was already pointed out by Fisher when he was working in in this very long instruction war architectures and this paper publishing is 1983 where he said to vector approach to program a vector machine the compiler or hand coder must make the data structures in the cold feet nearly exactly regular the regular structure built into the hardware right we need regular data parallelism and we need to be able to write the code in a way that we access this regular data organization in an efficient way and we put it in the vector registers in an efficient way and continue say saying it's not only hard to do in first place but just as hard to change that is you might have written your code for let's say vector length equal 50 and eventually you need to change this because for some reason the characteristics of your array change right so you will have to go and change the code so this is one of the main disadvantages of this vector processors and they have more limitations they can easily become from a memory can easily be kong about an egg why is that because I have a lot of compute power already right in the examples that we are discussing here we have for example for processing elements and these for presenting elements can be computing at the same time so if I need to feed these for processing elements I need to read four elements at the same time from memory right so I need I need higher bandwidth and actually if you go to any of the I mean GPUs for example most of the workloads we execute nowadays on GPUs they are bounded by the access to memory even though if you read the specifications you will see that the bandwidth is amazing like another 700 800 gigabyte per second it still is short for what the computing elements of the GPU need okay a little bit more in depth now let's talk about the vector registers each vector data register holds in M bit values let's say typical value Z for vector length is 32 and will be 32 if we operate on 32-bit elements for byte elements like integer or floating-point they will be this mb it will be 32 bit right and here you can see 3 vector registers vector register 0 1 and these will be whatever X so and we also have 3 special registers which are the control registers vector length we need to define what's the length of the vector that we are going to operate on they stride which we have already defined and the vector mask so back terma so first of all vector length can be a maximum of n this is something that we have already discussed in an example that we will see later this vector length will be 50 and then at some point we will see another example where where we will have to reduce this vector length to be effectively 15 in the in the example that we will show and then we have this vector mask why do we need this vector mask because sometimes unfortunately we cannot exploit all the lanes or the functional units at the same time and why is that because life is not so regular right so we need to somehow be able to decide when do we really want to operate on some a specific data element or not and that's why we need to use a mask and actually I'm going to introduce here a very important concept that unfortunate professor mu - I think couldn't cover when he explained branches branch prediction and this lecture that we recently have and it's called predication and we are going to see why predication is important because it's used in vector processors let's consider some C code some high-level language something like this I have an if and I have let's say that we are going to operate on a vector processor zero but for now this is just a scholar code right yes sequential sequential code and if V 0 I is 0 I will do this if not else I would do that you already know that the compiler for a sequential machine for a sysd machine will convert this into something like for example let's invent some some useful is a for example an branch equal 0 branch equal 0 will this will be account a conditional branch so we will jump if V 0 I is equal to 0 right if it's not we want branch and we will do this but if is if it's not 0 we will go somewhere where the else label is and we will do that of course we also need some unconditional jump here right and after executing the duties go to the end of the program so how is this going to be executed on a Sindhi machine we we are not we don't need to have branches like this one and this one we are going to use predication predication consists in having a special register called be mask which contains one beat per functional unit and the value of this vide obviously can be 0 or 1 and it will be 0 or 1 depending on the predicate what's the predicate the condition here the condition here so V mask will be equal to 0 or 1 depending on what's the value of V 0 I so for example if V 0 0 is equal to 0 this will be will be 1 if it's see the next one is equal to whatever it's not 0 this will be 0 and this will be 1 and this will be 0 and maybe this one will be 1 okay and then we will execute do this but the hardware will only execute these two DS for those lanes or those functional units where vector mask is equal to 1 and after that we will obtain the complement of V mask so V mass will be 0 1 0 1 and so on and then we will do that so this is how predication works oh is that better very good ok so this is how predication work we will see another example at the end of the lecture and probably I'll and tomorrow as well when we talk about the GPUs ok we said we will have deep pipelines okay something like this so these all these functional units they will be pipeline why's that because they are in v1 and v2 I have the input operands in v3 I want to start the result each of these v1 v2 v3 are divided into slots where I'm going to have this individual M bit operands and in each cycle I start executing one multiplication in this example so that in each of these pipeline stages seeks in this case I have the a specific computation whatever this stage is for each of the operands that I have in the input registers ok first example of real-world vector Mattie is gray one which was released in the in the 70s probably the first supercomputer and it was the fastest computer of its time absurd that this is in this schemata schematic that we have here we can see here the vector registers we have also here some vector functional units even this VM is probably the vector mask but also this machine it was a fastest not only because it was a vector matching but also because it was the fastest Escala it has the fastest color processor of its time so you can that's why you can also see here some scholar registers and some scholar functional units and why is that because recall and and all slow maybe your code is great is highly parallelizable vector i suppose you can strike a lot of parallelism regular parallelism from there but if some part of your program is inherently sequential and there is no way of paralyzing it your maximum speed-up will be limited by Amdahl's law you already know that say more Cray also knew that and that's why he put a lot of effort on designing a very powerful a scholar core as well if you want to see a prey with your own eyes you can go to cap to the building where our lab is and you will find this beautiful tray mp-28 I think this one was built in the 80s and also you can see next to it this Raspberry Pi which has exactly the same around the same computing power so this is how we evolved and here you can see some I mean they are next to the machines you can also find the datasheet here these are just some figures from from me in this particular case observe that this one has four colored CPUs so it was already multi processor or multi not not exactly multi-core but multi processor and here you can see vector registers color register functional units etc a little bit more information here for different models the number of CPUs size of the memory number of banks we are going to discuss what the banks are right now and more details about the functional units and even more about yeah so mainframe input/output Seuss's subsystem and storage device so and this is a more cray the father of supercomputers and the nice thing that he said is if you are if you were plowing a field which good you rather use to strong oxen or 1024 chickens well it will depend on what your field is right it will depend on what your computation is about if you have a lot of regular parallelism you will probably go for this if you don't have so much parallelism you but you need a lot of computing power you will probably go for this so I think that we are going to stop here for 10 minutes and we will start talking about the memory access memory banks ok yeah let's continue we already had ten ten minutes we were here and now we are going to talk about how we access memory and we will see why we need memory banks and why we need to have an interleaved memory in order to have an efficient access to memory and that's why the reason why is because we have backed or load some vector stores so we need to access be able to access many elements at the same time for read or for write for now we are going to assume the simplest case the stripe is one and all the elements are going to be loaded in two consecutive cycles as you will see and a way to achieve this is why having these banks by having memory banks we are going to be able to access to get from memory one element every cycle and this is something that without banks is not possible and why is that because unfortunately accessing memory takes usually much longer than one single cycle this is how Bank memory or interleaved memory looks like instead of having one single memory thing a monolithic array with one single MDR memory data register and M a our memory address register we are going to have a number of banks in this example 16 and each of them as you can see has its own MDR MMAR they still share something some important components as the data bus and the address bus so let's discuss how we are going to access them the first thing to to think about is how is the how are the memory address is going to be mapped into these into these banks right because if you have some sort of monolithic memory something like like this let's say that this is our memory this is element 0 this is element n minus 1 and I know that this is address 0 this is address 1 this is address 2 and so on and this is address n minus 1 right I have my M a are I am D R and if I want to read some specific address from memory I will put this address in the m AR and some time later I will get the value from later from from the memory and store it in the MDR some time later how much is it's a memory latency and this memory latency it's going to be M cycles so what's the problem here the problem is that if I need to access staying elements I will have to wait I mean I will have to wait in total 10 times M cycles right so the way of reducing this latency is by having a banked memory and in a bank memory I divide the monolithic array into something like this as you also have in the slide these are the memory banks this is Bank 0 1 2 up to let's say Bank 15 maybe we instead of having a 16 we can have 8 or we can have 32 banks ok and now the way that we are going to map the addresses into this Bank is by doing it in an interleaved way it's one possibility that this doesn't mean that always going to be the case but probably it will be the most usual case so this way address 0 is here in bank 0 others one is in bang-bang and here we'll have address 15 and here the next address is address 16 address 17 and so on right and this is this is address 31 so each of these banks has its own Mir and MDR Mir MDR Mir MDR they are sharing the address bus they are sharing the data bus and if I want to access let's say 16 elements and let's assume for now that that or strike is equal to 1 if I want to access 16 elements what I will do is in the very first cycle I going I'm going to issue the memory load for address let's say 0 so I will put address 0 here and I will start accessing this thing how many cycles do I need to access this element in address 0 well maybe I in still need M cycles right but the good thing is that one cycle later while I'm accessing this element I can start the access for address 1 and then I can start the access for address 2 and so on and if for example this M is equal 10 in cycle 10 well let's say this is equal 11 in cycle 10 I will get the data element that was in address or that is in address 0 and then I might issue one more data load and here one cycle later I'm getting data element in address one you see so this way I can fetch one element every cycle so after having paid let's say the latency of the first access this 11 cycles here I will have one data element every single cycle you see so my bandwidth it will be much higher there is another advantage here you know that to access memory I will probably need some address decoder right and with the address decoder essentially I mean here I will have the address and then I will activate the corresponding address right observe that this address decoder needs to be very very B so if it's so big I will that the latency will be higher right so if the latency is higher this M cycle will be a very high number of cycles but here they address decoder is much smaller so probably latency will be much smaller and probably also these M cycles will be lower than these M cycles here so that's another advantage okay let's go back to the slide and here you see one more picture the memory 16 memory banks and here you can see on the right the other day unit that we need to compute the address in vector machines this is also very easy very lightweight because we usually have one base address and we start adding one stride it was straight wagging or previous example but it can be something different depending on the value that we put in these special vector stride what was it stride vector or yes so how what I mean let's recap a little bit why do what do we need to get one element every cycle first of all in this example we are considering a stride equal to one because if we have a stride equal to one we will have we will be accessing address 0 here address 1 address 2 and so on so we can access all the banks in parallel we need also that the consecutive elements are interleaved across banks exactly as I was already assuming and another important thing is that we need that the number of banks is greater or equal to the bank latency observe that in the example that I just gave you here on paper we are assuming a latency of 11 cycles right and with this 11 cycles and 16 banks we can get one element every cycle whereas as soon as we start obtaining elements from the memory now think that the instead of having 16 banks we only have 8 banks what will happen we wouldn't have enough number of banks to compensate for the entire latency of the first access so we would have started a excesses to each of the eight bands and still will have to wait three more cycles to get the first element from banks forum Bank zero so in order to have an efficient memory access we need to have a number of banks that is at least as as high as the memory latency for each of the banks that's one important consideration another important consideration is the stride of selves that here we are assuming we were assuming a straight one and we were assuming a interleaved layout right so with this layout now let's consider that instead of accessing address 0 1 2 and so on we need to access the stride is equal 16 and then we need to access address 0 16 32 and so on what would be the problem the problem is that all these addresses are here in Bank 0 you see so this is what we call a bank conflict and the problem is that we cannot have accesses in parallel to different elements we will need to first access 0 wait for the 11 cycles and when we have element 0 we go to address 16 wait for 11 cycles and so on so this is one of the drawbacks of having banked memories but I mean still there are ways of dealing with this with this and we will talk about them later now let's continue with the slides and actually we are going to see some code start studying some code this is some color code observe that in whatever you say this is we have 1 vectorizable loop and in this vector I so value we are going over to vectors of 50 elements and we are calculating the element wise average of these elements and storing the result in vector C if we translate these or compile this into some ISA or scholar code will be something like this here you can see first of all move to raise their 0:50 which is the number of iterations number of times that we are going to execute this code here in register 1 the address a of array or vector a are two equal B are three equal C we need let's say just one single cycle for all these for these four instructions for each of them and then we start this loop right and in the loop the first thing to do we assume that this is memory access memory load class out to increment and we need 11 cycles for that to go to memory address array a and read something into register for go to memory to address whether there is a story in register 2 which is the address of register B vector B store eating in register 5 then we add these two then we shift to the right then we store the result and here we evaluate this the value are 0 which are we are decrementing every single iteration and if it's necessary we will jump and continue the execution for 50 times right so this is the latency of all these operations if all these instructions and a system matching if you count the number of dynamic instructions the total number of instruction that will be executed here it will be 304 and you can already also compute what's the total number of cycles that we will need to execute this in a matching if you do that you will obtain that number two thousand four cycles and this will be assuming that we have a monolithic array as a memory one single bank because we want to refer and in the vector example we are going to use a bank memory we are also we can also do the same and have a banked memory or interleaved memory for the S color processor let's assume we have 16 banks but here if you go back you could have for example two accesses at the same time if we assume that array a is in bank zero and array 1 is in bang-bang we are only be able to do two accesses in parallel because that's what we have here and if you assume that you can calculate was the total number of cycles that you will see that the number the value that you obtain overlapping the two load accesses as you can do if you have at least two banks you will obtain 1504 cycles we assume sixteen banks I don't need to explain this because I already explained it on paper but if you have any questions you can of course ask and now let's take a look at what happens when we generate baked vectorizable code or vector code for a Sindhi machine or code will look something like that right observe that first thing to do is set in the back row length equal to 50 let's assume that we can have vector length equal to 50 okay we can later discuss what if you only have 32 functional units yes of course this is something that we can discuss and we will do it at the end of the lecture but for now let's assume that we have 50 let's assume that well it's not not assume it's what we need for this code actually because in this for loop 50 iterations we are accessing always consecutive elements so our vector stride is 1 then we have in here these two instructions are two vector loads we go to array a or vector a we read something and we put it in vector register zero then we go to be read something put it in vector register one and then we have the vector addition vector shift right and vector store here they you have the latency of all these instructions the first two instructions are Scala they only take one single cycle there is no vector execution here why is that because we are we are just updating one single register in the whole matching array which is a V Lang or V stripe but the other instructions are vector instructions and the way that these vector instructions recall that is slide where we have the different functional units and we somehow pipeline the execution of the different for the different vector elements what we need to do is the same as we were doing for for memory we will have to pay the latency of the very first access the to memory or the very first computation for the first element in the vector array and after that we will be obtaining one new element or one new result every cycle that's why we have these in the case of the vector load 11 + VL n minus 1 in the case of for example vector add 4 plus VL n minus 1 as human that the latency of one add operation is 4 cycles and this is in total only saving dynamic instructions so compared to the 304 that we had for the scalar code only 7 so we only need to go to memory 7 times to read an instruction and you will see that this is also much faster for now we assume that there is no chaining what does does this mean this means that we cannot do data forwarding among or between different consecutive instructions we will see we will see in more detail later so essentially this means that if I execute this vector load first vector no ok execute the first vector load and then the second vector load of Sara die I need to execute both before doing the addition right but the way that I execute these two instructions is by accessing memory I need 11 cycles for the very first element class 1 cycle more for each of the remaining VM minus 1 elements this means that right after 11 cycles or right after 22 cycles let's say I can have the first element of B 0 and the first element of V 1 so this means that I could already start doing the addition for this first element of V 0 and V 1 if I do that I am assuming that I have better chaining that's kind of data forwarding so for now what we assume is that we have to wait until the very first vector load has completely finished the second vector load has completely finished for the 50 elements that the vector length the length tells me and only after that I can start executing the adding the addition and that's why this will be the timeline this is the way that we execute this with that we'll take two 285 let's take a look here because it's going to be easier if we look at the code here can you see it more or less I think this is better okay so look at the timeline first of all one cycle this cycle is for what for this movie instruction scholar instruction one cycle second cycle for the next scholar instruction and then I have first vector load for the vector load recall how we had to access memory we put the address in the memory port on the MA AR of Bank zero and we start accessing memory we need 11 cycles for de very first element and then we will start obtaining one element from the remaining banks every cycle right and that's V Lang minus one which is 49 so this is what I need to execute this first vector load because I only have one single memory power didn't this means I only have one single Mir and and one single MDR in each memory bank I cannot start executing the second load and the life finish with the first one so that's why after that I will have eleven plus 49 and because we are assuming that there is no vector chaining even though observe that here we already have the first element from b0 and the first element from Wiebe we cannot start with the addition until we have finished completely with the two loads and then we start four cycles for the first element of this vector add 49 more for the remaining elements VM minus 1 and so on so if you calculate you add all these numbers you will obtain these 285 cycles is that clear any question I think you you will need to think about this yourself a little bit and and yeah I think that if when you think about the example you will see it very clearly and it's very important to understand this very well ok now let's assume that we have vector chaining that is as soon as I get the element 0 of v1 in this example code that we have here as soon as I get it from memory I can forward it to the next functional unit which in this case is x even need to write to the vector register I can directly forward it to the multiply unit and when I'm done with the result note here that I'm going to store in some of the slots in v3 I can chain and store in the so and start the computation of the next addition so if I do that observe that the good thing is as I said before as soon as I finish with this 11 cycle which correspond to the load of the first element of the second vector load I can start the addition that will take 4 cycle plus 49 which is vector length minus 1 and as soon as I'm done with this addition I can start the next instruction which is the shift right you see so this way I can reduce the number of cycles a lot but I still observe that I cannot have accesses to memory in parallel actually even here this is a store operation even though the result for the first element of the vector register is completely down here after executing the shift right I have to wait a few cycles until I start the store operation and why is that because I have one single memory port in each Bank I only have one Mir and one MDR the 49 giving day 11 yes I have you need to have the you need to have the value in the the others in the mir after you and until you complete the entire memory access no no you cannot let's say save those nanoseconds or whatever for that what you need is what we are asking ourselves here and the way to do that is using let me show you the example using multiple ports I have Bank 0 here bang-bang and so on 15 recall that for each of these I have 1 m ir and 1 m dr i have also for bangbang I have also for band 15 and now what I could do is adding a second memory port for example with second M IR and second and D are so I can be accessing memory two addresses from each Bank at the same time and this is the way that I can have I can improve the memory access a lot and that's what we have down here and that's why still still recall that we have one single address bus and one single data bus so that's why we can start this second vector load only after one cycle after the first vector load started but using the second port here the second load port because actually in this example we are considering two ports for load and one port for stores we can start the second vector load and also because we have vector chaining you see how efficient we can execute this code here only 79 cycles which is more than or 19 times faster than the execution of the color code okay okay yeah let's continue questions what if question that you were asking before what if the number of data elements is greater than the number of elements in a vector register because as of now we have done say okay let's define vector length equal to 50 or let's let's say a vector length is equal to 32 right but now we won't have 1 million elements so we cannot operate on 1 million elements at the same time right so what are we going to do we will need to break loops here so if we have to operate on 1 million elements and the maximum vector length that we can have is 64 as in this example we will have to operate on 64 64 64 and now let's assume that the number of paper elements that we have to operate on is 129 if it's 129 we will first operate on 64 elements we will load at store and then we will jump we still need jams we still need branches in the vector code to operate on the next sixty-four elements and finally the last thing to do because the total size of the array or the vectors that we are considering is 129 we will have to change the vector length in my example to 1 in the example of the slide to 15 and then operate on this single element or these 15 elements okay this is what is called vectors strip-mining and the reason is because it's some somehow an analogy to what is called strip mining in in mining which essentially means that if you have a man and you want to find gold you will have to remove all the dust and all the dirt that is in rocks etc that are on top of the of the gold right in in more code the gold are the 64 elements that allow me to operate on all of them at the same time taking advantage of all the functional units that I have in the system while the dirt what I really don't want to have are these only 15 which are somehow wasting the power of the the computing power of my machine ok that was the first question second question what if vector data is not stored in a strided fashion that's also a very important question right because for now we are considering ok let's have a very fishing access so what we let's assume that vector stride is 1 which means that we are going to access all the elements of the array or I also gave you an example let's now assume that vector stride is 8 ok but still is regular right so I know that I can act I have to access element 0 and then element 8 and then element 16 because my stride is 8 and recall that we could even prefetch because we could know that we will be end up accessing element 32 on element 40 and so on right but this is not going to be the case in all in all codes right so that's why we need some way of using indirection and this is what is called scatter and operations I want to give you very quickly an example of what scatter operations or gather operations are and one good examples are sparse vectors what is a sparse vector is a vector let's assume that I have this in memory vector called C and it turns out that almost all the elements of this vector C are equal to zero except some of them for example this one is 4 and then I have another one it's a thing and then I have another one that is 50 and the rest of elements are zeros so this is a sparse vector so if I access it let's say in a dense way if I access it as if all the elements were useful it would be a waste of time right and a waste of energy as well because I could go over all the elements of the array and maybe this array has 1 million elements I know only 3 of them are useful right so what should I do here let's assume that these 4 is in address 1001 these guys an address 2050 and this address is 3072 what I will do here is having a second array let's call it D it's an index array or vector and this guy has 3 elements and in these elements what I have is the address of the elements that are different to different and 0 so this element of these pointing to this element of array C this one to this one and this one to this one and if I want to do whatever with these 3 values here the first thing to do is to access the eye and after having access the eye I will access array.c so if I want to get these four I have to Z which is the address of array C plus D 0 which is C plus 1000 1 equal 4 and for the next element C plus D 1 which is c plus 2050 which is equal 10 and so on right so these kind of indirect accesses are called scatter or gather accesses in this case in particular it's a gather access because I have this in memory and I'm gathering it I'm reading from memory ok if when I'm writing I talk about s cutter and this is what you have in in the next two slides so this is an example of gather here same as my example d is the index array C is the array that has the values that I want to access that's why the first thing to do is loading values from loading the contents of our array D and with this array D I do this load indirect to access array z you see and now example of the sparse vector that I already gave you and this is the same but for scatter here the index vector the vector D is that one this is the data vector that I want to store let's call it B for example and this is the vector C where I want to store what I do is I take this 3.14 I go to the index vector 0 and I store these 3.14 in the address base plus 0 being based equal to C the 6.5 is in address base plus 2 I store 6.5 and so on ok we need these instructions as well as you can see the memo we access here is not so efficient right because for every single read or write useful read or write I need to access memory twice but still I of course need a and then conditional operations we have already talked about that a conditional operations is where we need to use the mask operations where we need to use the vector mask I already gave you this example as well here this code is even simpler than the one that I used because we just we simply have have one if if a I is is not zero then I will do this multiplication if it's zero I don't do anything yep writing yeah gather scatter okay okay so math operations yeah is it's essentially the same example that I gave you before on paper you see for that particular code vector load from a vector load from B I compute I obtain the mask and I will only execute this multiplication for those elements where we find the corresponding bit of the mask equal to one if the bit is zero because they don't fulfill this predicate then I don't execute the multiplication for them and here you have another example that is more similar to the one that I gave you with if and else and essentially the same thing that we have already seen compare and obtain the vector mask compute whatever do this and then compliment VMs and then do that is that clear okay yeah and here you have the values of the vector masks you you can you can take a look one more thing about mask vector instructions because maybe you are asking yourself how do we really execute that on the hardware we already know that this bit of the mask if it's zero we don't do anything if it's one we execute application or the addition but how do we really implement that on the hardware well there are two possibilities the simplest one which is less efficient but is simpler essentially execute everything for every single element no matter if it's the mask that you can see here on the left hand side is 0-1 you just execute only if the mask is zero you don't write in the output register and if it's one you write okay that's the simplest implementation good thing that the hardware is simpler more efficient way doing what is called a density time implementation in this density time implementation the first thing that we will do is going over the entire vector mask and only issue the corresponding operation let's say the addition let's say the multiplication for those vector elements where the mask is equal to one that's why here in the pipeline you only see see one so you only see exactly see one four five and seven okay which one is better well I I hope that this one will be more efficient right because you are saving cycles you are not doing computation for nothing the drawback is that here the latency for one single addition or one single multiplication will take a little bit longer right because you need to first go over the entire mask and and scan it right inspect it because before starting the computation okay some more issues to keep in mind this is also something that we have already discussed now we are going to discuss it a little bit more with an example matrix multiplication and we have strides usually yes oh so what's the tone animal of cycles that you will need to execute this thing here and what's the total number of cycles that you need there you need ok whatever is your let's see 4 is the latency 4 plus VL M minus 1 right but here is 4 plus only the number of lanes or elements for which you are really operating ok ok so let's go back to the discussion about the strides recall the example if our stride is one perfect because I will be accessing one element in each of the consecutive bands a banks but I already gave you the example what if it's 16 we will have a 16 way Bank conflict which means that all 16 memory accesses will go to the same Bank right so the latency will be 16 times longer the total latency will be 16 times longer and this is always something that is going to happen if always if they stride and the number of banks are not relatively prime so for example if my number of banks is 16 16 is a power of 2 right so it's the only divisor of 16 is 2 if I'm using let's say an odd number for this trial let's say 1 3 5 I will never have a bank conflict so the my axis will be maximum efficiency but if it's not if they have some divisors in common for example 10 and 16 I will have some Bank conflicts and and that's unfortunately and unfortunate but that's the way it is there are ways of dealing with this as we will describe next well there just let me give you a very quick example let's consider the a storage of a matrix right and we are going to so the storage of the way that we saw a mattress at a two dimensional organization in memory can be in two different ways at least row major and column major what does it mean okay think about memory memory it's a linear array right no matter if we are using banks or a monolithic array memory it's actually something like this right this is address 0 this is address n minus 1 and this will be address 0 address 1 address 2 and so on and that's the way that the different locations or positions in memory are addressed linearly but what if I want to store something like this like a matrix with a row size equal 5 and column size equal 4 right number of columns is for number of so we're on number of columns is 5 number of rows is for how do I store them should I do it row major which means that element 0 1 2 and so on are going to be stored like this or maybe should I use a column major layout if this is because this is a element 4 this will be element 5 this will be element tang and so on so another possibility would be to store here element 0 and here element tang and here so sorry element 5 and here element thing and so on depending on the way that I store these like I can do row major or I can do column major that will depend on the machine that will depend might depend on the programming language as well it depends so for now in our example we are just going to assume that we use Roman Europe and with this row major layout we are going to multiply two matrices matrix a and matrix B so you know that if we want to multiply matrix a and matrix B what we really want to do is what with what we essentially need to do is calculating the dot product of row times column so I will have to calculate the dot product of a zero which is the first row of row of matrix a times the first column of B this B zero right I do the dot product I am taking a value and and I obtained the first element of matrix C that is a day output but now what's the problem here the problem is that I will use row major column major for both right so both are going to be a store in memory in the same way and if this is matrix a and this is matrix B and this is my memory matrix a will start for example here this is element 0 element 1 element 2 which is element 0 element 1 element 2 but for matrix B that will start somewhere here this is element 0 1 2 and so on and maybe this one here is element n right so I have element 0 of matrix B here an element thing of matrix B here so I have a strike when I calculate the dot product of this thing and this thing for a that's good I am just accessing consecutive elements so I am accessing consecutive blanks but for B I will have to access this element and this element and probably they will be in different banks but at some point unless this is tried which is 10 is relatively prime with the number of banks I will have a Bank conflict I think that is something that must be already clear right so using a strife unless these strides are relatively prime with respect to the number of banks I will have Bank conflicts Bank on so how can I deal with these bank conflicts there are several possibilities they straightforward one let's put more banks if we put more banks what's a problem my hardware will be more expensive right so probably it's not the the best way to deal with it another possibility is trying to find a better layout and it this is not always possible but in some cases it is and this is important if you want to be for example a good GPU programmer because there is one very useful type of memory and in the GPUs which is an on chip scratch pad which is actually a bank memory and this is the typical thing that if you are a GPU programmer you will have to optimize and take into account maybe some of you if you have already tried to program GPUs you might have already deal with that so what I could do for that one one way of optimizing the layout here and playing and having a better memory access for matrix B what I could do is transpose matrix B and then I will obtain B T such that instead of having element 0 and then element tang far away I will have element 0 here element in here element 20 here and so on and this will be 1 2 etc the good thing of doing these is that in memory I will have element 0 here and this will be 10 and this will be 20 and so on so now my stride is 1 also for mattress B when I multiply this thing and this thing ok it's just one example of possible let's say software optimization to have more efficient access to memory and then there are also some proposals hardware solutions like for example something would that was presented in this paper by Bob Brown very very let's say seminal work in 1991 which essentially is doing some kind of randomized mapping so you don't really know what bank you are accessing and you have some kind of hash function that for this address you go here and for the next address you go there and the good thing is the total number of Bank conflicts for typical applications is greatly reduced problem of having something like this in the hardware is that if you are the programmer and need to start trying to reason ok where is my address map you don't know that right it's more much more difficult to figure out at least ok yeah let's continue we are I think almost done with sim D processors just recall that the distinction between array M vector processor is poorest this distinction current Cindy processors like GPUs they are more like a combination of both recall so this is slide probably we will also take a look at this slide tomorrow and about how the instructions are really executed on the hardware we have all throw the whole class we have been talking about the vector length and vector length means the vector elements that you can be operating at the same time right and one possibility is having something like this I have a one single functional unit with a probably deep pipeline and I start issuing operations through this pipeline first of all I operate on a 0 and B 0 to obtain C 0 and then for element number 1 for element number 2 and so on the instruction is exactly the same in this particular case vector length is 8 but what I'm doing is obtaining one element every cycle and actually and in the example that we discussed for this vector code is that is what we considered right GPUs these days are more like something like this they are still pipeline but you not only have one single unit you may have more let's say 4 in this example so here the good thing is that exploiting parallelism not only for concurrency not only in time but also in S phase and this is how these functional unit my dreamy look you see four functional units up in the top four functional units on the bottom they might be for example addition and multiplication and observe that they have access to the same registers this is called a partition register file and essentially what this means is that in your matching you have a lot of registers and when you run your program you will need a certain number of registers and what the hardware is going to do is say okay how many what's my vector length it's 50 okay let's divide the total number of register by 50 and to each of these guys I'm going to give the registers that they need so that is why in the end what we are going to do is that in these all these lanes which are the combination of all the functional vertical functional units that we have together we are going to be executing vector for vector element 0 for vector element 4 for vector element a and so on and and the elements of laying 1 will be here elements are of length 2 will be here and so on so it's somehow also interleaf right you can see it and we will take a closer look tomorrow at this slide as well but this is how we are going to execute on this type of machine this type of mix between vector and array processor here for example we have your vector length is 32 but we only have ad units right so we need let's say four cycles to issue each of these instructions for the 32 elements we will continue tomorrow thank you very much for your attention [Applause] 
FL240cnJIRM,27,"lecture on Computer Architecture and Organization: Introduction to programming CUDA, lecture by Tongjai Yampaka, Faculty of Business and Information Technology, Rajamangala university of technology, Eastern campus.  Getting started using CUDA in Colaboratory.",2020-04-20T16:01:11Z,lecture on Computer Architecture and Organization: Introduction to programming CUDA,https://i.ytimg.com/vi/FL240cnJIRM/hqdefault.jpg,prabhas chongstitvatana,PT23M4S,false,295,3,5,0,N/A,hello I'm dr. yarmulke I will introduce a high-performance concept if you you single dad driven by the market demand for their town high-definition 3d graphics that were capable gothic was the unique part of you has involved into a hybrid El Monte trade many couples a cell with computational power and very high memory bandwidth the listen we had in 14 high capability between the CPU and a GPU is specialized for highly parallel computation CPU designs more transistors and evil to data processing better than data caching and full control this concept work for highly parallel computations because the CPU can hit merely external agencies with computation instead of our demon exa latency for a data cache and phone control the develop was a scene Maps data elements to parallel processing trace many applications death was a lattices can you have data parallel programming model to submit after complete his hands indeed Evangeline while self up he sells and maps to panel interest for example he made an immediate process leak advocates and said asked what was a sing-off mentally it made video encoding and decoding in mescaline study of each Sun and pattered lacunae th and can map inmate rocks and pixel to panel for sustained rail in fact many and collective outside the field of email processing encouraged by the taffeta process being said as physics simulation computers in their in finance are computers in their in biology in November 2006 a media interview who that general parallel computing computing platform and rhogam in Modell's that little bit parallel computing in immediate reviews to so many company computers in their problems in more efficient way dance with you CUDA is a software environment that allow developers to use the podcast as that high linear programming language other language CUDA is designed to support various language and application programming interface the event opment accosted used and many court abused mean - means the impulses and chips are now parallel systems that talent is to develop application stomp a testicles is perilous to elevate the increasing number of my sister calls such as 3d graphics applications games their paradism too many call TV use with very number of cows and it caused a tiki extra chance to first highly keep up twinkle the second German release and a lot value synchronizations less as in playas fall to the broken world as the minimum sense of language attentions this after transfer Wi-Fi can deter politicians and trade policy they guided programmer to perish under problem into sub problem can be solved independently in parallel by brock of trace and if the problem can be solved in parallel by always be integral this decomposition is selling to it by our own trades to cooperate with soul is the problem and at the same time and about automatic stability is well opt rest can be scheduled on after our about multiprocessors in CPU in any other concurrent e our secondary so - a compiled CUDA broken can execute on any number of monte processors GPU is built on an array of the mimetic processors when today's program is partitioned into blocks of test that is the Q independently from each other so - attribute list moment equal success will automatically execute the program in less time the main concepts behind who they're broken in divided into two paths of course the first ll cost is accused if you trade this car was caught and parallel called execute in many concurrent GPU tests across multiple parallel processing elements is called ey called @mention in heterogeneous programming that could have broke a mid model assume a system from post office of a host and device is mr. on separate memory Koerner operate of device family they learned hammer wise from santo on okay we allocate and copy dy memory as the a stanford actor between host family and device memory and an air show detail of cuda architecture Jimmy you can hitter can handle her sense of concurrent Hayes coulda broke a mean mu nu sub past even more I love a kernel lunch to specify more trade Dandan city you can execute concurrency and health to American Airlines time the trade goes into browse and browse I could into a quick I'm Colonel east execute as I keep up box up trace kuda allows for gamer to defy function called colonel that when call execute in time in parallel by in different coup de tres Antonio is defy using the Cobo decoration and number of coup de tres executed Colonel fogey when Colonel is specified using a new banquet instead is secured colonel given a unit rate ID asked an irritation for example cost you sing bill in wearable tech IDs and to vector a and B site in and start this out into vectors least yeah we stop the interest is the cubic a function by one pair wise addition for convenience cred idea is three component vector so that rate can be identified using one dimension to them in son not readable son bitrate index for me one day return to the Vincent all three dimension bra octave called our trade box this providers are aware to ignore computation of cod elements in a domain such that vector metric power room there is that limit to the number of tests per bar since our head of Bach aspect to the size on the same processor how and must say the limit member really start up that car alternatively use trade book may contain up to 1024 tail however a kernel can be executed by multiple equally chips trade box so there's the total number of trades it equal to the number of trades or bath times the number of box rocks okay night into one David sent to live in Sun or three dimension pick up trace bomb the number of trade box is up quick it's just be deterred by the size of the data being processed mystically exist the number of processors in their system is brockman in the grid can be identified by one David sent to them is an ultra dimension unique in the accessible within the kernel try to Bill Engvall I declare ago the dimension of the trade box is accessible with in the kernel try double in Brockton is and well above all slice of 16 x 16 256 traced although ability in this case is common choice that East create within a box to have one trait or metric remains as before for simplicity this example assume that the number of trade per click in each dimension it evenly divided by the number of trade Baba in that dimension that did not be the case that both are required to execute I didn't independently it must be possible to execute them in any order in parallel or in series this independently women are all trade bar to be scheduled in any other across any number of course and illustrate by victor enemy in the way programmer to write court that's the care with gamble oak are trade in above can cooperate by sharing guitarists on shared memory and by synchronizing their execution to coordinate memory access pop easily one can specify synchrony pie in the kernel by course interests in your function st. read as barrier at least altered in rock must wait before any other to persist ceremony keep an example of using ceremony in addition to since day the co-operative group API provide at least said after synchronization primitives for efficient compilation the ceremony it expect to be long that in say memory he is policing home the event head of CUDA CUDA has similar event head almost a decent return of more computation on CPU you see cafe api's code can lead from Emil Holly addressed in memory unified were shown Emily unified memory Sam Emily CUDA F for fast ceremony it's and that can be shared among train this is can be used at least a minute cast in the mean higher bandwidth than is learned impossible you seem to excel us back at our Lord and lead back to and from the CPU for support for editor and reply operation encoding insert Hector look up they have some limitation copy between host and divine Emily may recall a performant hit due to system bus bandwidth and latency unlike open cm to that interval GPUs are only available from Nvidia no number no emulator of for back fortunately is available for model deletions and a nest I will show according to you then you don't have the real TV and now I interview the collaboratory horrible agony girl called up for shot allows you to write and execute Python in your browser we still on FIFA descended by the exit to TV use easy turning continue our student at that directive or a ad said she'll call up and can make your work easier the document you are reading east on static static webpage but and either type in women call a collab notebook does let you write and execute code you can click press call for new sales while you're cold and creepy but one phone on your club kitchens closed you can see the new sale and write your code and then click the play button phone on your call you can see there is now in the below collab not books allow you to come by executable code and least take in a single document along with emails Haiti and now they take and mom when you create your own collab notebooks they asked out in your keuken dry and cow you can easy tell your Co lab notebooks with coworkers or friends allow them to comment on your notebook or even nickname to learn more you can see OB of call at websites to hear and in college football you can use a family of all our youthful living link here and in collab not to call a Pluto and Jupiter notebooks that can host I collect you can learn more about it to be tugboat X need to be turned up on Artie 10 sent for me TP you this notebook provide an introduction to computing until view in : in this notebook you connect to active view and they learn some basic tension for operation unbothered CPU a data view of the winners to be a y-value single to be you you can enemy contesting the cpu but you need to enable GPU further north or you can do we get to eat it notebooks that thing and selectively you from hardware you later duck down it is and on top setting you can see one time ty we can select python we operate on two and hardware a curator none is only CPU and you can select if you are tip you then you can save and now you can look if you wash and dad call apple iphone but we you say nvidia is a icon man the least out role did you that call Apple why for you test lab he for shine and who died was hunted by one and then you can use not only Python that also see also see landed on column and you need nvcc for Jupiter environment installation is successful bill nvcc back in on you on call a box ahead then you need Lord nvcc for Jupiter environment in your box appears okay and now you ready to write your TP you call in this life the Chabad equation I can l st file using the coordination and it function do add each corner value and this lie in this line indexing deceive you Elaine just lied a memory editing are other index things okay and this function for using CUDA edwinter in parallel now you need dick dick Alicia at dead-on dy t tu la la depth is la a RTP you let me is device la on tu le B and D wisely is Alessi on CPU and then allocate if you Buffalo forty vector to input a B and one output is the you sing CUDA memory allocation command syntax and copy input from host family to TPU buffer using could a family copy nap a is la a on TPU and air East LA a on CPU did function copied an a a prompt if you to LA a on TPU and lunch actually under to be with one trade for East element the number of coup data is the key to Cornell this lie call one Bob and fast head Robo CUDA device synchronized well for the Kenya to finish and he turn the lease house or any other and now copy a vector form CPU buffer to the hospital in using command who damn Emily copy copy or literally sell Alessi the desal farm unless the on TV you to Alessi on CPU you see khuddam family copy device to most and Qaddafi FC FA and FB is not command hooli least to llenita de la and now Koda main function if I input variable la a B and C a me and the input and say start output and called the view function and now shoulderless house you can in the South in the horse or in the CPU and who daddy while he said his reset device okay double-o shoddily styles for this cause this video explains sin simple how broken up CPU and I think this video can help your world thank you for your attention 
zsEeVpfGwg4,22,"Review of Gray Code, K-Maps, and Race Conditions
Introduction to Computer Architecture / Organization
HW 2: TGO 1.13-1.26, Example Q: 1.3-1.4",2014-08-28T19:16:38Z,El E 385 - Lecture 2 - 28 AUG 2014,https://i.ytimg.com/vi/zsEeVpfGwg4/hqdefault.jpg,Matthew Morrison,PT1H12M22S,false,153,1,0,0,0,hey good morning hey so for everybody who survived here Harry Potter marathons from their roommates I hope you're alive and well yes you have a question oh yeah I what I'm I will hand be handing these back the next class period so you will get them back immediately I mean as everybody turn in their assignment lousier ok great so in that case what we're going to be reviewing a three guys familiar with gray code karnaugh maps and then race conditions or anybody who's not familiar with those ok good so this hopefully that will fly by and then we're going to be going into the basics of the ear architecture of your organization so we're building up these medium scale integrated circuits towards an actual computer architecture so this is fitting these system attributes of the computer which contribute to the logical execution and then the code that you write you're able to write code that will be broken down and can be sent through the computer architecture so that's the end goal and it'd be kind and the time we're going to be discussing computing performance how you can actually change certain aspects of that of this computer architecture that you're pure organizations to make the computer run faster before I begin to anybody have any questions about the to recover lesbos-pretty straight board excellent let it do that what the heck i zoomed in date hundred alright so here we are okay so basically so here in this situation we have it in to us what's known as the synchronous up counter so the whole idea is you have a set of D flip-flops which some of you are familiar and they have a clock signal the square wave clock and this or what user what card one is positive edge triggered d flip-flops and the idea of these is that they only flip have a positive edge and the value is actually one so what's going on here is if it's a zero it just holds like a memory element that's the goal and so every time the clock goes to one it's supposed to increment so if you have 0 0 0 ideally what you have is this value should go up and it becomes 0 0 1 so that your no idea of their counter now there's an issue with these is that sometimes you get this thing these things called race conditions is we're going to discuss today in class because of the way the clockworks it goes to all three at the same time and you have instances where trim the transitions aren't always the same so you will get the rent method scenario the 1i half years nero 11 and ideally we want to get to 100 but because of the way it works they'll actually start out the positive edge it'll actually switch the middle one first 20 and then after that because of a way of bobby's place on it and will then 101 got a whole transition step here I'll underline it and then it goes to 101 and then finally what's the last bit the 100 now this is the key point here if the output feeds into a sequential system we were talking about last class it can't quite tell whether this is the right value this is the right value or this is the right so how do we this is what's known as a race condition we have these switches where you have intermediate values and we try to minimize those so in a combinational system you just get your input and you get your output immediately so you don't have to worry about that but in the sequential olmus becomes an issue so one way this is done is done with this thing called gray code and t0 one point 13 is great code is a set of continuous values in a circular list where the Hamming distance is one and what we mean by handing distance is that the number of values that change in every single transition so for example in regular binary code this transition we're actually switching all three bits right so the Hamming distance is through we have zero one one that goes to 100 so that's our worst-case scenario that we're trying to account for so what the gray code does is we have 0 0 1 0 0 0 0 0 0 1 and then if we go 2010 we're actually changing to bits so instead of noon as we float the middle bit and what's happening there is by only switching one bit we avoid the submit scenario we were just talking about in the synchronous up counter do I need to move up the truth table ok um I'm just trying to make sure that I get it so that way everybody can still see the topical guide objective which is at the top of the screen I'll read it again for you guys gray code is a set of continuous values in a circular list where the Hamming is one so this is a 3-bit great good okay so ever we should go see that is it okay um so here instead of going 2010 for movie month based n2 we go to 0 1 month is we're now only switching one bit same thing now we have 011 in the binary coding we can just change the least significant bit 2010 and so forth and so what I have below here at you guys probably have learned this before since you're familiar with gray code is this a little trick of how to generate the gray code for any number of bits so you can strike a 1-bit list 0 1 and then reflect it through have 0 1 1 0 and then you want to append the most significant that's like you normally would like you don't know when you go normally if you're writing binary the 0 1 0 1 and you have 0 0 1 1 in front of it so you want to attend so it's become 0 1 1 0 and then to add make it to bits it becomes 0 0 0 1 1 1 1 0 and then you do the same thing you reflect there's nary here 1 1 1 1 0 1 0 1 1 0 1 0 0 and then you add two zeros and ones so this is how we were able to derive the three bit grey coat if you were to do the same thing you could derive a four bit gray code by the great go to 650 acres and so forth so therefore you can come up with this circular list and you see at the end of the three victory come by only switching the one to a zero it comes back to the initial state so that's how you're able to get the circular most which is part of the definition and it turns out the properties of gray code are very very useful we're going to be using them a lot in developing state machines particularly with karnaugh maps in a little bit but i hope i've listed here enfamil with the towers of hanoi problem the whole idea is you're trying to move these disks on two different pipes in your trap that you know will move more than one yet the goal is to get them all on there and using gray code makes it a lot more simple to salt the house opponent problem and genetic algorithms because of these incremental changes in the ways that the actual think all of the elements in the GCAT how they change using gray code you can use those to map those more official tho those binary values aren't a one or two they still flood saying numeric value correct so they're they're unique so the whole idea of any binary machine language is you want some sort of unique binary value to represent so you're never going to get 0 1 1 and then 0 1 1 here so here 0 1 0 is always going to read but reflects 3 and then here for five or six one one one is always very blessed day so one is its unique it does reflect the binary value any other question okay so gray code are used in these things called karnaugh maps which are commonly refused refused referred to as K maps and they are a pictorial method used to minimize boolean expressions without having to use boolean algebra theorems and equation manipulation so we're not really going to go into using klein Mikulski reduction in this course but we are going to be using K maps a lot the types of questions that we produce on an exam we will be using for using K maps and came apps use this great code and the reason why is because I'll go this too it was a little bit is that you're able to see race conditions in sequential elements and by using by making sure that the overlapping elements within became app are not disjoint we can reduce these we can eliminate these race conditions and ensure that the value that we're sending out to the sequential state machine is correct every time and so the next value here we're going to discuss the difference between a min term and a max term a min term is the instance of the function where the output is 1 and it shouldn't symbolically as the function is the Sigma of mi and uppercase M is going to be the max term here so basically if you have an and gate this is a simple version so you have 00 01 10 and 11 and then the output serve 0 0 0 and 1 this is your min term that's a simple version and all of these are max terms so the summation of it would just be a equals 3 because 0 1 2 and 3 and then it'd be 0 1 and 2 and then you can use that to actually derive the end function in a very simple karnaugh map and so the max term is an instance of the function where the output is 0 is shown symbolically in this manner and I had a guide for Moses who actually when I asked him like things that students and interviews don't know he actually sent while this huge 5-minute ran about the number of times he asked students what min terms and Max terms are came after they have no clue so that's why it's a TG oh I know it's review but there's like yeah maybe I should do that if I people if industry people are ranting people not knowing it right so so but you guys understand the general idea of min terms and Max terms and so here's an example so this is a fun three input one output function and so we have three values of one forms are four values that are 100 110 1 110 and 111 so the min terms are derived as 3 5 6 and 7 and so see here we've built it up the midterms are 3 5 6 and 7 and by default because we don't have any don't cares in this all of our max terms are become the remainder so it's 0 1 2 and 4 and so building the k-map this is the actual karnaugh map itself here and the idea is what I've done is you can see here we have you can split these in any way you like so here you have a B and C hey is this column going down and bc is the comb going across and we count this way 00 01 11 and 10 using that gray code and so what you're able to do is it show kind of get down here in the example you can actually try to find ways in which they overlap we're all the minterms overlap and based on that you can actually derive a function that can serve as your circuit so in the example I gave here up a not bc are true these two got these two and these two right and so these are the true always when BC is 11 this is true when it's a be just these two for the green and for yellow it's AC and if we were to do the karnaugh map like this where I circled these two and made sure they weren't a disjoint this could be a BC sorry rights a little slow or a B or C so this would be if you did the karnaugh map in this manner and the reason why it's B or C here is because it's instance where it's always true if either value is 1 so you're able to look it all up that way and then from there if you're right a circuit yes originally we only do two or a so we can do three if we make it logically this yeah this is a valid count caught on that yeah we'd be weird if you did what's that okay well you're learning it like it now so a so if you you can only use 24 meaning that you get a karnaugh map and you could only have 2 4 8 and 16 inputs or likely roofing put it roommate is a bigger right well I mean we're going to go into bigger maps of course but you know you can still derive the function this way okay that's weird okay well what is here's an example it will make you feel a lot more comfortable that but this is it I mean this is a this is a classic this is actually an exam I mean I took a lot of these examples from the actual papers themselves so karna use this came out so he says it's okay okay so here's an example question 1.3 in the example question if I would give this on an exam I'd phrase it just like this where i will give you the min terms and the Maxim's and then you will derive the came out and come up with the function so in this case a B and C V make sure that you use grey coat and then here it's 0 2 4 5 8 10 12 so 0 to 1 so this becomes 10 4 5 11 was it 1000 8 this is 10 that's it goes well and then here I specifically adjustment to the redbox was we're on all the cds 00 the blue box goes around in 1801 so this becomes a is 0 and C a0 4a0 1a0 so that's how that comes out and I did why do you think I made the green part so they came up that way yeah they look they are walking around you want to make sure that you want to sure that that you get all the parts of the canine okay and then from there I indicate that the function for a green is a not be not be a ought V dot d not so you see here a not be not and then d naught is these two or a B naught D not right so that reduces to be not and D not and then red is just C naught D not because this is these and then blue is a not b not c not so then you can use those to come up with your final solution which is this yes um when I beg you a question like this on an exam um it will be part of a bigger question which will be getting through a little later but if you let's see that's a good question um I suppose in the interest if you're able to successfully identity or they're going to be going over his race condition and I'm going over the deficit what I watch them to account for each conditions and what I don't which is basically I guess sequential element or a combinational element the combinational elements I just want the smallest element possible whereas with sequential elements which I kind of did here you would want them to be out for so you think correctly derive became at addressing the question so you can derive the came out and come up with an equation that's fine okay so here TG o upside just does everybody understand this for a move on kind of questions yeah so the left column going down that's 80 in a not being on an minutes ya ain't not i'm very happy not so here's a CD and so all of the instances of c not being odd are taking off so that's why that's part of the equation for here um I all even wrapped around so that way i'll be going over this next so but all ideas you want to make sure that in the instance or boarding race conditions they are not they are not disappoint me if i only have these two and they wouldn't be connected to these at all that's known as a disjoint graph all right does anybody because it may not know what i mean by this joint okay the idea behind this joint is let's say i did this say i did this map and i did this this and that s so you could use those to derive a function and then use chemicals e to come up with a reduced function but this joint means that they're not overlapping meaning that they're not going together they are disjoint and what happens the property of it being disjoint and i can actually scroll down to the whole race conditions and disjoint to explain this a little more the race conditions are like what we described the beginning of class where you have those flipping and the ones and zeros and an up counter since asynchronous circuits do not have discrete timing signals to regulate circuit operation eliminating race conditions is a very important concern so what happens here is this in this problem here i'm gonna scroll up and I'll scroll back down I promise um this is a disjoint graph if we extended this this blue circle over then all three of them are going to be did and then they're joined together so this is an instance where you would have a race condition so I'm going to scroll back up just a little and then go over the definition of race condition a race condition is the behavior of electronic or software system where the output is dependent on the sequence or timing of other uncontrollable events so the instance of the uncontrollable event the beginning of the class is the up counter having to switch from 0 1 1 to 100 and we had that timing on the outlets and I've times gold at various and mine will save you lots of time in coding projects in this course and in your career so the whole idea is that you say you're designing a counter and you haven't accounted for race conditions well you're going to get weird outputs propagated further on into your circuit and your have a hard time troubleshooting it's like wait a minute why does this lal you give me bad outputs well it's because of some synchronous circuit they didn't properly attack a race condition yes the so you're saying that their digital really we keep them separate from each other no no you if there's this joint then you cannot guarantee that there are no race conditions so tjo to one point 17 because this answering your question raised hazards are easy to spot using a karnaugh map because a race condition may exist when moving any pair of adjacent but disjoint Regents circumsized on the map meaning that if they are if there is a region that does not connect to any other region there may be a race condition whereas if you were to connect them then the race condition can be guaranteed to not exist does that make sense so they're easy to spot on the karnaugh map and so I so here we spot them on the karnaugh map there it is now I can get rid of this in one of two ways I could extend this here or I can do another one here or I could even do I even II can extend the red one so that way those are taken up so there's multiple ways that you can look at it and if you're in the instance of trying to reach you know in the case of trying to reduce the size of the circuit then you have to account for all of this does that answer your question about race conditions of wear on that it's really yes so easy to the larger sword it depends it really depends on a date ultimately in this case it's easier to just move it over um in this case if you were to move this up then what happens is now all the instances of a our cupboards into the AIDS of SF except we're seeing Dean one would not be covered by that but this could potentially cover almost all of your bin terms and we're going to be going in here a little bit about what happens you're trying to cover as many as possible and what happens if you have an instance where / region is uniquely covering one of the elements and okay so these are um there is we're going to be going into prime implicants and the essential prime implicants in a little bit here so now that will answer your detail they answer your question more detail any other questions okay so did everybody understand one point 16 and one point 17 okay so example question 1.4 given the Fallen karnaugh map find any race conditions described how they need to be eliminated by drawing the updated k-map eliminating the hazards and stating the updated function so I've given you this came out that essentially probably solved it for you by answering your questions but I've kind of taken the liberty here acad not and blue is bc do you not therefore the original equation is the combination always great a not AC not a peanut or being seen on however there exists to race condition hazard since the blue region is disjoint at adjacent to the red and go to the red agree agree right so you can extend that region to join with the rain came at you look sort of the green region now it's no longer a race conditions you could extend a red one as long as you show your work it's me scroll down I don't an exam I wouldn't expect you to go into this much detail like I'm going into this much detail specifically to show you every step I went on in my head um so here an example of where I've said in the red we're move state 1110 and move 0 1 0 1 the output is meant to remain at one okay however due to the underlying certain limitation it may be possible to is so I'm sorry I'm describing what the race condition is here it may be possible at the instant what we're switching from here to here in the previous came out if you're switching then this goes in more detail to your question if you're switching if there's a disjoint if you're trying to switch out of this joint region into another region that's where the race condition may occur so the race conditions may occur when you're switching from here to here if you're going from this region to that region or if you're going from this region to that region that makes sense so that's where your race conditions may occur and so i'm here i show the solution by doing this on it I an exam type question what I expect you to say is this and what in so you can reduce the number amount of writing on your example questions just say here's the state the two places where the where they could potentially be a race condition say to me from 11 yeah 11 10 that's how I described it to 10 10 that's a potential race condition and then from 11 10 to 1 100 is the other potential race condition and then in part three is update k-map so the question I have a phrase is find any race conditions described how they may be eliminated so that finding the race conditions means state which ones they are and then update the k-map in that matter and then the last part is state the new equation so you can update the equation in that way and I've taken the liberty of reducing it for you as long as you actually correctly say what the equations are you supply does anybody have any questions about that so they can all be connected we need to yeah if I specifically say and if I specifically say it's a sequential element or race conditions need to be avoided in the problem and I'll have two examples some questions later where I show it will show you the difference in how i phrase the question yes just part of this example you've pasted into the Red you need wrap around but you said it might be easier to extend the blue it might yeah that would be that was your correct answer on an example yeah that's like i should say this may be fixed right here so if you're looking at this on the video later everybody this may that a lot of times there's not one just one correct answer sometimes you have to there's this is a good example where there's multiple correct answer as long as they are not alone with this joint then you have a correct rate where the circuit is prevented the race condition any other questions oh you know what I have let's see so here I'm comparing I should make it a little note here this is not you don't have to put this in the answer but I've compared and contrasted the two results so here this is a reduced version of the previous game app and here is a jersey function for the updated okay now now they're logically equivalent they'll give you the exact same in terms of Mac's turn however you have to you'll have more circuitry which will be used to ensure that the update the race conditions are prevented oh I'm sorry I probably should ask to anybody have any more questions about that before I move on yes you're doing what it's not like 3 or the buyer Ziggler is there any difference in making windows no I think it's really no different sometimes you just have an instance where you have two on one column and one on the other and the you still have to use gray code sort of example if you were doing eight and four you would use the 8-bit gray code on one side and four bit gray code on the other but it would it still works it definitely works and it was especially later when in the course when we're going into how to design various flip flops and how to use those to actually design sequential state machines those are very actually very common because you're going to have the four it's a good forum for state for state machine and you have to have the edges you can only represent that with one variable because you're representing the binary value to change between the states and this would be section board so you'll learn way more than you probably want to about this later but you will need to be able to do came out to do that any other questions ok ok ok so in this course we're going to be building up towards this thing called knit which I've kind of alluded to a couple time it's called a microprocessor without interlocking pipeline stages is an example of the digital computing system architecture that restudy here and I've listed the objectives five six and seven that expect you to be able to meet in this course and a lot of this kind of goes into knowing the architecture computer system knowing how to transfer information between the arithmetic logic unit memory the registers the data hat and we're going to touch on memory hierarchy and virtual memory design the end of the course an event of seven knowing the relationship between the hardware architecture and the assembly language instruction set so you have your code let me ask you this question how many of you have not coded in c before okay so for this course um I just brought to my attention yesterday that might solve you who have it um basically we're going to be going over very very simple code snippets of cee have any kind of all your coded in java is there anybody here in case of the more people over further nadella alright so i'm going to do very simple examples of c code and the idea is you should be able to know did have you know do you not know what an array is okay so everybody knows it already so you I will say this is an array this is an integer and we're going to be relating I'm not expecting you to be masters of see in this course so just so I'm just trying to make sure that what I up the course I prepare is I'm not going waiting over your heads and if there's any instance when we go over steep code later and you don't know what the heck i'm talking about feel free to come to my office hours talk to me i'll be more than happy to walk you through it okay so here's the general idea of a computer of hardware you have data path and control so the data paths that is kind of you can think of that is your combinational element where you're going to be doing isn't all of this all of these interfaces so you have your memory here being used to be the data path and your user gets put in English and eventually the news is going to get an output right so what's going to happen is the mem you're going to get an instruction and part of that instructions going to say here's the control this is what the instruction is supposed to do so based on the particular control is going to turn on certain parts of the data capture all other parts of the data path or manipulate that data path in certain ways that are visible to the programmer in order to prove send out boy for update the memory so you can think of this as your dip from the digital system definition from last class we have their inputs I have your processing element and you have your state element or your outputs so the hardware architecture is meant to mimic the debt the digital system and specifically GGO one point 18 is what is an instruction set architecture striction set architecture is a set of assembly language instructions that provide a link between hardware and software and that's so the whole point is you want to be able to keep your peeper code monkeys your computer scientists and your computer engineers in your life chosen to give them a set of things that are supposed to meet and they can work separately they don't have to constantly check with each other so your guys you're doing hardware you guys are electrical engineer you want to be able to design the most efficient performing digital system that meets the instruction set architecture and then the computer scientists the ones who one of them with much more familiarity well designed and compiler which drives the micro engine which the buck which then arrives down to the assembly language and the Machine language and that's where you eat is right here at the instruction set architecture okay so um here's another industry expert quote lets a guy named henry Petrovski who is written a prominent book engineering is human the role of failure and successful design and he said the most amazing achievement of computer software industry is its continuing cancellation of the steady is staggering gains made by computer hardware industry so the reason why I include this quote and the reason why it's important to understand computer architecture instruction set architectures and all of this building towards from the advanced digital systems up to this is because you need to be able to in order to bring your computer that has high performance and has a speedy execution of compiled code both the computer harbors and computer software engineers need to be on the same page and this is how it's done if you're not especially if you're dealing with java virtual machine which one is supposed to compile to any instruction set architecture that's the goal of it then you start getting significantly slower performance okay so I've kind of already talked about these already so instruction set architecture is designed to extract the most performance out of available hardware technology so basically let's say I'm a I'm your customer and I want to build a certain type of computer and you're all right had we can do this because it's very fast high performance and you're going to whole thing is it Lolo let's cost way too much there's no way in heck I'm paying for that now you have to make with design requires good compromises so now you're trying to reduce the size of the chip but still be able to maintain this high performance and you have to these are the kinds of things you have to worry about and hardware we use and being able to make the most efficient use of the making the common case fastsub ski is going to be a very important point and desire and so I've listed here some examples of instruction set architectures watch I'll abbreviate is is a risk which is what we're going to be learning in this course reduced instruction set computer probably enjoy the fact that is reduced so that for the whole idea is you're trying to make take a smaller set of instructions and perform the exact same code which means you're going to do a lot of hardware use a lot of code of AI love instances of register reuse it and so forth complex instruction set computers we're going to be studying briefly here but the whole idea is that you can actually build a floating point elements floating point arithmetic logic unit regular liqui tickle a GPS and try to execute multiple instructions simultaneously within the same data path whether require significantly more hardware but it can go a lot faster so there's trade-offs in theater and very large instruction word vliw is what happens when you're trying to make instructions that can account for up to 128 bits the data path we're going to be studying with a deuce instructions that computer for mess is 32 bits and every instruction is 32 bit complex since returning very much it's very large construction work and complex instruction can have varying lengths of instructions which makes it even which makes the controller more complicated so let me get later on the course we start talking about in like pipelining and data hazards you're going to see that every time we incorporate something to try to make the computer go faster it requires a trade-off my plan you can have up to five instructions in the same next data path at once now you have to worry about data structural and control hazards requires more control beauty more control hardware so but we'll go into that more in section 5 in section 6 but for now important thing now is that assembly language is a symbolic representation of machine instructions Joanne point 19 and machine language is a binary representation of machine instructions so how many of you have actually dealt with assembly language like five or six of you okay so I'm going to screw up just a little bit you should still be able to read the definition but basically let's say I have some very basic some be 60 code let's say I say a equals sorry it's taking so B plus C and then add the semicolon so how do I get from this C code down to translating some binary so the assembly language instruction which is the symbolic representation is going to be the symbolic representation of how it's going through the hardware so this is an add instruction and then s2 these are dollars on the other side st s1 dollars on s3 and what the symbolic representation means is i'm taking the values that are stored in East to registered s1 and s3 in this case the compiler will say I want to store the values of B and C into those registers and then I'm going to add them in the arithmetic logic unit and then store them in this register which is s2 so that is the symbolic representation of the instruction now the binary value of the instruction these first six bits here this 000000 that is what sent to the controllers it says all right this is the specific instruction I want to do now go and do it the next five bits 1 0 0 1 0 I mean you'll learn this more but that's the actual register representation of s 2 1 0 1 0 1 and the next 5 represent the two values were added and storing into the registers and then shift them out which we're going to be going into much more detail later we're not shifting and also it's all zeros and then these last five bits are known as the function and we're going to be going so much more detail but the whole idea is now we've taken some code we've broken it down symbolically and now we actually have a binary about it once we get into the data path of our digital system will actually execute instruction yes I declare Bob you've got three stored information for s1 s3 yes right right when we like I've done here when I talk about nips in more detail um I'm going to be alluding to the idea that mixes a compiler driven and coding with the micro and what that means is the compiler is going to learn all of the information that's free so if you have C or say you're safe d equals 1 C equals 5 and initiate all your variables at the beginning of the program the compiler will store those values into the register I am Emily and then based on that at when this instruction occurs what happens if either has been put in there by the compiler for a previous instruction correctly calculate the values of s1 and s3 at which point we can now put those into the arithmetic logic unit get a 32 good result and then store it in this other register later in the okay so I'm going to do something that I happened in nuclear power school quite a bit when we get to a concept that people kind of like in a row as it nope we're going to go into much more detail and you understand eventually I'm just giving you a taste they draw this big circle and right I believe and is the I believe button and it's not working right but let's say I believe right now you will know about this much more detail by the end of course but right now just I believe I believe dr. Morrison I believe so I mean at this point I'm not going to be asking you this stuff on an exam but i do want to give you an example of the difference between assembly language and machine language and i do mention here that a objective seven is understanding the relationship between the hardware architecture and it can computers assembly language so whole idea is how does the design digital system the advanced digital system translate this into something that actually does what you want it to do that's the objective we're going to get from oh my gosh what are all these zeros and ones mean to you understand even being with a design a system that does that guys that's my objective anyway oh okay so what I did I actually posted this code under content and reference files so for those of you who are familiar with see or know what see compiling is you can actually play with it and see what happens so you have Matt molto scene is a matrix multiplier which can multiply matrices of 100 by 100 together and letters have done these to compile airline in the hole I'm sorry the command line instructions and the whole idea is that this actually generates the assembly language and eventually Django generates the machine language so the code here generates your assembly language so it actually takes that code that I've posted there and tells you based on if specific compiler and if I should make a note the very last line here is well say GN you three point four point six that is the actual compiler that's driving the encoding of I get to your question as i right so these are going to be different based on your different compiler but the idea and the reason I'm showing this to you is gosh y'all let's give you yuxin code and it's actually saying what's actually going on in the data path so as an example here SP means stack pointer stack pointer points to place in the stack where your code is where the instructions that you want to ask you are so first instruction says move down 112 bits and that is where your code begins you want to be executing code above that because that's that's not your code that's other things if you ever incited TV software security course that's actually what's known you can upload a virus that changes this number it will actually do something called a stack smash or stack overflow and actually screw up all the rest of your coat but in the meantime and I really hope I'm not overwhelming denim ideas here's where you start and then move 100 into this register and is putting values so 100 remember I told you what's that Oh doing stuff like this yeah that's why I did and I'm just showing you on the screen but I mean I posted all the results so you only have to worry about burning the code you can actually look in there and see you just I've done with it and put it on blackboard all ready for you um but mu 0 1 0 0 is a hundred i told you that the code does matrix multiplication of 100 by 100 right so here's to finding the length of the array so I will skip most of this because I don't want to overwhelm you guys but you can see how the relationship between the code I gave she here if we want to play with the instructions this is the exact set of instructions of the compiler that you'd want to run once you actually run the code that will produce what I posted on blackboard but then that's going to happen is it's eventually going to translate this code to locations in memory where the instructions are going to be and then if you see these values these hex values here that's just hyper text representation of the binary instructions themselves so these are use here are the actual instructions that are obvious your question I think I get to your question yet oh yeah ok so these instructions here are the binary representations but code you're about to do on the specific machine that i read it was your push your cooking how does it I'll our work is that written in assembly or machine language um no the compiler is actually a whole different program that takes your program and compiles it down maybe yours hole courses that you can take on this but the whole idea is that you have your program right and goes into a compiler which is often written in a language called yak yacc and then from there you're going to get out assembly language if I if I go a lot slower I'll probably work better yeah there we go it doesn't go fast so you get your program and you got your compiler and then a lot of magic happens which my compiler teacher in college who is also one of my dissertation committee probably wouldn't like that I said that but but then you get out the assembly language as the result that's the whole idea that's but I won't get in tokens and abstract syntax trees and all that goes on inside the compiler but what you need to know is program magic box where there is a compiler and out nose assembly language does that just don't answer your question okay yes so five letters of the colleges there you go that's exactly out now you know and still still can't run it but now you know okay so for example something that I teach towards the when I in the class about exceptions I want to see won't stand the whole basically the talking about the exception program counter and the save register but those values that come out those are the values that come out when you get the blue screen of death so oftentimes people see I go I know exactly what that means now and that should be like things like that should be the kind of things you're going to equate and that explains this on a computer thanks that means it's starting to make sense so ok computer architecture versus computer organization is one point 20 computer architecture is system attributes of a computer which contribute to its logical execution and are visible to the programmer now what I mean by visible to the programmer means that you can put something in code and get a result and you know how it's being done so if you write if you have there's a program of all people is you can actually write language or if you're sadistic enough you can actually write the actual binary representation um that means invisible that means you know by doing this set of code then you will get this set inset result so the instruction set is an example computer architecture data representation io mechanisms you can actually have the code set of things to the screen toot speaker to keyboard to the printer and memory addressing meaning that you can write code using a raise so decide things to specific places in memory whereas a computer organization its physical key details of the computer that are transparent to the program so for example hardware implementation so let's say you have an arithmetic logic unit ok arithmetic logic unit is like you can put code in to the data path and will execute something within the ALU and put out put a result does a computer architecture however the underlying hardware you can't manipulate that code right that's organization so also control signals except if you have the up code that was sent to the controller for the control signals but themselves you can't manipulate the memory technology is that here you have a memory cache you can say i want to go here but based on the technology can't control how fast it goes right you can't do that code and another one down using design all right are examples of organization now I can tell you that the third question on your exam will be defined differentiators engineer architect computerization give me three examples of each kind no problem saying that to you and here's why my advisor dr. ranganathan has the student who is before me go went to go to Intel and he asked that exact question look I had no idea and he's like all right get out of here they refused to get he that was the first question in the interview he's supposed to be there for eight hours they throw them out you don't want that happening to you right so I'm going to ask that you know the difference between computer architecture and computer organization and be able to give examples yeah people can be really really good they flew them out they paid for his life in his hotel and just threw him out after one question good apple juice note i was able to spout the definitions without looking at the screen is because i've had them drilled into my head by my visor and I the IRA I thing is I never applied to Intel so yes I did all three there oh and I have a little spiel here so notice how I visible in France and listed there I know this whole thing about don't say invisible can you see the computer it is it invisible to you so if you write if you say computer organization is the invisible part of the computer what does that make you look like okay like a crazy person right like I see unicorns and computer organizations yeah transparent is the technical term so make sure that you don't like if you say invisible you'll lose a lot of credit so what time is it and I have changed the clock so that way it is the correct time by the way ok so now I want to give you guys are kind of a brief overview of what this architecture is going to be what I yeah there you go anybody is not drivin visit ok yeah I forgot this is true all true thanks examinee have any presenter we're kind of going on to what going in more detail about nips and i know i just threw a lot at you about computer architecture so if anybody has any questions from what's that check on pointing me out for after class ok so MIPS i have also on the same place where i posted the effect code I've also actually posted the original paper that was written by Patterson Tennessee which is a much smaller version of your textbook 66 pages but when nip stands for is micro processors without interlocks ice ages to be pretty cool last class I was talking about the tyranny of numbers and they thought that every fall it had to be connect to every other part so even though Jack killed we had gotten rid of this there's still this idea that every stage within the data path had to be connected to each other through this inner lives and so what they did was said we want to get rid of these interlocks and use these pipeline stages instead so they're breaking up the data path into sections and they have this section instruction fetch I'll be discussing that those five-year a minute instructions that tell me what the instruction is and then next clock instruction decode tell me what part is the off-road tell me which part goes to the registers tell me which part does breaks it down next stage execute send things to the rhythmic logic unit stop next stage data memory do I need to upload anything to the data memory or last a damn i'm just going to send it back to the registers like that ad unit that add instruction fetch the add instruction decode it using the opcode execute give me those two variables from the registers put them in the ALU themselves give me an output stage for doesn't know this page right back puts it in the registers themselves and so the main goal of the design is the main goal of nips is the design of high performance in the execution of compiled code so the whole goal is you have some compiled code we have gone past where your question was the compiler has already done stuff and then nips is going to break that down and execute it with high performance and that's how we're able to use this instruction set architecture by having the code down to the a is a and then from the ISA to the hard work yes in our stay here today it was great but interesting two-legged feel for you northwood here like that have one data stream so I voice broke it similarly the water stream ok so wait you've touched on is a problem called multiprocessing so you have multiple paths going through multiple instructions sisk complex instruction set computer actually can do multiple instructions at the same time with the same set of ones and zeros but what happens when you have multiple instructions going through multiple data paths some of them requiring the same registers at the same time that's going to be section 7 that we're going to go in section 7 or 8 if we get if we get there well the whole idea is that you have to make you have to do a lot of hazard detection checking so that kind of becomes a bottleneck on being able to use multiprocessors to be able to improve the performance of compiled code is that one of the main differences between yes and so the basic philosophy of MIPS yeah what's that this was Jackie we should indeed they do the hazard when your hyper threading though exactly exactly right now you're learning the operating system so i take it your computer engineer yeah okay that reason I know that's because the only the computer engineers are required to take operating so I don't haven't taken a class oh right the reason I'm saying that's because I hope that the double e's you don't have to take operating systems are just reliable hyper-threading circularbuffer ones that don't we're just going to a good boo boo which since you were up watching seven Harry Potter's last night read your roommates work it might as well it might as well be the same thing is it right okay so the basic philosophy of nips what time is it ok I have to present an instruction set that is from Pilar proven voting of the micro engine thus little or no decoding is required right so the whole idea is now once you have the compiler you've actually broken it down and you can the only decoding you need to do is hate this part of the binary patience and send it to the control unit send to the registers and so forth you used when you had to deal with these interlock pipeline stages you had to decode every stage every single time and that significantly reduced the performance of the computer okay does that make sense feel like I put too much in there so doing this simple booth intrusion um I want you to cut off 124 here all of this is not supposed to be part of it I apologize um the RISC architecture is simple in both the instruction set and hardware needs to implement that instruction set although business trip oh yeah no put it off here sorry after one sentence the rest of this is supposed to be another paragraph oh it doesn't matter if the RISC architecture is simple in both the instruction set and the hardware needed to implement that instruction set so not only is it trying to reduce the so you have code it's compiled down it uses a small number of instructions and you're trying to make a small data path to implement all those you're just using a lot of instructions to get these instruction done so for example of taking multiplication using a bunch of additions to implement it and in section 3 we're going to be going over different multiplication and division algorithms that use that this is done so the tpo should only be the first sentence although mips a simple harmonic augmentation the user level instruction set is not as straightforward and simplicity of the user muggle instruction set is secondary to performance goals meaning that there are times when for example there's a instruction called add immediate and that's like what happens if you have X minus 5 the immediate instruction is the actual number now they're trying to achieve high performance so you would think puts it before this you would think deserve you subtract greedy right you want X minus 5 but by optimizing the compiler you just perform two's complement and perform subtraction of the instruction so you only have add immediate instructions in the instruction set does anybody not know what to complement is ok everybody knows that good ok this is actually the whole instruction because this is a very important concept about data may only be operated in registers and load stores so repeated use of the registers is a basic block code prevent redundance load stores and redundant addressing calculations now the whole idea is that registers are your local copy of data there fast one use those and they are volatile meaning that if you were to shut the computer off uses that information so when you the non-volatile is your data memory and the whole idea is that once you get that you're loading it up into the data memory after you perform the high-speed calculations does that make sense this this how long's have a crew put since more operations that I rightfully relate to the computation that directly into the computation can be performed so if you remember that example I put our load instructions you actually calculate the address and data memory get that information and then load it into a register and then you can use it to perform an operation but if it's already in the arab store you just put in the ALU if the operation sort of alien be dumped it so it's faster and as simplified a pipeline structure has a fixed number of fudged ages and the same time line so the whole idea is that you have look bad lets you have three types of instructions we'll go into more detail later but they take different amounts of times and you use buffers to make it so that way each clock pulse has the same I'm therefore eliminating race conditions okay so we're going to stop at 126 actually already kind of set this out loud the five stages of a Memphis data path where the instruction fetch is gifted compiled nips buyers and student go destruction from the instruction memory and loads it to the data that construction decode decodes the machine instruction and generates the control signals determines the appropriate registers of memory location and the necessary operation so it's taking that binary value getting the opcode the different registers potential media instruction and so forth and getting that value execution performs the operation dictated by the machine introduction data memory Susumu net load and store operation when you scroll this up just a little bit if the operation that keeps that some information must be sort of memory it will be done in this stage so that's storing it to the slower memory otherwise you have to worry about it and then write back if you want to write back to the registers it's done in this stage so we did a 1.3 1.4 on the example questions right okay so your homework student that's due next Tuesday is a tea think this one it's going to be one point 13 through one point 26 and then example questions three and four hey there no other questions or 1.3 1.4 yes without me early in your life good night okay they were dining with ranchers equals 3 4 pcs one has to this group plc legal assessment right into a nice death okay so we'll be going to that one more detail in section for when I gotta turn off the why are you late it was totally like just forward right so I was going to say what up stuff for saying I got I'm turning off the screen you're going to send me the mail and then what happened I was going to but extending so when Zack 
WAQ14fM8SuY,27,"ðŸ“šðŸ“šðŸ“šðŸ“šðŸ“šðŸ“šðŸ“šðŸ“š
GOOD NEWS FOR COMPUTER ENGINEERS
INTRODUCING 
5 MINUTES ENGINEERING 
ðŸŽ“ðŸŽ“ðŸŽ“ðŸŽ“ðŸŽ“ðŸŽ“ðŸŽ“ðŸŽ“
SUBJECT :-

Discrete Mathematics (DM) 

Theory Of Computation (TOC) 

Artificial Intelligence(AI) 

Database Management System(DBMS)

Software Modeling and Designing(SMD)

Software Engineering and Project Planning(SEPM) 

Data mining and Warehouse(DMW)

Data analytics(DA)

Mobile Communication(MC) 

Computer networks(CN)

High performance Computing(HPC)

Operating system
System programming (SPOS)

Web technology(WT)

Internet of things(IOT)

Design and analysis of algorithm(DAA)

ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡
EACH AND EVERY TOPIC OF EACH AND EVERY SUBJECT (MENTIONED ABOVE) IN COMPUTER ENGINEERING LIFE IS EXPLAINED IN JUST 5 MINUTES.
ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡ðŸ’¡
THE EASIEST EXPLANATION EVER ON EVERY ENGINEERING SUBJECT IN JUST 5 MINUTES.

ðŸ™ðŸ™ðŸ™ðŸ™ðŸ™ðŸ™ðŸ™ðŸ™
YOU JUST NEED TO DO 
3 MAGICAL THINGS
LIKE
SHARE 
&
SUBSCRIBE 
TO MY YOUTUBE CHANNEL 
5 MINUTES ENGINEERING 
ðŸ“šðŸ“šðŸ“šðŸ“šðŸ“šðŸ“šðŸ“šðŸ“š",2020-03-29T15:13:07Z,Direct Memory Access (DMA) And Keypad Explained in Hindi l ERTOS Course,https://i.ytimg.com/vi/WAQ14fM8SuY/hqdefault.jpg,5 Minutes Engineering,PT7M36S,false,5900,233,2,0,14,hello and welcome to super fireman engineering Oscar will you watch even a Kamala Kahn you're like if you bought the better means at though topic Malika gommage India I'mI'm seek me unless someone's never understand develop allied DML Dulce function keep out of a function keeper yep you keep our kilo you keep our ship Rena waves know why yes it is the basic building block of your embedded system hardware who say ask whose block to put up calling it who's got some jeguk a cure but hey okay basic screw joining it like a new step a little youtuber title it houses almost m DM a direct memory access it is nothing but a direct memory access nom a barcode chihuahua licking escobar mi Barnabas alleluia Jerome systems kibarim a memory k bar ma embedded system given a mission from Barker a IOT versus kibarim Baskerville 10 - hey guess who Tom Henske bury me Jana putana brothers real dope IDM Iconium looks almost there - oxy DoD a MacArthur K DK direct memory executable samejima guillotine a karateka hey a couch technically buddy but me got the Civic Judaism mother cadet I just a fireman's engineering Optimus of Karthikeya Papa engineering exams coupe ask us okay I chop winter laughs okay was it very clear to DMA have you to tea mother turn the table each CPU key up cuz a core part of key system kept process FJ gustatory what mother cut the top is a acid acceptable ticket though up janaballah interesting her key artery mother kaltaka say hey hey Jana was a really nice Eppley I would like to get your attention to this respective point ki jo aapke some early upper memory a ticket or a i/o device and correct a beer memory or i/o device made Joby send and receive of the data honorable ionic Joe being don't Amishman a transfer una Bala yeah if the case a coordinate karma is commander Colonel yay - sorry cheese ahem if data transfer could they can now see the bulk of na oh sorry you cheese and hey Terry case a Java process our average CPU have a carry of Curtin general sense me normally yes ah Roger data transfer Cup coordination hair vodka processor big time ok leaking kawaki authen a data transfer 'no paper Barra non-efficient without them ok or a be tomorrow - CPU a llamar a processor a joke or parameter system escape especially tamam Hazara Camus Tanzania karni mata whose core resource qualified Colonel Tigh events for sequence Mulligan out that Joe processes ammunition Colonel there's a ball solid tamam both important other than this data transferred boss sorry comma they use a carnival tent though Volta remain on efficient computer room to his Keeley who say PC keys are worth parity hey or was a Roth hair would your mother take mother kaha jogger Bharata hey whoa ha d m.a.c he army simply up he says as a chief de Klerk device they close he by here DMA controller head this is direct memory access controller jockey mother's garage burrata processor Katara for who Quetta by to conquer Abner attention Muttley Yakima so maluma Yaka Meza maluma Juba data transfer aura is a magic town otaku tension lira okay to open it up to come courage SC data transfer yatta matter quick interrupted a stock TK whose could agree so much magic did you come you just saw path Abajo choke I guess out and see how that whole process is basically getting carried out with the support of controller see llamando Mary Hampton busting he can or you have a opcode you DMA controller a look at her pad in busca control temporary basis pay flail attack it controls these respective it takes the control of these bus on for a temporary period of time ticket yeah Happy Hanukkah memory or you hop 1/2 K IO devices ticket he's go through some a 3d member on it now which is very guessing he get like that - yeah memory IO devices obviously micarta heheheh pay aapke beta transpose oh hey history case a input devices a memory Kenda every memory say output device send and receive Jeff and I have no data transfer hey woof will carry out who the head without any active interruption of the processor mclubbe yaja pay akka edge of controller head yes our each is to control garage Obi data transfer her without any active interruption from the processor processor dr. cockney Bach Akita mom comes over gonna tear out the window perform Garuda orthotic a data transfer complete oh here whose kappa d notify Carter CPU GI Joe comic Oh Zapata omanakuttan Garcia interrupts GroupMe or fill up a servo control a little cpu akka is busca okay you see Lebanon gunky temporary basically a for a specific period of time temporary basically a control of the bus is gained by the controller ik by the DMA controller or a bar in by Safa cambogia pedo control opposite busca hi Amari process of Coachella without easter egg I said you have ready imagine a mother kartha has CPU key which is a very important thing I be at their ugly point either what function keypad is up simply keypad because a ticket --kavitha is me EMI basic when I explain girls at home what is actually happening in a keypad ex simple thought beggar makes a bullock representation a causa is representation miguel turn of course I put a button type structures your head go technical male Jiang as a simple simple structure of a shingle mill Jiang okay I'm Jessica for example like this okay like this whose coupon code symbols on it okay I see Co symbolism yeah like this a major representation here - keypad hey this is for simply a tape recorder yes sir if a tape recorder application Gil a keypad - Arg ahead okay obviously Park a main function counter what is the main purpose of this keypad providing the keypad to the human to the user tacky you jus be user head um II say opti system case I've in practice by giving input to by EA Jo representation here Jo structures had in K through keypad true drop a user here Vau apna input this attack command a sub tied to the system okay and who seek a basis posada processing ogre or apt output nick lega - yaja p jb egg as a input key telecom kuraki is kid threw up up in his system core input provided kar sakte ho jessica be many more iced tea functionality care for a prick order yet hummm a tee pad of an eye - Jessica girl musiah throw so fast-forward kannada bhavana Mirko okay - Maya de bomba muchacabra man a reverse may Tamiya the bound Husqvarna Jessica Carmack Oscar night Mooji say Laker night yeah happy Mooji first entry first fight of joy who say they can accept later my last title Josip Lakers night - yes no sorry chief a hair yeah okay according to the application with the head as per the a plea the things have been getting designed on your keypad okay other esse who notch occupy it is an application-specific yeah both important shove the application specific Geneva keypad Kavery Moscow Tusker a courtship dusk Assad to enunciate more application-specific SME wenatchee key basically the representation have to keep our couch or her or application couch re-perform garam means these two are two things should be actually compatible with each other s anyone watching a different area altogether different application could allow each other out keep a representation human group which already again please worsen human ko Jenga confuses I became a button the bow and he the bow to kisko the one has to do input Java system that vote which Allah Hazara okay - hey confusion Joe Hagen he you not yet and that is what the keypad designing should be kept into the mind that it is application-specific TK - those two are you video of course Amish MacGyver don't know topic Apache legume gay so by a video go like cut the your scissors aureus will do Shelby Jeff Cassar those things are thanks a lot those two for watching this video 
uKchSidT_Uo,27,"Digital Design and Computer Architecture, ETH ZÃ¼rich, Spring 2020 (https://safari.ethz.ch/digitaltechnik/spring2020/doku.php?id=start)

Lecture 11: Microarchitecture I
Lecturer: Professor Onur Mutlu (https://people.inf.ethz.ch/omutlu/)
Date: March 26, 2020

Slides (pptx): https://safari.ethz.ch/digitaltechnik/spring2020/lib/exe/fetch.php?media=onur-digitaldesign-2020-lecture12-microarchitecture-ii-beforelecture.pptx
Slides (pdf): https://safari.ethz.ch/digitaltechnik/spring2020/lib/exe/fetch.php?media=onur-digitaldesign-2020-lecture11-microarchitecture-i-beforelecture.pdf",2020-03-25T00:25:22Z,"Digital Design & Computer Architecture - Lecture 11: Microarchitecture I (ETH ZÃ¼rich, Spring 2020)",https://i.ytimg.com/vi/uKchSidT_Uo/hqdefault.jpg,Onur Mutlu Lectures,PT1H29M8S,false,6415,98,0,0,3,today we will start microarchitecture which is a fascinating topic that has fascinated many many people in the world and it keeps on fascinating many many more people especially these days it's designing all these processors are all about microarchitecture is a is a little portion of it because I say don't change as much as we will see today okay these are the readings basically for microarchitecture and next week we're going to go deeper into microarchitecture and talk about improving the performance by pipelining and we're going to cover a lot of issues on pipelining okay so this is our agenda for today and the next few lectures and then after next week we're going to go into out of order execution basically we're going to violate a lot of things that we set which is the one moment model instructions are executed one-by-one in sequential order we're going to violate that without order execution bargaining I think execute the instructions out of their original order okay that's not jump out of ourselves this is the one normal model that we discussed extensively last week so you know that well I'm going to go through again today this is the one line with machine one of the board on machines we discussed lc3 we may discuss more of it and this is the instruction cycle that you should know we're all off but we're going to talk about how to implement this instruction cycle in more detail today so it consists of fetch decode evaluate address fetch our hands execute and store result right and not all instructions go through all these stages and we talked about the instruction set architecture you know this instruction set architectures the interface between the software and the hardware what the software should expect and what the hardware should provide we're going to go more into this so what is mic architecture that clearly we see microarchitecture in this picture over here it's right here it's underneath the ISA and as you can see it's not exposed to the program there's a layer between the program and the microarchitecture that's the instruction set architecture essentially microarchitecture is an implementation of the instruction set architecture it's very simple how do we implement the is a basically we'll discuss this for many lectures and that's called microarchitecture and there can be many implementation of an ISA ISA is really an abstract specification right you get this instruction what should it do but they're at the lower level you have a lot of freedom to decide how to implement each instruction and each functionality so clearly there have been a lot of implementation of the mid-size say that you've been examining so far MIPS r2000 is what was one implementation it was a very simple processor a single issue in order execution is very similar it's similar to the one moment model execution but we're going to talk about today but are ten thousand was a much more higher performance original bit that was essentially implementing the same is a MIPS is a but it was doing things in a different way executing instruction lot of orders I said it was providing much higher performance for example Intel and AMD have implementations of the x86 I say which is very popular and there have been many many implementations of it I don't even list 8286 over here or the first Intel x86 processor over here but even even the younger ones are not the youngest ones today right and AMD has many other implementations also so these are essentially implementing the same ISA now that's not absolutely correct because I say also changed over time but the core of the ISA is the same they're they're extensions that are added to the ISA as you go from nineteen 1980s to night 2020 is for example ok so let's talk a little bit more on sa design and trade-offs before I go into mic architecture I want to basically make you think of it out of the box because it's important for you to think out of the box and we're going to talk about a different kind of thinking so you should know this very well they're tricky properties out the von Neumann model what are those anybody can tell me and no hands they're tricky property what is actually written over there it's a stored-program computer what's the other one I know somebody knows but maybe I cannot see the heads it's the sequential instruction processing right since every instruction is processed after the previous one so I mean stork program means that instructions are stored in a linear memory array and memory is unified between the instruction and data we discussed this and the interpretation of a stored value depends on the control signals so basically there is no distinction between instructions and data in the memory memory it looks like bits but how you interpret those bits depends on when you actually access those bits if you're accessing the bits in the instruction fetch stage you interpret them as instruction bits right if you're accessing the bits in the data fetch stage then you interpret them as data so basically at what point in the processing of the instruction you access memory depends on how you interpret the bits that are in the memory so hopefully this is clear we discussed this before but sequential instruction processing which you should know next time I asked it's critical to the one moment model basically one instructions process at a time and program counter the instruction pointer identifies the current instruction right so that tells you which what is the current instruction that you're processing right now and it's advanced sequentially you go to the next instruction after you finish the current instruction you increment PC by four in MIPS by one in lc3 right except for control transfer instructions which basically take you to some other place so that you don't have to go sequentially so that you can implement various forms of control flow like while loops do-while loops for loops if-else and if statements straight or K States okay so should definitely remember these properties they may come later on and this is essentially if you look at the one normal model of a computer you have a control unit that dictates everything you have this instruction pointer you go through this instruction pointer sequentially so that you can execute the program okay so I'll ask you the question is this the only way the computer can operate this is the only model that we can have anybody yes No so by now yes that's true yes but if you look at parallel programming yeah you have multiple of these things right but if you have if you look at multi processors they're essentially each of them is a one moment machine they are communicating with each other yes well we'll cover GPUs later on yeah so basically I think you're getting at the right answer now that you're warmed up you can you can give answers yes you have that's right exactly yeah but I want a much much more completely different model but I think yeah you're all right basically this is not the only way but I'm gonna propose a radically different way which is also important so basically the answer is no and we're gonna see many examples of how to look at different models but there's a qualified answer nobody has been the dominant way basically if you look at even if you look at multi processors today each thread each thread is obeying the one normal model please be when you look at the execution sequence of a single thread it's the same as one nine one model as exposed to the program underneath we can do many things that we will discuss but let's say let's take a look at a completely different way that's X that's potentially exposed to the program well I'm going to skip that so in one normal model the instructions fetched and executed in control flow order it's a specified by the instruction pointer so everything is sequential except for explicit control flow instructions so in a completely different way I'll call it as the data flow model there is no program calendar we get rid of it an instruction is fetched and execute it in what's called the data floor now what does that mean that means when it's all pans are ready you somehow figure out when the instructions or parents become ready and at that point you fetch the instruction and then you execute it sounds cool right there's no need for the program counter and this is a potentially much more parallel model because many instructions can become ready at the same time there is only available to up data and as I said there is no instruction pointer in this case instruction ordering happens purely based on how instructions depend on each other based on the data flow right so each instruction specifies which other instructions should receive the results you're doing an ad may be the result of that ad is needed by a future multiply and this add instruction specifies that the result should go to this multiply and that multiply instruction specifies that the result should be stored in memory so basically that is the input to another instruction that's a memory operation okay and an instruction can fire whenever all of its operands are receipt it says so that's how we execute there is no program counter that dictates okay you should execute this instruction now once you're done go to the next one once you're done go to the next one so there is no sequential element over here except for data flow so data flow is the only thing that did take the stage ordering there is no sequential control flow based ordering so as a result of this potentially many instructions can execute at the same time right and people have actually examined this model since 1970s and we're going to talk about that later on also so but let's take a look at the different elements so the step one Lyman program written in some sort of language you don't need to know exactly what it looks like in fact it's hard to decipher but you can easily figure out that you're adding an ester this instruction is adding two values storing the results in temporary place this instruction is doing this multiplication storing the result in the temporary place and you get E minus W and me plus W and then multiply them and store the result in Z it has some it does something so what is the significance of the program order I'll ask you this question you may think about that for a while what is the significance of the storage locations it's good to think about that also some of these are actually really temporary storage locations right you basically store the results in V and W and then you use them and that maybe you don't need them again right so in a sense those storage locations are there just because you need to specify them but maybe they're really used for communicating the results of this instruction to this instruction that needs that that results basically vnw are communicated to this instruction so data flow means that you communicate the results between the instructions so an example data flow view of this program would look like this so in this case we don't have an implicit sequential ordering right I actually put things in parallel with each other as you can see you have inputs a and B coming into this data flow graph and graph is very intuitive you can see that they're being added B is being multiplied by 2 the results goes to this - node and plus node and this result goes to this - northern plus node and the results of these nodes go into this test this final multiplication node and the result comes out as Z so let's call the data flow graph each node in the graph is an instruction each arc in the graph is a value it could be an input value or it could be the result produced by an instruction and whenever a value gets generated you have a token on that arc and an instruction fires or fetch gets fetched and executed whenever all of its tokens are ready for example this instruction fires execute gets executed whenever a and B are ready in this case that's the beginning of the program you supply the values and this instruction gets executed this instruction gets executed when the result of this is ready and when the result of this is ready at the same time because both of its up inputs are ready so it can fire ok so multiple instructions can fire at the same time concurrently of course we need to have the hardware to be able to do this right ok so which model is more natural T as a program would you program in the one moment sequential manner or the data format who says one moment sequential ok who says dataflow ok that's usually the case maybe this is a matter of nature versus nurture because people usually we learn programming this way right not necessarily this way yes yeah you can certainly use the FPGA to implement something like this it's very it's a very natural fit but you can also build up on Roman architecture on top of the FPGA it's a very calm it's a it's a flexible substrate yes very quickly so okay anyway let's not get ahead of ourselves but yes you need not to play with absolutely so if you would like to be able to do this calculation and parallel each multiplayer use we're gonna get to that later on okay so more on it but this is a very different model right there's a programmer you can certainly program this way also and now that's it that's a different way of designing an ISA it has implications on the mic architecture also as your colleague pointed out over here maybe you need to actually have multiple Al use maybe you don't if you don't maybe you don't exploit the benefits of the ISA okay so I already said this basically the dataflow machine a program consists of dataflow nodes and node fires it's fetched in executed when all its inputs are ready when all its inputs are tokens so this is one example this is the dataflow node and it's a potential representation and the instruction set architecture very similar to the instruction formats that we've seen you could have an instruction format that is an opcode that basically has argument 1 and argument to where it's coming from and then whether the arguments is ready or not and the destination of the result where the destination should go to write which instruction should receive this ok so you could actually build many different nodes so there's a conditional node for example let's take a look at this this basically what this does is it has two inputs data inputs X control inputs over here and it behaves it basically fires when both its inputs are ready of course you each have the data in the control and what it does is if the control input is false it takes the data input and sends it out of this output if the control input is true it takes the data input and send it out of this true output over here basically is where you can actually decide where throughout the data do you send it to this instruction or this instruction basically if if the input is true execute this instruction if the input is false execute the sense charge that's what happens over here basically that's how you can implement an if/else construct with this graphical language right and in this case because the input control input is false the data input goes out of the false output over here relational is another example how do you generate the trend false basically this is a comparison greater than comparison note it basically checks whether the left input is greater than the right input and it fires when both of them are ready and the result is essentially true or false depending on whether or not this equation evaluates to true or false in this case is true 10 is greater than 7 so you get a true token out I'm not going to cover the barrier synchronization you can think about that we're not going to use that but that's another example basically what this does is it it's it delays sending all of the tokens until all tokens come to this node okay so that's it let's have some fun with this program now that you've seen some examples can you guess what this program does assuming so let's take a look at this program all of it so N is a non-negative integer this is 1 so initially the value of the token over here on this arc is 1 the token on this arc is N and what we have is essentially a very simple dataflow program what this programmer decided to do is copy n just a copy node basically get n over here and then n over here and then greater than 0 note if n is greater than 0 you get true otherwise you get false and then you copy that boolean value to here and here now if this is true 1 gets passed over here otherwise you get the output over here so there's an interesting thing that's happening over here at the end of the loop let's say over here the value of 1 gets replaced so once the token is consumed you can actually put another token onto this arc over here that's a similar thing happens over here as you can see so basically this is the output of this function let's call function call the function and you get the output at some point internally there's something else happening over here as you can see we've copied in and over here this is true or false right and now if this is true then gets copied over here and then n gets copied over here so basically what's happening over here is you get this inputs coming down and then n coming down you multiply 1 times n and then you decrement n by 1 so okay did I give you enough hints can anybody guess what this program does yes absolutely yes it does calculate n factorial and if you guys think about it you'll also find it basically that's what it does you have 1 times n over here in the first iteration and then n minus 1 goes here right in the second iteration and then maybe you check if n minus 1 is greater than 0 if that's true you would multiply whatever it came out over here with n minus 1 and remember what came out of were here was n so you multiply n with n minus 1 coming here so now we have n times n minus 1 and then you circulate it back over here now you calculate n minus 2 over here and then M minus 2 you check whether that's greater than 0 if that's true you multiply n minus 2 with whatever came over here which was n times n minus 1 so basic and time n minus 1 n minus 2 gets over here so we keep doing that until n is not greater than 0 so what happens if n is not greater than 0 this this eval is to false if this is false when this node when this when both of the tokens are ready in this patch over here the input the data input is sent to the output and remember if we actually go through this we would get n times n minus 1 times n minus 2 dot dot dot one all right that's the multiplication so let's are out to them factorial and if this is false this copy note else to false the token that's coming out over here base please sent to ground so of course I didn't explain you the ground over here but basically it it's gets rid of the token over here and there's no token that's circulating in the program and the output is sent over here and there could be some other data flow graph that will consume this output right so your entire program much may be much bigger than this it can take this n factorial and input to some other function over here okay so this may be a batard to see from the back especially you can study it on your own but it's actually there's a dataflow program you didn't really do any sequential programming over here right it's completely well this may not be the most completely parallel way of implementing a factorial but it's one way of implementing factorial in a dataflow is a and again the principles are very simple in node fires when all of its inputs are ready and there's a specification of what the node does and basically eventually you get tokens out of this program and that's the result of that you get okay any questions quickly no okay have you seen the data flow model before anywhere probably not okay so hopefully this is some aisle by opening I'll get back to this later on because this is not the way not the common way we program today the common way program today is one normal model because we can reason about it more easily there are some languages that are that operates based on dataflow principle but no instructions that architecture is able to do this today this doesn't mean that it's not possible it's possible people are tried and they failed but what didn't they fail because it was very difficult to build a software stack on top of it and maybe that will change in the future but what didn't fail is underneath the principle that I showed you is actually employed in the process today so out of order execution is all about doing this without exposing it to the program so basically if you think about an out of order execution engine it's actually operating in a very similar way without actually you knowing it as a programmer the programmer programs like this underneath the out of order execution engine converts that program to this and then executes says in parallel and we will see that in like four or five before lectures okay so let's let's pop the stack element so I say level trade-off do we need an instruction pointer in the ISA if you're a designer architect designing and instruction set architecture you could say yes I want to control they're in sequential execution and if I make that choice an instruction is executed when the instruction pointer points to it an instruction point to automatically change the sequentially as you know if you say no one way of doing saying no is you do data-driven more parallel execution and instructions execute when all it's all parent values are available and there are many trade-offs associated with which we will not go into it right now but ease of programming which one's easier to program which one's easier to write compilers for sequential might be a bit easier performance which one's better at extracting parallelism out of the same piece of code Hardware complexity which one's more complex there are actually a lot of trade-offs as I said the similar trade-off can be made at the microarchitecture law so I say again it specifies how the programmer sees the instructions to be executed but mic architecture on the other hand is how the underlying implementation actually execute the instructions it may have nothing to do with how the program is seizing instructions to be executed as long as mic architecture reports to the programmer the order that the programmer expects so there are two or choices programmers is a sequential control flow execution order versus the data flow word but mic architecture underneath can execute instructions in any order as long as it obeys the semantics specified by the ISA when making the instruction results visible to the software and we will see this many times so programmer I should always see what what he or she expects right this the order specified by this book the contract the hardware software interface the ISA ok so we will get back to the ball nomina model and we'll assume one moment model for the rest of this class if you want to learn more about dataflow you can read some of these seminal papers this is where it was first introduced in 1974 or we will see that again in a later lecture actually when we talk about out of order execution but we will see it that the mic architecture level and if you're really impatient you can actually watch some of more advanced lectures so let's get back to one normal that's a sequential execution and all major instructors that architectures today use this model actually even GPUs somebody mentioned GPUs if you look at a single thread it's sequential execution but there are many threads that are executing in parallel as we will see in some number of lectures so you can see the names of the I say some of you some of them you may be familiar with risk 5 is that one of the popular ones today it's also online this 5 is actually very similar to mix so underneath at the microarchitecture level the execution model of almost all implementations or microarchitectures is very different so for example Intel 84 86 was one of the first to do pipeline instruction execution which you will see next week pipeline means you don't wait to finish this instruction before you start executing the next in charge you basically pipeline it's like an assembly line right when you're producing a car you don't finish one car and then you start the next car right in a factory you pipeline the production of the car parts of a car it gets complete and then you start the next car parts of that car gets complete and you start the next car it's a pipeline what that's actually against the one lineman model but the programmer doesn't see this you can do multiple instructions at a time you can do out of order execution and we will see these and you actually separate instruction and data caches as one of you mentioned breaks the one-line model also because you have separate pieces in dedicate the instruction and data so basically what happens underneath that is not consistent with the one moment model what is exposed to the programmer is not exposed to the software that's the key so that's the difference between is a and microarchitecture microarchitecture is what happens on the underneath so you can read these definitions I think yourself but basically this is a compete detection when you think about computer architecture are two kinds of definitions the traditional definition is this one which was actually provided by Jean M doll over here basically the attributes of a system as seen by the programmer as distinct from the organization of the data flow and control the logic design basically there is a distinction the implementation is not seen by the programmer but I the attributes that are seen by the program's the architecture sometimes we talk about computer architecture that encompasses all of it okay you can read this it's I don't want to go through readings so basically I say is the agreed upon interface between software and hardware Mike architecture is a specific implementation of an I say not visible to the software microprocessor is usually when we talk about a microprocessor contains the is a mic architecture and the circuits and when you hear architecture it's usually instruction set architecture plus mic architecture so let's take a look at some examples of this this is actually your homework contains some of this also but as part of the ISA versus microarchitecture if you let's let's go through the analogy of cars again right probably everybody drives cars here's you've used the gas pedal I assume if you if you draw draw a car and get that gas pedal remains very similar across many cars right that's that's an interface underneath how you implement the acceleration what happens when you press the gas back gas but or maybe different completely you may have an electric engine underneath you may have a mixed electric combustion engine underneath or whatever so basically the internals of the engine is what implements acceleration that's really the microarchitecture of the car whereas all of the interfaces that the user gets exposed to is the architecture of the car and Mike architecture may be completely different from architecture okay and I've said this before so in example you have the add instruction pretty much all is ace have similar add instructions and your add or implementation may be completely different underneath right you could have a bit serial adder we discussed you basically add one bit at a time every clock cycle if you want to do a 32-bit edition it takes 32 clock cycles or you could have every Oh Kerry Kerry otter that basically adds all of the bits at the same time but the Kerry needs to propagate ripple through the full adders or you could have a much more sophisticated carry look ahead adder which you can get to know if you're if you're really interested by reading this chapter over here that's faster so basically there's a trade-off that you have at the microarchitecture level how do you implement the add as long as it specifies what the add instruction is supposed to do you're correct architecture microarchitecture you can implement things to optimize speed energy performance power whatever you would like to as a result the is a so many implementations and microarchitecture usually changes faster than the ISA there are few ISAs but many microarchitectures over the years and this is this is because the interface is exposed to the software whenever you change the interface for the software to take advantage of it everything about needs to change the compiler needs to change the programmer needs to change they need to become aware of the change first of all and that takes a lot of time and if you actually make not a good change at the at the is a level people may not use it that's why I think that's why I say designs are very careful in terms of changing the is a significant okay so what does I say basic what is I say specify it's essentially what's in the manual this is an Intel manual example and you've already covered a lot of this but certainly we talked about instructions and memory but there are other things that actually are specified by the ISA like how interrupts and exceptions are handled how do you do access control how do you do i/o how you do tasks or thread management power management thermal management multi-threading multiprocessor support we're not going to cover these but be aware that these things need to be exposed to the programmer or the system software design of some sort of programmer right if you're designing operating systems for an architecture you need to know how how does the architecture treats the multiple threads or multiple processors okay so micro architecture is the implementation of the IC under specific design constraints and bolts anything done hardware with Alex for your software again this is a laundry list over here we're going to see some of these but a lot of a lot of systems today for example the voltage frequency scaling they basically change the voltage and the frequency of the processor depending on how the system is used to save power for example or to accelerate an application so if you may know of the turbo boost future feature and Intel systems they basically increase the voltage and frequency of a single processor if you're running a single thread so that you can execute that thread much faster now the question is is this a part of the microarchitecture is this a part of the ISA if you want to answer that question then the question you should ask is is this exposed to the software or is it's not exposed to the software that's the key question you should ask if it's not exposed to the software what does that mean that means that is this basically written in the book is there a way for the programmer to change this basically if the program is able to scale the walls to jump frequency using some instructions and that is part of the ISA if not if the microarchitecture automatically does this you may know that it's doing it but you cannot do anything about it at the programmer level program level then that's part of the mic architecture right so a lot of things these things actually can be part of the ISA or part of the mic architecture depending on whether or not they're exposed to the program usually pipelining is not exposed to the programmer but we will see some models later on where pipelining even pipelining is exposed to the program okay so let's take a look at a couple of examples add instructions opcode is that a part of the ISA or mic architecture many guesses yes that's part of the I say yes because it's basically shown to the programmer the program it needs to know use the opcode to do a write an assembly program what about the adder implementation bit serial vs. ripple-carry adder is that part of the microarchitecture or I say mic architecture yes this programmer doesn't really care whether it's a bit serial adder or ripple-carry adder unless in the ISA you have a you have different instructions for them right which is possible but today's Isis don't do that number of general-purpose registers I'll give these to you basically this is part of the ISA the assembly programmer needs to know the number of general-purpose registers to write programs number of cycles to execute their multiply instruction may be a bit tricky but this is really part of the microarchitecture again in the MIPS is a there is nothing that you should know about - how many cycles it takes to execute an instruction to write a program correct program number of Courts of the register file we will see these later on but this is part of the microarchitecture whether or not the machine employs pipeline instruction execution that's also part of the microarchitecture okay so remember microarchitecture the implementation of the ISA under specific design constraints and goals and you covered design point briefly in the past but design points is a set of consider design constraints and their importance that leads to trade-offs in both I I say in my practice so why do we have these different types of mic architectures because we want to satisfy some sort of design point right so costs performance maximum power consumption energy consumption available to you liable to time to market security safe to predict all these form the design point of your system and you need to choose the right components my crack assuming your fixture is a you choose the right mic architectural components to satisfy these bolts and clearly it's it's it's effort and design point in turn is determined by the problem you're solving the application space right the intended users and the market and that's actually very various also for example you may be designing the processors for scientific applications transaction based applications databases data processing networks basically very very different types of applications and they may dictate your design point and these don't end as you can see right genome analysis or you may be designing a processor that can execute all of them right which is essentially what these are today right these are very general purpose processors they can execute all these applications that's why you have these different choices in my parka texture okay so you know this is already so I'm going to skip this so really the soul of computer architecture is that trade offs I say level trade offs I giving you a glimpse of it there's a lot more over there in the is a level which we're not going to cover we're going to go a little bit more into mic architecture level trade offs but first we need to learn what micro architecture is and they're also system and testable trade offs how do we divide the labor between hardware and the software so my car takes your level trade offs what kind of an adder to use is there mic architecture level traders and computer architecture is really the science and art of making the appropriate trade offs to meet a design point I'm going to be Labour one point why art so if you I keep saying arts because it's really part the arts because you really don't know and cannot model the future really one basically you're designing systems for even three months later if you want to really truly make it a science it's very difficult because you need to really model what happens within those three months but we're designing systems actually that are going to be used like twenty years into the future right how do you actually make it a real science it's very difficult so basically you have new demands from the top new problems being executed on this machine that was designed five years ago literally and I need I have new demands there are new applications that are executing on this and this or do you wait too slow for the purpose of those applications and there are new issues and capabilities at the bottom over here that you may be missing and they're new demands and personalities all the users also use it also changing right but basically the future is not constant also so you have changing demands at the top changing issues and capabilities at the bottom and changing demands and personalities of the users so there's an analog form iconic architecture I like macro architecture as you know give you this analogue very quickly if the future is not constant and real architecture either right and there are many examples of this everywhere in the world this is one example over here you have a mill that's later uses a theatre restaurant and conference room right how many people have been there we was led to chief M Burnett right basically it was originally built as a brewery and then converted into a mill and then a cold store and then nowadays it's completely different right the architecture is the same the ISA is the same somewhat even that's changed slightly but they what it's used for the microarchitecture is very different and in fact internals have change also okay so another example this is Kraftwerk and you can see that the stuff that's internal is different but outside it looks like the same okay so with that let's go into mic architecture basic celibates now that we have an ISA we've established what an ISA is how do we implement it essentially how do we design a system that obeys the hardware software interface I'll also give another aside over here because there are many many creative solutions that you can come up with system can be solely hardware or a combination of hardware and software now what does this mean you may actually have an ISA that we exposed to the programmer programmer writes code in that internally you convert that iousy into some other is a this is called binary translation for example or Java virtual machines do that they actually take Java code translated into some internal is a and then you build a processor that executes that internal thing so there's a layer this is the ISA that's exposed to the programmer that's the hardware software interface well nuts hardware software interface actually programmer system interface let's say and then underneath you have this translation layer which may be software or hardware and you translate it into another is a that gets implemented in the microarchitecture so keep this in mind you can actually play a lot of tricks actually exceeded six I say is a very complicated I say so all x86 processors are doing exactly what I described programmer I takes its code Hardware underneath translate that execute code to something like MIPS something actually very simple not exact like MIPS but conceptually like MIPS and then the underlying Marc architecture is essentially like a MIPS like a micro architecture that we will discuss so we will assume Hardware implementation for most of the lectures but you can play tricks you can start with some is a that you expose and convert it to something else so basically how does the machine process instructions that's what we're going to cover and you'll assume one Lyman model forget about dataflow for now so what is this mean basically you have you're executing an instruction before the instruction you have some architectural or program or visible state you need to transform it or process it and it's transform that processing the instruction base could transform that state to architectural State Prime that's essentially what executing an instruction means according basical of course this needs to happen according to the is a specification of the instruction and I say specification of one normal model is sequential instruction processing specified by each instruction ok so this process instruction step we're going to go into detail and we're going to implement process instruction step these gaya C specify the abstract you what the architectural speed state prime should be given an instruction in the architectural State essentially the synaptic track finite state machine right you have state at the beginning you have instruction that needs to be executed and you have stayed at the end we can think of this as a finite state machine where the instructions specify something abstractly combinational so state is programmer visible state next state logic is the instruction execution specification from the is a point of view from the programmer point of view there are no intermediate States between the architectural state and architectural state prime microarchitecture on the other hand implements how the AAS is transformed to a s prime well yeah so there are many choices and implementation over here so we can have programmer invisible States to optimize the speed of execution you can have multiple state transitions per instruction if you will see both choices choice one this is called a single cycle microarchitecture an instruction transform architectural state to architectural state prime in a single clock cycle okay we're gonna see this it's not going to be very effective for various reasons we will see so choice to does this basically you you take multiple clock cycles to transform architectural state to architectural state prime what does this mean in the first clock cycle you do some stuff you update some internal state that's not visible to the programmer in the next clock cycle you do some other stuff to execute the instruction you update some Mike architectural state again that's not visible to the program in the next clock cycle you do some more stuff and finally when the instruction is done you update the architectural state so all of these intermediate states are not visible to the programmer and you take multiple clock cycles to be able to do that but the results are essentially the same architectural state is transformed to architectural state prime this is called a multi cycle microarchitecture it takes multiple cycles to transfer to execute an discharge and we will see an example of this also so the most basic instruction processing engine essentially takes a single clock cycle takes acute you use only combinational logic to implement instruction execution there are no intermediate program of invisible state updates so you take the architectural state a yes press instruction one clock cycle produce a s prime so you can think of this as what we've seen in the past right this is essentially sequential logic you have stage over here which has architectural States you execute the instruction with some combinational logic and then the instruction produced architectural state prime and a processor is essentially essentially consists of combinational logic that can implement all possible instructions that are specified in the ISA so we're gonna design a processor that looks like this okay so you should start thinking about what is the clock cycle time determined by we've seen this in lectures before timing and verification for example how long does it take to the maximum a combinational logic delay plus the set-up time plus the whole time of the sequential logic that's your clock cycle time and actually determined by what is the longest instruction to execute right what is the law longest delay that you have over here and the critical path essentially this is determined by the critical path of the combinational logic plus the set up and hold times and what is the critical path of the combination logic determined by that basically depends on what you do in that cycle and that is depend on what you should do to specify the operation of an instruction right so the longest instruction that you need to execute determines how fast that combinational logic will be of course what you do in that combination logic mattered if you use a ripple-carry adder let me finish this and then we'll take a break there so depending on depending on how you how you design the adder makes a big difference so what is this architectural state over here so just so that you know this is the programmer visible state right you have memory registers and program counter and instructions and programs specify how to transform the values of this program but visible state this is a s all of it and we transform it to a s Prime okay so this is probably a good place to take a break and now we're going to delve into the design of a single cycle machine okay so let's continue where we left off if you remember we were talking about a basic instruction processing engine which looks like this it's a single cycle machine now we're going to build this single cycle machine and we can also see the disadvantage of it so that we can motivate multi cycle machines and then later we're gonna look at the disadvantage of multi cycle machines so that will motivate pipelining and then we're going to look at this advantage of just pipelining and then will motivate out of order execution so these lectures will build upon each other ok this is a program invisible state and we've already set this basically actually I'm going to give you the high-level disadvantage before you will build the machines so clearly each instruction takes a single clock cycle in a single cycle machine although state updates are made at the end of the instructions execution the big disadvantage is that the slowest instruction determines the cycle time right because you need to implement every instruction a single cycle if one instruction let's say you're doing a floating-point device a very complicated division operation that takes a lot of time for example and that determines your clock cycle time now your maximum combinational logic delay is dictated by that and that may take may be very quick but you have only a single clock cycle to be able to execute every single instruction so that's not good so your clock frequency actually is lower in single cycle machines that's why multi cycle machines are nicer you break instruction processing into multiple cycles on stages and you can do state updates during an instruction execution as long as you don't make them visible to the architectural state architectural state updates happen at the end of the instructions execution the advantage is now you can break things into smaller stages maybe balance stages and the slowest stage determines the cycle path there's a difference between stage and so a stage and slowest instruction you have control over the stages of course you have control over the how fast you can implement instruction but you are limited because the division still is a division right it may take a long time where here you can chop things into smaller stages and each stage may be really fast little combinational logic you may have a lot of stages and both single cycle and multi cycle machines literally follow the one moment model at the micro architecture level basically it's the sequential they don't move to the next instruction before they finish the previous instruction you have to finish the current instruction so that you can move to the next so now let's take a look at how we build these machines basically we have a control unit as you know that processes the instruction step by step and we know of this instruction cycle there are six steps we've already covered this as you know and you already know that not all instructors require all six steps and this is the example that you've seen many times before so if you look at a single cycle machine all six phases of the instruction processing cycle take a single machine clock cycle to complete it if you go back to this all of this happens in a single clock cycle for any instruction we have in a multi cycle machine all six phases of the instruction processing cycle can take multiple machine clock cycles in fact each phase fetched for example can take multiple clock cycles if you want a valid address can take multiple clock cycles so that's that's the view you should take it so think about this here you don't not everything happens in a single clock cycle it may actually take for some instruction may take 20 clock cycles to finish okay so let's look let's look at this in some other way also basically what we're doing is transforming architectural state to architectural state prime you execute an instruction essentially that transformation is done by what's called the data path components or functional units function units are part of data path components these are units that operate on the data those units need to be told what to do to the data so you have a clear distinction between data path and control so that's an instruction processing engine fundamentally consists of these two components data path is the hardware elements that deal with and transform data signals these consist of the functional units that operate on the data like the arithmetic logic unit ALU or Hardware structures that enable the flow of data to the functional units and into the registers like wires and maxes and storage units that store data the registers as we will see as you've seen also before on the other end control is very different control consists of the hardware components that determine the control signals basically signals that specify what the data path elements should do to the data so a control signal of a MUX specifies which input should be passed through the multiplexer the control signal of the ALU specifies what function should the ALU perform at this point in time okay so we will see this distinction between control and data paths that's how we were going to build or a single cycle machine always keep in mind you have data path components and you have control components and you may use few data path components and you may control them differently for different instructions so different instructions may use the same data path component but your control signals will be different in terms of how you use those data path components so you can actually build a very simple machine with very simple data path components and control it in a nice way so that it can be powerful okay so if you think about the single cycle machine the control and data is they'll be different different between these two machine types basically control signals in a single cycle machine are generated in the same clock cycle as the one during which data signals are operated on basically control signals need to be generated during part of the clock cycle and then you use those control signals to control the data path elements so essentially everything related to instruction processing happens in one clock cycle so it's serialize you have to generate the control signals and then the data needs to flow based on those control signals in a multi cycle machine you don't have this constraint that's another beauty of a multi cycle machine basically can generate the control signals that you need in the next clock cycle during the current clock cycle while you're doing something right now you generate the control signals for the next clock cycle which means that you can overlap the latency of control crossing with the datapath operation so this way you can get multiple parallels and across clock cycles okay we will see this more clearly when we talk about multi cycle machines and if you read Appendix C this should be relatively clear by now okay so there are many ways of designing data path and control let's take a look at some of them so we have single cycle multi cycle pipelines we have single bus versus multi path so if you remember the lc3 you have a single bus design but actually in real machines you have multiple multiple buses as we will see you can have a hardwired control or microprogram micro code control we will see that later what does this mean based hardwired means control signals are generated by combinational logic or you could access a memory structure to get the control signals basically you have this instruction that you're decoding you want to get the control signals for the next clock cycle or this clock cycle do you have a decoder that's combinational that chain is the control signals or the access a memory location to get the control signals and control signals and the structure depends on the data path design as we will see in a little bit we're going to design the data path and the control but Before we jump into that let's do a little bit performance analysis so that you can appreciate the cell a bit more so what is what is performance mean basically performance is how long does it take to execute a program let's assume we're concerned with what just one program one program consists of many instructions essentially you need to calculate the time it takes to execute all of those instructions so to be able to get there we want to understand how long it takes to execute a single instruction so execution time of a single instruction is calculated like this basically how many cycles does it take to execute the instruction times the clock cycle time this makes sense of course right if you have an instruction let's say an ad do you know how many cycles it will take and you know how long each clock cycle is each clock cycle is 10 nanoseconds if the I'd take three clock cycles it takes 30 nanoseconds to execute the app each clock cycles 10 nanoseconds and multiplied takes 50 clock cycles it takes 500 clock cycles to execute the multiplier okay so the execution time of a program is basically you do this for every instruction you figure out how long each instruction takes and you sum those times up make sense right that's assuming you don't overlap the execution of the instructions clearly but this equation you're going to use it when you're over that but we're going to look at this in a different way so basically this is the iron law of performance analysis you have number of instructions times the average cycle per instruction times the clock cycle time the center that way of thinking about this you can average the cycles per instruction and this is how many instructions you have in the program average cycle for instruction for that and the clock cycle okay so let's take a look at we're gonna get back to us let's take a look at the single cycle mic architecture performance with this equation what a single cycle microarchitecture mean it means cycles per instruction is always 1 it doesn't matter what instructor you're executing you take only one cycle but the clock cycle time is long meaning that you actually need to ensure that the clock cycle time is long enough to execute the longest possible instruction multi cycle microarchitecture on the other hand cycles per instruction is different for each instruction and you can make the clock cycle time short because you're designing how long it takes to execute each stage and hopefully you can make the average CPI small also for example if you're executing ads most of the time add instructions most of the time that instruction may take only 3 cycles right maybe division takes 50 cycles yes but you're not executing that very often so your average CPI is closer to 3 as opposed to 50 and this way your CPI is small your clock cycle time is also small hopefully short and your number of instructions doesn't change between single cycle and multi-site because you know it's the same program so basically the advantage of multi cycle as we will see is you have two degrees of freedom to optimize independently you can optimize the clock cycle time by changing how long your stages are and you can optimize how many cycles it takes to execute each instruction for different instructions okay so now you know the basics of performance analysis yes exactly exactly yes that's the overheads that you will also see so basically that's your colleague pointed out that there is an overhead yes because you need to store the intermediate results and based on the timing and verification lecture you know that whenever you need to store something into a flip-flop there's additional delays and also there's overhead in terms of additional area because you need to have flip-flops to store these intermediate results so it's going to be more expensive to build this multi cycle machine but it's going to be faster and you will see that after multi cycle comes pipelining it's going to be even more expensive to do the pipeline okay so let's take a closer look into the single cycle microarchitecture this is essentially the abstract view now we're going to design what goes into the combinational logic over here I'm going to give you an example that's not from your book books but that's essentially similar you will see how similar everything is it's really important for you to see the difference is because the real word is going to be different from your book and it may be actually very different from your book but as long as you understand the fundamental this is going to be similar so if you're going to start with the state elements this is our program counter this is our register file this is our instruction memory and this our data memory so you can see because it's a single cycle machine there is no program counter Lowel every clock cycle you load the program counter because every clock cycle you process a different instruction right so automatically every at the positive edge or negative edge of the clock you load the program counter there's a control signal over here control signals are orange this is basically the control signal for register writes are you writing to a register and if you're writing to our edge astir this write register ID specifies register your writing - and these are the IDs for the read registers if you know MIPS so it takes five bits to specify registers and this is the data that's written that to be written to the register you're writing to and these are the two reports basically can meet two registers in the same clock cycle you provide an instruction address two instruction memory you get the instruction out you provide an address to memory you can read the data assuming this signal set memory so if you want to delete the memory you set the memory to one and memory to zero if you want to write to memory you set them right to one memory to zero give an address give a data and read data is undefined at that point exhibit fits you're you're not reading it according to this definition so basically we start with these building blocks that somebody has defined and your book actually has similar building blocks so these are the state elements that we have you have program counter you have the instruction memory and I already defined this you can read this on your own the register file it has 32 elements in MIPS 32-bit register file has to read ports and one write port here you have the right naval control signal and data memory has a single read/write port and I think we've already discussed this basically if you if the right enables one the data memory automatically takes the right data and writes it into the address if their idols it reads the address a on to read but in this case I'm going to also have a signal saying memory to explicitly specify that what happened there okay okay so for now we will very quickly we will assume combinational leads don't get hung up on this that much but basically I put of that because we're dealing with a single clock cycle output of the read data port is combinational function of the register file contents and the corresponding leads select port and we have synchronous writes basically the selected register is updated on the positive edge of the clock transition when right enables asserted this is it as you expected right so you cannot affect the read output between clock edges you have there data available for the clock cycle new you have the data that you've written that gets slashed at the beginning of the clock cycling you have that available so we're going to assume single cycle synchronous memory so basically we're going to be able to write to memory in a single cycle so if this takes a long time again your clock cycle is determined by this right so that's one of the disadvantage of the single cycle machine is also so you could also have a memory that tells when the data is ready and design your finite state machine based on that but let's not go into that right now and we're going to talk about that later ok so this is our basic data path I'm going to actually connect these components and this is going to be our data path so we have high generic steps this book is not required but it's very similar to your book instruction fetch instruction decode you will see all of this is going to be exactly the same actually almost the control signal names will be slightly different so instruction fetch is where you fetch the instruction ok let me do it over here you input the program counter into the address of the instruction memory and you get the instruction out and then you have a decode stage where you decode the instruction figure out what's supposed to do and then you operate you fetch the operands from the register file and then you execute the instruction or evaluate the memory address we'll call this the execute AG part of the data path and then once you have the address you may actually use that to fetch an operand from data memory and then you get the value from the data memory and write it back into the register file assuming this load if it's a store then this store needs to be having any staff and over here so essentially this is your data path for many different instructions you can use this data path and we're going to form this data path a little bit more rigorously this is just a picture right now so this is what's to call hopefully at the end of this lecture although I'm not sure if you're going to finish by the end of this lecture may be able to this looks complicated right now right most really not complicated once you figure out how each component is and the picture in your book is actually very similar to this okay so let's start so this is actually the picture in your book if you look over here glim celibate it's very similar okay so let's start building this data path from the ground up so somebody gave you an ISA instruction set architecture how do you design a machine that operates that basically is that instruction that architecture so somebody needs to give you the instructions clearly so we start with the ALU instructions first arithmetic and logical instructions and the data path for its medica and logical instructions are essentially similar so remember in MIPS we have our type ALU instructions we're going to build the data path for that first our type means you have three register operands these three RS are T the result goes into Rd and this is basically the semantics is that if the instruction is this the opcode is this and the function is specified like this you do the act basically what does that mean you take you access the general purpose register file with the indicator RS get the data add to it and then access the general purpose register file with the register ID RT add those two values that get from the register file and update the register file at location specified by Rd and then you increment program counter by four nothing surprising here and this is data path you need to be able to do that basically basically you translate this semantics into combinational state update logic using the elements that I've introduced earlier so what does that mean let's do this PC pop for first during this cycle what needs to happen these two things need to happen concurrently at the same time that's what adds specifies so it basically need to take PC add 4 to it and write the result back into the PC at the end of the clock cycle that's one part of what this happen and you need to have the data path component to do that also you need to read instruction memory fetch the instruction that now you got the instruction over here now the decode of the instruction assume that we have an ADD what does that mean you need to be able to read the registers register 1 and register 2 which brings you the data in the register file and then the ALU operation needs to be an act so you need the control logic needs to specify that the senate's there's a control signal as you see it's orange and the data flows this way during the clock cycle you read the registers ALU specified ad which means that you've done the addition the result needs to be written back into the register file as specified by the write register and you need to assert the reg light signal so reg writes control signal needs to be set to one so these are the data path elements you need two control signals reg right and your operation to be able to implement the app makes sense right very simple okay so here now you actually be implemented at but we could implement other things as any R type MIPS operation that goes through the ALU is implemented this way because you read two registers you write one destination the only difference is what the ALU operation is right for example if you're doing an or the ALU operation function is zero zero one if you're doing a bit twice and it's different if you're doing an ad the ALU operation is this and you might be doing other stuff also depending on what your is a specifies right so basically this is the control signal for the ALU and depending on what the instruction opcode or at the bottom that fun starts remember the function part in MIPS is you set the ALUs control signals accordingly okay we're done with our type ALU instructions now we have some basic data paths now we're going to build on it how do you do I type what is I type you don't have three register operands you have to register operands on one immediate one register operand is a source the other register operand is the destination and you basically what the instruction specifies is clearly you increment the PC you take the source register RS add it to a sign extended immediate that comes from the bottom 16 bits of the instruction and store the results into the general-purpose registers specified by RT in this case so it's the location where RT comes is all bit different from the bits that are specified if you go back this Rd is essentially these bits bits 11 through 15 over here whereas these are bits 16 through 21 okay so we're gonna add a MUX in the data path to be able to execute this in a similar data path let's take a look at that okay so this is we start with the data path with our type instruction now we're gonna add I type on top of us PC plus 4 stays the same you fetch the instruction now what if I typed it you need to read one of the source registers add to it the bottom 16 bits of the instruction sign extended to 32 bits so now you're al your patient needs to be an add still well in this case it's an ADD immediate the source is one of the registrar pens and the other Shore still a bit different now it doesn't come from the register file it comes from the sign extend immediate that we added to the data path so that we can implement this instruction so what do we do well we add Maxis they're actually two differences let's take a look at this one first basically you have a MUX that selects between the output of the secondary port of the register file and the sign extended immediate coming from the instruction how do you decide which one you actually choose basically you decide if the instructions I type if the instructions I type you choose the bottom input to the MUX because you want to add the immediate so the first source operands coming from the register file if the instructions are type you want to add two registers so you choose this one so this is the control signal we have it's called east is I type or ALU source so that's part of the data path elements so when you add a data path elements you also had a control signal over here to choose which part of the data path elements the data should flow through so that you can execute different instructions make sense we could have added a complete a separate ALU also write for just I type versus R type but that would be not so good probably right because that's a lot of overhead instead we use the same ALU and we decide which inputs should go to the second operand of the Ale okay that's one we also need to add this MUX the reason as I said earlier is this nation register of I type instructions comes from bits 15 through 11 no the other way the destination register of our type instructions comes from bits 15 through 11 whereas the I type comes from bits 20 through 16 so if the instructions I type you choose bit 23 16 otherwise you choose bits 15 3 11 as the right register identifier that's specified by the ISA we don't have control over here somebody specify the ISA this way the destination register here is RT bits 16 through 21 if you go back to our type the destination register here is bits 11 through 15 and if we want to use the same data path elements we have to add these muxes and control them appropriately so if you're executing an I type operation we set these as I type to 1 and the data path elements are controlled accordingly so that you do the add immediate and you could also do some other immediate operation based on the ALU operation okay so LC 3 what's similar if you this is just to jog your memory we've actually seen something like this before if you look at LC 3 this is add with one literal add immediate it has very similar control logic as you can see over here this is you have a source register coming from here you have another source register coming from here and the bits inside the instruction that distinguishes between R type and I type in lc3 selects whether you choose the second source register coming from the register file or you choose the sign-extended immediate coming from the instruction and this MUX is essentially very similar to this marks over here because the ISA is have add register add and register Potts immediate adds ok so hopefully you'll see that this there there's no magic here it's very the way you implement an ADD immediate and destruction with a simple data path with minimal elements is very similar across different is ace but the microarchitecture is slightly different as you can see based on the is a specification okay so now we are done with our type and I type arithmetic and logical instructions even though I didn't talk about logical you can imagine that shifts are implement in a similar way right let's take a look at extending this data path with data moment instructions so we're gonna add loads now this is lured word in MIPS it's a specification is like this you basically increment the PC you calculate the effective address by sign extending this offset and adding to it to the value of the base registers specified by this and then you access memory with this effective address ignore this translate for now in real MIPS you actually have a translation which we will talk about at the end of the course but basically you take this effective address and access memory and write the result back into general purpose register indicated by this RT so it's actually very similar to an eye type operation right this is immediate RTS destination RSS source it's actually very similar except the opcode is different right okay so we're going to use the same data path to incorporate load word so what do we need over here well PC plus 4 remains the same now essentially the instruction needs to read the register base register specified by RS so this is the first register that you get add to it a sign extended immediate coming from the bottom 15 months that's very similar to add immediate so we do an ad over here so we don't need to add anything actually because we already did this for ADD immediate load just happens to calculate effect effective address the same way add immediate adds a register to assign extend immediate so we didn't add anything over here but we need to be able to access memory in this case or the result of the ALU which is the effective address needs to get connected to the address over here and we need to ensure that we're not writing to memory so this control signal needs to be to zero we need to ensure that we're reading memory this control signal HP is set to 1 and the read data needs to get connected to this write data over here such that the data that comes out of memory at effective address is calculated by here is written into the register file to the register that's specified by bits 20 through 16 in the instruction and we already did that for at the at least we did this we added this much for I type instruction load word is essentially an I type instruction so you set the control signals similarly over here I mean I'm sure there's written of course we set the register write signal to 1 that's true for all instructions where you write into a register make sense right so that's our load word basically all we needed to do on top of add immediate is access memory using the ALU result and write the data coming out of memory over here but of course it's not that simple because we need to add some maxes now right because remember add immediate needs to write the data coming from the ALU results directly into the register file load word needs to get the data from the memory and write it into the destination register to be able to do both execute both in the same machine into a de max which we will see later store where does let's look at store word first store board is actually a bit easier because it calculates the address in a very similar way to load word and watch the result into the memory with that effective address well whatever the result result comes from the register file specified by RT so to be able to execute store words we calculate the effective address in arrays in the same way we do for load word so now you have the effective address over here now where does the data come from the data comes from essentially this register over here so let me give you the full data path basically I'm constructing it as I go but the the data comes from this register over here so it's really the second data second port so the first register base is this one you add to it sign extend the media to calculate the effective address and the data of store word comes from the second register port which is essentially our T over here okay so use that as a source register and store word and you set memory write signal to 1 then we read signal to 0 so that you write to memory and you ensure that the destination register is not written write registry write signal 0 because we're doing is 4 and store doesn't change a register it needs to write the memory make sense so this is the complete data path that can execute everything we've discussed so far so yeah this is actually not the this is a load store data path this is the load store plus a data path so this doesn't have the MUX over here and basically this is what we can do stores and loads but if you remember I said that you need a MUX over here so they add immediate or add writes its results from the ALU to the register file a load writes its result from the memory to the register file and basically you need to pick which one to do depending on which instruction you're executing so either another data path element this MUX you need to control the data path element accordingly depending on which instruction your execute in this case if we're executing a load is load you pick this MUX is controlled this law this is true and this MUX selects the signal coming out of the memory so that it can be transferred to the register file if it's not a load then the Mac selects the result coming out of the ALU output and writes it and sends it to the register file and you can see that these are the control signals basically this is controlled by the I type if the instruction I type this MUX selects this signal over here otherwise this signal if the instruction an I type you select the sign extended immediate over here if the instruction a store you write to memory it's instructional load you read from memory if the instructional load do you get the data coming out of this port of the MUX and this register right turns out to be if it's not a store you're running through a register assuming we just have the instructions we've seen so far okay so basically you construct the data path I can actually execute arithmetic and logical operations plus memory access operations now let's add control from instruction on top of this so we're going to look at the simple jump in charge this is going to complicate some other parts of the machine now so what is job jumps a jump is simple basically you calculate the target address by concatenating top four bits of the PC with the immediate value and shifting it by left by two is getting zeros at the bottom and then you basically set the target set the PC to that target address so it's actually not that that we can use exactly the same as strata same data path we're going to complicate this part because we're not going to touch anything in this data path of course we should do no harm no harm meaning clearly jump doesn't write to register or write from memory write to memory right so we should not try to register we should not write to memory that's no hard because when you exit when you when you execute a jump instruction you don't expect that to happen so clearly we're not going to write the register we're not going to write to memory I guess we could read from memory as long as we're not going to do anything with it but why we'd from memory if it's not necessary so this could potentially be an X and we don't care what happens over here because we're not going to use any of these results we're not going to write to a register or write to memory so these are don't cares but we need to do to start add some other data path elements over here so what are those basically we need to take the bottom let me go back to the instruction definition bottom 26 bits of the instruction concatenated with the top four bits of the program counter shifted left by two and then send it to load into the PC so if the instructor and that means add this month so if the instruction is a jump this is the data path elements that happens you basically take the top four bits of the PC use the PC to access the instruction memory to get the instruction you get the bottom twenty six bits of the instruction concatenate them write the result into the PC that's how you update the program come through the job if the instruction is not a jump program country is updated by PC plus 4 as you can see now we've added the data path elements to be able to execute both jumps and not jumps and if this is with all of the control signals but you need to set the control signals to be able to execute that jump correctly ok that's fun right but clearly as we add more instructions the machine is getting more complicated ok so there are other instructions like jump register jump and Link a jump register for example what it does is it calculates the address by reading a register and putting the register into the program counter so that's jump register for you so basically to add the data path elements the wires and the MUX input so that you could take the register that you're reading specified by the instruction bits and place it into the program counter so that's the data path to components that you need now this becomes more complicated it cannot be just is jump now this is a 3 input MUX you have 2 bits to select so if you're executing a jump register this should be the input that you select so now you add more control signals to be able to do that there's also other stuff that happens with jump and Link which we didn't discuss but you can so basically jump the link means that you jump to a register and you save the program counter in some other register ok so if you want to implement a link register sorry there jump and Link actually you get the address from the PC plus concatenation and then you save the register you save the PC to a register jump and link register is actually more complicated you jump to a register and then you basically save the PC to another register so you need to add the data path components to be able to execute that I'm not going to go through that right now so these are the other jumps over here basically ok so maybe I was wrong let me see so these are the other jumps in MIPS so this is what we jump and link yeah actually that's essentially what I said jump and link register this most complicated one you saved a PC passport where register and you use the source register to change the PC so you need to be able to provide the data path elements to be able to do that that will complicate the different parts of the machine so if you want to learn more about MIPS there's i'm--it's cheat sheet which we can update let's take a look at conditional branches there's going to complicate things even more but we need to be able to execute these so we need to be able to add data path elements as well as control signals to be able to do that so what is you remember branch if equal what does this instruction do it tests whether the data value RS is equal to RT and if that's the case it basically changes the program counter to a PC plus 4 plus sign extended immediate shifted by Q okay so if the registers are equal PC gets the target address that's computed like this otherwise the PC just goes to the next sequential instruction so we need to be able to do this so I'm not going to give you this in this slide first of all whenever you implement this you should ensure that you don't do any harm this doesn't write into any register write to memory that's why memory is omitted over here to be able to implement this you need to actually do a add more data path element so that you can subtract the source register from the destination register and then you get a value and then you need to compare if that value is equal to 0 so there's some logic that needs to be added if that value is equal to 0 now if that value is equal to 0 then you take the target address so where's that target address well we to add some more data path elements to actually calculate the target address so how do we calculate the target address basically that's PC plus sign x @ immediate shifted left by 2 so this is our sign extend immediate we use it actually as an ALU input before but we didn't shift at left by 2 now we just that left by 2 and then we get PC plus 4 which is here actually and added to add to this sign extend immediate shifted left by 2 that's the target address we have so that needs to be input into the program counter MUX over here so this MUX is getting complicated as you can see to be able to handle different types of branches now that's not enough now the input signal to this MUX is dependent on whether the source register is equal to the the whether the one source register is equal to the other source register right so you need to actually do some logic over here if if the two source registers are equal if the subtraction gives you zero then you need to select this target address otherwise the target address should be PC plus 4 so this PC source signal is actually now controlled by not just the instruction but something that's happening in the data path the result of the ALU so that's why it's a more complicated the control signal is not just a function of the instruction bits it's also a function of whether register 1 is equal to register 2 right ok so I'm gonna skip the delayed branch but so basically if you actually include all of those this is what happens and when we put it all together so let me let me go over the branch conditional branch case again I already described it to you but basically what happens is if you have a conditional branch you read the source register 1 you read source register 2 this MUX needs to select the source register to the LD operation needs to be subtract which is part of the ALU control over here and you subtract source register 1 from source register 2 and the result is branch condition over here it's not as simple because there you have different types of branches also that's why you have a branch condition there's a branch less than or equal for example so if the branch condition is true and if this is a branch instruction this becomes a 1 and this controls us MUX over here this MUX selects this ALU result which is PC plus 4 over here plus it shifted left by 2 sign extended immediate coming from the instruction that's essentially the target address of the conditional branch so this is basically is the MUX that controls whether the selecting between the target address versus PC plus 4 and that is depending on the branch condition as you can see over here and then there's another MUX that goes into the program counter that's the four input MUX I showed you in the earlier slide if it's not a jump you select this base page it's not a jump you select either PC plus 4 or the target address of the conditional branch and that gets fed into the program counter so that's how you can execute a conditional batch so I did go through all of the instructions clearly over here but you need to be able to do what we essentially did for every single instruction specified by the ISA to be able to build a MIPS processor and once you're done you need to ensure that it's correct of course and the control logic gives you the right control that's let's talk about the control logic relatively quickly but I've given you example of the control logic but we should also go through it essentially where what we've done is a single file a cycle hardwired control we're determining the control signals as a combinational function of the instruction that we execute right and we've gone through our are type I type and jump type instructions J type instructions so these are things that we've considered so far I don't talk about these but actually the data paths that are here is here covers that I'm not going to go through all of those but this is how you can determine the control signals let's take a look at an easy one which is register right so register right means that should you enable the reg write signal into the register file are you writing to a destination register how can you generate this control signal with combinational logic well this actually shows you how you do it basically you set the signal if the opcode is not store word if the opcode is another branch if the opcode is not a jump if the output is not a jump register everything else needs to write their register basically design combinational logic that sets this control signal to one for instructions that write to a destination register otherwise this signal should be set to do and this is the combination logic that leads to that name correct this is the MOX that we added coming out of the memory port and coming out of the ALU results do you select the ALU results to write into the register file or do you select the memory output Trident registry well you need to think when you're designing who writes to writes the memory outputs interesting only the Lord is come so if the opcode is a load word then the map to read signal is set to 1 otherwise it's set to 0 so if you go back to this picture over here actually the picture that yeah this is the mentor egg max over here one input comes from the ALU and the other comes from here okay so this is a good place to stop we're going to continue with control and we're going to go to multi cycle tomorrow see you tomorrow [Applause] 
c3mPdZA-Fmc,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2020 (https://safari.ethz.ch/architecture/fall2020/doku.php?id=start)

Lecture 1: Introduction and Basics
Lecturer: Professor Onur Mutlu (https://people.inf.ethz.ch/omutlu/)
Date: September 17, 2020

Slides (pptx): https://safari.ethz.ch/architecture/fall2020/lib/exe/fetch.php?media=onur-comparch-fall2020-lecture1-intro-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2020/lib/exe/fetch.php?media=onur-comparch-fall2020-lecture1-intro-afterlecture.pdf",2020-09-17T20:01:19Z,"Computer Architecture - Lecture 1: Introduction and Basics (ETH ZÃ¼rich, Fall 2020)",https://i.ytimg.com/vi/c3mPdZA-Fmc/hqdefault.jpg,Onur Mutlu Lectures,PT2H39M20S,false,27596,555,4,0,23,okay okay cool so uh we're going to cover computer architecture i hope folks are here for this and i hope you're excited about it uh we're going to cover a lot of interesting ideas in computer architecture and this is going to be the introduction lecture i have a lot of slides uh usually our lectures i have high bandwidth but as i said if you want to discuss anything let me know so let me introduce myself and our group first and then we'll go we'll jump into some reasons why computer architecture is very exciting today so i'm a professor at eth it's been almost five years now i think before this i was at carnegie mellon and i started the safari group over there you've done a lot of interesting work and you will see some of that over here and you'll see some presentations from my students as well i got my phd from ut austin uh i i worked on something called runhead execution we later look into that went into the processors and i uh also spent a lot of time in industry both collaborating with them and visiting them once in a while after my phd i started a computer architecture with microsoft research and you will see some works that we have done in that area and i worked at intel and amd designing microprocessors and more recently at google and vmware as well and if you know my web page this is my email address this gmail address is the best way to reach me frankly because it's it turns out to be the most reliable email and if you're interested in some of the things that i do they can you can visit them so uh the the core subject of this course is what i do research in also a computer architecture we also dabble into computer systems hardware security bioinformatics you'll see actually a lot of this during the course of this lecture and you can see some buzzwords over here memory and storage is what we're going to focus a lot on since it's a big bottleneck in existing systems hardware security safety predictability fault tolerance and we're going to especially talk about hardware software cooperation many of the lectures as you will also see in today's lectures and we're going to look into specialized architectures for various things as well so this is our current research mission essentially we want to build fundamentally better architectures and you will hopefully see what we mean by this this is also not just research it's also teaching in my opinion because you in teaching you really want to understand how an existing system is built so that you understand how you can potentially improve it going into the future and i have a slide related to that later on okay i have uh some more slide on what we do research on i want to talk about this because i believe teaching and research are very much related to each other and you will see examples of this in this course but if you're taking my freshman course on digital design and computer architecture you've also seen some examples basically these are some of the key directions that we're following secure fundamentally secure reliable and safe architectures how can we make this these things uh really trustworthy and this is really important because we rely on computing architectures pretty much in all aspects of our lives today and if if something fails it may be safety critical or it may actually open up security risks or it may reduce our reliability and availability for some things energy efficiency is very important we're going to talk a lot about that how to make things more energy efficient where does energy go in existing systems data movement is a big chunk of it for example we're going to talk about memory centric architectures as we go along we're looking into low latency and predictable architectures i think these are also very important and we're going to talk a lot about latency and different ways of tolerating it hiding it or getting rid of it in this course all across the course actually and today there's a huge need for specializing the architectures for various applications and this is a direction that we're also following and you will see examples of this uh in the area and i'm going to talk about all of these actually soon to motivate what we're doing so this picture you may have seen earlier if you have if you haven't seen you may want to brush up on some of the digital design and computer architecture lectures this is essentially the transformation hierarchy of computing you have a problem to solve and at the bottom layer at least in most modern technologies you have electrons to solve the problem and you talk to the electrons somehow except it's not easy to talk because we cannot speak electron language as you probably know or noticed so far well obviously if you do speak the language please let me know i think i'd like to learn it also but basically we translate uh the problem into different layers of abstraction so that we can eventually get electrons to do whatever we want them to do and there are multiple layers of abstraction because it's not easy to directly translate across multiple layers and these abstractions also help people to delineate things such that they don't need to know a lot about what goes on underneath so in this course we're going to deconstruct that a little bit but basically the narrow view of computer architecture which we may have been exposed to in your past architecture courses unless you're taking my course is that it's really about the software hardware interface and microarchitecture we're going to talk about that this is actually software hardware interface is what's visible to the programmer and micro architecture is what implements what the programmer expects the computer would do but in this course we're going to take a more expanded view and in my research in our research we actually take that expanded view also we really want to co-design across these layers somehow basically we have an algorithm let's say a machine learning algorithm that we want to solve can we actually make it very very efficient by co-designing across the layers potentially potentially repurpose the devices or change the devices slightly such that it can do matrix multiplications that are required by these machine learning algorithms much faster for example that's one example of co-design across the hierarchy another example is actually taking an algorithm and putting it into something like an fpga it doesn't go into the devices level but does go into the logic level if you actually put something onto an fpga that way you actually cut across the layers of the stack and co-design across the hierarchy so uh this is my expanded view and we're going to talk a lot about why it's needed and this is the axiom that i will propose at this point but basically if you want to achieve the highest efficiency and performance and also other metrics in my opinion reliability and security for example it's it's really necessary that we must take this expanded view of our computer architecture today because if you limit yourself to some middle layers over here you have you don't have a lot of freedom to manipulate or modify things such that it can be more efficient for example you may have an algorithm if you don't change the algorithm to fit the devices and if you just go through an interface that someone set for you that's rigid then you may not be able to actually repurpose the devices to exploit the parallelism in the algorithm we will talk about that when we talk about in-memory computation for example today's memory devices dram for example or some other memory device has a lot of parallelism internally but today the way we program them is through loads and stores in the isa simple load and store instruction okay if you're lucky you have vector loads and stores fine but there is no way to actually exploit what's happening in the device so if you want to do a bitwise operations in parallel in dram or in some other technology you cannot do that with existing middle layer computer architectures today the software interface limits your hardware interface also limits you but if you actually take an expanded view you can directly implement the algorithms inside the devices and potentially come up with new interfaces also that's also very important for productivity as we will discuss but cutting through these layers actually helps you co-designing algorithms all the way to the device helps you specialize as much as possible such that you can achieve the highest energy efficiency and performance and perhaps other design goals like fault tolerance and we will see examples of this happening especially today one of the major reasons is performance improvements are not as free as before and energy is not improving as good as it was before just with pure hardware technology scaling hardware technology skill has enabled us to essentially improve performance without modifying the software much more or less let's say of course if you're in a specialized domain like graphics for example you still need to modify your software a lot to fit the hardware but in the general purpose domain for a good chunk of the history of computing hardware has improved very fast and because of that software did not need to improve as much or at least as drastically it it could go through this interface for example that was set let's say 50 years ago x86 is one example of those interfaces but today it's very difficult because those improvements are not coming as fast and we will discuss this also in a little bit okay so that's the axiom and these are some of the current uh major topics that we're following in my group again i'm not going to talk about these in detail you can look at the slides we're going to talk a lot of this a lot of a lot of things that are related to this in this course because clearly this is an advanced course and i think it's really important to put research and teaching in perspective uh because we're actually working on a good chunk of uh what computer architecture covers today and it's good to give you examples firsthand from our own experience as to how to design uh future systems but you can see uh there's some interesting uh i already mentioned actually the top four over here but i didn't talk about this data driven and data aware but this basically talks about how to design architectures uh that make decisions in a more data driven manner in the sense that can we actually exploit machine learning or artificial intelligence techniques or develop new techniques such that the architectures themselves learn based on their decisions and data awareness basically can we actually make information higher level semantic information more available to the underlying architectures and maybe the devices such that we can optimize things based on the characteristics of the data for example if you're if you're manipulating uh some data that's not security critical you would not take the same actions on that data or you should not take the same actions on that data if the data itself was security critical right and if that information is properly communicated across the hierarchy you might design devices underlying devices that actually work on that security critical data in a much more specialized and potentially much more private way right so that's one example and we will hopefully see some other examples okay this is my research group actually this is a relatively incomplete and relatively old picture but it gives you an idea and a lot of the people whom you see over here are tas the phd students and postdocs uh not everyone over here is a phd student and postdoc some of them are actually bachelors and masters students like you actually i think most people who are taking this course are bachelors and masters students uh last time i looked into registration so and this was what i was talking about i strongly believe in multiple principles in uh doing research and this is one example i think teaching drives research and research drives teaching and this is essentially what we try to accomplish in in our courses as well uh basically these are two sides of the same coin teaching is essentially something that you tell the world uh that is already known by many people in the world research is very similar you discover something and you tell the world that you've discovered that thing so in a sense research is a specialized form of teaching and but there is also this extra creative step where you generate insight a new idea and you tell the world about that basically and in the end i think once you teach the concepts that leads to new research new ways of improving the world and vice versa also once you do research you you provide a new knowledge that that would be useful for future incarnations of courses okay and this is a loop and unfortunately i don't have very good powerpoint skills as you can see but this is my notation of the loop so you can actually replicate this as you go along okay so i think one of the other things that i strongly believe in and that will be apparent in this course is that we will focus a lot on insight like what is the insight that comes out of let's say a paper that we examine or an idea that we examine uh and i i like focusing on new ideas and encouraging new ideas as much as possible because new ideas really are really what changed the world in the end okay so that said these slides are for your benefit basically there are some overview talks that are linked over here you don't need to look at them but if you look at them early on they might actually help you with the lectures we're going to cover some form of these in the lectures and probably updated forms of these and if you're actually interested in knowing more about me and my views on research and education and how our research group works i'd recommend you watch this interview it's about one hour or so that was done by folks at university of southern california phd students who interviewed me and asked me actually really good questions they were actually very well prepared for this interview so okay kudos to them and if you're interested actually there's more but i'll leave you with these slides over here and finish the introduction okay so let me jump into why study computer architecture but before that are there any questions okay i don't see anyone saying anything hopefully people are listening which is fine but if you're not listening you can always watch this feature in the future also after it's on youtube okay so we're going to talk about computer architecture clearly and what is computer architecture most of you probably know but it's good to define things at least the way we we will see them in this course and i define it as a science and art of designing computing platforms and this is a very general definition as you can see that is consistent with the expanded view of computer architecture basically it goes from hardware interfaces system software programming model and also potentially touching the algorithm devices sides as well and the goal is to achieve a set of design goals basically you have some sort of design goals in your computing system and you want to achieve them it could be for example very ambitious you want to get the highest performance on earth on workloads xyz it's specialized as you can see it's very hard to get the highest performance on earth on all workloads that you can think of that's nice to aim but it's very difficult to get because you have to make some trade-offs unless you of course have a huge cost in the system and cost is always a constraint as we will see in this lecture or it could be a i want to get the longest battery life at a form factor that fits somewhere in my pocket or even smaller with cost less than some amounts right so that's now specifying even more constraints or i want to get the best average performance across all known workloads or all workloads i care about at the best performance cost ratio and this is actually a general purpose computing let's say this is what intel and amd have been doing for some time although their business models are also being threatened because maybe they need to specialize also because maybe this is not an easy task to do going forward but this is an example of general purpose computing the first one is an example of uh high performance computing at the it could be super computing for example but the first one could also be an example of very specialized accelerators as well so it could be a machine learning accelerator for example i want to get the ice performance on deep neural networks and it doesn't really need to be a computer that is millions of dollars it can be a computer that fits in your pocket still but it does actually a very fast inference in machine learning okay so these don't necessarily specify what kind of a machine you build these are really the design goals in the end and in the end designing different machines uh are different from each other clearly uh a super computer is different from a smartphone for example but many fundamental principles are similar and the goal of this course is really understanding how to make those fundamental tradeoffs such that you can use the right trade-offs and right techniques uh for different design goals and that's the idea and i believe a lot of techniques can actually be modified to be used in different parts of a system and that's what's happening also in existing systems for example acceleration is being used in pretty much all kinds of systems to satisfy design goals caches are another example right it's a very fundamental concept that we will talk about they're being used pretty much in all devices that we have today but they're different potentially some caches are software managed in some machine learning accelerators today because you know the access patterns potentially or you can figure out the excess battery or you can compile and figure out some of the access patterns whereas in general purpose computers because you cannot or you don't have the luxury to do that because you have so many workloads to execute you your caches may be hardware managed so these are some examples of fundamental principles and components that are similar but they may actually be used in different ways so let me go through some different platforms uh i'll go through these relatively quickly because these are just pictorial i mean these are some classic machines as you can see but they have different goals even on the left side and right side over here this thing has a different goal clearly thermals are really important here battery life is extremely important here and mechanical control is also important this thing is somebody's envisioning of a self-driving car i don't think we're there yet i don't think you'd be very comfortable driving this car like this guy is doing at this point but this clearly has a different design goal right like safety and security are probably extremely extremely important if you want to drive a car like this and this was actually one of the cars that google put a while back and this also shows you what needs to go into the computing infrastructure that actually enables that car to self-drive of course we don't want this sort of stuff in our cars right but unfortunately we may actually there needs to be a lot of research done such that we can reduce the amount of infrastructure that needs to be on a computer that drives your car this is another example it's a data center clearly a lot of our workloads execute whether you like it or not in data centers and they have different design goals here cooling is a huge issue for example but it's a different type of issue than cooling a system that looks like this right okay but fundamental principles like levers you can play with are maybe similar like well you do voltage scaling for example uh or you do some some different types of cooling okay and this is uh one of the fastest supercomputers this is tianje2 i don't know if it's the fastest supercomputer today but my slide is not updated clearly if it's not and clearly this is designed for extremely high performance [Music] for some workloads and cost is less of a concern in these whereas cost is a huge concern in this so that's one big difference between a data center and a super computer there's a huge cost differential between them again i'm not being comprehensive in all of the metrics clearly but there are many many metrics that you can evaluate these designs with so this is another example that google put into its data center this is the tensor processing unit tpu first generation and there's a nice isc 2007 paper a 17 paper that i might actually later assign in in your readings in your homeworks but basically they designed this so that they could do uh relatively fast inference in their data centers uh and this is specialized machine learning accelerator for neural networks and what they do is it's essentially a systolic array so if you've taken my course digital design and computer architecture course you've seen systolic arrays and this is a classic systolic array except there are many many things around it to make sure that the systolic array works i don't have the picture over here but this is a specialized exclamator and the reason it was put into the data centers were was it but it was too costly for google actually to do the inference using general purpose processors they would actually accelerate inference both gain performance as well as energy efficiency as well as cost by actually building these specialized chips and it's interesting because uh this the reason they could do this is because they're they're executing a lot of machine learning workloads that would fit and be excited on these chips if you were executing uh if the chunk of machine learning that you were doing on your system was much smaller it may not actually make sense to actually build a specialized chip for it because you incur a lot of non-returning engineering costs and of course you incur a lot of manufacturing costs as well but they made the decision that it was actually really important for them to have their specialized design and as a result they actually are employing these in their data centers and increasingly in their edge devices also going forward they also have an htpu if you're interested you can look for it also but this is not this is clearly not just google many many folks are doing this because they actually want fast machine learning responses whether they're inference or training okay this is another example and again this is this also has parts of machine learning in it it's a machine learning accelerator it's tesla's self-driving computer as they put it i believe and there's another specialized example and you can see that they're redundant chips for safety over here we're going to talk a little bit more about these i have actually same slides repeated later on for a different purpose now i'm showing you different platforms but clearly these have different design goals right this one has a different design goal google's one has a different design goal it's actually employed in their own data centers at least this particular version so it's not exposed to the entire world so they can actually afford a lot of tricks they can actually if the chip is not working perfectly let's say they can actually change the software internally without exposing anything to everywhere else in the world right so it's much more controlled and their goal is really cost and performance at the same time here but here if you look at tesla this is going to be employed in or it is already employed actually in real self-driving cars and it needs to work safely and securely so they actually designed things such that they have two chips that are operating in lockstep this is classic dual modular redundancy so if one gets a fault you can detect that fault by having two chips and clearly this is not something google would do because their goal is cost but this is not cost effective if your goal is cost but your goal is safety and security over here so clearly these different platforms have different goals as you can see and they're reflected in the design choices that happen now if you actually if you wanted actually safety or some sort of reliability at low cost this probably what is not what you would want to do you want to actually incorporate some other mechanisms uh checking mechanisms into your chip into hardware without a lot without doing a lot of replication now clearly there are trade-offs on that also right if you want to incorporate some other mechanisms that increase your hardware complexity in a single chip and it may er not be easy to adopt than just putting two chips that operate in lock step rights so this is a relatively simple and easy concept to provide some sort of reliability uh dual modular redundancy okay so i've given you a bunch of examples over here so i've repeated what computer architecture means over here again basically we want let me repeat it actually over here it's the science and art of designing selecting and interconnecting hardware components and designing the hardware software interface to create a computing system that meets functional performance energy consumption costs and other specific goals and we're going to cover essentially everything that we discussed over here in this course in different ways we're going to talk about design interconnections and all of these different goals so that goes that brings us back to this transformation hierarchy and again based on this definition we're actually uh looking at the expanded view over here so let me briefly talk about why we would like to study so hopefully you have you all have your reasons of why to study computer architecture hopefully some of them are beyond getting credits in this course which is also a fine reason but i think you would take much more out of the course if your reasons are broader than just getting credit uh in the end there are many other courses you can select in lieu of this one right but hopefully you're here because you're excited about the topic as well so basically there are multiple reasons clearly and i listed some at least prevalent ones for me here we would like to enable better systems because if you only if you can understand how you build things better you can build that make them better right we want to make computers faster cheaper smaller and more reliable again consistent with the design goals that we have and the design goals are always dictated by problems and we can do this by exploring the advances and changes in underlying technology and circuits and also adapting to the overlying software and what happens up at the upper layers right and doing so can enable new applications hopefully most of them are used for a good purpose but again technology is agnostic of ethics in a sense which is not necessarily a good thing but you have to develop the technology with the hope that it's going to be used for good you cannot prevent every single negative effect clearly if you're developing technology uh that i think i think ethics is a separate issue that uh really need to be dealt and dealt with in education but you can enable new applications hopefully they're good for humanity uh in the future right like life like 3d visualization is something that's happening today actually we still want more over here because as you scale the size of a screen for example this becomes difficult to do but with smaller screens we can actually do that today virtual reality can we actually have this course in virtual reality that would be nice but clearly we're not there at least in the general purpose at least without paying a whole lot of cost but i believe even with paying a lot of costs we're not there yet but we're getting closer there self-driving cars is a possibility today uh but again there are limits uh of how far you can push them will they be able to drive you on mountains probably not uh not as good at least uh well at least or are really crowded mountains let's say personalized genomics personalized medicine can we enable those so these are all actually uh possible with the expanded approach that i've uh discussed earlier to computer architecture and i think this is a very exciting area enabling new applications and also enabling better solutions to problems in the end we're solving problems and these applications are there to solve some sort of problems hopefully good problems for humans but even if you don't enable new applications you can enable better solutions to problems because as we mentioned software innovation is built on train trends and chains and computer architecture and very large performance improvements at the lower layers of the stack architecture circuits and devices have enabled that innovation and architecture is actually becoming a lot more important today because some of the scaling issues that are happening in circuits are not providing us with a lot of performance improvement and efficiency at least with the circuits that we've been building for decades like the cmos circuits that we've been used to there may be potential for other circuitry that are more unconventional but there's nothing really really [Music] readily available within five years to replace existing cmos circuits in my opinion today so we we still need to actually push heavily on architecture to enable better solutions more than more so than before and finally i think uh studying computer architecture enables you to understand why computers work the way they do and this is i think in general understanding things is good for many reasons as we will discuss in a little bit but but basically i believe that today is a very exciting time to study computer architecture there is a there are so many open problems an industry is in a large paradigm shift to novel architectures people exactly uh people have a good idea of what problems to solve although that's debatable sometimes also but assuming that you have a good idea of what comes to soul there are many potential system designs possible and it's not clear what the best options are so there are actually many difficult problems that are motivated that are motivating this shift and they're also caused by the shift so this is sometimes a chicken and egg problem also some of these over here but for example we have a lot of workloads that we want to make faster and more efficient because our life depends on them uh it solves problems hopefully at least not maybe even if not immediately maybe down the road and we want to accelerate them we're seeing huge power energy and thermal constraints especially with the existing technologies but also with the scale of the systems that we're building like i always have a problem with my cell phone it's it's always running out of battery right and i don't think i'm the most intensive user but it's limiting what i can do with these devices complexity of design is a very much a hardware and software view of designs are becoming extremely complicated how can we actually make them easier to design maybe there are other ways of designing computer architectures to make things simpler as i mentioned there are huge difficulties in technology scaling today which we will also talk about uh memories age bottleneck memory and storage are becoming even bigger bottlenecks they're already a huge bottleneck how do we actually solve them a lot of the answers are architectural in the end but again i love the art when i see when i say architecture it's really spanning algorithms to devices over here but architecture is really at the center uh their reliability problems their probability problems are security and privacy issues i'm going to actually talk about some of these in a little bit just to motivate more and the important thing is there are no clear definitive answers to these problems at this point that's why studying computer architecture is really exciting because you can actually find answers to these problems and actually solve really important problems going into the future and these problems actually affect all parts of the computing stack uh if you do not want to change the way we design the systems so there's this is the computing stack over here and the problem that you have at the technology scaling can get exposed all the way up to the programmer and the algorithm actually your algorithm may not work anymore because you may have bit flips in your memory or you may have security problems because of that we will talk about that but i think the the po another point in the slide is user is also an important part over here of the computing stack it's usually not depicted but i try to depict it over here and user interacts with many parts of the system clearly they may be there to solve the problems they may be actually algorithm designers they may be programming and language programmers they may be dealing with runtime system they may be dealing with isa but traditionally user is here but there are also other users of a computing system who folks who are actually programming at a very low level folks who are actually designing the microarchitecture potentially but at the higher level we have many new demands today at the lower level we have many new issues at the bottom so people actually talk about top and the bottom and i will have uh some some discussion on that in a little bit uh and also users are changing it's good to consider uh this uh i think today's user in 2020 is very different from uh user in 20 2010 was very different from user in 2000 and was very different from user into 1960s clearly today i think users a lot more demanding of what a computer should do a lot more demanding of battery life for example and if you go back a little bit i don't think users will be very happy with what you've done that's that's the that's the interesting thing about technology right you push technology to the edge and you cannot go back after some point because going back actually takes you to a much worse quality of life assuming the technology is used for a good purpose of course right okay so there again these things actually uh lead to new problems and there are no clear definitive answers to these problems okay i already set a lot of these basically computing landscape is very different today even from 10 years ago software and humanity trends have changed a lot i mean today you're living in times where we have to do this online we have to do this course online right and there's a good reason for it clearly we have a pandemic today going on and that has changed a lot of those trends even within the course of last year and that has changed a lot of the requirements on how our computers should actually behave and we're finding in my opinion that our computers are not keeping up with the requirements uh because i don't i mean even though i enjoy doing this lecture i don't think we're actually operating in the best possible way with this lecture and that's mainly because of the computing infrastructure and maybe some other things as well okay and also down the technologies have their issues also and their resulting requirements and constraints so as a result today every component and its interfaces as well the entire system designs are being re-examined so all of you are actually very lucky this is a really nice point in time in computer architecture because people are actually questioning things they are open to new ideas that was not the case in 1990s for example in 1990s the world of computing was really dominated by these general purpose computers that are designed by intel and amd so there were really two major companies at least in the processor space but processor space was extremely dominant at that time today it's less so today we have a lot of these accelerators uh that are that have gone into the field uh there are gpus that have gone into the field so general purpose processors are not as dominant but in the 1990s general purpose processors were dominant there was a very dominant paradigm which is essentially we want to make this a high performance pipelined out of order superscalar machine as fast as possible not even as efficient as possible it was as fast as possible on executing general purpose workloads and it was general purpose workloads and that's period basically so that clearly limits the designs that you can come up with right because a lot of the things are already set for you how do you make this super scalar out of order execution engine that has a lot of speculation as fast as possible but today the world is very different that engine is still there but it's a very small part of the entire system that we have today we have many heterogeneous processors and accelerators my cell phone itself has tens of them your self-driving car has hundreds hundreds of them we have different types of memories different types of storage and they need to become more intelligent people are pushing for that and we have graphics processing units for example that are threatening a lot of the workloads to be pushed away from general purpose uh cpus also okay so that brings me to the axiom basically we really need to take this expanded view i already said this so i'm going to skip the slide let me actually talk about a couple of things that are interesting here basically i i'm pushing for this expanded view but uh there there's some debate that is also going on in terms of whether we should really focus on the bottom or the top and i i want to give you a historical perspective of this i wish we were actually in a room then i would ask you the question how many people know about this [Music] lecture that was delivered by physicist richard feynman that essentially is titled there's plenty of room at the bottom i guess you can raise your hands but i find that very cumbersome over here uh feel free to raise your hand i don't see any hands okay fine but this is a very famous lecture that was delivered by feinman in 1959 as you can see you can read about it also you can read the full lecture transcript as well basically here feignman argued that there is a huge room at the bottom meaning at the devices and physics layers that we should really push forward to enable better computing devices or better devices that would enable good things in human life you can see basically this wikipedia article says payment considered the possibility of direct manipulation of individual atoms as a more powerful form of synthetic chemistry than you than those used at the time and later actually this talk was uh i don't necessarily want to say a catalyst but it was in some samsung is a catalyst for nanotechnology which is really really small devices and you can see there's some more over here basically in this lecture he basically said we should really focus on the bottom because it can enable denser computer circuitry we can have much better microscopes than what's possible at the time and later some of the ideas that he put forward were realized by uh different folks as you can see over here and he also suggested that you could make nanoscale machines that can arrange the atoms the way we want and you can do chemical synthesis and there are some other ideas that you can see in this lecture like swallowing the doctor which is essentially having a small surgical robot that you swallow like a swallowing doctor is a pretty accurate description an insightful description of this device i don't think this device exists yet although it's very similar very very small types of devices that do very little things clearly not surgeries at least large scale surgeries exist but this is the room at the bottom although i think some of these things span across the stack as well like swallowing the doctor requires uh all the way from algorithms or into the devices but devices are what really enables this over here so i i believe actually there's still plenty of room at the bottom some people may think that moore's law is ending or have ended fine i don't think it has ended yet there's a lot more to actually explore over here there's a lot more innovation that needs to go on i don't have moore's law slides over here i don't want to bore you with those you can look them up in my previous lectures and we may actually talk about them later on we'll talk about a lot of moore's law related issues clearly scaling issues but i believe actually we need to investigate a lot more into the bottom because assuming that moore's law will at some point end or will become infeasible which is essentially equal to end ending then we need to have some other solution at the bottom and i think there are a lot of potential solutions at the bottom if we actually think differently there's also this relatively recent paper that was published as an opinion article in one of the journals that argues that there's plenty of room at the top i mean this is not necessarily something new but i think these authors basically say that we should really focus on the top but there's not because there's not much left at the bottom i agree with the focus on the top over here i think it's good to focus on the top they also say that basically computer architecture software and algorithms are going to be a lot more important and i also agree with that actually but i don't think it's really it's really the case that the bottom has gone away basically basically i think the top will become more important no question about that we need to be a lot more efficient in terms of how we do software how we think about algorithms how we think about architectures and we need to actually do a lot more co-designing but not with uh not by being exhausting to the bottom so i'll revisit my axiom and i will in the light of these two works that i mentioned over here i will say i think there's plenty of room both at the root at the top and the bottom but it's much more so when you communicate well between and optimize across the top and the bottom so you may actually be missing a lot of what's happening at the bottom meaning at the devices level at the circuits level and maybe changing them slightly if you actually focus only on the top which is dangerous and clearly you may be missing a lot uh at the at the top if you actually focus only on the bottom and but of course focusing across the stack is not easy you need to really communicate well across the the hierarchy or across the transformation layers that i mentioned so we actually need to figure out how to actually enable that communication and optimization much better also so hence the expanded view basically because we really need to exploit what's happening at the top uh the knowledge that we have at the top and the opportunity that we have at the bottom over here and i think this brings us to more cross-layer design we're going to cover this in some parts of the lectures and this will be a foreshadowing i'm not going to talk about some of these papers that but this is something we focus a lot in our research how to actually expose more information from the software layer to the hardware layer such that we can actually do a lot better decisions at the hardware layer but also at the lower levels of the system software also because for example we don't have a lot of information exposed in terms of even something like locality of different data structures uh today from the software to the system software even or certainly the hardware but if you can expose some of that information we can enable uh different things and we can enable much more efficient systems so this is one example we're going to talk about and there are many many optimizations that you can do this is something from the paper again you don't need to read all of these slides but if you're interested you can go and look at the paper and this paper may be one of those papers that we assign in one of the homeworks so that you can do reviews so i'm going to talk about course logistics a little bit more tomorrow but or maybe we're going to upload some lecture on course logistics for 20 minutes or so but you you'll be reviewing some papers and we'll be covering a lot of concepts and whenever i talk about reviewing papers that's what it means it's going to be part of the homeworks and if we don't assign a paper for review you may actually be interested in looking at that paper because it may sound interesting for example you may enable approximation in memory much better if the software actually provides some semantic hints in terms of the properties of the data to the underlying architecture and you may actually design the devices better such that some devices may be fundamentally approximate and the data itself that can't tolerate that approximation can be mapped to those devices that are fundamentally let's say nuts fully correct right that way actually you can overcome some of the limitations that we have at the bottom today because one of the big issues that's happening at the technology level today is how to guarantee reliability now if you can relax that meaning the software tells you i don't need reliability in this section and i i only this i only need some sort of guarantee but it's loose meaning i can tolerate some error rate then that's fine then that's much better now you can actually have a lot more room at the bottom because the top layer says i don't need that much guarantee that was demanded of you in the past because i'm more intelligent at the top also so this is a sort of closed layer design that's actually really important i think going forward and this is why i believe there's a huge amount of opportunity at the top as well as the bottom because these things are not independent of each other what you do at the top and what you can expose from the top also enables you to do different things at the bottom such that you can scale much better so hopefully that expanded view is much more clear to you with this particular example but there are many many other examples that i'm not going to go into right now and this actually spans clearly algorithms to devices because your algorithms can be approximate you can actually increase the approximation in your algorithms also and your hardware can be designed to be approximate also and you can match things in a nice way assuming you communicate well between the hardware and the algorithms okay this is another example it's actually we will talk about this also later on this may also be assigned but this is a special case of expressive interfaces where you actually talk about the locality characteristics of different data structures and you communicate them to a gpu and the gpu does memory allocation and threat scheduling and memory scheduling in an appropriate way to maximize the locality of different structures and this is actually a work that we had done with nvidia and nvidia is actually implementing this in its future processors uh at least that's what what was told to us uh i don't know if it has been done or it's going to be done this is another example it's similar to what i have earlier talked about basically if you know the memory area vulnerabilities of different parts of your application and different applications you can have heterogeneous reliability memories that give you different kinds of reliability guarantees and you can map your application and data structures and different ways to these different memories of course there needs to be a lot to enable this but you can see that we've taken some baby steps along that direction about six years ago this is a more recent example you will see this as well er early on again this takes advantage of the fact that you don't need to be perfect at the algorithm level you can tolerate some errors and that's especially true in neural networks today if if you're doing neural network inference for example there's some amount of noise you can tolerate uh if your data is not correct for example or if you have some sort of bit flips in your data and if you know what parts of your neural network can tolerate and by how much it can tolerate those errors then you can actually map those parts of your neural network to different parts of your memory and different parts of your memory would be designed to give you different reliability guarantees so some part is for example not so reliable but it's extremely efficient and low latency because it's getting rid of the reliability guarantees it gives you some sort of a loose reliability guarantee but some other part is extremely reliable but it's not so efficient meaning it cannot be accessed with low latency because you need full reliability and you need to spend energy to access it because because again you need some reliability so that's the idea over here this is again a co-design across the stack taking advantage of the opportunities at the algorithm layer and matching it with new opportunities at the hardware layer and again at the algorithm layer you can play tricks also so this paper plays a trick it basically retrains the network such that it can adapt to the characteristics of this approximate hardware error rates of this approximate hardware so you can actually co-design through iterations as well so this is one example and we will cover this later on in the lectures and this is another example that looks into co-designing software and hardware together to accelerate indexing operations and spark sparse matrix operations multiplication or addition this basically doesn't go into the device level but it goes into how to actually optimize the memory controller such that it can enable uh much faster indexing when you're operating on compressed data and it also proposes a compression compression mechanism that's much more aminable to hardware acceleration again we're going to cover this in this course but i just want to give you give you these examples okay one last example i think virtual memory is something that cuts across the stack to begin with if there's actually one architectural idea that has been extremely successful in the general purpose domain that cuts across both software and hardware it's been really virtual memory it's enabled a lot of easy life to the programmers because programmers don't need to worry about managing physical memory because they actually can assume very large memory right infinite memory uh that's what virtual memory is and again i'm going to assume some knowledge in this course because i assume you're taking some computer architecture course and we cover virtual memory and digital design and computer architecture for example uh in this course we're gonna talk about how to make virtual memory even better or or we're gonna actually deconstruct and we're gonna ask the question do we actually really need the virtual memory as it is done in the rigid way it is done today basically you chop the virtual memory space into pages and you do allocation at the page ground layer t and you do protection the page granularity also and you need to keep track of things in terms of pages but this becomes quickly not so scalable actually especially if you have huge amounts of memory and it becomes inflexible also because the the the granularity of pages let's say four kilobytes or eight kilobytes uh maybe that's not the right granularity of your data structures right maybe you really want to specify things in terms of your data structures because you really want to treat this data structure as a unit because it has some characteristics and you don't want to chop it into different pieces for some reason it has some security characteristics it has some similar locality characteristics and you don't want to be translating separately for different parts of your data structure for example because at least overhead and this particular paper talks about taking a step i think it's a bold step also in terms of redesigning the virtual memory framework such that you're much more flexible and you're much less rigid and essentially you can do much more optimizations going into the future and i'm going to leave it at that but we will hopefully talk about that later in the course of the lectures also but there's another example of the design that's cross layer that's cutting across the stack because this also requires a lot of hardware support uh for example the hardware actually does itself does the translation in the memory controller over here memory controls a lot of functionality here to enable flexibility to the software at the higher end okay so that brings me to some examples of interesting things happening in computer architecture today although i've already given you some examples from our research for example but i now i want to go into some more examples to motivate you even further today is a really exciting time and we're going to cover some topics but hopefully this will set you some give you some perspective also in terms of what's happening in industry today but before that let me ask you any questions anybody listening maybe someone can give me a heartbeat hello okay okay that's good that's good i i also i guess uh i don't have the time the zoom interface is terrible actually okay i have my time on my other computer so i have to look at my other computer to see the time uh okay thanks jason uh okay yeah again if you have questions please interrupt me uh okay so let me give you a tour of some interesting things that are happening in computer architecture today and today means 2019 let's say 2017 2020 time frame and you can see that a lot of these things are very recent and i'm going to categorize them in terms of different uh let's say metrics we're going to look at some things that are aimed towards higher performance and higher energy efficiency and this is one example over here this is something that was not there until last year this is persistent memory this was introduced by intel and you can see that it's large it's 128 gigabytes it's bigger than dram modules now dram is a problem it's volatile we're going to talk about the next lecture it needs to be refreshed and volatility caused a lot of issues clearly if you want to write something to persistent storage you first write it to dram but then you need to flush it into storage and that takes a lot of overhead actually because if you want to manipulate something let's say that's on disk that needs to be persistent manipulate the file you first need to bring it into the dram that takes some time you need to buffer it in the erm you need to do the manipulation and you need to write write it back to storage which is persistent that takes a lot of effort and time and energy efficiency actually we have a paper related to that that i'm going to mention when we talk about non-volatile memory so the fact that memory has been not persistent or volatile for decades and decades has led to a lot of inefficiency in existing systems but recently things have changed and people worked on introducing non-volatile main memory uh that doesn't lose its data after you plug it off from power uh which creates a lot of opportunity right now now we can have very large so there are multiple advantages of this this is based on 3dx point technology intel calls it it's probably phase change memory which we will talk about actually even though i don't think there's still any public disclosure that this space change memory if you actually look at the history of how things happen and history of patterns and everything you will very very strongly conclude that this underlying technology that's inside this uh module is really phase change memory but basically there's been a lot of research on face change memory which is non-volatile that has enabled these devices that you can buy you can actually buy these devices and plug it into your computer it could function replace your dram or it could be it could augment your dram also as another type of main memory now this is very nice because this is something that has happened recently this also shows you the fact that people are very willing to innovate in the memory domain but as well as in the computing domain in general so the reason why this is denser is because the fundamental memory technology the face change memory technology is much more scalable than the um and we will talk about that in this course and also it's it's still not as high performance as uh dram itself and that's going to be one of the research topics that we will talk about how do you bridge the gap between dram and these other technologies like face change memory but it gives you much higher capacity and it's persistent so if you're manipulating persistent data maybe in the end overall system performance improves because you don't need to buffer persistent data and dram and write it back to storage you directly manipulate it in this bite addressable [Music] persistent main memory so that gets rid of a lot of the system software inefficiency you don't perhaps you don't need to do a file system call for example you don't need to do all of those security checks that are done in software it could actually be done in hardware and you could manipulate [Music] update a persistent byte relatively easily without going through a lot of software checks and stack okay so this is one example and i think this is a fascinating example that's appeared because you can actually use this and clearly things will get better as the manufacturing process becomes more mature you will get higher density chips you'll get more reliable chips and also hopefully you will get higher performance chips although i wouldn't hold my breath too much on the increased performance of these particular chips unless they actually change some of the architectural design as we will discuss when we talk about latency for example because fundamentally efficient memory technology is not extremely fast there are some fundamental limits to the technology itself because of sensing and also writing itself as much more power hungry compared to dram so there are trade-offs compared to the um of this memory but it is there and it is today it was not there ten years ago but this is also very personal for me because we have actually been working on enabling phase change memory as a technology to replace dram or augment dm for a decade for more than a decade now actually this is something we had started at mike while i was at microsoft research in 2007. the original idea was actually in 2007 but the paper was published in 2009 you can see that we looked at how to architect phase change memory is a scalable dm alternative so you could this also gives you an idea that things do not happen as fast ideas like this do not uh get real as fast of course we didn't solve a lot of technology problems and manufacturing problems intel needed to solve we actually looked at it from an architectural perspective and showed that architecturally this could be a good idea to actually look into phase change memory as something that would replace dram but at a technology level there is a lot of manufacturing related issues that need to have been sold to enable it and one example is how do you actually build a read device that can read reliably from a phase change memory cell but this gives you an idea of this is almost a crosstag thing actually you although it doesn't go into the algorithms layer layer you need to design the architecture for it but also you need to design the underlying circuits and devices to operate correctly okay so uh i'm actually really happy that this got realized because after we wrote this paper many people were asking me so when is this going to happen and i never had a good answer i always had to say okay probably it's going to happen five years from now and five years was always a good safe target and it's sometimes it's an aggressive target actually i uh to to say to suggest in a technology like this but now i can say oh it's already out there right okay so if you're interested you can actually take a look at this paper and also another paper that we collaborated with more folks who were working on working on phase change memory about working on patients memory and we collaborated with them and wrote a more comprehensive shorter more easily accessible paper for a general audience okay so that's one example of what's happening today and i think this is quite exciting there will be more coming and we will talk about this when we talk about non-volatile memory and hybrid memories let me give you another example this is this has nothing to do with our research but i think this i find it also this fascinating because this is something people had examined uh maybe 20 30 40 years ago uh but it's written it was never really fully examined because people dismissed it for various reasons and some of the reasons are reliability power considerations thermal considerations cost considerations those are all really important but these folks at cerebrus which is a startup that is essentially building in machine learning accelerator built this chip and this chip is essentially a good pretty much all of the wafer they call the wafer scale engine normally what happens is you take a silicon wafer you cut it into pieces that kind of look like a chunk of this as you can see over here and that becomes your chip for example largest gpu is a small chunk of a wafer you cut it and that's your gpu and you put it on a motherboard or somewhere basically and you can see that uh a largest gpu today is about 21 billion transistors 815 square millimeters cerebros took the approach that saying that okay this is not enough if you want to build a very fast and efficient machine learning accelerator we really want many many computation units on chip and we want really fast access to memory on chip so that's really important actually i believe the second reason was actually one of the major reasons why they wanted this wafer scale engine in fact they have a talk at chips this year where they talked about the memory and software issues and they said that the necessity of having large amounts of memory very close to large amounts of computation power draw us to actually design this wafer scale engine and what this wafer scale engine enables is clearly many many transistors on a chip you can see that they actually used those transistors to implement 400 000 cores of course our machine learning accelerators in their case it's specialized it's not general purpose but they have a lot of memory also a really large chunk of this [Music] chip is actually large amounts of memory i don't remember unfortunately the exact number i have to look it up but it's it's more than 18 gigabytes i think okay don't quote me on this number but i believe in the presentation they mentioned that they have 18 gigabytes of uh sram on this chip now that's very powerful because now you have you can fit very large models on this chip and you don't need to go to the app you don't need to go off chip so you don't have the downside many many other machine learning accelerators have today and this is really a bold move because clearly this doesn't come for free you need to overcome a lot of design issues first of all how do you actually make all of those 1.2 trillion transistors work really well and not also work really well but also clearly they're not going to work really well because there will be faults if you have these large numbers you will you will have faults so how do you actually ensure that this comes at a reasonable cost point [Music] how do you cool this chip how do you uh yeah supply power to this chip uh how do you ensure thermal design issues are alleviated uh so we can talk about that maybe in some other lecture uh when we talk about this but this is also a very interesting direction and this is happening in industry uh clearly today because people are finding that the workloads are so demanding that they cannot really take the same approach it's good that people are actually exploring these new ideas it's not clear if this is going to be successful but if it's going to be uh there's only one way to actually learn if it's going to be successful by building it and trying pushing as hard as possible to actually enable it and i believe this will not be necessarily just for machine learning going into the future machine learning is a really good application that demands this much performance but this is actually a really good chip that puts a lot of computation and memory close together it doesn't do computational memory so don't get me wrong what what they have done is they've said competition and memory are far away so we're spending a lot of bottleneck a lot of performance energy because of this bottleneck so how can we put them together close by competition memory is one option and we will see that in a couple of slides i believe but what they said is just get them close by don't go off chip and that's the idea over here at least one of the ideas over here but again they're trade-offs and that tradeoffs need to work out nicely if this is to be successful okay you can imagine other applications executing on this also but i'm going to stop here uh there are really there are some interesting talks that you can watch uh from cerebros and i think anatec has an article maybe it's not the maybe this is not the most up-to-date article on it but you can you can search for it and there are a lot of really interesting architectural research issues over here uh also okay next slide okay this is the processing and memory approach these cerebros folks took the took this route but other people said okay why don't we build processors next to memory inside the dram chip itself and that's the idea and this is something that we have been working on for example for more than seven years now and the idea is uh today's again there's a huge bottleneck between processor and memory there's a huge dichotomy whenever processor needs to access data it needs to go off chip everything works nicely assuming you don't need to go off chip of course again you can be better of course but going off chip is a huge energy and performance bottleneck [Music] and we've also discussed that later in the lecture so these folks said that okay why don't we put processors inside the dram chip such that these processors are very close to a drm bank in fact there is one processor associated with each dram bank and that processor has high bandwidth low latency access to the dm bank and it can process data you have a programming model for this you can distribute the work across the processors inside each drm bank and hopefully parallelize the work and hopefully most of your work gets done in this accelerator inside the module and you don't need to communicate with the cpu much and that's the idea over here clearly this changes your programming model also there are downsides it increases your cost in terms of building the memory chip because memory chips are optimized for higher density and now you're actually reducing the density because you need to put a processor that's optimized for that needs to be optimized for logic process which is not easy in a drm process and there's cost issues there are reliable tissues they're kind of similar issues actually over here as well there are thermal issues there are power issues that need to be sold there also programming issues at the higher level how do you program this such that you can get the best out of it but that's also true over here how do you program this thing such that you can get the best out of it in fact programming is a big effort over here what is the software stack over here to ensure your machine learning accelerator uh is mapped nicely over here so there's a compilation part over here also and there needs to be a compilation part that makes life easier for people over here as well okay so these are two different approaches to a somewhat similar problem i'm of course reducing things to the memory problem but a memory the memory problem is one of the big reasons why these chips are designed uh the way they're designed okay so this is interesting because we're going to talk a lot about the processing in memory in this lecture and it's good to have real companies doing this sort of work and we're actually collaborating with this company upmam which is based in france in terms of understanding their workloads we have access to the chips and if you're interested we can actually talk about that in more details i've already mentioned this is another example of what's happening in industry today there are machine learning accelerators and there are many different types tesla is clearly one important company that has designed self-driving cars and they're designing specialized chips for their cars and one of the major reasons is because now they can actually design their software stack together with their hardware such that whatever is needed from their design goals of the entire car they can optimize the hardware for it also and that's one of the reasons why they are designing their chips as opposed to buying their chips from somebody else who's designing their chips because now they can actually have a lot more control on what the hardware can do for them at the software layer such that they can design a full system in a much better way again this points to the expanded view right so they can put two chips for example much more easily than someone else this is their board also as you can see okay i'm not going to go into details of this clearly this is one version one generation of them but you can also use this youtube presentation by their chief architect talking about this chip and giving a lot of detail actually that's not normal in industry today it's good there used to be a time where industry actually used to write papers that talk a lot about how their processors are designed i see less and less of that today unfortunately it'd be good to see more and more of that going into the future and yeah but this is a presentation where you can glean more into what may be going on underneath okay so again this sort of accelerator is designed uh by a company who whose normal task is actually designing cars not designing chips right they they do it so that they can optimize across the stack and achieve their goals that's true for google also right google is not necessarily a hardware company although there has always been a very small hardware component but now there's a much bigger hardware component because they're finding their software needs they've already found that their software needs the acceleration that's provided by the hardware and they can have a lot more control over the hardware and what it does if they actually design their own hardware so you can see these large systems companies maybe traditionally called software companies or internet companies or whatever companies you're going to call them they're really becoming co-design companies across the stack and once they actually design chips for machine learning they can actually use those chips for other things so keep that in mind if you actually have a chip designed for x at some point later some very smart person will figure out or will ask the question oh can i use it for my problem i'm not doing x but i'm doing y but can actually repurpose this thing for my problem and once that happens those chips become even more valuable going into the future and keep that in mind this is actually a very fundamental principle i think in architecture and that has happened very successfully gpus for example gpus are clearly designed designed for x initially uh in the in the 1990s let's say and the x was over there graphics right their sole purpose was to accelerate graphics because graphics was extremely demanding and you need to render the frames as quickly as possible and there was no way you could do that without having a graphics engine but later on some smart folks or creative fox asked the question okay i don't care about x graphics but i care about this other computation molecular dynamics let's say and can i use this processor that someone designed for graphics for my own application and they've gone through the difficult path of actually doing that but by doing that they've enabled a path of this hardware that was designed for x to become much more general purpose so that's a way of adopting new ideas that's an interesting way of adopting new ideas uh in hardware you design the hardware it may be specialized to begin with but if somebody figures out and always that figuring out comes from asking the right question which is can i use it for something else and if somebody figures out to a way to use that hardware for something else things become more general purpose and now gpus are actually in my opinion very close to cpus in terms of the workloads they can execute in the world and as a result a company like nvidia has been extremely successful right you may have heard the news that nvidia is purchasing arm and there's a good reason for it because they actually want to have potentially the best of both worlds if they actually want to become even stronger right but that has been enabled because gpus have been extremely successful and some and because somebody asked the question can i do something else than graphics with these gpus okay tpus are in my opinion another example currently i'm not sure if they're used for something else other than machine learning but over time they're growing and they're enlarging their even machine learning base for example the first tpu by google was designed only for inference basically only for tasks where you actually input something into the model and the model gives you a classification result let's say but now you can actually train the model in the new generation tpus basically these are all designed for training as well they can do both training and inference so that has clearly expanded uh its its application domain but you can see that there are other things that were added high bandwidth memory clearly memory is really important parallelism is important floating point operation is important so everything has grown bigger in the second generation tpu and i'm not sure if you will get a chance to cover systolic arrays in this course but if you really want to know what's the fundamental thing that makes these tpus works is it's really a systolic array and basically the idea is it's a systolic architecture you input data and you have processing elements internally and processing elements communicate internally without accessing external memory or external buffers all the communication is in a data flow systolic data flow fashion and at every clock data moves from one element to another element and it's like a heartbeat basically yeah and when systolic architectures were actually introduced their name comes from clearly uh heart related thing systolic right by ht kung in 1970s late 1970s he mentioned that this is like a heartbeat basically [Music] the heart pumps data and the data travels through the veins and it visits processing elements internally in the systolic array and eventually output comes back to the heart and then it gets recirculated so heart is kind of like memory in this case but while the data is traveling through the veins through the systolic infrastructure it's you're not accessing memory clearly you're not accessing hearts right it's being manipulated it changes and you do operations on it so that you don't need to access the heart so that you get very high bandwidth and very high efficiency inside the systolic array and that's the idea so imagine a different kind of interconnection between our cells and our hearts if every single cell had a dedicated connection to the heart it had to go through a load and store to get to get actually a piece of blood it wouldn't be a very scalable system clearly and a systolic architecture has a similar analogy over here first of all heart needed a much higher bandwidth in that case if every single cell had to access it separately as opposed to having this sort of systolic data flow in it but also it wouldn't be a very scalable system uh partly because of that okay so that's the idea of systolic architecture and you can see that it's used for matrix multiplication in google's at least first generation tpu which is really the core of many machine learning algorithms but you can also see the entire system design over here it's programmed with pcie well it's accessed with pcie over here and you can see that there's a lot of data set up that needs to happen just like in other systolic architectures and there's a lot of control so there's a lot of software effort so from the perspective of software this is really fast hardware basically from a correctness perspective software is unaware of the systolic nature of the matrix unit but for performance it does worry about the latency of the unit this is google's way of saying our software seamlessly executes on the systolic engine but we need to really care about what's happening around the systolic engine as well as its latency and we need to actually stage the data appropriately in our software so we need to modify the software to make sure this works that's the idea basically and that's true if you're taking my digital design architecture course you would know that that's true you can make a systolic architecture work on uh at high performance only if you uh orchestrate your data uh nicely to its inputs otherwise you may lose a lot of efficiency so a lot of the machine learning excitators are systolic architecture-based accelerators today actually for good reason for this very reason but it's good to actually think out of the box there may be other ways of designing these machining accelerators okay i don't want to take sides and companies clearly there are many many other machine learning chips that are built by other companies i talked about two of them over here i think maybe three of them but you can find many others and there are many other startups i believe many others to come and as i said initially they may be built for machine learning because this is a very intensive workload and in fact the machine learning became so popular because some folks figured out how to accelerate neural networks using gpus and they showed really high accuracy and really fast performance in 2012 and that's why machine learning really took off after 2012. if the gpus were not there they wouldn't have taken off but gpus are graphics processing units right there's there's an interesting anomaly over here you're exciting machine learning on a graphics processing unit what's called the graphics processing unit now we're building machine learning chips to accelerate machine learning but as an architect we always look into the future rights these are not necessarily going to be used for machine learning for ever in my opinion uh in my opinion they will enable other applications going into the future genomics could be one potentially but it's good to have that historical perspective to say okay the narrow view is that these are machine learning chips but the broader view is who knows what else these will be used for what else can we use them for how else can we adapt them to actually enable the next big revolution so that's the idea over here so you may not like necessarily the focus on machine learning and i think there is a lot actually potentially a lot of hype also going on in the field in addition to fundamental research and fundamental changes because clearly it's a huge application today but i think there's a bigger opportunity than just machine learning going into the future and this is another example i think i got this from this particular website i don't know if this is fully correct i didn't verify it but clearly there are many companies that are designing machine learning chips today there are many startups also i'm not sure yeah you can see cerebros over here as you can see so it's that at least partly it's correct okay and there's also a lot of benchmarking effort we will talk about that there's a lot of compiler development efforts and there's a lot of ip effort that's going on in this also so it's really truly across the stack all the way from algorithms to devices i'm not sure if this uh thing actually covers some of the emerging and even more forward-looking devices like making use of memristors or non-volatile memory to actually do a machine learning inference inside the memory chips uh i don't see actually any of these startups but i don't know all of these startups over here so there's actually more than uh more companies uh doing this than what what is what is depicted in this picture over here okay so that was performance energy efficiency any questions on that so even in performance and energy efficiency we have a lot as you can see and this is industry i haven't even talked about research in the uh in the past few slides at least now let's focus on reliability and security this is another area that's very interesting computer architecture today i'm going to talk about multiple things robham is clearly something that's very close to my heart and several of my students hearts because we have worked on it for some time now but this is an example real vulnerability that's out in the field today that we have found in 2014 collaboratively with intel uh and uh the idea is just like this basically you have a hammer and you're nailing the dram chips of course it's a joke basically but basically you can predictably induce bit flips and commodity amp chips that you can buy today and at the time we tested [Music] we found out that more than 80 of the tested dm chips were vulnerable you could basically predictably induce bit flips through software without modifying anything in hardware and i'm going to talk about this in more detail later on because this is important and it's very interesting uh that in 2014 this was true and in 2020 this is still true so i don't need to change my slide for 2020 because we have recent results that were published in june 2020 of this year i'm going to actually reference that and we're going to talk about that in this course that 80 of the tested dm chips that we have tested are still vulnerable even though they're different dm chips today so this is actually interesting because people were fascinated with the connection between reliability and security for decades we're not the first one to suggest this connection but i believe that roehammer is the first example of how a simple hardware failure mechanism can create a widespread system security vulnerability at the software stack at the algorithm layers basically and as a result uh people uh in this semi uh popular culture uh article uh they're talking about forgetting software people are exploiting hackers are actually exploiting physics and this is actually a good high level depiction of what's going on in what raw hammer because you get bit flips due to technology scaling and physical issues at the lowest level somebody can exploit that bit flips to take over a system that's the idea so let me give you if if you haven't seen this brief before let me give you an idea of what this is it's actually uh one of the downsides of again zoom is i cannot ask you questions and expect an answer with low latency right i can ask you the question do you know about roll hammer in a in a class we can actually do this relatively easily but in zoom unfortunately we cannot do it so maybe we should try to find a solution to this collaboratively with my tas i don't know if there's an easy solution to this because this is something that you ask and it's good to have an opinion on but it's not really a necessary thing it's not really like a quiz right this sort of interaction is nice in a classroom but i don't think we have been able to replicate the sort of interaction with our teleconferencing mechanisms yet okay that aside what is rob hammer basically if you look at a memory chip a drm chip specifically it consists of rows and we're going to look at this structure a lot in this course and if you want to access a bit or a location in a row you need to activate that row which means that you need to apply high voltage to the word line over here it's also called opening arrow now if you want to access some other row in the same bank you need to deactivate the strobe which means that you need to apply low voltage to that draw yeah and you need to close the row basically now this is a normal activate and this applying low voltage is called pre-charge india now if you it turns out if you do this repeatedly through software or hardware you could build a memory controller that does this or you could actually do induces in software basically repeatedly apply high voltage low voltage high voltage level to high voltage level to actuate pre-charge activates which i'd activate pre-charge it turns out in most modern drm chips existing or adjacent physically adjacent cells some of the physically adjacent cells are actually vulnerable they basically flip from one to z or zero to one depending on the encoding we'll cover this more later on but we call this the hammer draw when we actually initially found this and we call these victim rows and we show that actually in most real dm chips that you could buy at that time you could induce the sort of disturbance there so this is really a disturbance error if you take a step back and if you think about fundamentals of reliability disturbance is a fundamental mechanism of inducing errors in adjacent cells because because things things are close to each other there are some coupling effects and whenever you access one cell adjacent cells get disturbed and this in this case the specific form is read disturbance because you're reading the cells so you're not even writing to the cells basically what what happens is you're reading the cells that clearly should not disturb any other cell no cell should be disturbed no data value should change but that's not happening some data values being changed because cells are too close to each other because the the the the voltage that you're applying for this word line is affecting the value the charge that is stored in some vulnerable cells in some other word lines over here now we can go into the details of exactly why this happens although there is no perfect way of verifying it there are there are hypotheses on this but i'm not going to do that in this lecture but uh the fact that it happens in real chips uh is a cause for concern clearly because this should not happen okay let's take a look at and this is a scaling problem so these are three major dm manufacturers if you didn't know there are only three major dm manufacturers today in industry they shall remain nameless but you can guess who they are you can easily find out who they are and this is an industry-wide problem it's not a problem specific to a manufacturer as you can see more than 80 of the chips of all manufacturers that we tested are vulnerable and i will talk about the scaling problem later on but this scaling problem is a reliability problem now there might be workloads who are doing this consecutive activates many times to induce bit flips and that's actually true but there are not that many workloads that are doing this but it's actually more of a security problem and when we wrote the paper that introduced the problem he said memory isolation is a key property of a reliable and secure computing system and access to one memory address should not have unintended side effects on data stored in other addresses and we believe that you could actually construct an attack where you could hijack a computer by causing bit flips and by causing the right blips bit flips at the right locations you could get uh you could get basically root permissions to the entire system and once you get root permissions to the system all bets are off you could do anything to the system right now we hypothesized that but we didn't construct an attack these good folks from google projects here actually construct a real attack where they ever actually do that exactly and this became a huge security problem after this there were many papers published that we will briefly talk about in one of the later lectures because this is really a a really interesting phenomenon where a single bit flip that you can predictably induce at a very low level actually disrupts the entire stack at the high levels because your security is compromised okay we're going to talk more about drove hammer but i like this insightful very high level analogy that was posed by a hacker on twitter he basically says it's like breaking into an apartment by repeatedly slamming a neighbor's door until the vibrations magically open the door that you were after and i like this analogy because it's it's it's accurate at some level okay we're going to cover this paper at some point uh in the course if you're interested you can take a look at that this is the paper that introduced draw hammer and we may actually cover this paper because we wrote this retrospective with jeremy who is one of the tas actually that looks at essentially all of the works that have built on roehammer in the last let's say around six years since the paper our paper was published both at the security areas architecture areas solutions areas problem areas plus at the circuit levels and device levels uh in terms of how to model and understand roll hammer so if you're interested in a very comprehensive perspective on raw hammer i would recommend looking into it but this is something that's happening today and interestingly again when when i talked about people about roehammer at least people people who may not exactly know what's going on in the em industry they would ask me the question is this problem sold yet it's been your paper has been out for six years right now i can now definitively answer that question saying that no and we will also cover some of these works that we have published in 2020 where we've analyzed a lot of devices today showing that robham is actually getting worse in existing devices if you don't employ any mitigations it's actually clearly worse but there are no mitigations that we know of today that can lead to low performance overhead today this paper shows that and i'd recommend you take a look at it if you're interested right now but we will actually cover part of that later on so it's interesting that this technology scaling problem is fundamentally getting worse because cells are getting closer manufacturers are packing more cells denser as a result there's a lot more coupling that's going on and cells are more vulnerable to this sort of redisturbance noise today so the number of hammers that you need to do to a rove to actually induce these bit flips is actually reducing today so the order is on the order of 10 000 or so today which is not so good when we actually wrote our paper in 2014 it was on the order of 100 000 or so okay so what is worse is actually uh well it's not necessarily worse but what is also a bit sad let's say is dr manufacturers actually read our work and actually implemented solutions to the problem but their solutions are not perfect meaning we have also found out that you could bypass the solutions that dr manufacturers said were perfect they actually have statements uh in writing saying that they have sold raw hammer but that's not true basically you could actually construct attacks against existing mitigation mechanisms ddr manufacturers employ and show that you uh you can take you can again induce these bit flips that are not supposed to be inducible with the mitigations in place and this is still an interesting area of study and we have infrastructures like an fpga based drm testing infrastructure that we have been working on for some time i'm going to talk about that in the next few lectures but this sort of infrastructure enables this sort of research also to find out what's really going on underneath with the mitigations in place so you can actually reverse engineer some of these mitigations in parts or in full uh to understand really uh what's going on in existing devices but also show that the security problem is not sold so we will talk about security uh later on hardware security later on in this course specifically about roadhammer but also beyond that also so solving a problem and preventing a security uh problems actually not as easy so you need to be really careful in terms of how you design the solution so we're going to talk about some rowhammer solutions but i do not believe that people know of the best solution to roll hammer today at this point and certainly not the dr manufacturers okay so there's also another work that looks at similar things but the idea in this work is to try to figure out can we actually declare a declare a chip rob hammer fee and it turns out it's a very difficult problem so it would be nice actually if we do some amount of testing and say okay after this amount of testing i say this chip is roadhammer free the question is can we do that without any doubts and this paper tries to answer the question from a vendors like the cloud provider's perspective if you are a club cloud provider today let's say a data center manufacturer or even a super computer uh designer although not not as many super computer designers exist clearly but more cloud providers exist you got these dram chips now you're going to put them in your infrastructure or if you're i mean if you're if you're designing a mobile system right like apple or huawei or somebody else right or samsung you're going to put these chips in your system you may want to ask the question okay i don't have a good raw hammer solution in place clearly all the raw hammer solutions that are put in dram are not working am i still vulnerable throw hammer so i'm going to do some testing to figure this out now if you do the software testing basically i think the question the answer is you cannot figure this out at this point so there needs to be more research to figure out a good methodology and answer to this question in my opinion if you're the hardware testing i think if you have an fpga based memory controller and if you rigorously test yes you may build more confidence potentially as to your vulnerability but again i believe you cannot be perfect because there are many many variables that go into row hammer like data patterns and there are some dynamic issues like aging that we don't know about at this point so there's a lot of interesting research questions over here but again my point is uh i'm i'm excited about draw hammers i'm taking more time than actually i intended to on this we're going to talk more about it but my point is today we have these real security issues that industry is dealing with and in 2020 actually because of these works again roehammer became a big issue in industry people figured out that dm manufacturers have really not done due diligence to the problem even though in my opinion they should have done it over the course of six plus years but okay they didn't and now we really need to solve this sort of problem because this problem is not going to get better into the future it's going to get worse okay another one security issue this is again architectural security issue probably many people heard about meltdown inspector and if you've taken my class you've definitely heard about this somebody suggested that we actually use the hand feature i agree you can raise your hand if you learned about meltdown inspector but again this is kind of not nice to monitor okay i see i see hands okay i see two hands wow okay cool at least i can see some people are listening here that's good this is good feedback thank you okay so meltdown and spectre many people have heard these were introduced in 2018 and these are there because of the push for high performance without uh in my opinion legitimately not thinking about security security has never been a big concern until these two things have been discovered so basically meltdown and spectre show that because of speculation and caching someone can steal secret data from the system even though your program and data are perfectly correct there are no bit flips like row hammer your hardware behaves according to the specification which is the isa your hardware never promised that you cannot leak data so it's important to keep that in mind the contract that we have is the isa and isa doesn't have a specification about information leakage so even though it may be easy to blame manufacturers that are vulnerable to meltdown and spectre this information leakage or lack of information leakage was never promised this is really an attack that no one has really anticipated in the contract and even though there are no software vulnerabilities on box basically so why because uh speculative execution uh due to branch prediction uh leaves traces of secret data in the processor's cache and pipelining of course and out of order execution is a sophisticated form of speculative execution and you basically leave trace of data in your processor cache which is internal storage as you know it basically brings data that's not supposed to be brought or accessed if there was no speculative execution because you may be executing on the wrong path right in case of spectre especially in case of meltdown it's some other issue basically permissions are not checked at the right place but again because things are happening on the wrong path the manufacturers decided some of those permissions should not be checked so they were behaving according to the specification they just didn't anticipate that this would cause some information leakage and as a result a malicious program can inspect the contents of your cache to infer the secret data that it's not supposed to access of course it's not an easy attack to construct again neither is row hammer but i believe raw hammer is easier because you can actually induce real bit flips in row hammer if you can induce a real bit flip that's very dangerous whereas here i think it's a bit harder and the malicious program can actually force another program to speculatively execute code that actually leaves trace of secret data so you can actually use some other techniques that other people have developed to actually force another program to execute some code that is going to lead to some secret data that you want to access again you need to coordinate really well so it's not easy attack but if you can do it and some people have demonstrated that you can do it in existing processors then you can still you can give it access to data that you're not supposed to get access to and this was discovered by multiple people around the same time this is one particular example google project zero actually is good at discovering these sort of vulnerabilities both software and hardware and they actually were one of the co-discoverers of this and actually multiple other folks discovered this as well and one of the reasons they actually went when they went through this path was they were fascinated with roe hammer and they did a lot of roadhammer research and they later asked the question it's it's always start with a question basically can i do something else with based on what i've learned and they discovered meltdown and spectre because they were curious enough to ask that question first of all they were curious enough to say okay roadhammer is really interesting can i take over the system with it and then they didn't stop there they asked the question can i do something else that's different from rowhammer to take over the system now that i have developed the sophisticated knowledge of microarchitecture underneath and that's the beauty of architecture research i think you can always go from question to question you don't limit yourself to particular questions or particular things that you've been fascinated with okay so that's the security and reliability part and both of these well the last one i think meltdown and spec spectra are more security but raw hammer is more reliability and there's more that will come when we talk about memory reliability okay i will ask any questions again you can raise your hand otherwise i will continue okay so let's talk about another reason why many interesting things are happening in computer architecture today and that's more demanding work goals and we've already kind of discussed this more demanding workloads actually demands more performance and efficiency but we haven't talked about some even more demanding workloads in my opinion going into the future and this is one example that i believe is really hot today because of covet 19 like for for doing uh yeah we'll we'll discuss in the next slide but basically it's a genome sequence sequencing machine you can see that it's small it can be employed in portable scenarios it's very low power overhead it's a machine that can sequence your genome you can buy it for a thousand bucks i think you need to have the kit also to sequence of course but you can do that assuming that you can do all of that which is not really that hard you can sequence genomes all over the world in fact people are using this machine to sequence i think covered 19 right now clearly to look for kobit 19. but before that they were sequencing viruses in africa to understand things in places where you could not afford to have these large sequencing machines and this sequencing machine is not perfect it has a lot of errors so what you do is you sequence many many times and you do a lot of post-processing so there's a huge computational analysis that you need to do to understand what you sequenced and then to ask questions like okay here's what i'm seeing covet 19 or even more sophisticated questions right i think the first question is not that hard but more sophisticated questions okay i've sequenced a billion uh different genomes what are the similarities here how can i figure out who is related to whom this is a question that's asked by metagenomics for example it's a lot bigger data clearly than sequencing a single genome and asking for code 19. so there are a lot of potential things that you can do with these devices but clearly we're today boltnik by the computational analysis that we can do in fact the sequencing itself uh of the genome is not as as big of a bottleneck today because we have actually found out many good technologies nanopore sequencing is one nice technology we'll talk about that later on but dna passes through a nanoscale hole and while it passes through a nanoscale hole it causes current perturbations and there's some circuitry to detect that those current perturbations to detect and different different bases in the dna cause different current perturbations and this circuitry detects those different current perturbations and actually declare different bases but that's not a perfect process there's a lot of variation associated with it so there's a lot of error built into the system so you need to do a lot of computational analysis partly because of that error and partly because you actually need to do computational analysis to begin with to really reconstruct the genome and actually compare to other things so there's a lot more processing that needs to go on and i believe this is a future workload that's going to be even more important and potentially some of those machine learning accelerators can be repurposed for uh for accelerating workloads that look like this because it turns out uh it turns out some of the steps that happen in in in in the analysis are actually machine learning based okay so clearly we can generate a lot of data and at least a huge performance energy bottleneck and you can see this is actually a slide from early january when code 19 was just starting or at least some parts of the world were getting more exposed to it than uh before and basically uh you can see that these nanopore sequencers have been employed heavily to actually do the outbreak surveillance to do the sequencing on site near sample as you can see over here and there there's more art there are more articles actually on this there are some scientific articles that talk about the effectiveness in my opinion but we can actually cover that very briefly and when we talk about acceleration of different workloads and in particular genomics so we care uh for many reasons uh actually before uh clearly before covert 19 we were working on this we actually started working on this in 2007 or so i will give you a story when we talk about genome sequencing and there was always a question what will this be good good for and we now see that there is one really good application for it that we never anticipated and that was covet 19 if you actually had some sort of so today these devices actually don't have computational capability they can do sequencing then you need to transfer the data to a laptop desktop or a data center to actually do the analysis now our goal is actually to do analysis in the device itself so that you can actually quickly get an answer and that would be private also you could actually do your uh sequencing on your own never give your data to anyone else in the world and you would privately figure out whether you have coveted 19 or not right now that would be nice but we're not there yet but it's interesting that working on fundamental problems that are important that are bottlenecks can actually lead to some other applications making use of those uh things that are discovered along the way and i think this is my another point to take away in this particular lecture you you may as an architect you may always get the question okay you're building this thing but there's no application for it where are the applications there's no demand now i think people are more intelligent today to ask this sort of question but you may actually get that question because sometimes there are things happening in the world to question people's intelligence also it's always good to give some of these examples where you never anticipate an application before you do something to solve a problem you design new architecture and maybe you target some other application but your design actually may be used for a completely different application in the end and that's one good way of seeing the benefits of your design so an architect is really designing for the future not for today so if you get the question why are you designing this no one is going to use this you can always say patience look at these other examples and that's why i'm designing this maybe someone 10 years down the road will actually find an application to actually make use of this much better than what i can imagine today because as an architect you may not it's actually an unfair question also in my opinion to an architect because an architect designs things to solve problems sometimes specialized yes but sometimes not specialized you cannot anticipate every single thing your computer is going to be used for i also uh give this from there's also examples from real architecture right building architecture some building architectures are designed for purpose x now they're using they're being used for purpose y right even though they don't have a lot of flexibility so that's real life also you cannot anticipate all of the use cases and sometimes the use cases that are really good are going to happen 10 years down the road systolic arrays are very good examples also a very good example also systolic arrays were introduced in 1978 by h.d kung and at that time they thought the best application they could come up with was image processing vision processing which is a good application no question about that but they never anticipated that they would actually be the accelerator of choice for machine learning in particular neural networks today and that was 40 years later give or take a couple of years not exactly 40 years but let's say 35 36 years so that's the beauty of a fundamental architectural idea leading to innovation applications later on so keep that in mind we're not designing for today we're designing for future that's why keeping in mind the fundamentalness of ideas is important okay going back to genome analysis today we know it's important clearly i think very people very few people would debate this maybe some politicians who don't still believe that covet 19 exists or is important and you can guess who they may be very few people would argue that it's not important but we're actually really bottlenecked by how we design our computational system so scientific discovery and medical uh improvements are really baldness by the computing systems today not the sequencing machines there's always room for improvement for sequencing machines also but really the real wonder is computational so we've been actually looking into designing these architectures uh to be much faster uh this is one now relatively old one mohammed is a ta also in the course he did this work he's also still doing work to make it better this is an fpga based architecture to accelerate things we're going to talk about those you can actually do in-memory computation to actually accelerate these things that's one another example over here and we're going to talk about even new things this is actually going to be published in micro 2020 we just put up the version of this article on archive and this is basically again a cross-layer design actually i think all of these are cross layer designs these are actually all start from the algorithm and here we stop at the logic we co-designed the logic and the algorithm in fpga here we actually co-design kind of the devices let's say loosely uh we have 3d stacked memory to actually uh and we actually repurpose the algorithm change the algorithm such that it can fit the architecture 3d stacked memory such that you can do in-memory processing of genome sequence analysis and here again it's cross layer you redesign the algorithms to and the architecture together to make things high performance low power in this particular case we're looking at a very fundamental component of genome sequence analysis which is approximate string matching how do you find the differences between two strings very fast and efficiently and again this is a more general problem as you can see from genome sequence analysis in sequence analysis it's important because you want to compare a substring of a dna to some other substring that you may or may not know in the case of metagenomics you want to compare all substrings of a dna to potentially all substrings of other dnase which is a very complex problem so you want this approximate string matching to be extremely fast and efficient but in terms of discovering let's say whether you're vulnerable to code 19 or whatever again you need to do something similar so it's very fundamental to do this but also text analysis if you want to compare a text to another text what you do is again approximate string matching so the application is actually very various in this case also so in this particular case we looked at the specific case of genome sequence analysis and we designed an approximate string matching acceleration framework again this is an accelerator again it turns out it's systolic array based so a core of one at least one of the accelerators there are multiple accelerators to be able to do this but one of the accelerators the systolic array-based so again this is a question maybe there are applications that are out there that can use existing machine learning accelerators in different ways with some repurposing so this may be one of the papers that i would recommend because this is really hot off the press let's say it's it just literally got published on archive uh this today i think i got the notice from damla today but the paper is going to officially appear at micro uh in in october one month from today okay so we're going to cover this bioinformatics genomics because i think this is an a really good example of algorithm architecture logic devices co-design going into the future and if you're not patient you can watch one of the talks that i've delivered muhammad also actually has some talks that are even more comprehensive on this so you can take a look at them but we'll we'll have some lectures talking about this okay so that's those are examples and i think those are fascinating examples from genome analysis but there are other applications also not just genome analysis more let's say conventional applications because these things have been around for a long time clearly databases graph processing data center workloads and they're all bottlenecked by data in the end today so we will talk about that and they may actually all benefit from specialized acceleration mechanisms also it turns out a lot of them would benefit from in-memory processing there's a lot of work on the processing in memory for graph processing for example that shows really really good results accelerations more than two orders of magnitude for example and also energy improvements or over on two orders of magnitude that's true for databases also we may get to talk about that but keep this in mind today there's a huge demand from the workloads because data is growing workloads are becoming extremely bottlenecked by uh communica by storage memory communication and computation and we really need to specialize the architectures in some way to these workloads and that's true for uh the mobile end also but the mobile end is actually very interesting there it's not the low end you don't want to call this low end because some of the most sophisticated processors actually go into these machines and there are workloads that are very demanding over here also like machine learning inference there are specialized accelerators that are already in those machines in mobile devices but web browsing video playback etc okay so this is one data points from a paper that we're going to cover later on but we've done the study with google to understand the workload that i've shown you over here these four workloads and we found out that more than 60 of the entire system energy in a mobile device is spent on moving data across the memory hierarchy not not doing computation just moving data such that the data is ready for the processor to operate on so this is a huge waste as you can see and you can already start asking the question perhaps is this the right thing to do and we're going to ask that question also in this course a lot we're going to talk about trade-offs we're going to talk about the status quo but we're also always going to ask the question is this a good trade-off and clearly uh not only us it's not only us who are asking the question but these folks from cerebros asked the question also right and their answer was no this is not the right thing to do so we're going to co-locate huge amounts of processing with huge amounts of memory on a wafer scale processor folks from up i'm also asked a similar question and they said no we're going to put processing units inside the dram so that we're not going to move data a lot okay okay so i think i've covered some example things that are happening in computer architecture today so hopefully this is exciting but there's more actually there's new computing paradigms processing in memory processing near data neuromorphic computing fundamentally secure and dependable computers uh i think there's some more interesting things like quantum computing but they're more much more specialized and much more out into the future and not necessarily uh in the hands of a computing folks i should say because there's a lot that needs to be done in the physics as well still over there or at the boundary of physics and computing which is really fascinating i think but there's more in that area so you can put quantum computing over here and maybe other biological computing paradigms over here could be very interesting obvious neuromorphic computing is one that's closer to the computing end clearly there are new accelerators which we have talked about these are algorithm hardware i should also add slash devices over here algorithm hardware device code designs and we're going to look at that and there are clearly new memories and storage systems as well so it's a fascinating time to be here and as i said they're increasingly demanding applications and never be discouraged by questions by someone saying that okay i don't believe in this because there's no application for it dream and they will come i will give you an anecdote over here since this is the right time i think even though we're awfully behind i think in terms of what i wanted to cover but it's okay as long as people are listening and interesting it's fine we don't need to cover anything i intended to cover but basically in the 1980s the state of the art was at least in the risk domain was mips r2000 so mips r2000 is an inorder i believe dual wide issue processor but it's it's a very simple processor it's much less simple than what we have covered in digital design and computing computer architecture so if you've taken that course you actually design processors that are faster than it or at least conceptually know about processors that are much faster than it in the 90s when after that after it was designed mips r2000 some people were fascinated with them they said oh why do we need a computer that's faster than mips r2000 and this was actually a real sentiment at the time and and gladly there were people who did not listen to those people they said we always want faster computers we don't know what the applications will be and at that time actually applications were even harder to imagine than today because we have a lot of examples today but in the 1980s it's harder to come up with applications because world is is not as communicated uh the world is not as connected at that time as you as you might potentially imagine right that's 40 years ago so there were folks who did not listen and they called actually these people naysayers let's say they say nay to new technology and they basically push forward with building high performance processors and clearly the rest is history today we have processors that are much much more powerful than mips r2000 and we're really glad that mips r2000 is not the de facto computing in fact there is no computer that looks like mips r2000 even even simple microcontrollers are actually more sophisticated today if you look at edge devices for example iot devices people want to put processors that are much more sophisticated than mips r r 2000 today in those age devices so basically if someone asks says we don't want new computers or more efficient computers you can say dream and applications will come and today there is increasingly diversion complex trade-offs i alluded to this but i never gave you some numbers related to this i will do that very quickly over here but basically today trade-offs have changed compared to let's say 40-50 years ago in the past computation was expensive doing a double precision floating point operation was much more expensive than what's depicted in this picture today using some technology parameters assumed by bill dally uh who is the chief scientist of nvidia i think it may be the cto right now actually i think that may be true but again you can look it up basically a 64-bit double precision floating up point operation is about 20 picojoules assuming some technology assumptions again these are all depend on technology but this is going down 20 picojoules maybe not as fast as it was going down before but it's going down because technology scaling is still continuing again there may be people who may think that it's not continuing but it is true that it's continuing people are actually designing chips that are looking into five nanometers today if it weren't continuing they wouldn't be designing chips that way okay okay my point is this is very small today compared to a memory access a memory x 16 nano joules that's about 800 x right okay maybe just an exaggeration today i actually exaggerated even more by making three orders of magnitude but the real number is between two to three orders of magnitude now if you have this question if you have this picture in front of you and then you're an architect and you want to say okay i have this workload and it's it doesn't always have perfect locality actually in fact sometimes it's random access it's doing graph analytics and i just want to access my neighbors and collect the data do some operation and move on on the graph so it doesn't have very good locality does it really make sense for me to do those operations inside a processor because i'm going to pay the cost of multiple memory accesses to do the operation just to do one operation and the caches are not effective in the processor because i don't have locality in the program so that's a good question to ask and if you actually ask that question your answer will almost immediately be no to that probably saying that no i don't want to do this because even ignoring performance i really want to not pay the cost of 16 nanojews for every single memory access just to do a much more simple computation so why don't i put the computational units near memory such that i don't pay this heavy cost of data transfer that's the idea in computation memory that's why people are looking into this and this trade-off is becoming worse and worse why is it becoming worse technology scaling has enabled logic performance to performance and energy performs to be much higher and energy to be much lower over decades and decades so whereas unfortunately memory is dominated by interconnect and interconnects performance and energy have not scaled as fast and this is very fundamental scaling technology scaling you can see that even on chip interconnect is much less efficient right going from one to one end of the chip to another end basically traversing 40 millimeters over here costs one nano joules which is a lot compared to 20 figure joules so interconnect has not scaled well whereas logic gates that are used to implement arithmetic and logic actually have scaled real as a result memory is a bottleneck a communication is a bonding so if you do the thought experiment go back 70 years ago when one neumann was actually proposing his one neumann architecture the trade-off was very different at that time we didn't have 70 years of technology scaling so this was actually extremely expensive and memory access was relatively cheap it was actually if you if you try to gather some data which is not really easy this was actually two orders of magnitude more expensive than memory access and as a result the design choices we have made for designing computers 70 years ago perhaps are not the best design choices for designing computers today where technology scaling has basically made this really cheap okay so keep this in mind technology that's why is extremely important for how you design and look into an architecture and that's the bottom part the bottom part actually should drive a lot of the things that you do in the architecture but again architecture is not just about the bottom if it was just about the bottom then maybe you could say okay it's so easy i just put processors inside the arm right okay i pay the cost whatever you may have some money to pay the cost bear the cost of it and you put process in the app but no one may use it because architecture is also about the top right you design these processors there's also software that executes it and if you do what i just suggested you say okay bottom tells me that i should really not be moving the data into the processor and instead designing processors inside the draft and i'm going to do that and i've done it but nobody's using my computer why not and the answer may be actually because you're now bottlenecked by the top because you haven't considered the top while you're actually designing the bottom or while you're actually designing your architecture motivated by the bottom right and that's the idea so you need to be really careful in changing the uh the way your architecture is designed because that change may break all of the software that's executing that was happily executing previously on your architecture because now you demand a new programming model new software new software stack on top of your architecture which is not really easy to move and that's the idea and i will discuss some other ideas that have not been successful ibm cell processor is one example it was designed for a sony playstation for example it was not successful because it essentially did something even less revolutionary than what i just suggest not it didn't move computation to memory but it got rid of coherence for example between processors and this turned out to be a mess there are also some other idiosyncrasies it was heterogeneous a little bit before its time but because it got rid of coherence across these heterogeneous processors it became a mess in programming and as a result it was not really adopted uh well intel titanium was another example again it didn't change the paradigm but it required a lot of effort on the software stack so if as an architect you're always in the middle you cannot ignore the top and you cannot ignore the bottom if you ignore well if you ignore both then you're not an architect anymore but if you ignore one your life becomes much worse you may get lucky because even if you ignore one let's say the top or the bottom one of them may work out because somebody really intelligent says okay i'm going to make you lucky because i'm i have exactly a program that would execute well on your hardware but you don't want to be just lucky right you want to actually be more intelligent than try to actually improve the probability of the adoption of the new architecture that you have okay so clearly we're at a point in time today that a lot of these complex trade-offs are becoming increasingly diverging and complex and we are re-examining things as a result our systems are looking much more different so this is a past system basically there's not much heterogeneity in this system again this is a very simplified picture general purpose system but modern systems are actually much more different there's a lot of hydrogenating the system gpus are a first-class citizen processors are internally heterogeneous there are other accelerators in the system there are fpgas that are also being connected to memory increasingly i think they're going to be connected even better increasingly they may be integrated into the processor itself memory side is also becoming more hybrid heterogeneous is becoming more persistent so clearly this is becoming different and that's the fascinating part i think okay i already said this i think every components interfaces as well as the entire system designs are being re-examined today okay so this gives us an opportunity basically as as students who are studying computer architecture so some of you may become architects in major places some of you may become researchers who may invent the new paradigms some of you may become software engineers who actually need to design the next generation systems but if you don't know about architecture it's not enough basically to just be a software engineer but basically whatever you decide to do later on you can potentially revolutionize the way computers are built if you understand both the hardware and the software and change it accordingly because you can now reinvent new paradigms for computation communication and storage or you can decide which paradigms to use and i will recommend you this book the structure of scientific revolutions which is really not about a computer architecture it's really about how scientific revolutions happen in science and this book basically theorizes that there are three steps and there's a step let's let's go into the normal science let's say normal science is a time where science is very happy let's say although some people may not be happy but science some scientists may not be happy science is very happy there's a dominant theory that's used to explain things for example the dominant theory at some point in astronomy was the the sun revolves around the earth right and this was accepted and people tried to find ways of showing that and it was business as usual if you question that you actually could be executed also right there were times in history of science where this could happen and exceptions were considered anomalies if someone said oh okay but there's this evidence that sun actually rolls around earth actually revolves around the sun no that's an anomaly if you push forward with this too much you go to the guillotine okay that's normal size it could be pretty harsh as you can see [Music] but over time it turns out assuming science operates uh without a lot of interference from politics and religion or whatever the scientific method finds a way of enabling these exceptions to become the norm basically you find more and more exceptions and now your theory cannot explain those exceptions now people say okay maybe we're doing something wrong assuming people are reasonable right uh assuming scientific method prevails that's what people are reasonable means in today's world you've got to question the reasonability of people a little bit looking at what's going on all across the world but okay i don't want to inject too much of that in this now okay so basically a revolutionary science happens basically people start underlying this re-examining the underlying assumptions right that's revolutionary science at this point they basically say okay this theory is not working we don't believe sun is revolving around the earth so what is going on and at that point they start this pre-paradigm science there's no consensus in the field assuming there are multiple competing theories there may be multiple competing theories somebody says earth revolves around the sun somebody says earth revolves around pluto whatever now scientific method works out and it's at a later point you go into a normal sign saying okay we've figured out the earth revolves around the sun right and this all plays out with the scientific method and this happens to a revolution because the dominant theory is questioned and questioned and questioned then there's enough evidence that builds up against the dominant theory okay i gave you an example from astronomy clearly and i think it works really nicely in a lot of scientific domains as thomas kuhn also shows in his book which i would really recommend but the it's similar in computer engine engineering also actually engineering yes of course it's different from science right in engineering you don't just explain things you also build things which is really what makes engineering different from science basically you have this active nature that actually changes uh the future with your creativity but again you're not bound by you cannot really violate science that's the interesting part right you you really are bound by the scientific method in the end so when you engineer things you may engineer a processor let's say there may be a dominant theory which is the dominant theory that i mentioned earlier this is the normal engineering the normal science let's say you're building these general purpose out of order super scalar heavily pipelined processors and that's the only way of doing things you throw me a graphics workload yeah i cannot execute it very efficiently but that's an exception that's not the norm you throw me something else that's specialized okay who doesn't who cares about that right who cares about machine learning nobody's able to execute anyway that happened in 1990s that's why machine learning algorithms did not uh do very well at that time basically because the normal science we don't have these processors that can execute them well and we're not going to change the paradigm because those are exceptions okay over time exceptions become too much this processor that we're building is way too complex not just the workloads but also the technology itself i cannot verify it i cannot power it it's consuming too much energy so now the anomalies are building up right the exceptions are building up so people question okay is this the right way of building processors or at least in all for all of our applications now then people go into this pre-paradigm science there's no clear consensus in the field and that's where we are right now actually frankly today because we are out of that single processor uh paradigm i believe actually we were pushed into kind of the stage of multi-core paradigm for some time people were actually fascinated by multi-core but they quickly figured out that this is not this cannot be a dominant theory for everything so maybe multicore was considered as a dominant paradigm but today we are not actually at the point with that where a single dominant way of designing processors exists there are many different ways maybe you can think about principles like heterogeneity maybe actually the dominant paradigm is embracing heterogeneity and designing heterogeneous things but that's very high level clearly right so uh that's why i like thinking of things from this analysis perspective of scientific revolutions because we're actually in the middle of a revolution in terms of how we design computing systems today we don't have a clear consensus that's why a lot of these things interesting things are being tried today and even in the memory space like in drum processing uh people actually pushing startups for it machine learning x-rays graph accelerators genomics accelerators many many things i don't even want to talk about graphics because graphics is out there now it's uh part of the normal science almost um graphics processing should be there right so it's it's a very lucky time to actually design this and this work working to work on computer architecture and this is thomas kuhn and i would really recommend this book because it's not an easy read but it actually gives you a a lot of good perspective so okay this is my takeaway it's an exciting time to be understanding and designing computing architectures today there are many challenging and exciting problems in platform design today that no one has tackled or even thought about before and that can have huge impact on the world's future and i think we're seeing this with cobit 19 machine learning this is driven by huge hunger for data new applications even greater realism and who knows what we cannot really anticipate everything that we see some of the statute driven by not so nice purposes like uh surveillance purposes by especially big governments that also happens and some of the competing infrastructures i designed for that actually some of the best supercomputers uh are there put into use for those purposes right cryptography purposes for not good cryptography purposes also so there are many applications out there and as i said uh technology is really agnostic to ethics so we can easily collect more data than we can analyze and understand today that's that has not changed whether it's for a good purpose or a bad purpose and we also have many challenging exciting problems that are driven by significant difficulties in keeping up with the hunger at the technology layer technology is not keeping up as fast as before energy reliability complexity security and scalability all our problems that are caused by lower level technology issues because it's not scaling as well as before maybe we should be looking at different technologies or making it scale with this co-design across the hierarchy so i think you guys are at a very lucky time when i was starting computer architecture in 1997 uh it was a different time basically clearly machine learning was not here actually none of this that i said over here power was becoming important at that time basically power is perhaps the only thing that's here on the slide that was getting pushed into mainstream processors there were early papers appearing saying that power is important we should pay more attention to it and here's some designs clearly that has gone a long way but today fast forward let's say almost 25 years from when i started you see that it's a much more exciting place because i think i think things are exciting if things are more open if you know the answers to all the questions it's less exciting right then then it's it's not the fundamental innovation is not there anymore or at least it's not as fast anymore okay any questions okay i see something let's see this chat doesn't itself oh wow can we take a 15-minute break i guess that's a that's a fair question so let's take it let's take a break but uh i would like it to be a little bit less let's make it let's come back at 241 so that i can cover some fundamentals so that we don't be too late and you can you can watch the lecture if you if some of you need to go also but let's take a 10 minute break that's a that's a very good point i didn't think about the break okay another question i guess uh we're we're officially in the break so there's another question asking will we change the starting time to 13 next week so let me do it look at the poll i haven't looked at the poll results uh so we'll get back to you on that so let's take the break at this point and then continue okay great i think let's get started just wait a second okay you can see me also i assume okay okay i think there were some questions about course logistics so uh let's defer them to tomorrow i'll cover course logistics tomorrow uh okay let me go to this okay so hopefully i've given you some perspective of uh what computer architecture is about today and hopefully you all are excited about being part of this but let's start with some fundamentals also i'm going to give you some other perspective in a different way right now i guess i'll ask you this question what is this and if you're a fast enough typer you can answer it although i may not be able to see your answer okay i don't see any answers in chat somebody answered it okay they got it right yes train station stadloven you may have seen this if you've already taken my digital design course and there's a reason why i put it because i'm going to make comparisons to this architecture itself and hopefully take away some lessons basically the answer is uh yes it's one of stadlehoffen it's a beautiful place actually sometimes too crowded especially these days that things cannot be too crowded as you can imagine but it's also the major first major piece of a famous architect and i assume some of you know who that architect architect is anybody but before we get into that basically this is a description of that the train station has several of the features that became signatures of his work this architect's work straight lines and right angles are rare and that's true actually which is interesting things are always bending in some way and this architect is actually an eth alumnus he has a phd in civil engineering and he is santiago galatra and he built many many things which we're also going to look at but you can you can see that there's an architectural style and it's good to compare the uniqueness of that architecture to some other architecture so this is another train station uh i i took this picture from online and somebody in my class later figured out that the state the station in germany i guess you can read something there's some side channel information that's going on over here that you can figure that out so it may not be the perfect comparison but okay it'll work for my analogy this is basically a vanilla tray station that's out there anywhere else in the world i think almost and i think it's a bit different from a style over it's a little bit different because it reflects a particular style of architecting a system right i mean this is also a system that's being architected with some principles and it has some other trade-offs that are made but clearly there's a difference in the in the style and trade-offs that are made right so it's good to keep that in mind so this thing uh what we're going to do in this course examining ideas and different architectures and looking at different sort of trade-offs and developing new ideas and better architectures is not so dissimilar to what i'm just talking about here comparing two different train stations so this is another train station at least something that houses a train station underneath it's the it's the oculus okay i won't bore you with answering uh asking all these questions over here this becomes much better if we're in the same in the same room and interactive clearly but this is oculus it's in the middle of new york city and clearly this uh has a similarity to stahlhoven actually because stalhofen is somehow modeled after a bird and this also is somehow modeled after a bird so this is also the man this is uh so style hoffman was the first work of the first major piece of kalatjava this uh oculus is actually the is the masterpiece let's say oh it's considered the masterpiece of him and clearly he had some idea of what it would do basically design resembling a bird being released from a child sand and clear the roof was originally designed to mechanically open to increase the light and ventilation so there are some design constraints as you can see people are going to be in the space and those design constraints uh may or may not be satisfied right and clearly some people like it this actually cost people a lot uh i think we will see this cost if we don't see it i will let you know but you can see some praise over here which is last line is very interesting it's a pleasure to report for once that public officials are not overstating the case when they describe a design as breathtaking so that's it also clearly has some satirical nature to politicians as you can see so of course any architecture is not immune to design constraints so you can see design constraints actually curbed the design here security is an important design constraint as we will cover in this course also uh so somebody said santiago's bird has grown a beak in the name of security its ribs have doubled in number and its wings have lost their intestines of glass so they had to eliminate glass you can see at some particular places and dot dot dot now it may evoke a slender stegosaurus then it does a bird and if you're curious about what a stegosaurus is i'm assuming this person is not using it in the best possible context when they're criticizing the design to resemble the stegosaurus okay anyway so there are a lot of jokes clearly but design constraints are real over here and then the design was further modified to eliminate the opening and closing of the roof mechanism because of budget and space constraints which was really a mission by the architect and this also shows us that there's constraints space chip area budget cost cost is always a constraint actually if someone tells you cost is not a constraint they will hit that constraint at some point they may not think it's a constraint but they will hit that constraint and it turns out this is the most at that time at least the world's most expensive transportation hub it's almost close to 4 billion dollars so it's not very cheap so even though it was not very cheap to begin with you couldn't do everything you envisioned to do in the architecture and that's true for a real computing architecture also if you design an architecture you may envision a lot of things in the end you may end up cutting things that's really true in the end for various reasons you may want to push out the chip to the market because your customers are not waiting otherwise you're going to lose a lot more and you may actually not put in some of the security features that's different from this over here you may not actually decide to put in some security features over there that's possible or you may not be able to test things as exhaustively as you should so some bugs may slip or you may actually say okay i'm not going to have this feature anymore so what you usually do is you whenever you design architecture you have a priority list of features and you try to get the highest priority features in and many of them always get left out in my experience at least okay so that was the point of this basically design constraints no one is immune to the design constraints and this is prevalent in the architecture space that's very much prevalent in the computer architecture space also so another question this is uh i don't know if anybody is going to answer this one but what is this i guess i'll give you two or three seconds see how fast you can type or i cannot guess suck if nobody answered i cannot guess if you can type of course but okay but basically this is another thing uh and it's falling water it was close to pittsburgh where i used to teach and it's the masterpiece of another famous architect who is frank lloyd wright and this house it was built as residents for 12 months actually who was a wealthy who were a wealthy family close to pittsburgh it's a beautiful place actually i went there many times while i was there and clearly it has received a lot of praise uh maybe less criticism at the time times are changing so criticism is easy to uh i guess throw at people today on twitter and anywhere but basically it had a similar issue uh it's it's basically it was designed to be much more ambitious but at some point kaufman said we're not paying you anymore stop and he stopped and he stopped at a point that was good at least reasonably good as you can see although there are some things you can see that they were not completely finished and they need to restore it once in a while clearly but basically [Music] this was too costly also for its time okay so i would say our first computer architecture assignment is going and visiting uh vaughan of stalhofen and looking at it from a different perspective i think this is best done after you think about the trade-offs that we examine a little bit you can repeat it for oculus you can repeat it for falling water and hopefully you will never see architecture the same after this course i would suggest appreciating the beauty and the out of the box and creative thinking because there's a lot of creative thinking and that i went in that i've gone into that has gone into actually the bond of their strengths and weaknesses and goals of design you can read about them you can also think about them you can drive principles on your own for good design innovation and this is possible you may not be an architect that's fine you may be a computer architect but you can actually still reasonably evaluate things evaluation is going to be important actually in this course also how we evaluate computing architectures and feel free to send me messages about this i actually still receive messages from past students who who have gone to oculus for example i see i receive pictures from them which is interesting and you can apply what you've learned in this course also to them and hopefully you will be able to think out of the box but let's take a look at one example we're not going to do this exhaustively we want to find the differences between this and that and this is this which is bono stadium and that is that which is a random bonoff in germany so what are the differences again i'm not going to bore you with that clearly there are a bunch of differences but you can see there are no straight things here everything is curbing over here in stalhofan so that's one of the design choices and this thing resembles a bird again uh or ribs of a bird as you can see over here so there's some resemblance so clearly you can list them later but my point is there's always an evaluation criteria for the design true for architecture true for computer architecture functionality does it mean the specification how reliable is it what is the space requirement area requirement was the cost expandability can it be used for other purposes what is the comfort level happiness aesthetics and who knows what else right there's security clearly somewhere over here that i didn't put but all of those exist so how to evaluate goodness of design is always a critical question and we will struggle with that at times but there are clearly some metrics to look into that and i will also say that no design is good at everything this is something that we will talk about in memory for example there's no single memory type that's good at every single metric there's no single processor that's good at every single workload or metric it's always a trade-off and it's really good important to understand those trade-offs here and we're going to we're going to cover this course from the perspective of those trade-offs now let me pose you another question how was calatrava able to design especially key buildings you can have many guesses in terms of this but it's not so much different from designing a computer architecture in the end basically i think i will point out especially the last two over here which is principal design so he was a very principal designer that's why he was so successful and we will at least flash the principle in a little bit and he was also very strongly committed to understanding of and uh basically the use of fundamentals so he was he started very fundamental and applied the principles on top of those fundamentals i think these two are for sure but he also did other things of of course hard work perseverance dedication over decades he built some experience it was creative out of the box he had a good understanding of past designs good judgment and intuition and he had a strong skill combination in math architecture art and engineering and he was probably lucky to be supported with funding and you have the initiative to actually get the funding so actually these things work together probably and hopefully i think you will be exposed to a lot of these in this course and hopefully develop and enhance many of these skills in this course i think good judgment and intuition is very important because not everything that's in architecture is science some of it is art because how do you actually guess what's going to happen 10 years down the road right because some of the things that you design are going to be executed 10 years down the road now some of it can be science because you can do modeling and prediction but not all of it can be science because you don't know exactly what will happen if you knew exactly what would happen in 10 years from now that would be really nice actually for many many things right okay so basically this these are the principles this is in kala java's words to me there are two overriding principles to be found in nature which are the most appropriate for building one is the optimal use of material and the other is the capacity of organisms to change shape to grow and to move so a lot of his works actually open up for example and other folks say that these are inspired by natural forms like bird wings and human body there's another example of kala trava architecture which is in lisbon that was the first place i was in lisbon actually after i uh went there by plane but you can see the antromorphic design over here they actually called xomorphic architecture there's a loose wikipedia article about it but it is based on some principles and this one reminds me of a bird for other people it may remind of a dinosaur or something else this is another one in sevilla it's it's like a pigeon as you can see for other people it's harp but i guess it was meant to be a pigeon okay another quote from another famous architect which has very similar properties actually and similar success is frank lloyd wright he basically says architecture should be based upon principle not upon precedent basically precedent is what comes before you should understand it but not replicate it basically design in a principled manner so as a result he did not design this house which is a classic american house in the woods instead he designed this masterpiece as you can see and it's based on some principles organic architecture is a principle over here and there's another view of that piece basically you can see that it's in harmony with the nature over here actually these cantilevers over here are imitating the waterfall the building is constructed on okay let's see another view okay so let's get to the high level goals of this course basically we would like to understand the principles and the precedence in this course as much as possible architecture is a very wide area especially if you expand it the way i expand it so we'll cover examples of course we won't be able to cover everything comprehensively but hopefully we'll cover important examples that would be useful and hopefully based on such understanding this will enable you to evaluate also different designs and ideas enable you to develop principle designs enable you to develop hopefully novel out of the box designs and the focus on principles precedence and how to use them for new designs in computer architecture my phd advisor was il pat who is this person over here basically he's a very famous computer architect who has with his students invented a lot of the dynamic scheduling mechanisms that went into intel pentium pro a lot of out of water execution super scale execution branch prediction all of that and together with precise exceptions uh was part of the hps model he invented and also branch prediction mechanisms but he basically says the role of the architect is looking everywhere basically you have to have eyes everywhere you need to and this is my re-rendering of what he has said with my commentary and experience of course basically you need to be able to look back to the past and understand and figure out what you've done wrong and what has been done wrong analyze and evaluate it understand the workloads and trade-offs you need to be able to look forward to the future and be open to it you need to be the dreamer and listen to the dreamers and push the state of the art evaluate new design choices if you actually keep keep producing the same thing over and over that will not look forward we need to look up basically what we've been discussing top understand what's happening in the software problems and their nature and you need to look down understand the capabilities of the underlying technology because if you do all of these actually really well then you can actually change things really really strongly and in a bigger way okay we can talk more about this but i will i will not talk but i will give you some takeaways basically i think being an architect is not easy you need to consider many things in designing a new system you need to have good intuition and insight into ideas and trade-offs as a result of this not many people are architects right it's that's that is true actually if you look at computing system stack architecture is actually a relatively small uh group of people because they need to do a lot of different things i mean i'm not saying any part of the stack is easy but this part is actually so critical to a lot of the things at the top as well as the bottom that not many people actually do it but it's fun it can be very rewarding and enables a great future in the end because many scientific and everyday life innovations would not have been possible without architectural innovation that have enabled very high performance systems and very high efficiency systems uh my cell phone would not have been possible with good architectures for example and there are many other examples self-driving vehicles hopefully genomics accelerators if you have a kovit 19 testing cell phone that would be in good part part of an architect in the end and hopefully this course will enable you to become a good computer architect so i'll take a few more minutes just to finish this thought basically we're going to cover hardware software interface and how it affects the software higher levels as well as the lower levels so basically you hopefully take in a competing systems course you're hopefully taking a digital design course if you have not taken it from me you can actually look at the lectures my course is a bit different from digital design in general we don't go into as much detail and nitty gritty of digital logic but we actually cover a lot of paradigms so in the computing systems we normally look at how does a system execute the program assuming something's in the digital logic you don't go below the interface in digital logic you actually in a traditional digital logic course you talk about logic gates and wires and how to build a computer basically an architect is really thinking about a computer that meets this system design goal so the architect really needs to know what's going on in both parts because his or her choices critically affect both the software programmer and the hardware designer in the end so that's why levels of transformation is very important basically this is richard hamming who is a touring award winner he developed hamming codes and he one of his favorite quotes is the purpose of computing is to gain insight not generate numbers basically so we gain and generate insight by solving problems and we ensure problems are sold by electrons i'm not going to cover everything over here but this gives you an overview of the transformation hierarchy this was covered in digital design and computing architecture basically we define what these different things are and you can look at the slide afterwards after the lecture or you can also watch the lectures but basically all of these pieces are there to ensure that the problem gets translated to electrons and i would also recommend that you take a look at hamming's famous work on error correcting codes it could introduce concept of hamming distance which is one way of actually uh looking uh matching different strings but it's not approximate uh it basically tells you how many are what are the differences between strings it doesn't really account for how many insertions are there how many deletions are there how many substitutions are there but this just basically tells you how many locations are there in which these two symbols of equal length strings is different and he developed a theory of codes that are used for error detection and correction also and i will also recommend that you take a look at his uh uh talk on you and your research because it has very good insights in terms of how to do research he actually did research at bell labs for more than 30 years i believe and he developed insights on how to be a good researcher and i would definitely recommend that work so i think as i mentioned a user-centric view may be a good view of these transformation layers why are these transformations layered there layer there basically these levels of transformation are actually good because they create abstractions what's an abstraction basically a higher level only needs to know about the interface of the lower layer what's happening they don't need to know about what happens at the lower level and what is implemented there which is really nice because now you can keep your sanity right if you're a programmer you don't need to know about what goes on underneath in terms of compilation in terms of systems in terms of how the hardware executes those programs right at the high level you're happy exactly what i said over here right abstraction is really good for improving productivity no you don't need to worry about decisions made in underlying levels you don't even need to know about them right like programming in java is better than programming in c which is better than programming in assembly which is better than programming binary which is better than programming by specifying the control signals of each transistor every cycle for productivity purposes performs a different issue over here but for productivity if you had to actually dictate what's the input what the control signal of each transistor should be as a programmer the high level programmer good luck programming sophisticated programs right you will not get there anytime soon but then why are we going to break those abstractions and try to understand across the stack here first of all i think i've already given you the answer from an applications perspective we really need to optimize across the stack to get highest efficiency and performance but there's another reason which is when you run into problems and most people run into problems they write software that doesn't run fast so as long as everything goes well not knowing what happens underneath or above is not a problem but what if the program you wrote is running slow what if it's not running correctly what if it consumes too much energy what if your computer just shut down and you have no idea why and what if someone just compromised your system and you have no idea how and on the other side what if the hardware you designed is too hard to program like the cell processor like potential processing in memory engines what if the hardware you designed is too slow because it doesn't provide the right primitives to the software to actually exploit the hardware and what if it's not reliable and what if it doesn't provide the right abstractions and finally what if you want to design a much more efficient and higher performance system which i already said before basically these are the reasons to cross the abstraction layers because if you don't have the answers to this then you're at the mercy of someone else and someone else may not be actually experiencing the exact same problem so you will not know enough and you will not fix things fast enough if you don't know the answers so the two key i believe a computer architect is actually comfortable in having these answers maybe not all of them in all complicated systems but at least understanding fundamental tradeoffs here but basically two key goals of this course are to understand how a processor works underneath the software layer and how decisions made and hardware affect the software programmer so we're looking at we're going to look at both sides of the coin and hopefully you will become comfortable in making design optimization decisions that cross the boundaries of different layers and different components so i think i'm going to stop at this point at this point basically i'm i was going to give you some examples of crossing the abstraction layers and i'm going to start with an example in multi-core systems which is which leads to memory performance attacks but i think we're going to pick up at this point tomorrow and talk about memory performance attacks and hopefully talk about the em refresh as well does that sound good okay hopefully it does i received one feedback saying yes that's good multiple okay i have one question before uh one two things before we leave off uh one is please fill out the survey uh to see if we can change the time of the lecture to one to four since things are online some people are not able to attend anyway it'd be good if to get more participation in it and the second thing is uh there's some construction going on around here so i hear some noise do you also hear hear that noise okay let's see no okay that's good i'm really happy to hear that no didn't hear very good that means that my headset microphone which i invested in before these lectures is paying off okay that sounds good all right i will see you i will see everyone tomorrow then take care 
ZEcCgaiI7iI,27,"Subscribe to Ekeeda Channel to access more videos https://www.youtube.com/c/Ekeeda?sub_confirmation=1

Visit Website: https://ekeeda.com/

Android App: https://play.google.com/store/apps/details?id=student.ekeeda.com.ekeeda_student&hl=en_IN

iOS App: https://apps.apple.com/in/app/ekeeda/id1442131224

#OnlineVideoLectures
#EkeedaOnlineLectures
#EkeedaVideoLectures
#EkeedaVideoTutorial",2020-09-05T01:34:26Z,"Computer Organization & Architecture (GATE CSE) - Instruction Set Architecture Basics - 4 Sep, 6 PM",https://i.ytimg.com/vi/ZEcCgaiI7iI/hqdefault.jpg,Ekeeda,PT1H45S,false,592,25,0,0,4,[Music] hi um am i audible um send me messages because taking it for first time just i wanted to know if my picture and voice is completely added i'd like to know um yeah of course it is audible people are saying that it is idle fine okay fine yeah um now this is daniel i'm gonna take computer architecture for you guys when it comes to my profile i finished my m tech in computer science and i also did my mba and msc mathematics from sv university i am in the gate field for the i've been in the gate field for the past 12 years with various institutions and when it comes to my teaching experience all together i do have 15 years of experience i do teach a lot of subjects many subjects but here uh in this session we're just going to cover just computer architecture yeah when it comes to ekeda ekeda is the largest mdm brand for online courses especially in the technical stream when it comes to the technical stream ekeda almost has you know the 10 different streams of engineering content when it comes to the game recently ekeda has an initiative to provide an online training for the gate here a student will get any any branch student will get almost 500 hours of content online recorded content in addition to which he will also get an additional content of the live lectures in and around 400 to 500 hours till the time he joins till uh the time he gives the game okay now let us dive into the topic of the computer architecture okay but before that um let us introduce and uh send me hi i just would like to see you guys how many of you people are actually participating here am i completely audible and visual first of all [Music] hello hello hello hello hello yeah yeah it's give me time now let us start the computer architecture in what way we are going to deal with this computer architecture and maybe um when we talk about this computer architecture let me tell you guys that this is something which is very different from the other subject because here we are not doing any kind of a programming rather we are not getting into uh the details of the operating systems or any kind of stuff but rather here we are just designing the system if you can stay focused here just simply we are designing the system right now we all guys we need to think from the perspective of a designer we are no more an end user rather we need to think from the perspective of a design that means that we need to design the computer system entirely being an architectural guide you are just expected to build a computer system okay so here first of all remove all the stuff from your brains whatever you learn because all together you are going to witness and understand the complete uh in-depth detail of the architecture right now okay now but before we do that let us understand some basic things before we get into the actual details okay that is when you uh talk about the architecture first of all we need to talk about from where this the term computer architecture has been generated has been invented right okay let me take the help of the history of the computer architecture okay yeah when you talk about architecture first of all we need to understand about a brief history of the computer system okay here initially a computer system argued people as you know that it just executes a program sir what is the program anything is just nothing but a program whatever you do is just nothing but a program you just sit before the computer system when you type something obviously a program will get executed either you move your mouse or you type something or you just listen to your song or you play a game or you watch a movie anything is just nothing but a program this program will eventually get executed so obviously when you look at the computer system at a basic level it just executes program nothing other than that so first of all we need to understand that in what way we are going to write the programs and in what way where we are going to store the programs this is the thing okay if you are a programmer you need to understand that a program is a set of instructions a program is a combination and a collection of instructions right now when you want to write a program you need to understand several programming language of course you we can render several programming languages like c c plus plus java or whatever okay now after you write a program then there must be a storage provision where you can store the programs i hope you got it that is initially when you look at the computer system at the basic architectural level simply is just nothing but a program executing unit okay other than that it does nothing so if you want to design a computer system being a designer you need to understand that in what way the programs will be written by a programmer and where you can provide some provision to store all those programs as it's shown here a computer is all about executing the programs of course the program is a set of instructions and a programmer is needed to write a program and thereby he will be able to execute the program okay in the initial days of the computing system of the programming in the initial days of the programming let me give you a brief history that is if you want to write a program in 1960s in what we are going to write a program thing is at 1960s in the earliest days of computing there was no memory at all when you look at your computer system you can see that there is a memory okay of course there is a primary memory and there is a secondary memory we will only talk about the primary memory which is the main memory because that is the only physical memory available to us okay but here when you were a programmer in 1960 that computer system which was used in those days doesn't have any memory at all doesn't have any memory at all which means that a programmer although he writes a program there is no storage provision then where a programmer can write a program simple the thing is at that time the program was used to have some punctured cats i know that i mean i think that you might have heard about a puncher cut a puncher card is a piece of a pepper or a material on which some punches the holes will be made a pattern of holes will be made okay if you want to write 100 lines of program you need to collect as many punched cards as required to write your program okay so you once after you collect a vast number of cards then you start punching you need to know in what pattern you are going to make the holes because each and every pattern of poles is going to give you one instruction okay a set of all those cards is just nothing but a program so there at that time the memory was completely physical meaning on a puncher card the only form of the memory that was available in 1960s was punch a programmer if he's a program it's just not enough to know how to write a program you also need to know how to make a pattern of polls on a punched card okay which means that which means that it was a very difficult process to write a program in 1960s it's a very difficult process okay but there is something called one human architecture what is one human architecture after punching cards mechanism in one human architecture system there is something called memory invented this is complete this completely changes the entire look of the computer system the entire feel of the computer system the entire way the programs are executed in the computer system this is all about monument architecture once again initial days in 1960s if you want to write a program collect the punch regards make a pattern of phones write a program after which you will be able to you know feed all those cards to the system then the program will get executed there is no storage provision at all but after the invention of the von neumann architecture there might be a doubt do you have any idea about monument architecture what is one human architecture mean a computer system with the memory was in the initial days called as an ongoing architecture okay simple if you want to understand so what is this term monument architecture it is not a biggest idea the one human architecture is just nothing but a computer system with a provisional main memory okay so there the memory was invented if you are a programmer after the invention of the volume architecture just only the thing that you need to do is take in a keyboard and type your program and store the program in the memory this is what a one human architecture computer system is all about okay um i would like to know i mean do we have any doubts till we treat you because we're going to discuss a lot of stuff here in this class i would like to you know understand if we have any doubts if you guys have any doubts fine so far no doubts well i'm good fine once again volume architecture meaning a computer system with a memory if you are a programmer the thing that you need to do is just you write the program by means of using a keyboard type the program store it into the mail this is the fundamental rule of an one human architecture what is the rule if you want to execute a program you need to store the program in the memory you need to store the program in the minute this is the thing unless you store the program in the main memory there is no way you can execute that at all if you failed or ok you wrote a program and you you know without any error let us say that you wrote the program but will you be able to execute without you store that in the memory no you can't this is what the fundamental rule of the monument architecture that is once again just remember this throughout your life because being an architecture guy you need to understand the important fact of the computer system how it is going to execute a program if you want to execute a program that program has to be residing in the main memory again so what is main memory nothing main memory is a memory which is available on the cpu motherboard main memory is not your secondary memory what is secondary memory okay let us talk about the the configuration of the computer systems your configuration will compare of your computer system when you look at you can have a 4gb or 8gb of the memory main memory and you can also have 1tb or 2tb secondary map some people they do have a i mean they do need to understand the difference between main memory in the secondary of course most of you guys as you already know main memory is physical secondary memory is not physical now what is main memory the main memory is the memory chip which is available on the cpu motherboard which means that that is the only memory available for a computer system okay what is secondary memory the secondary memory is a dump yacht okay now let us understand in what way we gonna differentiate in between main memory and a second trainer here let us see i do have a cpu this is my cpu okay and the cpu is supported with something called amine and also there is something called a secondary memory here okay now this is let me tell you that this is main memory and this is secondary manner okay the difference between main memory and secondary memory is cpu will communicate with the main memory directly but cpu cannot communicate with the secondary memory directly okay because secondary memory is a peripheral device when you look at the secondary memory you need to think that this is this peripheral device this is just nothing but a peripheral device this is peripheral device this is as like as your you know keyboard or mouse or any other peripheral device which is connected external to the externally to the computer system but when it comes to the main memory this main memory is directly available this main memory is directly available on your cpu motherboard this main memory is directly available in cpu motherboard that's the reason why this main memory is called as a physical memory and the secondary memory cannot be considered as a memory at all when a programmer writes a program this program must be made available in the main memory only okay once after program has been loaded into the main memory this program will be executed by the cpu okay so which means that the basic fundamental rule of a program that is needed to be executed is first of all this program must be made available in main this is the thing okay yeah initially we used a puncher cut after which then there comes our main memory in main memory we are going to have all the programs deciding okay after the programs are loaded into the main memory then the cpu will be able to execute this map here what's the program you know that a program is a set of instruction what's the program let me write it here a program is a set of instructions right when you talk about an instruction this instruction will also have a corresponding data when you talk about an instruction this instruction will also have a corresponding data for an example there is an instruction and a comma b which means that a is a variable that is needed to be added to b okay which means that this instruction will have two data elements one is a one more is b one is a and one more is b okay so here this main memory comprises of instructions and data there is an instruction part and there is a data point the instruction part is going to hold the instructions and the data part is going to hold the data element this is the instruction part this is the data the instruction part is going to hold the instructions and the data part is going to hold the data elements more about instruction part and data part we're going to see in the upcoming classes any doubt so far i'm waiting for you guys to ask me any doubts first um you're fine there are no doubts so let me go ahead so here being a designer what should be people design what is the thing that is needed to be designed here now again we are designing a computer system aren't we we are going to design a computer system if you want to design a computer system being an architectural guide what is the thing that you need to design simple things that is first of all we need to design a main memory cpu and a main memory because main memory is going to hold programs and cp is going to execute the programs and then we just we start with the zp because you know that the cpu is the heart right the cpu is going to execute the programs so what we do is first we are going to design the cpu and followed by the memory but when you talk about a cpu is the cpu a single entity no cpu cannot be a single entity again cpu is a collection of components right in cpu there is something called a register site there is something called a register set and there is something like control unit and there is something called an alu okay so here when we design a cpu first of all we need to independently design this register side and control unit and ame all together collectively collectively we're going to design a cpu all these things together we will um constitute something called scp now in this session we're going to focus on designing the register set okay because cpu is going to hold three major components one is the register side one more is control unit and the third one is alu okay now first of all we understand about these components what is the purpose of each and every component here and then we go ahead with designing the things the first thing that we're going to design in the cpu is a register set so we need to understand what is a register set what is the purpose of a register set to be designed and maintained within a cpu okay now understand register set okay when you talk about a registrar's site you know that the computer system is going to execute the programs right what's the program a program is a set of instructions a program is just nothing but a set of instructions okay and all these instructions are available in main memory all these instructions are available in main menu the instructions are available in here memory now when the programs are getting executed one by one this instructions will be transferred to the cp as any program is getting executed one by one these instructions must be featured by the cpu thereby it will be able to execute okay for that reason we should be able to provide some intermediate storage provision within the cpu to access these instructions one by one okay let us say there is a first instruction here let me write it here there is a first instruction there is a second instruction there is a third instruction there is a fourth instruction it is a program is a set of instructions but where those are available these are available only in the main memory but when you want to execute you require a cpu right when cpu executes this it is going to access these instructions one by one it calls instruction number one when instruction one goes to the cpu it should be able to sit within the cpu for a stipulated time as long as it gets executed okay so for that reason for that reason we need to have a storage provision within the cpu which is going to hold the instructions concurrently as long as they get executed so for that reason we are going to design a register search we are going to design a register set okay this register set gives a provision to store the instructions within the cpu and the programs are in the main memory cpu execute the program executing instructions one by one when an instruction executes cpu should accommodate inside and register set exist right then after we design the register set then we go for a control unit what is the control unit the control unit is going to synchronize all the activities of the system the controlling is just going to synchronize all the activities of the system rather at this time point at this point of period we can think a control unit as a controller of the total system that is when you talk about a computer system the computer system is going to hold a cpu in cpu we're going to have a register set we're going to have a control unit and an ali of course a main memory is available within the cpu within the computer system so this control unit is going to synchronize all these components okay this controls the total activities of the system okay now the important thing you need to understand in a controls in a control unit is this control unit is going to hold and master the system clock as you people already know that the system will have a clock right the system always will have a clock every computer system will be provided with a clock every computer system is provided with a clock this clock will be handled by the control unit the control end is going to take the help of the clock and synchronize and controls the activities of the total system okay then what is the alu when it comes to the alien the alu is going to perform arithmetic and logical operations when it comes to the alu it is going to perform arithmetic and logical operations you know what are the different arithmetic operations what are the logical operations as a computer system is going to execute the programs and the programs is going to programs are going to be a set of instructions each and every instruction when it gets executed it is going to perform either an arithmetic operation or a logical operation this can be done in the area okay this alu can perform either an arithmetic operation or a logical operations okay the instruction will perform an alu operation okay but there are there is some exception some instructions won't perform any area operation but during the course of this computer architecture you are going to understand the different types of instructions okay now let us talk about the design of the register set straight okay so for what we learned if you can have a look at we are going to design the computer system we are going to design the computer system the computer system is the collection of is the collection of a cpu and a main memory and a secondary memory when you want to design when you want to design you must be able to design a cpu in addition to the main memory the cpu is going to hold a register side control unit and alu but when it comes to the main memory it is going to hold all the programs so what to design initially we need to design the cp when you design the cpu when you design the cpu you need to design three entities the first one is a register set second one is the control unit the third one is an area why do we require a register set every time when an instruction is being executed the instruction will come into the cpu and will sit in the cpu for a while that that means that we should be able to accommodate an instruction within the cpu as long as it is getting executed so if you want to execute any instruction first of all that instruction must be available within the register set of the cpu so for that we need to design a register set the next thing is the controller what is the control unit the control unit is a total controller of a system this control unit is going to use a clock okay by using a clock in a timely manner the control end is going to hold the all the activities of the system okay controlling is going to master all the activities of the system finally when it comes to the alu the aru is going to perform either an arithmetic operation or a logical operation okay as you know right the resources side is going to pro provide a basic storage provision within the cpu every time when you execute instruction that instruction with an associated data should be residing within the cpu okay yeah now let us concentrate on designing a register search [Music] oh yeah let us start with this register set design okay now stay focused so far we talked about some fundamentals how the program is going to get residing in the computer system and find what is going to get executed and now we will focus on the register set design stay focused here let us not get into the details already unless we are completely comfortable with the things what we have already that is what we learned so far is needed to be completely understood here we are no more an end user here we are simply the designers you can't think from the point of view of an end user at all you need to think from the point of view of a designer only okay so for that reason you never think from the point of view of a designer of a end user or a program we are designing the system as we know that cpu design is involved in register design and control unit design and an alu design we will start with this registered design okay now when you talk about register design in what way we need to design simple the registers let us stay focused yeah there might be a question asked when i talk about registers sir what is the register please let me please let me know some people they might they may ask sir what is a register okay if you want to understand a register first of all you need to understand something called a flip flop if you want to understand about a register you should be able to understand about a flip flop a flip flop is a binary cell which can hold a 0 r1 one single flip flop can hold either a zero or a one at any point of period you know that the computer system is going is is simply is going to hold only a collection of zeros and ones okay eventually when it comes to the main memory are the internal architecture of the computer system it supports only two two digits two binary digits one is a zero and one is a one okay so if you want to store either a zero or one you do require a flip flop a flip flop can store a zero or a one at a time a single flip flop can hold only one digit it could be either a zero or one there are several flip flops okay there are several flip flops such as rsp flaw or you know jk flip flop or a d flip flop or a t flip flop i am not talking about the architecture of the flip phones i am just giving you an idea that we do have several kinds of flip flops depends on the application we can choose one of these flip flops if you can understand only the the basic flip flop rather the mother flip flop of all the flip flops is rs okay this can be considered as mother of all the flip flops because either it's a jk flip flop or a d flip flop or a t flip flop are just derived from rs okay forget about it when it comes to a flip flop it can store either zero or one at a time only one editor okay which means that if you want to store some numbers you do require a collection of flip flops okay now here exactly the definition of a register you have seen that is a register is a set of flip flops a register is a set of flip flops if you want to store an 8 bit number you require a collection of eight flip flops together you require a collection of eight flip flops together one two three four five six seven eight okay now you can clearly see this is a register okay eventually if you can understand a register is a collection of flip flops simple what we need to do is we need to design this register set okay so if you want to design a register set i can have three options i can either go for a single accumulator organization or a general registered organization or a stack organization but try to understand when you talk about a stack organization um altogether it's a different ball game it is completely different from single accumulated organization at a general basis to organization it might be too technical to discuss about the labels of single accumulator or general registrar stack organization already it is very early to discuss about this because these are when it comes to designing a register set the essential thing that you need to understand is the cpu should hold several registers at a time okay the way you need to organize is different i mean is can be of different types one is single accumulator okay in this way you can organize these registers second one is generally system in general list way also you can organize this register the third one is a stack organization this stack is common for every computer system okay let me tell you you do have an eight zero eight six at your home you do have eight zero eight five at your home you do have uh something called i7 at your home i5 i9 anything in every computer system there will be a stack okay stack is common either you go for a single accumulator organization or a general resist organization stack is common stack is completely common so what we do is while we design a register site first of all we'll take single accumulator organization into account we will design our register set with a single accumulator organization okay once a free understand a single accumulator organization then generalist organization stack organization will be a cakewalk okay then generalist organization and a stack organization is just a cakewalk yeah now i got live streaming to the chat so far it was off okay now um i hope you've got a complete idea about what we're going to design here initially we're going to design a single accumulator organization all right let us stay focused we are going to design a computer system with a single accumulator organization okay yeah let me design the single accumulator organization registers we are designing a register set the fan tab is not working properly in which we're going to take single accumulator organization okay so now we are going to design a single accumulator organization computer system okay we're going to understand several different registers which are available in single accumulator optimization okay the basic registers that are available in single accumulator organizations are as follows the first one is something called a pc which is called a program counter second thing is something called an address register and the third one is something called um an instruction register and the fourth one is something called a data register fifth one is something called is an accumulator okay this is an important one okay sixth one is something called a temporary register seventh one is something called an input register and eighth one is something called an output register okay these are the registers that are available in a single accumulator organization let us understand what are these registers available what are these registers up okay what is single accumulator oh sorry what is program counter address register instruction register data register accumulator in a temporary register input register and output register let us stay focused and understand okay let us see the purpose you do not think about any other thing other than the purpose of each and every register as of now okay let us have a look at program count when it comes to the program counter program counter is going to hold the address of the next instruction address of next instruction the program counter is going to hold the address of the next program counter is going to hold the address of the next instruction address register we will talk about address register after some time instruction register that instruction register is going to hold the current instruction which is getting executed okay the instruction the current instruction that is getting executed concurrently okay data register is going to hold the current data element that is let us say that when you execute an instruction you know that we do require an operand okay so at this point of period you need to understand that the data register is going to hold the offering of the current instruction okay we will understand clearly i am going to give you a clear picture in what way these instructions are going to hold the data what i am explaining you right now okay the accumulator what is this accumulator this accumulator is going to hold the output of every instruction the accumulator is going to hold the output of every instruction the temporary register the temporary register is going to hold the temporary data elements what is the temporary data element sometimes when you write a program there might be some temporary variables those variables when they are needed will be called into the temporary register okay so temporary register is going to hold some temporary variables whenever those are needed to execute within the program input register what is an input register when you type something by using a keyboard a character will get generated right when you type something by using a keyboard a character will get generated that generated character will get transferred to the input register okay so input register is going to hold the inputs that are generated by the input devices by the user and the output register what is the output register the output register is going to hold the outputs that are needed to be sent to the peripheral devices you got me already yes exactly what is output register when you want to give an output let us say you want to take a printer when you want to take a printout this printout must be a set of characters right when you want to take a printout some set of characters will have to get transferred to the printer right so all these characters initially one by one will get transferred to the output register from output register to the printer the characters will get transferred okay the thing that is once again the what you call all these regis the purpose of all these registers is when it comes to the program counter yeah here you can see that there are several registers in that you just identify something called an instruction register here this is an instruction which is you can clearly see this instruction register can also be called as a program counter which is going to hold the address of the next instruction sorry this this is program register program register sorry okay this program register is going to hold the address of the next instruction pc will have generally two names program counter or instruction pointer this could be called either as a program counter or an instruction point this is this cannot be called as a program register in this ppt it is given wrong but you need to understand this is a program count okay what is the program counter program counter is going to hold the address part of the next instruction i'm going to show it here okay here let us see this 23rd this 23rd address for the current instruction let us see okay cpu is currently executing the instruction which is at the 23rd address there is a current instruction okay which means that the cpu is executing 23rd instruction concurrently at that time the program counter is going to hold 24 which means that the program counter is going to hold address part of the next instruction all right that is exactly what the program counter is all about the program counter is going to call the address part of the next instruction okay i will talk about this address register later then there is something called instruction register this instruction register is going to hold the current instruction that is being executed concurrently that is what about the instruction register data register this data register is going to hold the memory output the data register is going to hold the memory operand the current instruction which is getting executed requires an operator that operand will be held by the data register okay accumulator what is the accumulator i told you people that accumulator is going to hold every output only you remember this not any other thing accumulator is going to hold the output of every instruction okay for an example i'll show it here okay i do have several hundred instruction let us see hundreds altogether my program has a set of hundred instructions okay when first instruction gets executed in this set of hundred instructions there is an accumulator in the cpu that will get update okay after executing the first instruction the probe the accumulator value is something x okay means that x is the output of the first instruction then after executing the second instruction this accumulator will get updated again because second instruction also gets executed so accumulator will get updated let us say after executing the second instruction the contents of the accumulator is y meaning after executing the first two instruction the partial of output of your program is y i hope you got the point exactly that means that every time when an instruction is getting executed the output of the instruction is going to be held within the accumulator okay so if you want to see a partial output at the end of any instruction you just watch the contents of the accumulator which will give you the partial output finally if you want to see the complete output of the program after executing all the instruction in in current example after executing the total 100 instructions you just watch the contents of the accumulator accumulator is going to provide you the final output once again what do you mean by cumulative accumulator is going to hold the partial output during the execution of any instruction but it is going to give you the final output after executing the final 100 instruction within your program okay so this is what the accumulator is all about okay yeah the next thing is as i told you that temporary resist when it comes to the temporary gesture it holds the temporary data elements okay and and we enhance more about the temporary registers once after we get a complete gist of all the registers okay because let me tell you something that is when you execute a program this program may require some temporary variables i'll tell you what it is you execute a c program or a java program but when you try to pass the parameters in a function to a function arguments to a function what is the scope of this temporary uh variables this temporary variable scope which are used to pass the parameters to a function are you know in which the scope is pretty much limited what is the scope the scope is just in and around just four to five lines right when the scope of a variable is pretty much confined to a localized area of a program this particular variable will be coming and residing in a temporary register such as parameters passing the parameters arguments to a function okay so all these temporary variables will come and reside in the temporary register whenever those are needed okay as you already know that input register the input register is going to hold the inputs from the input devices and the output register is going to hold the outputs which are going to be sent to the peripheral devices yeah now let us talk about an address register okay so far do we have any doubts let us have a some doubt session do you hand it out so far video is not visible i'm just working in a working it on this way yeah i think it is clear right now any doubt so far fine um i i think there are no doubts here fine if that is the case if that is the case let me go and talk about address register okay let me give you some fundamental idea about an address register okay there is cp okay and there is main memory okay now cpu where the programs are available the programs are just available in the the programs are just available in the main memory if you can stay focused the programs are just available in the main method right yeah but the cpo is going to get the instructions are data from main memory only for that cpu need to send an address to the memory in which the content can be accessed for example cpu want to access the content which is available in 20th address if you want to access the content which is available in 20th address okay so under 20th address let us say that there is a content see if you want to access the content which is available at 20th address for that cpu should send an address reference 20 to the main map for that reason cpu employs something called an address register this address register will be given the addresses that are needed to be located within the main memory okay every time when cpu wants to look at an address immediately that address will be transferred to the address register this address register is a mediator to locate the contents so sorry to look at the addresses within the main memory in this case cpu want to search for an address so if you want to search for an address 20. cpu will do it only by using the address register this address register is a mediator who takes the addresses from the cpu and goes to the main memory and finds out where this address is for an example address resist is like a postman okay he is going to collect the address and he is going to locate the exact location of this particular address in this case 20 20 is given to address register that address is going to hold 20 and he'll be able to look at 20 here so thereby he will be able to you know collect the data which is available in 20 so address register is the register which is essentially used to send the addresses and locate them within the main memory by the cpu whenever cpu wants to refer to an address immediately that address will be transferred to the address register this is what the address register is all about so now let us take let us understand all the registers uh in a nutshell as i already told you these are the registers the first one is the data register second one is an address register third one is an accumulator instruction the order might be different the order that i told you people is program counter address register instruction register data register accumulator temporary register input register and output register order might be different but the purpose of each and every register is clearly classified and separate for example program counter holds only the address part of the next instruction but when it comes to the address register address is going to hold any address that is needed to be located in the main memory when it comes to the instruction register instruction register is going to hold the current instruction which is going to get executed currently okay that is the data register data resist is going to hold the current data operand which is needed to execute the current instruction accumulator of course as you know that accumulator is going to hold the outputs okay temporary register the temporary registry is going to hold the temporary variables input register the input register is going to take the inputs whereas the output register is going to send the outputs to the peripheral devices okay that's it for this session um just i want to know any doubts you have so far regarding the session and sorry regarding the topic that is completed within the session about the registers within the single accumulator organization okay you're fine there is no doubt i am summarizing this me 
dlCi4ymj2Vs,22,"Handwritten Notes of Computer Organization & Architecture(COA)   by paying Rs 99/- at Paytm no. 97173 95658 and sending receipt of  payment to Whatsapp No. 97173 95658 or email id Lk9001@gmail.com.... 
You can also use UPI id    Lk9001@icici for payment


The IOP is similar to CPU except that it handles only the details of I/O processing. The IOP can fetch and execute its own instructions
#COA Question Bank #AKTU Exam #important questions #COA important questions

Handwritten Notes of Computer Organization & Architecture(COA)   by paying Rs 99/- at Paytm no. 97173 95658 and sending receipt of  payment to Whatsapp No. 97173 95658 or email id Lk9001@gmail.com.... 
You can also use UPI id    Lk9001@icici for payment

Welcome to LS Academy for Technical Education. 

Computer Organization  GATE  Lectures  Course FREE Playlist

https://www.youtube.com/playlist?list=PL_obO5Qb5QTHXgOwI4DdOdakQSOF0jImO

Automata  GATE  Lectures  Course FREE Playlist https://www.youtube.com/playlist?list=PL_obO5Qb5QTEihQ35PgzjZSh7PveVt-iF

Pushdown Automata(PDA)   GATE  Lectures  Course FREE Playlist

https://www.youtube.com/playlist?list=PL_obO5Qb5QTFVXM1l0CswlxvuqN4as0SQ

 Operating system  GATE Lectures FREE playlist : https://www.youtube.com/playlist?list=PL_obO5Qb5QTE9s7QCNjqk97Q4UUgOseWP

Microprocessor Lecture FREE Lecture Playlist : https://www.youtube.com/playlist?list=PL_obO5Qb5QTFJmsXRXw75",2019-11-29T14:08:28Z,Input output processor IOP |  IOP and DMA Communication | COA Lecture series,https://i.ytimg.com/vi/dlCi4ymj2Vs/hqdefault.jpg,LS Academy for Technical Education,PT17M38S,false,8817,237,3,0,21,hello friends in today's video we'll discuss about the input/output processor input odd professor hekia AutoCAD wonder you can use key other computer systems Kivar among little more discuss currently so subsequently skidoo basic concept of Skyrim discussed contain pod proposed circuit requirement Kalpana Hamada paused esse game nem will last two videos about their thinking modes of transport in method of thing first one is the programming board put second is interrupted in output and third one is the DMA OC key ugly step here in pot proposed lon yes again Namco program in wahpeton with a Thank You program and for put mikata cpu devices car continuously status check kinda Hamlet I am based cars are a thing the device reading you a tea cup the CPU data transfer an anchor there jovial gunner at can I have technique earth if the device lead in your party was simpler Bogatyr the other device is school status check innominate I am wished cursor at time to CPU performance degrade of tier ii yo' method predominant interpretive import put was to MacArthur TCP you jab you edited also kannada the device status love other technique eartha CP what equates will tell you my job courtesy job device radio cody apply straight of basicity that now i am ready for the data transfer the second method map Annika comparatively here advantage millet on a key devices course status CPU Barbour check Nankin Aperta your device code ready we have ta CPU code status bay city to intervene in for put my cpu becauseit I'm Matata your device is curl straight a chef can mCP waistcoat aha lakyn Abby CPU of involvement time for potato transform an interrupt event may become Joey were transfer over CPU key helps a Hoover taken status Joker device ready hey mousse q+ or transfer can you only focus key through only see puke involvement rahaga whose problem both organically hummanity Samantha do you see Arthur that is DMA direct memory access died memory access macabre seep union are all come Kalia see unique a grittier CP unique so how do you square DM $11 TK or ask okay they get up cause I do 30 via the obviate data transfer karahi so CPUs may get another DMA method may kiss EP you it collects a hardware you skirt that DMA controller DMA controller goes CPU of nice re buses key or xxx ahead or sadness if you cared or details kuiba the I up with Navarre transfer Kearney is a drastic and named CK Jovi is our information everything I read can i sorry information cpu CUDA DMA controller code AB DMA controller Gator data bus kyo 30 liquor in poor memory visual transfer core therethis ah audio transfer Kumble to add data to confirm got that the cpu go it's come on the cpu castle Kipner alta BMI method man DMA controller go bus key or 30 Dana or import phone instruction guy is equal to the V what Donna give is address to data transfer can man if they were trans mechanic or transformer read Kanaya right can I become a merese e-ticket yeah Nikki CPU kiss man world head emi method movie information instruction where is equal to capital of anime to keep neva down with kaha song Gary donkey ride donkey plus bus key or 30 we provide grant thanks Cody ma-kun of zero alko or come conically ugly method ayah in pure processor so input output processor is basically the advancement of the DMA method cpu can all go or come cardiogram you scurrying in poor professor - heck yeah in purpose sir may I may I think just computer my CPU CRA you saw such a core those people use car they discuss aversive come kawasaki GTP improbable transfer say one code Elkin Yanni DMA jobs may use key all those case our southern Jacare a galaxy dedicated processor chris kelly used car a in pure productivity is killing horas processor combination on the input processor so what is the improper processor an inverter posture is a CPU that is specially dedicated to perform only the input/output transfers although it can also perform the all function there the cpu function performs Yanni eh I have a requirement oh so he be sorry automatic logic operation concept I miss our instruction guys good cursor tie your knee completely similar a cascade CP ok leg in how many especially kiss can use ki in for Pretoria later operations can be disagrees you up come man CP where you master CP where you scope with problem now go up now or conquer city ck to importer processor may be up god xep use chaotic or induce recipe were scared yup a hell of a lot CPUs your men CPUs come get the master CPU or joe in for Peter to typically use care gives coming again slave CPU PK to the input/output was hilariously of CPU way yeah because could report guru yoga um girl kappa man cpu cojito port Karenga ck to see puker all is no or only minimum gears compared to the DMA method ok secure Husqvarna participant through Los Angeles hours give around a clear thing is really cover the input or purpose earth is similar to the CPU except that it is especially designed to handle only the input output processing CK up the key both CPU an improper posture exists in the system the only exist cotton system a lake in CPU is the master IOP is the slave okay abortion out they can see puke after all gay as compared to the mmm I thought ok I'm gonna give out the output DMA cahier does maintain proper posture DMM a CPU emerges again with Agatha Sarah busses car transfer the data DMA Co or Saudis of my org a rata whoa input on restriction well in property related instruction was equal to RK d MMO Yozora sake of calm memory case address a really radical 9 PK Ethne vodka name transfer or real gun I arrived on line to become be Omni Giardia Finnish cardiac is co CP Yuka CPU is master of Cayuga Java in purpose a used car obey to CPU Cassell for silk is not all negiah CB UK naranja way in pururava circa here Madonna kya instructions for every memory may infer odd person related a Bunco the key memory my arm guy is equal to career jump come finish Rosa then informed to the me yogi CPU man CPU okay master him to CPU car also kept me in the game today input on purpose ergo here with on Mak main memory may up is a dressy input/output instructions go fetch Kirk execute can line buggy sagas Aragon Wonka Rica in purple sir Corina Jeff come cut ammonia Gazzara kazar any buses Caillou's Kanaka hospitals or connect with mechanics of couch com1 gotta go input/output processor lake in the sarah company show yoga to improper posture big screen from forgive diagur - or a transfer curly a CPU cooler main CPU - CPU just initiated gotta process Co buggies are a compound a tie import proposition the why chiefly QE the CPU only initiate the input/output program yeah initiative I you people you Matata et Harper never remains water program yeah input on instructions per day 1 CK is keyboard input output power gather their own input or instruction well secured gotta independent from the CPU up NEBOSH target is the CPU company will not love nahi rehna CPUs will cover all mega job IBM knock on finished car key was in from Kanagawa CPU CK the inverted was a gurgle karaoke in purple processor memory main put operation sections per day way olga fetchick Arreaga or go execute Garriga take a memory say fetch key inform instruction as cute key as he could go necessary pathology Mooji read Kanaya right connect kidney word read read the naked never transfer Ghanaian Iggy Oh sod my memory Kiki's address second name you saw a compiler concur rather DMLs cpu Karratha works aragon khan kanika input/output processor caraga ck so the iup fetches and executes the input auto instructions to facilitate input/output transferred if required it can perform all the functions of cpu our journey to suppose Oregon September similar problem way up here to see you up Cocker time seek a tour of the gimmick diagram the Kiowa get the car again proposed circus again just care ecosystem digging up commenced EP where new masters if you were obviously of CPI just come back at the i/o Piketty domain CPA or i/o PCP where you can memory bus go through you don't know me the only memory bus Keith Lucas second hundred mm buddy second it today yeah any job being coop was no good communication can now so you could regarding America through cutting a communication ticket Donna browser master CPU or slave IUP both communicated to each other through the memory input/output processor CAG are the imposter dedicatedly Kiska leaves K at the peripheral devices any key input output device is killing us kita score become me and you should have dedicated like a skinny a CPU carlota mechanical again for because OV e commerce or a Gazzara of kuru this quake alexa posny degree in port would buzz k improbable was Caillou's kirk a gerbil transfer can my input device in memory me a block or theories cebu of Methuselah gambler tÃ¡rrega job IOP of Nakamura vallega though most of Calgary got CPU coup in from car they think this is this is the basic role of the input/output processor ugly figure man diagram the high gas CPU or IUP okay say I do say so communicate car thing any CPU Cacioppo information I because any Oggy yeah IUP Coco every information CPU cousin yogi to Vogue is steady Casey communicate critiquing cookie examination Mecca Mecca be the why you pee attire in communication will put here there how information is exchanged between the CPU and import purpose to Jessica Mauboy subset of attack is sub sepilok of the jab be CPU in Porto Cervo communicate karna hoga CPUs of sepilok agar-agar CPU subsequently chat cariocas coup in purpose circle catch Ikaruga k input/output posture PCRF to tease em busy tony catch Ikaruga key if what is the status of the poor purpose Kellogg already in barbourville circusy comma busy a total comlink arrival semi-great Garriga otherwise agar molly jean' portrait was are free here the wasps who come a lot karna the way after you are become Korea otherwise she puke agar agar apni could handle carry cows come to see puke aggregates of sapling cpu send the instruction to teach the IOP path is comic legacy piece of symbolic instruction by jaga check Kinnikinnick is Christ Rita's input process circa chaotic in poor posture free hey yen portrait was so busy in part with roster Garriga jovial sky still does yoga other free hair toe or our IOP busy hello jewelry still does yoga Rubisco memory ski through memory locations K through cure Kobe I'm now about the other the CP or IO be I wasn't guessing I'm gonna get cut their name buddy get through to a memory you through in Boracay status montague iike busy or free oh ye of my status base the time think a goddess Thetis okay hey if it still tsuki Kamala a girl in purple purse or free him Jenny Prosecco common hing karai - you gotta send a start input-output instruction to IO to see bouquet of Auriga yo squad again quadruple circle K by memory me is he starred in a starting address important instructions above knee up squares it would carry a operation gargamuni bus finally indicate cars in nevada agar in purpose or free hair though starting address in property instructions curve this curve is the IOP KO panyi you come da memo conga rhythm CPU code is Araceli guinea I was near come disco dandiya in verbal instructions come e io p Cooper Sivaji acapella addressee and purple instructions come DK o ru p kaveri go jaha business starting address with iron parappa instructions car whose memory location go XS k raiga kiss purpose killing the hopper in property instructions for the only in four blocks on your body unless Caserta kunkka was there input/output processor program struction program ticket the memory access Careca in proper instructions cause you're gonna take a DM ticket instruction square is equal to girl gets cookie about Alaia Kuya pathi lega memory sick if never transfer can name a I starting address of memory were transfer can nine or read Kanaya right Kenai you compiled a CPU DMA Kopitar a house method one you can hop or IOP could Cara CK Leggett I obey because calculate at the ma kettle player any DMA be are use cursive kiss-kiss odd in quadruple circus earth the second part of a circle would become right thinking to conduct input/output transfer using DMA instruction guys we go to Gary Guardian about dodging about yeah so can i a second life using DMA or job transfer customers I go to a get status it will prepare curly okay buddy transfer Buddha warning 1 CK JC transfer complete ogre in Porto Corsa Jessie he input commitment your widget they were transfer Conejo transfer come political lega would be a very go see picot interrupt Caraga see if you focusing on turbo car Iike you both on acrylic main Omnicom finish Courtney I do we have never in Commodore theater Cebu and Turkey is beat my CP Yukio gerado see Fiona da ba da da da gay ye starting address in proper instruction ska whose beat us abusive kiss me involved our CP involved with another program to become con the greatest beach man in powerful processor CPU loosely programmable 0 via the CPU got all the Baraka bygones Joey comp Oracle lega or CPU go and drop karega yen from gonna kill a chimera transfer complete Kalia so JC is current up tiger cpu go in term of the cpu go disturb car number Donna kilig when I'm gonna come Cordia thrown CPU Kaveri go input output was circa status manager status means ugly I'm gonna come kardia here yeah we are busy ho thinking I'll get into output cross circle Jesse's initiative long is new priced at us remember to get through base your CPU core cable is here can now I am free Jana key man I'm gonna camp Oracle yeah now I am free CK top 9 still does its name memory locations give the Lucas Cobain's dear CPU go see booster - a curry go harmony free yoga is come we did a transfer the was successfully I hope in a complete Katya so check is traitors word any Jovi's consider stuff which ad carry gossip you for the correct transfer kaha wait also successfully conduct okay take a dose give a CPU Ibiza to overall gamma gamma logic is CPU nature of cocky I hope evilly process no it's put him a third man Serio Czechia in portable circuits till dusk every year busy a girl free hand Porthos coveted Iggy hopper memory may inform instructions for the way above my Ivana is he could create of Cambodian from karate yoga in barber posture instruction square secured quarter DMA consider a transfer kata or transfer completed only give all CPU could interpret kata yin foam connect le chemin optic um Katia because at the end CPU is the master CPU Go button on the rule okay Shibuya see pathology AHA my interrupt is CPUs question - record time very status scope ml day is kamala already in power transfer successfully computer it's come at libecki overall your name what purpose are you skier who cpu core all or minimize Carnegie Lee hum knee IUP introduced via a special processor you can eradicate already Omni in power put related transfers Kelly okay though you secure Laurel CPU or you know CP romance if you skip performance or announcer Ju scoop a load or Camus Jenga you see what does your company come your functions perform concept a CPU ticket so overall I hope ego use kinase a system keep of Orleans or improving so this is all about the imported water game powerful processor hey Jia was Kirk are all have a kiss that he gives a CPU are in purpose area those racing communicate growth in during the import put transfers there it's all about the IOP I hope of Lanka sahaja yoga if you like this video currently share and subscribe thank 
shqAvwagW78,27,"This video is a lecture for Chapter 2 Part 1: Number Systems for DCN1013/BCN1043 Computer Architecture and Organization. You can gain access to more information in our google classroom account.


For Universiti Malaysia Pahang Students Enrolled in this class (BCN1043/DCN1013). Please complete the google form in the link below for confirmation of your attendance:
https://forms.gle/6pMcYXRLLpHMnGjx9

 You can download the google classroom app at: 
https://play.google.com/store/apps/de...
or
Open it directly in your browser at: 
https://edu.google.com/products/class...",2020-11-03T09:04:57Z,DCN1013/BCN1043-Chap.2-1 Computer Architecture & Organization- Chapter 2-Part 1: Number Systems,https://i.ytimg.com/vi/shqAvwagW78/hqdefault.jpg,Syafiq F.K. Academia & Solutions,PT27M26S,false,663,44,0,0,0,very good afternoon to all students okay today we will meet again for another chapter under computer architecture and organization so last week we have been discussing about the computer system and the components inside the computer so this week we will discuss about the number systems of the computer okay which is the language that has been used by the computer itself so thank you for the support for all my videos for those who subscribe to my channel hopefully this will benefit you all in your studies so for those who haven't subscribed my channel please do subscribe and i appreciate if you can drop a few likes on my videos okay so let's start with our class for today so this week chapter is about the number system so basically what is a number system so number system is a system that we use to symbolize or to communicate of certain values in our life like for example if you have numbers uh for example quantity of certain things in order to communicate about those quantity efficiently we use this number system however there is a lot of symbol involving number system and depending on the civilization involve creating the number system they are different in representation so for example in egyptian number system okay you show here left you can see at the beginning it's almost countable and when it's got more than 10 okay you see that it becomes another symbol entirely so this one is not a representative of a good number system that we can use these days another example is this one which is by the civilization of roti so you can see that we have different kind of representation of numbers okay we did a number system so number system is useful when we want to use it to quantify things however when the things got too many okay when you want to do a calculation with it a proper number system is required okay you cannot do a calculation with a people subtract with the frog so this kind of number system is difficult for us to do calculation with so with our current number system that we have which is the decimal number system okay so decimal number system is based on the arabic letters okay we see the roman letters you see the number system of nomination is also difficult for us to to calculate so what we use right now is based on the arabic number system which is the decimal so for computers we have four types of number system that we use exactly however basically the one that being processed by our computer will go up until uh we'll only use binary however on top of it user in assembly language or even in our user define function we can go up until decimal based on the compiler itself so basically if the four number system that we are using right now is base 10 which is decimal base 2 is which is binary base 16 which is hexadecimal and base 8 which is octal so what does it mean by base okay if you based on 10 means that there are 10 digits in the numbers which is start from 0 until 9. so when it finish okay you will increment to the left and start with 1 and on the right will remain back to 0 which is 10 okay and then it will increment again again again until 99 and then when it goes into 99 okay we'll increment one on the left and the nine will revert back to zero and in the other right we refer back to zero it become one hundred okay so the same concept is being used for binary hexadecimal optimal as well so binary instead of having 10 digits we only have two digits which is 0 and 1. so firstly what happened okay when you have zero and you have one when you want to go to two okay the one will revert back to zero and it will increment on the left okay as one okay so that's why we have 10 in binary is 2 in decimal okay for hexadecimal we have 16 digits which is from 0 until f so what does it mean by zero until f so you have zero until nine and then continue with a b c d e f okay so that's why you can see that the representation of hexadecimal includes alphabet as well okay so after f f after f it will revert back to 10 and then it goes to 11 12 until one f and then go back to 20 and continue on until ff okay and then go back to 100. so if you convert them into decimal will be entirely different numbers than uh in hexadecimal so usually hexadecimal is used to represent large decimal number okay or large binary number okay in our computer so and then the final one is octal so octal is basically based on eight digits which is from zero until seven so after seven we will go until after seven it will be 10 okay after 17 it will become 20 okay so this is how it works in the computer so again okay so what we use every day is the decimal number system what we call decimal recording is based on 10 and it's multiplied by 10 so here in this case 4728 you got 4 times 10 power of 3 plus 7 times 10 power of 2 plus 2 times 10 of 1 and 8 plus 8 times plus 8 times 10 power of 0 so this totally here you got 4728 so here in case you have a bigger number okay so you have the the prefix using a certain symbol to describe the number whether it's too large or too low like for example you have tera to describe 10 billion you have giga to describe 10 million okay you have mega i'm sorry giga to get 10 billion okay so omega to describe 10 million okay and kilo to describe 10 000 okay so here you have including up until 3 until -12 okay melee micro nano pico so this is based on the radius of 10 so when we look at binary instead of 10 we are talking about two so if you say that it's a 100 in binary means that 1 times 10 power of 2 y because 10 power of 2 here because it is in the location number 2. so here is location 0 location 1 location 2 so 1 times 10 power of 2 0 times 10 power of 1 and 0 times 10 power of 0 so total of this is 4 okay in this case here the total of it is 43 okay it goes up until 2 power 5 which is at this location which is the sixth location is 2 power 5. so binary is basically very difficult to human okay because you need to read all the digits in order to understand and there are a lot of digits okay so we are usually more comfortable with numbers since we have 10 fingers with decimal number system we have 10 fingers so it's easy for us to calculate using decimal however convex conversion between binary and decimal okay so they are complex however it's not that difficult for us to do okay but in case we want to do it fast okay it's very very time consuming and frustrating for you to conduct it uh properly so in this case if you see okay we have one one zero one 1 so here we have okay so here is 1 times 2 power 0 2 power 4 to the power of 2 to the power of 3 to power 4 to power 5 okay so here total of this you can get 55. so when we talk about the binary system we also have representation using prefix which is on the giga mega kilo okay so the power of 2 usually in representing kilo you have 2 power of 10 which is the total value is 1024 2 power 20 means what 100 1 48 057 this is mega and top of 30 is giga so that's why when you buy hard days like for example you buy a hard disk of one gigabyte okay actually it doesn't mean that it has only one billion uh one billion but it has more than this because it is calculated based on the radix of two okay the bonus is calculated based on radix for two okay so then we have hexadecimal so hexadecimal is basically for us to simplify the binary representation so when you have four binaries we group them and represent them by using hexadecimal so you have four binaries here if representing zero it becomes zero one become one okay six becomes six seven becomes seven and up until f f is one one one all ones become f okay all zeros is zero so from the right you increment one and then go back to zero increment one go back to one okay and the increment uh doesn't increment here one one and then when you create another one zero zero one okay okay and then zero one zero zero one zero 1 okay so here we start from the right side so here one okay and then when you want to increase it it becomes 10 and then next you increase it it decreases by one okay here and then when you increase these two will become 100 so you will increase up until f so this is the maximum hexadecimal system so basically since it is represented by 16 hexadecimal digit so this is what we call a base 16 number system so means that if you want to convert them into into decimal you have two times with 16 power of something based on the location so in case of here 2c okay you need to uh what you need to do is that you have 2 times 16 power of 1 and c times 16 power of 0 c in decimal is 12 so you have 12 times 16 power of 0 here so the total of this is 44 in decimal okay so it's easier to convert between binary and hexadecimal okay so binary to hexadecimal next decimal to banner is easy by using uh converting only four digit by four digits so if you have four binary you can convert it immediately using this formula okay for example you have zero zero zero one zero zero zero zero so immediately you can know that it's a zero at eight okay or zero one one one one one one one one one okay so admittedly you know that it says seven f okay so it's easier for you to convert using uh binary to hexadecimal okay so basically if we talk about a decimal okay you have 0 until 15 this is where hexadecimal is represented from 0 until f so you have 16 digits here okay so the binary of this is basically an increment of four digits binary okay up until from zero zero zero zero until one one one one okay in terms of a fraction representation okay so previously before this okay i show you how we convert using fixed point representation means that we start with two power of zero okay so for floating point representation we doesn't start with the two power of zero we start with which decimal point it's uh it starts with in this case if you want to convert from decimal into binary okay binary how do you how do i convert them okay in this case like for example we start with 2 power of negative 3 here okay and then 2 power negative 1 to the power 0 to the power of 1 and up until 2 power of 4 okay so here in this case okay what we do is that okay we convert them okay in this case we have this binary here supposedly there are a point here okay there's a point in the in in this line okay this line we have a point so here in this case you have 2 power 4 in case of 16 okay we want to convert this value actually okay so you have in 16 you have one okay and then two you got one okay one here and then next is two products 0.5 and 0.125 so the total of here is nineteen six point six two five so the binary is one zero zero one one zero zero one one dot point one zero one here okay so this is basically how you represent fraction so let's take a look okay in terms of fraction usually we are represented based on the data types okay so some data types okay in programming you have integer data types float data tab so data types is uh is used to represent the bits for representing those numbers so in this case okay for floating point representation here we have 32 bits where 8 bits will represent okay uh the exponent here so means that 14 15 16 is represented under this okay uh in the front and then the significance here is in the front okay uh significant here is basically the one in the front here so the mantissa here is the significant exponent here is the bias exponent so if you say that 1.59 means that here is basically they will change it into one five nine so one five nine okay and then the bias here is fourteen okay y is fourteen so the sign okay sign here is the negative whether it is positive or negative so in this case here we have it representing this sign where is negative or positive so other than numbers we use the ascii code okay here to represent the binary as well so in case you are typing characters based on your keyboard okay like for example you have alphabet in your keyboard so your keyboard will push out okay this uh the binary that is being represented by this hexadecimal here so in case of 20 means that is zero zero one zero zero zero zero zero okay so this is representing the alphabets okay so you have this ascii table if you have a certain binary you can convert those binary into uh alphabets okay based on this ascii table okay so you have other than ascii like for example unicode okay so all of them based on certain standards of design for us to communicate efficiently using the computer for based on the binary itself so the binary also representing colors okay so previously we have 256 colors so how many bits they're representing 256 colors so 2 power of x so x is basically 7 if you have 2 power of 7 bits okay listen you have seven bits okay so you can have 256 options of colors okay so now these days we have 24 bit color so means that you have two power of 24 options of colors available in your pc so how many bits okay it's a lot more than 16 million possible colors with 24 bits okay so next okay i would like to teach you how to convert from those number system so firstly okay let's take a look at binary to decimal how do you convert from binary to decimal so basically okay the way you convert from binary to decimal is basically the numbers times two power of the number location okay start from the right is location 0 location 1 location 2 location 3 location 4 location 5 here so start from the right okay 0 until 5 so calculate 1 times 2 power 5 plus 0 times 2 power 4 plus 1 times 2 power of 3 plus 0 times the power of 2 up until 1 times 2 power of 0 so the total of this values this equation is 41 so you got your decimal already here okay so when you want to convert from decimal to binary okay we do a divided remainder method okay here so how do you do it in case here we have 26 decimal one to convert to base 2 okay so first we do 26 divided by 2 is 13 so there are no remainder so it becomes 0 and then we pull down okay 13 and then 13 divided by 2 is 6 with remainder 1 so here the remaining 1 becomes the next number and then 6 divided by 2 we got 3 remainder 0 so in case case is in this case 0 and then 3 divided by 2 is 1 we remain the 1 here and 1 divided by 2 is 0 with remainder 1 so your binary that you get for 26 decimal is one one zero one zero which is this is the binary one one zero one zero so in case of floating point number okay how do you calculate you convert them into binary so in this case floating point number when you want to do it you need to start with zero point something zero point something so here okay for for real number here in this case okay you start from the right but the floating point number you start from the left so here in this case zero point eight seven five times two you got 1.75 so the one here will become the remainder so one you pull out and put it in the front here and then you push down the 0.75 so 0.75 times 2 we got you got 1.5 okay pull out the one the next one and then you push down the 0.5 okay and then you times two you go one okay push out the one and pull down the zero and at the end you got zero which is the end of the binary so the binary in this case you got is zero dot one one one zero okay so when you want to convert from hexadecimal to decimal okay basically you need to do the same thing like the binary but in however since it's a base 16 okay you need to use the power of 16. so in this case you have a3f 16 you want to convert to decimal so a times 16 power of 2 plus 3 times 16 power of 1 plus f times 16 power of 0 so a is basically 10 f is basically 15 and you total them you got 2 623 okay and when you want to convert from decimal to hexadecimal okay you just divide it by 16 okay so here the same thing we use the remainder method so 425 divided by 16 is equal to 26 with remainder nine okay 26 will remain at nine so nine will be pulled out and put at the back so then the the 26 will be pushed down and divided again with 16 which is equal one where the remainder is 10 which is equal to a so then you pull out the a for the next digit and then the last one one okay you push down one divided by 16 equals zero okay which is you have the remainder one and you pull out one as the last digit in the hexadecimal okay so finally how to convert between hexadecimal and binary so this one is easy if you have a hexadecimal okay so you have one one zero a d e i d e one sorry so d okay in hexadecimal you got one one zero one e is one one one zero and one is zero zero zero one so you combine all of them you got the binary and when you want to reverse from binary to hexadecimal here you just divided four digits and you convert the four digits into the hexadecimal separately in this case you got 9 and 1 and here you got 91 in hexadecimal okay so next is we have octal okay so after for you to conduct octal basically you have to do binary first so when you convert to binary okay so actually similar in this case convert to binary you got this number and when you got this binary you separate it to three digits each okay hexa symbol using four by three digits but after you separate using three so in this case when you separate using three you got one zero zero is four in binary zero zero one is one in binary one zero zero is four in binary one one is seven in binary and zero zero one is 1 in binary so you got okay so you got here 1 7 4 1 4 is an octa okay so 1 7 4 1 4 base 8 is the value of this octal number so when you want to convert it back to binary you just convert this into three digit binary and combine them again and you've got the full binary and you can do you want to convert using into hexadecimal or decimal is up to you okay so my advice when you want to convert number system basically the easiest way okay if you want to convert any number system to decimal convert it to binary first and then convert the binary into decimal so in this case okay you will make your calculation easier okay when you want to convert decimal into certain number system methods of octal or hexadecimal convert them into binary first and then from the binary separate those binary to become hexadecimal and octal okay so [Music] okay that's all for the first half of the number system chapter thank you very much for paying attention to what i'm teaching so i hope i can see you again on my next video so stay tuned i will upload my second part of the video soon so for those who subscribe and like my channel thank you very much for those who haven't i would appreciate your subscription and your likes so i see you guys next time thank you here daughter shelby from zika rosamund represent presenting from university malaysia so stay safe stay healthy and take care bye bye 
YinsPbgr_mM,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2018 (https://safari.ethz.ch/architecture/fall2018)
Lecture 4b: Main Memory Trends and Importance
Lecturer: Professor Onur Mutlu (http://people.inf.ethz.ch/omutlu)
Date: September 27, 2018
Slides (pptx): https://safari.ethz.ch/architecture/fall2018/lib/exe/fetch.php?media=onur-comparch-fall2018-lecture4b-mainmemoryanddramfundamentals-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2018/lib/exe/fetch.php?media=onur-comparch-fall2018-lecture4b-mainmemoryanddramfundamentals-afterlecture.pdf",2018-09-29T20:33:17Z,"Computer Architecture - Lecture 4b: Main Memory Trends and Importance (ETH ZÃ¼rich, Fall 2018)",https://i.ytimg.com/vi/YinsPbgr_mM/hqdefault.jpg,Onur Mutlu Lectures,PT29M23S,false,736,11,0,0,2,okay let's start main memory a little bit we're not gonna finish it clearly but it's good to start so what I'll do I think I'll let the leave you early as opposed to taking a break is that good since we took a late break maybe we can finish a 20 past so 25 minutes okay just shout at me if I'm going over because I'm not I don't always communicate with that device that doesn't tell me although I guess we could set an alarm I guess you're my alarm now okay we're going to talk about main memory we're gonna switch gears a little bit I'm caches are very important of course but we also want to look at something really important maybe even more important going forward main memory and yeah and this is what I intend to cover clearly we're not going to cover it in the 25 minutes or you a broad perspective we'll talk about yeah fundamentals and operation we'll talk a little bit about memory controllers and then we'll go into latency energy issues in the UN and memory so main memory store I first some readings I guess this is recommended reading we wrote this paper with my PhD student blob Anya in 2014 or so we eat updated basically but there's a lot of the things that are in this paper are still really important except some of the more recent work is not included in that paper I can take a look at it it basically covers a lot of ground on the memory systems and the research problems and opportunities and there are some recent papers that I'm going to flash through these are references for you if you're interested this is very recent actually talks about processing in memory which we're going to talk of err well I already said that and we're gonna I think I already mentioned some of these saw and this is a relatively old paper but it's a short paper that basically provides a position on how Mamie should scale and what kind of approaches we should take going forward know and I think I mentioned this also but let's see if we have time to cover so let's take Christ it's a fascinating area also so there are some tools also if you're interested there are a lot of memory system simulators that we have in the group we may have actually some assignments related to this but maybe not this year but if you're interested you can take a look okay so these are some required readings actually you're going to read some of these I think if you want to learn more about um organization operation basics I would strongly recommend that you look at especially these earlier sections of these papers because these sections of these papers are I think very clearly described how DM operates and it's very concise so you can quickly get an idea of how DM operates and we were discussed the DM refresh basics and we're going to cover also a simulation at some point in these lectures like how do you evaluate a future future memory memory systems and we're going to talk about simulators and different granularities and levels of simulation you're doing RTL simulation for example it's not the best way of exploring the design space but it's important at some point to get your feet on the ground right if you always do high level simulation you may fool yourself we're going to talk about that okay but let's start with why is memory so important especially today because today it's extremely important and everybody knows that we're constrained by memory and everybody is trying to minimize the memory accesses because they know that when you go out to memory it's not good but people's hands are relatively tied because you may not be always able to minimize the memory accesses and that's why it's becoming even more important because computation is pushing the boundaries I'll give you the performance perspective this is alpha 21 to 64 we discussed just now right that's the that's the machine that implements virtual not reporting and also the physical copies physical cache copies or the register file yeah now physically register file copies so dick Seitz was the chief architect of that alpha and in microprocessor report actually we can put up the link to that article on the website he wrote a paper after this processor was released he said basically it's the memory stupid in this paper basically discusses he essentially says we designed this processor such that it could finish four instructions per cycle it's one of the fastest processors of its time actually it was faster than Intel's processors especially in terms of clock cycle 500 megahertz it's supposed to finish four instructions per cycle and then they measure performance they figure out its finishing one instruction every four cycles basically it's operating at 1/16 of its peak bandwidth right why well it's waiting for data to come back from memory and it's at a very aggressive cache hierarchy at the time you can read this and there is also a paper by Rick Kessler that talks about the alpha 21 to 64 micro architecture in actually micro in 1999 we should put that up also that's a beautiful description of the micro architecture and they put a lot of effort into the memory system yet they still get 1/16 of the performance ok that's 1996 this is my PhD thesis which we'll talk about I proposed ran ahead execution which is a way of tolerating memory latencies and our motivation was basically this this is a study that we did on simulation using Intel workloads basically the work caused many work goes that Intel used to design as processors at the time very varied workloads all the way from CPU intensive to and a web intensive and if you look at that those workloads and analyze the performance most of the time the processor is waiting for memory this is 55% of the time you're stalled waiting for an l2 cache miss to be serviced and you're doing some work making progress basically retiring instructions during this time ok not a lot right most of the time you're waiting for me ok and this is the paper if you're interested oh I don't tell that slide but there's another slide that's supposed to be here fast-forward ok I'm going to describe the slide fast for we're going to look at it later on actually but somehow it's not here that was my thesis was around 2006 okay fast forward 2016 let's say Google publishes this paper in 2015 on their workloads and data center and it's a beautiful paper it describes the bottlenecks in their data center and they look at how much time the processor is spending doing what and they find that more than 50% of the time the waiting for me in their workloads in fact their workloads are even more intensive and about 10% of the time the processor is retiring instructions doing useful work the rest of the time there are delays that are indirectly caused by memory and some other things so that's a span of just 20 years right and nothing has changed the clusters are still waiting for memory and I can guarantee you that if somebody does that study five years from now if something doesn't change the processor would be waiting for memory and maybe more maybe a lot more so during that time processors have clearly improved but we're putting we're wasting a lot of performance potential because we're not doing anything during that time we're just waiting for memory okay so that's the performance perfect that's why memory is important this is empirical data from many many different folks let's look at the energy perspective also so this is a slide from Bill Daley from scipy to keynote and as you can see over here he basically shows what is the energy cost of doing different things inside a chip not everything but some interesting things I'll point out a couple of things this is a 64-bit double-precision floating-point operation it consumes 20 Pico joules this is a dram read or write it consumes 16 nano joules and the difference between them is 800 X I cook the liberty of rounding up a thousand X because our system where this is a thousand X their system array this is lower also but it's basically two to three orders of magnitude difference later we'll talk about some other studies that measure this and if you optimize that you get maybe 120 X or so which is still two orders of magnitude difference so basically this operation it's actually complicated double precision floating point it consumes very little energy because it's on chip whereas when you're doing your memory access you're exercising those huge interconnects and they're consuming a lot of energy and there's a lot of energy also consumed inside the DRAM chip and they're also energy consumed because of the conversion going from on chipped off chip as well as a result you get this 100 mm X difference in terms of energy and if you look over here there's some other interesting later going from one part of the chip to another part of the chip for a large chip as you can see four hundred millimeter square is about one manager still about 16x difference right in that case one order of magnitude so going off chip is not good okay so you can argue with all of these numbers of course and people try to improve all of these numbers but this is improving and this is also improving but the relative difference is remaining the same so it's good to ask the question right now looking at this figure assuming you don't have a whole lot of locality in your system does it make sense to actually if you're doing it to a floating-point operation does it make sense to bring two items that are located in different place in memory do the operation and write it back to some other location in memory you're doing Mesa 3d and accesses to do very simple mouths relatively simple today in terms of energy computation it may not make sense right you're basically exercising this huge disparity in terms of energy okay so that's one of the motivators for doing something else as opposed to processing here maybe we should be processing some close to the memory such that we don't actually incur this huge energy cost rate okay so a reliability perspective another issue so basically energy of memory is dominating a lot of things I'll say is that one more thing over here this actually didn't used to be the case if you go back 70 years ago or so this was actually much more complicated than a memory access it's hard to get exact numbers but the cost of a complicated operation was about two orders of magnitude worse than a memory access at that time since logic has improved quite a bit and we know exactly how to do these things and Moore's Law has enabled a lot of integration of course so we've improved this parked computation part quite a bit but over the course of seventy years or so the interconnect and the memory part didn't improve as much interconnect certainly didn't scale very well that's why interconnect even on chip interconnect as a rel to a large bottleneck but off chip interconnect is a huge bottleneck okay let's look at the reliability perspective so energy and performance of memory are limiting our us today but reliable T is also very important as we discussed with Rho Hammer I'm not going to talk about draw hammer in more detail but these scaling issues that we have with memory is affecting our systems and this is what I mentioned to you earlier also basically they're reliable to your memory is reducing because things are getting too close each other and this is causing errors in the systems and we've already discussed this as you know earlier okay so let me cover some trends challenge than opportunities in main memory in the remaining amount of time so we're going to focus a lot on this part of the system main memory and you need some sort of working storage in whatever you design of course it's huge in service right it's huge in supercomputers actually it's important in mobile systems a good chunk of the system is memory but it's also true for many of the smaller systems well I guess desktop is not smaller but it's a small how many of you have desktops today okay that's more than I expected how many of you don't don't have desktops and just use your laptop okay that's increasing if I ask this question thirty years ago maybe that the answer would be different right how many of you don't even have laptops okay that's good you don't do your assignments on iPhone or Android phone right okay basically my memory exists everywhere and it must scale in many dimensions like size capacity technology efficiency cost and the algorithms we use managed to maintain the performance cost and the scaling benefits that we need from the systems and it doesn't today we're attaching many many things right to main memory processors FPGA is GPUs and heterogeneous combination of all of those and some accelerators that I don't even put over here video accelerators for example they all need memory and they all contend for memory also this is my cartoonish picture showing the importance of memory basically if you look at the scarred finished picture almost everything is dedicated to data movement and storage almost everything is the memory system basically just the red parts the core parts are dedicated to computation everything else is moving and storing data or controlling the moment and storing of data caches other caches memory controllers interconnects memory interconnects and even bigger data stores right basically the takeaway is most of the system is dedicated to storing and moving data and we've seen in the lab in the previous lectures that we built these hierarchies such that we don't actually go far away to get the data and but but the design choices we made actually result in a system that looks like this and if you quantify this actually more than definitely more than eighty percent and more than 90 percent of the system is dedicated to structures that just move and store data even if you look at the course there's an l1 cache over here which is not depicted there's a register file there's a lot of interconnect to move the data from there right so most of the system computation is very little actually even in GPUs today GPUs are some of the one of the engines that actually has the heaviest computational power per area today but even there most of the parts most of the areas dedicated to caches interconnects memory other caches and memory controllers okay so I'll give you a couple of trends in the next few slides basically there's some recent technology architecture and application trends that lead to some new requirements from the main memory system that's one of the reasons main memory has become even more important and that exacerbate the old requirements we're going to talk about them as a result hopefully I've shown you with a row hammer as an example that D and memory controls as we know them today are unlikely to satisfy the requirements that's why a lot is changing in the memory space today and there also some emerging memory technologies like phase change memory which we briefly discussed that happened to be non-volatile that enables some new opportunities like manipulating persistent data directly inside the memory user by merging the memory and the storage system so given this we need to rethink the entire main memory system and the systems we're designing around it to fix the issues we're having with memory technology and enable emerging technologies and while satisfying all those requirements so let's take a look let's talk about some of these requirements first so these are in one slide these are some of the major trends that are affecting main memory today I think some of these are clear and I don't have reliable too over here but they'll come in the next slide but the need for memory capacity bandwidth and quality of service performance is increasing this driven by the fact that we can do computation really easily basically we can put many many computation units on a chip and we know how to design them reasonably multicore is an example the source of multi-agent right we can design accelerators we can design course whatever those are really easy to design for us today of course our specific challenges inside them but compared to memory we know how to design them and fit them in logic relatively well applications are increasingly data intensive there's an increasing hunger and demand for data we've discussed this earlier we're gonna discuss it in more and people want to consolidate things more and more on a single system so if you have a single node if you want to get the highest efficiency out of it you would like to put many applications running together on top of it this is true for cloud computing it's true for mobile systems you want to run many applications here at the same time because that's good for improving efficiency of the system in data centers for example if you dedicate one data center to run mail that's not good because that goes under utilized even web search that goes underutilized so it's good to actually put other workloads on it like user workloads for example and they quote they get co-located so you get much better efficiency in terms of energy performance well energy cooling even area of the data center as opposed to building 10 different data centers for 10 different workloads why don't why not have one data center great I think this critically affects sustainability for sure so this is one example Trent basically this is data from an Iskra 2009 paper by HP labs and university of michigan david's could plotted how fast the core counters increasing and DM capacity is increasing so they're doubling at different rates and core counts is increasing much faster as a result they projected that memory capacity per cores dropping or expect to drop by 30% every two years which is not good because software has always relied on having more memory capacity to have more features in a thread right now you could argue of course with these trends these are real data points that they had but you could argue okay what about 2018 right now you may do the study yourself but maybe the trend is similar in GPUs for example especially maybe it's different but now if it's if the trend is different you may actually actually want to ask the question why aren't people putting more cores at the same rate well maybe because they are limited by memory because if you cannot supply date to the course if you cannot supply capacity it also makes no sense to put more course right okay and trans are actually much worse for memory bandwidth per core and we're going to talk about that memory bandwidth is increasing by about ten percent every year their jumps of course like when we go to 3d stack technology which we will also talk about but it's not good basically where we know how to design course we can maybe write this curve nicely but we don't know how to supply the cords with memory capacity and memory bandwidth with data essentially let's top some fun with these trends over here these are DRAM trends over the last 18 years this is what's happened to the DM chip basically how much has it capacity increased how much has it been with increased then how much of the latency increased any guesses what do you think capacity will be in the 18 years yeah I see that okay yeah that's pretty much like that except we're having trouble right now right so it used to be increasing nicely exponentially as you can see that's a log scale over here and this is Moore's law and it enabled by Moore's Law but we're having also scaling difficulties at the circuit level as a result the EM chip capacities are not increasing as fast but they're actually not bad compared to this so bandwidth has increased only 20x this is also limited by power and other considerations and we're not doing really well over here so for latency latency has reduced by 30 percent so we're going to talk a lot about latency even though latency is very important right but this is easy to improve because of Moore's law now we're having difficulties even in those easy to improve things bandwidth is usually a function of money you can throw more money at it and you can get better because you can actually have lots of channels for example right you can you can have more pins you can throw money at it so this is not that that hard to fix of course cost is a very important consideration so it's not that easier as well latency even if you pay money you may not be able to get it so we're going to talk about latency a lot as well so you need to have good system design and a lot of thinking to improve this and partly this is because of the business model of how we design memories today partly because of the mindset the mindset has never been that latency centric it has been more ok this business model is very good for our capacity so let's make some money out of it I think this is of course a simplification of it but there there's a lot of truth in the way it affects mindset right so people are of the mindset that capacity is really important but as a result they may not pay attention to the latency a lot but latency is important so these are some backward looking applications from my perspective backward looking because they already exists we know relatively a lot about them and they're actually limited by almost everything they need capacity they need bandwidth they need latency so these are applications that are really driving the memory problems but they're also new applications like genomics and machine learning that are driving the memory needs even further ok the next trend is energy in power in memory thus a key system design concern today there's a nice paper that we will put up by charge lafurgey and others at IBM research again 15 years ago they showed that about more than 40% of the entire system energy spent on the off chip memory hierarchy of chip memory hierarchy including the ERM of chip caches of chip storage of chip interconnect at that time 15 years ago today well not exactly today but power8 IBM power8 about eight years ago they show that more than 40% of the entire system power is just India another paper that analyzes GPU power consumption shows that more than 40% of the system power is India so a lot of things came on chip DM has remained off chip and most of the power is going there right now also applications are intensive as well and one of the issues that we've discussed that affects the year I'm scaling quite a lot is GM consumes power when it's not used you need to periodically refresh it so you actually waste a lot of power just to refresh the um as well okay so energy n power is another perspective and we've already given I've already given you the relative numbers in terms of how much energy spent for accessing memory versus doing computation right so on top of all of this technology scaling is also ending and all an example that I motivated this with but there are other technologies scaling issues in DM and people have been projecting ITRs international technology roadmap of semiconductors has been projecting that DM the future side of ADM cell will not scale easily below some nanometers and I like x over here because they like changing their projections clearly and people are pushing for lower x clearly to drive the hot capacity requirement capacity up but it's true that there is a scalability limit we'll see what that limit is but it's become very difficult so as you reduce the size of the cell you get a lot of benefits our capacity our identity lower cost lower energy although this is becoming also more debatable with the change in the with the end of the NART scaling which we discussed right basically the i'm scaling has already become increasingly difficult and we've talked about some of these things there are a lot of papers that be written and as a result there is proliferation in the memory system design today basically are some emerging memory technology like 3d stack drm reduced latency dram hopefully lower cost ones lower power idea and non-volatile memory there's a lot more I didn't put everything but if you look at the memory space some of which we are going to cover there are different types of the um and there different types of memory also and they have advantages but they also have disadvantages I'm not going to go through these detail detail but the purpose of this slide is to show that we don't have a main memory technology even in the ER that's good at everything now if you see the sort of picture different technologies and different greens and reds you can come up with high hybrid memory system it but basically this has been driven partly because of the scaling issues in charge memories are becoming difficult to scale it's difficult to place and control charge this is the flash cell which we may talk about later it's also charge based but DRAM is charge based and reliable sensing becomes difficult that the charge storage unit size reduces with these memory technologies and as a result we have the DM scaling problem which we have talked about in row hammer right the slide should be familiar to you and as a result we are having these reliability problems and this is a paper that I would recommend that you read it's not a required reading because we cannot make every reading required but this is a very good example of the reliable trends in memory and as you as we've discussed earlier in row hammer it's important to build infrastructures to actually understand such issues not only reliable tea but also latency refresh I'll refresh as a form of reliability I think but such a fundamental form of reliability you cannot get away from that right may be errors you can protect but refresh yeah yeah if we refresh is so fundamental to the app and we built this infrastructure that I described earlier so if you're interested in doing studies we can talk about that too and we've seen that because of these trends you have these problems right security problems also so I'm gonna skip these and I talked about this paper I don't know if we assigned this is required meeting but clearly industry is very very concerned about this also people who don't normally talk to each other industry are writing papers about it there it's basically they're proposing to architecting controls and DRM because we're having a lot of issues with memory so we're at this point right now in memory and this paper is proposing something that already exists and they proposed you should have error correcting codes inside memory because it's very difficult to tackle these reliability problems at the circuit level or the device level so you'd better correct the errors when they happen with error correcting codes so this error code can cause occupy area inside the memory you cannot use it basically it's lost capacity right that's one of the reasons capacity is not going to improve as fast because you're going to use some of that capacity for something else to correct errors and this already happening mobile memories have error correcting codes going forward and I believe all of the memories will have error correction quotes if they're beyond some capacity but if you object okay if you went back if you went to a DM manufacturers about ten years ago or even five years ago well I guess five years already passed since paper so maybe things are going fast I guess I'm getting older but if you went back to them if we went to them and ten years ago and said oh I want ECC era correcting code in your memory they would kick you out of the road because they care so much about capacity but now mindset is changing because there are other problems that are really driving things reliability he's a huge issue as a result now they have air correcting codes inside the main memory right okay and so basically we're going to talk about how do we solve the memory problem and I'm going to give you three broad directions over here we're going to talk about fixing it somehow making memory and control is more intelligent just like that paper that I mentioned proposed right go architecting memory controllers idea this requires some new interfaces new functions and new architecture I call this a system memory Co design in that memory scaling paper that I mentioned we call it the system memory Co design we're going to talk about that we're going to talk about another direction eliminating or potentially minimizing it can we replace this technology that's having problems with some other technology or maybe augment it so this is going to take us to new technologies and system-wide rethinking of memory and storage and hybrid memory systems which we're going to talk about and we're going to talk about embracing it potentially maybe design heterogeneous memories none of which are perfect and map data intelligently across them this opens up a new space in data management and maybe users for example some of your memory is very reliable some of your memory is extremely unreliable but some of your data doesn't care about being extremely unreliable on a being on extremely unreliable storage right because you can tolerate the errors at the algorithmic level so in the next few lectures we're going to talk about that but keep in your mind that we're going to talk a lot about software hardware device cooperation it's good because we're at this point in memory for various reasons but one of the reasons has been we are so focused on improving one components or one part of the system as opposed to looking having a more holistic view of the system and improving the system together with its with all of its components and I think we're going to look at solutions that are really exciting from that perspective and we're gonna exercise I'm thinking across the hierarchy labs ok this may be a good place to stop I you didn't stop me but you should have stopped me five minutes ago I guess it was interesting that's why I didn't stop me okay this is probably a good place to stop we're going to pick it up and start with essentially some solution directions and then go into more fundamentals of diagram and then there are really interesting things in memory I'll see you next week 
So9SR3qpWsM,27,"Operating System: Computer System Architecture
Topics discussed:
1) Types of computer systems based on the number of general purpose processors.
2) Single Processor Systems.
3) Multiprocessor Systems.
4) Advantages of multiprocessor systems.
5) Types of multiprocessor systems.
5.1) Symmetric multiprocessor systems.
5.2) Asymmetric multiprocessor systems.
6) Clustered systems.
7) Types of clustered systems.
7.1) Symmetric clustered systems.
7.2) Asymmetric clustered systems.

Follow Neso Academy on Instagram: @nesoacademy

Contribute: http://www.nesoacademy.org/donate

Memberships: https://bit.ly/2U7YSPI

Books: http://www.nesoacademy.org/recommended-books

Website â–º http://www.nesoacademy.org/
Facebook â–º https://goo.gl/Nt0PmB
Twitter      â–º https://twitter.com/nesoacademy

Music:
Axol x Alex Skrindo - You [NCS Release]

#OperatingSystemByNeso #os #OperatingSystem #ComputerSystemArchitecture",2018-02-21T01:30:46Z,Computer System Architecture,https://i.ytimg.com/vi/So9SR3qpWsM/hqdefault.jpg,Neso Academy,PT13M54S,false,252473,2657,69,0,92,in the previous lectures we have been studying about the basics of operating system and we have seen things like computer system operations and we have also seen structures like storage structures and input/output structures now in this lecture we'll be studying about computer system architecture and mainly about types of computer systems based on number of general-purpose processors that means we are going to study how we can categorize computer systems based on the number of general-purpose processors that they use all right so let's see how we can categorize our computer systems based on the number of general-purpose processors that they have so first of all we have single processor systems which from the name itself you may already have understood that it has only a single processor and then we have the multiprocessor systems in which there are two or more processors and then we have the clustered systems in which two or more systems are coupled or clustered together in order to perform certain tasks now let us explain each one of these systems and see what are their differences and what are the advantages of them and what are the types of this processor systems that we have so first of all we have the single processor system so in the single processor system one main CPU capable of executing a general-purpose instruction set including instructions from user processes is present that means in this single processor system we only have one main CPU which is capable of executing the general-purpose instruction sets as well as the instructions from the user processes now other special-purpose processors are also present which perform device specific tasks now in the single processor system apart from the main CPU there are also other processors which are present which does not do the general-purpose task but it performs some device specific tasks now what do we mean by this this means that we have certain devices in our computers like our keyboard our disk and so on so for all this there may be some microprocessor which is specified to do a specific task relate to that device like for example let's say that we have our keyboard now when you press a key on your keyboard the keystroke has to be converted to some kind of code let's say a binary code so that the computer actually understands what you have pressed or what you are typing so in order to convert that keystroke to a code there is a little microprocessor present on your keyboard which is going to perform only that task of converting your keystrokes to some kind of codes now those are the special-purpose processes that we are talking about so this kind of special purpose processors are also present but they are not for general purpose but for specific purposes now you may question me saying that I said it is a single processor system but here I have already showed you more than one processors I said that there is a general-purpose processor and also special-purpose processor yes that is right but the answer to this is that we give the name to this processor systems based on the number of general-purpose processors that we have so even though there are special purpose processors which performs some little task specific to the devices but since we have only one general-purpose processor we call it a single processor system so that is why we call it single processor system all right now let's go to the next one so the next one is multiprocessor system so from the name itself we can understand that in this there will be more than one processors or we can say there are two or more processors and also it is known as parallel systems or tightly coupled systems so this multiprocessor systems are also known as parallel systems or tightly coupled systems and these two terms will be discussing later as we move ahead in this subject so this multiprocessor systems they have two or more processors in close communication sharing the computer bus and sometimes a clock memory and peripheral devices so unlike the single processor system which had only a single processor in this multiprocessor system we have two or more processors which are closely communicating with each other because they need to work together to perform certain tasks and what are those tasks they may be a single task or different tasks depending on the kind of system that we have so in order to work together they need to have a close communication and synchronization with each other so that is why we said that they are in close communication and they also share the computer resources that we have now let us see what are the advantages of having this multiprocessor systems over single processor systems so coming to the advantages the first advantages increased throughput so throughput is something that we can use to measure the performance of our system so sometimes it is also known as the amount of data that can be transferred from one location to another so throughput is like the measure of the performance of the system so we can say that in multiprocessor systems we will have increased throughput or increased performance and why is that the answer is simple that is because we have more than one processors which will parallely do our work and it will make it faster and more efficient so that is what we mean by increased throughput and then the second advantage is economy of scale now why do we call it economic I say that multiprocessor systems are more economic as compared to the single processor system and that is because in multiprocessor systems we have this different processors sharing the resources of our computer or our system but in case of single processor system if you want to have the same amount of power you may need to have multiple single processor systems with their own individual resources but here we see that we have multiple processors sharing the resources so in that way we can have economy of scale that means it is more economic as compared to single processor system so if you have a multiprocessor system with three processors and if you want to have the same kind of setup in your single processor system you may need to have three single processor systems three individual single processor systems with their own individual resources so we see that this is more economic and then the other advantages increase reliability now this is more reliable as compared to single processor system and why is that the reason is again the same because we have more than one processors and why is it reliable that is because in this multiprocessor system even if a single processor fails we still have other processors which will back us up and which will still keep us running so let's say that you have 10 processors in your multiprocessor system and let's say that one of them fails so even if one of them fails the remaining 9 can share the work that was being done by the one that is failed and without giving you a complete failure your system will still work even though the performance may reduce a bit but in case of single processor system if your single processor fails that means it is a total failure your whole system breaks down but in multiprocessor system even if one fails the remaining can help to get the work done without causing a total failure of the system so that is why we say that it is more reliable and it has increased reliability as compared to the single processor system alright so those were the advantages of multiprocessor systems now let us see what are the types of multiprocessor systems that we can have so basically there are two types of multiprocessor systems first one is called symmetric multiprocessing and the second one is a symmetric multiprocessing now let us see what are the difference between these two type of multi processing systems so in symmetric multiprocessing we have our different processors here CPU 1 2 & 3 denotes the different processors that we have and processes P 1 P 2 and P 3 these denotes the processes that has to be executed now in symmetric multiprocessing all these CPUs or all these processors they are actually the same that means they are similar to each other and they all participate in performing these tasks or processes P 1 P 2 and P 3 so all the CPUs are involved in all these tasks so that is known as symmetric multiprocessing now in a symmetric multiprocessing unlike this one where the CPUs 1 2 & 3 where similar to each other here we have a master slave approach so here this boxes denotes the CPUs so one of the CPU or one of the processor will act as a master and the remaining processors will be slave so the master what it does is it monitors this other processors and it assigns the tasks for these processors so the master is like a monitor which is guiding or supervising these other slaves and then the slaves they take care of particular processes so let's say here's slave one is taking care of process number one slave two for p2 and slave three for p3 and we see that slave one is only taking care of process P 1 and process P 1 is not taken care of by slave 2 or slave 3 so P 1 is only for slave 1 and so is p2 and p3 for slave - and slave 3 respectively and here also if something fails the master will take care of how to distribute the load after one fails and things like that so the master is the monitor and the rest are the slaves but in symmetric multiprocessing we see that there is no master slave approach but all the processors are peers or they are similar to each other and they are all involved in all the tasks that is present so that is a difference between symmetric multiprocessing in a symmetric multiprocessing now let us come to the next type of computer system that we have based on the number of processors which is the clustered systems so in this cluster systems like multiprocessor systems cluster systems gather together multiple CPUs to accomplish computational work so this is also somewhat similar to our multiprocessor systems so here the cluster system they gather together multiple CPUs to accomplish computational work now you may be thinking if that is the case then what is the difference between multiprocessor system and clustered systems so the difference is that they are composed of two or more individual systems coupled together so in multiprocessor system what we had was two or more processors so only the processors were the multiple things that we had but in clustered system we have the complete system that is coupled together that means two or more individual systems that means complete systems not only the processor but the complete systems are coupled together and they form a cluster so that we mean by this clustered systems so this system it provides high availability now you may have already guessed why it provides high availability that is because we have more than one systems coupled together so even if one fails the rest of the systems together can take care of the task that was being accomplished by the one that failed so you are not having a complete system failure even if one or two fails because there are others that can take care of the system so a chance of complete failure is very less that is why we say that it provides high availability and even in this cluster system this can be structured either a symmetrically or symmetrically so just like our multiprocessor systems even this clustered system it can be structured either as symmetrically or symmetrically so in a symmetric what happens so in SME trick one machine is in a hot standby mode and in the others run the applications so we know that in a symmetric mode or we are having is a master/slave approach so even in this one one machine it is in a hot standby mode so that is like the master and then the other systems they run the applications so this machine in hot standby mode what it does is it monitors the other servers or the other systems which are running and if it finds that some of them fails or if let's say one of them fails this machine in the hot standby mode will take its place so one is monitoring and the rest of them are running and then in symmetric mode what happens is two or more horse run applications and they monitor each other so unlike this where one is monitoring here all the systems are all the hosts that are there they are running the application so they are involved in certain tasks and instead of one system monitoring every one here all the systems they monitor each other so if you think which is better asymmetric or symmetric obviously it is a symmetric one that is because here all the resources can be shared and used efficiently as compared to the asymmetry structuring so there was about the clustered systems so those were the types of computer systems based on the number of general-purpose processors we discuss about single processor system processor system and clustered systems and we also saw the advantages of one over the others and also we have seen the types of these systems that we have so I hope you understood these types of computer systems based on the number of general-purpose processors and I hope this was clear to you thank you for watching and see you in the next one [Applause] [Music] 
GNm-niWesmw,27,"In this lecture, basics of Different registers of IAS Computer has been addressed.",2021-01-01T18:53:05Z,Lecture 08 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/GNm-niWesmw/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT13M1S,false,58,1,0,0,9,[Music] architecture and temporarily registered the control unit operates the ias by fetching instructions from memory and executing them one at [Music] capability mq register so a uh memory buffer register memory of a register contains a word to be stored in memory or sent to the io unit or it is used to receive a word from memory or from the i o units microprocessors [Music] keyboard specifies the address in memory of the word to be written from or read into the mbr memory buffer register memory take a ack word data data memory take it again contains the address of the next construction pair to be faced from memory program counter address of the next accumulated [Music] is computer science and engineering 
GdmOByxp5RA,22,Computer organization and architecture lecture series by Rosna P Haroon,2019-06-08T19:28:42Z,Important topics with clarity(COA KTU syllabus),https://i.ytimg.com/vi/GdmOByxp5RA/hqdefault.jpg,Computer Science and Engineering mentor,PT13M13S,false,2894,61,0,0,23,hello an important questions of course important topics in Romania prodigies at what's the clarity proper and the robotically para new DVD and the important topics the model by a pariah another okay so first imodium some of the important topics in the owner by addressability white addressability but another what is white a disability at the parlour by addressing schemes render biten by till they address ability screams and on the big container on the plate Tarantino other with example is when you need to optic stems are the Mata para you look I didn't individual medium already have prodigies agenda but outer other video gone her talk then second one dimension agree instruction types in depth instructions in the 101 address instruction day two addresses a row addresses three addressing about it now an insertion types in death other question by Perla previous was seen by a person is overreaching under tender water one who has to address instructions and examples on the other end in session on the children then steps in executing and in session the first module illicit come order your number and expecting a little goes there are no steps in executing an instruction instruction executes a insomnia in the key and steps yeah they might anomaly instruction program counter and address and Dawa artists Laputa instruction fetch Illya then in stationary for - area and the operands in and Angela's in Algeria member islandhoppper and Samira and addressing modes cheetah operands story in a location different area map operation area vision and the to operations and unshod execute a big yeah that means it was cell tonight package area this I could continuous one step set finished and subroutine calls stash a protein called Scylla and an erisa protein or car the one else uprooted number the nested sub protein so alone again I'm gonna handle a in this stack another data structure all sadhana so so stacks are protein called self mocha then condition quartz condition course enriched it in Bengal a condition coordinate the condition codes condition called syllabary another Nala condition course our number status register laverna condition about sin and avoiding Veria on the k ID on the side on the zero we did the zero flag pin ml no overflow flag III on a condition called Salaria condition code Seppala previous pasta memorandum or the previous was to be a person of children then different type of address a more see-through a length peak of senators pretty anniversary on a different type of addressing modes the indirect addressing more direct addressing mode register addressing mode you know implicit address a more immediate addressing order I'm going to report addressing more cinder and after some words in the video already uploaded she didn't definitely on door OH then pinniped a parameter and guru a Erica and I that was the disco machine architecture Nordic a steady Anna and then I'm Lee first modular coronated stairs a process and a thoroughly resistor address a motor and I know divided on Engelbert each of whom other kuru specific case study which it the EA atom indicates live there are special purposes this day is over I am a Duvalier the Carter same old see you see em okay a steady hand is actually the previous questions ritual which it under $10 a Citroen topics are compulsory and can agree motive on della cover a year Tom then model - model - LF or nourish it in English single person multiples organizational data person says the processor I wish L upon the angle loading in Erie can show the gain there we explain the explain the the can't explain the contents the contents in the marais Mattila in this opera at the processor Natalja data path and thunder show the can Emilia xalapa the Nathalie registers are the natural Indian and organisation Mary can show the internal organization of the processors enjoy the child internal data path inside the processor is you will have same answer but under you didn't a murky data path organized am on the single best sushi to Professor cheetah weather constantly separate items an English president apparent single personal tables on another a parameter a diagram which it especially in the regarding organizational ER diagram unimportant a diagram are the needed but I am that the diagram in could you pick on you automatically you will be astonished attack the thing they say remember as a young they were long then booth algorithm and the multiplication where the Phyllis a cache a multiplier approach you under both Alvaro del portillo the money important they're both Ellsworth in a flowchart to me important and a flowchart or when I they separated over again show the cap of the cola render numbers are positive numbers and empty number service on the booth the recording though it done for EM at the ball of the network application was from GN Empire penny array multiplier is also important and the 2x2 array multiply Row three by two are a multiplier or either a multiplier and Anglia are then a corn scepter other than Appetit sugar other were cheated then restoring unknown restore and efficient job restoring unknown Western Division differentiate a energy of the game are the water than in the practical examples then floating point multiplication and division floating point multiplication dimensional flow chart is important flow chart developer different at flow chart or a canary cage over the dinner it's a flow chart an Iranian comparison a particular chart over - Iranian their steps easily we can remember okay model Tolec regarding a lanka then model 3l over ago memory map but I you and I you my Prada you other their each other lighter are you in there facing off you're gonna hurt up then then I give interfacing elements like yoga then as well and a member in my better you I um I put I owe you a silly memory memory memory mapped i/o no children to bury sherry and a same address space input devices in memory might take a slit take insulin in the middle then I floated I do my but I never cheated on you in memory of different address spaces you can you say another as a Paula what this program controlled are you in in the but then interrupts and Domino gossip all day multiple devices my enterprise simple yeah another hand till I say in general mcarthur mind it apart method Center on the polling on the vectored introducer other Paula interesting in a marinara daisy chain daisy chain is also important the pole invented interrupts some important and Sarah perfected in terms of Mahaprabhu Cho the cam pin a direct memory access DMA DMA is most important question DMA direct memory access and what decided to stay in Cyprus stealing essentially related to the Emily other karna third we do and I went slow then other words standard are you interface a standard i/o interfaces laminator I'm a girl syllabus illa moon is surrounded interfaces and a barnacle on the PC a meniscus e on the USB happy you are speaking in a separate after picture and ever us with and nature episode yeah Indian architecture you are speeding their architectural desperately I mean it it shows it yeah okay a pictorial guiding the modules really miserable the model foreigner children intimately you see it is khorium but enough for a more model on a model for I belong no no memory hierarchy on the memory hierarchy no children again I'm memory resizer cost st the NFC data memory organization Anna it took are the level is Penniman memory very finicky showing up in a processor registers or about our organization memory hierarchy no constant Mangala Pannell static Ramon dynamic Ram Anoka the mill and differences mocha little diagram commercially but it's Augusta active terminal a full transistors which Atari mother wonder automatically goes to water Michelle dynamic family number or attracts its everyplace you then a capacitor a tough smart dynamic Ram Elizabeth charge a capacitor let's charge at an information story another so I would refresh them that shall force to follow him and the advantage an especial dynamic family normally never eats a chicken a mother and the disadvantage and then different type of Rome is Amica var was independent expect a young girl was seen on different types of from either he draw me prom-prom other general the general italic Ariana everybody can agree on is a stupid suitable then cache memory and map a function it is also important what is cache memory cache memory leak a shift and on a cache miss and Anna the olet riittÃ¤Ã¤ operation is right to operation remember right though right backer and a technical death when Emma happy functions are more than ever turn and a director ma p associate him happy and set associative mapping model for the keys here to score a chamber turn Laura medulla then modify once itself Mordor five and six Scylla viola is a short title and the model and a model five immortal six other one damn order 5x a passive annelid tourmalet method and inshallah mode or fiber mode or six in in London than every other before nobody can easily you can spur forty bucks back here another but the module is Anoka then modify will wear another basic arithmetic logic and shift to micro operations set up a basic item at tecnolÃ³gico operation situation that machine's to micro operations in conditional control statements simple to show the cylinder tender conditional control set by the macarons over the care in the form Arsenal about automatic illogical shifting micro operations and the meta control statements ananka then a deterministic purity say before bit and intimate a circuit it is a general can show the game anyway you'll be safe already our agenda will be longer about a little bit later committee circuit and onion explained since they became different to be turned so we'll take an angular and the full adder which it at the replay see them are they at the boiler a per the operation with Chautala certain onion now we will about neutral other but then what is cell operations are restricted italic empower another analogy operation will change the anybody seen that Allah been a 4-bit shift at the same or the for British appear under the diagram a cheater body yeah finished after success is important status in the diagram buddy karyam the TN status is information but he came up with you to come parade during NSA in an already example Samaria with the diagram their processor unit is a processing unit is in a country offered which he turns over the gap a buggy processor than unity poly modules in the urban as the confessional about if if 10-6 the motility processing unit to show the kingdom and vanilla can't overdose an unexplained Shana in approach a tender karna then accommodated the same accumulated is same Angelica wrestling the onion in a video uploaded she didn't abandon oka make obey and a manganese have already shaved or look back chef shell a potion Watauga can return a cue or a paella but accumulator porcelain Tiana topics a municipal Anoka and then module for six models axilla are covered in early in I love since anahatha minguk expect a on the constants in higher numbers hello shiraito Nakamura six cylinder on the control unit unit is saying control internet is say nananana movin the method in the heart away to control a micro program to control the PLA control but shall have a separate hardware to control it you can separate item I programmed under it shows again these two is most important PLA control mode is cherry here toka karna other words assisting of the polar net horizontal line but take a micro instruction differential between horizontal ever take a micro instruction open India protege that isn't one of the cola then micro problem sequencer micro program sequence a then micro program to CPU organization here at the top with soda and it entered operation in the Iranian approach a the tailor if time permits I will upload it on tomorrow no cut the maximum mocha but in our ecosystem compulsively but he kept and all units all units in shall happen and organization that you can show they came there like a different methods for the same in control unit in that he can show they can then I learned in a hard way to control a different a test Frenchy and Eric and show the kingdom I programmed in different I did show you care about never in expecting amputee Horace or never take a micro instruction micro program see cancer micro program to see you organization is a microprocessor consider para go Simba people enjoy Chitralekha preemies okay then upon an ITEX Amur they allow more the westerns again thank you 
geXldvoB6hk,27,COA,2017-12-25T14:32:30Z,"Computer Architecture and Organization Lecture 1 Layers, Languages and VMs",https://i.ytimg.com/vi/geXldvoB6hk/hqdefault.jpg,Dr Kapil Nagwanshi,PT9M16S,false,90,4,0,0,0,languages levels and virtual machines the problem can be attacked in two ways both involve designing a new set of instruction that is more convenient for people to use them the set of built-in machine instruction taken together these new instructions also form a language which we will call Alvin just as the built in machine instruction from a language which we will call l0 the two approaches differ in the way program written in Alvin are executed by the computer which after all can only execute programs written in machine language l0 one method of executing a program written in l1 is first to replace each instruction in it by an equivalent sequence of instruction in al 0 the resulting program consists entirely of alt 0 instruction the computer then executes the new al 0 program instead of old and run program this technique is called translation the other technique is to write a program in l0 then takes program in l1 as input data and carries them out by examining each instruction in turn and executing the equivalent sequence of al 0 instruction directly this technique does not require first generating a new program in l0 it is called interpretation and the program that carries it out is called an interpreter translation and interpretations are similar in both methods the computer carries out instruction in l1 by executing equivalent sequence of instruction in Al Jarreau the difference is that the translation the entire l1 program is first converted to n LG to program the alvin program is thrown away and then the new l0 program is loaded into computer's memory and executed during execution new generated l0 program is running and in control of computer in interpolation after l1 instruction is examined and decoded it is carried out immediately no translated program is generated here the interpreter is in control of computer - it the Alban program is just data both method and increasingly a combination of two are widely used rather than thinking in terms of translation and interpretation it is often simple to imagine the existence of hypothetical computer or virtual machine whose machine language is l1 let us call this virtual machine m1 and let us call the machine corresponding to L 0 and M 0 if such a machine could be constructed cheaply enough there would be no need for having language l 0 or a machine that executed programs in al 0 at all people could simply write their programs in l1 and have the computer executed them directly even if virtual machine whose language is l1 is too expensive or complicated to construct out of electronic circuit people can still write program for it these programs can be either interpreted or translated by a program written in al 0 that itself can be directly executed by existing computer in other words people can write programs for virtual machine just as though they really existed to make translation or interpretation practical the languages a 0 and L 1 must not be too different this consistent often this constraint often means that al 1 all the better than l0 will still be far from ideal for most applications this result is perhaps discovering in light of original purpose for creating Alvin relieving the grammar of the burden of having to express algorithm in a language more suited to machines than people however the situation is not hopeless the obvious approach is to invent us another set of instruction that is more people oriented and less machine oriented than l1 the third set also perform a language which we will call l2 and with virtual machine and two people can write program in Al you just as though a virtual machine with Al 2 as its machine language really existed such programs can be either translated to l1 or executed by an interpreter written in l1 the invention of a whole series of languages each one more convenient than its predecessor can be can go on adding indefinitely until a suitable one is finally at you each language is uses its predecessor as a basis so we may view a computer using this technique as a series of layers or levels one on top of another as shown in the figure the bottom most language or level is the simplest and topmost language or level is the most sophisticated there is an important relationship between a language and a virtual machine each machine has machine language consisting of all instruction that the machine can execute in effect of machine a machine defines language similarly a language defines a machine namely the machine that can execute all programs written in the language of course the machine defines by a certain language may be enormous ly complicated and expensive to construct directly out of electronic circuit but we can imagine it nevertheless a machine with C or C++ or Java as its machine language we b-complex indeed but could be built using today's technology there is a good reason however for not building such a computer it would not be cost-effective compared to other techniques merely being double is not good enough a practical design must be cost effective as well in certain sense a computer with an levels can be regarded as an different virtual machines each one off with a different machine language we will use the term level and virtual machine interchangeably however faced please note that like many terms in computer science what your machine has other meaning as well we will look at another one of these letter on the coming lectures only programs written in language 0l0 can be directly carried out by the electronic circuits without the need of intervening translation or interpretation programs written in l1 l2 r2 l1 must be either interpreted by an interpreter running on lower level or translated to another language corresponding to a lower level a person who writes programs for the level n virtual machine need not be aware of underlying interpreter and translator the machine is structure ensures that these programs will somehow be executed it is of no real interest whether they are carried out step-by-step by an interpreter with intern is also carried out by another interpreter or whether they are carried out by the electronic circuit directly the same result appears in both cases the programs are executed most programmers using an N level machine are interested only in the top level the one these resembling the machine language at very bottom however people interested in undal understanding how a computer really works must study all the levels people who design new computers or new levels must also be familiar with livers other than the top one the concept and technique of constructing machines as a series of levels and details of levels themselves form the main 
v0FH2LpDZsg,27,"Computer organization lectures for GATE, Complete Computer Organization lecture series. Computer Architecture and Organization for GATE, Computer Organization tutorial. 

. _______________________________________

1. Digital logic design tutorial (DLD Tutorial):

https://www.youtube.com/watch?v=baF-cxSl8TA&list=PL4hV_Krcqz_J4K8dEFsqzm3zJIgzFF9MQ&index=2&t=16s

2. Computer Organization Tutorial: 

https://www.youtube.com/watch?v=ayJBTJLt4cQ&list=PL4hV_Krcqz_JaY3JmbrDgy5tipHrOmGBW&index=2&t=6s

3. Computer Networks Tutorial:

https://www.youtube.com/watch?v=yTnAB4IMU8g&list=PL4hV_Krcqz_KLIzfuShdbDiAdyrhJbwF6&index=2&t=1s

4. Operating Systems Tutorial:

https://www.youtube.com/watch?v=Fd9ucp6_hho&list=PL4hV_Krcqz_KyOBQEm6825QQJ6m2c1JRY&index=2&t=1s

5. Database Tutorial (DBMS Tutorial) | SQL Tutorial

https://www.youtube.com/watch?v=mwlKkUmhLeU&list=PL4hV_Krcqz_IbPUf3mAJbje5XQdPORrYi&index=2&t=0s

6. C programming Tutorial:

https://www.youtube.com/watch?v=zmLv-IjU000&list=PL4hV_Krcqz_JhUAojsTolbrTarJPrtzvM&index=2&t=1s

7. Algorithms Tutorial (DAA Tutorial):

https://www.youtube.com/watch?v=l51gzYCnA8k&list=PL4hV_Krcqz_L_qeClFzxcr9sJCIF5MUGe&index=2&t=10s

8. Data Structures Tutorial | DS Tutorial:

https://www.youtube.com/watch?v=56OA2C9Uxmc&list=PL4hV_Krcqz_KzWhCr3zJj3_z4wkSLSP7v&index=2&t=0s

9. Problem solving using Data structures and Algorithms Tutorial:

https://www.youtube.com/watch?v=wwWGOkYk500&list=PL4hV_Krcqz_LqMkNHswMN868hL1Klj2Li&index=2&t=1s

10. Probability Tutorial | Permutation and Combination Tutorial:

https://www.youtube.com/watch?v=6DeqpQFUPpI&list=PL4hV_Krcqz_Kp449S66_fmbaaVakWAAjc&index=2&t=7s

11. Interview Puzzles Tutorial:

https://www.youtube.com/watch?v=eOUYaaSkwq4&list=PL4hV_Krcqz_JyjXz-8DDzz3XgdkZiMF3F&index=2&t=258s .
.
12. Aptitude Video lectures for Placements | GATE | SSC | Bank PO | Quantitative Aptitude lectures

https://www.youtube.com/watch?v=oiPb-qAWME0&list=PL4hV_Krcqz_KjZl0UzQGnXTPC4xsbA0fS",2018-12-14T20:13:40Z,Memory Organization-10: Direct mapped cache example-2,https://i.ytimg.com/vi/v0FH2LpDZsg/hqdefault.jpg,GATE Video Lectures - Success GATEway,PT4M40S,false,1079,16,0,0,1,okay now come to one more example of direct mapping the it is a gate 2015 question consider a machine with a byte addressable main memory of to keep our 20 bytes block size of 16 bytes and a direct mapped cache having to keep our 12 cache lines let the address of two consecutive bytes in man memory be e 2 0 1 F and E 2 0 2 0 in hexadecimal what are the tag and cache lines address in hexadecimal for memory address this bus so much my tour Sunnah album locator Rohini would Yun Dana actually cookies make even a yeah ko given a key Smith after head sorry they say it is a direct map cache no to keep our 20 bytes to keep our 20 bytes a 2 point of 1 byte we need physical address 20 bits this 20 bit is divided into 3 sections because of direct mapped cache it is a direct map cache direct mapped cache team parts medical care third one is tech second dish cache upset and thirties word oblique by topside but so much my how many bytes in of block that is 16 for bus so much my 4-bit for byte offset know how many cache lines cache lines mclubbe cache blocks got a point cache block see cache lines ot hence to keep our 12 is mukluks 12 x 12 + 4 6 the remaining is for line for beans now that is all in bits but so much Maria no option de man memory addresses e20 1f this is in hexadecimal job a piece a binary may convert keurig a you will gain 20 disease binary number no interesting part make you binary may break nahi Tyrone it's Cabana 50 alien tech bits kidney app for former club hexadecimal cup 1 alphabet 12 mutlar hexadecimal k3 alphabet and word of said for Butler hexadecimal cup a built now of D and ignore the answer deco it is a 10 it is a cache of sent and it is a word or by Dobson clearness what is the question what are the tag and cache line address what is the tag e if there be there e ok really now there are two options a and C what is cash offset that is 2 0 1 that's why answer is e adhirata risk of what upset couch owatta the word of satis f yeah byte offset keoki it's nibble the Yankee memory is byte addressable gotta find up simply create Java physical address in Muscatine sections midi white girl Joe be data to patients me given it yeah hopper Jewett important path I will you have cache line cache line is nothing but cash you got a point okay is there any doubt you 
HnTJoRvIC6M,22,"Ù…Ù† Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø³Ø§Ù‚ ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" Ù„Ù„Ø¯ÙƒØªÙˆØ± ÙˆØ³Ø§Ù… Ø¹Ø§Ø´ÙˆØ± Ø§Ù„Ù…Ø­Ø§Ø¶Ø± ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¨ØºØ²Ø© .

Computer Architecture and Organization course by Dr. Wesam Ashour, lecturer at the Islamic university of Gaza - Palestine.

Course language: Arabic",2016-12-20T07:43:57Z,"22- Computer Architecture ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" - Chapter 5 multicycle",https://i.ytimg.com/vi/HnTJoRvIC6M/hqdefault.jpg,Abdallah Safi,PT47M45S,false,2347,14,0,0,0,associative undecided with signals in the contraction of your big dog and shagging Helena Kira who's back [Music] [Music] [Music] in the hot plate and good signals mijita single cycle he shut they put hardware canonization in you own facility it should clear the government is watching their father but my cubicle aha entire control signals applaud Kanna Veda si que lleva la cerca de ver de Cerio Colosseo can investigate Popo Yatta Yatta clock striking video by addition the top Zana McConaughey kinetic official away applied Alhambra compromise afro Delgado then Safra in calorie Mohammed comes down outwards shaking the table together cyclonic Allen in an University and careful instruction the goethe has a hardware Americana recycle recycle recycle yawning they are out put the lid on it one over here Rosario had a young age antenna RecycleMania package after de oaxaca tobacco cessation achievement and a spa in sequential silver lace at my dish and together football after me output with the table Janna totally their district Fuhrman should will be shaking with the devil Koran and the bull and mean we grow the group cyclopÃ¦dia and he is you I had the TV output we shall be decided a little chihuahua young again I'm gonna do left village [Music] [Music] [Music] [Music] a little fish sandwich yeah had in instruction book to her to register for salsa i are will be sydney hand so with a Baba to thank you they shall fetch and we had destruction which is one tricky [Music] million matter dr. add instruction register like we decided that we had to be good every day last combinations to play like k4 upper understand in which function so much man Russia [Music] [Music] I never thought I'd ever need to know Naruto you see why this year everything in the road know we'll get it Bob dissociative you to lay off that which the mr. Richie Quran jugular this equals for the PC [Music] [Music] [Music] [Music] see what we see we see we see is right [Music] every dollar I as far who is a big  West Michigan circulation in signal - signal Shannon Sanderson and it suffers me their money will have a design as far as my free when you survive ooh [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] find the partial match and we see the top the net is we register come on Mauro exactly that there will be an salvation [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] Yani mmm ago ahaha each to step three the power and Cooney comes shortly and an associate [Music] [Music] [Music] step 3 [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] yeah good from step one [Music] [Music] but cannibal because ganador and recipient are but in gala more inclusive for our gather for the wedding Finch when I say be done destruction they can dig it with the mattock the tower the victim would welcome an gilsu with rational argument mother listen without the Pik a good image move [Music] [Music] [Music] [Music] [Music] yeah [Music] [Music] [Music] [Music] [Music] yeah I think so [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] 
k-UoK-fl_M0,27,This video contains the differences between Hardwired control unit and Micro-programmed control unit in Computer Organization,2018-11-13T07:23:25Z,DIFFERENCES BETWEEN HARDWIRED CONTROL UNIT AND MICROPROGRAMMED CONTROL UNIT IN COMPUTER ORGANIZATION,https://i.ytimg.com/vi/k-UoK-fl_M0/hqdefault.jpg,DIVVELA SRINIVASA RAO,PT7M49S,false,9822,125,34,0,25,hi friends today I am giving a lecture on differences between hard-boiled control unit and micro program control unit in computer organization so it is one of the important question in computer organization so there are two ways of control organization first one is hardwired control unit and the second one is micro program control unit so now we go for differences between Hardware control unit and micro program control unit so the first point is in Hardware control unit the control logic is implemented by different components such as logic gates decoders multiplexers and other digital structures okay so by using these components we can implement the control logic in Hardware control unit but in the case of micro program control unit the control logic is implemented by using a sequence of micro operations so here a micro operation is an elementary operation that can be performed on the operands okay so these sequences of micro operations are initiated by or it are initiated by programming the control memory so here the control memory is one of the important component in the micro proof control unit so in that control memory a micro program is stored so a sequence of micro operations are initiated by programming the control memory so that control memory contains the control information so by using this micro program so a control logic is implemented through a sequence of micro operations next the second point here design modifications can be done by changing the lives between different components okay so here design modifications are done among different components by making changes in wiring so here we have to make changes in wiring by providing the design modification so design modification can be done in the case of hard word control unit only by making changes in wiring so here design modifications are done by making changes in the micro program that is stored in the control memory okay here we are changing the micro program that is stored in the control memory here we have to make changes among various different components only by changing voice only by making changes in voice only third point is here RISC architecture is there risk is nothing but reduced instruction set computer so it uses only limited number of instruction so most of the risk pictures use Hardware control unit most of the RISC architectures so reduced instruction set computer so most of the RISC architectures use the hardware control unit but RISC architectures does not use the micro program control unit next for point in hardware is organized by using multiple twice ok here we are using multiple wires between different components so the hardware is organized by using multiple voice here a micro program is organized by using sequence of micro operations or instructions so here micro program is nothing but it is a sequence of micro instructions so here a micro program is organized by using a sequence of micro instructions here a hardware is organized by using multiple voice ok so among these two control organizations which is fast so here Hardware control unit is fast compare it to micro program control unit so here micro program control you it is a slower compared to hard-boil control unit ok so it uses less area or least area but here it uses maximum area to accommodate next one is it is implemented in hardware it is implemented in hardware but here it is implemented in software because so here the modifications can be done among the various components by using micro so microprogram is nothing but software so here a hardware it is implemented in hardware so it is implemented in software so it is more flexible it is not flimsy it is not flexible to I so more instruction are more system specifications but here it is flexible to BI system specifications are instruc adding more instructions okay so it is not flexible to accommodate more number of instructions or more system specifications but here it is a it is very flexible to add some instructions or some modifications or adding some system specifications it is very flexible it is not flexible so these are the main differences between Hardware control unit and micro program control unit 
9jOuzjE9OcM,27,"Len Shustek
Computer History Museum

April 16, 2020

View the full playlist: https://www.youtube.com/playlist?list=PLoROMvodv4rMWw6rRoeSpkiseTHzWj6vu",2020-05-11T18:49:05Z,Stanford Seminar - Learning from history: the how and why of starting a computer history museum,https://i.ytimg.com/vi/9jOuzjE9OcM/hqdefault.jpg,stanfordonline,PT54M56S,false,1745,47,1,0,N/A,"okay welcome to EE 380 spring a computer systems the Stanford computer systems lab claw Club spring 2019 2020 the year of the kovat I'm Andy Freeman and the other course organizer is Dennis Alice most museums are paintings other visual arts like dance plays etc have their own little of their own display medias and so most museums end up being paintings with museums I always liked or technology artifacts cars coal mines submarines now you know the museum I'm talking about trains tractors I grew up in a farming community where there was actually a tractor museum but the most important technological artifacts in the last 50 years are computers however computers visually tend to be fairly boring outside of 1970s and 80s movies and the other problem of the computers is that their tools and their owners once they don't they cease to be useful to their owners they just discard them they don't pass them down to other owners of need less or are willing to pay as much and there's no collectors so they essentially disappear or yeah they just basically disappear which makes collecting them into museums which is a very good way of educating future people getting people interested is incredibly hard today's speaker lunch such htech has solved that problem so without further ado you're on great well thank you Andy I am Lynn szostak and I'm the chairman of the board of the Computer History Museum and if you haven't been there you should take a visit to it because it's just down the street it's over on shoreline and 101 now of course right now we are closed because of the pandemic but we're hoping that pretty soon we will open up again let me give you 30 seconds of my background so you can understand how I came to this Odyssey I was born and brought up in New York I went to the Polytechnic Institute of Brooklyn got a masters and bachelors in physics Polytechnic by the way is now in WIU's engineering school and then in 1970 I changed everything about my life went to Stanford changed my field of study and enrolled as a graduate student in the relatively new computer science department which then was a graduate only department and was part of the humanities and Sciences division of the school not the engineering school eventually had a great time at Stanford and at SLAC escaped with the PhD went to Carnegie Mellon was on the faculty there currently computer science for a little while came back to Silicon Valley and out of a period of 16 years did two different startups both in the networking field and one of them the second one was a relatively successful so I decided to retire let me retire and go back to my first job which was teaching so this time at Stanford I started teaching none of this background qualifies me to start a museum of any sort yet it happened here's what happened if you look at the Stanford online calendar catalog you will see a course listed as EE 282 computer systems architecture it's basically the fundamentals of designing computers many of you have taken it or will taken it will take it it talks about architecture and instruction sets processor technology memories i/o system software virtualization and so forth I happen to be very familiar with this course because I took it fifty years ago when I first got to Stanford in 1970-71 I don't remember which quarter and then I was taught by legendary professor of EE Edward McCluskey famous for all sorts of techniques for designing computers including something you probably don't learn about anymore the Quine McCluskey minimization algorithm but it's still at the bottom of their log and VHDL is the way they compiled what you design and I level language into detail gate designs so I remembered this course and in fact 25 years after I took it it was the course that I taught in the winter quarter of 1994 now these three different versions of the same course separated by 25 years had some things in common at very many implementation details that were different and the focuses were different when I taught heard it was designing microprocessors when Ed McCluskey taught it to us it was the design of mini computers and when it's being taught now notice that it says notebooks smartphones and data centers that's fine the fundamentals are still the same and you are all learning how to design computers when I taught it it was with the beta test version of the Hennessy and Patterson classic now classic textbook computer architecture a quantitative approach compared to what I talked 25 years earlier there was one important difference most of the history had been erased when ad McCluskey taught us about for example virtual memory in paging systems he distributed to us the original papers from Tom Kilburn and others at the University of Manchester describing the Atlas computer which at the time was the most powerful computer in the world it was a transistorized computer with a whopping 16,000 48 bit memory the words of memory could add 500,000 ads a second and it was the first machine that had virtual memory I thought it was a fascinating way to learn technology to understand where it came from so one of the things I started doing was to spend the first five minutes in a kind of Show and Tell moment bringing in old artifacts like this this is a piece of genome dolls PhD thesis at the University of Wisconsin he was a PhD in physics but he decided that he wanted to design a computer and he did it was called the whisk and this is a piece of his computer built out of vacuum tubes which some of you may not know ever existed or what they do this module is probably one flip-flop so this is one bit of information I found that the students were interested at least in five minute sections in history and I decided as a result of that that something needed to be done to introduce history I wasn't going to change the curriculum at Stanford so I thought my contribution to this problem would be to start a museum look we ought to learn about the history of our field if you're physicists you learn about Newton and Einstein if you're a chemist you learn about Lavoie ZN boyle if you're a biology major you learn about Mendel and Darwin why don't computing science computer science and electrical engineering students learn about how our earth and von Neumann and Eckert people who created our field so my crazy idea was to start a museum there were several advantages one is we've all had the problem of having people come to Silicon Valley and say show it to me show me Silicon Valley and why don't you show them you drive around and you can see s fold parking lots and concrete tilt-up buildings how come if we're the center of the world for computing that there isn't an computer museum in Silicon Valley so I decided I would start it was a totally nutso idea I knew nothing about starting or running a museum but I started writing white papers this was the first one from October of 1995 I started shopping the concept around I tried to get people interested in in the idea and many were and I realized quickly that the important thing was not about collecting artifacts and he talked about the difficulty of collecting objects that's true but a museum is not a repository of objects it's a repository of stories so I want to take a little brief detour now and talk about an example story of the kind that I wanted to preserve and present and we do preserve and present in the computer museum we eventually built it's actually the most embarrassing question that people ask us who invented the computer everybody knows who invented the aeroplane and the phonograph and the light bulb how come you don't know who invented the computer no you asked some young folks and they'll say well maybe it was Steve Jobs or Steve Wozniak or Bill Gates of course none of that is true the problem is it's a difficult question to answer it's a complicated story but if you put me up against a wall and say you have to choose one inventor of the computer who is it that's my answer this handsome general and Charles Babbage he was the son of a wealthy banker living in London he didn't have to work for a living he became the location professor of physics at Cambridge University which was the position that Isaac Newton had had and he spent his time thinking about things one of the things he did in doing his mathematics was to compare all of the different texts that gave mathematical functions in those days if he wanted to find out a sine or a cosine or a logarithm you didn't compute it that was very tedious you looked it up in a book where it had been pre computed and he compared all of these books and they all differed on at least every page there were some differences and obviously one or the other or both of them were wrong at one point he said I wish to god these calculations had been executed by steam which was a metaphor he meant it was some way to do it mechanically with precision and accuracy so that you know the results would be correct and he devoted the rest of his life to designing machines that would do things like that to precisely calculate mathematical functions there was an easy way to do science cosines and logarithms you can approximate them with a higher order polynomial equation you can use the method of differences once you start with the first few values of computing subsequent values so here's an example this is a second-order polynomial equation the second difference is constant so the way to compute the next value of the function for X equal 40 is just to do two additions easy so Babbage decided he would build a machine to do this a calculator the technology available to him at the time was not electronics it was mechanics so he designed something he called the difference engine he began in 1823 to calculate by the method of differences sixth order polynomials using 16 digit numbers this was not an analogue computer this was a decimal digital calculator that little wheel down there is an example of how he stores one digit it's a rotating brass wheel that can be in one of ten positions indicating what the value of that number is it also in his design had a complicated printing mechanism because he didn't want the result to have to be manually transcribed and typeset because that introduces the possibility for errors so he designed this machine now how did he do it you know nowadays when we designed machines we use CAD computer-aided design well Babbage had his own kind of CAD it was cardboard aided design these are original pieces from Charles Babbage of cardboard mock-ups showing the kinds of parts he needed to build in order to create his machine so he designed it he went out for funding to build it he got funding from the only venture capitalist of the day which was Queen Victoria who gave him seventeen thousand pounds that's about two million dollars worth of money today and said go build it what happened he spent that 17,000 pounds ran out of the money it took him 19 years he built about half of the parts the 25,000 parts that his machine would have needed and he got into fights with his craftsmen he wasn't good at managing people so the result was a total collapse of the project and his calculator was never built now one of the reasons was that his eye was not on the ball anymore while he was building this he had a better idea for something else he wanted to build this happens to engineers all the time instead of finishing the project they're working on now they want to work on version 2 or some Better Project and Babbage's Better Project was a computer he called it the analytical engine this is a top-down view of the engine he would have built each one of those circles is a tall column of those digit wheels like we saw before so this would have been a huge machine the size of a locomotive all mechanical no electronics powered by a steam engine it was a computer that we would recognize today it had a separate CPU which he called the mill it had memory which he called the store it had pipelining so that you could have multiple instructions in progress at the same time that parallel processing so you could do multiple things at a time that conditional branches so you could decide what to do based on the result of a prior calculations it had loops it had micro code so that you could encode complicated things with one instructions and he had invented things that we reinvented 150 years later like carry look-ahead sophisticated math algorithms it was an amazing machine it also of course had software you had to program it he did not put the software in memory instead he put the software on punched cards these were cards that he had seen used since the beginning of the 1800s by jacquard on looms to automate the weaving process and he had the idea of using those cards to encode the instructions that his computer would follow well how do you make a loop you connect one end of the cards to the other end of the cards into a big loop and then it repeats over and over again the same set of sequences now this was a great design which Babbage did not publish he worked alone and had no collaborators he only had one collaborator an unlikely one it was this woman Ada Lovelace she was the daughter of the poet Lord Byron estranged from his wife and Eva's mother decided that in order to keep her from going crazy like her poet father she would be tutored in mathematics and she took correspondence courses from famous mathematicians of the day like Augustus de Morgan famous form de Morgan's laws now this was extremely unusual for a woman to be involved in science in the 1800's but she did it she was brilliant she worked on the programming if you will of Babbage's analytical engine now she wasn't really the world's first programmer Babbage had never yet designed the instruction sets what she did were things like this with Babbage this is an execution trace of the program that would run on the analytical engine to compute Bernoulli numbers so it would be easy to imagine what the program would have to be to create that sequence of operations she published when Babbage did not translate her to an Italian description of Babbage's machine and and worked with Babbage on the refinement of the machine in order to support these kinds of calculations that image was entirely focused on mathematical computation one of the things that Ada Lovelace saw was that it could be used for more than that at some point she said it might act upon other things beside number the engine might pose elaborate and scientific pieces of music she even foreshadowed AI at one point she said it does not appear to me that cerebral manner needs to be more unmanageable to mathematics than side aerial and planetary matter I hope to bequeath to generations a calculus of the nervous system she wanted to understand how the analytical engine could be used to create intelligence none of this ever happened because Babbage never even attempted to build his analytical engine so was Babbage a nut did he designed machines that could never have worked well 150 years later the London Science Museum found 20 pristine drawings in the archive done by Babbage of a machine that he designed after the analytical engine it was a simplified version of his original Difference Engine and this fellow Doron Swade curator of computing at the London Science Museum decided to see if Babbage was a nut or was he a brilliant engineer he took Babbage's original drawings worked for 17 years to build 8,000 pieces of iron bronze and steel using exactly the same manufacturing techniques that Babbage could have used built the machine 11 feet long 7 feet high waists 9,000 pounds this cost several million dollars about the same as Babbage burned through 150 years earlier and this is the result the thing on the right is the microcode if you will it's the cam stack that controls the machine the crank on the side is what powers it there are 31 digit decimal numbers in stacks along those vertical columns and the thing to the left is the printing machine that automatically creates a printing plate in quartz perfect you you you you you the London Science Museum actually built two of these machines we had one of them on display at the Computer History Museum for eight years until it went back to its owner who had paid the several million dollars to have it built now why did Babbage fail clearly his Difference Engine built exactly the way he designed it works perfectly maybe his analytical engine would have worked had he built it there's a project now to investigate whether we could build it had that happened we would have had computers a hundred years earlier than we did we would have had a computer in 1848 instead of 1948 so why did Babbage fail it wasn't for lack of brilliant engineering there are many reasons one of which is he didn't know how to organize the project and run the company this is sound familiar there are probably 25 companies within 20 miles of Stanford which are who are going to fail because they have brilliant engineering and don't know how to manage the project create now Babbage almost had someone who would do that Ada Lovelace wrote to Babbage in 1843 saying if I am delay before you in the course of a year or two explicit and honorable propositions for executing your engine would they be any chance of allowing myself to conduct the business for you she's basically offering to be the CEO of his company and he turned her down who knows why maybe he you know they didn't trust the idea of a woman doing it I think it was in large part because Babbage was bullheaded and wouldn't have accepted anyone in that position in any case Babbage died a bitter and disappointed old man nothing he had ever designed was a success he said at one point another age must be the judge of my talent and we judge his talent in engineering a plus in management F I said none of his inventions had ever succeeded there was only one it was this it was the cowcatcher the thing in front of a locomotive that pushes cattle off the track but that's not the legacy he wanted to live delete to us so I think this story is a great story and these are the kinds of stories that I wanted the museum to be able to tell so I was going to go off and create a museum first thing I did is to look around for other examples so I traveled the US and the world looking for examples of computer museums I found great examples of technology museums you know in Chicago the London Science Museum the deutsches museum I figured surely there must be some if you look at how many other Technology museums there are there are thousands of railroad museums an automobile museum some aviation museums and I discovered there were no computer museums this is a picture of all the computer museums before 1980 there was only one exception and that was a museum that Gordon Bell had started in Boston now Gordon Bell was a legendary is a legendary computer architect he was a head of engineering and Digital Equipment Corporation for many years responsible for them bb8 pdp-11 the backs and so forth and so on he wrote a great book called computing structures readings and examples which reprints many of the papers about his fear of computers and he decided he wanted to see somebody collect all of those computers that was fact one fact two is the machine on the lower right is whirlwind it was a legendary machine built at MIT they discarded it Gordon called up the Smithsonian saying this is a national international treasure it needs to be preserved and the Smithsonian said we don't collect computers so Gordon said I'm gonna have to start a museum of my own so inside a Digital Equipment Corporation in 1979 he started a computer museum at a museum store and exhibits had problem getting funding Digital thanks to Gordon was paying for it but obviously if it was called the digital computer museum which meant Digital Equipment Corporation they couldn't get anyone else to finance it so they moved they moved to the wharf in Boston right next to the Children's Museum in front of the big milk bottle they created a computer museum that ran from 1984 in 1999 they had exhibits this is a diorama showing the UNIVAC one computer they had programs this is a list of the programs going on in 1982 I think it was they started the Fellows Awards giving honor honors to important pioneers that's grace Murray hopper who received the first such award in 1983 but the problem was that in by the 1980s much of the computer industry had left master so it was in Silicon Valley they had a hard time getting funding they began to morph into a Kids Museum which was a fine thing they had a big walkthrough computer where kids climbed over giant replicas of personal computers but it wasn't the history museum that Gordon wanted to start and they were in trouble it was clear that they weren't going to survive all of the good stuff most of the good stuff was no longer on display it was in the back room in Gwynn Bell wardens wife who ran the museum would give you a private tour I said okay that's sad so I returned to Silicon Valley kept planning learned the lessons from Boston computer museum I thought and then I got a call from Gordon Bell who said I have a deal for you don't start your own Museum restart what we tried to do in Boston join the board of the Boston computer museum to start a West Coast outpost which reinvigorates the original history mission and that's what we did in the winter of 1997 now my first objective was to preserve the artifacts they were one-of-a-kind things that could no longer be collected again I managed to finagle some free space from a friend who was running a Technology Center at Moffett Field under the aegis of Ames NASA Ames he's a world war two vintage Quonset huts with leaky roofs notice the sandbags in front of the doors on the right right in the shadow of the big dirigible hangar that still had its skin on in those days and we commissioned seven huge tractor trailers to move the artifacts out of Boston to California it didn't quite have the authority to do that but I figured possession was nine-tenths of the law I wanted these things to be preserved so we moved them sometimes a little hokey we didn't have a forklift that could drop down this piece of the sage so we backed two trucks uh back back to each other and lowered the tailgates in synchronization so that we could get the machine down to the ground it was an amateur effort to be sure we even moved objects into the dirigible hangar I managed to get a forklift license and move some of the stuff around I think I'm the only computer science PhD in the world with a forklift license at least as far as I know we started colouring and inventory increate adalah exhibit called visible storage where basically we set up all of the equipment or the stuff that was interesting looking and let people tour around it had not a lot of labels it was just stuff it was great for the cognoscenti people who knew what they were looking at loved it it wasn't the museum I was dreaming a building but it was a step in the right direction we got some press this is an article in the San Francisco Examiner about our efforts to build the museum so one of the questions is where to build it I had some conversations with folks at Stanford which we're handle izing my Keller who I think is still there ahead of the library said why don't you build it on Stanford campus on the opposite side of palm drive from the Cantor Arts Museum so we would have arts on one side and Technology on the other side it's the location where the being Performance Center is now I thought that was a great idea so I talked to the chairman of the board of this Stanford trustees and he said great idea if you've got 20 or 30 years to make that happen go for it I didn't try number two was at NASA Ames at Moffett Field NASA was going to develop a research park this is their conceptual plan of the research park the in in red it's circled computer museum they were going to give us three acres right in front of the big dirigible hangar to build our museum was going to be a land lease from the government they were inside the hangar going to create a California Air and Space Museum sounded like a tantalizing prospect we get an architectural competition we had bone core models built over the building we were going to create it was great fun we were headed into the trap that many museums fall into of putting our emphasis on building a building instead of building a museum well there were three things that happened that saved us from that trap one is of course that the blossom Museum closed so we were now free to do whatever we wanted we were our own California corporation the second is we realized pretty quickly that NASA moves with the speed of a federal bureaucracy and we were startup people we wanted to move more quickly than their research part which 25 years ago they had that wonderful land for is still in progress and maybe someday we'll build it the third thing that happened in about 1999 2000 was the dot-com bust up until that time there were no buildings available in Silicon Valley they were all full and suddenly there were lots of see-through buildings buildings that had been abandoned and were available like this one this at the corner of shoreline in 101 was the sales and marketing headquarters of Silicon Graphics famous computer company at the time founded by Stanford professor Jim Clark and it was empty so what can graphics had sold it to a developer and I fell in love with it I took one look at it and said this is a museum they built it as a museum only they didn't know it at the time it was filled with 300 purple cubicles for Silicon Graphics so I wanted to buy it you the problem is it costs 25 million dollars that we didn't have so I went to our board and said I want to borrow 25 million dollars it was a crazy notion because many of the people on our board had been on the board of the Boston computer museum where they had failed raising less than a million dollars to retire their mortgage and here it is I was asking with no support for the notion that we could ever pay it off to borrow 25 million dollars well now they let me do it it turns out in retrospect to have been a brilliant move accidentally brilliant that 25 million dollar property now is probably worth about two hundred and fifty million dollars and we own it we don't have to pay rent to anyone so we moved yet again now we had decent forklifts this is the Kray to being moved off the truck into the shoreline building we created version 2 of visible storage again a lot of computers without a lot of labels we didn't have the money to build exhibits yet but we did other things too we had all this great space we had lots of volunteers we started restoring old computers upper left is an IBM 1620 upper right is the first computer from Digital Equipment Corporation pdp-1 lower left is the an IBM 1401 installation with two running 1401 s and lower right is the world's first hard disk the Rama designed in 1955 by IBM at their San Jose Research Center with the exception of the 1620 on the upper left all three of these are still running and we operate them and demonstrate them to the public all the time here's an example of a crowd watching the 1401 system being demonstrated we also demonstrated really old computers like this Babbage difference engine that I described earlier we cranked it every day it worked perfectly we were often running even though we didn't have permanent exhibits we reinstituted the fellow awards that had started with Grace Hopper in Boston this is the 2008 film awards with Gene Bartok who was one of the original ENIAC programmers and Bob Metcalfe and Linus Torvalds so sure you know those names one of the things we started with high priority is to do video histories videotaped oral histories of computer pioneers we're losing them at an incredible rate if you don't get them on tape now they're going to be lost forever nobody ever interviewed Alan Turing there is no video of Alan Turing so by now we've done about a thousand oral histories they're all long videotaped multiple hours this is one from Paul Walker you may not know that name but he was the fellow who wrote the Das disk operating system for the Apple 2 the one on the bottom is John Backus a few years before he died he was the inventor implementer of Fortran one of the first high-level computer languages we started writing blogs we started releasing collecting and releasing source code so the blog on the left describes the release of Apple to das source code source code is the literature of computing and if we don't preserve it we've lost a lot of our history we did public programs we did interim exhibits waiting to be able to raise the big bucks for doing our permanent exhibit we posted lots of videos online both new videos like the presentation that Paul Baron inventor of packet-switching did for us and historic films like univac all of these films are up online if you go to youtube.com slash Computer History you can see them all we instituted educational programs educational programs were about I would say 20% history 80% new tech but it is a way to experiment with the idea of inspiring kids with a combination of what's happening now and what happened in the past we eventually ran out of space in our building and knew that we were about to what I wanted to create a big permanent exhibit so we bought it another building over in the East Bay where real estate is a little bit cheaper we bought a 25,000 square foot warehouse for storing all of the stuff that's not on display like every Museum what you can put on display is a small fraction of your collection and if you want a really cool experience talk to me sometime we'll be getting a private tour of our warehouse PS we'd finally raise the money to be able to build our first new permanent exhibition so it's called revolution the first 2,000 years of computing it's a big exhibit it covers everything from pre computing calculation to the Internet and the World Wide Web this is the mini computer gallery it's designed for not just the geek but for the general public who's interested in where computers came from everybody these days is a computer user it's pretty high marks it's big it's got 19 galleries 1,500 objects 5,000 images 16 films four theaters it's been a great success we opened it in 2011 in 2015 we want them to do more exhibits we were running out of space again so we bought our third building this one is a 50,000 square foot building in the East Bay for the archive which is paper software video that's where we do scanning of documents and films and photographs it's where researchers come to visit to use the archive it's where we recover old software moving stuff into that building gave us the opportunity to take its space in the main building and create a software exhibit this is a very different kind of exhibit than anything we've done before it's it has history in it has technology in it but it says much about the social impact of the technology and where it might be going as the history it's a it's a case study kind of exhibit so it's an experiment in doing different kinds of exhibits for different audiences so brief summary of the museum in numbers this is of course before covered 19 we are like all museums in the world closed but we were getting 130,000 exhibit visitors a year probably as many who came for events both hours and rental events we now have a hundred and forty-four thousands probably now after 150 thousand objects in our collection got 75 employees who are still being paid even though they're working from home and 200 volunteers and we raised a bunch of money to make this work we are of course struggling in this period when we're closed like all museums but we have a pretty good chance of surviving we have assets we have a 30 million dollar endowment we started this museum with the idea that we are creating an institution that will live in perpetuity and I don't know whether we've done that or not I'll know somebody will know 50 or 100 years from now I won't be here to know but hopefully it will happen and I should point out that after 25 years I decided to step down as chairman of the board and new administration will be taking over and I think the museum will be in good hands from moving into the future it's been an amazing journey but remember as our exhibits designer Kirsten - I've taught me a museum is not about objects it's about stories so I want to end with another story that we that we tell in the museum and I think is important for people to know it's about the memory that made computers work by the end of the 1940s everybody knew how to build computers john von neumann had described maybe not invented but had described that the right way to do it was to have enough memory city so that you could store the program in memory as well as the data the question for the engineers is what do you build that memory out of here are some of the examples of things that early computers used to store memory on the top left is a rotating magnetic drum it's mechanical so it's slow it's also relatively unreliable on the lower left is a mercury delay line that's the main memory for the UNIVAC one computer it's 18 tubes of mercury with a speaker basically at one end and a microphone at the other end and you induced standing waves in the tube of mercury and you could encode a thousand bits of information stored that get recirculated around so this is one thousand 18 bit words that was the entire main memory of the UNIVAC one computer expensive unreliable if you stamped your foot next to it as it was running it would disturb the standing waves in the mercury and induce errors in the middle as the Whitlam Williams killed burned tube which is basically a TV screen old cathode ray tube where you would paint dots on the screen and the thing that's folded down would be folded up which is a screen that would detect when a spot on the screen was illuminated whether there was a dot there before so it was a destructive readout system but it was entirely mechanical you could store a thousand bits on the face of the screen it ran at electronic speeds and it was the memory that was used for the first machine that ran a stored-program it was June 21st 1948 at the University of Manchester the machine called the baby it was a 17 line program the first piece of software ever to run inside memory the thing on the right was a very complicated device designed by RCA which was basically a plasma tube where you would store on plasma bits of information very expensive they barely ever got it to work it was the memory that was initially stored on the johniac computer now notice that three out of the four of these is all except for this electron I have the property that you're not building any specific thing for a bit that seemed like a good idea it seemed too tedious to have to build something specifically for each bit you depended rather on a bulk analog property of magnetic materials or an electrostatic screen or traveling waves in a tube of mercury none of these were the things that made computers work but what made them work was a return to that original idea of building something for every bit it was magnetic core memory it was one of the things that I brought in as a show until I this is the third world's first magnetic core memory it's from the world one computer this is a core plane that has 1,000 bits you put 16 of these core planes together you get 1k 16-bit words and that's the way all computers worked for the next 25 years let me be sure my screen here so how does corner memory work well it's based on an old principle of hysteresis that dates back to the late 1800s basically certain magnetic materials like a ferrite ring have the property that as you increase the electric current which is to say the strength of the magnetic field pretty much nothing happens for a while until suddenly it snaps and the magnetism changes from one direction to the other and then you can do things in the other direction and you continually change the saturation nothing much happened and then suddenly it steps into the other direction and by the way once it's snapped it records that information forever because it never gets demagnetized unless you put a magnetic field on it that core plane I just showed you might have encoding of a program for the whirlwind that's 65 or 70 years old okay so how do you address this memory and how do you create a practical memory out of it you can't afford to put wires through every core that are individual well there are two clever ideas one is because of the way hysteresis works you can put half of a con through a core and nothing happens so you can have one wire that goes through in the x-direction multiple cores one wire that goes through the y-direction through multiple cores nothing happens and only one core will switch because it gets current from both wires gets over the threshold for the hysteresis curve and the magnetism will switch so how do you know that it's switched well the switching magnetic field induces an electrical current you put a sense wire through all of those cores and then you can sense when the addressed core has changed magnetization so that's the principle on which a single core plane works this is a sixth this is one bit of a 16-bit memory main memory for a computer its destructive readout notice so that once you've read out the core if it was a one it is now a one if it was a zero it's not a one so whatever you've done when you read it you have to rewrite clever idea cores got smaller and smaller it was used for all computers from then until 1972 or three when Intel came out with the first semiconductor memory okay so great invention who invented it it's a similarly complicated story to the inventor of the computer here's one person who contributed and this is a delicious story because Frederic may was a sidewalk and Street inspector in Los Angeles and he moonlighted in his kitchen laboratory on magnetic materials and how he might use them for electrical purposes in 1947 he patented something called a memory transformer it was basically a transformer that would remember its prior state and it worked and when IBM got around to wanting to use magnetic core memory they purchased the patent the rights to that patent from baiii in 1956 well that wasn't the only invention on wang from harvard and later from wang labs in 1949 patented the idea of destructive readout that patent ii was purchased by IBM IBM knew that this was the way to build computers another interesting story by the way is that IBM was very late into the computer business but once they got into it they came in with both feet and dominated the industry relatively quickly by adopting new technology like core memory third fellow Yann Reichman from RCA he was actually the guy who had invented this electron but he also worked later on core memories in a 1950 patented the notion of coincident current IBM lisent that patent as well contributor number four Jay Forrester from MIT in 1951 patented something that's very much like reichman's patent not exactly the same coincident current IBM needed that patent as well but IBM was unwilling to license it what they said was that we will license you for a royalty of two cents procore that was anathema to Tom Watson he refused it he went to court to try to invalidate their patent on the basis of reichman's patent and that court fight went on for years eventually IBM lost they had to license that patent in 1964 from MIT for thirteen million dollars which was the largest patent sellin settlement and technology at the time it was a traumatic event for the computer industry because it made engineers realized that the technology they use and the patents that exist on them are extremely important it was also frustrating to people like Jay Forrester I love this quote he said it took us about seven years to convince them that was a good idea in the next seven years in the patent course that convinced them that they had all not thought of at first success as a thousand father's but failure isn't often so if you have a successful idea you better be prepared to protect it Jenny Forrester was our second fellow of the computer Museum in 1995 and he died relatively recently 2016 the computer of that world that that Forrester worked at at MIT was called the whirlwind computer and it was coincidentally the computer that Gordon Bell wanted the Smithsonian to collect and since they refused he decided to start a computer museum so in some ways the core memory story and the whirlwind story are all linked together in history of the museum it is still a work in progress one of the projects were working on now is reading the old world wind tapes we have about a hundred and fifty of these medical magnetic tapes that were recorded on whirlwind that haven't been read for 50 60 years something like that we are beginning to read them now I developed a software based recovery system for data from magnetic tapes we basically digitize the analog signals coming off the head of the tape and then in software reconstruct the bits the original data and the software that was running on these machines many many years ago if you're interested by the way all of my software is up on github then you can take a look at it at the link so it's been an amazing Odyssey lots of lessons to be learned one about history when I was a kid a student I wasn't that interested in history but I've obviously gotten really interested in history now it can be inspiring you can learn from it David Kennedy in the history department at Stanford said it's an imperfect but indispensable guide to the future look if all you know about is the present that's a single point you can draw any straight lines of the future through it at any angle if you understand history at least you have some other points you can do some curve fitting you can do some prediction about what is likely in the future you can learn about what things were tried and failed the other lesson for me was don't be afraid to do something new and unusual hey knew nothing about museums and I was able to succeed that road always happened so you have to be prepared to fail but it was a great Odyssey and I encourage you all to to be bold and try things that you don't know how to do and might fail it that's all it's a little disconcerting to be doing this without a live audience oh I can't get any questions but if you have any questions feel free to email me the email address is there of course visit the computer museum and our website computer history org there's lots of material there lots of historic material visit the YouTube channel where we have thousands and thousands of videos up there come visit once we open again if you're interested in more of my thoughts on the history of the museum I have a couple of links there two papers I've written so enjoy history enjoy the future thank you "
Ou-D8x72xBc,27,This is the first lecture of the web series on Computer organization for gate computer science. This lecture covers the memory hierarchy concept.,2018-07-23T13:39:18Z,1. Memory Hierarchy - Computer Organization - Gate,https://i.ytimg.com/vi/Ou-D8x72xBc/hqdefault.jpg,PacketPrep,PT8M12S,false,24101,273,3,0,18,"[Music] hi this is Krishna puja and US packet prep we are in a digital century where data is the most important element almost all the billion-dollar companies hugely rely on the data and where do you think this data is stored it is stored in a device called memory that is a hardware integrated circuit which can hold the data for immediate or later use and I'm pretty sure you know what memory is so let me not go too much into the definition now imagine that - you are building a desktop system and you have a lot of options you may go with a processor with 1 MB cache or maybe 2 MB or maybe 3 MB cache then you might pick up a ram of 4gb or 8gb or even 16gb you may go with 4 GB RAM if you're a very light user or you may both 16gb ram if you're a gamer then you go for at least one terabyte of hard disk and if your budget permits you may go with 120 GB of SSD now why do we need to have these many memory devices why can't we build a system with just one memory device something like processor with 1gb cache or maybe a processor with one terabyte artist just one memory device the issue is performance versus cost cache is super fast but expensive and hardness is super slow and cheap and the reason to go with different levels of memory is to strike a balance between cost and speed we need to build a system which is high on performance and affordable at the cost and just to give you some perspective on how this works out let me take some data relevant to 2018 so here we have cache then we have RAM then one SSD and the hard disk let me also include one processor in 2018 the CPU is able to execute instructions at a lightning speed it hardly takes 0.3 nanoseconds to exceed one instruction so this is a speed of the CPU now when the CPU makes a request to the cache for l1 cache the memory access time is about one nanosecond and for l2 cache it is about four nanoseconds and for ram it aches about 100 nanoseconds SSD it's about 16 microsecond or I can say it as 16,000 nanoseconds and the hard disk drive it's about two milliseconds what if I converted into nanoseconds it is 2 million nanoseconds look at this speed of CBO it hardly takes point 3 nanoseconds but look at hard disk it is taking 2 million nanoseconds that is pretty slow isn't it if I try to compare in terms of cycles and this rate 1 nanosecond means it's hardly 3 cycles because this is 3 times of this and this is about 12 cycles this will be 300 cycles this will be for 8k cycles this will be 6 million cycles so what this 6 million cycles means is CPU can execute at a rate of one instruction per cycle and if the data has to be fetched from hard disk it literally takes 6 million cycles so basically the CPU has to halt its operations for almost 6 million cycles so this will drastically degrade the performance of the CPU now look at the cache cache is able to supply the data at a rate of 3 cycles which is far better than hard disk now let me also give you the cost comparison cache we cannot directly purchase outside so let me take the example of a cross search if I take i3 processor with maybe about 3 MB cache it's costing around the 4,000 rupees so just for example sake I'll assume that the cost involved for i three processor maybe is 3000 and for 3mb cache roughly rupees thousand just for example say this may not be correct but that's fine and coming to the RAM and 8gb ddr3 waves 120gb SSD is almost fifteen thousand rupees and 1tb hard disk is about 4,000 Dobis I cannot directly compare the cost here because each one is of different capacity so for a better understanding let me write the cost for 1gb if I assume 3 MB cash is about thousand rupees if I have to build a system with 1 GB cache it will cost me almost 3.5 lakh rupees and then coming to the RAM 8 GB RAM it's costing 8 thousand rupees so for 1 GB it will be rupees thousand then 120 GB SSD is 15,000 so for 1 GB it costs around 125 rupees and for 1 TB hard disk it's 4,000 so 1 GB cost is rupees 4 now look at the cost hard disk is dead cheap but look at the cache very much expensive so if I build a system of 1 GB capacity with cash this is the cost and if I build the system with 1 GB capacity only with hard disk it is just rupees for this will be super slow so there is no point in building a system which is very slow so I need to strike a balance between performance and cost and that is the reason why we pick up different storage devices and we place it in a hierarchical structure something like this the first level if I take this as l1 this eigen address cache and this cache is super fast and expensive l2 is main memory not basically the RAM which is moderately fast and not that costly so I can say it's of less cost and level 3 we can think of auxilary memory this is actually slow but it's dead cheap and the reason to draw this pyramid-shaped diagram is it's based on the capacity generally we pick up cash of 1 MB to 3 MB size so very less space main memory we take around 2 GB it can go almost up to 16 GB this could be around 500 GB to almost 10 terabytes huge storage capacity but slow less storage capacity but fast and one more thing to note the cache and main memory the data is volatile what it means is as long as the power supply is there the data will be stored and whenever we switch it off the data will be lost but in auxilary memory this is non-volatile and that is the reason why we use auxiliary memory as a backup device we store all the photographs videos everything into the positive memory that's basically the hard disk all right this much categorization is sufficient now different books give it in a different way they even include registers at the top level and sometimes they give the cache as l1 cache l2 cache so instead of three levels they go up to almost five level six levels here I've drawn only three levels just to keep it simple and this is sufficient for gate [Music] "
DsOApvT21w0,27,"Part - 1 : Computer Architecture and Organization -  Computer System - I , II
OPEN BOX Education
Learn Everything",2018-08-19T11:50:22Z,"Part  1: Computer Architecture and Organization -  Computer System - I , II",https://i.ytimg.com/vi/DsOApvT21w0/hqdefault.jpg,OPENBOX Education,PT39M43S,false,3023,42,1,0,1,computer system fun learning objectives at the end of this topic you will be able to define a computer system list the hardware and software components of a computer system in the initial days and now and then you will know the various functions of a computer system after all this you will be in a position to differentiate between computer architecture and organization the definition of organization will lead you to understand and explain what interconnection structure of the paths connecting the peripherals to the computer are and what the organization of the system bus is so let us begin the first chapter outcomes by the end of this topic you will be able to implement a computer based system process component or program apply knowledge of computing and mathematics appropriate to the discipline use the fundamentals of computer organization as a tool in the engineering of digital systems in simple terms a computer system is a machine that receives input from the user inputs are usually given through keyboard and some of other input units are optical mark reader mouse and a joystick then the computer receives and processes the data processing involves executing a program which might involve the consumption of the received data and then it provides a result which is meaningful to the user the results produced by the computer systems that are called outputs and are usually obtained on monitors other output devices are printer and plotter the computer system also consists of a storage unit that helps the computer in storing the received inputs and the results produced as output this subject yields to the study of such a system inside out so the main hardware components of a computer system are input unit which is responsible for receiving the inputs from the user output unit the place where the user could see or get the result of computation a unit which processes data called processing unit a unit that stores the unprocessed as well as processed data called memory and the most important component is the system bus that interconnects all the four together nowadays we see a number of plug-and-play and removable devices like the CD ROMs flash drives digital camera and magnetic disks which are devices that sometimes act as in / 2 nets and sometimes as output units these kinds of devices which act as input as well as output devices are called input/output devices or i/o devices or peripheral units even the keyboard and monitor interact with a CPU in the half duplex mode so they are also considered as input/output devices so we can restate that the main components of a modern computer system are the central processing unit that is CPU which consists of control unit arithmetic and logic unit and a few registers the main memory that stores the currently used programs and data the i/o devices and the system bus what we have seen till now is the hardware components of a computer system and the interconnection let us see if there exists a software component too for that let us observe the working of a simple calculator the user provides the input which is a simple instruction through the keypad the device processes and produces the result within no time a calculator is an example of a simple computer which is capable of performing eval defined set of arithmetic and logic operations and the user provides instructions to do tasks which are a direct execution of these operations it consists of a sequence of arithmetic and logic functions within and when the user provides data with the operations to be performed it performs the operations and provides result the block diagram for the same is shown here such a computer can be called a hardware programmed computer if we design a computer which has a more general-purpose arithmetic and logic unit that is a set of arithmetic and logic functions which are basic to all kinds of calculations like for example using full adders to multiply numbers 1 1 1 when multiplied with 1 0 we have to shift the partial products 4-0 we put zeros and 4-1 we copy the multiplicand and then add to get the result here then such a computer would require an external source to control the basic arithmetic and logic functional units this external source is called the control unit acts on the basis of the instructions by generating control signals appropriate to the instruction the block diagram is shown here and this computer is a software programmed computer or a general-purpose computer with general-purpose hardware the system accepts data and control signals and produces results the control signals are supplied on the basis of what instruction the control unit receives this implies that for every instruction there is a set of control signals to control the ALU so that facility provides a convenience for the user to provide the tasks to be performed by the computer in bulk the tasks are given in the form of a logical list of instructions called a program all the codes are instructions which happen assigned a set of control signals constitute the software component of a computer technically termed as the instruction set this is the instruction set for IAS computer a general-purpose computer designed by von neumann and his colleagues at the Princeton Institute for Advanced Studies since the instructions need to be provided in bulk to the general-purpose computer a concept called stored program was introduced and implemented in designing the IES computer the structure of the is computer is shown here it consists of a main memory which stores both data and instructions an arithmetic and logic unit ALU a control unit which interprets the instructions in memory and causes them to be executed an input and output equipment of which the input module is used to feed the program into the computer system a means of reporting results as the output module IO modules are operated by the control unit almost all computer designs are based on concepts developed by John von Neumann such a design is referred to as the one Neumann architecture and is based on three key concepts first is the stored program concept where data and instructions are stored in a single readwrite memory the locations of the readwrite memory are sequential in order and so the contents are addressable by location without regard to the type of data contained there the instructions of the program are stored in a sequential order in sequential occasions of the memory so execution occurs in a sequential fashion unless explicitly modified from one instruction to the next here is shown a few internal components of the major components of a computer system in a typical processor a register call the program counter PC holds the address of the instruction to be fetched next it has an instruction register I are distort instruction the CPU exchanges data with memory for this purpose it makes use of two registers a memory address register M AR which specifies the address in the memory for the next read or write and a memory offer register MBR which contains the data to be written into memory or receives the data read from memory an i/o address register I Oh er specifies a particular input output device an i/o buffer i/o BR register is used for the exchange of data between an i/o module and the CPU now let us see what the major functions of a computer are the basic function performed by a computer is execution of a program since a program is a set of instruction a computer actually execute instruction after instruction till the end of the program in a computer system the processor also called CPU does the actual work by executing instructions specified in the program the processing time required for a single instruction is called an instruction cycle each instruction cycle consists of fetching the instruction decoding the instruction executing the instruction at the beginning of each instruction cycle the process of fetches and instruction from memory and the fetched instruction is loaded into the instruction register IR the instruction contains bits that specify the action the processor is to take the processor interprets the instruction that is it decodes the instruction and performs the required action that is execute the instruction this whole process repeats in a cycle as is illustrated here as a flowchart consider a program consisting of three simple elio instructions which is residing in a memory first instruction is fetched decoded and executed in the first cycle then the next cycle is for instruction 2 similarly for the third instruction and since the program gets over the process halts for this program the instruction cycle will be dealt with in detail in the second unit the second function of a computer is interrupts handling all tasks in a computer system whether it is a printing task transferring a bulk of bytes from the secondary memory or getting the input through a keyboard no task commences or finishes without the knowledge of the CPU for example the CPU finds whether the printer is in ready mode this it does by sending a command say are you ready the printer if ready replies I am ready then the CPU sent the document to be printed the printers buffer stores the document starts printing and after it finishes it sends an interrupt to the CPU saying I'm done here I am done interrupts the CPU in its normal barking every computer is provided with a mechanism to handle the interruptions by IO and memory devices interrupts are provided primarily as a way to improve processing efficiency because most external devices are much slower than the processor the interrupt handling also forms a part of the instruction cycle because it involves the CPU this flow diagram incorporates the interrupt handling into instruction cycle the CPU is normally doing its work that is the fetching decoding and executing while the interrupts receiving ports are closed for receiving interrupts when the user needs a print of some document for example it then checks if the printer is available and supplies the task to the printer and enables its interrupt receiving ports continuous checking for interrupts periodically while doing its tasks if an interrupt is detected the CPU goes in to process the interrupt interrupt handling will be dealt with in detail in units 5 & 6 the third function of a computer system is IO function an IO module example a disk controller can exchange data directly with the processor the processor can read data from or write data to an i/o module in this later case the processor identifies a specific device that is controlled by a particular IO module the IO module can exchange data to a cleaveth memory in such a case the processor grants permission to the i/o module to read from or to write to memory this operation is known as direct memory access or DMA now we are in a position to explain the difference between computer architecture and organization computer architecture refers to those attributes of a system visible to a programmer are those attributes that have a direct impact on the logical execution of a program architecture concerns with the software aspects of a system design like the instruction set number of bits used to represent various data types io mechanisms and addressing modes computer organization refers to the operational units and the interconnections that relies the architectural specifications Hardware details transparent to the programmer interfaces between computer and peripherals memory technology now we are in a position to explain the difference between computer architecture and organization computer architecture refers to those attributes of a system visible to a programmer are those attributes that have a direct impact on the logical execution of a program architecture concerns with the software aspects of a system design like the instruction set number of bits used to represent various data types io mechanisms and addressing modes computer organization refers to the operational units and the interconnections that relies the architectural specifications Hardware details transparent to the programmer interfaces between computer and peripherals memory technology now we talk about interconnection structures a computer system consists of a set of components or modules of three basic types processor memory and i/o that communicate with each other in effect a computer is a network of basic modules the collection of parts connecting the various modules is called the interconnection structure the design of the structure will depend on the exchanges that must be made among modules let us see what the requirements of these modules are one memory module any memory module will consist of inverts of equal length each word is assigned a unique numerical address 0 1 and so on up to n minus 1 a word of data can be read from or can be written into the memory the natures of the operation are indicated by read and write control signals the location for the operation is specified by an address when the data is read or written it has to be in the bus before going into or coming from the memory to IO module from an internal to the computer system point of view io is functionally similar to memory there are two operations read and write an IO module may control more than one external device we can refer to each of the interfaces to an external device as a port and give each a unique address in addition there are external data paths for the input and output of data with an external device finally an IO module may be able to send interrupt signals to the processor 3 processor the processor reads in instructions and data writes out data after processing and uses control signals to control the overall operation of the system it also receives interrupt signals for the CPU has the following connections with IO so the interconnection structure must support the following types of transfers memory to processor the processor reads an instruction or a unit of data from memory processor to memory the processor writes a unit of data to memory i/o to processor the processor reads data from an i/o device via an i/o module processor to IO the processor sends data to the i/o device IO to or from memory a bus is a pathway connecting two or more devices through which these devices communicate it is a shared transmission mode but its structure and interconnection design makes the access to the medium andr free it consists of a multiple communication line capable of transmitting signals representing binary one and binary 0 a bus structure is shown here it consists of the data lines which provide a path for moving data among system modules the data bus may consist of 32 64 128 or even more separate lines the number of lines being referred to as the width of the data bus because each line can carry only one bit at a time the number of line determines how many bits can be transferred at a time the width of the data bus is a key factor in determining overall system performance the address lines are used to designate the source or destination of the data on the data bus the width of the address bus determines the maximum possible memory capacity of the system the control lines are used to control the access to and the use of data and address lines because the data and address lines are shared by all components must be a means of controlling their use typical control lines include memory right Casas data on the bus to be written into the address location memory read Casas data from the address location to be placed on the bus I oh right Casas data on the bus to be output to the addressed IO port bio read Casas data from the addressed IO port to be placed on the bus transfer ACK indicates that data has been accepted from or placed on the bus bus request indicates that a module needs to gain control of the bus bus grant indicates that a requesting module has been granted control of the bus interrupt request indicates that an interrupt is pending interrupts ack acknowledges that the pending interrupt has been recognized clock is used to synchronize operations reset initializes all modules summary let's summarize the topic a computer system is a machine that receives input from the user processes the Tara and then it provides a result which is meaningful for the user a computer which has a more general-purpose arithmetic and logic unit that is a set of arithmetic and logic functions which have basic to all kinds of calculations interrupts are provided to improve processing efficiency because most external devices are much slower than the processor [Music] computer system 2 [Music] learning objectives at the end of this topic you will be able to explain the block diagram and the working of arithmetic and logic unit in the CPU visualize the binary representation of an integer so that it can be stored in memory perform integer arithmetic in binary understand fixed point representation one of the two schemes for representing decimal numbers in binary understand floating point representation second amongst the two methods for representing decimal numbers in binary outcomes by the end of this topic you will be able to apply the calculations performed by arithmetic and logic unit in the CPU for several engineering applications perform addition and subtraction of numbers represented using ones in two's complement notation let's take the first topic the ALU the ALU is that part of the computer that actually performs arithmetic and logical operations on data all of the other elements of the computer system that is the control unit and the registers of the CPU the memory and i/o are there mainly to bring data into the ALU for it to process and then to take the results back out a block diagram of ALU with its inputs and outputs are shown data are presented to the ALU in registers and the results of an operation are stored in registers these registers are temporary storage locations within the processor that are connected by signal paths to the ALU the ALU may also set flags as a result of an operation for example an overflow flag is set to one if the result of a computation exceeds the length of the register into which it is to be stored the flag values are also stored in registers within the processor the control unit provides signals that control the operations of the ALU and the movement of the data into and out of the ALU ALU executes what is sought in the instruction ALU consists of components that the form different functions as told by the architect these components are based on the use of simple digital logic devices that can store binary digits and perform simple boolean logic operations for example the addition of 2 4 bit numbers secretary consists of 4 full adders a naught a 1 a 2 a 3 are the bits of the number a from right to left B naught B 1 B 2 B 3 are the bits of the number B from right to left the result can be seen through the output s naught s 1 s 2 and s 3 with carry C out a set of arithmetic instructions for the eius computer as shown the following numbers are integers numbers can be represented and stored in computer systems in binary 1 and 0 only self there is a requirement to represent integers in binary a positive integer say plus 25 also 25 in decimal can be represented in binary by converting the decimal number into binary number so the decimal number 25 can be represented as 1 1 0 0 1 since processor registers or memory locations are of regular size may be 8-bit 16-bit 32-bit or 64-bit then the proper representation of 25 is 0 0 0 1 1 0 0 1 in binary in the 8-bit setup but how can we represent -25 in binary there has to be a method for representing the sign also that is how the following three methods were proposed to represent a negative integer they are signed magnitude representation signed ones complement representation and signed two's complement representation signed magnitude representation involves breaking the binary representation into two paths sign part and a magnitude part one bit value can be used to exhibit the sign 0 4 + + 1 4 - so an 8-bit register can accommodate signed decimal numbers from plus 0 to plus 127 which means the negative counterparts are going to be minus 0 to minus 127 but minus 0 has no meaning so signed magnitude representation has a limitation let us see the signed ones complement representation it involves the following rule all the positive integers are in n normal binary forms and the negative numbers are represented as ones complement of the positive counterpart for example minus 25 is represented as one's complement of 25 the resulting number has 1 as its MSB we know in 8 bits we can have + 0 - + 127 with a zero in MSB taking one's complement of these numbers we have numbers from minus 0 to minus 127 this shows that once complement representation has limitations 2 let us see signed two's complement representation the rule is the same as that for one's complement but here we take the two's complement of the positive counterpart here there are no conflicts for 0 that is why signed two's complement method is widely used let us do a few arithmetic with integers and view them in terms of integer representation following the rules of ordinary arithmetic the addition of two numbers in the signed magnitude system will be performed in the following way if the signs are the same we add the two magnitudes and give the sum the common sign if the signs are different we subtract the smaller magnitude from the larger and give the result the sign of the larger magnitude for example plus 25 if added to - 37 - of 37 + 25 is equal to - of 12 and is done by subtracting the smaller magnitude 25 from the larger magnitude 37 and using the sign of 37 for the sign of the result this is a process that requires the comparison of the signs and the magnitudes and then performing either addition or subtraction the rule for adding numbers in the signed two's complement system does not require a comparison of subtraction only addition and complementation the procedure is very simple and can be stated as follows add the two numbers including the sign bits and discard any carry out of the sign leftmost bit position note that negative numbers must initially be in two's complement and that if the sum obtained after the addition is negative it is in two's complement form on any addition the result may be larger dan can be held in the white size being used for example minus 65 plus minus 64 is equal to minus 129 which cannot be accommodated within 8 bits this condition is called overflow when overflow occurs the ALU must signal this fact so that no attempt is made to use the result to detect overflow the following rule is observed overflow rule if two numbers are added and they're both positive are both negative then overflow occurs if and only if the result has the opposite sign subtraction of two signed binary numbers when negative numbers are in two's complement form is faced simple and can be stated as follows take the two's complement of the subtrahend including the sign bit and add it to the minuend including the sign bit a carry out of the sign bit position is discarded for example the positive subtrahend 7 when subtracted from a positive minuend - 2 - 7 we get minus 5 this is because we take the two's complement of 7 and add it to 2 similarly 5 -2 -5 -2 5 - - - 7 - -7 + 6 - for this procedure stems from the fact that a subtraction operation can be changed to an addition operation if the sign of the subtrahend is changed this is demonstrated by the following relationship plus or minus a minus of plus b is equal to plus or minus a plus of minus b plus or minus a minus of minus b is equal to plus or minus a plus of plus P but changing a positive number to a negative number is easily done by taking its two's complement the reverse is also true because the complement of a negative number in complement form produces the equivalent positive number positive integers including zero can be represented as unsigned numbers however to represent negative integers we need a notation for negative values in ordinary arithmetic a negative number is indicated by a minus sign and a positive number by a plus sign it is customary to represent the sign with a bit placed in the leftmost position of the number the convention is to make the sign bit equal to zero for positive and to one for negative in addition to the sign a number may have a binary or decimal point there are two ways of specifying the position of the binary point in a register by giving it a fixed position or by employing a floating-point representation the fixed point method assumes that the binary point is always fixed in one position the two positions most widely used are a binary point in the extreme left of the register to make the stored number a fraction and a binary point in the extreme right of the register to make the stored number an integer in either case the binary point is not actually present but its presence is assumed from the fact that the number stored in the register is treated as a fraction or as an integer in order to represent positive 24.9 in the fixed representation format the number is broken into two parts one the fractional part and two the integer part I said the fraction is prefixed with a dart an integer is followed by dart the floating-point representation of a number has two parts the first part represents a signed fixed point number called the mantissa the second part designates the position of the decimal a binary point and is called the exponent the fixed point mantissa may be a fraction or an integer for example the decimal number plus six one three two point seven eight nine is represented in floating-point with a fraction and an exponent as follows fraction is plus zero point six one three two seven eight nine exponent plus zero for floating point is always interpreted to represent a number in the following form M into R to the power a fee-only the mantissa m and the exponent e are physically represented in the register including their signs the radix r and the radix point position of the mantissa are always assumed a floating point binary number is represented in a similar manner except that it uses base two for the exponent for example the binary number plus one zero zero one point 1 1 is represented with an 8 bit fraction and six bit exponent as follows fraction 0 1 0 0 1 1 1 0 exponent 0 0 0 1 0 0 the fraction has a 0 in the leftmost position to denote positive the binary point of the fraction follows the sign bit but is not shown in the register the Exponential's an equivalent binary number plus for the floating-point number is equivalent to a floating-point number is said to be normalized if the most significant digit of the mantissa is nonzero for example the decimal number 350 is normalized but 0 0 0 3 5 is not regardless of where the position of the radix point is assumed to be in the mantissa the number is normalized only if its leftmost digit is nonzero summary let's summarize the topic the ALU is third part of the computer that actually performs arithmetic and logical operations on the data the control unit provides signals that controls the operation of the ALU and the movement of the data into and out of the ALU overflow rule if two numbers are added in they're both positive or both negative then overflow occurs a fan only if if the result has an opposite side subtraction Jewell to subtract one number sets your hand from another minuend take the two's complement negation of the subtrahend and add it to the menu in assigned fixed point number cost the mantissa the position of the decimal of binary point called the exponents 
6olR3y_UHRo,27,"#COA#computer#lastmomenttuitions
Computer Organisation & Architecture Full Course- https://bit.ly/2lPFO8G

Engineering Mathematics 03 (VIdeos + Handmade Notes) - https://bit.ly/2GaM8yY

Branches Available: Comps, IT, Mechanical, EXTC, Electrical, Civil, Production, Instrumentation

Other Second Year Engineering Courses :

Semester 03 -

Engineering Mathematics 03 - https://bit.ly/2GaM8yY
Discrete Mathematics  - https://bit.ly/2kAtbOJ
Electronic Circuits & Communication  Fundamentals - https://bit.ly/2kdlmOu
Digital Logic Design & Analysis  - https://bit.ly/2Xb7y6x
Data structure : https://bit.ly/3pXRr9k

Semester 04 -

Engineering Mathematics 04 - https://bit.ly/2lVXj77
Analysis of Algorithms - https://bit.ly/2kLGKL8
Computer Graphics  - https://bit.ly/2mdTzy3
Operating Systems   - https://bit.ly/3g3eXxw

First Year Engineering Courses :

Engineering Mathematics 1 - https://bit.ly/2lUPezA
Engineering Physics 1 - https://bit.ly/2lRbElo
Engineering Chemistry 1 - https://bit.ly/2kAOAY2
Basic Electrical Engineering - https://bit.ly/2VPQlyW
Engineering Mechanics - https://bit.ly/2kNxjuH

Engineering Mathematics 2 - https://bit.ly/2kASuQG
Engineering Physics 2  - https://bit.ly/2kNy0UP 
Engineering Chemistry 2 - https://bit.ly/2kAtWr3
Engineering Drawing - https://bit.ly/2maYwHV
Structured Programming Approach - https://bit.ly/2kNxKVR

Exclusive Courses :
Engineering Mathematics 03 (VIdeos + Handmade Notes) - https://bit.ly/2GaM8yY
Aptitude Preparation (with tips & tricks + Notes) - https://bit.ly/2kmlyLA


Other Engineering Courses : 

Microprocessor - https://bit.ly/2mk7mDs
Database Management Systems - https://bit.ly/2lWJ4ir
Computer Networks - https://bit.ly/2mcoURH


System Programming & Compiler Construction - https://bit.ly/2ma4Xei
Cryptography & System Security - https://bit.ly/2mdw7kw
Data Warehousing & Mining - https://bit.ly/2PRCqoP
Machine Learning - https://bit.ly/2Xp4dmH
Software Engineering - https://bit.ly/2lRb9bb

Digital Signal & Image Processing - https://bit.ly/2lOqUzE
Mobile Communication &  Computing - https://bit.ly/2lOqKIy
Artificial Intelligence & Soft Computing - https://bit.ly/2mgKk0b
Big Data Analysis - https://bit.ly/2mdvPtW

Human Machine Interaction - https://bit.ly/2Ts3PRh
Wireless Adhoc Network - https://bit.ly/3gdW6Qs
Distributed Computing - https://bit.ly/2WQLoI0
Natural Language Processing - https://bit.ly/3cUcX8F

Technical & Placement Preparation :

Interview Series - https://bit.ly/2ki9U4l
Python (Beginners) - https://bit.ly/2mgLR6r
Arduino (Beginners) - https://bit.ly/2mj7dAb
UML Diagrams - https://bit.ly/2mj54od
SQL (Beginners) - https://bit.ly/3gcLoK6",2020-03-02T05:30:02Z,IO Module  in Hindi |  COA | Computer Organization and Architecture Lectures,https://i.ytimg.com/vi/6olR3y_UHRo/hqdefault.jpg,Last moment tuitions,PT11M49S,false,7882,195,9,0,25,[Music] so here friends welcome to last of my t-shirts where and I'm Jason it h may you see you acre fifth module kite topic of explain karoonga I am modulus cut detail about the hockey que hay que c'est comme cut there is K times some jenkki um look so Charlie a Shirou cut it to i/o module care basically are you guy amazingly IO koala taco I a model of questioning input-output model Elena Dana input model of care other input devices multiple devices you say I'm not yet a little or output devices MATLAB clear SAT messages comdata there three or the input output or a cough mirogoj om cosas Malka input/output devices co computer desert cadet car there which compose the i/o module so our model care I am ordered Kootenai the I am model is a device that acts as a bridge between computer system and i/o device mallamma - I am modulates yato device a a computer system computer system maked a parameter but the cpu are the hard disk Ottawa or i/o device Yamanaka a keyboard mouse webcam which gave each me as a bridge come cursor it comes a simple Alaska concept a beach photos are my easy size scale block diagram patata WABCO no fear yes um - I made a peripheral device I say that my neck webcam Brega either my mouth either made I keyboard raiga never cut already vs. if Quinta Pedro dinner at the boats are anything so II look a beach or a generator system busses system buses metal aapke buses MATLAB case of other roads roads MATLAB connect connections MATLAB Yampa rocker CPU Ram raiga CPU RAM in compete my communication bus serve the bus line so they ISM so in core is peripheral devices case at concurrent kata IO module so IO module say I'm CPU kitchen that you're part SEP whose calm look ESCO ferry Pharaoh's who kinetic art they go fear yeah I am modules other coach named will keep bridge edge or input devices call power output devices Co computer kinetic components cannot connect Carter Soho pub Co IO devices card F or I owe more look at academic which patellar oka home diptych Here I am for you to head its cofunction or requirement scale function MATLAB egg yo-yo device Kartika or Dasara he IOT was kisses are you not care cows keys a rule I am module key to fear parallelo control and timing me of course um jeito be some joy a processor but our ma a processor or he could stoke Omkara or yeh mera IO module a IO module a a biack address buses Minako pathology buses are used for a data transfer MATLAB buses bus lines accrued this all the transit data flow curse a particular case it was relegated toffee Robby TC comes say processor name is address bus cop occurred Erica but I am more dill Kimiko Jessica make you squeal a bus line kiss I wrote that or sub-module okay Apatow no Jen Exodus co-packer Kinnear except a kooky both important address without to Roger Nixon scope pecado Pisco pin a Malaga system hangar Jenga to attempt upon the use curse after the very hamper castle rata control and timing control Malacca egg time pay upon those who control concept so control kiss : i you can't decide kind of control or timing care kidney time the Galvan Tech chisca control a sec there so control and timing x03 upper they say come up is a routine a crane i/o module key the video notch a Lucena processor communication a be a scenario may occur some jerky I am modular processor cop was mechanic communication WA but Lobby I am model push a guy a by image address buzina processor bodega by tuples dominate rook may require model transfer car carrier 803 a cathode looking with my communication wanna know communication make important requirement io module today these red device communication a be some Jo sub could sort okay IBM era processor a MRI or modulate I a model cabbage commander barber bridge this act curtain or mera it Erick input device a for example keep Mira America Boreum Briscoe connected a para be Amira Indo Naga Santo connection established Permira is Cassatt connection established Hany either could either are America someone show up the device can't be communicate Colonel certainly enough I ve seen re Omega Rio model limp which occupy Terrace at Google Earth Orica those never lucky ha my a connection make which problem are I could resolve course Africa the device communication Honam is already a hampered three a biack IIM module key requirement Ivana bio model be little eggie process circus arts and device case of the communication wenatchee a this thread dated buffering I mean data buffering cavitus a key AB Copa de aqui I adore input/output system a kiss my use of the read read Kearney me I mean I met a processor ever persuade I got a man around me p.m. era IO module a OD ay Mira quite a device a the other data I say jaga but a validator is Arab leg other data is that up yoga mat Lamia Thor Ryoga yeah right over the couch Bo Suk there Behrami happy samyama either cpu sanitize my transfer Quran I know someday Pendragon magnetized my daughter o though I mean occurring awesome job yeah I am Adi linear my dearest - JA - JA raja raja raja raja rice a pulsar Anita there is no Takenaga diagram put up some two samples ready together but abuse mess eh eh oh dear Donna is Kyle sighs both benign the filly Charlie upload neo para or H Jota Jota data 420 Humber Portugal the field of Iquique t0 is my sequence important ragamala is kept a CPA Janacek variation Archie a si Nikita - NHA but about energy my situation my problem whether or not I be able to watch SEP message based a happy some Germany made those quick document with aperture patchy Amiga orders coming in message dollar a document Belgium in a whatsapp a patch Ambika or miners come messaged a lucky buy a document cool clinically but can become e cayuga brunettes fluid o en a pasta each a leap on Jetta S on Achaia a situation owner GA nae abuse core message document Kohlberg's could augment in a milena hope up with some jog amicable wrong situation make con Kamata buffer buffer can buff Eric showed us a partly yourself so who can rotate out the sequence mother who sequence koppikhar character or job tahini Guyana who is Connie base narrator a guru yoga - yoga philia yoga or Philly a yoga to buffer issues may help Gotham logo or last her error detection Abby error detection Domenico some Dothraki a besom Joe yes CPU a memory i/o devices oil hamper matter could stow input bodega dono bad characters si a Chinook say he said botany or para military a toe up now Joey I am model with now capable Rena cheeky we say bath correct visca el patch on a or mousse a Turkish wall curio a fear of course only when I was said oh I'm a pathetic risk ok her face where error codes give a course of my inner butterfly or a be your last day one of the king aka Joe I am modules and I io module skin types nicked a blue tabs be coffee important a by Tommy of CO is Capernaum a hot tea tell make video but I know Allah make a Cooper a a cake watch Makai a cake is what important day he smell on my way - take idea there's ok heika so Phil program die ok the program are you MATLAB a BSc P where yes CPU but our or yes I'm - yup now I am motivated or Yampa reg buzz - a trespass example they Roman ABI address was core CPU used car a or the address bus is so chilly but our coach Anya is Camilla near to a same path Caraga he scope Omega by the Austin America Korea a callable address bus Dominic Mirko Daytona Mego come address mascot to a CPU Cuyahoga yeah address buses Cody Arreaga a be address vs. Korea or CPU McCauley beta CPU couch Rell see Peter  badass upon ideas muskie Scalia I oh good idea but CPU her I'm a Czech car that I go keep my hot passionate my for example Jakarta gigabyte they're a combo Gannicus there are come was a lot of Maricopa the other night res bus summer job he smack our eggy jumpy amantha CPUs co-op not any other sub put or focal my daughter the doctor Cusco come me other no program I put some yoga be interrupted driven by Thoreau same scenario ampere yam panic CPU raiga yeah but I could address bus but I am but a coyote wisely go a be a CPU KVI or device comma lega key by the Austin America bus parchment come address bus but man cool a little coming opposite hunger the Philly is good another Canada tell squad response kxs the other but are you humping a foo cut narita yukawa I be water by up knock omkarnath some tech my couch or come reptile a tour no father and him Canada's copy CPU by Cebu cover Bicknell order there was I give a  are these kinda czar make or ooga may kisara couch Karenga or a garter I have okay address busca come to tumor quake interrupted a interrupting them at love me back message it already there acaba me acaba personally Eluga to interrupt driven to something or gap go Abby da make a direct memory access direct memory access model of camera on Omega s mana UI or device I know it's cause she pewter bad karma cabeza with me I gotta go good certo q Rodney a either do you are a donkey amber or a shooter at this Konami DMA okay was that I got a book would be J a Virgo push metal coke heater say you gotta gotta go cheese there Dumba some job codes to hope you do some de rigueur key input modules care is a programmed i/o interrupt three one eye or DMA parties here is coming next video my details of explain caraga a requirement sympathy Nikki Latakia some heika one of our model here care yeah yeah yeah bridge a kiss cabbage may CPU corner masala I was car part of a referral describe each my egg bridges at Curtin is a function scan control and timing s processor communication a control and timing MATLAB whopper smoke control on timing beside Carnegie Kiska Pascal quickness Amitha kreega processor Mottola processor I am Monica communication device communication with loved avascular communication data buffering care where they come look net error detection Monica or finally I open a sketch time to say K so hope a video of course and watch my yoga I got a video put so much my don't like / K static oh subscribe caramel mug Khulna update those numbers are the child share karma or essay or videos I never on it so thank you for watching all the best for your exams [Music] you 
VPqRDpuzzwQ,22,"Subject: Computer Science
Courses: Computer Architecture and Organisation",2019-02-05T05:43:08Z,Instruction Set Architecture,https://i.ytimg.com/vi/VPqRDpuzzwQ/hqdefault.jpg,Ch-13 Computer Science and Engineering,PT38M15S,false,60,3,0,0,0,[Music] welcome to the fifth lecture that is instruction set architecture here we are looking into a computer system in two programmers point of view like the assembly programmer that what as an assembly programmer we will have the view of the computer system so this instruction set architecture serves as an interface between the software and the hardware here by Hardware we mean the processor system like we need to know that in the processor what all registers we have and those registers supports what kind of features what kind of features are supported by those registers typically consists of information regarding programmers view of the architectures that is as I said the registers address and data buses etc and it also consists of the instruction set now many instruction set architectures are not specific to a particular computer architecture and they survive across generation like if you see Intel x86 series it it across architecture it has it has got no change so they survive across the generations so the various generation that they have developed they survived with that now let us see what are the important instructions set design issues that should be taken into consideration first is number of explicit operands by operands what do you mean let's say we have I have an instruction add add in is an instruction and add r1 I have another instruction add r1 comma R 2 I have another instruction add r1 comma R 2 comma r3 by number of explicit operands here we have a single operand specified in this particular instruction here we have two operands specified in this particular instruction and here we have three and if we have something where we do not specify any operand then there is no operand specified and we explicitly take some operands for this operation we will also see that so there can be 0 address instruction there can be 1 address instruction there can be to address instruction or there can be 3 address instruction location of operands now C by location of operand what do you mean by that add r1 comma LOC a r1 is a processor register so this is within processor location a is a location in memory so this is an operand which is present in memory and not in processor register so by location of the operands we mean either it is in a register or it is in accumulator or it is in memory so it can be either in registers or in accumulator or in memory we will see what is an accumulator in course of time now specification of operant location as I said just now that like add r1 that means whether this operand is a register or a memory location that a computer will never understand computer is a Lima so you have to specify to the computer that okay your first operand is a register so you have to look into a register to get the value so this will be an register address where you have to go and see the value now LOC a you have to specify that this is a memory location and you have to go to that memory location to access this value so by addressing modes is the way to specify the operands in your instruction so how the operands that is r1 and location a specified in a in this particular instruction is addressing mode and there can be various addressing modes like register immediate indirect relative etc we will see in detail later no size of the operands supported it can be a bite it can be half word it can be a word it can be a double x and supported operation by supported operation we mean that how many operation you want to specify and what kind of operation an operation can be of various types when I say ad mul sub these are all arithmetic operation I can also tell you some other operations like move these are data transfer operation load/store these are data transfer operations and there can be many other various kinds of operation that we will see but suppose I supported operation we mean that how many kinds of operation you are supporting in your architecture now this actually shows that evolution of instruction set initially in 1960s we were having accumulator based system by accumulator based system let me tell what it is when I write ad R 1 by this r1 will be added with what our one will be added with our register present in your processor called accumulator so accumulator is a register which if we have an instruction like add r1 by default the value of r1 is added with accumulator and the result is stowed back in accumulator itself so accumulator based accumulator based machines were there in 1960s so this is an accumulator based instruction where add X is written just now what I said the content of accumulator will be added with the content of location specified by X and the result will be stored in accumulator in 1960s to 70s another kind of instruction set emerged that is Stagg least what do you mean by stand based stack paste is a kind of this you have a portion of memory called stack this is your and this is your top of the stack and let's say some details are stored here one by one if you just say add and you don't specify any operand here then by default the first two elements of the stack will be taken out that is 5 n 10 will be added and will be stored back here so this becomes 15 by this what we mean is that we are doing some operation where we are loading the data we are storing the data in the stack and then we are performing the operation where we need not have to specify any operand by default the operands are taken from the stack so here it's a 0 address instruction where the top in the top of the stack we store the top of the stack plus the next we add those and we keep it in top of the stack next comes memory memory based in 70s and 80s and the representative system is IBM 360 with her which has both to address and three address instructions add a comma B where the data from memory location a is added with data from memory location B and the result is stored back in a similarly a3 address instruction where a and B are added and stored back in memory location a now we also have register memory based systems instruction set by register memory based system means one operant will be your register and one operant will be your memory location so here load or 1 comma X what do you mean by that load from X load from memory location pointed by X into r1 similarly we can also have store store the value of r3 into some other location and finally we have three address instructions where we are specifying three addresses and what does it do here r1 stores the value of r2 and r3 the result after addition is stored in our one so in a single step we can do this now let us see some example code sequence for executing some sample instruction that is z equals 2x plus y using the various instruction set architecture that i have told in the previous slide so let us consider this stack based machine so we have to perform this task z equals to X plus y so first we have to push X next we have to push Y and then both of these are now in stack talk to position on the stack when we perform this ad then these two values are taken out added and store back in the top of the stack and finally when we do pop then the value form top the stack is taken out and store back in set so let me explain here in this way so by push X X is added here push y Y is added here and then once we perform add X plus y is added and stored back here and then when we perform pop X then X is a memory location where we will add sorry Z pop Z Z is a memory location where we will take out the result of X plus y and store it in Scent next see an accumulator based system in an accumulator based system you have to if you have to perform the same operation Z equals to X plus y then what you have to do both x and y are some values in store in memory you have to load X so whatever value is in X will be loaded and will be stored in accumulator load X will load the value of location X and will store in accumulator and then add Y will actually add the content of location Y with accumulator and stowed back in accumulator so we can see in the ALU one value is coming from the accumulator and another is coming from memory we are adding these two values and we are storing back the result back in accumulator so all instructions assume that one of the operand and also the result is enough special register as I said called accumulator next you see registered memory machine in registered memory machine how this can be performed so we have a register and from memory some data's are loaded some data loaded load r2 comma X what it will do from our from X location X the data of location X will be loaded in r2 when we do add r2 comma Y in our to the content of our two which were which was nothing but X will be added with Y whatever value pointed by this location Y will be added and stowed back in r2 and finally we have to store this result our to in Z so we store this result r2 in set so here one of the operand is assumed to be in register and another in memory so that is why this is your Lu one of the operand is your register so value is coming from register another is coming from your memory and then finally the result is getting stored here in some register and finally the store will store back the result in some memory location now we see register register machine so in register register machine what you need to do is that you have to load everything into some registers first so here instead of doing add operation or any kind of operation if you want to perform you have to perform only on registers and not on any memory location that is why in register register machine you have to first load all the values of the locations memory location into some register that is why we are you using two back-to-back load in the first load value of x will be loaded in r1 in next the value of y will be loaded in r2 so now both my x and y value are loaded in two registers and now I will add these two registers and store the value in Part A now finally I have to store the result in Z so r3 will be stored in set this kind of architecture is also called load store architecture by load store architecture we mean that only these two instructions load and store will be used to access the memory no other instructions will be used to access the memory earlier what we have seen if you if you recall we have been using instruction like this let's add r1 comma a where a is a memory location so in this instruction you are allowing one register operation and one memory operation but in load store architecture what will happen you cannot access this so what you have to do you have to load a so load r1 comma a so in our 1 let us say or let us say R 2 comma a so in R 2 you have loaded the content of location a and then you can do add r1 comma R 2 but you cannot do add r1 comma a in load store architecture in load/store only you can use load and store store to access memory this is the memory this is a memory so you can only access load store instruction to access memory let us see about the general-purpose registers so if you if you recall older architectures have large number of special purpose register like we talked about program counter stack pointer some index register flag registers were also present accumulator we have been talking but in newer architectures we have more number of GPRS that is general-purpose registers and instead of having special purpose register most of the operations are performed using general-purpose registers and why that is so so the compiler have can assign some variables to registers so there are so many variables that can be used and they can be assigned into registers and registers are much faster than memory so once you load the data into the registers and you are performing operation within the register it will be much faster but once you have to load the data from your memory to register and then only you can perform the operation within register more compact instruction encoding as fewer bits are required to specify register so the instruction encoding can be much less why they are saying like see you are bringing everything into registers and the registers cannot be unlimited as compared to the memory addresses memory addresses a 32-bit memory addresses will have 32-bit but if you have 40 register or 100 registers how many bits do you require for 100 register you will require a maximum of 7 bits to encode that so that is what it means that more compact instruction encoding can be performed as fewer bits are required to specify registers so many processors have 32 or more general-purpose registers now coming to comparison between various architectural types so I've discussed about many machines tag-based accumulator based register register registered memory now let us do an comparison and figure out that which one is better than the other firstly we will do that with the example so we will be executing this particular instruction this particular instruction will be executed and we will see that with various architectures how many steps it takes to execute this particular instruction for stack based what you have to do so these are all memory location data so what we need to do so we have to first push a because we need to put this values into stack and then only we can perform the operation then we have to push B then we can specify the operation depth that will perform that particular operation and store back in the stack next we will do push a push C and push P we have pushed first a then B then C then B why we have done so just see we have performed this operation now we have to perform this operation for doing this operation we will first perform C into B and then name that is why we have first put a and then we have put C and B now when we do the next operation mul then what will happen B and C will be added multiplied and we stood back and now if we do a sub then a - beam to see will happen so we will get this particular value a minus B into C this has performed so we have performed this part we have already performed this part earlier which is stone in the stack now if you just perform a sub again then what will happen a by B minus a minus B into C will take place so now in my stack we have a by B - a - B - C this particular operation but finally we have to store this into a memory location by so we are storing this into y by doing a pop so pop will take out the result from top of the stack and put the result in to Y so how many steps basically we took to execute this 1 2 3 4 5 6 7 8 9 and 10 so we took ten steps next we see accumulator architecture in an accumulator architecture the typical instructions that we will be using is load store and of course along with that we can use other memory operation along with and so first we load C then we mul B so B is multiplied with C and stored in the accumulator but we have to store back the results somewhere because later we will be again using it so we store the result in D then we load a again then we sub D and we study so the operation a - seen to be is performed and yet story D now finally what we have to do we have to perform this and we have to subtract this from this so then again we load a and div B this will make a divided by B and it is stored in accumulator and then we do sub D so whatever is in D will be subtracted from whatever was in accumulator so what was in accumulator was a by B and then it gets subtracted and finally we store the result in Y so whatever was in accumulator is the result of this we stood back in Y and now let us see how many steps we require to execute this so we require one two three four five six seven eight nine and ten so we require ten steps let us see memory memory architecture so the typical instruction 3 instruction is R at X Y Z sub we can have this so if we can have this then what we are doing we are doing dividing a comma B and storing in D then we are doing C into B storing it in E then we are subtracting and E and then we are storing back in E and finally we are and B are divided so the there is a little wrong so this div will be D comma so just we this div will be D comma a by B so D contains a divided by B then we do mul mul we are doing C comma B so II will have C into B and finally we do sub e comma a comma E that means whatever is in we are subtracting we are storing it in E comma a minus e that is a - this seen to be so basically it will be D - e that we will be storing and so we have calculated the first part and finally an e by V was there in D so finally we do sub y comma D comma E where D is a by B - we do this part so it is basically a minus C into B so here to look into how many steps do we require to handle this we require 1 2 3 and 4 steps now typical instruction if it is two operand then how much step it would have been required so move decom I so first we are moving D to a then we are dividing it that is d equals to a by B we are dividing storing back in D again we are moving C to E and then we are multiplying C and B storing back in E and then we are dividing sub a comma E and finally we are doing sub D comma so the idea of showing one instruction and different operant is that you see the same instruction is requiring four steps to execute here and the same operation is requiring six steps to execute here but in this where it is requiring four steps we must clearly see that there are three operands and three operands our memory operand so how many memory operation we have to perform to execute this particular instruction first we need to fetch this instruction next we need to fetch a we need to fetch B and finally we are also storing back the result in D same way we are doing it here here so total for memory operations are required here and here how many memory operation required one to fetch this instruction and two more to get the data from here and depending on the instruction what we are doing those many instructions for writing it back into memory location we require one more so the idea here is that although the number of instructions required is less but may be in turn it depends on the number of memory operation that you are performing to execute an instruction next is load store in load store instruction only as I said the the instruction that will is required to access the memory is load and store so first of all we have three operands that is a B and C we will load that into three different registers first load into r1 load B into R 2 and node C into R 3 finally we do div where we divide a by B and store it in our 4 then we mul B and C that is r2 and r3 store it in our 5 then we sub R 1 and R 1 minus R 5 and store the result back in r5 so we got this one and finally we sub R 4 where we have stored our a divided by B or 4 comma R 5 so it will subtract our 4 minus r 5 and store back in r4 so finally the result is stored in R for which I have to store in Y location so from our 4 it will be transferred to Y C this load load load and store are the 4 memory operation that we are performing and all others are loading these instruction will be from memory but inside we are only performing register operations which will be much much faster that we will be seeing that how it is faster in later course of time so what are the pros and cons that we see so the load store architecture forms the basis of RISC instruction set architecture reduced instruction set computing instruction set architecture in this course we shall explore one such risk I see that is MIPS what it does it helps in reducing the memory traffic once the memory data are loaded into registers as you have seen that if only load and store instructions are used to access memory then we can load the data in a go using number of load operations and then we perform all the operations within processor so the processor will be performing all the operations with some register values because we have already loaded those data from your memory into the register and then finally if I have to store again back to register to some memory location I will be storing it back to some memory location so compiler once knows this can of course generate very efficient code and additional overhead for save and restore during procedure or interrupt calls and return will be there because now we have many registers to save and restore but again save and restore will be much more it won't be so much like what I wanted to say is that it is of course an additional overhead for saving and restoring during procedure call because you have to save the data and store but this can be done in an efficient way if we can store it in some other registers as well but although this is an overhead let me see so these are the pros and cons of registers so by this we came to the end of lecture 5 and we also came to the end of week 1 lecture so II summarize what we have studied in week one we started with how computer has evolved then we have seen that how an instruction can get executed so to execute an instruction we perform set of steps instructions data are stored in memory to execute it we have to bring it from memory to processor execute it and store it back we have also seen that what kind of software's are existing like application software and system softwares we have also seen that both von Neumann architecture and Howard architecture are required and in the last lecture we have seen that how this instruction set architectures has evolved what kind of machines were in use early stages accumulator based machine then stack based machine then finally how we are now in a stage where we perform some kind of operation faster so our thrust is how we can execute programs faster so in this course in the next lecture we will be seeing that how these concepts can be further used to enhance the speed of a computer thank you 
j8NnE1YeSN0,27,"In this video you will get to know about Von Neumann's Architecture. This is the earliest architecture also known as stored memory architecture. Students find this topic complex but after watching this video you will understand all the concepts clearly. These concepts are important for competitive exams like GATE, UGC NET, NIELIT, PSUs, DSSSB etc. and college/university exams also.
#storedMemoryConcept#VonNewmann'sArchitecture#COA

â–ºFull Course of Computer Architecture:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHMonh3G6QNKq53C6oNXGrX

Other subject playlist Link:
--------------------------------------------------------------------------------------------------------------------------------------
â–ºOperating System: 
https://www.youtube.com/playlist?list=PLxCzCOWd7aiGz9donHRrE9I3Mwn6XdP8p
â–ºDatabase Management System:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiFAN6I8CuViBuCdJgiOkT2Y
â–º Theory of Computation
https://www.youtube.com/playlist?list=PLxCzCOWd7aiFM9Lj5G9G_76adtyb4ef7i
â–ºArtificial Intelligence:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHGhOHV-nwb0HR5US5GFKFI
â–ºComputer Networks:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiGFBD2-2joCpWOLUrDLvVV_
â–ºDesign and Analysis of algorithms (DAA):
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHcmS4i14bI0VrMbZTUvlTa
â–ºStructured Query Language (SQL):
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHqU4HKL7-SITyuSIcD93id 

---------------------------------------------------------------------------------------------------------------------------------------

Our Social Media:
â–º Subscribe us on YouTube-https://www.youtube.com/gatesmashers
â–º Like Our page on Facebook -  https://www.facebook.com/gatesmashers
â–º Follow us on Instagram-https://www.instagram.com/gate.smashers

--------------------------------------------------------------------------------------------------------------------------------------
â–ºA small donation would help us continue making GREAT Lectures for you.
Be a Member & Give your Support on bellow link : https://www.youtube.com/channel/UCJihyK0A38SZ6SdJirEdIOw/join
UPI: gatesmashers@apl
â–ºFor any other Contribution like notes pdfs, feedback ,suggestion etc
gatesmashersconribution@gmail.com
â–ºFor Business Query
gatesmashers2018@gmail.com",2019-07-24T15:05:22Z,L-1.2: Von Neumann's Architecture | Stored Memory Concept in Computer Architecture,https://i.ytimg.com/vi/j8NnE1YeSN0/hqdefault.jpg,Gate Smashers,PT9M40S,false,217326,4218,66,0,197,hello though so gates matches webcast socket a at least you remember this is seminary Von Neumanns architecture yes cobalt is stored memory architecture yes stored memory program is schematic libcast storm america kid jaha p.m. doji Jim memory K under acting her aid to data of do sub program yeah disco boots to set of instructions data what luck the same program came to lifting a processing Kimberly Tennessee program a into a is equal to 10 B is equal to 20 C is equal to a plus B must look Jo B Mary variables a kind of variable zoom up a define Korea a constant defined career yeh jo Soraka Zara data him campus tour caring a memory can the consi memory main memory Java home use Kathy or do some caste or caring a program program at locked set of instructions instructions County a key come cook Casa Carmela both are a different type of instructions said you have Marcus available a like a graph news to Naga MBA move commander America's a double D add a sub branch statement I he sorry two commands and neglect processor kilakila commands in sorry command scope or is de taco kappa rockman a key memory mirakuru consume area main memory ha main memory can de a pisco a lot luck addresses pin except to couch addresses do a bar P up instructions for accepted couch addresses joab our PR accept a de taco is scheduled opposite Harvard architecture taja Von Neumanns k APPA Sita Fukunaga key instruction said Joe a Volm Alana memory American gay ordered data volume a lot memory American la coneja p.m. same memory key under Joe Abaddon OGG Eritrea or other up the ANSI deco art Kate I am GTA IV architecture use career on mayhem maximum career main memory key under him data or program Co Iraq they do Samara passata epic component but Olivia Von Neumanns diagram to a von Neumann architecture he basically heavy Beckett aqui con con say components a basic computer architecture k Joe help Kurt a data coprocessor lemme mclubbe input go out put coconut karna me input to output may convert condemn a poor goat second so a token may memory jump him though joke or a create data or instructions go Escobar at Amara CPU CPU central processing unit or yeah he said you have main camera startup that is Kimber Alohomora pass components at Pella automatic and logical unit but love Anu yeah Luka basically purpose care a new key and the boss re circuits you have available a digital Mia melodic circuits but they enter her home are percept actress Jim key helps him Katia addition subtraction multiplication division and operation or operation adsorb and honor Marquez like shift operation boss Raj operations from use Kathy automatic operation yes a logical operation in Co home preform catechist K through ALU Kayla or ALU mineral Alex circuits available on Joe help Kurt there in sorry operations Co perform their main then come re box and that registers registers oh hello basically care up the fastest memories your available on America's mohair resistors here temporary data Joe available temporary data cost over there or size you have a bot is smaller smallest size you have Oh memory concave all their registers or temporary leo have a de taco school career size get Rojo sector basically let's say eight bitkha not la egg by Tobia 16 bits qey to you basically Americus a sequence of bits of TM sequence of bits Co a.m. gavel tail registers Yap casick toki sequence of flip-flops kooky flip-flop video havoc bit coast or can make a comma there the multiple flip-flops co-op combined car doge a vocab on JK register clicking happy Joe important point over here Kiera district you use career job America's already main memory available to memory jump ahead to register cacao fiber or register cozy pukey chip KO / eq makea where Oscar is LS speed speed MATLAB Joby operation on perform Corral let's say is equal to 10 B is equal to 20 C is equal to a plus B a B AJ data handicapper a main memory may perform e cayuga automatic operation table a performer ALU Kember dojo Anu excuse me read a barrage of the first order main memory all the way memory be faster lick incompatible to a new body slower iboga cos speed again mismatch karegi to ink on the counter Roger they say or egg dear a camera obviously a Cooper bird an ayah to us bird and kokum karmically kochia registers registers kakete data could temporarily but luck Joe intermediate results and Joe output I input on Co intermediately or temporarily store curly Cataumet a boss our Allah glory Jesus the name of Lisa Nova although his coma Hajek a pure detail me disappearing a lake in Jessup program counter next instruction to execute carnea Oski Joe address McCune sorata program counter register escuela vimar passata accumulator the intermediate results on Co store Carnegie Kamata input resistor output registers an me are a memory addressable to use oh sorry registers a he helped car taken a ke processing cateura fast chemically or a mu key or main memory key may actress intermediate or cataract Tom Curren death the opposite a control unit control unit again brought the important here of effector control unit car control unit confirm r+ doechigi activate timing signal or dull throb opposite IRP control signals timing signal basically care timing MATLAB cones instruction Pirelli execute avocados kobato gimmick love let's say Americas program camera there are instructions of course instructions capella execute correct escoba me execute Karla Yi basically brought the important factor ho sakta kiya grop let's say this occurs relative up karma may Pataki instructions care pallium collected you have oppressed cart a theorem geared Althea fish cave Adam cavity raise your body ready retort a mclubbe heroic sequence of instruction a sequence of Jewish steps in co-op early China karna again in company checker Roberto output you have o Allah bow Satya to Islam cooker make aliyah made dedicated hardware chai a or more hardware basically care control you so timing signals your mo timing GOI a pigeon eat Cathy a hurry timing signal pay couch the couch action gyro perform or that demo purpose at the control signal control signals cab asically Fonda hi kiddo Ethan SR energies to serve our APIs input output system Electra both sorry registers an income read beaker 16 Kendra butcher ie to be Korea in sorrow cook Casa control Carmen a Saluki update Alexis Carrel or justice ke or busier but look you see or come to execute correct in sorry read write operations Co perform chemical energy still camber help me use Kerr take control signals go control signal up K mind Akita a yes a mirage Oh arms every legs their own cook a say move carbonic gas instruction on kodi neeya each instruction corn their armor of mind their - yaja page of control signals a control signal in cook control courtesy Cacioppo resistors go and read colonel access car microscope and like a microscope odd way to a control unit ERP bought the important job major role a vocal a current is von Neumann architecture best to body basics architecture in 1945 create IMPD aware lake in Agra you happy me into point enough Roy yeck data Jo hey Cassie process of that hibari passage of main memory camera of data for active awesome data call Curtis CPU Kanda or CPM registers code process Kathy well of ALU Joe Hammond coprocessor K output cote d'ivoire amid memory made attack or main memory save on next job a viola user cover a trap a showgirl yeah yes ii remember a my store car money is 3 km data co-processor 30 up Kouhei be healthcare pair up to pathology as a computer science people key are calm Jota case a computer Kevlar - yoohoo Scooby hop a choker - yes are a main component Sasuke both America's input/output system Battle of America's peripheral devices input devices like keyboard monitors or output device ever a person monitor to eat oh sorry device is an overnight next level pietÃ  so basically you Marcus yay Joe main architectural yay both attacking components Co in components calf either care about mean components cook Cassie attach cursor team attach can make a MATLAB a kid network topology the same networks my part a a mesh topology have bus topology our topology mohel Katya a lot of components cuckoo can main memory be hibari pass registers me a control you read her Mottola be pure CP we're in Kona casick connect Carla Hubble Co correct car their bus K through there are different types of buses but love address buzzer Mary pass data bus a allegoric type Cuba since our intercom cassia iam Barney this is Karen gig next video may yeah uh basically implemented a multiplexer ski helps a to each of ornaments diagram for newbies architecture a basically a quiz me a Africa data stored memory program memory candor Kurata sale or a scale of America's core Kunze components a or own component star find occur who Cassie performed at this so this is all about the Von Neumanns architecture thank you 
jJ8BSfULP3E,27,"What is IAS architecture? 
Here I have tried to explain the basic functions and features of IAS Architecture.

-----------------------------------------------------------------------------------------------------------

If you are looking for a course on #ComputerArchitecture follow my playlist....

Computer Architecture playlist - https://goo.gl/7qqJ6X

-----------------------------------------------------------------------------------------------------------

Go to my channel - https://goo.gl/B53iZL
and check out other playlists on 
Tech hacks -https://goo.gl/cBB6tf 

-----------------------------------------------------------------------------------------------------------

""TheÂ IAS machineÂ was the first electronicÂ computerÂ to be built at theÂ Institute for Advanced StudyÂ (IAS) inÂ Princeton, New Jersey, U.S. It is sometimes called the von Neumann machine, since the paper describing its design was edited byÂ John von Neumann, a mathematics professor at both Princeton University and IAS. The computer was built from late 1945 until 1951 under his direction.Â The general organization is calledÂ Von Neumann architecture, even though it was both conceived and implemented by others. "" 
-Wikipedia




This Video was made using Bandicam.",2017-09-24T11:58:43Z,IAS Architecture - COMPUTER ARCHITECTURE AND ORGANISATION - 1,https://i.ytimg.com/vi/jJ8BSfULP3E/hqdefault.jpg,Ritveak Dugar,PT15M44S,false,12113,193,15,0,35,[Music] hi this is worthless I'm from vit and I'm making this video as a part of my digital assignment for the subject computer architecture and an organization so the topic that I that I am going to deal with is is architecture this is very important for this computer architecture subject so stay tuned and see if I can explain it to you so let's begin with introduction basically computer architecture is the computer structure which we have for any machine make its the computer architecture contains all the instruction formats or addressing modes or instruction sets and general organization of all the CPU registers so here we go what happens is in the previous systems the works which we did was extremely tedious so we came up with a better system and that was is computer it is also known as one human machine so it had a new concept and which was known as stored program concept what it says is that the program which is stored in a computer in the computer is stored along with some relevant data so the therefore it has three parts the main memory the CPU and the input or output equipment so what happens actually is that program counter contains the instruction of next address of the next instruction and then which is actually stored in main memory and then the commands are sent to the arithmetic logic unit it processes the data the command and it gives the output memory of is so memory of is is the capacity of memory is accessed in terms of words so there a total of thousand storage locations in is with that means thousand words for each word there is like 40 bits and you can use those 40 bits for either storing an instruction or a number if you store a number it will the first bit will contain the sign bit either it is positive or negative and the rest of 39 bits store the value and if you are storing an instruction then it has two parts of code and address you can see here the opcode part stores 8 bits and address power stores 12 bits so therefore we can have two instructions in a word the left instruction from 0 to 20 and the right instruction from 20 to 39 and if we store a number we have one sign bit and from 0 to 1 of course and from 1 to 39 we have the value bit value in one human machine we have very various type of registers so let's see one by one the first one is MB our memory buffer register here it is what happens is it contains the word which is to be stored in memory or which is just received from memory like it is yeah so MB are directly talks it directly has contact contact with main memory so main memory gives things to MBA and MBA gives things back to the memory although MBR is you can say MBR is the central point because it is rightly connected to the mathematical logic unit as well as main memory as well as other registers and input/output equipment also so basically mb r mb r the memory buffer register is the central point of IES architecture now let's see memory address register address register is here it basically stores the address in address which is being given by MBR for PC that means like it stores the address which is to be accessed from the main memory the next one is instruction register instruction register as as you can recall that I just told you in an instruction there are two parts up code and address so the OP code which is 8-bit is stored in IR like whenever an instruction goes to MBR from the main memory it comes and it gets bifurcated from okay I'll come to that part later now let's see instruction buffer register it is also a register which stores the right instruction here you can see there's one right instruction and one left instruction so the right hand side of the instruction is stored in IBR and the left hand side is again diversed into opcode and address the opcode isn't stored in higher and address in ma are the PC of course stores the address of the next instruction and AC the accumulator and multiplier question holds the result of the arithmetic logic out holds the result which is given out by ALU and there are the various instruction sets like as I told up code stored up code needs 8-bit so here 1 2 3 4 5 6 7 8 8 bits for each instruction there is a unique hock code since all the machines works on like is works on binary digits so the each we can we can't just write load we have to use a binary up code for it like a binary code for it which is known as opcode so for this load which have only one you can say one part has this OP code whereas the load which has two parts has this awkward so you can see even a single thing like load has different kinds of it like load MQ what it does is basically loads the content which is stored in MQ to the accumulator and load mq comma M X what it does is it it loads the value of the data presented X location in the memory to MQ so each load has a different function so hence has a different opcode similarly there is store and jump jump plus there are different instruction sets overall we have divided we can divide instruction set into few parts like data transfer load store these basically transfer the data from one register to another and then there is unconditional branch which means we have to take like this jump excuse me jump what it does is basically when this condition master when this condition matched then you jump from this instruction to death that is unconditional similarly conditional then we have arithmetic instructions like add subtract multiply divide left shift right shift okay these two are ins interesting because it doesn't have any you can say it doesn't have any part on in on its right side what it does is basically LSH left shift right multiplies the accumulator content by two it basically shifts the contents one one bit left so it multiplies it by two and right shift divides the content by two and then there is address modify we can modify the address and all these over the is instruction set now let's see them so now let's see how the memory thing works in is let this be a question let me this be a problem for us so the first word is this it it has two values two instructions add MX 501 which is the right instruction and load MX 500 that is that right lift insertion similarly word 2 has store MX 500 and any other instruction in the right path so this is our problem and since we have used at the memory locations 500 and 500 1 we have a value stored at it so in at memory location 500 which is present at mimic mean memory it stores 3 and location number 501 stores a value for n Maimon main memory and here in this table you can see the current contents of all the registers so before starting the machine the PC the program counter holds the value address of the next instruction since the next instruction is a word which is stored at memory location 1 so PC contains value 1 so let's see how it works first of all PC gives the value of 1 or the next instruction to Mao Mao is nothing but memory address register so now ma also contains the value 1 amia gives that in address 2 main memory it says main memory just give the values present at word 1 - mb a-- so now ambien has the instruction the whole word the left part as well as right part so M it it is the work of mb a-- that at bifurcates these two instructions and give the left part 2 sorry yeah so it gives the right part to idea and the left part to irn MA so the a demux 501 is given to IBF which was the right instruction and the left instruction is now divided into two opcode as well as the address so address is given to Ma and opcode to aya aya contains the value 500 so it is the work of MA are two further center two and main memory then main memory C is okay the address is 500 and the value is three so I will give the three value 3-2 MB are now MBR has only three it gives two since the work for MBA is now to pass it to air you so Lu gives the value three to accumulator an accumulator content is now updated you can see here it has now three now this atom X thing is given to iron so before this just see the instruction register had the value had the opcode load so what it did is load the value present at MX to accumulator so the value three is loaded to accumulator now the left side instruction which was add MX 501 is again divided into two parts the opcode and haya and the head rest in ma are now address again amir gives this 501 value to main memory and since the first word is being completed like the first instruction is being done so the PC content is updated to 2 because the next instruction after this would be the second world so ma are now gives the address value to mean Marie may mean memory again access the exercise and processes it to before so the for value is again given to MBR now the command is add and the value is 4 so the AC content will be added with the value 4 and hence it has updated to be 7 and this process goes on now the again PC gives the well the address of the next instruction which is to come a hurricane the observed to main memory main memory again gives the whole world to MBR MBR again but forgets the instruction and this process goes on and that is how the main thing the memory thing happens in is so let's see the instruction format for make in every instruction there are two parts the awkward part and the address now we can do a lot of things with that the opcode has eight bits only but the address part we can have either one Nitra one address to an address or three address so how it works is like this if we have one address like add X then it basically adds the value present at X X memory location so the data presented at memory location X is added to AC and AC content is updated when we have two registers like add r1 and r2 then the r1 content is updated by adding r1 and r2 similarly if we have three registers like add r1 r2 and r3 then r1 content is updated by the value R 2 plus R 3 so this is how it works 3 to raise it to address instructions or 3 address instructions and then there is one 0 address instructions also like it is basically done for staff purposes like it it has a zero address we don't have to add anything after add like if we have pooka we push a into the stack and then we push B so the stack now has a and then above it it has B now if we give a command add it will add a and B and the value would be stored so that is how 0 at this instruction works now there are different addressing modes like you can directly add give the data to a command like add 40 so it will add 40 to the accumulator or we can give a register add r1 width which means that add the contents of r1 to accumulator we can also write things like add R 1 comma R 2 that means add the content of R 2 and R 1 and store it to R 1 so that is how addressing modes works so there are different type of addressing modes employed immediate register is showing direct order increment or decrement what it does is it basically increments the value present and AC by one so Auto increment increases by one or two document decreases by one and then there is direct addressing indirect addressing relative index pacers you can read all of them you can just pause the video and read the contents that's all for now thanks for watching I know I stammered a bit at places but that was my first time so sorry for that forgive me I hope I can help make more videos if the responses are good please let me know if you have any query drop your questions in the comment section thanks and do subscribe and like the video 
sZjSBFceV_o,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2017 (https://safari.ethz.ch/architecture/fall2017)
Lecture 4: Main Memory and DRAM Fundamentals
Lecturer: Professor Onur Mutlu (http://people.inf.ethz.ch/omutlu)
Date: September 28, 2017
Slides (ppt): https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=onur-comparch-fall2017-lecture4-mainmemoryanddramfundamentals-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2017/lib/exe/fetch.php?media=onur-comparch-fall2017-lecture4-mainmemoryanddramfundamentals-afterlecture.pdf",2017-09-29T09:36:52Z,"Computer Architecture - Lecture 4: Main Memory and DRAM Fundamentals (ETH ZÃ¼rich, Fall 2017)",https://i.ytimg.com/vi/sZjSBFceV_o/hqdefault.jpg,Onur Mutlu Lectures,PT2H29M39S,false,4374,33,0,0,1,"let's get started what sounds better it there's no shrieking voice or sound coming from coming from the static okay we're going to continue from where we left off last time although I did some reordering in the lectures that were planned I gave you a static schedule earlier but we're going to do a different dynamic schedule just because how things worked out so before we start I office hours tomorrow if you want to stop by for some reason stop by during that time and if you need to meet me for some other reason you should schedule schedule accordingly and you can do that over email or by talking to me of course email is not always reliable so you can transmit but you may not get a response right it's good to retransmit multiple times or find some other way of doing things okay this is what we covered last time in the last lecture we talked a lot about caching that's why today I want to switch to something different and we're going to come back to caching later we talked about effective cache design memory level parallelism and miss buffers we we stopped at the Miss buffers hopefully you know all of these concepts all of these are important concepts in today's micro processors today this is the goal I stopped over here actually enabling high bandwidth memories we'll talk about high bandwidth memories a little bit because that's everywhere caches register files memory you need high bandwidth and there are some fundamental techniques to enable high bandwidth memories we'll look at the trade-offs and then we will start main memory I promised that we would start multi-core issues and caching but I will defer that to future because multi-core issues and memory and caching is best covered later on in my opinion and I think it's good for you to get a broader view of the main memory system before we cover the resource sharing quality of service type of issues and we will first I'll give you a broad perspective some of the issues some of it you've seen before but some of it you have not seen and then we'll talk about DN fundamentals and operation and memory controllers does that sound good okay it's an aggressive agenda so we'll see how much of it we'll cover okay this is a review of one thing that we've studied last time basically for you to just refresh your minds we're not going to talk about this in this lecture but we talk about hybrid cash replacement but this is an example of heterogeneous policy because no single policy provides the highest performance and as we've discussed we the idea here was to implement both policies and pick the one that is expected to perform best at runtime so you do it dynamically basically and we discussed how to do that right there are trade-offs associated with it and this is a general thing basically whenever you have two different policies usually you have different workloads doing well with the different policies and maybe you have different times during the execution of work both that is doing well with a particular policy and sometimes doing well with some other particular policy so the idea is to implement multiple policies and select the best during runtime and you will see this over and over and we will talk about this in the memory system also this doesn't apply only to policies but to different technologies for example or two different things if you will two different cores also does it make sense to have a large core and replicate it to get a multi-core system or does it make sense to get a small core and replicate it for a multi-core system well why not have both types both a large core and a small core and when the workload needs the large core use the large core when the workload needs the small core use the small core this is already happening arms big little system is an example of it but as I said much earlier IBM implemented something like this I heard your genius multi core processor so that the concept of heterogeneity is very pervasive in the systems we designed today so it's going to things are going to become more heterogeneous and this is one example of heterogeneous policies and heterogeneity and hybrid same idea okay so you have this assigned reading which we didn't give you a due date yet but we're going to put the required readings on the hot hot crap system it's called hot CRP but the person who developed it actually called the top cap that's actually the conference review package it's basically that's that's why CRP but you can add an at a in between it's it's that's the system that's used by almost all major conferences to review papers so we're gonna put put all of the required papers over there so that you can do their you use as we've discussed how many of you have logged into the system okay good is it easy to use okay what should be pretty reasonable okay we don't have due dates on some of these but feel free to do them I mean we'll eventually give you yeah I know you're not here for the grades but will eventually give you extra credit for the for the stuff that you do don't worry okay let's jump into enabling high bandwidth memories so why do we want high bandwidth well one one reason is processors can generate multiple cache and memory access per cycle and we've seen how to support that in the cache right this is the idea of memory level parallelism and also non-blocking caches support that memory level parallelism so how do we ensure the cache and memory can handle multiple access in the same clock cycle we talked about the Miss buffers but there should be more right the substrate itself needs to deliver you multiple accesses so they will go over several solutions a combination of which is implement in multiple systems some of them are harder to do at this point true most reporting virtual multi porting multiple cache copies and banking interleaving so true multi porting and banking are very heavily used in today's systems but these other solutions are interesting solutions and they give you the space of architecting memories and they were used at some point in the past maybe they're used also today in some in some designs who knows okay the first idea if you want to access this is the memory itself if you want to access this memory itself in this case it's an SRAM cell as you can see right it's two cross-coupled inverters and then to access transistors over here this is one port right you have to access transistors such that you have the bit line and the bit line bar and then you can sense that data now if you want to be able to have multiple accesses you have multiple ports right this is called true multi working each memory cell has multiple read or write ports in this case they read portrait so maybe when you're reading from this cell another cell you're reading using the other port bit line two that way you can actually sustain two accesses from two different cells that's the idea now what is the downside of this as you can see well what's the upside you can perfectly get two axes you're eating from this cell using this bit line you're using from the reading from some other cell using the other bit line so you have to truly concurrent accesses that way right you multi ported the cell of course this comes at a cost the cost is latency power and area well clearly you've doubled the access circuitry as you can see over here instead of one access transistor on each side you have to access transistor instead of one bit line nice idea to bit-lines and also you need increased latency because now you're sensing the same parts in the circuit and so you have actually extra electrical loading over here and the power also increases and to make it work as fast as a single port you actually need to increase the area as well so this is expensive and also there are other issues related to this which is what if you what if you want to read and write to the same location at the same time well this doesn't help you with that at all in fact this makes your life harder if you actually do the wrong thing so there's no other way of fixing this other than having some peripheral logic that detects the fact that you need to read and write from the same location at the same time so you really need the dependency check logic if you will and that could be implemented outside the memory or inside the memory that depends right for example you could figure that out before you send the memory access into the memory and say oh I'm going to do the read first and then the write first depending on what the order is required from the program but one way of doing it inside the memory is as you can see you basically have some semaphore cells that say oh if you're going to read and write at the same time you need to you cannot do it you need to order it somehow that's one example that I've picked up from the web but that's the idea you have dual port but the semaphore cells need to ensure that you cannot do read and write to the same cell at the same time and this is one example of this but there are other ways of doing it so CPU can arbitrate and ensure that the memory is not written and read from the same location at the same time or written and written at the same time so that's a dependency okay so that's two part multi porting and this is one way of supporting multiple accesses but there are many other ways one other is which again an engineer is someone who does for a dime what any fool can do for a dollar right well we don't want that extra port how do we get it well we were truly a multi-port the idea over here is timeshare a single port if you can afford to do it do the first access in the first half of the clock cycle and do the second access in the second half of the clock cycle let's say that way you can actually do two accesses in a given cycle you need to design your memory to support it and your acts each access need to be significantly shorter than its clock cycle in this case right basically need to make this work I'm not going to go into the details of how to make it work but it's possible to make it work and it's actually used in one of the flagship processors of its time alpha 21 to 64 so that you can get high bandwidth from the data cache of that processor this was the fastest processor it's had the highest frequency of its time like 500 megahertz in the 1990s 1996 or so and there's a beautiful paper about it which I may recommend later on of course the upside is clearly you don't have an extra port but you need some design complexity to make sure that this works the downside is is a scalable what if you want four accesses per cycle well can you really sustain to have only one-fourth of the clock cycle and achieve one access and that 1/4 but if you have 1/16 accesses per clock cycle 3264 clearly this is not scalable right well it could be scalable if your clock cycles really long but if you're designing good system hopefully that's not the case right and usually the data cache access is one of the determinants of your clock cycle so this is hard to scale and why would we want 64 accesses per clock cycle let's think about a superscalar processor where you can issue let's say eight instructions per cycle at a given time right and they only tax us two registers so eight times two is sixteen you already have sixteen accesses per cycle to the register file okay so another solution which is perhaps more scalable than this one is this basically the picture shows it all instead of having one copy have two copies of memory in this case cache it could be register file it could be whatever if you have two copies that are identical you can do the first load from this copy and the second load from this copy right essentially you can do two loads per cycle in this case that's good then of course you need to ensure that the data is consistent so if you have a store it updates both locations both replicas if you will and that becomes the bottleneck over here right loads can proceed in parallel though that's nice and this is also used in a previous version of the alpha processor 2164 which is not the out of order version but in order super scalar version and they essentially did this make sense right cool again there's a scalability issue over here if you want to scale this to many many ports now your area increases proportionally right before it was time your time needs to diminish proportionally now it's space right space needs increased proportionally to support the ports and also store operations caused a bottleneck right now you need to actually show ensure that the store rights to all of those copies okay this is interesting because actually something like this not not exact like this but something like this was also used in 21 to 64 when I talk about 21 to 64 that's one of the very interesting out of order execution process that's why I keep talking about that and you had the register file and there were two copies of the register file in different clusters and they didn't keep them perfectly consistent meaning a store could write to this register file and that stores data would go to the other register file only one cycle later so it was the compilers job to ensure that nobody read this register file before it was written - so I needed a scheduler and the compiler compiles so ensure that the instructions are ordered such way such that the store that was done in this cluster the dependent instruction that is depend on a store that it dependents an instruction that's dependent on a store that was done in this cluster was either sent to this cluster or if it were sent to this cluster it were delayed by at least one cycle from the execution of the store over here so here you can see you can actually play tricks right if you punt the problem on the software the software can try to make your life easier the store doesn't need to get reflected at the same cycle over here or you can perfectly partition you can say well I'm not going to have all of the stores update here you don't have a perfect replicas done right okay so the final solution that I'm going to discuss that's used in most processors today in some form is banking and that's essentially this but it's not a copy anymore it's really a partition basically you take the address space and you partition it into separate banks addresses even at is let's say even addresses go here all addresses go here you don't have this coherence problem between the stores because a store only updates data that's over here right and bits in address the terms which Bank and address maps to in this case you have two ports so a bit a single business necessary so which piece to use for bank address is an issue right this is similar to which bits to use for index into a cache because here again you can get conflicts we'll talk about that in a little bit okay so up side over here is no increase in data store area because you're partitioning it you're not replicating it so that's a big upside the downside that you get is if two accesses go to this location so you want to support two accesses from a load two different loads if they happen to access even addresses at the same time well tough luck you have to serialize them you cannot serve them in parallel so that's called a bank conflict and the other issue over here is you need to have some interconnects in the input and output to do the routing right here when you have the replicas you don't need an interconnect you can send any load over here and they get serviced because all of the addresses are here whereas here you send a load over here that happens to be generated but if it maps over here you need to have a way of routing it that way right because it's not a replica it's a partition okay and usually it's a crossbar interconnect but it doesn't have to be we'll talk about interconnects later in lecture but if it's a crossbar interconnect and you you make life easier of course okay crossbar means any input is connected to any output directly okay so a bank conflicts we've discussed this basically if you have concurrent requests to the same bank you have a conflict I need to somehow deal with it one way of dealing with it is having peripheral logic control logic that serializes the accesses right and that's usually how it's done but there are other ways other potential ways of dealing it for example one from one thing we've discussed is minimizing it's right how do you minimize the bank conflicts again go back to the cashing lecture last time you can use hashing in the bank to address the bank bits right you can use a randomizing hash function that minimizes the probability of Bank ax Bank conflicts certainly that's possible that's one way of doing it in hardware or maybe in software you can somehow try to schedule instructions such that things that are accessing even addresses are not scheduled at the same time right that's one way of doing it that's possible also so again there are many potential ways of reducing it okay what else do I want to say so this is a scalable solution it's more scalable it's as scalable as this interconnect but at least space is more scalable because you're partitioning the space you're not replicating the space but now your interconnect becomes a bottleneck as you keep scaling so you there's no free lunch if you up all of these are different trade-offs makes okay we'll see this concept in this lecture more and more because memory is heavily banked today for example register files are heavily banked l1 caches l2 caches all of those are heavily banked to support multiple accesses I mean there's another benefit of doing this sort of banking you can turn off banks rights today we're in a power limited environment and if you predict that there is not going to be no access to a bank you can turn it off for a while you can turn off the clock such that you're not wasting energy and power and if you for example happen to find out that you're not accessing one bank for a long time and you don't need the parallelism for some reason you can turned off for longer periods of time so these tricks are employed in modern systems because we're very power limited it was developed for other reasons of course but you can actually take advantage of the fact that things are partitioned for power savings as well ok so this is what I just described is really a general principle it's called banking well not the banking in finance it's a different sort of banking maybe not as lucrative maybe this I don't know but it's also called interleaving and it's called it's called interleaving because what you're really doing is you're interleaving addresses across these different partitions so even addresses go here all registers go here but that's not the only way of interleaving these rights you can say Bank 0 contains addresses that's at the top half of memory 0 through n divided by 2 minus 1 and Bank 1 contains all of them memory that's at the bottom half for the top half whatever the other health right so that's another way of interleaving the questions which one is better right and the other way of interleaving is having a hash function I thought actually randomized those things ok so let me let me give you another look at this problem because there is one reason for banking is high bandwidth but it's also good for low latency and if you start from a different perspective your perspective is you want a huge memory and you can design a huge array let's say instead of having two areas like this you basically have a huge long race now the problem is that that monolithic memory array takes too long to access because you need to activate the word line into actually it's a bit line and that bit line is long and that interconnect is one of the major limiters of latency today that bit lines is long and it also doesn't enable multiple axes in parallel if you have a single array single portrait so if you want to reduce the latency of the memory area access and also enable multiple axes in parallel you go with banking you divide the area into multiple banks that can be accessed independently either in the same cycle or in consecutive cycles and each Bank is smaller than the entire memory storage and access latency is two different banks can be overlapped that way make sense so that's another perspective right first I give you the perspective of if you want multiple memory access you partition this array but if you want lower latency you also partition that arrays okay a key issue same issue as before how do you map data to different banks how do you interleave data across banks so there's something over here that I didn't really discuss this picture doesn't depict it really well and this picture shows the crossbar interconnect maybe you can get rid of this crossbar interconnect if your accesses doesn't have to be concurrent in the exact same cycle if you can afford to start one access in one cycle and the next accident the other cycle in the next tax is on the other cycle you don't need an interconnect well at least you don't need to have multiple buses inputs and outputs that way you still need to route the data but you don't need to exercise all of them at the same time right so you can for example deal with a bus that way you need some sort of interconnect of course right maybe not a cross part so you can have a single bus that is connected to Bank 0 and bank 1 and at a given cycle only one of them loads that bus make sense so for example for the inputs let's say you have the address bus and it's not a crossbar anymore you send one request and then only one of them one of that requests gets enabled into this Bank because you need an enable signal that enables this Bank but the bus also drives this one but you don't enable it you disable it basically an example it could be a single bus over here and the output could be a single bus only the bank that is providing data in that cycle can enable can drive that bus make sense but this is you can you can do this trick only if you don't need truly concurrent access in the same cycle you Bates could start one axis in one cycle the next axis in the next cycle and the Nexus in x-axis in the next cycle as long as the first access goes to bank 0 the next axis goes to bank 1 the data of the first axis comes 1 cycle after the bank latency and the data for the next axis comes 1 cycle after that you can almost paralyzed both of the accesses but starting is happening every other cycle make sense hopefully that's clear ok good any questions you can shout at this moment ok so basically that's what this is referring to you either start accesses in consecutive cycles and still overlap most of the latency if your bank capital agency is dominant or in the same cycle in some cases you cannot afford to start accessing consecutive cycles rights for example if you're accessing your register file if you designed your processor to issue 8 instructions per cycle if you serialize them you're going to lose a lot of the benefit of that so there you really need multiple accesses per cycle so this may not be a good idea starting those accessing consecutive cycles whereas in the memory sight and the register file access is also short compared to how long it takes to start the access right but if you are on the DRAM side as we will see the latency of the bank is very long to begin with so what is another cycle to start the access you start the first axis in one cycle the next axis on the other cycle for example and also there are some other issues that we will discuss so if you if you have this crossbar as you can see it's costly if you want to have eight banks for example you need to have eight inputs and eight places that you can connect each of those inputs to but if you have a single bus it's just a single line and just some drivers light right well that's simple will you what this will be more clear when we cover the interconnects so the cost is much less right if you are able to start accesses every other cycle your cost is you can make your cost much less and that's one of the main reasons why I am in the main memory will that start accessing consecutive cycles whereas in the register file we need to start accesses usually in the same cycle ok any thoughts questions burning hopefully this is simple relatively good but this is I think very interesting because it gives you all the interesting trade-offs design trade-offs that you can make how many of you have thought about virtual market multi porting in the past doing one access in health of the cycle in the next axis in the next self of a cycle pretty cute idea right if you can make it work if your if your clock cycles long enough ok so they're a bunch of readings but not all of these are required this is only if you're interested well this is required I would recommend this one pager that's really coined the term memory level parallelism it's by nd glue it's in the wild and crazy ideas session it's a one page document that talks about why MLP memory level parallelism is much more important than instruction level parallelism from his perspective and some of these cover the memory loss URLs and we're going to talk about some of these later on ok let's jump to the main memory system so well I'll give you some more readings again this is not required I'll talk about the required ones this is recommended there's a long reading that talks about the state-of-the-art in main memory circuit 2015 there's a shorter version but that's even older so I wouldn't I would recommend the longer one so we're going to look at dear I'm quite a bit and there's no real good book that talks about this in my opinion at least but you can look at some of the articles that we have published and I'll give you some particular sections of some of the work so if you really want to quickly understand how DRM operates internally I'd recommend sections 1 and 2 of this paper tiered latency diagram which we may cover at some point and sections 1 and 2 of this paper a case for summary level parallelism indium which we may also cover at some point and we've also this is one of your potential required readings so if you read section 1 and 2 of this paper the Rader paper you have a good idea of the refresh problem anyway ok and so we'll talk about simulation and you'll do some simulation later laps so it's one of the questions we always have in system design is how do you evaluate future systems or main memory systems so this is a reading about an open source simulator it's brief description that's actually widely used by both industry and academia at this point and we may use it in some of the some of the assignments here but regardless of whether or not we may use it you're going to build a simulator that's kind of similar but at what probably simply but you can read this four-page paper also to get an idea of the DRAM landscape and I'm going to reference it later on as well but I would like to first give you a broad overview of the main memory system to give you the problems because since we started this course with memory rights caches are such a fundamental part and they actually play into this also but main memory is an even more important part right now in in systems and I'll give you the perspective from four four different ways the first perspective is the performance perspective actually I've given you one of the perspectives the security perspective earlier right with the Rope hammer problem I'm going to come back to that but one of the reasons main memory is important is it's really a limiter of performance so this is something that was said by dick Seitz the excites is actually the original alpha architect so alph architecture to give you a little bit of history I talked about alpha now let me give you the history so there there was this architecture called wax virtual address extensions from Digital Equipment Corporation that was one of the dominant architectures for late 1970s 80s 90s dot dot dot and that was a very complicated architecture and digital equipment actually built a lot of machines that was some of the fastest machines over the world and then at some point they said oh we're gonna scrap this architecture we're gonna come up with something clean and alpha was the clean architecture that they designed and it was actually a very clean architecture it doesn't exist anymore unfortunately because of various issues which we can discuss over some other thing not lecture but that architecture was designed the excites was actually one of the primary architects of it hey he wrote the manuals of it by the way he's going to come and give a distinguished lecture in the CS department some sometime in October or November so I definitely recommend that you attend that he's not gonna talk about this I think but he's going to talk about data centuries I believe but doesn't matter so the Alpha architecture they designed they designed some of the flagship process the ones that I mentioned 21 0 64 164 264 264 was the out of order execution superscalar engine and was designed to be a for wide issue engine for byte meaning it's you can make issue and execute for instructions fetch issue execute for instructions per cycle so ideal you want 4 instructions per cycle performance right so the excites figured out they despite all of the techniques they've employed the instructions per cycle they get out of the machine is 1/4 instead of 4 they get 1 over 4 so it's that 16 1 1/16 the capability of the machine and he figured out the reason is this most of the time the machine is waiting for memory to deliver the data that's why he said it's the memory stupid now he may not have them the first person who said this but he is the first person who wrote about it that I could find on the internet so I didn't do my homework perfectly I could have searched all of the libraries of the world but again ok this is actually data from one of the papers that I recommended this is one of my earlier papers other people have shown this data later as well as earlier as well but basically this is if you have a 100 828 entry window machine that can look at 128 instructions ahead of the current instruction how much time does it spend on truly computation and how much time does it spend stalling waiting for stuff so about 68% of the time it's Waits it's so it's stalling it's not doing forward progress basically it's waiting for something and about 55% of the time what is waiting for is l2 cache misses basically waiting for memory despite all of the stuff that we do caching prefetching dot dot prefetching we haven't talked about what we'll talk about we're still bound by the l3 cache misses on memory intensive workloads if you're in work code fits in cache there is no problem of course but these are more interesting workloads in my opinion that do not fit in cash you have big datasets and you're analyzing data you're doing stuff basically interesting things so this is true for other people I've shown it for databases actually there is a nice paper in vldb I believe 1999 by Natasha I LaMacchia who's at EPFL right now that looks at where do the cycles go in my database and what they showed is something similar basically and this was done with Intel so we looked at a lot of Intel workloads and if you want to look at the work those you can you can actually look at the paper but I'm not going to delve into that basically that's the performance perspective that's where we are in memory and this is this is still true actually actually Google recently published a paper in 2015 where they analyzed all of their data center workloads and they looked at micro architectural analysis of that workload we should put that paper online and what they found is something similar on a lot of the work goats 40 to 50 percent of the time maybe higher is they're just waiting for memory ok so the second perspective is an energy perspective that's important also this is from Bill Daly's keynote at I think in 2015 this is some old technology parameters but that's ok things have not changed as much since then let's focus on the energy cost of doing different things in a node so this is the energy cost of a 256-bit access to an 8 kilobyte SRAM for example it costs 50 Pico joules what about a 64-bit double-precision floating-point operation it costs 20 Pico joules with some technology assumptions of course from I mean informed from Nvidia and dadada if you look at a deer I'm access a memory access on the other end it costs 19 nano joules that's almost three orders of magnitude right so basically a memory access consumes three orders of magnitude the energy of a complex addition over here and double precision floating point addition is one of the complex additions that you have it's not the most complex operation that you implement but it's one of the complex operations so given this I think it's good to think about the balance of the system right so if you if you rewind about 70 years ago when one Newman wrote the classical paper on one name and architecture and try to figure out what were the parameters like at that time you would see that this was red and that was green at that time meaning the cost of at that point there was no 64-bit double-precision floating-point operation but the cost of a complex Edition was much more expensive than the cost of a single memory accesses about two orders of magnitude let's say one to two orders of magnitude that study is not easy to do of course right people were less concerned with energy at that time today energy is a critical design concern so that's another perspective and I'll give you some more perspective about the energy in a little bit from real studies that you may actually want to read there is a third perspective which is a reliability perspective I don't think I've shown this in the lecture before but basically reliable to your memory is decreasing because we're putting more cells in a given chip area and we've discussed this previously before but this is one study that one of my students did with Facebook and basically what we've done is we've gone and analyzed every single memory error that in every single server that Facebook has and they have a lot of servers they may have the most servers actually in the world I'm not sure that's a hard thing to say but clearly it's it's clear that they have so many servers that they don't let us publish the number of servers and amount of memory because that affects their stock price somehow because people keep speculating on that but doesn't matter if you do that very large scale study and analyze all of those errors across a year or so in the service you find that the server failure rate is increasing the server failure rates is positively correlated with the chip density of G and that's employed in the service and if you want to understand this metric you really need to read the paper because there's a lot of normalization that goes into this metric but basically reliability of memory is reducing and it's affecting the servers that we have in the field and you've already seen their world hammer vulnerability from the previous lecture item any of you remember that okay we're gonna delve a little bit more into that today but maybe not enough more so clearly there's a security perspective also and this is always related in my opinion so if you have a reliability problem if you don't fix it if you expose it to the program or somehow you always have a security problem you may not think that yeah you have a security problem but you may not know what's going on also okay so we've seen this before so there's a security problem also okay so the reliability and security perspective I recommend to this paper before and this is very recent so it's even more recent but it's not as broad as the memory it's more from the reliability and security perspective so you can read this at your spare time okay so that's hopefully motivates some of the perspectives I'll do some more motivation so we'll stay at a higher level before we get into the fundamentals of DRM a little bit more any questions so far is this interesting boring no you don't think memory is a problem that's okay memory is not it memory is a problem if you're actually putting your system running your system at the fastest possible speed if memory is not a problem then you're not utilizing your system very well that's what that's always my argument in today's systems okay so basically we're going to focus the next couple of lectures on main memory and that's is a critical component of all systems that we built today and we're going to look at many issues like size technology efficiency cost and management algorithms again I can reorder the lectures a little bit my goal is to cover the fundamentals of main memory and two lectures hopefully but if you look at processors and caches FPGAs GPUs whatever substrate it has this memory interface and that memory interface is constant and it's basically connected to DRAM today and usually if you're actually really well utilizing your system your bottleneck is that interface how much how much fast fast you can bring data through that interface actually I had the perfect example of this just before coming to the lecture you guys know ton n borrower here there's a door over there that's revolving right so that's the memory bottleneck for you if there are many people at the same time trying to get out or get in or both at the same time you'll see that that's a bottleneck it takes a long time to get out or get it because that thing is so slow it cannot accommodate multiple people at the same time right so that's very similar to the bottleneck that we have maybe I could replace that with a picture of the tenon bar okay so this is another view of the system this is my pictorial view I drew this in xfig how many of you use xfig oh man I'm too old xfig is a drawing program that's in Linux and it's nice because I can work I can work with it I cannot work with PowerPoint but basically what this is showing over here is this actually showing a system that we have not looked at before one of the choices that we will have in caches is sharing the caches or making them private to the cores I briefly alluded to that but in other choices we actually have a single monolithic shared cache or a distributed shared cache so what this is depicting is you have a distributed shared cache and actually it's like a banking but it's a higher abstraction level you are really distributing parts of the cache and partitioning them across a different course okay but that's not my point that's that's something fast-forwarding we will talk about that but if you look at a system today most of the system is really dedicated for caches interconnects other caches other interconnects memory controllers other interconnects memory other interconnects and other memory storage is a type of memory also so basically most of the system that we designed today is dedicated to just storing and moving data what we've covered in digital circuits this core is a little part of the system today in fact most people know how to design this really well but most people don't know how to manage the data really well okay so let me give you a broader perspective of the memory system basically we'll talk about some recent technology architecture and application trends that lead to some new requirements from the system and that exacerbates some old requirements and as we've seen before in the caching memory hierarchy lecture we always wanted a lot from the system right zero latency zero costs infinite bandwidth infinite capacity but there are some new requirements also on top of this or at least expanding requirements so hopefully I will give you some examples showing that DM memory is important because DM and memory controllers as we design them today are unlikely to satisfy these requirements and we're already experiencing some issues like row hammer as we've discussed and there's some emerging memory technologies that are non-volatile that enable new opportunities so we'll talk about that today because this is becoming it's already become a reality actually how many of you know about Intel's obtained system ok some of you do so that's basically I don't know what it is exactly but it's it's like a storage right now that's based on phase change memory which is a new technology at least and enable technology the goal is to at at some point that technology will replace the RAM or augment dia but then there are a lot of trade-offs associated with so today there's a possibility of some emerging memory technologies potentially replacing DM and they're non-volatile which means that they may actually place both the um and flash or they have some position in the hierarchy that that it changes the higher significantly so you guys are at a good point in time when you're studying computer architecture because computer architecture as I said was not this exciting 20 years ago ok so given this people are actually rethinking the main memory system to fix issues that are happening with DM and enable some of these emerging technologies while satisfying all the requirements now let me give you the broad perspective of some of the trends so these are some trends there's an increasing need for a million memory capacity bandwidth and quality of service predictability actually because multiple things are sharing the memory system I should I put latency over here but I don't have enough space so put that latency over there latency is always important so why is this happening there we're putting increasingly more cores and agents accelerators into the system they're demanding more applications are becoming increasingly intensive and you can see this everywhere right my favorite example really genomics today we can actually sequence genome so fast and so cheaply that we're really overwhelming the capabilities of the system storing it and analyzing it and we don't know we don't even have yeah exactly because of other reasons also but it because the main reason is really the applications are data intensive right and we want to consolidate more and more consolidation is the idea of putting multiple applications on the same note for example or same substrate hardware substrate we want to do that more and more because if you put more things in the same node you're more efficiently utilizing that note both in terms of area energy costs and cooling power right people are for example going to cloud computing which you may have heard of I assume I don't know what it means but for me what it means is really you send many many applications to a data center and data center execute them all at the same time so that's one example of consolidation right instead of having a dedicated data center doing a dedicated application like search today it's not only doing search but it's also doing many many other people's applications at the same time if you want to utilize the system best you actually want to consolidate not only at the data center level but also at the react level but also at the node level and also at the processor level right all of those you really want everything share things to be consolidated and this happening in other systems also for example my cell phone is running many things at the same time right ok so let me give you some chance so this is one trend this from a Niska paper that was published in 2009 by HP Labs and University of Michigan what they looked at is they basically plotted the core count and the I'm capacity dim capacity will talk about quite a dim is later on but basically it's a DRAM module we will go and construct deconstruct where Adam is later on but basically they've shown that the core count is doubling approximately every two years and um dim capacity is doubling approximately three years even though both trends are exponential they're exponential they're linear in this because this is really a logarithmic axis but even both the both the times are exponential there is a difference in the exponents and as the results there's a gap basically they've suggested that memory capacity for course dropping by 30% every two years which means that if your applications are not somehow scaling or the scaling factor if your computation data needs are scaling as fast as the number of course you have a problem right you're not sustaining the data and also if memory capacity per core is dropping that means that a single thread is getting less space and how do you this puts a burden on the software designer because the software has traditionally relied on having more space in memory to actually get more performance or more features or more whatever you want from software so this is a problem and as with any graph you should always be critical right maybe you can and that's one of the purposes of this course it's good to be critical and say oh yeah but what is what about after 2010 right well this was published in 2009 so it can forgive the office for not having data points for over here but you can always say or maybe the core count is not increasing as fast and you may be right actually maybe the general-purpose cores are not increasing that fast but we're putting many many more accelerators in the system today that heterogeneity is actually adding more pressure in terms of memory capacity so if you look at the trends for memory bandwidth per core they're actually much worse memory bandwidth is increasing by about 10 percent per year which is sad but there is a jump as we will discuss later on and if you're actually demand demanding a lot of data there's a problem here so basically the takeaway is we're just--we're we know how to put more cores into the system but we don't know how to supply data to it both in terms of storage and the bandwidth so basically we're in other words we're putting more cores we're starving them ok let's let's have some more fun with these trends so this is the DM improvement and this is the last 18 years I've giving you the capacity so capacity actually is bad right but it's not the worst of the problems as I've already said but if you look at the capacity improvements over the last 18 years it's not been that bad it's exponential and as you see because of the year I'm scaling issues we're having some trouble right now it's becoming a step function if you draw it that way so it's about it's improved significantly what do you think about bandwidth over the last 18 years and he guesses as to how much has it improved in the past 18 years been with his transactions per second or bytes per second you can get out of DRM nobody gets guessing anything is it hundred X zero I want to guess 6 X okay well it's not it's not terrible I guess it's not a bad yes but it's really 20 X but if as you can see over here it's kind of tapering off also because of power limitations so traditionally badams has been improved by increasing the frequency of the memory bus and that frequency is tapering off because of power limitations today it's increasing still but not not as fast as you can see but this is still not the worst of our problems perhaps what about latency clearly I've given you a bonus by saying it's not the worst of our problems but it has to be less than 20x yes three you've seen this presentation I think that's good that you get you get brownie points for remembering you could have remembered it as I don't know 3 X that's possible also yeah basically latency has improved by only one point three X like 30% over the last 18 years and in fact some latency parameters have increased and we may talk about that later on so this is a problem because latency is still important and we're going to talk about that this doesn't mean that I mean this actually shows something which is very interesting I think latency requires more work to improve it's actually more expensive thing to get in almost everything whereas capacity is easier because we are really relying on the technology scaling to get the capacity if we can fix the reliable tissues that's fine that's why this is important and bandwidth is also dependent on technology and it's also an easier problem to solve as we've discussed you can actually do tricks right but latency is a bit harder and also what the other reasons why this has been this way is DN has been very much driven by costs if you want to reduce cost reducing latency goes against the reducing cost growth whereas capacity if you increase cost per bit so if you increase the amount of bits that you can store for a given area that reduces your cost okay we'll talk about that what I mean before we talk about that I think it's good to keep in perspective some of the applications that are really important Diann late since these acts are still critical for performance other things are also critical for example in memory databases have been enabled because of that huge capacity increase but now they may be limited by some of the latency because the transactions are not as fast and there are some nice papers that are referenced over here that you may you can look at that talk about the latency issues graph processing which is employed and almost all big data data intensive workloads in-memory data analytics and the data center were closed they're all limited by latency in fact this is the paper that I referenced from Google in Eska 2015 that talks about memory latency being a big problem ok ok we'll talk about latency later on yes No yeah well it's because it's really the interconnects is not really scaling very well the interconnect latency is not reducing well that's one of the issues but it's also a conscious choice made by the DI manufacturers they're using the extra transistors they get for capacity and not for latency so there are multiple reasons well we'll get to that later on so if you're interested this tiered latency DRAM paper that I talked about earlier gives you a very good perspective of the capacity latency trade-off but we will get to that as well okay let's talk about energy and power as well we've briefly looked into it but let me give you the system perspective so this was a paper written by IBM in 2003 where they showed in their big iron servers big data center service about 40 to 50 percent of the energy is spent on the object memory hierarchy now fast-forward 2010 another IBM paper in power8 they show that about 40% more than 40% of the power India is India in a GPU well more than 40% of the power is India so when they did the study in 2003 actually off chip memory heart it was not just DRAM but also of chip caches of chip interconnect off chip storage dot dot but now DM is consuming actually comparatively more power today and we've already seen that GM consumes power even when it's not used periodic refresh that's a problem okay on top of this the UM technology scaling is ending I'll come to a nice point and then we'll take a break but basically people I've been thinking that DRM is not going to scale easily below some size as you reduce the size of the circuit it becomes much less reliable as a result ITRs is an international technology roadmap of semiconductors they've been doing projections onto future technology for years and years they renamed themselves something else that I don't remember at the moment but they've been projecting that DML at scale we also have technology in node and scaling reducing the size of the DM circuit has provided many benefits as we've seen like capacity being the most important part but lowering the cost and reasonable energy lower the energy scaling so if this ends then there's a problem and the sortie ending actually there's increasing no hammer is a problem that we will discuss a little bit more but there are other problems in DM which is increasing cell leakage current refresh reduced reliability increasing manufacturing difficulties and there a bunch of papers that talk about it so it's difficult to significantly improve capacity and energy with the same technology on top of this there are some emerging memory technologies that are promising which are going to happen in this table BB covered so there's 3d stacked yeah DM is also changing because people are seeing that there is a problem that we're having with DM so they're experimenting with things like 3d stack DMS or the exists actually you can buy it in GPUs it provides you higher bandwidth the ideas you have a logic layer underneath the memory layers and the memory layer and logically are connected with very fast very high bandwidth through-silicon vias is the wires that are specially designed to cut across different dyes now that's an engineering challenge to actually design this but people have designed this it comes at a higher cost but if use a higher bandwidth there is a reduced latency DM which we briefly talked about here tiered latency an for example is an example gives you lower latency low power DM this actually these things employ a low party I'm actually this absolu is low party I'm also it doesn't employ the high power DM that you have in service gives you low power and we will talk about the emerging technologies in a lecture and this gives you larger much much larger capacity actually hopefully an Intel thing is called 3d xpoint actually it's I think that's obtained is the name of the chip but of course there's always downsides right this comes at small capacity this comes at higher costs this comes at higher latency and higher cost and this comes that's potentially many many other higher things but the key thing is how do you actually take advantage of the greens while avoiding the Reds as much as possible ok I think this may be a good place for us to take a break let's do a 6 minute break right now I'm randomly picking the number of breaks but all right I think our six minutes are up right I didn't keep time but at least we had some margin okay so if you have any questions we have a microphone here so we should pass around the microphone so that's at least the questions get recorded then you can other people who watch it later can get the questions as well so you're you're the runner okay so I think if you look at this picture I think this is really exciting actually because if you rewind again five years maybe ten years you wouldn't see any of this almost at least in the markets but a lot of these are in the market today even the NBN right but we'll get to that okay so let's talk about what are the some of the issues like why is it why is the becoming much harder to scale and the real problem is this charge the memory that we have today in DRM is charged based in DRM basically restore charge in the capacitor and just charge escapes in flash memory which we may talk about later on you it's you trap the charge and this what is called the floating gate and this charge also escapes actually diem needs refresh right flash memory is non-volatile right you guys know that does it need refresh how many people think that it needs refresh yes no okay there's some staunch knows well it turns out it does need refresh also as we scale the technology one of the reasons why this breaks down is that charge gets lost over time so if you've written a lot into the flash memory the retention capability of the cell degrades over time so if you look at this flash memory specification for example it can say you can write to a cell at most three thousand times that's actually true you can write to a cell at most three thousand times and after that it's not guaranteed that your rights will be stored now the problem is as flash memory ages as you keep writing to it as you keep writing to a cell you actually caused some problems in the floating gate which I'm not going to go into detail about but we can talk about if we talk about flash memory it turns out charge storage becomes harder and if you start we fishing it you can write more to flash memory so in at least enterprise enterprise scale SSDs SSDs that are employed in data centers today people are refreshing flash memory so that they can get much longer lifetime out of it so that they can store data for longer so basically it's very fundamental to charge based memory this was not a problem in the past ten years ago for example they were not refreshing it because things were still large and you had a lot of charge storage and this was not as big of a problem but now if the people have reduced the size of this to very very small notes some small sizes as a result you have very little charge in the gate and that escapes easily and that's one of the problems the other problem is reliable sensing also it becomes difficult retention is difficult but reliable sensing is also difficult as the storage unit size reduces and we've seen this with DRAM with Rob hammer I'm gonna talk a little bit more about that today but similar issues exist in flash memory also when you read from one location you disturb the other location when you write to one location you disturb the other location as well as this location so there are a lot of issues that happen with charge now the emerging technologies which you may get to later on not not in this lecture but we will definitely get some time in later lecture they're fundamentally different because the charge storage the storage is not dependent on the charge it's depend on the resistance so whether you have high resistance or low resistance denotes whether you have a 0 or 1 and that's more scalable ok so let's look at DM so basically any memory NIC requires a storage device and an access device as we discussed in the past and this must be large enough for reliable sensing and this must be access device must be large enough for low leakage and high retention time in DRAM there's also other parts of the access to high-speed lines and some fire as we've discussed right when we talked about the diagram earlier in memory hierarchy lecture lecture 2 now if you reduce the size of the circuit both of these properties become difficult to maintain in case of DM and this was the value that was assigned to X earlier in mice by ITRs in 2009 in 2009 they said scaling the size of the circuit below 35 nano meters feature size is challenging what do you guys think the feature size of an existing DRAM chip is a dram cell today 35 it's it's a dimension of the dram cell today we're actually much less than 35 18 I think she started this lecture before so 18 is actually where we are around today actually that's I mean where we will be is much lower than that hopefully but clearly we're at 18 but people have been projecting that this is going to be challenging to reduce the size of the circuit because of reliable tissues and we know of that I mean I've already shown you the picture you reliability has become a problem in DRAM and this is the paper you might want to take a look at if you want to see that study but it's not required again so actually people including us I've been studying DRAM quite a bit I think we were the first ones to build an infrastructure like this these are basic PTAs that are connected to a heater as well as a PC and a temperature controller where you can study different effects on DM do the testing basically and if you really want to do something like this it's not part of this course but you can certainly look at this paper and look at our infrastructure where you can get an FPGA and download this code and test any DRAM chip that you can get your hands on and that you can put on that poor FPGA board that you have and what this does is basically at at the C++ level you can actually send commands to DM and test the DM for example you can say disable refresh for this amount of time and then read the data see if the data matches what I've written to earlier that way you can do a lot of different tests so one of the advantage of building such infrastructure you can study all of these scaling issues and real DRAM chips so whenever you are actually looking at a problem in DRAM for example how do you study it is of course a good question right how do I really understand what's going on in the DRAM chips the best way is really taking the real chips and testing them and understanding the issues so and this is the infrastructure that you may want to look at again but basically while my students were testing these chips one of the things that they've discovered it's based on our experience on flash memory actually because we knew that there was a lot of Reedus turba facts on flash memory is that you can actually do read disturbance in DM also it can predictably induce memory errors and most DM memory chips and you know this problem this is a roll hammer problem I'm going to go through this relatively quickly because you've seen it but it's this is an example of the problem that we're having with memory basically we're having so many problems that with reliability that it's they're already causing system security vulnerabilities and I've shown you this picture before basically the problem is you need to access a row if you want to access a bytes in memory and for that you need to activate throughout we're going to see this more later either today or in the next lecture so you activate the row which basically enables the word line and that happens by applying high voltage to that word line now if you want to access some other role in this Bank you basically close the word line this is called pre charge you prepare the array for the next access right by applying low voltage over here and do some other stuff to prepare the area to prepare the bit lines such that they can sense the next axis now this called activate pre charge that's what you need for a read actually usually you do activates reads pre charge actually HP charge now if you're actually going through the robot for you to actually do it and then pre charge right now if you don't do the read but if you actually do this activate pre charge actually pre charge actually pre charge actuate pre charge activate pre charge over and over enough times before the cells get refreshed within a refresh interval this is what happens actually even if you do the read that doesn't matter I'm going to show you that but this is for the pictorial purposes what happens in many DRAM chips today is that adjacent rows get bit flips 0 SL was charged over here it gets discharged after you do this hammering what's called hemming and we call this the hammer draw these are the victim rows they're physically a Jason you're gonna read the paper assuming you choose that as one of your papers but this is happening today because cells are just too close to each other these cells in this hammer draw are so close to each other that they're affecting the victim rolls in fact if you look at a dram chip today it's it's so dense that these cells are really not linear almost there they're almost like hexagonal they're in two intertwined with each other almost which is really interesting and in fact the other picture that I showed you earlier is not real from a physical perspective the capacitor looks like this right in a real DRAM chip capacitor looks like this basically it's etched 3d and the aspect ratio and the access transistor is on top and the aspect ratio of this capacitor is about 50 to 90 X the size of that transistor so that you because you want to make the capacitor as little as possible so that you can put a lot of capacitors so how do you make it make it little as possible you go vertical if you go vertical you need to store enough charge how do you need to already store enough charge well you deep deep down and dig dig down deeper right that's the idea so that's how these things get these are actually really interesting engineering problems that people have solved but the problem is we got so once you do that you lose a lot of isolation across the cells and whenever you so one of the reasons for a row hammer as you will read in the paper is whenever you activate this row it turns out these two word lines are coupled when you activate this with a high voltage you're also applying a little voltage inadvertently to this word line now if you apply a little voltage to this word line it's getting the capacitors are getting connected and there are some cells that are vulnerable to this role hammering effect and they're leaking charged little and then if you apply this high voltage again they're leaking charged again if you apply this IOLTA j-- again they're leaking charged again if you do it enough times before the memory gets refreshed you may actually lose the charge in some honorable cells because they just get depleted right that's the idea that's one of the why this happening you don't have enough isolation and you have word line to word line coupling between the cells okay and we've tested a bunch of chips and you can see that more than 80% of the chips that are tested from different manufacturers are vulnerable to this effect do you want to give the microphone to him it's all of it yeah latency overhead for sure yeah yeah just my question is that you said there's a bit flip does it mean that like if there was a zero it can become one or only one becomes zero because the thing that you described currently is yeah fix leaks so basically it leaks means that it has something and then it becomes zero so that so okay let me let me handle it you should read the paper if you're interested the paper actually shows that but zero doesn't mean discharged so basically the leakage happens from charge to discharged but you may encode the charge bit as zero or one but it's always it's always one direction yeah leakage goes one direction that's correct yes now there are some very very small teases where it may go the other direction but we're not going to talk about that intuitively it goes one direction but once you actually go into this small scale it turns out there are so many weird things that happen in the circuits I call this quantum like effects that there it's not you cannot guarantee that it's only in one direction there are some cases when you test the devices sometimes you actually gain charge which is really interesting it does happen because there's some noise in the sense amplifier and that actually can get into the cell anyway that's one of the potential reasons but that's what makes it really exciting also because really you're really trying to understand what's going on you people I built the system but you don't even understand it necessarily at some level right okay but but for most cases it's the leak charge for an overwhelming majority of the myths okay so this is a scaling problem because as I showed you in an earlier lecture in the past this didn't happen cells were far away from each other they had enough isolation but after 2010 they started getting much closer to each other that this become started becoming an issue and all of the memory modules that you've tested between 2012 and 2013 were actually vulnerable to this effect independent of the manufacturers as you can see right who shall remain nameless okay so this is actually a bigger problem because it's not just activate and pre-charge but you can induce these effects with user level codes that looks like this you can basically download this code and execute on your system and what this code does is basically it you need to do this hammering inside that memory bank right to be able to do that you need to bypass the caches that's what these cache flushes all are all about you need to avoid cache hits so that by flushing X from the cache and you need to avoid draw hits to X by actually reading Y in another row okay remember there's a row buffer you need to bypass that row buffer is essentially another cache right at that level so you need to bypass all the caches to induce this effect that's why you have these cache line flushes and also the reading another row so what this program does is it selects address x and y such that they map to the same Bank which is another issue how do you know which bank things map to its do some reverse engineering and it ping pongs access takes an Y and if the chip is vulnerable it causes some bit flips you can download it and test it actually it's fun okay so this happens in real systems using that code you can see that you get errors if the DRAM chip is vulnerable if the memory controller is fast enough for accessing the DRAM chip enough times before the cells get refreshed remember refresh to every 64 milliseconds in today's chips and it turns out that's true for these systems that we've tested you get errors and areas are of course not proportional to the access rate because there's some some threshold number of accesses that you need to do to actually generate these errors and after that point you may get a lot of errors okay and as we've seen the serial reliability and security issue right this is the paper that you may read one of the one of the three and memory isolation is a key property of a reliable and secure computing system and access to one memory address should not have unintended side effects on data stored in other addresses that's the first sentence of the paper and when we wrote the paper we said that you could actually design tech to take over a system and that was obvious to us but while we were working on Google actually did that attack this is Google project zero this I would recommend their blog post also if you're interested in system security what they did was they exploited this narrow hammer mechanism failure mechanism they called the bug I like thinking about is the failure mechanism it's really read disturbance when you're doing a read you're disturbing other cells and they basically published what I copied over here part of part of which what I copied over here they test the selection of laptops and they found a subset of them are vulnerable to this problem and they were they build to working privilege escalation exploits to that use this effect one of them takes over the Google native client which is not that interesting in my opinion although it's interesting for Google of course it's their virtual machine in a sense but the other is much more interesting basically you can actually at the user level first of all what they did was they actually took our test and developed it to be further such that they could hammer a role from both sides so if you actually have a role over here and if you're hammering the physically adjacent one at the bottom and the physically adjacent one at the top you can get more errors so they were smart and they were able to induce these bit flips to gain kernel privileges when you run their program as a user learned process and what they did was they were able to induce bit flips and the page table and the entry of a program and once you actually gain write access to your own page table which is essentially what they did this exact their words from their blog post actually it's copy and paste they were able to gain write access to the own page table of the user level process and gain read/write access to all of physical memory and once at that point if you know the structure of the operating system if you know what's in memory you actually have everything that you need to do whatever you want in the system right okay and we've seen this before I think I like this analogy the most as I mentioned earlier right it's like if you want to gain access to I don't know to that door somehow you keep banging on the store and then the perturbations that you caused because of that banging magically opens up that door which shouldn't happen right okay so later people show that these are just potential recommendations for you your interest in this area for example in system security and especially the hardware related security issues these folks from tu grass showed that you can actually induce these attacks using JavaScript on a remote server and take over that remote server and these folks show that you could actually take over an android-based system as we've discussed this is actually very nice paper that talks about how to do that okay and this is one of your readings as you see so there's a lot more analysis of course in the reading you can take a look at it but let's take a look at some of the solutions to the problem which we haven't talked about before so how do you solve the problem as I said earlier one solution could be increasing the refresh rate right and this paper actually covers many more solutions so I should look at that paper for more detail but I'll talk about increasing the refresh rate that's one thing that you can do in systems that are outside today so if you have a vulnerable system today one solution is really changing the refresh rate increasing it it's not clear if it will actually solve the problem because as our paper shows you need to increase the refresh rate by 7x to get rid of every single error but that was the solution that was adopted by many many manufacturers this is Apple security patch where they acknowledged roarhammer and they say this leads the memory consumption and they say this issue was mitigated by increasing memory refresh rates well memory refresh rate of course reduces the vulnerability because you cannot do as many hammerings before the cells get refreshed right and these guys were nice because they cited our paper and there were other also other people so who released similar patches and this this works if you cannot patch your memory controller right this actually has a lesson right now if you if you're able to patch your memory controller with a software patch you can potentially do this if you're able to change your refresh rate maybe double your refresh rate they don't tell about how much they change the refresh rate also so this may actually get you thinking oh if you have a much more flexible memory controller maybe we can do even more right while the system is running in the field if we find some other bugs we can patch things if you have a reliability lecture we can talk about that also but modern processors provide limited patch ability in the system so you can pass the micro code for example if it's tractions not executing correctly you can change the microcode a little bit but it's very limits it's it's true for this case also it's very limited but luckily this is one solution the question is is a good solution you're increasing your memory refresh rate right and we've discussed earlier that's never a refresh is already a problem right this consumes power its leads to performance issues at least equality of service issues so we don't want to do that necessarily but this may be the only thing that you have you can do on existing systems in the field so I'd encourage you to think about other solutions potentially error correcting codes is another one but if you already don't have an error correcting code here what do you what do you do actually most systems don't a very correcting codes this this laptop for example memory doesn't have any correcting codes not today at least this doesn't have very correcting codes data centers sure they have very correcting codes but if you read our paper you'll also see that sing simple error correcting codes is not enough to get rid of all of the errors you can actually get 4-bit errors in a 64-bit data burst which is beyond the correction capability of existing error correcting codes that are employed in modern dia ok so we should probably have a reliability lecture that's more general later on ok so let's talk about the solution if you're eating the paper you'll see the solution but I'd like to highlight this because whenever you have problems it's always more efficient to have solutions that are targeted for the problem you can say refresh the solution but that's a very broad thing that affects a lot of other things ideally you would like to solve the problem without affecting other things right more while affecting other things as minimally as possible ECC is another solution let's say you have a stronger ECC code that can correct but that affects a lot of other things also because that's error correcting code is actually expensive right because you need a separate in today's into today you need to have separate D I'm chips to store the error correcting codes so what is the potential solution the solution idea in this paper is basically very simple when you activate a row basically after you after you close are off after you pre charge a row you activate one of its neighbors with a very low probability and you can make that probability programmable to change your tolerance for example to change your reliability guarantee so with probability five out of a thousand times so 500 thousand times you clot when you close some Rove you probabilistically activate the adjacent neighboring groves if you do that you get a very good reliable to guarantee why does this work because to be to be able to hammer you need to have enough number of accesses feral and if your probability is set nicely you'll refresh before those number of accidents happen right you don't refresh across the board you normally fresh all of DM you just refresh the adjacent rows so that's the idea and by adjusting the value of this probability you can vary the strength of protection against errors and you can some of you are reading the paper you can see that the performance analysis is very favorable you still get a slowdown even with these refreshes but not as much as increasing the refresh rate across the board and the other nice thing is this is stateless it's low cost low complexity you don't need to track anything in the system other than figuring out when to generate these additional refreshes okay so how do you actually implement this or even a solution like this is not implementable today actually you need to change something in the system you can change something in the DRAM chip or you can change something in the DM controller memory controller as the paper discusses the paper you're reading discusses let's talk about the DRAM chip first so the idea is whenever you do an access you want to activate an adjacent row how do you do that without disturbing anything else in the system one idea is potentially doing this when you're refreshing right when you're refreshing you're actually refreshing some more rows some other roles that you recently accessed or after you back act after you've closed they're all you actually refresh that row so if there is enough timing slack in the parameters enough slack and the timing parameters you can do that now if you to be able to understand this you need to understand how DM works basically today whenever you say I activate DRAM the memory there's a protocol between the memory control and the DRAM that says that's synchronous there's a latency that's specified you send it activates from the memory control DM and the memory candy on doesn't say I'm done with the activate it just doesn't say anything actually the memory controller assumes that that activate is done after I don't know 35 milliseconds let's say there's a timing parameter associated with it so all of that protocol is synchronous which is actually a problematic protocol also which we will talk about later on but it's very synchronous so what manufacturers today do is had a lot of slack to those parameters so that they can optimize a lot of things so even though the data may be ready much earlier the memory controls still needs to wait and these parameters are actually designed for worst-case conditions 85 degrees operation 85 degrees Celsius operation for example at the worst case temperatures so that's why they're they have a lot of slack at reasonable operating conditions does that make sense so it's a very synchronous protocol it turns out there's a lot of slack in those parameters just like the refresh that we've seen tonight you don't need to refresh every 64 milliseconds you can refresh every second some cells still can retain data as long as you know what to refresh so similarly with the same timing parameters you don't really need to wait for I don't know for 35 nanoseconds you can actually wait for 5 nanoseconds let's say I'm just exaggerating and all of the rest of the time is useless wait today that's what these papers actually show with real DM chips so if you have this slack what you can do is when you pre charge an array perhaps you can quickly activate the adjacent rows inside the DRAM chip and get rid of the raw Highmore problems maybe that's a possibility right that's a possible solution in the answer I believe some of this is implemented by manufacturers today so that they can get rid of their old hammer but it's not clear how they can scale that into the future the other solution is putting some of the test of the memory controller basically memory controller refresh is adjacent rows after it's sends a pre-charge command it's it says oh I'm going to activate these adjacent rows also with very little probability now the problem here is memory controller today doesn't have enough information about the yeah we'll talk about that again later on but the memory controller doesn't know exactly which rows are adjacent to each other physically adjacent to each other India why because the year internal remaps the addresses now it turns out most of the roles are most of the roles that are linearly adjacent and the physical address space are adjacent in the physical chip also but there's some remapping that happens internally in India and why does this happen because of reliability issues actually it turns out what dr manufacturers after they test the chip they figure out their role is not working well so internally they have some address mapping logics that changes the address of that drove to something that redirects that role to some other role inside the app so that grow gets remapped somewhere else physically in the chip so the memory controller doesn't have enough information about the physical adjacency of addresses India and that's is a problem that's why this cannot be done in the memory controller today so you need to give that information memory controller should know how the data of the addresses are mapped to different roles India right that's what you need to know to figure out what what to refresh and if you know that then this is possible okay any questions I've got into some depth over here so hopefully I don't lose many people but if you read the paper a lot of this will become much much more clear okay so let me pull back a little bit because this solution I think is very very interesting and important and people should employ more of it and it's not just we have been talking about this but some industry folks are also writing papers about it too so first of all they're writing papers about the seriousness of the problem and this is a beautiful paper that was written by to two companies that that never pay never write papers together this is the first time I've seen their written papers they wrote this in in a forum where we invite the papers in the memory forum Samsung and Intel they don't even talk to each other it's not clear but they did find it important to write this paper where they talked about DRM process scaling challenges as you can see this is directly from their slides refresh variable retention time which we may talk about more and the right latency increasing at problems but it doesn't matter you can read the paper it's a four-page paper but the solution they proposed is have an intelligent controller and quark detect it with the UM so that you can solve some of these issues very similar to what I just discussed right the memory controller knows the address mapping inside the DRAM such that it knows what to refresh basically today solving these issues just by scaling the process is difficult just in the DRAM is difficult you really need an intelligent controller that's covering up some of these issues solving these issues and row hammer is just one problem right there might be other reliable tissues like refresh for example if you have an intelligent controller that can do balloon filters just like we've discussed later and do the Refresh based on those binning of the rolls well that can actually enable your process scaling much better because perhaps you don't you're not bound by the refresh rate for all of the cells right many cells you can refresh that much higher much lower rates okay so I would recommend this paper also actually this will be on their reading list and as you know I recommend this paper this also covers a lot of issues and it talks about that paper - okay so let me we're going to transition back but let's let's talk about the flash a little bit also we may actually dedicate a lecture also similar issues happen in flash as I said in fact row hammer was inspired by the fact that we knew flash was vulnerable to read disturb effects and this is another infrastructure that we've built this again an FPGA based testing infrastructure for NAND flash chips and it's actually a really old one right now and there are a bunch of issues like program interference we disturb errors as you can see and there are a bunch of vulnerabilities in flash but I'm not going to talk about it right now just but you can take a look at this paper if you're really interested this is the state of the art survey on how errors happen how they're mitigated and how they're potentially recovered from in today's solid state drives but at some point we may actually have a flash memory lecture but this just to give you the idea that these issues exist in flash also and they also pose vulnerabilities so one of the big issues I think in memory today aside from everything else we've discussed latency dot dot dot bandwidth remember I started with performance but we're also now having fundamental security reliable safety threatened because of these lower-level problems so it's important to consider these and architectures also and people are already considering okay I'm giving you a broad stroke let me give you one more broad stroke and then we're going to jump into more detail more fundamentals again so now these are fundamental don't get me wrong these are really fundamental these fundamental things are affecting everything on top but I want to give a more basic background on the UM in a little bit so how do we solve the memory problem in general since we're having issues like this I think there are three broad solution directions one is fixing it making memory and controls more intelligent that's what we've discussed right just now we're having new interfaces new functions and new architectures inside memory I call this a system memory core design core designing the system and the memory or the controller and the memory together and that's essentially what can enable you to solve problems but this cannot enable new opportunities also and if you have a 3d stacked DRM as we've discussed earlier if you have a logic layer underneath theorem and very high bandwidth connection if you can put anything in that logic layer a processor let's say then you can actually do something like this you can have a different interface that logic layer you can tell that logic layer logic layer execute this function for me right we can have a field programmable gate array inside inside that logic layer so we're going to talk about that in later lectures but that logic layer can also do raw hammer protection can also the error correction can also do many many things so a new new things can open up because of the problems that we have in memory the second solution reaction which we're also going to cover later in lectures is eliminating or potentially minimizing the problem if not eliminating it if we can come up with some other technology that doesn't have these issues would it be nice right and people actually sought that for a long time and other people said good luck all the time this was always a tough problem right coming up with a new technology but today we may actually have some technologies like phase change memory MRAM magnetic memory and memory stirs you may have heard of which we will also talk about in a later lecture but this can actually enable system-wide rethinking of how we actually design memory and storage in the third solution direction I think is very interesting also which is at from the reliability perspective maybe embracing the problem right saying okay we're not going to produce really really reliable memories at low cost we can produce them at high cost sure but high cost is not what enables progress really fast if we cannot produce all of them to be low cost why not produce most of them to be high low cost but unreliable and some of it to be high cost and reliable and put these things together and partition our data intelligently such that we can get the best of both worlds right unreliable memory use your high capacity low cost reliable memory gives you protection from all of these security and reliability issues so I think this is a little bit a little bit more far-fetched but I think it's very interesting to examine today so this could lead to new models for data management and maybe usage I'll give you one example of this I mean before I give you the example if you look at all of these solutions they require rethinking they require cooperation that cost the stack right software hardware devices because the solution is not only at the device level clearly we're having problems with the device level not at the architecture level only not at the technology level not at the software level in fact think about solving their own hammer problem only in software how do you do that again I'll say good luck to you it's tough I think it may be possible I don't know actually there been some papers related to it by examining the performance counters and trying to figure out whether you're having a particular role based on the performance counters but the overheads that you get into at that level become too high in my opinion they've shown low overheads but it's not clear if that will work in under all conditions in all systems ok so you really need to think across the stack okay let me I'm continuing the broad strokes over here just to motivate the later parts so what is this first solution direction I'm going to give you the three solution reactions again and with a broad stroke we want a more memory centric system design that's essentially what we're thinking of right if the controller is closer to me that's your you're not more memory centric and this enables new memory architectures new interface and new function if you look at the industry today this is already slowly starting to happen again five years ago ten years ago it was not this way ddr4 well at that point it was ddr2 ddr3 was the main thing but today we have a huge bifurcation heterogeneity in the space of memory as I showed you earlier right and people are very open to new interface and new functions and also better waste management we haven't talked about this as much but we're wasting a lot of memory today if you think about it so there have been a lot of studies that showed that most of the memory store zeros you can actually do that study yourself figure out how a fraction what fraction of your memory stores yours but there's a story from there's a paper from 2005 iske para Stenstrom from Chalmers and one of his students showed that 30% of the memory do you have stored zeros 30 is a lot but our results actually show even higher so how do you actually deal with that waste do you really want to store zeros in memory why not compress it or encoded in some different way right and people are looking into that so they're a bunch of key issues to tackle hopefully I've convinced you that enabling reliability at low cost is critical for high capacity but reducing energy reducing latency improving bandwidth reducing waste in terms of both capacity bandwidth and latency and enabling computation close to data is important and I'm we're going to talk about various of these as the lecture progresses but this one is important because this can actually change the paradigm one nomen paradigm as we've discussed and this can also solve a lot of the issues at the same time right that way you're less your latency is lower your energy is lower your bandwidth requirements may be lower and maybe you can actually fix the reliability issues with computation close data just like we've discussed earlier right so if you can actually do a computation close data you can actually solve some of these issues together so we're going to talk more and if you're interested these are some of the papers that talk about DRAM so I'm not expecting you guys to eat all of them well that's just to give you a perspective okay so the second solution direction let me give you the second direction we're not going to talk about in this lecture or it may be in a few like Union a few lectures but as I said there are some emerging memory technologies that are more scalable than diem and there's more skill because they're resistive they encode data in terms of resistance levels and on top of that they're also non-volatile diem is volatile obviously very refreshing it flashes volatile if flash is non-volatile but very refreshing it that's interesting also right actually even though these memories are non volatile they also require refresh but we'll talk about that when we get to it so refresh is not really something that you can get away with in my opinion in my opinion memory is very fundamental at some point you'll need to refresh memory if you scale it to very very small dimensions of course it's dependent on technology some technologies like tape maybe may be much better in terms of retaining data but if in at least the electronic technology is like this you may actually have a problem so if you actually etch some thing onto a rock but you need to refresh it that's also memory right it's a very interesting storage device that's been around for I don't know thousands and thousands of years fetching something on Iraq maybe you don't need to refresh it right anyway ok so one example of this is face change memory this actual really old technology is not emerging it had emerged in the past but not in the form of main memory in fact humph you've used I assume many of you use rewritable CDs right yeah we writable CDs use phase change memory is the technology basically what this does is it was developed in 1960s it's it stores charge by changing the phase of material some material one examples child cocaine eyeglass and you read data by detecting that materials resistance there's no charge associated with it now if you want to know more about the material wait for the later lecture or you can do some reading on your own on the papers that I'll talk about in a little bit not talk about show you in a little bit but basically this was you have true resistance levels one is high and one's law so it can detect the resistance of it but in the rewritable CDs you actually have two different state well you have two different states that are two different resistance levels but you know these two different states also have different optical reflectivities meaning that if you shine light on it you get a different output based on the states that the cell is in that's exactly how the rewritable CDs work but that reading process is very slow that's essentially your device right shining light on thing today people have developed wheat devices that can detect the resistance reliably I'm much much more quickly as a result this has become very interesting and all technology has become really interesting because people the the if you will the mmm the storage device didn't change but the access device changed so the access device has become much more reliable and faster and different IBM actually did a lot of work in this so they they I believe they should really get the credit or making this work although other people didn't publish so maybe IBM published and they should get the credit because of publishing right so this is expected scale to much lower nanometers than DM remember at the same time ITRs predicted that DM will not be easily scale below 35 nano meters they also predicted that they scale price change memory will scale to 9 nanometers the expectations are even lower now expectations are higher meaning that the scalability is going to be to even lower dimensions and this was actually prototype that 20 nanometers by IBM and it's a beautiful paper that talks about the lower levels of the technology in IBM Journal of research and development so if you're interested you should definitely read that paper and it's also expect to be denser than DM for another reason not only because you can make the cell much smaller but you can store the cell multiple bits per cell instead of having one bit like in DM indium if you want to store multiple bits in a single capacitor you need to divide the capacitor divide the charge level into four right if you want to store two bits now that's very difficult to do actually or sense and fire becomes crazy the complex if you want to do that but in this case the resistance range is very large so it can actually divide into four divided into eight size that you can store two bits three bits per cell just like flash today's flash actually stores 3 bits per cell when people are looking into more more ok the problem is these emerging technologies including phase change memory patient memory is just one example have other shortcomings and latency energy endurance they're all shortcomings for example you write to a fascia memory cell ten to the eight times and you cannot write to it anymore just like flash this has an endurance problem not all technologies have the same problems the key question is can we somehow enable them to replace augments or maybe even surpass the year right so we're going to talk about that but I think this gives you an overview and these are some of the papers that you may be interested so this is the I think going forward this is really what things will look like remember talking about hybrid cash management that's one of the reasons why I start that way and we want probably hybrid memory systems as well because there is no single technology if you if you list all of the dimensions that you want from memory in terms of metrics and if you list all the technologies that we have available or emerging there is no single technology that's the best or green at least Green at everything green mean reasonably good some technology arm for example is fast and durable but it has other issues it's expensive as a result it has to be small it's leaky it's volatile some other technology like PCM can be large non-volatile low-cost but it's slow its energy inefficient in terms of active energy its energy inefficient in terms of stat in energy efficient in terms of static energy because you don't need to refresh it but also very salt so if you have different technology like this to get the best of both worlds perhaps you put them together and design the hardware and the software to manage the data allocation and movement to achieve the Greenes as much as possible while avoiding the Reds as much as possible right so that's what we're systems are actually headed today today this is happening in the DM space for example maybe we don't perfectly have this new technology PCM but you can think of this as high capacity low capacity I shouldn't say the red ones first the fast the UM high bandwidth the UM but it's low capacity basically because it may be 3d stacked and some other DM that's larger low lower cost but slower and where did I say and slower and lower bands right so it can actually have two different types of dia in the system and this already happening in the GPU space it will happen it's going to happen in existing in in the multi-core space also but it could actually increase so it's good to get prepared for the seller genius memory technologies let me give you one example so we're going to cover in this course both DRM and this other technologies as well but let me give one example just to stretch your minds remember I said embracing it as a possibility right how could we embrace unreliable memory so if you look at this abstract picture what this shows is different data in a given application and the memory error vulnerability of that data what is vulnerable to mean for example how much does the user care about a bit flip in that particular data structure let's say now if your data structure is a pixel in your video maybe you don't really care right if it just change the pixel that you don't need you're not even going to notice right maybe when you're watching my videos if there's a thousand errors in that video you don't care right as long as you can read what's going on so that I'll ever wonder about your some data is really low actually where's some other data if you get a bit flip in that data the system crashes right think about a page table entry for example or you get a vulnerability like security vulnerability or you get completely bogus data right for example the index of your database maybe that's not a good idea we're ready to get errors in so clearly there is a vulnerability difference between different data in programs also different programs as well but even data within a program so if we can somehow quantify that and let's assume somebody magically does this probably probably the programmer says this is vulnerable data and this is air tolerant data perhaps you can map that data to different memory systems or different memory modules one could be reliable memory and the other could be low-cost and less reliable memory so we call this the heterogeneous reliable T memory that's one example of hybrid memory but this could be high cost but very well protected very well tested chips if you testing them your cost heart goes high by the way if you're asking where the cost one of the real cost of manufacturing these chips today comes from the testing really so micro processors for example about 50% of the costs and the time also is spent on testing indium it's worse the testing is actually more than 70% today even despite all of this testing things like roll hammer get out right so you can see the testing and verification is really the the biggest most cost the aspect of manufacturing today so if you can get them not so tested not so much maybe are more errors ok but my data is tolerance right maybe you have very coarse grain error detection right you have a very coarse grain parity for example across the entire memory and then you figure out what's going on wrong so that's another possibility right if you if you want to tolerate a little bit of error but that's that's expensive of course well the latency of fixing that error is expensive but maybe at least you have an idea oh there's an error so if you can do this you can actually get some benefits so we've done the study with Microsoft where my students yishun went there and modified their web search workload and what's what these big data center companies today do is they employ ECC so for various reasons they don't want to deal with memory errors they basically buy all of their servers with error correcting codes and if you look at the cost of error correcting codes it's expensive it basically adds 12.5% area to your DM module and if you really want to hire error correcting codes you actually need to add more like and this paper describes different error correcting codes and if you have a lecture we'll talk about that also but if you go to Mac Microsoft Facebook Google Twitter they all employ our cracking codes in their data centers which makes sense I think because they don't want to deal with these errors right even even actually there was a really nice talk at the dependent at the same conference that this work was presented at by Google one of the Google engineers said that we employ our correcting codes but even then we see memory years and it took us about nine months the a single problem and it turned out it's a memory error so if your hardware is not reliable then there's a problem so I don't remember the name of the engineer or the or the or the name of the talk but it was a very nice talk at this conference so you can perhaps link it if if something exists that so basically what ition did with the Microsoft web search workload is he manually partitioned the data he did the hard work to figure out what's vulnerable and what's tolerant of course that's a judgment call also and he changed the web search workload and such that the tolerant data goes to low-cost memory and vulnerable data goes to reliable memory if you do that you can get rid of a lot of this ECC that's in the system error correcting code that was one of our targets and you can reduce the hardware cost by signature by a significant amount about five percent in this case five percent is an important number we'll get back to that and you can still achieve a single server availability target of 99.9 percent so basically you could because of errors you will get errors clearly but still you're available most of the time and if you actually design your application there's a distributed application right web searches you can actually scale it if you actually design it such that you can tolerate errors at the larger scale at the program level at the algorithm level then you can actually increase your availability much more by reducing your hardware cost but you can also think of this as if you reduce your hardware cost as much you can actually add more memory add real capacity into your system basically you're getting rid of the ECC in your system if you want to keep the same cost you can add 4.7% more real memory in your system right that enables your application to be bigger right you're not wasting the memory space for your Bible tea so reducing cost is always good for increasing the scalability of your system okay so if you're interested in this you should take a look at this paper it's not required again I'll give you a lot of references over here but that's that's what this paper introduces heterogeneous reliable team at me and I think there's a huge design space only in heterogeneous reliable to memory but also in heterogeneous memory itself because the region's memory could be anything right it's not just reliable T but latency dot dot dot okay let me give you this one and then Oh Missy okay we still have some time I'll give you a break at 15 so I've discussed a lot of issues one other issue with memory we briefly discussed with caches also is interference so when you have cores that are sharing main memory you can put a lot of accelerators over here also they basically are sharing a medium right when they share that medium they interfere with each other when one cores request is getting serviced by a bank for example another cores request may be delayed and we've seen this in the memory performance attacks that we've discussed and uncontrolled interference leads to many problems quality of service performance you in a data center for example if you want to consolidate applications this application that's really important may get delayed by some other application that's not important so you may actually violate your service level agreement guarantees because the memory doesn't support quality of service today a lot of data center folks are actually very reluctant on consolidating more as there is out there data centers are not very well utilized so what they do is they put applications that are really that have tight service level agreements that require some good performance alone on the system even though the system may not be utilized basically but if you actually had some support in memory saying you could actually prioritize the applications according to the service level agreement saying could actually do much better so there's a lot of wastage that happens in the system because there's interference in the main memory that gets uncontrolled and that has huge implications on the software stack because people cannot consolidate things if you have a service performance requirement from this application you cannot collocate that application together with some other application that may potentially cause this application to violate the performance requirements right so now you see because the hardware is as the since your first problem software is designed such that it doesn't co-locate operating system is designed such that it doesn't call okay tap legations that leads to huge efficient inefficiency at the higher level because you may have a lot of course but you're using two of them maybe out of a thousand course because you don't you're not consolidating because of that performance requirement so that's essentially what's happening today today memory interference between core uncontrolled we're gonna get back to Cash's also Akash's people are adding controls but we'll talk about that and this least on fairness starvation and low performance remember the memory performance attacks and as a result you get an uncontrollable system unpredictable system and vulnerable system better that are vulnerable to attacks so the solution we're going to talk about is quality making the memory sur memory system more quality of service aware this kind of obvious rate designing the hardware to provide a configurable substrate that's fair and that can be controllable by the software and we'll talk about a bunch of issues over here including caching and the software will be designed to configure the resources to satisfy different quality of service goals the software can say this application gets should be prioritized such that it achieves its performance bound right performance target or if it's achieving its performance target at this point don't give more resources to this application right because it has a lot of slack in its performance target so you can use those resources too for some other application that may or may not be achieving its performance plan so you can actually make these decisions at the software level if the hardware provides a good substrate ok and we'll see some examples so this problem is getting worse so this is one example of a system if you actually have a bunch of agents that look like this they may be sharing some caches actually in some systems GPUs and CPUs are sharing caches and they may be sharing caches at other levels also and there are multiple threads over here that are sharing the l1 cache as well and they may be sharing the memory controllers and the memory itself is heterogeneous also so how do you make sense out of the system and you have this problem actually already in this this looks like this I think although I cannot see the inside of this at the moment but but basically a cell phone is a perfect example of this you have heterogeneous agents and you have interference between all of these heterogeneous agents at cache level interconnect level memory level dot the key question is how do you allocate the resources caches interconnect memory both in terms of latency bandwidth capacity as we will talk about to mitigate the interference that's happening and to provide predictable performance for those applications that need it and to maximize the system performance so that's a big problem in today's systems because that really determines your efficiency in the end if while you're actually doing a really important task on your phone your virus checker takes all of the bandwidth then you have a problem right that's why the phones are actually one of the first to incorporate these quality of service of our memory controllers that we will talk about okay so I'm going to skip this this is just to give you an idea of some of the papers we're going to get back to this later on but this is a really good point to take a break unless there is a really burning question okay I'll give you a seven minute break all right I think we can start we have we had one minute margin that's eight minutes total and that's exactly like the memory interface today basically I advertise that you'll have a six minute break but you get eight minutes I guess the memory interface is a little bit different it's advertised that the latency is 50 nano seconds but the real latency is 10 milliseconds internally so there's a lot of waste okay so we stopped here if you're interested in this dimension quality of service you can take a look at these works I'm not going to cover it again but we're going to get back to quality of service both in terms of caches as well as memory system in resource management in general later on but now let's answer this question I guess how can we fix the memory problem and design memory systems of the future to be able to do that I think you need to really understand what's going on underneath as we've discussed right so we first need to understand the principles of memory and DRM and that's what we're going to spend some time on for the rest of this lecture and probably in the next lecture and memory controllers also how is it done today and how it can be done and techniques for reducing and tolerating memory latency again these can be reordered but this is really important also and potential memory technologies that can compete with the and maybe that's not the best way of posing it but potential memory technologies that are good that could be useful okay so let's start with the main memory and main memory is actually really simple it looks like this basically this is the abstraction you can abstract the memory system like this you basically supply an address I'm even ignoring the command at this point it's an N bit address and you get a K bit of data and you need to enable a chip maybe right enable if you want to write the data actually right so this is of course you may need some other things but this is my abstraction and memory system is interesting because as you go as you peel it you'll see stuff that looks like this internally at the high-level abstraction it may look like this if you think about DRAM for example I have I don't know how big of a memory you guys have but this is 8 gigabytes I think so you basically have 2 to the N times whatever data that I'm getting it's in a dear I'm into your face today I'm getting 64 bits in one cycle so this is 64 and this is now you can make the division right 8 gigabytes divided by 64 if you're buys divided by 8 it's really one gigabyte times K that's the abstraction right the abstraction is that I'm getting 64 bits of data from an 8 gigabyte memory now or I'm writing to its right but internally it's really composed of many many smaller things that look like this just like we've seen the bank abstraction earlier right Bank you have a monolithic memory you divide it into smaller banks even the bank abstraction is an abstraction actually as we will see in DM when you call a bank it's not a monolithic physical structure even that is divided into smaller things called mats or sub arrays basically you have sub banks inside there because if you have a monolithic bank that has 32,000 rows in the bit line that bit line is really slow so it's really divided into 32 1000 bit lines sub-race and you really read a summary when you read it amplify the data and drive it with big bit lines some other bit lines outside so all of memory is really you start with this and you divide it it's it gives you good latency good bandwidth with performance in the end but that's the attraction for example it's a memory chip you can enable that chip you can write enable to it and this is the bank abstraction also right so if you just to give you the analogy with this a bank looks like this right basically we've seen this before this is the memory bank organization you need to supply some raw address and the column address and you activate a row which brings the data and then you max out the data using the column address that's it and we've seen this in DRAM as you know the sister I'm sorry we've seen this in SRAM and we've seen The Scindia these are from previous lectures the differences are relatively small if you look at its right an SRAM also actually this may look like a bank but it's internally it's divided into smaller things but DRAM as you can see one difference that we've discussed it you don't supply the raw address and the column address at the same time whereas here you supply the row address in the column address potentially at the same time but indeed and we don't supply at the same time because you have a chip boundary over here and if you actually supply both of them at the same time what would happen is you need more address bits meaning you need more pins your chip is more expensive so your supply it's in consecutive cycles for example or you can supply it together with consecutive commands activate command spring brings the address of the row and a read command brings the address of the column right okay so all memory looks like this internally it may be different cells and actually this is the phase change memory cell let me take an aside very quickly again phase change memory cell is usually depicted this way you have a heater that heats the material that's very high degree Celsius 650 50 degrees Celsius for example and you by heating changing the material hey if I heating you change the phase of the material and you change the states but even this if you want to put it into a memory it looks like this basically your word line and a bit line Nexus device and you need to bring the data into some sort of sense I'm fire and since it's send some part will be very different because of the technology but it looks like this and you're still limited by the same problems if your bit line is too long your latency is too long so instead of having a huge monolithic thing you partition it into smaller things okay so we've seen this also so let's talk about some fundamental concepts in memory before we go more so physical address space hopefully you guys know this is the maximum size of main memory total number of uniquely identifiable locations basically kind of obvious physical address stability the minimum size of data in memory that can be addressed but you can have byte address ability where their disability and 64-bit address ability for example and there's also a property of the is a write for example but this is not only so this is you can you can have the address ability of a chip as well write the address ability that you have in the ISA may be different than the address ability that you have in the chip for example a chip can provide you only four bits potentially but you put together 16 of these chips to get four bits if you get 64 bits total does that make sense okay okay that's obvious so yeah this is what I said basic microarchitecture addressability depends on the abstraction level of the implementation maybe this is a bunch of words over here but for example if today the DM interface is 64 bits and what happens at the when the processor issues a memory request it really requests a cache block right a cache block let's say 64 bytes which means that it needs to do eight memory accesses eight of these 64 bits to get data out of the UM right to get a single cache block with a 64 bit interface so it really depends on the abstraction level implementation where you are looking at and also what you're looking at so internally in the DM chip in a single DM chip you don't supply 64 bits because 64 bits actually is too much because you need 64 data pins and these data pins are actually really large so a single DRAM chip today can supply data with only at the 4-bit level or 8-bit level or 16-bit level and there are some cases where you have 32-bit chips but those are expensive so what you do is you put together multiple chips in a dim dual inline memory module and you operate them concurrently such that if you want to get 64 bits and if your chip supplies four bits you have 16 of them if you want to get 64 bits and if your chip supplies eight bits you have 88 of them so most memory dims that you know of probably have 8 of these chips right if you look at it here a module it has 8 that's the reason why it has 8 you want to get 64 bits because that's what the processor bus is and each ship supplies 8 bits that's what's called a rank actually so it's a rank of soldiers you can think of the chips as the rank of soldiers that respond to the same command alright that's very simple actually but the serial it's all really driven by the cost the reason you don't get 64 bits out of this diandra or even 64 bytes out of the DM chip is its cost you're really you're you don't want to add more pins to that chip you can but you have to pay money ok and at some point actually you don't have enough space in the boundary of the chip to add enough pins right okay so alignments is another issue does the hardware support unaligned access transparently the software we're not going to cover that in this course as much you can go back to our digital circuits course which we've covered and another question is does the ISA even support unaligned access right but suffice it to say that let's say if your is a supports unaligned accesses and you want you want to bite that spans let's say um do you want you want two bytes from this cache line and you want two bytes from this cache line how do you do that they it maps to two different cache lines basically the the four bytes that you're trying to access with a load match to two different cache lines what do you do well you need to have two memory requests right you need to access to a two cache blocks so that's the problem with unaligned access that's one example of an underlined access the only example but this this complicates the design of memory because who does this who supports this do you punt on the software such that the software says so I'm not supporting any alignment any online and accesses do it yourself make sure you align your data that's one possibility or I'm supporting this now whenever you get an address you need to have a state machine that figures out where the data is and brings back to cache lines for example and this becomes more complicated with virtual memory and interleaving we've already talked about right and interleaving is a very fundamental concept that I'm repeated over here it's its banking basically ok so this is the interleaving example with my handwriting I guess if we need to have more handwriting we're going to use this one later on but so far we haven't need it so interleaving basically this is the same concept that we've discussed before assume each Bank supplies a word in this case a word is 32 bits I just made it up it doesn't have to be but basically this is what the banking that I described in words earlier was you have a single bus this is a date of us in this case it's four bytes and each bank can supply 32 bits 4 bytes but only one bank can drive the data bus so this is the gate this is a gating logic for example if you've started access to this bank and if you are waiting for data you enable this one and the data goes back to the data bus and in the next cycle you can enable this one and the data goes this way but these two banks can operate in parallel right you can you cannot so if I didn't draw the address bus over here but you can imagine the address bus exactly like this but going the other way the gating is actually the gating logic is that way and this way over here but basically what happens over here is you send the address to only one bank in a given cycle you start the access in the next cycle you send the address to the other bank in other address and then you can start the access for the next word in that case and then the both of the accesses happen in parallel and then this Bank returns the data and then you need to enable this and get the data back and then then in the next cycle this Bank returns data and you get you get the output of that bank to the data bus and then you get the word out of it so this is one way this is how the UM is actually maybe not exact like this but it has a single bus single data bus single address bus separate going to these chips and single command bus going through these chips so they the the banks actually share the address command and data buses of course we have the interleaving problem as we've discussed earlier right what's gran Larry to do you interleave at do you in to leave the first word here like address if you if you have word addressable as I say address you are here at just one here address through here at the three here dot do you do that ping ponging in the interleaving that's called fine-grain into you're leaving or do we did more coarse grain as we discussed right the top bit of the 32 bit address determines where the data is okay that's an example again with my handwriting assuming this is your physical address 13 bits again I just made it up assume you have a four byte word this is byte addressable this is these two bits determine which byte in the word you're addressing and for example you can select this bit to choose the bank's right if you select that bit then the first address 0 is in this Bank address 1 is in this Bank address 2 is here at the 3 is here so you're doing fine grained interleaving you're choosing a lower order address bit but you could choose a higher order address bit to be your bank bit in that case half of the the first half of the physical address space gets mapped to the bank 0 the second health goes to bank 1 or you could choose there and emits inside here right so that's now this clearly determines where your bits are and this also determines your conflicts then conflict access patterns ok we're gonna get back to that so some questions and concepts actually this is this makes more sense if you've taken digital circuits with me and nobody has here because those guys are still taking second year third year classes I assume that's ok but I can remind you that there is a lecture in digital circuits that we covered cray-1 cray-1 is one of the earliest supercomputers and among other really interesting characteristics of the supercomputers one of the characteristics was it had a memory that had 16 banks and there was a good reason why they had 16 bank memory because this is a vector processor this actually can do multiple memory accesses per cycle very very easily because operating on huge vectors but it turned out the bank access latency was 11 cycles but they had so much memory parallelism now it makes no sense if you have a single bank you would wait the 11 cycles and only after that you can issue the next axis so they wanted to sustain one access per cycle so if you want to sustain that how many banks do you need well the answer is 16 because it doesn't work right if you have eight banks and if your bank access latency is 11 cycles you need to stall for 3 cycles to start some other access rights because you can start one access every 11 cycles for a given Bank but if you have 16 banks if your data is mapped nicely of course assuming you have no Bank conflicts then you can start one access every site every cycle and then get data for one access every cycle right because basically you need to have enough banks the banks greater than the number of cycles that are required for the bank access latency actually it could be 11 but the problem with the 11 is that we've discussed last time as you're addressing becomes complicated right 16 is a lot easier you could have 32 also right ok so that's basically and what they did was they did word interleaving essentially at that time they interleaved works we're going to look at today's DRAM systems they're interleave at they're all grand Larry for example or cache block grant Larry so word ground there T is long gone in existing systems it happens in some embedded systems actually because some cases where you there are many cases where some of the accelerators for example need to access very small amounts of data not not cache line cache block data there you can actually do smaller size accesses but in systems this for example is doing cache block accesses to many so it makes no sense to really in to leave below the ground layer to have a cache block okay so the other question we already discussed this so clearly there design over here cannot have sixteen accesses per cycle right because the memory actually shares this bus like we've seen over here the different banks share the data bus and address with send command buses again because of cost reasons so certainly you could potentially design a memory system that can fully supply all of the accesses at the same time multiplex started per cycle and finished per cycle but that comes at a cost as we've seen it earlier so because of that as we've discussed DM banks share buses it's cost is very important over there whereas l1 data caches have multiple fully independent banks so you don't serialize accesses to different banks there is not a single data pass for example you have a crossbar between the course to the l2 cache also and some in some designs for Sun Niagra for example at a crossbar between 8 cores and eight different l2 banks but l1 is even even more latency critical so they have multiple fully independent banks and that's true for the register files also but if you get far away from the processor cost becomes a bigger concern because these was they're actually much bigger also at that point ok so this is a bank abstraction and as I've shown you it's very similar right this is Bank 0 it supplies you 32 bits for example juan quirÃ³s this reactor will be 32 K for example but even this is an abstraction actually I mean this is an abstraction multiple different ways one is it could actually be smaller there are sub banks as I've discussed and as we will talk about I'll give you a reference to it also but you don't get 32 bits at a time as we've discussed right this really comes from different chips as I said because this 32 bits is costly if it comes from a single chip so actually what you do is you have four different chips each providing 8 bits and this is called a rank and in this case only bank 0 is shown of drank it's actually you can have another other banks in each chips this is one chip there's another chip another chip another chip they're also applying different 8 bits of the 32 bit that you're looking for and you have a single chip enable for them so but you don't enable each of one of them individually but you have an enable signal going through all of them that's why they're called their rank it's like the rank of soldiers again right you give a command all of them are supposed to do the same thing that's essentially what they're doing you recommend all of them are enabled at the same time and all of them are giving you data except they're giving different pieces of the 32 bits okay and we've discussed everything over here you can read this on your own basically the idea is to produce an 8-bit perching pin chip but control operate them as a rank so that we can get 32 bits in a single read because producing a 32 bit per pin chip is expensive very to pin chip is expensive that's okay any questions now we'll go into more a bit more detail yeah okay all right so this is another view of the thing because we're going to actually go I think it'll be top-down and bottom-up I'm not sure we'll see we'll do the top-down later so basically you start with a channel actually there are multiple channels also but channel is what the processor is connected to any channel can have multiple dims I like thinking of them as modules dual inline memory module and each DIMM has a bunch of ranks usually one these days each rank is consistence ists of chips as we've just shown each chip consists of multiple banks so I haven't shown that over here but if you draw the chip boundary over here it actually consists of let's say 8 banks over here and each Bank consists of rows and columns and as you've seen each row each this two-dimensional structure consists of a single cell at the cross point right ok let's start with the bank as we've done before but adn Bank is a two-dimensional array of cells rows by columns unfortunately a dear I'm Rob is also called the DRAM page somebody at some point decided that it's nice to call it a page without thinking about the virtual memory subsystem where it they're also called a page so if you see page in the terminology in DM that's DM page that's not a virtual memory page which is different it can be the same size potentially but usually not they don't have to be basically sense amplifiers are also called the row buffer so each address that you issued to a DRAM chip is a row column pair basically in today's dia now if I was giving this lecture ten years ago I would be talking about some weird forms of DRM and I'm happy that I'm not giving this lecture ten years ago or maybe even even 20 years ago there were so many different types of weird erm but this is the one that has dominated the field today which doesn't mean that it's gonna dominate the field later on but basically we as we've seen before initially you have a row that's closed row buffer that doesn't contain anything and if you want to access that row and the row buffer doesn't contain anything you need to activate this activate command opens the row which places are all into the row buffer as we've seen again and then you can issue a read and write this read and write command reads or writes the column in the row buffer simple and then if you want to access another role after that you need to pre-charge you need to issue a pre charge command that closes the row and prepares the bank for the next axis basically charges a bit line such that you can sense the next axis and these are very well described in the required readings that I mentioned the chapters 1 and 2 of the TLD mmm and sell papers that go into very and clearly described within a few pages very quickly so if you access an open role open role means the row is already in the row buffer as again we've seen before there is no need for the activate comment right you can just simply issue a read and write command giving the column address ok this is a view of the DRAM chip internally well actually it's a dram bank here but you can imagine the DM chip also it's really a combination of these banks but basically you get the address from the DRAM controller it gets latched into this role address latch in this case it's 14 bits it doesn't have to be it depends on the DM and the DRAM Bank has two to the fourteen rows in this case and to the ten columns as you can see so as they resolved that bank could stores 2 to 24 bytes over here and you get the data here which gets amplified in this row buffer and later after you activate the data it gets into the row buffer you can send the column address it gets lights again and then you also send a command read or write for example well if you're doing reading basically you max out the data the 8 bits that you want based on the address and that goes back to the DM controller and each of these chips operate in parallel so that you get 64 bits and this is where the command comes but I didn't grout the command over here to not complicate this as we've discussed there D I'm ship consists of multiple of these banks sharing address data and command buses is there anything else that I want to say over here and as you know this is an abstraction as well so you clear if you think about this is a lot of interconnect rights it's a long interconnect that's exactly why you don't have even through the 14 some banks actually have to use the 15 rows today but this Bank is subdivided internally into sub banks and whenever you access you really access the sub bank ok but that sub bank is not visible today to the memory controller although we were trying to make that visible so what happens is you have very little interconnect to activate 0 so we actually have row buffers that I didn't show you over here other sense amplifiers internally that exists over there which is really interesting if you go into that so a bank operation I'm going to go through this really quickly because I've shown you this earlier if you remember you send their own address which brings the data into the row buffer that's the activates and then you send the column address and which gets the data out of the row buffer now if your next access is to the same role you get a row buffer hits you don't need to send the activate you send the column address and the read command if the next access is to the same row buffer role again Rob 0 you get an hits in the row buffer again and you send the column add that's clearly somebody needs to keep track of this row buffer right what's open in this Bank and that's the memory controller and if you get another address to a different row now you need to pre charge the array which closes the row buffer that's called the row buffer conflict and it takes time to pre-charge the array and you activate your 1 which brings the data into the role and then row buffer and then you issue a column address and get the data that you want okay it's pretty simple basically that's how the bank operates the memory controller needs to ensure that this operates correctly of course and their timing parameters related to this the time there's a timing parameter saying after I issue the activates I have to wait by this much to issue the read command after I issue the read command I have to wait this much until I can pre charge the erase and they're a bunch of these timing parameters in Indiana chip today it's there there there's so many timing parameters so that the control can optimize but that makes the controller complicated as well so there's a trade-off over there it didn't always used to be this way thirty years ago there was a synchronous DRAM where you could send a command and you would wait and the DRAM would return data and say I've done I'm done with what you wanted me to do that's another protocol right but today people move the synchronous DN because it was easier to design the circuitry and make it much much faster asynchronous was hard but maybe we may be going back to some asynchronous circuitry in the future we'll see okay so as we said the and chip consists of multiple banks eight is a common number today I think it's 16 in ddr4 or lpddr4 I don't I don't know but it's it's increasing also it comes at a cost or their there are weird things that people are adding it's a it's a very interesting space it's not only a technical space but it's a political space because somebody needs to agree on the interface right and these are very different companies yes well I did yeah I didn't go into the detail of the circuit operation but what really happens is when you actually activates you connect the word you connect the capacitors to the bit line all of the capacitance that Rome to the bit line charge sharing happens and what happens is the data gets destroyed in the role you basically sense the data you really destroyed the data completely in the role but then after some point sensing after you sense the data or while you're sensing that it's enzyme fire kicks in and one of the functions of the sense amplifier is to first amplify the data and figure out whether it's a one or zero or charged or discharged and the second is once you amplify the data to restore the data into the cell so the activates operation actually does all of this it does restore the data back into the cell and the readings that I mentioned actually explained that very clearly okay so banks share the command address data buses we've said the chip itself is a narrow interface so changing the number of banks size of the interface pins whether or not command address data buses are shared has significant impact on the DRAM system cost as a result where we are here where we are where we are in the historical space or historical progression of memory but this doesn't mean that this cannot be changed okay so this is actually an oldie I'm chip by now this is from a datasheet I believe from a micron data sheet I don't remember but you can see it's you can see what it's very similar to what I drove this I know you can do the calculation 128 Meg by 8 bit the edge of its 8 bit because you have 8 data out pins as you can see over here if you can read that and you can see that there's an address register there's a roll address box as I've shown you row address latch and decoder for different banks each Bank is independent as you can see after this point it's dependent at the at this point because they cannot they have to share this logic but here they don't they don't have any sharing of the logic and you have some column address counter and latch and what else should I show you and internally there's some prefetching that happens actually even though you you want to put data out it bits data internally you prefetch data so you see that the data path already has 32 bits right you fetched 32 bits and then you store them so the next access can be much faster this is called burst mode in DM you can basically have a burst because internally you have some prefetching you don't just get the the next 8 bits you get after one 8 bit access is much quicker that's because of the prefetching not because of the row buffer overs is another thing right ok so you get 8 bits but the next 8 bits can be faster next 8 bits can be faster next humming faster as long as you are within this 32-bits over here because the data is supplied directly from the sweet latch and right is very similar also actually over here okay you can we can study data sheets that look like this it's fun and also you can see that there's a refresh counter over here right you basically roll address can come from the external address pins or internally generated refresh address and as we've discussed today what happens in DM is the memory controller says refresh it doesn't tell what parts of the DM to refresh the memory control just says refresh and the DM internally generates which addresses to refresh and again the protocol is the memory controller says refresh and waits for a while such that the DM refreshes itself and the assumption is that DM does something to refresh stuff the memory controller just waits for that long that's called TR FC refresh cycle basically ok let's look at the rank and module a little bit basically we've said that multiple chips are operated together to form a wide interface that's what I rank is and all chips comprising I rank are controlled at the same time they respond to a single command and I'd like to soldier analogy you get kicked out of the army if you're actually don't obey the command right that's you cannot you cannot disobey the rank okay you share the address are in command busses about provide different data so ID our module consists of one or more ranks for example a dual inline memory module I don't want to go into the detail of what's a dual inline what's a single inline it's not important really so basically the module is what you plug into your motherboard and if you have chips with 8-bit interface to read eight bytes you use eight chips and at them and this is one example as you can see this is eight chips in a time and they all share the command buses they all share the data buses yeah no no they all sorry not share the data buses but they have they hear the command bus it's not depicting it's really well unfortunately what I drew earlier was depicting it much better they share the command bus but they reply with different data right so that you with eight different data points are over here actually this is a much better picture okay but I think one thing over here if you want to add error correcting codes today what how it's done is you actually add an another chip over here let's store their correcting codes so if you actually ever have seen a nine chip thing that's really an error correcting code e-wrap so it needs a wider interface so that you can get the error correcting code so it's more expensive not only because you have an extra chip but also because the DIMM becomes larger right or the DIMM requires more pins in this case right over here okay so this is my pictorial view of a 64-bit wide dim as you can see the command bus is shared but the addresses are actually as you can see over here you get different eight bits from different chips and you have a 64-bit wide channel over here and the memory controller and the address bus is shared I don't think it's clear in this picture I should fix that picture it's somebody else's picture okay so what's the advantage of a range like clubbing together different chips like this the advantage is now you can you can both have higher capacity and wide interface right you're getting both actually you have to get both unfortunately and also a memory controller doesn't need to deal with individual chips anymore the memory controller can say obey basically it's operating at the 64 bit level not 8-bit level the disadvantage is that now granularity writes your accesses cannot be smaller than the interface with what if you want just one bite well too bad you cannot get it because you're dim is designed to be 64 bits so if you want to have increase your capacity you add multiple dims again this is an ugly picture but that's okay basically the idea is to have multiple dims and interconnect them some somehow the summation apology if you follow it it doesn't matter even if it's not a mesh topology you can make it a mesh topology again that's not important what it looks like is not important the fact is that you basically have a controller and you can have multiple dims attached to it interconnected in some manner in fact it could be a daisy chain daisy chain mean meaning you have a single single connection here and then a buffer here and then you send the command if the command is not for this dim distant passes it along to the next one if it's not to that damn it did pass it along to that one if it's not to that them it pass it along to that one that's called the daisy chain now that's not the best way of designing and interconnect in fact there was this this dim called fully buffered imp that exactly operated that way which is not around anymore thankfully but that was basically daisy chained and the fact that the time it took to access this dim was much longer than the time it took this to access the stem and it's at all of other problems also like power consumption but basically if you connect multiple dims you get an even higher capacity the disadvantage all these interconnect complexity and energy consumption right so how do you get higher capacity is always a good question right the best way of getting capacity is really having a higher density chip that's why people are trying to scale DRAM right we want to actually have much much higher density chips such that you don't have to deal with this complexity in fact later we may see how other people deal with complexity people actually add in memory controllers I think yeah IBM called it SMC which I don't remember what SMC stands for right now but scalable memory controller is a good good good guess probably but basically it you add all of these intermediate memory controllers such that you can scale the capacity of memory the problem is interconnect complexity and energy consumption and additional latency that you get so scaling memory capacity is actually not easy ok ok sodium channels basically you can have two independent channels two memory controllers that look like this in fact most modern systems have multiple memory channels Intel i7 I believe has three memory channels maybe four I don't know now these could be independent or dependent dependent means let's say you have a 64-bit wide channel here we have a single memory controller it's not this picture you have a single memory controller 64 bits come from here 64 bits come from here if you want to get 128 bits at the same time you can have two channels right operated in lockstep it's not shown here okay and this is a generalized memory structure so basically it's an n-dimensional memory structure you have a channel you have multiple dim multiple modules and if you look at each module it consists of multiple DRAM chips each chip consists of multiple banks each Bank consists of Colin slash Rose and that's where the cache line resides so it takes awhile to get to your cache line and I think I will probably stop with this slide but I recommend this is another paper this paper that talks about the subarrays in the year but this is another view of the system I think this is there may be a nicer view right so you can see the channels banks ranks but essentially this is what you have in your computers today okay so maybe this is a good place to stop both right on time and right on the boundary any burning questions okay otherwise I'll see you either in the office hours tomorrow or next week have a good weekend "
YIHM38jv7-I,22,"Subject: Computer Science
Courses: Computer Architecture and Organisation",2019-02-05T05:42:30Z,Instruction Format and Addressing Modes,https://i.ytimg.com/vi/YIHM38jv7-I/hqdefault.jpg,Ch-13 Computer Science and Engineering,PT37M33S,false,1744,34,0,0,3,[Music] so welcome to the next lecture instruction format and addressing modes so what do you mean by instruction format so we know what is an instruction and instruction format is what it comprises of two parts first part is the opcode and the next part is the operand so what is an opcode so if we take an example let us say add r1 r2 opcode specifies the operation to be performed that means here what operation is to be performed the operation here is adding to register values and these two register values will be added and stored back in some register this is this instruction specifies so r1 will store r1 plus r2 add is the operation code that specifies the operation to be performed by the instruction and we can have various categories of instruction so this is an arithmetic instruction we can have an instruction called move what this instruction will do this instruction will move the data from r2 to r1 so here our one will have the value of r2 such kind of instructions are called data transfer instructions we can have other control instructions this is arithmetic instruction this is data transfer instruction we can have other branching instruction what kind of branching instruction let's say I want to go to some location so we can have an instruction called branch to some location let's a 16-bit hexadecimal number I am saying for a10 this is a branch instruction specifying that this instruction opcode is a branch instruction and go to this particular location so branch to this particular location will move to this particular location and whatever data is there in this particular location it will be added with PC and it will calculate the current instruction that needs to be executed because branch to this location means in some particular location some instruction is present which we need to execute so this part of the instruction we call it and of code and this part is the operand now see what can be an operand operand specifies either a single source or there can be two source and a destination of the operation and source operate can be specified by an immediate data I can just specify a number so that is an immediate data or by naming a register they're just now I have shown how we can just give the name of the register or specifying an memory address like specifying and memory address meaning we can have an instruction add r1 comma LOC a so in this we are specifying one operand a register another operand or memory location so an operand can be a register or memory location or I can also have an instruction which I call add immediate where I am adding with register one some immediate value let's say 10 so this kind of operand can also be specified and but this operand cannot be the destination operand a destination operand should always be either a register or it can be let's say he can have another instruction that means content of location a and the content of r1 will be added and then the result will be stored back in LOC a in loc a you will have content of LOC and r1 will be added and it will be stored in LOC so instruction consists of two parts one can be operation of code another will be operand and what all operand can be present I also discussed that number of operand varies from instruction to instruction that also we have already discussed that we can have a 0 address instruction we can have one address instruction we can even have a to address or even have a three address instruction so number of operands that are present in an instruction that may vary also specifying an operand we need to know the various addressing modes so coming to what is addressing modes we will be looking into in more details addressing modes actually is a way by which the location of the operand is specified in the instruction so there can be many possible addressing modes immediate direct indirect relative and index and many more are possible we will be seeing a few of them now let us see this instruction format if we have just the opcode like let us take some example of NOP NOP means no operation no operation instruction specifies the processor that no operation will be taken care at this particular cycle halt halt the execution for this this instruction halt will specify that hold the execution for some time we can have one address instruction where only one address is specified along with the opcode we can have to address instruction where we can specify two operand we can have to address instruction where both can be memory location addicts comma Y over it will be more we can have another instruction where one can be registered another can be memory operation or we can have another kind of instruction a three address instruction where all our registers so these are various instruction formats now consider a 32-bit instruction example so we have a 32-bit instruction so suppose our instruction set architecture is having only 32 bit instruction fixed size instructions makes the decoding easier let us understand this statement what do you mean by fixed size instruction makes the decoding easier let's say I'm just giving a example this is not corresponding to any real stuff let us say we have a instruction and and we have r1 and LOC a so this is an instruction and I have a 32-bit instruction and this 32-bit instruction some bits will be reserved for your opcode some bits will be reserved for your register so this can be registered destination this can be registered source and this can be your memory location if this is so the total is 32-bit and now we have to specify the bits in following let's say we have a total of 32 registers if you have a total of 32 registers how many bits will be required to specify a single register 32 bit so 2 to the power 5 so the first register can be represented so 5 bits we will be requiring to represent a single register so this 0 0 0 0 will be register 0 0 0 0 0 1 will be register 1 and so on and the last register will be register 31 so 5 bits will be required to specify one of the register so 5 bits will be required for this 5 bits will be required for this let's say you can specify a memory location which is 20 bit you can only have specify a 20-bit memory location so this will be 20 and then how many bits are remaining for the opcode then 20 plus 10 30 we have we have to frame or make our instruction within 32 bit so we have 2 bit left for opcode so if we have 2 bit left for opcode we can have maximum of how many possibilities 0 0 0 1 1 0 & 1 1 there can be 4 operation operation one operation to operation 3 and operation for these 4 operation can be taken care by this if we have this kind of format I am just giving you a example and this doesn't correspond to a real machine but rather just to give you an idea that how the instruction format will looks like so coming here fixed size instruction makes the decoding is here this statement easier how like you already know coming to this diagram once more you already know that first two bit is your operation up code so you can only check this code to know okay this is the operation you already know that these two is your register and you already know that this 20 bit is your memory location so if you already know that all these bits are fixed the total instruction 32 bit is fixed and these bits are fixed from these diseases we can have various kinds of instruction like here we have only one memory location if we afford to have we can have two memory location we can also have that but if you have such kind of this kind of mechanism where you already know well in advance okay these are fixed and this is how we have to do the decoding so the decoding task becomes much easier so the same thing that I have discussed some instruction encoding examples are shown assume that there are 32 register if we have 32 register all of 32 bits then how many bits are required to specify a resistor five bits if there are 64 register you will be requiring 6 bit and so on now let us see some format let's say this is the format where you have these many bits for opcode 26 27 28 29 30 and 31 6 bit for of good and these are for the register five five bit for register and then you have a 16 bit immediate data the 16 bit immediate data can be specified here let us take a sample example where we load our 1 100 or 2 what we are loading we are loading the content from memory location pointed by 10 plus r2 so first we have to add 10 plus r2 and then we have to put this 10 plus r2 in the MA activate the read control signal get the value of it and then what we do we load it into r1 1 these are the following steps that can be required to execute this instruction so let us see where all it will be pleased this is the destination register r1 1 so R 1 1 will be placed here so r1 1 is 0 1 0 1 1 opcode for load will be loaded in first 6 bits then this is the source operand one of the source operand is r2 it is loaded here and 100 is the immediate value which is loaded here so this is how you can encode your this instruction into this binary form again node can be having some value let's say 0 0 0 0 0 1 will be the load value so you can represent this using this particular format for this particular instruction let us move on now if we have limited of good facility that means we can only have 64 operations that are possible with 6 bit so if you want to increase that what can be done what we can do here is that here this of code will give you that what kind of function it will be taking care of like it can be an Lu function it can be a data transfer operation it can be something else or other an exact function will be specified by this 11 bit so let us see this so it is at R 2 R 5 and r8 so the content of r8 and R 5 will be added and will be stored in R 2 now we see that we know that this is an ALU operation this is the destination that is our to the source there are two source four sources are 5 and the next sources are 8 and this a new function is an odd function so we are encoding this particular instruction using this particular bits now moving on to addressing modes now see what we have understood till now that we can see this that we have instruction and we by seeing we are saying that ok this is a register this is a memory operation memory location but again you have to instruct the computer that see this is a register this is a memory location then only the your processor will do the required thing it will go to the memory location get the data it will go to a particular register and get the data so what are addressing modes and dressing modes are the ways by which the location of an operand is specified in an instruction let us move on what is an addressing mode so as I said they specify the mechanism by which the operand data can be loaded it specifies that the location of an operand in the instruction how the location of the operand is specified whether it is a register we have to get it from register or you have to get it from memory location etc some instructions that architectures are quite complex and supports many addressing modes but instruction set architecture that are based on load store they usually support very simple addressing modes so this is very important if you want to have complex addressing modes some of the instructions are architecture do have it but this load store architecture basically they support very limited number of addressing modes now what are the various addressing modes that exist immediate direct indirect register register indirect indexed stack relative auto increment or to decrement etc based etcetera now see I'm telling you about so many addressing modes but all computers all architectures will not have all the addressing modes this is the class of addressing modes that are existing in all the architectures but different architectures have a set of are addressing modes so we shall broke look into first some common addressing modes and how do they work coming to immediate addressing mode here the operand is a part of the instruction itself so you need not have to go anywhere to get the operand rather your operand is a part of your instruction so here no memory references requires no memory access is required to get the operand and it is fast but limited range because you cannot specify you can only specify a limited number using immediate mode click Add hash 25 when we write hash that means it's an immediate data so when we write add hash 25 that means 25 will be added with accumulator whatever value is in the accumulator it will be added with that and it will be stored in accumulator similarly so there is a mistake so this will be a die and this will be add so here this this will be add we are adding add our two okay sorry this is also an immediate data that we are adding so this immediate data will be added with our two and the result will be stored in our one so we have an out good is this and we have an immediate data that immediate data is 42 here which is added with content of R 2 and getting stored in R 1 moving on with direct addressing mode here the instruction contains a field that holds the memory address of the operand so in this direct addressing mode is that your that the field which specifies the memory address is your effective address that means this is the address from where you will get the operand so the operand here let's say this is 2 0 a 6 is the location and content of 2 0 is 6 will be the exact operon that you are looking for like here add R 1 comma 2 0 a 6 meaning whatever content is in a 2-0 is 6 will be added with R 1 and it will be stored back in R 1 now here how many memory operation is required as in the instruction you are specifying that dress so you have to go to this particular address and fetch the instruction so going to this particular a dress will give take one more memory axis so a single memory access is required to access the operand no additional calculation is required to determine the operand address and Limited address space so if this address space is 16-bit so it is limited so we can only have direct addressing within that 16-bit span so this is pictorially we can show this is the opcode this is the operand address so this is in the memory so in the memory this operant is there you go to that address you get that operand this is direct addressing let us move on indirect addressing the name itself suggests when it is indirect that means in the instruction what it contains it contains a field that holds the memory address which in turn holds the memory address of the operand so let us see this with an example let's say we have an instruction add r1 comma LOC a so if you have written add r1 comma loce and this is your memory loc a loc a contain another address loc B and now you will not get your operand from LOC a rather you will get your operand from LOC B so you go to LOC B and your operant is here so it is indirect it is not direct such that I go to location a and get the value like I got it in the previous case in this particular case you have to go to this location this location will give you one the location and you go to that particular location that will give you the value okay so in this case you can see that you are requiring two memory access so two memory accesses are required to get the operand value slower but can access large address space like in previous case we can only access that 16 bit but using indirect addressing the address space can be a little bit extended and it is not limited to number of bits in the operand address as I said like direct addressing so here add r1 comma to 0 is 6 where the content of this content of this memory location will give you the operand so this is the operand address first this is a pointer as I explained and then from there you go to another address which will give you the exact operand moving on with register addressing so register addressing is again straightforward the operand is held in a register and the instruction specifies the register number very few number of bits are needed as the number of registers is limited faster execution but no memory access is required for getting the operand so register addressing means you are specifying in the instruction the registers and you go to that particular register get the value so the value can be there no memory access is required for this you only go go to this particular register get the value and do the operation as required so model load store architecture supports large number of registers so as I said this is the register Bank so register number is specified in the instruction and you go to that particular register to get the Opera moving on with register indirect addressing mode here the instruction specifies a register and the register holds the memory address where the operant is stowed so this is also a kind of indirect thing where instead of a memory location here I am putting that value in a register putting the memory address in a register and then we are and this register holds what it holds the memory address but not the operand you have to go to that memory address to access the operand can access a large address space so one fewer memory axis as compared to indirect addressing mode so in here how many memory addresses required first you are hitting a register and then you are hitting this r5 now r5 contains some memory location you have to go to that particular memory location and get the value then you do accordingly so just see here this register will give you a memory location and this memory location is fetched from the memory the data from this memory location which is in the register is fetched from the memory and we get the data the operand so this is register indirect in the register we are putting an address and that address stores my operant moving on with relative addressing relative addressing is always with respect to PC why it is required let us see that the instruction this kind of addressing modes the instruction specifies an offset of displacement which is added to the program counter to get the effective address of the operand since the number of bits to specify the offset is limited the range of relative addressing is also limited so if a 12 bit offset is specified it can have values ranging from minus 2 0 4 8 2 + 2 0 4 8 let us understand this relative addressing means with respect to PC that means relative to PC how much you can go so for branch instruction if you recall what happens in branch instructions in the branch instruction we specify a branch address so when we specify a branch address that means I have to go to that particular location how will I go to that particular location to go to that particular location in the PC you have to load that particular address how will you load that particular address so this is an offset that is given in the instruction that is added or subtracted depending on where you are branching you are branching above or you are branching below the address that particular branch address will be added with the content of the PC and then the PC will be loaded with a new address where you have to go for branching so in such kind of in such kind of cases we require relative addressing modes so here you have an opcode this is the offset the offset is added with the content of PC and then where you go and you press the operand so your operant is residing by adding these two content then you fetch then you get the operand moving on with indexed addressing mode here in the previous case we have seen in relative addressing modes the content of PC is added with the offset value now here either a special purpose register or a general purpose register is used as index register in this addressing modes and where it is required you consider an array what is an airy array is a consecutive memory location so if you load a particular and you know the first address of an array how will you go to the next address next address we do we add plus with respect to how your memory is organized okay you add that and you go to the next location then you go to the next location and so on in a similar fashion here in index addressing mode it is somewhat required that you add that general purpose register value it can be used for indeed it can be used as index register and this instruction specifies an offset of displacement which is added to the index register to get the effective address of the operand so let us see with this example it will be more clear now see 1 0 5 0 in bracket r 3 that means content of R 3 will be added with 1 0 5 0 and then that location will give me the operand and where it can be used as I said let us first try to understand this once more so 1 0 5 0 is added with R 3 we get a value that value is loaded searched in memory that particular address is searched in memory and we get the operand and it can be used to sequentially access the elements of an array so in the element of an array we load the first address and then we move to the next next next and rest by adding an offset to it so offset gives the starting address of the array and the index register value specifies the array element to be used the first can be zeroth element then the next then next and so on so here this index register you get this added with this offset this particular address is searched and became the operand from there next comes to come to stack addressing in stack addressing we already know the operant is implicitly on the top of the stack and it is used in zero address machines much earlier where if I specify add this automatically means that the first two element on the top of the first top two elements of the stack will be taken out and will be added and will be stored back there push X will push the x value in to top of the stack pop X will take out the top value to this location so many processors have a special register which is called a stack pointer that keeps track of a top of the stack there are some other addressing modes as well like base addressing moons base addressing mode so in base addressing mode the processor has a special register called base register or segment register and then what happens here is all operand addresses generated are added to the base register to get the final memory address so now we have a register with respect to base register the processor generates address let's say the processor generates the address from 0 0 1 2 3 and then you have stored some address in base register let's say 1 0 2 4 so 1 0 2 4 will be added with that particular address to go to the exact address so this is what base addressing means and it allows easy movement of code and data in the memory we can also have another addressing mode or to increment an auto decrement addressing mode it was first introduced in a pdp-11 computer and pdp-11 was one of the most popular mini computers in the 1980s so auto increment and auto decrement it means that if you load an address loaded register with some address you can auto increment it you can access that value then you increment it odd or to decrement means you access the value then you decrement it so either way you can implement this so Auto increment Auto decrement we have also seen in C like a plus plus and a minus - Auto decrement an auto increment operators so in the similar way we can have such kind of addressing modes also so now we came to the end of lecture seven so what we have seen in this lecture is that addressing modes which are very important and the instruction format we will move on with the next in the next lecture we will be seeing now the types of architecture thank you 
bWsa7vv-ico,27,,2021-01-16T07:40:34Z,Lecture 13 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/bWsa7vv-ico/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT10M16S,false,61,1,0,0,9,computer architecture and it suspends execution of the current program being executed it executes hilo shitty suspense saves its context it is processor does the following number one it suspends execution of the current program being executed and subsets context second it sets the program counter to the starting address of an interrupt handler routine next after shikha saving the address of the next instruction to be executed that is the current through context of the program counter and any other data relevant to the processor's current activity so context money program counters activated an interrupt is just that an interaction of the normal so interrupt is nothing but an interruption of the normal sequence of instruction when the interrupt processing is completed chocolate interrupt processing [Music] foreign 
aDWBji_KY98,27,"In this video, we have discussed the Processor Organization. To understand the organization of the processor, let us consider the requirements placed on the processor, the things that it must do:
Fetch instruction, Interpret instruction, Fetch data, Process data , Write data. The Processor organization simplified view and the detailed view is discussed.

Playlist to watch 

8085 Microprocessor  :- https://www.youtube.com/playlist?list=PLiZaRKKs2Osi0SJ1wTunOEWv6WaWzjoSR

Latest Technology :- https://www.youtube.com/playlist?list=PLiZaRKKs2OshMdySpQy7NeLSI_3Mubn_W

8051 Microcontroller :- https://www.youtube.com/playlist?list=PLiZaRKKs2Oshaz-Nnn72SLzJzEg9krRj8
8086 Microprocessor :-https://www.youtube.com/playlist?list=PLiZaRKKs2OsjCt1oZSdBzZAIju4f0KioB
PIC 18 Microcontroller :-https://www.youtube.com/playlist?list=PLiZaRKKs2OsiXSeO1-II33oU03ZmqrmUN

Skills and Jobs:- https://www.youtube.com/playlist?list=PLiZaRKKs2OshFx3Glzc5MkTKo0eVf4_GZ


Arduino :- https://www.youtube.com/playlist?list=PLiZaRKKs2OsghHcmUNbwzYLVlvDclMLhL

AVR Microcontroller :- https://www.youtube.com/playlist?list=PLiZaRKKs2OsiJJ4VQft50GjODDA5MUf2x

Internet of Things (IOT) :- https://www.youtube.com/playlist?list=PLiZaRKKs2OsiLEUMxTH_H8p_yZQa-s91O

80386 Microprocessor  : -https://www.youtube.com/playlist?list=PLiZaRKKs2OshzVrgaBEm-pcJWe8MPiFR_
Computer Organization and Architecture: - https://www.youtube.com/playlist?list=PLiZaRKKs2OsiFB7NZAsS1QlZdneL_RcBY
Digital Logic Design:- 
https://www.youtube.com/playlist?list=PLiZaRKKs2Osi8qYWwE7nL7M9oQ1_EvZkv

Software Engineering and Project Management: -
https://www.youtube.com/playlist?list=PLiZaRKKs2OsiqAHn757B_wychAhhUxAA7
Follow Us on :-

website : - http://engineeringonline4u.com/

Facebook :- https://www.facebook.com/ulhaskumar.gokhale

Instagram :- https://www.instagram.com/gokhaleulhaskumar/

Twitter :- https://twitter.com/ugokhale

Thanks for Watching!

Enjoy !

Like, Share and Subscribe !

Our Channel : - https://www.youtube.com/engineeringtechnology4u

Also, Hit the bell icon for getting notified about all my future videos
#ProcessorOrganization",2019-10-31T07:11:11Z,PROCESSOR ORGANIZATION,https://i.ytimg.com/vi/aDWBji_KY98/hqdefault.jpg,Engineering and Technology 4 U,PT12M43S,false,2725,48,12,0,1,[Music] the topic for today is processor organization now organization of a processor can be understood by considering the requirements placed on the processor the things that it must do if for example it fetches the instructions from the memory then because the instruction and then face the data from the memory or i/o then after fetching the data it processes the data it will perform arithmetic or logical operation on the data and then it will write the data either in memory or to IO processor so for all these operations a processor needs small internal memory in today's video we'll discuss the simplified view of processor and a detailed view of processor let us start welcome to our channel engineering and technology for you if you are not subscribed to our channel kindly subscribe and press the bell icon so that you get notifications for our future videos on this subject the topic for today's processor organization let us start with the introduction see to understand the organization of processor were to consider the requirements placed on the processor that is nothing but the things that it must do first is the pitching of instruction say the processor reads an instruction from memory I say Katie or main memory or register then it will interpret the instruction interpret the instruction means decoding of the instruction so this may for this it will determine the action to be taken structure then once that is decided if the instruction requires data it will face the data again the data will come from memory our IO device then once we get the data the data will be processed so the processor processes the data by processing means it will either do arithmetic or logical operation on the data now whatever the processing of the data done by the processor this result has to be written in memory or to the i/o device but that operation we called it as the right data in this way the processor needs to store some data temporarily it must remember the location of the last instruction so that it knows from where to get the next instruction it also needs to store the data in other words the processor needs a small internal memory so let us see the organization of the processor the first is the CPU with the system bus in this Shanda processor consisting of the ALU that is the arithmetic and logic unit then there is a control unit and we have set of resistors so the ALU will perform the actual computation or processing of data so other arithmetic or logical or operations they are performed by them Ealing and the control unit controls the movement of data and instructions into or out of the processor and also controls the operation of the a then here no addition to this alien control unit we are shown the resistors but this resistors are nothing but the internal memory Oh consisting of tourist locations that's why they are called as the registers but these are internal to the processor and the processor has buzz database and address was connected to the processor to these buses combinely they are called as the system bus they are connected to the processor and the data instruction that will flow through all these buses so the address bus will carry the address the data bus will carry the data and the control bus will carry the control signals when this way this three buses will control the flow of data and the processor will perform the different operations on the date now let us see the detailed view of the processor internal structure of the CPU so here the central processing unit or CPU or processor whatever we call it it consists of the arithmetic and logic unit inside the arithmetic and logic unit we have the status flags Hey for example we have the straightest plane here means the carry flag auxilary carry flag and sign flag then the zero flag Perry declined that's flex they are called as the status flags will be present in the arithmetic and logic unit and then we have the shifter is for shifting the data left or right then there is a complement complement the data and we have the arithmetic and boolean logic which will perform the arithmetic and the logic operation on the data so the data transfer and logic control paths are indicated here and there is an internal processor bus here you can see internal CPU us so this is bi-directional data and then again we have bi-directional flop data to the resistors and controlling it with also how the flow of data and control signals and this is the the what photo date in this way the the ALU operates fast and because L you in fact operates only on the data in the internal processor memory it it will perform the arithmetic operations on the date and this internal bus it will carry the data from resistors with the to the claw of the data will be through this internal C pubis so in this way the detailed structure when we study the detail structure will to study all the resistors present in this CPU as well as the details of the arithmetic and logic unit so this is how the processor will be organized with this now you can see the similarity between the computer and the processor so here sector of the computer and the internal structure of the processor they are similar because computer also consists of say the major elements in computer and major elements in the processor we have or computer we have the processor then input and output that is IO and memory so when they are connected it forms the computer system and the flow of data will be from memory to the processor our input/output device to the processor and from processor to the input/output device or the memory in this way the data will flow from one element to other element in case of computer similarly for the processor we have a control unit ALU and resistors and that's why the data will flow in this three elements in case of the processor so that's why the organization of the processor is similar to the organization of the computer so with this we come to the end of this video if you have any questions you can contact me on Facebook Twitter Gmail or Instagram then if you liked the video press the like button and share with your friends with delft of this icon which is available on the YouTube and subscribe to our channel Engineering and Technology for you don't forget to press the Bella that if you raise the bail icon after subscribing will get notifications for our future videos so that's why you not have to search the videos on YouTube and directly will get the notification for the videos then thanks for watching have a nice day [Music] [Music] 
tM6Mpyk2bjE,27,,2020-04-16T17:08:03Z,Computer Architecture and Organization Unit-IV DMA- Lecture-4 Er. Maneesh Singhal,https://i.ytimg.com/vi/tM6Mpyk2bjE/hqdefault.jpg,Arya College of Engineering and IT,PT7M41S,false,38,3,3,0,0,the subject is computer architecture and organization this is unit number four input output organization lecture number four the topic is DMA direct memory access myself money single assistant professor department of computer science and information technology are the college of engineering and information technology contained direct memory access DMA direct memory access in this in the direct memory access the interface transfer the data into and out of memory unit through the memory buses that transfer of data between here a fast storage device such as magnetic disk and memories of unlimited by the speed of the CPU removing the CPU from the path and letting the peripheral devices Menace the memory buses directly would improve the speed of transfer this transfer technique is called direct memory access DMA during the DMA transfer the CPU is ideal and has no control of memory buses a DMA controller take over the buses too many that transfer directly between the input output device and memory direct memory access DMA the CPU may be placed in an ideal estate in a variety of ways one common method extensively used in microprocessor is to disable the buses through special control signals such as bus request VR and bus grant BG these two control signals in the CPU that facilitates the DMA transfer the bus request be our input is used by the DMA controller to request the CPU benda when this input is active the CPU terminates the tradition of the current instruction in place the address bus data bus and read write lines into a high impedance state the high impedance state means that the output is disconnected this is the diagram of the DMA direct memory access CPU buses signal from DMA transfer there are two inputs bus request and thus grant there are address was data bus read signal write signal when high impedance disabled then busy is enable the CPU activate the bus grant output to inform the external DMA that the person requests PR can now take control of the buses to conduct memory transfer without processor when the DMV terminates that transfer it disabled the bus request be Airlines the CPU disables the bus grant BG take control of the buses and return to its normal operation the transfer can be made in several ways that are TM a crushed and secondly cycle is telling the what is the DME crushed in DMA first transfer a block sequence constitute constant consisting of a number of memory words is transfer in continuous brushed pile the DMA controller is master of the memory buses cycling is tailing cycling is taking allow the DMA controller to transfer one data word at a time after which it must return control of the buses to the CPU the DMA controller needs the usual circuit of an interface to communicate with the CPU and input/output device the DMA controller has three register address register word counter register and third is control register what is the address register the address register contains an address to specify the desire location in memory second is word count Rajesh WC holds a number of words to be transferred the register is increased and decreased by the after the H word transfer and initially internally tested for Giro control register control register specifies the mode of the transfer the unit communicates what is DMA controller the unit communicates with the CPU by the data buses and controllers the register in the DMA are selected by the CPU through the address was by enabling the dsds means DMA select and rs means register select input the Rd means read and the blower means write input are bi-directional when the BG it means bus grant input is 0 the CPU can communicate with the DME register through the data buses to read from all right to the DME registers when VG is equal to 1 the DMA can communicate directly with the memory by specifying an address in the address buses and activating the Rd order lower control this is the diagram of the DME controller there are data bus buffer there are a transfer there are address register word count register and control register the internal buses are connected through the different data bus buffer and address bus and an address Arista word count register and control register to input device DMA request and DMA acknowledgement they are TM e select register select read/write bus request was granted in fact what is DMA transfer the CPU communicates with the DMA through the address and data buses as with any interface unit the DME has its own address which activates the D s and RS lines the CPU initialize the DMA throw the data was burns the DME received the start control command it can transfer between the peripheral and the memory when VG is equal to G Rd and W are our input lines allow the CPU to communicate with the internal DME register when B is equal to when the Rd NW are our output lines from the DMA controller to the random access memory to specify the read or write operation of data this is the brief summary in the direct memory access it means DMA the interface transfer the data into the output of memory unit through the memory buses thank you 
6XGrG1xd7_4,27,"Demonstrating simply the Integrated Circuit Cost as in Chapter 1.

This is tutorial 1(part1) of ECEN 402 Computer Architecture course as taught at Nile University.
Book: MK Computer Organization and Design 5th Edition.
Professor: Dr. Mahmoud Allam
Teaching Assistant: Ahmed Mosharafa

Link to download the PDF https://www.dropbox.com/s/9sa4dvg7aw6imqx/Tutorial%201.pdf?dl=0",2016-03-10T00:47:17Z,Tutorial 1(Part 1: Integrated Circuit Cost Demonstration),https://i.ytimg.com/vi/6XGrG1xd7_4/hqdefault.jpg,Ahmed Mosharafa,PT3M16S,false,5181,34,3,0,1,hello and welcome to computer architecture here I will demonstrate the integrated circuit coast in the beginning we have the basic rule for word as calculating the cost per die the cost per day we're really interested in calculated is the cost per wafer over dies per wafer times yield so let's demonstrate some terminologies before starting so what's the wafer the wafer is actually the slice of semiconductor material such as silicon shown here this is really the silicon wafer so the icepad wafer what I really dyes dyes are the individual rectangular sections that are cut from a wafer to form our chips so if we zoom in here we find that these small cuts or the dies so the yield is the one remaining what's the yield the yield is as the name implies the percentage of good dies from the total number of dies in the wave so to calculate the cost pair the die we divide the cost per the whole wafer over the dies per wafer times yield so what do we have from here we have the cost per wafer the cost is the amount of money I pay for the wafer shown here so that's not a calculated thing the second thing is that I spare waiver how do I know that I spare wafer here appears the dies per wafer equation it's really the wayfarer area over die area this is really an approximation why is that an approximation because the the silicon wafer is rounded as you see so the dyes on the edge are not that of use so this is approximated here as the total wave affair area over the die area disregarding the ones on the edges so here we knew how to calculate the dyes per wafer so the one over one plus defects per area per area means such times diarrhea over 200 square recurring cost per wafer over time's the really percentage of the good dies makes sense right so 
FOQ2O844Ybs,22,This intro has nice differences between computer organization and computer architecture .,2017-09-20T03:28:29Z,Introduction to computer evolution,https://i.ytimg.com/vi/FOQ2O844Ybs/hqdefault.jpg,Computer Organization and Architecture,PT34M15S,false,61,0,0,0,0,"[Music] I welcome you all to the moon calls on computer architecture and organization in this particular course we expect to cover various aspects of computer design where we will be seeing how we can make a computer faster how a computer actually works how the information data are stored there and various other aspects the lectures will span over 12 weeks where we will cover the instruction set architecture processor design arithmetic and logic unit design memory unit design input/output system design and then we will also cover parallel processing and pipeline to start with this course I will come to first evolution of computer system so we all know that computer has become a part and parcel of our daily lives we cannot disagree to this fact we see everywhere computers that is some kind of processing unit you see you think about a laptop which we use in our daily use tablets mobile phones which are used by one and all today and intelligent appliances like of course your smartphone is one apart from that you have Smart Watch smart cracker and various other appliances so computer has become a part and parcel of our life so we need to understand how a computer actually works so what is there inside a computer so we in this particular course we will be seeing all these various aspects where the two terms computer architecture and computer organization will be taken care now coming to what is computer architecture and what is computer organization the title of the course so here computer architecture consists of those attributes of the system that are visible to the programmer by this what we mean is how the various components of a computer system are integrated to achieve the desired level of performance in an analogue II I can say that you think of an architect who does who plans the entire design of your house but it is ultimately the civil engineers who actually does the exact building like what kind what kind of construction will be taken care of how the construction will be taken care of how much percentage of cement breaks will be there are all taken care by a civil engineer so in that respect the design of components and functional blocks using which computer systems are built comes to the organizational part so I will take a very small example like you have in your computer some functional blocks like your processor unit inside processor unit we will be seeing that we have many other components like your registers your lÃ¼ and all other units I will just take a small example let's say I will happen but that adder what kind of adder I will be having that is to the discretion of the computer organization whether we will have a carry save adder or a carry you can't edit or anything else so these are the two different aspects of computer organization and computer architecture that we will be seeing in this particular course coming to the historical perspective how computers have evolved over the years so whenever there is a need for doing certain things then only something comes up like a constant quest of building automatic computing machines have driven the development of computers so in the initial efforts some mechanical devices like police livers gears they were built during World War two mechanical relays were used to perform some kind of computation like using small relays people design circuits to carry out the operations then comes vacuum tubes where the first electronic computer called ENIAC was developed and from then semiconductor transistors were developed when semiconductor transistors came into picture then the journey of miniaturization started first with small scale integration then people moved with medium scale integration then large scale integration then to very large scale integration and now the error of ultra large scale integration where we stand today so this is the first mechanical calculator that was invented by B Pascal so this particular calculator could add only two numbers it can only add two numbers or it can only subtract two numbers and if you wanted to do multiplication and division it could have been done by repeated addition or repeated subtraction then the Babbage engine came this was the first automatic computing engine designed by father of computer Charles Babbage in the nineteenth century but he could not build that only he designed that later in 2002 that is 153 years after it was designed it was built and it consistent it consists of 8,000 parts and it weighted 5 tons and it was 11 feet in length so you can imagine how large it is the first electric electronic computer was built which is called ENIAC electrical numerical integrated and calculator it was developed by University of Pennsylvania and it uses 18000 vacuum tubes and waited 30 tons and it also occupied a very large space which is 30 feet cross 50 feet so what is a vacuum tube vacuum tube is a device that controls electric current between electrodes in an evacuated container in a team so using those vacuum tubes the first computer niak was built it also disappeared it a huge amount of heat next was how would mark one this was built at the University of Howard in 1944 with support from IBM and it used mechanical release and of course some electric signals were also used to work with the release to represent the data and it also waited 35 tons and required 500 miles of very so these are the computers which were built in the early stages then comes in 60s and 70s where the popular mainframe computer came into picture so this popular computer IBM system/360 was introduced it introduces many advanced architectural concept that we use today but you can see from the picture that how big it was but some as I said some of the architectural aspects or concepts that were used in that computer appeared in today's microprocessor several decades later and now where we stand we stand where the modern processor chip is that has come in dual core quad core and also in six core variant so you can see that how many core today's computer has and it is a 64-bit processor that comes with various microarchitectures these are the various microarchitectures within this is an i7 but it has also got various other architectures where they are called Haskell Haswell Nehalem Sandy Bridge etc now coming to the generation of computers so broadly we can classify the growth of computer how computer has evolved into some generations so these actually represent some features of operating system and representative machines so between the years 1945 to 54 vacuum tubes and relays were used and the representative system uses machine language so if you want to enter some data into the computer for processing you have to enter in machine language or either in assembly language no high-level language were used to enter the data then comes the second generation where transistors memories are you processors are the main technologies here batch processing this is an property of OS that was used and you could enter the data in high-level language then comes the third generation where we were into semiconductor industry where small scale integration and medium scale integration integrated circuits were used and in that IBM 360 Intel 8 zero zero eight where the first one to make the step into this computer market then in the fourth and fifth generation this large-scale integration very large-scale integration came into picture and we could see multi processors now what has happened is like the space has become larger because the components that we use to build those basic blocks has become small so in the same space earlier we could keep small number of components now we could keep more number of components into it and hence this multiprocessors comes into picture and now we are in the era of ultra large-scale integration with many scalable architectures and post CMOS technologies and now we also have massively parallel processors like Pentium Sun ultra workstations Express now you see that evolution of the types of computer so initially mainframes came in the year 60s and late 70 in between 60s and 70s then came mini computers then came workstations and finally we see personal computers and then further we see laptops and smartphones so every smartphone today is having some processing capability that is why we call it smart phone with some smart features into it and we also have now millimeter scale sensors and these are not only sensors that it will send something rather these sensors are having some intelligent capability like some processing capability which can process those data from the collected from the sensors and also it has the communication feature so that it can communicate to others if required so this millimeter scale sensors have both the capabilities of processing as well as communication so we can see how the evolution of types of computer systems have come up and now where we stand in the future we see large scale IOT based systems where these sensors it's a very small sensor will have not only the sensing capability but also the processing capability wearable computing also and intelligent objects now you see this evolution of PC form practice over the years this is the first standard ATX where you can see the motherboard slot you can see this is a motherboard you can see the processor you can see the slots for memories for all other components now how that miniaturization has taken place first it was with it was standard ATX then micro ATX then goes mini ITX now nano ITX now we are in pico ITX so it is roughly equivalent to the size of a pen that our form factors evolution of PC form factors over the years how it has evolved so we are in the era of miniaturization and now with miniaturization of course we are able to do some good thing for performance but at the same time power issue is one of the important factor that we have to also handle which we cannot deny now inside a laptop if you see what all components we have so we are there is a we have shrinked the entire component then miniaturization in feature size of all parts have been done not only that if you have to make if you want to make a system smaller then it has to be smaller for all other components which consists of that particular component so hard drive is now getting replaced by flash based memory devices and because of miniaturization cooling is a major issue that has come now this is the famous Moore's law that refers to an observation made by Intel co-founder Gordon Moore in 1965 what he noticed is that the number of transistors per square inch on integrated circuits had doubled every year since invention and thus law predicts that this trend will continue into the foreseeable picture but now Moore's law stand here where it says that the number of transistor that can be placed in an integrated circuit get doubles in every 18 months and this has hold over a period now you can see this diagram which shows actually this graph actually shows how this how moose this is the number of transistor per chip and this is over the years how it has grown so this shows that the straight line actually indicates that Moore's law holds and starting from four zero zero four we are now in Ivy Bridge Sandy Bridge Ivy Bridge various architectural advancement has taken place and number of transistor that can be put and put in inner chi' has doubled in every 18 months and where they stand today this Moore's law shows you a picture and as long as it is a straight line we can say that Moore's law holds now coming to the simplified block diagram of a computer system so in this diagram what you can see is that we have a processor the processor is having control unit and arithmetic logic unit we have a memory memory is divided into primary memory and secondary memory and you have input devices and output devices in this architecture course we will be taking each and every aspect of this design like we will be seeing the processor unit design in the processor unit design there are two parts we will be seeing control unit design and arithmetic logic unit design we will also look into the memory memory is broadly divided into primary memory and secondary memory what we have in primary memory we will be seeing what we have in secondary memory we will be seen and of course how the inputs are given to the computer and how we get the outputs from the computer that also we will be C so all instructions and data are stored in memory whatever instruction or data that we want to execute that is stored in memory and every time an instruction and data is required it is brought from the memory to the processor for execution and input/output devices are connected to it so if input is required input is taken from an input device processing takes place in the processor and the output is provided enough output device this typical architecture where we store both program and data in the memory alongside we call it von Neumann architecture we will be seeing this in more detail little later now let us see what is there inside a processor a processor is also called a central processing unit it consists of an control unit and it consists of an arithmetic logic unit all the calculations that happens inside the CPU so any arithmetic computation so we need to add two numbers we need to divide two numbers or any arithmetic operations that are performed in sidon ALU the control unit basically generates the sequence of control signals to carry out all operations so all the operations that are performed it is it the control unit generates those sequence of control signals that is when I say that we will execute an instruction so the computer you have to instruct the computer okay execute this particular instruction so giving the computer some kind of instruction some kind of control signals that yes now you have to excuse this now you have to execute that and finally you have to store the result or you have to display the result so all these steps that we are instructing to a computer is generated by the control unit so processor fetches an instruction for memory from memory for execution and what is an instruction and instruction actually specify the exact operation that is to be performed so the instruction will tell you okay add a comma B so a B are some operands that performs the required operation what is the operation the operation is at so you have to instruct that yes this is an operation that has to be performed on some operands like it also specifies the data that is to be that are to be operated on and now a program what is a program I have talked about a single instruction that is add a comma B now a program is a set of instruction like I need to add ten numbers so adding ten numbers I have to write a set of some set of some instructions though those set of some instructions are written to perform that particular task what is the task and in ten numbers so that is nothing but a program so a program is a set of instructions that constitute a program now what is the role of Lu Lu or the processor unit we can say it consists of several registers some registers are called general-purpose registers and some are called special purpose registers and some of them are temporary storage so what are these registers registers are some storage unit and these registers are used to store data then again we compute some operation and again store back that result into it so we store also the data so we store data for computation and after the computation is performed we also store back the data it contains circuitry to carry out the logic operation so basically when we say we are adding two numbers we are subtracting two numbers we are doing some kind of operation some kind of logic operation is performed to carry out that particular operation so it also contains circuitry to carry out arithmetic operations like addition subtraction multiplication division we will be seeing in more detail every aspect of Lu in a separate unit now during execution the data or the operands are brought in and stored in some registered the desired operation is carried out and the result is stored back in some register or memory so what does it mean it means that during an instruction execution what happens my instructions and data are stored in some either in some memory location so we have to bring the data from those memory location into some of the registers perform the operation and then we store back the data into either register or into those memory location finally what is the role of control unit as I said earlier only it acts as the it generates the signals that is necessary to perform the task so it acts as a nerve center that senses the states of various functional units and send control signals to control the states that means let's say you have to carry out an operation or you have to add r2 and r3 and store back in r1 so r2 and r3 r2 register r1 is also an register so what you need to do is that you need to enable the output of r2 and r3 such that the output of r2 and r3 is available in a place where you can do the operation after the operation is performed then it is stored in the result is stored in the register r3 so you have to store the output in a circuit into the of the address circuit into register r1 now when we say an instruction is fetched from the memory an instruction let us say this is an instruction add a comma B this is an instruction and r1 comma this is also an instruction so this instruction will do what it consists of two parts the first part we call it opcode and next part is the operand of course specifies what operation I will be doing and operant is on which we will be doing the operation so in this particular case the operation that I will be doing is ad and on which I will be operating is our 1 and part these are the 2 registers where I will be doing the operation now memory unit there are two types of memory subsystems two main types of course we will see that again primary memory we have other kinds of things are the kinds of memories so primary or main memory stores the active instructions and data for program being executed on the processor and the secondary memory is used as backup and stores all active and inactive programs and data typically the files now the processor can only has a direct access to primary memory as I said the programs and data is stored in your primary memory and whenever it is required the processor arts ask it from your primary memory and not from your secondary memory and in reality the memory system is implemented as a hierarchy of several levels so we have l1 cache as I said your primary memory you will be seeing it as different levels so we have l1 cache we have l2 cache we have l3 cache primary memory and secondary memory what is the objective of all these things to make the processing faster so we will be seeing all these aspects in course of time but for now you must know that in the memory unit we store instructions and data and for processing of those instructions on data you have to bring those instructions and data to the processor to execute it now we have various different types of memory random access memory read-only memory we have magnetic disk we have flash memories random access memory it is used for cache and primary memory and read and write access times are independent of the location being accessed that means you either access location one or you access location the last location or the middle location the access time will be same read only memories are used as a part of primary memory to store some fixed data that is not required to be changed magnetic disk it uses direction of magnetization of tiny magnetic particles on a metallic surface to store the data and the access time vary depending on the location being accessed and these are used in secondary memory now flash memories are coming in to market which is replacing this magnetic disk as secondary memory they are much faster and smaller in size and they do not have any movable now these are the pictures showing these our first picture shows the me primary memory next one is the ROM this is an hard disk if you open a hard disk it looks like this this one is the SSD and so on coming to the input unit input unit is used to feed data to the computer system the inputs that we received from the computer so commonly used devices are keyboards mouse joystick and camera these are the relevant pictures that are shown and the output unit is used to send the result of some computation to outside world like printer is used to print the data LCD screen or LED screen is used to see the output on the screen you have speakers you have projection system that are also used as an output unit so these are some of the relevant pictures showing the output unit so we have come to the end of lecture one where we have seen how computer has evolved over the years and how what are the main components main functional components of a computer system and how these components are required and how we can execute a particular instruction thank you "
BeN9X4d5hk4,22,CS501 Advanced Computer Architecture,2008-07-14T13:52:49Z,CS501_Lecture01,https://i.ytimg.com/vi/BeN9X4d5hk4/hqdefault.jpg,vu,PT58M1S,false,41691,64,18,0,21,smilla rahmanir rahim I am dr. Norma hamish a professor Department of Electrical Engineering University of Engineering and Technology Lahore today we are going to start with a new course on computer architecture the course material has been primarily prepared by dr. Anjali who is a professor at all  me Institute of computer science at University of Engineering and Technology Lahore today in the first lecture we'll start with some fundamentals of computer architecture we'll just try to understand what computer architecture is and then some generalized notions and gradually we will go to more complex notions and try to understand more insight into the different parts and different subsystems of a computer let us now look at a few slides by now you know that how a digital computer operates a computer operates under the control of instructions stored in its own memory unit that can accept data as an input process this data arithmetic aliy and or logically produce output from the processing and then store the results for any future use the generalized block diagram is shown in this slide as you know there are three major parts of any computer a memory arithmetic logic unit and control the input and output are connected to this main block within this subsystem the connection between different units is through different buses the main buses are data bus address bus and control bus you see in the block diagram for the generalized computer the CPU the central processing unit is connected to the address bus control bus and the data bus similarly the memory contents could be accessed through these address buses or data bus the input/output connects the peripherals to the main computer what is computer architecture architecture generally could be considered as an art or science of building or it could be interpreted as a style and method of design and construction the term computer architecture was coined at IBM in 60s it was used to refer to the programmer visible portion of the instruction set of the IBM 360 family of computers the structure of a computer that a machine language programmer must understand to write correct programs for the machine to be visible and to be operative there is usually a confusion between the two terms computer architecture and computer organization computer architecture usually refers to those attributes of a computer which tell us how to design a computer like set instruction set how the memory would be organized how would be the interconnection between the memory and different buses whereas the computer organization usually refers to the operational features of a computer for example what particular instructions should be there in a set of computer instruction that belongs to computer architecture whereas how these computer instructions would be implemented this is a feature belonging to computer organization let us now look at a few more slides related to the computer architecture now let us see who is a computer architect the answer to this question is that computer architect is a person who designs computers what do we mean by design design is the process of devising a system component or process to meet desired needs what is meant by design it's a decision making process and usually it is iterative in which the basic sciences and mathematics and engineering sciences are applied to convert resources optimally to meet a stated objective there is a very close analogy between automobiles and computers there are people who know how to drive a car and similarly there are people who know how to operate a computer these are computer users the computer users just are interested in an efficient use of the computer similarly there are people who could repair automobiles these are technicians exactly in the same way there are people who could repair computers our computer technicians finally there are very few people in the world who could really design new cars these are the automobile designers exactly in the same way there are very few people who could really design and invent new computers and these are exactly the people who are the computer architects we are going to learn how to design computers in this course however before we can understand how to design we should know some of the existing designs for different computers this will look into this course as we go through this course before the design of any computer an abstraction usually helps in the design we are going to look at a few abstractions now let us see some of the slides on abstraction the abstraction could be looked at from three angles from system point of view from logic design point of view and circuit point of view from system design point of view there are three abstractions which are usually used the first abstraction usually called processor memory switch level abstraction looks into system components and their interconnections the components are specified in this abstraction in the form of a block diagram the processor includes the data path as well as the control ii system level abstraction is the instruction set level the function of each instruction is defined in this abstraction the emphasis is on the behavior of the system rather than the hardware structure of the system the third system design abstraction is the register transfer level this is the hardware abstraction hardware structure is more visible in this abstraction the basic elements indicated in the abstraction are the registers the second abstraction is the logic design abstraction this also called gate level abstraction we use gates and flip-flops the behavior in this case is less visible while the hardware structure dominates the next abstraction is the circuit design abstraction circuit design abstraction emphasizes and gives a visibility to resistors transistors capacitors diodes and other components finally the lowest level of abstraction is the mask level this shows the silicon structure of the chip the layout and its implementation is given in integrated circuit design in this abstraction the layout design is useful for final fabrication however the system design is most commonly used by a computer architect on successful completion of this course a student is supposed to understand the different points of view of studying computer architecture in particular the instruction set abstraction the register level abstraction should be very clearly understood secondly the student is supposed to know the combinational and sequential circuits and should be able to design more complex structures of ALU arithmetic logic unit by using combinational and sequential circuits finally the different memory structures should also be visible and a student should be able to understand the organization and utilization of different memory structures the course outline which we are going to follow would be starting with a very simple computer organization then computer architecture and then we will develop a very simple example of a simplified processor for understanding the different notions of computer architecture the next slide would show which textbook and which reference book books we are going to use for this particular course the textbook for this course is computer systems design and architecture by VP wiring and H of Jordan it was published in 1997 there are other two excellent reference books computer architecture a qualitative approach this is the second edition by Patterson and it's a 1996 edition computer organization and architecture by William Stallings is also an excellent book and sixth edition is available and is published by prentice hall now from these text books and the reference books a student would be able to understand clearly computer organization computer architecture the design of ISA bus other examples of different processors design of CPU and advanced topics in processor design the interfacing with input and output and implementation of arithmetic logic unit and finally the memory subsystems let us now look at different modules which we have designed for this particular course altogether there would be 11 modules for this course the first module would cover up the computer organization and architecture design it would also cover different levels of abstraction in digital system design and an introduction to different topics to be followed in following modules would be discussed second module would discuss in detail the computer organization in organization the different perspectives of different people about computers would be discussed in particular we will see the point of view of users the point of view of designers and the point of view of final evaluation of the organization general operation of a stored-program digital computer would be discussed the concept of fetch execute process would be discussed and finally the concept of ISA would be looked into the third module will discuss foundations of computer architecture a taxonomy of computers and the instruction set would be discussed in detail the instruction set features of some example processors would be discussed different addressing modes of the processors would be discussed with specific examples the reduced instruction and the complex instruction architectures that is the risk and sisk architectures would be discussed in detail finally the measures of performance would be looked into the next module 4 would develop an example processor introduction to the ISA and instruction formats would be given coding examples with hand assembly would be taken up using RTL we will describe a simple example processor s RC the implementation with register transfer logic and digital logic circuits would be taken module 5 would discuss the design and development of a si outline of the thinking process for is a design would be taken up we will introduce the is a concept of a typical example called Falcon a we will also take up the learning for this example as Falcon a module six would take another example processor Falcon II followed by Eagle and modified Eagle finally in this module we will make a comparison of all these four IOC's module seven would take up the important feature of CPU design which is a core of computer architecture course a uni bus data path implementation for the SRC will be taken up structural RTL descriptions for the SRC instructions would be looked into we will discuss logic design for the uni bus s r/c we'll also see how the control unit would be developed the two and three bus processor implementations would also be looked in more detail the machine reset process would be taken up and explained and some of the machine exceptions would be analyzed module eight would discuss some of the advanced topics in processor design like pipelining instruction level parallelism and finally the micro programming module nine would take up the i/o interfacing we will see how the i/o interface design could be implemented will see the programmed haeyo concept and also the interrupt-driven are your concept and finally the process of direct memory access the DMA would be analyzed module 10 would take up the implementation of arithmetic logic unit examples with addition subtraction multiplication and division both for integer implementation and floating-point implementation would be taken up the last module 11 would discuss different memory subsystems we will look into the general organization of memory the hierarchy of the memory the cache memory and the final virtual memory organization with that our course would be finalized and hopefully we will learn the most important concepts of the computer architecture we have observed that a computer architect is a person who has to design computers he has to optimize all the resources available to him for a create an efficient design he needs to understand the perception of computers by different people there could be the current perceptions let us look at four different perceptions number one the perception by a user of the computer the view by a programmer of the computer and the view by an architect of a computer and finally the logic designer of a computer all these perceptions would finally help and architect a computer architect to better design a computer let us look into these perceptions in a little bit more detail now we look first at how the user perceives the computer let us see at this light a user is a person employing the computer to do useful work useful work is quite a relative term for a for an office assistant the useful work could be in the form of using a spreadsheet or a word processor or a user could develop programs in higher-level language for an engineering student the computer could be used in the form of a CAD cam device in all cases however one thing is quite clear for the user the internal structure of the computer is totally hidden its obscure from him he is not interested in the detailed implementation of the computer however he is interested how fast the computer can execute his application he is interested in execution speed the storage capacity and the functionality of the peripheral devices what different peripheral devices could be connected could he use a USB port or not as a user he would like one would like to have as fast a computer as possible and as much memory as could be made available now after looking at the users view let us now see the next view how a programmer views the computer programmer in our context is a person who programs either in machine language or in assembly language machine language is the native language of a computer whatever is the eventual language used by the programmer computer is going to store and process data in the form of bits and bytes and the storage is in the form ones and zeros in early days the machine language programming was possible and it was done but it is really tedious to program in machine language the programmer to facilitate the job changed over to assembly language the assembly language programming is basically an English like language indicating the instructions or commands executed by the computer let us look at some features of the assembly language programming assembly language commands are a symbolic representation of machine language command using English like key words called mnemonics using assembly language commands it's much easier to program then as compared to machine language in the form of long strings of ones and zeros the commands in assembly language have a one-to-one correspondence with machine language commands however there are a few exceptions the exceptions could be that where we have a jump you have learnt details of assembly language programming in an earlier course just to illustrate one example just please have a look at this slide where an example from the Intel x86 family of computers is illustrated in the first column the instructions are given in assembly language second column indicates the machine language equivalent in the form of binary ones and zeros to write it in a short form the third column illustrates the hex notation and the last and fourth column indicates what type of instruction is being executed for example in the first row is an add instruction where the contents of registers DX + CX are added and the result is stored in register CX this is an example of arithmetic instruction similarly the second example the move instruction is a data transfer instruction the third row in it's an example of a logic operation and finally the last row indicates a control instruction in the form of jump here you may note that alpha is just a label it does not represent an absolute address and it would depend after execution that where you are going to jump to let us now look at some of the features where assembly language program would be used it appears that it's not easy to program in assembly or in machine language nevertheless assembly language is not dead and it is still very frequently used the question is who are the users of assembly language first of all you will appreciate that anybody who has to design a machine that means an architect should very clearly understand how the assembly language could be written what are different assembly language instructions so the first user of assembly language is a machine designer the second user of assembly language is a compiler writer the compiler writer would just convert write a program which would convert higher-level language into either directly machine language or first to assembly and then to machine language now if one is writing programs in assembly language then the conversion to machine language could be more efficient nevertheless if a compiler is used to Verte a higher-level language into a machine language then on one hand a unique code may not be available and secondly this court may not be extremely efficient so therefore the compiler writer needs to appreciate and understand the different features of an assembly language programming third user of assembly language could be a writer of the time and space critical code there are some real-time applications where the time is critical and the operations need to follow a particular sequence in that case also one needs to understand the assembly language very clearly finally the current applications are in embedded systems one example of the embedded system is a normal mobile phone a mobile phone has a CPU a processor and it has a complete code to do all the functions now an assembly language programmer can very efficiently program in an embedded environment now for appropriate programming an assembly language programmer would need certain tools and let us now have a look at these set of tools and how these would be utilized by the programmer the useful tools for assembly language programmers are the assembler the linker the debugger or monitor and other parts of a development system now in an earlier course you have learned in detail what an assembler is just to review assembler is a program written for converting the assembly language program into its equivalent machine language program using a computer program this computer program is called an assembler and the process of conversion is called the assembly process the assembly process can also be done by hand without using a computer however it is tedious and error-prone an assembler that runs on one processor and translates an assembly language program of another machine language is called a cross assembler it is an appropriate tool sometimes when we do not have approach to a specific processor we can just run a cross assembler using an other processor the linker just gives us a possibility of dividing a big program into sub modules when developing large programs separate modules can be developed and assembled by different persons working at the same time the linker links these different modules together to form a single module for loading and execution a linker resolves cross-references and determines the starting point for execution of the program the debugger is another program which is pretty handy working in assembly language is tedious and error-prone when we run a ns the syntax errors could be eliminated or we could point out to the syntax error while running an assembler however run time errors often crash the system instead of smoothly returning the user to the operating system a debugger which is sometimes also called a monitor is a computer program used to help in finding the errors logical errors in the program useful functions which are available with most of the debuggers include display and alter the contents of memory and CPU registers and flex that means if during runtime of a particular program written in assembly one finds that contents of a particular register needs to be changed these could be changed by using a debugger instead of writing and running the assembler again disassembly or reverse assembly of machine code is possible by using a debugger finally an extremely useful facility in the debugger is to have breakpoints by putting up breakpoints one could stop the program at any point of time and view the contents of memory and different registers this could also be implemented in the form of single stepping that means one instruction at a time is executed development tools is a set of hardware and software tools available to the programmer for appropriate development and debug debugging of the program for assembly language program the tools available are we could have efficient tools for development some of these tools constitute hardware and others constitute software the software tools available to the programmer are assemblers compiler debugger whereas the hardware tools are emulators and the logic analyzers now another tool which is a simulator is normally implemented in the form of software all these tools enable a programmer in assembly language to develop an assembly language program in an efficient manner and without bugs let us look at some of the differences between higher-level language programming and assembly language programming in general there is a many to many mapping between higher-level language and equivalent assembly language constructs this means that if a higher level language program is written that would not always necessarily correspond to a unique assembly language program in higher-level language the type checking is provided this means that proper verification of the type of variables is available at compile time the compiler also allows to determine the requirements for the memory a compiler also helps to detect bad programming practices most of the machines have no type checking this means that in assembly language the type checking would not be done the machine would look only data and instructions as a string of bits the instructions are interpreted as type usually limited to either a signed or an unsigned integer or it could be considered as a floating-point number a given 32-bit world for example in an instruction could be considered as an integer or a floating point number or it could be considered as four ASCII characters most of the earlier computers and the present-day computers operate on the concept of stored program the program along with the data is stored in the memory of the computer this could be fetched from a hard disk for example placed in the memory of the computer and then executed an instruction is fetched from the memory it is decoded and executed this process of fetching and execution go instruction by instruction one after the other unless a jump instruction is encountered an instruction registered in the CPU is the register which holds the current instruction being executed after this instruction is executed the program counter which is another register which holds the address of the next instruction to be fetched from the memory that is updated unless there is a jump the next instruction would be the instruction in the queue available in the memory this process of fetch and execute would continue unless we meet a jump in the program this is a very simple and a very important concept which is based which is implemented in the execution of a program in the computer this is an oversimplified picture the present-day computers they have the facility of executing multiple instructions simultaneously we will look into these features in a later lecture and then we will consider also the concept of pipelining at this moment to summarize we could say that the execution goes by fetching the instructions from the memory decoding the instructions executing the instructions updating the contents of the program counter which would contain or point the next instruction to be executed this idea of fetch and execute could be illustrated by a simple animation let us look at this animation looking at the animation on the left hand side the various registers of the CPU are indicated in this example all these registers are 16-bit registers PC is the program counter I R is the instruction register on the right side we have the addressable memory in this case from 0 0 0 0 to F F F F the control unit just coordinates different activities the contents of the program counter are to start with 3 F F F and this program counter contents are given on the address bus the data from the given address NPC is fetched in this case this data would be delivered and stored in the instruction register I our and CPU reading this data in IR with both would decode this instruction and execute after decoding the contents of the program counter would be upgraded to 4 0 0 2 that means the last instruction executed had a length of 3 words so therefore the PC is having the contents for 0 0 2 which is 3 added to the previous contents in case of a branch instruction the contents of the PC are replaced by the address of the next instruction contained in the present branch instruction and the current status of the processor stored in a register called the processor status register it is also called flag register to summarize the entire process of reading memory incrementing the PC and decoding the instruction is known as the fetch and execute principle of the stored program computer as I have already said that most of the present-day computers still utilize the fetch and execute principle however the advanced features have also been incorporated like pipelining which we will discuss in a later lecture after looking into the stored program concept let us look at the instruction set architecture this is an extremely important programming model for an assembly language programmer the set of instructions available for a given processor along with the memory space and the registers available to the programmer in CPU are the resources which are to be utilized by the assembly language programmer and this is defined as is a instruction set architecture let us look at ISA in a little bit more detail in the following slide is a stands for instruction set architecture and it could be viewed as a model for the computer it includes the instruction set memory space and all the programmer accessible registers one should note that the total memory space is not necessarily the physical memory of the processor usually this is the maximum addressable memory which is available to the programmer the physical memory is usually less than this particular maximum addressable space this is a model serves as an interface between the program and the functional units of the computer after looking at programmers view let us come to the view of a computer architect as we know computer architect has to design a an overall computer system he has to optimize different subsystems before he can optimize he has to determine which parameters are to be optimized the given circumstances could be just for least-cost that could be a parameter or maximum performance minimum execution time simple instruction set different features are to be looked into however one thing is quite clear that a computer architect has to provide an instruction set architecture which could be later used for a for programming purposes by a programmer let us look into some of the available tools and the features which are to be provided by a computer architect let us see that on the next slide a computer architect is concerned with design and performance of the entire system he will design an efficient high assay for programming and to provide optimum performance of implementation a computer architect would design the hardware for best implementation of the instructions the computer architect would use performance measurement tools some of the benchmark programs to see whether the goals are met or not an architect has to balance the performance of different building blocks such as CPU memory i/o devices and the interconnections he needs to meet the performance goals at the lowest cost some of the useful tools available to the computer architect are software models simulators and emulators performance benchmark programs specialized measurement programs data flow and bottleneck analysis subsystem balance analysis different parts manufacturing and testing cost analysis finally we look at the perception of a logic designer the logic designer works at gate level he has to work very closely with a computer architect let us look at the next slide in more detail what is the perception of a logic designer a logic designer designs the machine at the logic gate level he determines whether the architect meets the cost and performance goals a single person or a single team may be performing both the jobs of a computer architect and a logic designer the tools available to the logic designers are usually the CAD tools consisting of logic design and simulation packages printed circuit layout tools integrated circuit design and layout tools logic analyzers and oscilloscopes hardware development system the logic designer analyzer needs to know the implementation domain the implementation domain is the collection of hardware devices with which the logic designer works examples of the implementation domain could be TTL gallium arsenide chips PL ace and VLSI on silicon some examples are shown in the next slide in this slide you see the implementation of a 2 to 1 multiplexer in generic form with a select you could get either of the two inputs transmitted to the output we could also show this 2 to 1 multiplexer in a different implementation this is the TTL implementation where the inputs are given to different pin numbers of the IC shown with a 74 series in the next implementation the same 2 to 1 multiplexer is shown in fiber optics directional coupler switch the classical logic design usually deals with the state diagram it implements the sequential logic considering the machine as a finite state machine however the traditional techniques used in logic design could not be efficiently used for design of computers in today's lectures we have got an introduction to the outline of the course we discussed the distinction between computer organization and computer architecture we looked into the perception of different people about computers we looked into the stored program concept and fetch execute cycle finally we saw the instruction set architecture next time we are going to look into the classification of different architectures of computers I hope through today's lecture you have become familiar with the terminology and now you know who is a computer architect in the next lecture we are going to talk in more detail about the architectural features of the computer for today for the office 
qYkVqLtqLE0,27,This video contains the description about SISD computer in Flynn's Classification in Computer Organization,2020-01-08T11:02:40Z,PART-1: FLYNN'S CLASSIFICATION OF COMPUTERS | SISD | FLYNN'S  CLASSIFICATION | COMPUTER ORGANIZATION,https://i.ytimg.com/vi/qYkVqLtqLE0/hqdefault.jpg,DIVVELA SRINIVASA RAO,PT11M58S,false,1360,26,2,0,4,hi friends today I am giving a lecture on slings classification of computers in computer organization I advanced computer architecture okay so this is one of the most important concept in computer organization and architecture okay so for the organization of the computer system based on the number of instructions and data that are manipulated simultaneously for that purpose prints proposal a classification okay the flings classifies the computers into four categories based on the number of instructions under data items that are manipulated simultaneously first one is si is the single instruction stream and the single data stream second one is single instruction spring and multiple data streams SIMD third one is multiple instruction stream and multiple data stream a miami-fort one multiple instruction stream single data stream mi SP okay so before going to the FASB SIMD of my SP of my MD okay what is instruction stream and bodies data stream so generally the programs are are stored in the main memory a program is a collection of instructions that are stored in the memory to process that instructions okay to process that instructions okay that instructions can be read from the memory okay so the sequence of instructions read from the memory is called as instruction stream so instruction straight is nothing but the sequence of instructions read from the main memory is called as instruction stream once the instructions are read from the memory by me CPU then what are the operations that can be performed by the CPU that is called as data stream so data stream is nothing but the operations performed on the data in the processor is called as data stream so based on the instruction stream under data streams slings classifies the computer into four categories si s ba si MB mi s be a my MV o so this letter shows the four major categories of Flynn's classification he has singled here multiple here single here multiple so single single instruction stream stinking data stream that is si ESP single instruction stream multiple data stream that is SIMD multiple instruction stream single data stream that is my SB multiple instruction multiple data stream that is called as my mV okay speaking Gil instruction suite single data speed that is si SB single instruction stream multiple data stream that is si and be multiple instruction stream still a data stream that is my sb so multiple instruction swing multiple data stream that is called as a my SP okay so once again I am Telling instruction spring is but the sequence of instructions read from the main memory confuse an instruction stream the operation can be performed on the data in the processor and data stream okay now we go for individual what is s is B parties SMB parties and my speak parties and my empty now we go for si s B computers that means single instruction stream and the single data stream okay so this diagram shows the SI st computer it represents the organization of a single computer the consists of a single control unit single processing unit and main memory here in this si SP computer instructions are executed sequentially one after the other okay so the SAS P computer may not have parallel processing capability in case parallel processing capability can be achieved by means of multiple functional units that means multiple air use and also mud pipeline processing okay here sa SP computer may not have parallel processing capability suppose we require parallel processing capability to FASD computer we may require multiple functional units that means multiple air use water pipeline so by using these two concepts we have to each other parallel processing capability in FASD computer so generally in Astraeus the computer instructions are executed sequentially one after the other okay now here in the main memory the instructions are stored in the main memory these instructions are executed sequentially one after the other okay for that purpose the control unit reads the instructions from the main memory for decoding purpose okay so at that time the sequence of instructions send it to the control unit as the instruction stream now the control unit sends that instructions to the processing unit what process the instructions now the processing unit not is nothing but here you the processing unit execute the instructions by taking the data from the main memory for that purpose we have to use data screen okay so now the processing unit executes the instructions by taking the data from the main memory and execute that instructions whatever the output that can be generated from the processor unit so that can be send it to the main memory okay here the data can be slowed between the processor in a processing unit and main memory and the main memory and processing unit so the data can be communication between bi-directionally between these two units processing unit and main memory okay so in this way we have to process the instructions in FASB computer okay once again I am Telling here the instructions are stored in the memory okay so that instructions are executed sequentially by the processing unit so first the control unit reads the instructions from the main memory and the instruction screen for decoding purpose once that decodes Depot once the decor phase is completed by the control unit then sends the instructions to the processing unit okay now all the instructions are stored in the processing unit that is ALU now the processing unit can execute the instructions by taking the data from the main memory as they the screen now here the instructions and the data is available in the processing unit now the processing unit executes the instructions so then it produces the output data then output data again send it to the main memory as they take a screen now the output is stored in the main memory okay this is the how to execute the instructions that are stored in the memory by the processing unit in si este computers okay so si is the computer the examples per sasb computers are older generation computers mini computers and workstations so these are the examples para si s be computers okay in saas-fee computer so some computers have one functional unit and some computers how multiple okay here in this in these computers in structures are executed sequentially one after the other so in these computers instructions are executed parently by using multiple functional units okay so as far as the computers have one functional unit so far that one the examples for that one our IBM 7:01 IBM 1 6 2 0 s piously computers have multiple functionality so that computers are IBM 360 91 CDC 6600 so these are SISD computers have multiple functional units so further computers the examples i evm 316 and he mercy logistics X 0 0 ok since the ESP computers to have single functional unit for that one the examples are IBM 7:01 idea 1 6 2 0 ok now we go for SIMD computers 
WAO_W6Hpzyk,27,"#InstructionFormat#COA

â–ºFull Course of Computer Architecture:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHMonh3G6QNKq53C6oNXGrX

Other subject playlist Link:
--------------------------------------------------------------------------------------------------------------------------------------
â–ºDesign and Analysis of algorithms (DAA):
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHcmS4i14bI0VrMbZTUvlTa
â–ºDatabase Management System:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiFAN6I8CuViBuCdJgiOkT2Y
â–º Theory of Computation
https://www.youtube.com/playlist?list=PLxCzCOWd7aiFM9Lj5G9G_76adtyb4ef7i
â–ºArtificial Intelligence:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHGhOHV-nwb0HR5US5GFKFI
â–ºOperating System: 
https://www.youtube.com/playlist?list=PLxCzCOWd7aiGz9donHRrE9I3Mwn6XdP8p
â–ºComputer Networks:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiGFBD2-2joCpWOLUrDLvVV_
â–ºStructured Query Language (SQL):
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHqU4HKL7-SITyuSIcD93id 
â–ºDiscrete Mathematics:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiH2wwES9vPWsEL6ipTaUSl3
â–ºCompiler Design:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiEKtKSIHYusizkESC42diyc
â–ºNumber System:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiFOet6KEEqDff1aXEGLdUzn
â–ºCloud Computing & BIG Data:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiHRHVUtR-O52MsrdUSrzuy4
â–ºSoftware Engineering:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiEed7SKZBnC6ypFDWYLRvB2
â–ºData Structure:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiEwaANNt3OqJPVIxwp2ebiT
â–ºGraph Theory:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiG0M5FqjyoqB20Edk0tyzVt
â–ºProgramming in C:
https://www.youtube.com/playlist?list=PLxCzCOWd7aiGmiGl_DOuRMJYG8tOVuapB
---------------------------------------------------------------------------------------------------------------------------------------

Our Social Media:
â–º Subscribe us on YouTube-https://www.youtube.com/gatesmashers
â–ºTelegram Channel Link: https://telegram.me/gatesmashersofficial
â–º Like Our page on Facebook -  https://www.facebook.com/gatesmashers
â–º Follow us on Instagram-https://www.instagram.com/gate.smashers

--------------------------------------------------------------------------------------------------------------------------------------
â–ºA small donation would help us continue making GREAT Lectures for you.
â–ºBe a Member & Give your Support on bellow link : https://www.youtube.com/channel/UCJihyK0A38SZ6SdJirEdIOw/join
â–ºUPI: gatesmashers@apl
â–ºFor any other Contribution like notes pdfs, feedback ,suggestion etc
gatesmashersconribution@gmail.com
â–ºFor Business Query
gatesmashers2018@gmail.com",2019-08-23T17:39:26Z,L-1.13: What is Instruction Format | Understand Computer Organisation with Simple Story,https://i.ytimg.com/vi/WAO_W6Hpzyk/hqdefault.jpg,Gate Smashers,PT10M40S,false,164892,3831,70,0,183,hello those so gates measures make us forget their ask you mm computer organization and architecture car important concept discuss carnage array just say up Co computer organization keep porridge oka honey a bow so much Miyagi kooky students karthika a journey cry and the topics or cram carrying a graph concepts co2 up Jojo scenario Havana pick lenita no actual scenario computer organization and architecture Kasey Kahne kurta whoa miss video made hisses carnage Aria this ma'am instruction instruction K format Co or K Jota construction actual me in sorry but Pocoyo PT Siskin Desiree yay video opko health care Aggie computer organization and architecture car near about 70 to 80 percent concept Kissimmee Navy whose give a joke on scepter Bosnia Baker key video service active Lake in actual McDonough pure aÃ§aÃ­ story que vous homeopathy Cisco Nigeria though actual Makani start career let's say if I write a program in C language talk about si Lang will be simple program with no hash include stdio.h a midtown let's say into a is equal to 10 into B is equal to 20 and let's say into C C is equal to a plus B simple many addition of two numbers caprara Mallika John Pitman is no variables Li a or b 1021 my values the X Celia seem a man a a plus B cos Amidala and let's say I want to print the C value and that's it my sympathetic program you have a leak at the main front a key K Java miss program to save Garcia schema this cookie Karthik compiled care they interpret okay finally a convert of the machine language may which is what micro instructions or whoa machine language concave all their own Goomba this set of instructions a convertible jaga kiss may set of instructions made MATLAB yay Joe had 10 or 20 a head data on his own car a happy a plus B yet Rasika have instructional QT instruction am Kirk area key up or be Colo on my a values a sign Kuro memory may value the signage ng or be Co and then seeking aluminum taqueria a plus B Co add Kirk a store crayons kev adams eco caca reprint Karev - here sorry Jew instructions or data in those geocode data or instruction cone card out there memory came there yeah the Von Neumanns Oh architectural key have data or memory data or instruction Co memory can the rock there - Connie hockey assist our DoDEA programs um data or instruction copy RAF key memory and meth lab rampion thorough omnibus d'italia s of your sick technique alpha data you have o register came there be a sec the dojo gay yeah to register maya jaga yeah Phil go copy a jaga memory can the Licking journal Evo hump alley memory came the leak at the Baja CA Monaco call carrying a copy register came the kookie ALU Joey a and you perform kreega al-youm o'clock Joe hamara CPU her CPU and ricotta ALU and compute see you that is what control unit so a a new key responsibility ia addition Co perform corner Yuuki automatic logical unit or just CPU I would directly kissed a connected au pair both the set of registers in America's registers doing different types of registers OTA accumulator data register input register output register instruction register different types of registers they took CPU Joe have instructed registers ke saath kaam kata or data or instruction copy Padilla memory can they're both generally like contiguity location because if they're non contiguous allocation because it then you can you blend that you mixture a knuckler concepts car you see was just the students go so many attack here computer organization comes under care basic funding schedule basic summary of okay so it's medium you up could be cool basic summary discuss oj gyaku pathologies jaga cooking up a operating system concept we use got a yeah p.m. database kabhi use career yeah p.m. network abhi two different types of technologies a me up a who's got a though yup Amana palak kadhi sukiyaki programmer compiled kia-ora many data or instruction core memory can the rakia this is according to the Von Neumanns architecture disk you were already many video you Yahoo's could check collective the data or instruction memory key and Ravi Abu baraa next Comcare hemara next comma whose instruction core fetch karma instruction capacity a instruction parvati key calm so al you look Oh Papa Gazzaniga a new doe become Carlotta Das Kapital szeliga escape ass job instruction I just Maura brainer aluminium camera brain leaking brain kapa haka szeliga homologa sensors bouquet occurring it would break ether of quick message you have a pastoring a brain osco process Kirk a who's to convert Kerry Kikuchi actions me vahik Amamiya P career ha marcus data instruction oggi subsequently instruction job car naka us go home Nicole Garner vocal car camera ALU kodi neeya to his cobalt M instruction fetch Kearney to instruction ERP concept startup an instruction Kendra hamari pasaba Takeo who taka ki home computer organization may 7080 percent I'm amused at the instruction instruction cycle pipelining may be instruction tree actually instruction the format care to the journal format instruction car whose make out that subsequently Hotel Mary pass addressing mode eco Tamara pisaq code or a Gautam oppas operand age of teen GD a mandatory that is what mode which is what addressing mode addressing mode Kevin Takata kiss operand co-op doomed REO operon math lab data key apne 1020 kiss data Cooper counter now this data coop doomed REO boom memory key under time both memory candor kiss address me yeah Burgess Turkey and the register may kiss address Mia the hoppy papa rasta address some Coverity yeah Harper at em awkward yatta awkward Matata kick perform kegger man Jesse's coaching according I'm a plus karna addition subtraction multiplication division shift Colonel Jovi Omnicom carnea whoa have occurred to you happy addressing mode semi path alakazoom detail making his video Kabaddi k addressing modes con con su Thea buttock journals are concept madero Europe a key addressing mode basically patata address kiss even decode in Rio to a postal address vertical air to Khyber cow sector key after data do you have a directly Perot but la Papa Judy Tom of constant have or would direct again him paper - ho September day turning up a para yup America address for a Kia Sentra's pooja or data hallo asabi attack erp register kalispera our register page aho also register city council s abuse at a chaotic memory location by Java or vote memory location octo a gimmick address their fill of quit inducer address vigil was a beaut huh space coveted indirect kind of addressing to hit different types of addressing mode without their this operand Co this data co-op memory can the dual reow QK a or b Jessie openly declare Keanu a or b co-owned a memory may coach a location cardi whoa allocation car Kia Houska at the co-op yoga ye actual make honey or opcode cache Keokuk arnica addition Carla multiplication karma subtraction Kia perform karna operation whoa operation European do you think gee Joker mixtures you okay I want a instruction Joe a volunteer oh-ho taka is instruction core memory with job everybody a house go fetch Kirkuk a girthier register came the lake at the origin so memory pasok specialized this rota that is instruction register escape array may be many already discuss key aware instruction register can the hamari pass Joha instruction of the air instruction Cajon calcareous calm Cole Pima clubs could decode carmakers get their freedom find out Kurt aq operand come up here ja operand Damacio print : lying gay also operand go home accumulator Mele carrying a data register Malika engage waha pay a lu k Auriga who's came the addition perform kirk a or finally output register Kobo addition there they are output register kakaka print rocco llamo neat recover data they they or finally after results you have o 30 10 plus 20 30 to have a paper in Torian to a purity pure acharya opieop a focus Arco Carnegie instruction caper of caution Apogee instruction kiddos sighs Joel enter Voki scope a dependent care thea believeth would depend cat a type of computer organization must love agar aapke single accumulator system would only use got em single address who speaking over a copper and kahi address auga general register organization may host sector archaea happy a doe operand Oh Tino friend Bo Suk they are three address instruction Bo Sathya to jitney address Jahangir O'Neill and he burped eiv Kiske instruction T or other instruction key length but a key talk basically instruction register copy sighs but I got or I got instruction register look uh sighs but I got the bus Kavya Heidegger homnibus Kamara maybe discuss key aware Q key sorry videos up was me related a common bus yukia's opium karaoke memory same register me lying a register SIA me and Luca dengue ALU cm register me dengue he just sorry Charlie Jana yay Cahaba travel or a data bus Cooper the Coleman was do you have to use career Pokemon bus : Ogawa beseech in eg Joe Cooper depend Korea Doha miss video Chabad discus carrying a up a different types of computer organization like in yahoo Piarco pathology aki opcode muslim car laka operand is data to proc on karnovski address copy or mode bitanga Kibo actual my hair camp Evo address actual may K avatar a KO memory address a kiev or district address i qiyamah direct aprender yep kupatana mood to eek basics a basic story of classic to vector assay is story Kogan of mind american gonna to a co computer organization you have a bill cool is ela Giga q ki aapke do 60 to 70% slavers Anna what EC Cooper eBay's day or students kakete in in topics coaches a party Nia key instruction chaotic carpe who's gotta cast a fetch curtain top I use girl a or finally Cassie output 1 Thea waha page a page of confused Abhishek a one Co a concept so much mini oughtta the next we do ma'am this is cunning Aikido or you happy instruction me to opera and a yuppie a copper and address ooga though operand contra saga t no prank address Suganya 0 operand muslims kabocha 0 address instruction 1 address to address three address a key scope or depend Cathy a a computer organization key to Taipei who's Cooper depend Katya so thank you 
9mLDO4eA5g0,22,"Ù…Ù† Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø³Ø§Ù‚ ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" Ù„Ù„Ø¯ÙƒØªÙˆØ± ÙˆØ³Ø§Ù… Ø¹Ø§Ø´ÙˆØ± Ø§Ù„Ù…Ø­Ø§Ø¶Ø± ÙÙŠ Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¨ØºØ²Ø© .

Computer Architecture and Organization course by Dr. Wesam Ashour, lecturer at the Islamic university of Gaza - Palestine.

Course language: Arabic",2016-12-01T13:56:11Z,"19- Computer Architecture ""Ø¹Ù…Ø§Ø±Ø© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨"" - Chapter 5",https://i.ytimg.com/vi/9mLDO4eA5g0/hqdefault.jpg,Abdallah Safi,PT48M28S,false,1488,4,0,0,0,right I would even say a terrific in design excellence I can multiply could be signals in the interpolated come on my signals Ramana what it instructions the math mom can you at the energy a death we have these signals how can he rather have these signals we wish to kill Ali Mohammed Mohammed instruction patella confined in via the pub atmosphere satisfaction with Jerry say orcas have it is the other way I will be sure you give motherly weather phenomena signal water within which is destination religious right sucker I mean our monitoring zero young minion there is a mesh that is and you saw has at the end of it family died in a giant mirror which is sulphides notion that algebra to sign extension has a restriction I could if we go over to restore with the hell Nene was on exhibition I wish for work now very shocked attorney in my opinion the doctor with telekinetic haha the cooperhog very in just right and then Packham meta neutrality surprise hi until that education about the Mensch way yeah and you can input the data came out of each other Oh bancini it's by John see you the original iteration chopper that signal jump mahat whoever grabbed a very systematic saw accept the exam will never lead balloon I run some Oh Oh is not I see whoo-hee mr. Barba CEATEC madaba hi in hardware with the Hartman yeah signal signal signal signal people deserve quality today and the red is watch hello gentlemen easy Sehun Harlan to the right to choose the national popularity and officials met with our game definition of the Amish man - yummy banana they are modest and Raburn and mechanical have taken the house and we will acquire the real rock where you have the the real design that the control in the end but in general that has a condition in Kupwara it does anything Oh Oh Oh issue the general Jenny but at the end we shall they give you salvation and out cities there I would would read by inner inner you sooner what and then she must toss the stack when she you know any some mama there's someone he was my head the Tabaka a slack the right there a post-hoc the same hope a leisure with the serum leave as defective instruction together we can give the people with internally with the iMac 20 opera it was funny when we start with - le gusta veÃ±a veÃ±a James - booster in semiconductor astonish our panic an acapella the Seville have cost in this philosophy Salima Diane charisma that care about the competition between CDC Ramadan Mubarak Hamas issue barrier illuminate the receptor Sharma family chemicals the man managing a nearly headless to add that cept animal born ahevesi the media and a good sugar except star and then I stopped for a minute fashion accessory 
pU83MBPZIBM,22,"Subject: Computer Science
Courses: Computer Architecture and Organisation",2019-02-05T05:40:20Z,Measuring CPU Performance,https://i.ytimg.com/vi/pU83MBPZIBM/hqdefault.jpg,Ch-13 Computer Science and Engineering,PT41M58S,false,740,12,1,0,4,[Music] welcome to week 3 lecture so in the last couple of weeks what we have studied is how we can execute an instruction what are the various architectures that are existing what are addressing modes instruction format and various other things that are necessary for this particular course apart from that we have used or we have said about a simulator that is spin and we have seen that how we can write programs using low-level assembly language in this particular week we will be looking into how we can measure the performance of a CPU we know that for any program you need some instructions to execute that particular program now how you can actually measure the CPU performance by that I mean that you can run the same program in two different CPU and then how you can tell that which one is better so in this particular week we will be looking into the various aspects of CPU performance and then we can say at the end of this week lecture that how can you say that my CPU is better than the or the CPU so coming to the introduction we know that most processors execute instructions in a synchronous manner using a clock that runs at a constant clock rate or frequency so what do we mean by that by this what we mean is processors execute some instruction that we have seen now how that instruction get executed is through a clock that means a clock is coming and at the positive edge of the clock or within that clock period we can say certain task is performed and within a time period the according depending on the frequency the number of clocks comes in that is clock is coming one by one by one now what is clock cycle time clock cycle time is the reciprocal of clock rate that is C is 1 by F clock cycle time is often termed as period that is clock period which is the reciprocal of your frequency that is 1 by F first let us see these two factors that is F and C in some detail look at this so this is also a clock and this also a clock let's set this clock this is the off period this is the on bd8 and this whole is the one period this is the off period this is the on period so this is another period so this is one period this is another period this is another period and so on and let us say we have another clock whose period is little more so what we are doing here this is the off period and this is the font detail so this is the total clock clapping similarly for this this is the off period this is the on period so this is your total clock and so now you see that this particular clock the time period is period is small that is your clock cycle time is small and in this clock the clock cycle time is more and what we know that in some processor is performing a task with respect to these clocks so whenever this clock is coming a particular task is getting performed and so what we can analyze from these two clocks let us see that for the first clock let's say the frequency is 1 gigahertz ok that is 10 to the power 9 Hertz so what will be the time period as I said time period will be 1 by F so time period will be 1 by 10 to the power 9 second that is 1 nanosecond so the period this period is 1 nanosecond and in 1 second the clock is going how many times these mini 1 by 10 to the power 9 times sorry total is 10 to the power 9 Hertz so the clock is on and off on and off those many time period now let us see about this particular clock in this particular clock the frequency is 500 megahertz so as it is 500 megahertz then F is 500 into 10 to the power 6 Hertz so what will be the time period that is T equals to 1 by F this will be 1 by 500 into 10 to the power 6 seconds that is 2 nanoseconds that means for the previous clock the time period is 1 nanosecond for the next clock for this one the time period is 2 nanoseconds so which which will be faster obviously the first one will be faster than the next one so now that we know that how we can say how we can relate clock frequency with clock period so both are related next we see that on what factor does this clock rate depends on so there are two important factors on which this clock rate depends the first one is the implementation technology so by implementation technology what we mean is that with the advancement of technology the size of this transistors are becoming smaller and smaller and with that the clock speed is becoming faster the clock is becoming faster basically so this is a factor where the clock frequency dependence another is the CPU organization what do you mean by CPU organization that we use by CPU organization what we mean is that suppose in a clock period we see that some part of the instruction is executed so basically an instruction is divided into some cycles I mean each of the work of that particular instruction is performed in those cycles and by CPU organization we mean how we can organize the CPU such that the number of tasks that can be performed within that clock period is maximized so in turn we can say that for executing a particular program we require some set of instructions now to execute those instructions we require some clock cycles and ultimately if if if the clock period is less then the total time required to execute that instruction will be even less so these are the two factors on which the clock rate actually depends so they said just now that the machine instruction typically consists of number of elementary micro operation that very number and complexity depending on the instruction and the CPU organization used so by this what we mean that let's say we will take an example here add r1 comma r2 to execute this instruction what we say that a machine instruction typically consists of number of elementary micro operations so to execute this particular instruction we require certain steps 1 2 3 and so on what are the steps so what we need to do first this particular instruction is stored in memory so you have to bring that from the memory so first is the fetch phase once you fetch how will you fetch it the content of PC should be made available so using some operation some control signals we will make the content of PC available that is PC out we will see in details next but in a simple word I will say that once we do this PC out the content of PC is irrelevant to some bus and then we have to put this value in ma our memory address register we know we have discussed about all these register memory address register contains the address of an instruction or the data that is to be read from the memory so this we do PC out ma are in then we read and then we do some other things so first we fetch the instruction so with just only these steps we cannot fetch we may require some more steps to fetch now what the point is one these particular steps can be performed let's say in one clock cycle and there can be many more steps let's say to execute this machine instruction what we require we require some basic steps let's say we require six steps to execute this instruction and we see that each of these steps require one one clock cycle so this is what we meant by a machine instruction typically consists of number of elementary micro operations that very number and complexity depending on the instruction and the CPU organization used now if you use a different CPU organization these steps that I'm saying might be different like say for one type of organization one type of organization it requires six steps for another time type two organization it may take four steps so we really cannot say that how you can differentiate so it depends on the organization used and it depends on the complexity of the instruction as well now moving on as I said a micro operation is an elementary Hardware operation that can be carried out in one clock cycle so all the set of instruction as I said PC out ma are in read and so on can be executed in one clock cycle and in one clock cycle what we can do basically is some register transfer instructions some ALU operation instruction because all those are within the CPU and for that we don't have to bring it from the memory so whenever you have to bring it from the memory that we have to see that how much time will be required for that inside the operation transferring between CPU to CPU is much more faster compared to transferring or getting a data from memory to CP so there can be register transfer operation arithmetic logic operation thus a single machine instruction may take one or more CPU cycles to complete we can characterize an instruction by cycles for instructions what do you mean by cycles per instructions as I said an instruction is divided into some basic operation some micro instructions are executed to execute that machine instruction and all those message machine instructions that all those micro instructions that are getting executed requires some cycles so ultimately an instruction takes certain amount of cycles to execute it so that is called cycles per instructions so every instruction is taking some cycles to execute and that is termed as cycles per instruction and what is this average CPI of a program average CPI of a program as we can say that see there can be many instructions and many instructions can have different CPI's so average CPI of all instruction executed in a program on a given processor so we average it that means some instruction let's say takes five cycle some instruction takes seven cycles some instruction takes four cycle we average it out and then we say this is the average CPI as different instructions can have different CPAs we will see in detail so for a given program compiled to run on a specific machine we define the following parameters okay for a given program now we are talking about a program that is compiled on a machine these are the parameters that are important what are the parameters the total number of instructions executed we call it instruction count so for a program what is the total number of instruction that is executed that is instruction count the average number of cycles per instruction as I have already discussed that is the CPI so if there are 20 instruction and each requires some some some cycles so cycles per instruction is the total average number of cycles per instruction and finally the clock cycles time or the period of the machine so now what will be the total execution time so the total execution time can be computed as we call it XT instruction count multiplied by number of cycles required per instruction and multiplied by the clock cycles time or we can see the period so how do we evaluate and compare the performances of several machines next we will see that so what is the execution time so now the execution time is the number of instructions multiplied by the CPI cycles per instruction multiplied by the instruction multiplied by the dock period or you can say the clock cycle time so you multiply all these things you will get the execution time we call it X T now we will see how we evaluate and come compare between the performance of several machines so one of the easiest method that can be used to compare is like we measure the execution time of a program of two machines that is a and B and we see that execution time of a is XT a and execution time of B is X DB so what is the performance performance is 1 by X of X T of a and performance of B is 1 by that means this is the computation time so let's say the one processor performs a task in 10 seconds and another processor performs the task in 2 seconds which one is better the processor which performs a task faster that means in less time is better so the processor which performs with in 2 seconds will be much much better so that is why performance is 1 by X of T that is the reciprocal of this or performance of B is 1 by X T B now we can estimate the speed-up of machine a over machine B as speed up is performance of a divided by performance of B and performance of a is 1 by XT and performance of B is 1 by X DB so if you just put on these two values in this place you will get the speed-up of x TB divided by X T a that is the speed-up of machine a over machine B now let us take some examples let's say a program is run on three different machines so you have a b and c and execution time our execution time sir 10 25 and 75 so what you can see from this what we can say that a that takes 10 is 2.5 times faster than b so b is taking 25 so we divide 25 by 10 and we get 2.5 so we can say that a is 2.5 times faster than B similarly let us compare a and C where we can say that a is 7.5 so 75 divided by 10 we get 7.5 times so a is 7.5 times faster than C and similarly B is 3 times faster than C so B is 25 so 75 divided by 25 we get 3 so B is 3 times faster than C simple for one program but what if we will have different set of programs how do we compare this shall be discussed in course of time now let us take an example so I say a program is running on a machine with the following parameters so what are the parameters I give you the total number of instructions so what is your total number of instruction this is your I see average CPI for the program so what is the average CPI taking into account of different CPI is for these 50 million instructions we get a CPI of 2.7 and what is the CPU clock rate so we are giving you clock rate that is frequency so from frequency you can find out the time period by C will be your 1 divided by 2 into 10 to the power 9 so that comes down to 0.5 into 10 to the power minus 9 seconds and as we know that the execution time is IC into CPI into C and all those values are provided here so we get XT as this multiplied by this multiplied by clock period or clock cycle time which is coming to 0.06 75-second now see what are the factors that affect performance first one is the hardware technology definitely if you make your clock cycle time smaller then it is pretty obvious that you will be getting better result but at the same time you have to cope up with the operations that can be performed in one clock period or one clock cycle next is the hardware technology that is the organization so what all factors that depends on this organization first is the clock cycle time another is the cycles per instruction so let's say when we talk about organization it depends on like how your various hardware's are actually mapped so depending on that definitely your CPI that is cycles per instruction will vary you have an efficient organization where your cycles per instruction can be reduced so this particular factor that is hardware technology that is organization depends these are the factors that will govern that that is your CPI as well as your clock cycle time now instruction set architecture what all factors will dependent what is the instruction set architecture we have seen one of the is a so instruction set architecture by instruction set architecture we mean that how various kinds of instruction you are putting it in your architecture how many types how many varieties you are putting it there so definitely if you restrict the number of instruction to a minimum level for a program you might require more number of instruction counts but at the same time if you say that you require you have an instruction you have more number of instruction but those more number of instruction and to execute a program that those instruction can be used so there are two things that can happen basically what I'm trying to mean is that let's say you have add and multiply instruction and in one case you only have add instruction so in this particular architecture if you want to multiply two numbers you can simply used mul r1 r2 but in the same case if you want to use repeated addition then you have to use a loop and that loop will perform that addition number of times so this particular add instruction may be used in that loop repeatedly so the idea is if you have more number of instruction you might require less number of instruction in the end to execute a program in a similar case in this case you have less number of total instruction but in that case you require more number of instruction to execute the same program so this is the difference by this so this instruction set architecture will these two factors will depend one is your CPI cycles per instruction another is instruction count both will depend now the compiler technology now the different compilers can generate different codes so one compiler that generates some let's say for the same program one compiler is taking ten instructions another complete compiler is taking twenty instruction so so this I see very greatly on the compiler technology that is used so nowadays the compiler are becoming more and more intelligent and they are going in hand with the hardware so the compiler must know what kind of our hardware architecture you are using such that it will generate the code in a fashion that can be easy for execution and so such that it will also require less number of instructions and of course the program that a program what other factors will depend both the CPI and the instruction count so as I discussed in the previous slide I see depends on program used the compiler and the instruction set architecture CPI also depends on program used compiler the instruction set architecture as well as the CPU organization how you organize your CPU how you organize your various hardware is will also take this into account the final CPI so final CPI will definitely depend on the CPU organization that you are using and finally the C depends entirely on the technology used to implement the CPU and this is very unfortunate that it is very difficult to change one parameter in complete isolation from the others so the basic technologies are very much interrelated to each other so we really cannot vary one parameter completely compared to the other so what is the trade-off here if you see a RISC machine the number of instructions per program is more so increases the number of instruction per program but at the same time it decreases the CPI and the clock cycle time because the instructions and hence the implementations are simple but in sisk decreases the number of instructions program but increases the CPI and a clock cycle time because many instructions are more complex so overall what has been found it has been found that the RISC architecture gives better performance so let me tell you with the same example that we have taken earlier so we have multiplied instruction and this is the instruction count that is IC is 1 here and for this let's say the instruction count the loop executes four times okay so in such cases depending on how many number four multiplied five times with five or four more multipliers ten times so it depends on that now IC is one but the number of cycles required to execute that means the micro operations that you are using steps in each steps we are performing something that might be more so CPI cycles per instruction will be more in this case may be I see is more but the overall let's say it will take less number of cycles so CPI will be less here but I see they'll be more so this is a trade-off we can say so for in one case we can have more I see where the CPI will be less in some case we have less I see that is instruction count but the CPI in turn will be more let us take an example suppose that a machine a executes sub program with an average CPI of 2.3 consider another machine B with the same instruction set and a better compiler that execute the same program with 20% less instructions and with a CPI of 1.7 at one point to be guards what should be the clock rate of a so that the two machines have the same performance so for both the machines CPI is given and one machine execute the same program with 10% less instruction so one takes hundred percent and another takes twenty percent less that means 80 percent so for a I see a will remain same this is 2.3 and this is C that is what we have to find it out clock of e and for this IC is 20 percent less so it will become 80 percent so point eight zero into IC into 1.7 into this is the clock rate this is the period so the clock rate is 1.2 gigahertz that is one point 2 into 10 to the power 9 1 divided by that will give you the clock period of this you just solve this you will get a clock period of 0.49 into 10 to the power minus 9 second which is coming to 2.0 for Kika Hertz so we need 2 point 0 4 gigahertz of clock for a machine a such that both the result should be same let us take another example where consider the earlier example with the instruction count of 50 million average CPI of 2.7 and clock rate of two gigahertz suppose we use a new compiler on the same program for which new I see is 40 million and the new CPI has also increased which is three also we have a faster CPU implementation with clock of clock rate of 2.4 gigahertz so these are the different so one is having to Gig ours this is having 2.4 but the CPI of this is less CPI of this is more but the instruction count of this is even more and this is less so if you compare this what will be the speed up you have to find the execution time of old one compared to execution time of new so you just put those values you get execution time old which is of this one and you put all these values you get the execution time of this one so which is coming down to 1.3 5 so we can say that it is 35% faster let us take another thing which is instruction types and CPI consider a program executing on a processor with n types of classes of instruction so generally we do not have one kind of instruction right we have load store instruction we have data transfer instruction within the CPU we have variety of instructions basically so these classes are divided into let's a load store ALU branch etc so I see of I that is number of instruction of type I executed CPI is cycles per instruction of type by the following expression follows from this so till now we were saying that this is the total I see this is the total CPI now we divide it we say that there are various kinds of instructions so we can have various kinds of instruction and each of these instruction can have different CPI's so each instruction will have different CPI so CPU cycles clock cycles will be instruction count of I into CPI fi summation of that similarly instruction count will be considering all the instruction of all the types I see of type I where I goes from 1 to n so there are n type of instructions and what will be CPI now CPI will be summation of instruction count and cycles per instruction divided by instruction count total instruction count so summation of IC of I divided by IC into CP i-- so fraction this is what fraction of instruction I type I instruction execute it let us take one more example of where we consider the implementation of an instruction set architecture where the instructions can be classified into four types basically so the CPI values of these four types instructions are 1 2 3 & 4 respectively to code sequence have the following instruction count that means there are two code sequence and these are the various instruction count of type 1 instruction current of type 2 and so on so now you see the CPU cycles for CS 120 multiplied by 1 because type 1 the CPI is 1 type 2 multiplied by 2 so which is type 2 x - 15 x - 5 x 3 and 2 x 4 so this will give you the total CPU cycles that is 73 so what will be the cycles per instruction so total instruction 20 plus 15 plus 5 plus 2 which is coming down to 42 total instructions so CPI will be CPU cycle divided by 42 which is coming down to one point seven four similarly for the next one total CPU cycles is eighty and CPI is two point two so we can see that it greatly depends on both the type of instruction and what is the CPI of that type of instruction so both varies so this is instruction frequency and CPI where CPI can be expressed in terms of frequencies of various instruction types that are executed in a program so fi denotes the frequency of execution of type AI so fi is IC or Phi divided by total instruction count and CPI we have already shown in the previous slide can be expressed in terms of frequency so we substitute this here so we get frequency multiplied by CPI or I now let us take another example where suppose for an implementation of a risk RSA there are four instruction types with their frequency of occurrence for a typical mix of programs let us say and CPI as shown in table below so this is the frequency at which load instruction is executed this is the frequency at which store instruction is executed and this is the frequency for a he'll you and branch so and the CPI is given cycles per instruction it is the frequency at which it is happening so what is the frequency that is zero point two and the CPI is four so if you want to find CPI you can find five multiplied by CPI fi so 20% point to 0 x 4 8% point 0 8 x 3 and so on and we get 1 point 8 8 let us take another example suppose that the program is running on a machine with the following instruction types CPI values and frequency of occurrence the CPU designer gives two options the first option is to reduce the CPI instruction of type e 21.1% so type a instruction we are reducing 21.1% and reduce CPI instruction of type b to 1.5 so CPI of type b is reduced to 1.6 so this is type is reduced from 1 point 3 to 1 point 1 and type b is reduced from 2 point 2 to 1 point 6 now let us see so average CPI for a will be 6000 point 60 percent so point 6 0 multiplied with 1 point 1 this is the new CPI and all the rest CPI remains the same so we get 1 point 4 4 for each similarly the CPI for B is 0.6 0 into no change here but here we change to 1 point 6 this 2 point 2 becomes 1 point 6 and this remains same so we get this so from this what we clearly can say is that option is better but you see what we have done option is we have reduced from 1 point 3 to 1 point 1 but you see the frequency of this instruction that is getting executed that is 60% so it is much more so you must take into account the frequency so some instruction which are frequently getting executed and you reduce the CPU even less amount but you are using that particular instruction much more so in that case you will get a better result even if you are reducing certain CPI to a great extent but that is not executed more so you see that type B is executing only the frequency of education is 10% so in that case if you reduce it to 1.6 also you are not getting a better result compared to when you are reducing a to just 1.1 so we came to the end of lecture 12 where we have seen that various things that effects the CPU performance and next we will see in some more detail in the next lecture 
PTbx2MXkdDU,28,introduction to Computer Architecture and Organization,2016-10-08T04:21:12Z,introduction to Computer Architecture and Organization,https://i.ytimg.com/vi/PTbx2MXkdDU/hqdefault.jpg,Digital Megatrends,PT31M42S,false,339,3,0,0,0,no need to be serious can smell aroma r-rahim so today we learned about actually we bite actually chunk of knowledge what is this computer organization and and I don't know sometimes you've got cause okay it's computer architecture and organization we are saying on one side we have software we have what software there's applications that's application any application do you understand like what mobile application like you have my focus of work okay you are using the browser you may use something else that's all application and application basically when you try to run any application you see what L got Adam number one okay then you use some programming language maybe what any program language now basically divided actually you got the thing like object code then you are giving it to the lowest level software there's a compiler that's what I'm fine remember one thing why do we give compile and in Java we don't actually have this stuff like we object code why that because because compiler on a right side sees the hardware okay understand i'm saying this is where we understand what is the architecture because compiler one side sees what's the hardware in the harbor side you can see for example what can registers I have ok what kind of instructions I have how much instructions i have in my hand so what kind of things i will be doing how to convert this object cool is the mashing language but off the link ok there are the linker also which links the other files together in history we are doing clinking three files together in a project right we created projects start linking things ok and we had actually converted to national language means we actually have to know what the hardware is exactly that's well that's how then only we can convert our language into the into the mashing language is it so compiler actually is directly is seeing the hardware okay it's def seen the harder visual environment okay your programming language is a programmer you don't know actually when you write AC program you have no knowledge maybe of what registers the system has put your writing for the program in our uniform physiol and 32-bit processor and it may have it may happen 64-bit processor you never know you're writing your C programs and riggers and you don't know the architecture and organization of your system you are no knowledge but you're right but actually who is mapping this gap it is actually the compiler that's what we are trying to give we are trying to map the gap from application to the actual physics you have an application and you have physics on a downside understand given site then run over run by the recording log try to understand concepts this is a very important concept people maximum times keep this concert on the wire ok so I'm saying application and I'm say in physics we have to map the gap we had actually mapped this application out of the physics but what the electrons run well actually what happens is what you want that happens ok so we are trying to pull this abstraction labels one on one so I'm saying boo loo gone through electronic language give it the OS and compile the compiler converts to executable code because if knows the hard way it actually knows what hardware right but there's a problem with it there's a problem in this approach if we do like this we say hey compiler work on a hardware it is a problem in this is what happened before nineteen sixty when people write in architecture and implement it they write something they say hey this is my design I should do this and build this bull exactly this thing this is what was fused the design and implementation was fused i will submit my terms and give it a proper terms to these now my architecture and micro architecture or organization was fused means the design and the implementation fuse in 1960 before that okay so what happened by that what happens if we if we fuse that if we if we ask you is that what happens if we exactly right here our architecture design that how my heart is and implement it okay then you this becomes inflexible autumn this becomes inflexible ok to order you have a change in the implementation for example you are using d-lab there has been a tremendous change the ram has come up these days the technology wise okay so that means they threw off that machine and put me on with different architecture application why is there a lot of changes application was a lot of changes once if you see your Mac suffered in before huh and our today you see 20 30 max a good is totally different so we have what they are tremendous change the both sides from this side has like that size from this side as well as there is a tremendous change so we have this we map maybe this and that between this and I will map if we what I'm telling you is I write a design and I made me a machine out of there that means I can't change I have to throw it off right but now i am telling you is all I'm telling you is do not implement the architecture like this rather separate architecture from implementation you say hey I have this thing the design phase okay but do not start implementing same implementation separate it now you say for example I have a house which has this is this but morning may have different technology maybe you were building the beginning the house was same house was built using the boot that you start using the bricks and the mud that they kept the cement then they've came the you know p 0 p now you becomes obsolete okay now here comes in the new paints and all that but again the architecture the same building the house but how do you implement it has changed implementation has changed meditation is watch it changed okay you are used as a question right in special songs salsa music using f a b c d 5 MW or no on 1 2 3 4 10 12 15 10 12 13 14 15 socks on this 1 2 3 4 10 2 a 3 Felicity a be silly for one map one two three four then 10 jewelry will start doing this is so insane sure what I'm saying that there is an idea that we should do what we should separate implementation from the design so we are saying that architecture basically is a design after you basically is what which stays here we stays between the loyal software and the hardware ok the oxygen which is here the gap I have put in here except with the gabba gabba what gap of our clips which says how do you implement the low level software how we have you actually this loan and software interact with the hardware it is an interface between the lowly software and the hardware ok same to face between these two guys so the first thing which comes up is instruction set architecture ok we understand we have an instruction set architecture is the main thing in our architecture how what instructions you have you want to execute this particular thing on this machine what instructions machine has ok we will understand easy indet able to read it as this chapter what is instructions to architecture does it make sense once n then you need to know what register transfer language we need to know electric on register transfer language rtl ok what you are not only this thing you need to know what addressing modes you have what addressing modes you have how we address will you put your operands in the wheel and take it as a chapter addressing modes their own servers are addressing modes do we take our variables in the ramp or in a cash or in registers how big registers are if we put our eyes to how big it is how we caches you understand these all things come up here in the computer of which I say CA to go okay so there are some time you the way the chill girl happens so what I'm saying so you need you need to come up here and design something most important thing is in a saucer set architecture now before nineteen sixty you had in the successor architecture you understood what I sisters we have all that I'm starting already but it has a province do not design an implement we say we have a car for example user this thing and that thing it means it has a converter which pumps while okay from the car from the tank tank and all that we have this engine which we generates energy and all that but don't start implementing it why because if you start implementing the same stuff you can then change right you said you have a converter which pumps oil but today you have the processors in that okay which is not mechanical which is hundred-person automatic which brings oil even single single drop is you know saved you root 3 you know then started euro3 in your cars in the motivation of this start putting computed it in kerala today we have around 50 60 computers in it processors working together you can do that you can use totally different technology to pull up your eye sees the way we have gone VLSI technology is it every time we are squeezing our circle squeezing or yesterday i was saying you see that they have both a chip of a grain size rice grain size and they can make up a no tremendous smallest then they will have a smaller devices small optimizing is faster devices okay how you know nanotechnology has changed a bit of the nano technology is changing and changing and changing the way we use mobile phones way back big and off no functionality today we have small devices so powerful it's because of these you condensing of chips when you could condense chips condensed you condense chips there are no two today you have ok Samsung Note 2 if you have in your hand right that processor which it had we would have dropped it in service before you drunk it the server will have this much abrazo a big server will have across and today you're in a hand such a big processor and possible to put up in a small device because technology is changing but if you fuse instructions architecture and the implementation you're literally throw off this whole machine you throw this architecture and write Nemo architecture numbers we say right architecture design but do not it start implementing it ok but implement it separately how you bella mente it does not architect this job let it be the organization microarchitecture who have to be implemented this is what Intel is doing it Intel is you know x86 processors with an x86 processors what it calls my x86 8 9 8 63 or 64 and 6 and so on then pandya one and Adam to then came the three mp4 mp4 we started your course then concours and you know it's going on architecture the same are getting the same that may be more normal ok we have a lot of architectures like that's controlled basis we have data flow based octave is also we're totally you know to mendes architectures these days going on ok when we did a flow architecture which we will take up what is that it a flow architecture what is control flow architecture what is you know one newman convention warm and other models ok we'll see that we will read them these are ready you know intriguing concepts so what I'm saying is the architecture was same x86 and we have come up and change it let's started changing it sorry what let's start changing this no knock pictures honey why we separate architecture from organization so what is in product of architecture in general design and here we say obstruction of these Devils to read about these levels is ultimate architecture is it so what is architecture a software or it's a hardware neither it is software not hardware it isn't normally it is a model it's what after architected is a model it's neither software right now it's not hard way it is a model it is a contract right now it is a contract between lower level software and the hardware it's a contract that hardware is saying that I will do these things okay and software actually you want to use those things and it exactly does that okay the Machine exactly does that what it is meant for using the computer architecture okay it is the program will sleep it's what computer doctor she is a programmers view of looking into the machine because we will be interested what addressing modes we have what registers we have is it so it is a programmer sphere is what okay so computer organization is this electronic engineers you is there a chronic in general were working on that you are saying that you need to do these these things this is our programmer we will try to do that and I don't worry about how they implemented I don't worry how actually the implement this is then the hardware engineer will just actually implement that may be there for example we say hey onion addition I need a multiplication I need to multiple programming wants that I 1 or 2 plus 2 into 2 and give me the answer I don't care how implemented okay maybe you have a multiplication unit well say to cross 4 equals to this is multiplication unit you input it two and four and how this comes is what but how you implement it i don't know the black box for me i do not want to see inside what lies inside programa program want to see that i have in my destruction architecture such a setup duction all the instructions which can multiply the smashing can multiply for me I'm interested only in that maybe they will use what they will use a multiplier unit or they may use an error when you use in what which I'm basically doing interval two plus two plus two plus two using an algorithm implemented using a software okay they actually do what they are adding it together 2 plus 2 plus 2 238 are they using a software to implement this or are these Hardware in multiplication unit which that we multiply a concern so I don't care we understand i am as a programmer interested in computer architecture CA not co as a hardware is you know i am interested in organization that to go into this black box and see how i believe what techniques are we use you have ah now you are your object it is telling me that this smashing after you telling me that i can register this much a resistor okay but how implements in history what techniques you what technology you use what devices you use what case you use what circuits use what we are ascetic knowledge you use okay you are using MOSFETs are using flip-flops or using capacitors any case okay i don't care as an architect is it i am interested in what you have it's a contract between hardware and software how implement is there some some different story how implement is actually the organizations or magnet do you understand are we are we little bit understanding difference between objects organization today sure so it was fusion 1960s when idea this term comes up from from the IBM in 1960 when they built IBM 360 mashing they are actually what they had actually four lines of computers they are watching four lines of computers one for business one for research 14 market 14 other guys okay they have four and they these four lines are actually different architectures the business objects were different than the scientific object now some mash is used for sign hypothesis I'm actually using your business purposes a mash use it for gaming imagine use for maybe some other things okay we had different purposes machines now the combined together all they make a single architecture and this thing implement it differently for different purposes this is what we have a family concept in indep x86 these are all x86 families okay we had 286 where 306 there for 6 and so on you will have an assignment okay which is the history of computer you'll understand what is this the first understand history of computing stay o generations first generation second generation history that you will prepare tomorrow from any book okay any undergraduate book you will have that will have history of computers in here comes up and what is happening then later on when the transistors came up with ICS came up okay and so on is it that's what PC Sarkar gowns stuff will come up repute tomorrow some yoga asana yes history like 80 x + 3 y 4 over 4 the Hydrans to love it as summer and also on emu chicken up ways even i get to get I will ask everybody and everybody should have a note of that book sipping or the node tuna again you should have known of that and everybody have a chuck do you understand this class we proactive I'm telling you if you do not want to read let's see that home if only one or two guys are here I am more than happy I don't know I don't need anybody right if more two guys are here but breeding enough for me okay I don't want to make crowd okay i want to make quality people around me that's why I'm telling you this here because maximum gang of people or territory is not the written is something about subject and i will like to teach i would love to you something beautiful something you don't know how much beauty lies in beneath it the thing is that you have to come up you have behaved like computer science students which is lacking these days and i'm fed up teaching now and really fed up teaching because CUDA is even beyond the beginning essence you go John popper are you better go giving me crap address so Messiah ikea place immediately otherwise a teacher becomes 0 active more and more if the students are active and i became lazy day in day out lazy day not even go ladies i know i had so much of further to teach now i am losing my charge because of students as a foolish people all around coming up to the computer is nice what they should go to the you know read arts and all they can't bring computer science unfortunately they have no zest and zeal to learn so what I'm saying you is if you want to come here you're welcome but be proactive come up with the preparedness okay this is a well-known scientist reset opportunities are for those who have prepared okay of course it's not for anybody else it's for those who are prepared so make your brain prepared that this is something different this is where you learn something big ok so for that you have worked hard there's no alternative then you have to work harder so you have to go to the home you have to come up with the book ok and you have some real but you have no maximum times students have stopped real go to the teacher they think okay it is it I will have a copy of this this is enough Oh me maybe I go to another teacher another teacher from denying baggage and all don't do that open up a book and start reading from the book okay where you actually do what come up with the history you understand you outside the history of computers that should be on every day our gives will assign the next I'm it will not be a firm pc software stuff but house Madeline's of topics okay like I would say what is a great crave because there was no I am started working on craigs an example of go to discuss at the end of this object organization introduction okay as an example what is the craic what is super scalar architecture what is vector processing these are little advanced it not advance but people are reading it way back look like we're stopping but you have to know all these things but this will happen only know we can make this subject something will understand what is quad-core doing multi-quote will discuss multi-quote only when you are what and i feel you are the guys who deserve these things but when i feel you are just an ordinary people then i will wind up and start seeing what is in the syllabus and try to finish it up you understand so it is up to you and am again wondering if anybody don't want to read stay at home you will find nothing here you can board a nerd a out okay so what is architectural civilizations so in 1960 we said we had four lines of computers in IEM ok and IBM does what I finished brings recession you're not tearing it another question so what I'm saying is I'm saying before I computer or might have different things we said combine this make one architecture and implement is what access six families does they are making a way back when they were 80 processes okay there are 8-bit processors and then they have come up to the 128-bit not with 64-bit is in the wall and no wonder if it is coming up so what I'm saying is they have gone so far architecture the same it is only the organization which are changing so we I myself was telling people that architecture is dead you know three years back for us that was telling people with up is dead organization is key but today we have come up in new realm you know one or two years that you started to three different architecture it's also now understand architecture was dead because if you see x86 family I read from the diesel seemeth that octavia don't get it you know advanced from rockridge book that undergraduate book something you know graduate books after graduate pose with your books that architecture was then it was except six this star architecture and what are they were kicking on his organization the implemented different we implemented different we implement but the same one room and stuff for example what is the one woman doing what is this octave what is control flow architectural what is data flow octave to be discussing these terms and big terms we will understand this off what is data flow and we understand the programming language of computational model then what is that okay and they have come up a long way same archetype chief architect she was dead this is what I used tell people the object is there are musicians kicking butt today we have different architectures the op is so the people maximal tell people use it say that there is no future and not when you need computer hardened objection you are not actually you reading this for building a system for building a new system you're not using to them only to understand you are using only contest because architecture will not change but now things have changed you can read organizational architecture basically two bull system which is more efficient bull the system is more faster okay you have one architecture for example and are you implemented organization in different way for example a faster okay but cost your system but if you pulled a faster and cause the system there is your market this is business will be down because they will not read people who will buy it there is a one philosophy people will buy it when another class of people wanna cheap okay but slower then some people who want to bite you know which is business oriented some people who want to do business transactions on it they need computer like that some people who need to sign a few competitions on that they need computer like that some people who need it for ICU for example there is in hospital there's an ICU where they need a real-time systems real-time systems actually do what they are not worried about interface because if you see windows 8 lot of graphics lot of computation is no wasted on coloring graphics shadows transparency lot of things you don't know that you need performance you need if some input come should do the job if for example the heart is sinking it should open up the ventilator for the for the patient it should not worry about the looks and graphics and colors it should do whole competition on what on the process on a job is what we call the real-time systems for example the system is monitoring the r RR is monitoring the sky that is the enemy is coming or not it should not worry about the graphics and looks at all what should worry about Sheree bother if the enemy's plane is visit visible it should contact the no missiles missile system file if for example you see in the Israel they have they have the missile god okay they have the battery will the gods the and I missile battery but if missile comes in anywhere in the into the territory they have a missile system if i'll the exact to their target and then destroy it in here they are anti missile target battery there so the systems which are monitoring that piece of area they will not worry about the graphics and all they were even what stimulus and response quick respond to the stables they ask that job and we can you same architecture but all the relation is different implementation is different we understand but if you have a you know a system to us out we need a fancy system of people who use a droid their homes they want to cover for desktop and all that even they do nothing on that maybe we watch your mp3 music on their headphones and they know nothing about computers right but they they want to this should be fantastic the cover should come up huh and all that and at that moment you want graphics oriented stuff and all you don't need the atomic performance they are users who have different tastes and different actually needs and you can do that using same architecture but different organizations so what is architecture it is a software or hardware it is in more model once sometimes we put up hardware into it also why we'll discuss tomorrow sometimes put up otherwise it's a model it's not hardware it's not because it contracts between harder okay does make sense yes sir and organization is the realization of architecture tribal organization is it it will be having our organization is the realization of object how we actually implemented so do we have a clip of nursery so that's all good 
hBTy_-dJiro,27,This video contains the differences between Computer Organization and Computer Architecture.,2019-08-25T05:43:22Z,COMPARE COMPUTER ORGANIZATION AND ARCHITECTURE,https://i.ytimg.com/vi/hBTy_-dJiro/hqdefault.jpg,DIVVELA SRINIVASA RAO,PT9M,false,3784,80,2,0,5,hi friends today I am giving a lecture on computer organization and computer architecture in computer organization and architecture courses so it is one of the most important question it is one of the most commonly asked concept in computer organization and architecture so these are the differences between computer organization and computer architecture now we go for differences the first point in computer organization is computer organization is concerned with the what are the hardware components of the computer are there and how they are connected together to perform a particular task so these are the points that are described by the computer organization next one whereas computer architecture is the concerned with the structure and behavior of the computer system okay what is the structure the computer system follows and how it is the design and how it works that are the points that are described by the computer architecture okay next second point in computer organization while designing a computer system computer organization is considered after the architecture okay first we are considering the architecture of birth that we are considering the computer organization whereas in the case of computer architecture while design a computer system okay we are considered computer architecture as the first after that computer organization is considered white because so here computer architecture is nothing but designing of a computer system once we are designing a computer system and how it is works okay so because of this reason computer architecture is considered first by designing a computer system after that computer architecture we are considering computer organization next third point in computer organization computer organization involves the physical components of a computer system what are the physical components of a computer system like adders subtractors signals peripheral devices sub cube design okay these are the physical components of a computer system that are involved by the computer organization whereas computer architecture involves the logical components of a computer system what are the logical components of a computer system instruction sets addressing mode okay catch optimization etc okay these are the logical components of a computer system that are involved by the computer architecture next fourth point in computer organization so organization describes how the computer system works okay how the computer system works to perform a particular task to perform a particular program to execute a particular program okay organization describes how the computer system works to execute a particular program or task whereas architecture describes how the computer system is designed okay how the computer system is the design that is described by the computer architecture because of this reason okay we are considering computer architecture is the first after that computer organization is concerned first we want to design the computer system after designing the computer system how it is word okay so because of this reason computer architecture is the considered first after that we are considering computer organization next one if the point in computer organization computer organization deals with the components of the computer and how they are interconnected to perform ekam particular tasks so it deals with the what are the components of a computer for example so input devices output devices control unit memory unit automatic closet unit okay so these are the components of a computer how they are interconnected to perform a particular task so that are deals by the computer organization whereas here computer architecture acts as a interface between the hardware and software okay what are the hardware components are there what are the software components are there so computer architecture acts as the interface between the hardware and software it is the intermediary between the hardware and software next six the point in computer organization computer organization describes only lower level design issues okay what are the low level design issues that are occurred okay that are described by the computer organization whereas computer architecture describes high level design issues what are the design issues that are occurred at that high level so that are described by the computer architecture because computer architecture is considered by designing a computer system so because of that reason so it describes the high level design issues whereas computer organization deals with low level design issues next seven point in computer organization computer organization defines the physical accepts of a computer system okay what are the physical components are there in a computer system so that are defined by the computer organization there as computer architecture defines the logical acts of accepts of a computer system what are the logical aspects of a computer system instruction set cache optimization addressing modes so these are the logical as accepts of a computer system that are defined by the computer architecture so these are the main differences between computer organization a computer architecture so thank you thank you for watching this video if you liked this video please subscribe my channel a so the Bellis really Massaro if you liked this video please share this video - we were friends I classmates 
7Mw6T879R_Y,28,"Hello welcome to GATE lectures by well academy

*******NOTES Link will Posted once video Completes 100 likes also Subscribe to Channel*****

Click here to subscribe well Academy
https://www.youtube.com/wellacademy1

computer organization instruction set | Instruction Format | computer organization for GATE in hindi | Lecture - 02

About Course
Hello Friends Welcome to Well Academy in from this Course i have started Computer Organization and Architecture for GATE and the subject Computer organization and Architecture in hindi is taught, So watch this computer organization and architecture tutorial till last video all the lectures of computer organization and architecture are FREE 

About This video
In this video computer organization instruction set in hindi is covered also  Instruction Format so you will learn in Detail what is instruction Register and Instruction Format in detail
and now some videos will be on computer organization tutorial,if are searching for this following lectures then you will get your content in this video
computer organization lectures,
computer organization gate,
computer organization for gate,




DBMS Gate Lectures Full Course FREE Playlist : 
https://goo.gl/Z7AAyV

Facebook Me : https://goo.gl/2zQDpD

Click here to subscribe well Academy
https://www.youtube.com/wellacademy1

GATE Lectures by Well Academy Facebook Group
https://www.facebook.com/groups/1392049960910003/

Thank you for watching share with your friends 
Follow on :
Facebook page : https://www.facebook.com/wellacademy/
Instagram page : https://instagram.com/well_academy
Twitter : https://twitter.com/well_academy",2018-02-17T12:52:46Z,computer organization instruction set | Instruction Format | computer organization for GATE in hindi,https://i.ytimg.com/vi/7Mw6T879R_Y/hqdefault.jpg,Well Academy,PT5M54S,false,91596,2230,71,0,92,[Music] hello friends welcome to El Academy memo of dual or last rodeo my computer organization start cardiotonic or its video member neville on last video agora brainy they key to video the ecclesia to give is your series or give a bootie continually valley so last video my my stop key at the instructions way so it's video improperly instruction kibarim about carnival a he structure my explain course you count a memory miss a register register may say ALU now Algar simple way me vodka at the homes OB program licked a Harappa visit program leak away to instruction programs kuch is tarah sodium line-by-line instruction leak there now is nobuta ke ke Joe instructions of licked an Evoque embrittle pc the resin Musco interpret carnivore Time Machine Co interpret karna porta to have a heater instruction here whose Koger interpret Karnataka still as a corrected Nahas a suppose Mahalo a construction Lea or whoa instruction Co machine yoga both knee instruction register report of carita so instruction register kata is the rocky format know that you have a first totem mode Jagoda opcode and the third one which is called a operand okay so you think is a instruction register below T or normally saw a computer may have a present OTA now one by one September I made a guinea mode game mode any addressing mode here who can decide Karthikeya operands historicity aware now opcode K suppose instruction addition multiplication could speak operations performed are they use calm opcode both a addition multiplication division increment stir again okay so yeah opcode a object operand key buskers at the Opera and meth lab care agar muta-do number couplers converges at two plus three connect to each of plus hair o plus k op code or two or three k ma operand here now a brand illegal anther ago they making us a polymer of Chewbacca's oh yeh jo o pronto Jana move memory miss early ages ages a given a last time of copa-data kid iridium exes naked a memory may say suppose you're a supporter value linear to my sky drizzling on the register can the registered measurement le Melaleuca so yeah happy memory method directly nearly address Lozada the most probably happy Cuyahoga address the yoga ball sorry chance sa lake in hemara instructions okinawa possible is 0 address instruction home yup in one address instruction oh yeah fit to address instructional Agora bath Carruth we have a ROM lick the instruction add a so you add a comma Club K given address instruction IQ canopic and Trust the our Jeff Gordon bath carey i NC increment accumulator to he k 0 address instruction a qk Hapuku we address the any a directly instructions perform karna so lamp is zero interest instruction one interest or to address now a job Brandon busca address K I am PBOT sorry possibility a girl 0 addresses that means address key question tiny so our am saying execute curse that they make in Java the one address a couple kissing ik X's other addresses to you have it check on apathetic on same mode may have concerned racing mode EMP use case as I've why because both sorry possibilities a kabhi kabhi kabhi offered directly from america address the other Agha Kabhi Kabhi register caja does the idea yoga or copy esse address the away oh god Joe address batatinha mahram america address memory Makaha pay yo Nicky Hager of below to you Joe address jiske me basque around iridium access card a memory say - cam memory card dresser - it a possibility we do see possibility care of the address of register other register mezzo t3 possibility of a address busca de yoga hampi is characterized the avoid mclubbe address si address the yoga Jahan pay memory care dress the yoga so yeah pho double pointer the Kurama double J so J mu j VAR address the address exactly president okay address of memory Houska address the yoga so it's Rawat confusing deselected lake in couch examples all carrying a toccata chandÃ¢ga now yeah address agar address instruction have one addressed to addresses of commode pathology so easily a gauge of concept and I'm directly designing been enjoying it so um Casas start occurring I'm starting in memory mode of addressing says so next video semi start an envelope addressing mode say licking throat about optimal register key parameter do not register kappa repeatedly on am so directly addressing mode measuring it basics of registers they can get types of organizations taking it then i'm addressing mode she should look arrange escape about opcode kibarim it and then I'm a gay is flammable naval a so it's medium Amanda instructions cabeza Kafka ba-da-da keeps the restrictions leadeth a instruction register calendar org is the resep yes are it is a performative so up kupatana che mode mode of addressing a big guy of coquettish lk opcode k OB possession way or a prank a booby was a gel gay or a bronco addressing mode case of say XS Kia's are there so I hope is to rockin step up to sahaja yoga other of nice video go like Nakia to like a DJ or description with many up good notes D here is video key or jesse's you say videos who teach ie keynotes update oh these I hear or description is sorry detail hack notes of cookies there's a linear now thank you so much for watching this video Agora a BTech well acadÃ©mico subscribe link here to subscribe curly J or by like incorporates kediri turkey Mary Jo B updates a walk or regularly mentor a so thank you so much and take care [Music] 
w9IMYNUsmUE,19,"Stanford EE282, ""Computer Architecture and Organization""
Lecture #16: virtual memory; the DEC Alpha 21064 memory system
Professor: Len Shustek
March 7, 1995",2017-10-20T00:56:22Z,Shustek EE282 1995 03 07,https://i.ytimg.com/vi/w9IMYNUsmUE/hqdefault.jpg,Len Shustek,PT1H13M,false,51,0,0,0,0,"hello welcome back there are two handouts today one of which is the real handout number 25 which is a problem set number for solutions the other one is a fake handout number 25 which is supposed to be handout number 26 which is set of lecture notes on Io which will probably not get to today but will start next time handout numbering scheme is a pipeline and that pipeline had a read after write hazard that we weren't fully interlocked for so we executed it incorrectly so in fact I don't know what numbers are gonna be on the handouts this ITN gets but if you get to number 25 an electro note that is actually on number 26 okay last time we were talking about virtual memory but I would like to back up just a second to go back and revisit some things about suti pseudo associative caches some questions that came up so let's look at that for a second one is the way we compute the average memory access time now I remember I'm sure you're all familiar with it because you're doing it for the programming assignment pseudo associative cache is the scheme where in a direct mapped cache you look at one place for the match and if it's not there you look at another place for the match and if it's not there than you miss and what I had written down as an expression for the average memory access time was something like this it's the time it takes you to do it which is probably one cycle based on doing the first test at the first location and then there's some time based on some number of time that you miss in that first shot and that that's the miss rate that you would have gotten had it been a regular direct mapped cache times some cost to do the the second probe so the model is you try hit if it works you're done if not then this percentage of the time you did that and you had this that's also the Miss determination time to see whether or not it's not there and then some percentage of time which is the miss rate in the direct map cache costs you to do the second probe then if you miss that as a third phase there's some percentage of the time which if if this really is like a two-way associative cache it's the Miss rate of a two-way associative cache at the same time times the Miss penalty to go to disk to go to main memory and it's a kind of three-step process now what the book actually derives is a slightly different formula that shows this miss ratio or that uses for the Miss ratio times the cost of the second probe the miss ratio of the direct mapped cache - the Miss ratio of the 2-way cache and really what that's doing is saying that if you miss in this second probe then it didn't cost you anything because you've already gotten started on the access to main memory so that's a different model for the way the sudio associative cache is working that says well first you try the personal and then in parallel you try the second location and also go to main memory assuming that the first one might miss so it's kind of parallel operation depending on how you implement the pseudo associative cache you might use one model or the other I think this one tends to be more realistic in general for these caches we've not assumed that you sort of speculatively execute both pads and try to do a hit in the cache and at the same time begin accessing main memory assuming you might do a Miss so if you if your model is that you do these things in sequence then you get the expression that I originally showed if your model is that you start the two second parts in parallel the second probe and the access to memory then you get the expression of the book question about that the other thing I wanted to do was to correct an answer I gave to a question that was asked well I didn't didn't tell the whole truth and that question was whether or not the algorithm as originally described actually works remember we talked about the the use of the rehash bit to get better performance in this pathological case we're alternating accesses would cause us to do a Miss on on every axis the question is if you don't do that does the does the associative cache work and the answer is well almost you've got to make one minor modification let's let's look at this pseudo associative cache remember the cache has a tag part and a data part and because of this high order bit flipping we're doing we've essentially divided it into two halves and we try first for a match in the first half and then if we don't match there we look for a match in the second half well the virtual addresses that were or the memory addresses that were using to reference this has a tag part in it and has an index part in it and we're using the index part as the subscript if you will into this array that says which entry to look at and we might pick an address that maps into this location and we put its data here we might pick another address and let's say that it initially maps into the pair of this item which is to say that these indexes are the same except that they differ by the higher or bit so this has a one in the high order bit and this has a 0 and higher of it well let's say that we wind up rehashing this entry into that entry for some reason and let's further say that the rest of these tag bits are identical if that's the case then by looking at this entry in the cache how can you tell whether it's this address or the data for this address in that cache ok or the data for this cache for this address in that cache location that happens to be in its alternate location the answer is you can't tell and and that's the algorithm that Hugh's than the cache is broken but it's easy to fix and the way to fix it is just to include in the tag of the next bit in the address the most significant bit of the index so if you include the most significant bit of the index as part of the tag in the cache make it one bit wider then this pseudo associated algorithm works still has the problem we talked about last time where there are some fairly we can be pathological only slightly ill cases of addresses that will cause a lot of misses in the cache and that's why the the rehash bit algorithm was invented you won't feel comfortable with that rehash bit algorithm you understand sort of intuitively how it works so do you want me to say some words about it yes okay let me yeah let me sort of describe it a little bit I don't want to describe it in all gory detail because then it's not fun for you to figure out on your own basically remember the first hash function is B of a so we look in location B of A in the cache now we've also got this Arbit that now tells us whether or not whatever is residing in that cache location is in its primary location so if we're looking for address a and we look at B of a and it also happens to be the case that the are bid is 0 then we know that we've got a hit because we've matched in its primary location we know that it's one of the two locations because B of n maps there and since our bid is zero we know that it's it's a primary location and it can't be somebody else from that from a different location and wound up here now one of the invariants of this whole process which you should keep in mind is that the the two are bits can't both be one the two are bits of two related locations can't both be one so that of the two locations that are related to each other by having a higher or a bit different both of them can't be occupied by somebody in their alternate position and if we if we guarantee that then you'll you'll see how this algorithm works out so let's say that we're not in a primary position or or that the guy who's in there is someone else now we look to see is he rehashed from an alternate location if he's if the guy currently in B of a is rehashed from an alternate location we don't have to check our alternate location because if we were in our o'the location then both of those entries would be rehashed they would both have me orbit one so in fact if we're not in our primary but somebody else is in our primary that's rehashed then we don't have to check our secondary location and we can just replace ourselves basically put ourself in the place where B of A is and then we're primary so we set the are bit equal to zero anything so far okay so now we know that somebody is in our primary location and he's a primary so we could be in our secondary location so we have to check our secondary location and we check to see is there somebody matching our index in the secondary location and incidentally for it to be us the art of it has to be one if that's the case then we have a hit but we want to swap to get one cycle access time if we happen to make an access to our location the next time now remember when you do the swap you've got to think about whether or not you need to change the orbits in order to keep them correct part of what you've got to get right in the programming assignment if it's the case that that either the somebody else's tag is in that location or the Arbit is zero meaning it's somebody's primary location it can't be us in which case we we've got to throw him out so we replace whoever is in that location which is our secondary location and then we swap to make us primary and again you've got to watch the orbits to make sure that the arbiter is on for people who are in their alternate location and off for people who are in their primary location and this will guarantee to that that there are no of the two items there aren't people occupying it in their secondary location in both slots that opening what those operators what are those up these operators or and those are end of the rant yeah I guess in the sea notation would be ampersand ampersand or something okay it's hard to see it thank you sorry what sorry what I am not showing you sort of intentionally and I don't want to draw this algorithm in such precise detail at all you need to do for the programming exercises you know transcribe it from a flowchart into C without really understanding what's going on I think it's important to understand how this algorithm works and one of the details that I'm sort of buzzing over here is where you said or reset the are bits in order to guarantee that this algorithm is correct I know it's mean of me any other questions about caches and to go with soceity and like okay now we go back to virtual memory a virtual memory was this technique which was originally designed so that programmers could get more virtual memory than they had real memory no it's in the notes and I want to get finished with virtual memory and then we'll come back to that thank you very much so virtual memory is a way to make big virtual addresses into small physical addresses and it turns out it also acts as a cache and the sort of general scheme for translating virtual memory addresses into physical memory addresses was something like this this is a kind of typical multi-level memory address translation where you divide the virtual address into different parts and you use each part to look up a entry in a page table which either points to another entry in the page table which might point to another entry in the page table which eventually will point in main memory to a physical page or have the entry mark saying that that page doesn't exist or have the entry mark that says that page exists but is not in physical memory it's out in main memory somebody pointed out that I kept referring to this as a 32-bit virtual address and that 10 plus 10 plus 10 plus 13 in no base that they could think of added up to 32 just true the dec alpha which is what this scheme was modeled after actually has a 64-bit virtual address mapping into a 32-bit physical address and most current implementations they're only using the first forty-three bits of that virtual address but as programmers write programs bigger than to the 43 they may eventually extend the virtual address translation to use more of the 64 bits of virtual address so that solved the first problem which was that the page table was enormous so now the page table can be much smaller because it only Maps the part of the virtual address that's really in use but the other problem was that after having devoted all of our time to being very clever at having one cycle memory axises many of in many of the cases now for every memory access we've got to do three other memory access to access the page table translation so the solution to that was yet another cache address translation cache or as IBM called it and everybody else does too a translation lookaside buffer and it's it's typically a cache that is very fast is designed to have an extremely low miss rate much less than 1% is desirable tenth of a percent is more like you like what you want so it might have some place in the range of eight to a thousand entries it only needs to hold translations for pages that are actually in main memory pages that are not in main memory that are either invalid or are written out to disk obviously don't have to be translated as part of it's fast process because in any case it's gonna take you a long time to process it so so it's only a subset of the pages that are actually existing in main memory and in order to get the Miss rate down as low as possible these caches are typically fully associative or very close to fully associative now what happens when it misses well it's certainly not as bad as in virtual memory going to disk in fact it's it's a relatively small miss penalty something on the order of 10 to 50 cycles after all basically what it needs to do is to look up entries in water more page tables those page tables might in fact be cached as well so you might be doing cache hits in doing the translation process now in fact there are two different schemes the original address translation schemes as done by IBM and their mainframes were all done in hardware when the miss occurred in fact it trapped to a piece of micro code used in the implementation of the machine that would do the hardware lookup the in the page tables and there a bunch of other machines as well that continue to do that x86 has hardware translation lookaside buffer processing but you don't have to if you carefully hone and tune the interrupts process for translation lookaside buffer misses you can do it in software without causing too much of a miss penalty so MIPS in fact does that one of the advantages is that it if you do that right then in fact the page table structure and the algorithm to use for looking up things and page tables is not built into the hardware it's only indication of it's only a software artifact so you're free to change the page table structure if you wish although it's unlikely any certainly any user would ever want to do that it's an interesting area where the interaction between hardware and software can sometimes get you into a little bit of trouble I think actually though a couple of months ago there was a paper published in the ACM transactions on computer systems talking about in fact the MIPS TLB system and how it works in its memory in it's a software control page table sorry TLB miss scheme it turns out it's not quite all left to the software in the myth processor in fact what they do is they divide the TLB into two different sections it's a 64 element fully associative TLB and the first eight entries are intended to be dedicated for the operating system and when you take a TLB miss the hardware gives the software interrupts process some assistance in deciding which TLB entry should be replaced remember this is a fully associative cache we've got to do something like LRU if we're lucky to figure out which item to throw out in fact what the MIPS processor does is not LRU it's that random algorithm that we discussed before that turns out my friend to be almost as good as there are you in lots of different cases but the model for the MIPS was that those TLB entries they get thrown out are only for user processors processes not for kernel processes in fact the MIPS has a multi-level page table much like the Alpha if we can zoom in on this this this shows a level 3 page table going to a level 2 page table going to a level 1 page table for user pages it turns out they have another path where level 2 page table entries point directly to pages for the kernel and those pages don't participate in the TLB discard policy these are the eight TLB entries the it doesn't give you any assistance for in doing page no TLB and free replacements well that worked great because this was all designed for an ultrix like system and it turned out that the kernel mapped by those eight TLB entries fit perfectly the a TLB entries that were sort of fixed or intended to be fixed mapped perfectly onto the size of the the ultras kernel then they ran a different operating system and it all broke down and in fact they show some numbers here for what happens as they move from one operating system to another we can probably just look at this part here this is the way they originally designed the system for ultrix these are level 1 user TLB misses and there are 9 million of them a lot of them and then level 1 kernel misses those eight special TLB entries that were dedicated to the kernel there were many fewer of them 135,000 misses in this case the hardware was well designed to match what the software was currently doing but then you run other versions of unix-like operating systems like OSF and various places flavors of mock with with and without the Andrew file system and suddenly the number of TLB misses for the kernel jumps up by more than a factor of 10 goes up from 135 thousand to one and a half million or two and a half million or 2.7 million becoming a significant fraction in some cases of the overall TLB misses so I guess the moral of the story is if you're going to design your hardware to take into account a particular software environment you've got to try to anticipate as much as possible the natural evolution of that operating system and don't build something into the hardware which will come to bite you later on I think in the later versions of the myths processors they change that strategy they changed does anybody know the details they change the number of TLB entries they change the number of perhaps that I think it's the the at least one of the proposals in that paper is to change the proportion of TLB entries that are allocated to kernel space versus allocated to user space to try to get around the problem that the match between the hardware and the software isn't what it used to be this is what a translation lookaside buffer looks like it's a kind of standard cache in the sense that it uses the virtual page number part of the address to compare to the virtual page number tag in the cache typically fully associative so all of these entries are compared at once to the virtual page number and if there's a match there's a hit and the physical page number is part of the cache is concatenated with the page offset which flows through unmolested it doesn't participate in the TLB process now there are other bits that we might want to put in here certainly there needs to be a valid bit saying whether that particular cache line is valid notice now that we're doing all of the references to physical memory without looking up things in the page table so other status bits that are in the page table may have to be put into the translation lookaside buffer like for example write-protect bit if you need to check whether this particular process has legal right to modify a page that's a bit that used to be in the page table when page table wasn't accessed anymore so you've got to add something to the TLB to be able to to do that another thing you might want to consider adding to the TLB is a dirty bit for purpose of doing the LRU page well for being able to dump pages out to disk when they need to be replaced you need to keep track of which pages are dirty and if we're not making those changes in the page table we'd better make those changes in the translation lookaside buffer and then later have a have a way to communicate that information back to the page table and eventually be used to write memory out okay that's tlbs and before that we talked about caches and the interesting question is how do the two of them interact we have to do a translation process and we also somehow have to look in a cache to see whether we can avoid going to main memory in what sequence should we do those operations well the one that makes the most sense is that you'd like to treat the cache as just something that makes main memory faster and if main memory is addressed with physical addresses then you'd like to address the cache with physical addresses so you treat this whole subsystem here is basically just a fast main memory well that means you have to do the translation first you take the virtual address virtual page number and offset turn it into a physical address before you access the cache and then access the cache with the physical address so the index field that you use for looking up in the cache and the tag field that you use to check to see whether the correct value is in the cache for both physical addresses that works great the problem is that it's a serialized operation you can't do them in part in parallel in that take at least a two cycle hit for every memory reference the first cycle to do the translation the second cycle to use the translated address to do the cache look up so it works and it's been used by machines like the the VAX but it has this unfortunate property of turning what used to be a great one cycle cash it into a two cycle cash it so what can we do about that well one thing we can do about that is to decide to do the cash first put the tlb the translation process in front of main memory and that means that what we're storing in the cash is all based on virtual addresses we take the virtual page number so it take the virtual address and we look that up in the cache using some number of its bits which may or may not correspond exactly to the virtual page number and look up in cash what we check to see whether the correct value is in the cache is a part of the virtual address it's a tag basically the cache is just a way to get fast access to virtual virtual address data and then if it misses we do the translation to real addresses and go to main memory well this solves at least one problem it allows us to in most cases get back to a one cycle memory access the problem is that there are lots of other problems and the next slide talks about some of them one is that remember this is a multi programming environment and we're running different processes which may be using virtual the same virtual addresses to reference different data they may all all the users running on this UNIX machine they believe that their program begins at absolute location zero yet their different physical addresses our cache contains one entry for virtual address zero so we're going to be in trouble one way to solve that problem is just to wipe out the translation at the time that which occurs and in fact guarantee me that different that you can't have two identical virtual addresses is going to the same physical address another way to do that is to include as part of the cache tag an address space ID I didn't mean here flush the TLB and then flush the cache I think you've got to wipe out the cache because it's got virtual addresses for it's got real data for virtual address zero and when you switch a process you've got different real data for that virtual address zero so you've got to throw out of the cache the old values well one way to avoid throwing it out is to mark the data in the cache with something like a process ID or address base ID that says in addition to what virtual address this data corresponds to which process did it correspond to so that means when you do a switch from one process to the other you don't have to throw out everything because virtual address 0 from the next process won't match virtual address 0 from the previous process because the tag the address based ID part of the tag won't match turns out this is a big win the next chart shows some data that just three different cases one is if you're not doing any process switching at all the second is if you process switch but include in the cache these process IDs so you don't have to purge and the third is the case where you purchase the entire cache well when the Miss rate is high it really doesn't matter you're gonna suffer whether or not your process switching or using process IDs or purging the cache basically it's terrible in all cases when the miss rate goes down though if you look down here it shows that including the process IDs as part of the cache tag get you almost as good as not doing any process switching at all in terms of the overall miss rate so if you're going to use virtual caches that including the process ID is a good idea now this obviously depends on the frequency with which processes are switched if you don't switch very often then it's not going to have this big in effect all right don't you also have to purge the TLB though on a for the same reason um yes I guess you would also have to purge the TLB and for the same reason okay more problems with virtual memory and caches one is that there's a problem with aliases if if we want to have the same data being addressed using two different virtual addresses remember we showed that as part of the advantages for virtual memory that we can have two different processes using two different virtual addresses to get to the same data if we use virtual addresses as the basis of storing things in the cache then in fact we'll wind up with that data appearing twice in the cache so that's a problem we can either prohibit it or we can have some kind of a Hardware scheme that forces all of the data blocks in the cache to actually represent unique physical addresses which is a hard thing to do or we can begin to restrict the addresses that we sign in either virtual memory space or physical memory Shakers space two shared blocks and I'll talk more about the Sun in a second the other problem that comes up with virtual with caches that use virtual addresses is that IO devices are typically using real addresses and in order to access data which is in the cache they're going to have to do a kind of reverse translation from physical address to virtual address in a similar vein the shared memory multiprocessors you can think of that as a as an i/o device basically another process which is trying to access your data in the cache would have to know about the virtual addresses and invalidate cache entries they typically know a physical address which is what they're trying to access and they would have to based on those physical addresses understand what's in your cache which is indexed by virtual address so again it needs a kind of remapping to virtual addresses or another scheme you can use is to have in the cache both virtual tags and physical tags at the same time physical tags to be used by other processors and i/o devices or virtual pegs and virtual tags to be used by the the current process that wants to get one cycle accesses without having to do the address translation people tend not to use virtual caches virtual address caches these days the original sparks did and they got around some of these problems by various address restrictions in the virtual address space in the physical address space I'm not sure whether current SPARC implementations use virtual address caches still or not but there's another technique it turns out that if we're clever we can arrange to do the cache access and that these part of the cache access and the address translation in parallel remember that the virtual address is divided into a page number which has to get translated to a physical page number and an offset which doesn't get translated to anything which gets concatenated later with the physical page number well if it turns out and if we can arrange it so that the part of the address which is used to do the indexing into the cache contains only the bits of the address that are not going to get translated which is to say it's the size of a virtual memory page or less then we can start doing the lookup in the cache based on this cache index which is a virtual address at the same time that we're sending the virtual page number through the TLB to get translated and these are operations that roughly take the same amount of time and so they're both a kind of memory lookup operation this is probably certainly associated this may or may not be associated then by the time the data is out of the cache we've got to do a tag check we've got to check the tag part of the cache with the rest of the address but now we can check it with the real part of the already translated address so what we store in the cache is the physical tang tag based on the physical address of the translated address but we're indexing into the cache using a virtual index using the part of the virtual address another way to look at them the memory address layout is that from a paging point of view we've got in this case 19 bits worth a virtual page number and 13 bits worth of page offset for an 8k byte page from a caching point of view we've got 19 bits worth of cache tag we've got 4 bits worth of byte offset for a 16 byte line and 9 bits worth of cache index so we've got 512 one entries since and we can do this lookup in parallel with the virtual address translation so we're back to one cycle if you include the tag check as part of that one cycle so the the cache data is now based on physical addresses again we don't have the problem with aliasing and snooping from other processors and i/o but this only will work when while this is wrong it only will work when the the offset bits or the the cache index bits are not needed for the TLB look look up and that restricts the organization of our cache it means that the cache size has to be less than the page size x times the associativity like this is the size of the page you can make the cache as associative as you like but you can't increase the number of sets because as soon as you do that then you've started to use as your index an address that might get translated later as part of the virtual trivial translation so in order to make caches large you have to keep increasing the associativity this is a scheme that IBM mainframes used all the and in fact in order to keep increasing the size of the cash as they is main memory increased and and program working said increased they had to increase associativity to sixteen times or thirty-two times I don't know how far they eventually got before they ran out of money to be able to apply comparators to making associativity very high in fact this is probably the most common solution to the problem of doing simultaneous virtual memory translations and cache look up I noticed that 20 100 64 we looked up looked at did exactly that in fact it limited its caches because they were non-associated they were direct mapped to exactly a k4 instruction cache and data cache exactly for this reason they didn't they want to be able to do the lookup in parallel questions about that okay I just want a hint that in fact that's not the only thing you can do all is not lost if a few of the page number bits are used are needed for cash lookup and people have used some clever schemes in order to be able to violate that constraint basically to have some of the bits being used in cash the bits that might be translated as part of the virtual address translation and then these are called so-called color bits because they basically try to divide pages into different equivalence classes and all of the pages with those two B bits 0 0 0 you consider red pages and they're all 0 1 like the green pages or something like that I think that's where the term colored bits came from basically it's a way of just dividing the pages and the difference of different equivalence classes and there are several different ways that you can use this one is that you can decide to restrict the places that virtual pages can be mapped to physical pages in memory right now in that case each page frame and main memory is is a different color and you can by doing that guarantee that for for those 3 bits for those color bits that the virtual address is the same as the real address that there will be no change in these bits as they get translated from the virtual page number to a real patient of it what that gets you is that if you have different page numbers which will be different places in the cache then you can guarantee that there will be different physical addresses because they'll be pages that have a different color and they will have been mapped to a different physical location another trick that you can use with the same kind of color bits is to have the sort of opposite a case let's say that we want to have different virtual addresses going to the same physical address because we want to do sharing we want two different processes to share the same physical address let's say that for process number one it has a virtual address which looks like P 1 P 2 P 3 P 4 let's say that these are our text digits or something like that and an offset 1 and an offset 2 so this is for process 1 and then process 2 wants to be able to get to the same page but it's using a different virtual address let's say p5 p6 p7 p8 and the same offset and let's say that this is the part that is used for the cache look up well if you allow those two virtual addresses then in fact they're never going to be able to reference the same data even if you made this translation all mapped to the same physical page there's a problem because these will go if it includes this these will go to different sets in the cache well one way to deal with that is to in software assign virtual addresses so that p4 and p8 are the same basically it restricts the addresses of virtual pages that can be shared based on how many of the virtual address bits are used as part of the cache look up and if you do that you can still have different virtual addresses at least different in the higher orbits that share the same physical address and still be able to do cash lookup and translation in parallel a little bit bizarre but it is done these are the kinds of places where the architecture and the implementation begin to mix in messy ways in this case it's more of a software architecture in the operating system that is beginning to interact with nitty gritties in the way that caching and address translation is done from a purist point of view it's probably not the most desirable kind of design space but on the other hand before we leave paging I guess I need to say a few words about segmentation although segmentation is kind of out of favor these days segmentation and paging are very similar in many ways one way to look at it is that paging has one fixed page size or maybe a few different page sizes it's often the case that the page sizes it fixed is 4k or 8k or something like that some systems allow you to have a choice of let's say two different page sizes sometimes that's useful because if you use a very large page size like modern on 256k or bigger you can use that for something that has to be physically contiguous like if you have the frame buffer you know an image buffer in your machine which you're addressing with virtual memory it's not reordered the way normal memory pages are so by accessing it with one or a few very large pages you can for example take less TLB space so sometimes a system that allows a small number of different page sizes can be an advantage is that a question segmentation you can view as a kind of paging scheme but where every page has an individual size basically the descriptor the page table entry if you will not hold a segment table entry describes exactly how big that page is or that segment is and it could be down to the byte level it could be a 37 byte segment there are some systems in fact that have both paging and segmentation the x86 is one example that first does segmentation and then does paging and you can choose to use the machine with one or the other or both and these days it's more and more that people are using the paging only and not the segmentation what are the distinctions well for one thing paging is nice for the programmer in some ways because it's invisible addressing is just by one word which might be 32 or 64 bits but it's a flat address space in the case of segmentation it's a structured address space there's a segment part and there's an offset part and depending on the segment part the offset part can have different possible values because all of the segment's can be different sizes so it's exposed to the programmer now this may be a good thing for example I guess that it talks about it on the last line here this may map a model that you have in the high-level language for how objects are to be allocated and shared and if you X if you allocate or assign an individual segment to each object and you can feel free to define what an object is then in terms of protection and in terms of memory allocation it may map the addressing scheme and the hardware may map better to the software's model of what's going on in terms of page replacement if we do fixed size paging it's easy because any page frame and memory is equivalent to any other unless we're doing the color bit scheme of course in the case of segmentation they're all variable sized objects so it's harder to do allocation it's likeif allocation and software systems they both have memory fragmentation in the case of paging the fragmentation is internal to a page if you want to make your program bigger you make it bigger in units of 4k or 8k or 256 K whatever your page size happens to be the case of segmentation you can make your program bigger in units of single segments which might be single bytes the problem is that the memory fragmentation is external to the segment's when you look for a place to put a segment in main memory you may find lots of little pieces that are wasted and no big piece which can fit the segment that you're trying to bring in from this same terms of disk tract traffic this is a kind of weaker argument paging tends to be more efficient because it's easy to do multiple transfers of fixed size objects it's also easy to have tables that you can use to look up where on the disk those pages are and it becomes a little more difficult for segments that are written out to disk because they have variable sizes segmentation is is on the wane although had been popular in the past as you begin to take segmentation to more elaborate lengths people have built segmented and machines that have both segments the capabilities that expand the kind of protection scheme that processors are allowed to use any questions about paging segmentation virtual memory things faster okay now I want to cycle back to something that I think appears earlier in your notes that I skipped too because I wanted to get through the virtual memory stuff and this was along the theme back when we were talking about caches about how to make the whole process of looking things up in a cache and dealing with cache hits and filling up the cash from main memory as fast as possible so these few slides look at main memory itself and tries to figure out how it is that we can make main memory faster when we're accessing main memory to fill up the cache well one kind of obvious thing is to make it wider let's say that we have a cache that has a 32-bit bit path remember that my notation here is that lower case B is bit and upper case B is byte that explicit but I think caught onto that after a while here is a cache that has a 32 byte line and it has a 32 bit interface both to the CPU and to a memory so that when CPU wants one word it gets it in a single cycle when the cache misses and needs to fill from main memory it needs to go through eight cycles of four bytes each 32 bits in order to fill up the so one simple way to make of that process faster is to double the width of the cache so it's got 64 bits in 64 bits out it now needs only four cycles to fill four main memory one of Isha's instead of two instead of eight so we've doubled the bandwidth so this is a particularly good match for caches that have wide lines notice now that we've got to choose from the outputs of these cache Ram chips say which half we're going to be sending to the CPU so we've just now added a multiplexer in the path from the cache to the CPU which increases the hit time so it's a multiplexer it's in a bad position because it's made the hit access to the cache slower than it used to be that's one of the disadvantages of making a memory wider the other is that all of these data paths suddenly got wider which costs more pins more buses more space more chip space or more board space the other kind of funny argument is that it's almost a marketing argument that it changes the increment in which you can buy additional memory if each one of these things is a I don't know one megabit let's say it's a one megabit chip then you've got eight megabits or a megabyte of memory and if you want an additional megabyte of memory you just add another eight chips if you double the width then you can't do that in increments of eight anymore you have to do that in increments of 16 so when your customer wants to buy more memory he suddenly gotta buy two megabytes instead of one megabyte or sixteen megabytes instead of eight megabytes it makes the choice of main memory more granular than in the case where it was a narrower memory that's sometimes a problem okay that's one way make it wider but other way is to make it wider but basically put the multiplexer in a different place instead of a 32-bit wide cache fed by a 32-bit wide memory we have a 32-bit wide cache fed by a 64-bit wide memory and we assign the addresses in these memories basically treat them as two separate memories so that all of the even address is going one and all of the odd addresses go on the other and those again our words because these are 32 bits wide so if you look at the address two bits ago to specify the byte within the word in each of these banks the next bit up goes to choose which of these banks that word resides in and that's that's the sense in which these are even right addresses and then the other bits left over go to choose a particular word banks the nice thing about this especially if they're separate and independent banks is that in fact they can operate simultaneously so for cache misses it can give instructions to both memories at the same time to fetch an even word and fetch an odd word and to almost double the bandwidth into the cache for refilling the line now it's almost double because in fact the cache can't read more than 32 bits at a time in it's only a 32 bits wide memory so let's say that it takes 100 nanoseconds to read from bank 1 in the memory at the end of this time you transfer to the cache word 1 if you start bank 2 at the same time then it's going to be available if data is going to be available at the same time the cache can't read it at the same time so it's going to have to read it a cycle later a cache cycle later on to envision that is that we started the access to bank to let's say 10 nanoseconds later 10 nanoseconds is the cash cycle time we have four words to read in so we do two more like this and basically it takes 210 nanoseconds to fill the line so it's slightly more than half what it would have been if we had not done interleaving so with no interleaving of course and the original schema would have taken four hundred nanoseconds so it's not quite as good as making it twice as wide but it means that cache is only 32 bits wide instead of 64 bits wide everything we need to say there questions about that okay borrowing in a little further and I don't want to go over this in great detail but but I think you ought to understand at least in some level of detail how the memory chips themselves actually work and what people are doing to make of the chips themselves be faster most of the dynamic random access memories use a scheme that was invented back in 1974 by moss tech to supply addresses to the chips in two parts basically you take the address which is being fed into the RAM chip you divided into a upper part a row address select part and the lower part which is a column address select part and instead of giving all of those bits to the chip at one time you give half of them during one cycle and half of them during another cycle one of the main motivations for this certainly back in the old days was to reduce the number of pins that were on each of these individual ICS you've had the number of address lines so what would typically happen is that you would set up the address line feeding into the chip then you would raise a line called row address strobe that says that the row address is ready and the chip internally would memorize the row part of the address and you would change the address to be the column address of the location you wanted and raise the comma dress strobe line and then sometime later the the data would trickle out for the old original mas tech chips I think there were six bits of row address select six bits of column address select there was a data inline a data outline a row address select column address select power and ground so that's 2 4 6 18 bits 18 pins all together on the chip one of the other characteristics of these dynamic Rams is that the data is basically stored on what represents a leaky capacitor and if you leave the data in there for too long without referencing it then it degrades so one of the things you've got to do is that every once in a while where every once in a while is something like every two milliseconds you've got to read out the data and refresh it we charge the capacitors so to speak with them the charge back on them and that typically represents an overhead of about two percent of time which is inaccessible to the cache or the CPU because the RAM is somehow doing its own internal refresh various later schemes elaborated this and allowed some of the Refresh to be hidden in processes to going on inside the chip while allowing concurrent access from the CPU or from okay now these are new Rams the static Rams are typically a lot faster they don't have this refresh problem so you don't have to refresh the memory but they typically have something like 1/16 the capacity so almost all of the main memories for modern computers have built out of these and anagrams so given that main design what is it that you can do to spiff up this Ram chip to make it run faster and people have done a whole variety of different things one of them is to realize that you fetched out an entire row of bits at once in the RAM there's no reason to do that again if the bit that you're going to access next as part of that same No so a lot of the chips have page mode where you do the row address drove once to get the row it gets stored in the temporary buffer on the chip and then you do column address strobe to get the individual bits within the row in any order you like modification of that is to basically use column address strobe only as a kind of clock and every time you clock column address strobe you get the next bit of the row that kind of scheme is great for doing cash refills because you know that the data you're accessing is going to be sequential so you do the word rest robe once you do the column address strobe multiple times and you get three sequential bits for the cache line another kind of minor tweak to that is to make the raw memory look like static memory instead of using any clocks at all you just change the address anytime you want to get a different value out for a bit within that row now the kind of memory that people are designing these days are synchronous Rams which in addition to or instead of having column address select select and row address select strobes they are synchronous in the sense that they have a freerunning clock into them which allows them to be internally pipelined much the way CPUs are pipeline so you get overlap between the next row access and the data transfer of the previous row access another scheme for making memories faster is basically to put a cache inside the chip itself in a sense the row buffer is a kind of cache but you can consider initially she has chips to do this consider elaborating that row buffer with a larger cache perhaps multiple rows and you may or may not in cache and store the tag information within that chip might because each of these chips is typically a single bit or a few bits of a word you might consider externalizing the tag storage into a separate tag storage controller or memory controller kind of a way of having the cache chef inside the chip and half outside the chip that is that persistence without caches could be persistence without caches it could be for systems that have caches that just want the main memory to be faster well not the level depresses well let's say that you've got two different cache misses let's say you've got a cache miss in data memory and a cache miss an instruction memory you might be able to start filling both caches at the same time using rows that have been fetched out of main memory and are cached in the chip so you can get single cycle fills into both data cache and instruction cache from the same memory chips and still not have to go through a complete access time on the chip because the chip itself is caching both the instruction part and the data part that you're trying to reference just another example we're putting caches in series is not as silly as it seems I can still give you a performance increase just 30 seconds on a radically new Ram design Rambis has proposed effect from some set for people like Horowitz and my farm world are mostly involved in this of making RAM chips look nothing at all like RAM chips make them really look like independent computers that process a transaction and what you send to the RAM chip is an up code saying what you want to do you know read or write or something like that and operands like the address and the packet size and have it processed those things in pipeline fashion with caches on the chip and very high speed synchronous transfers in the typical Ram bus type chip Peter to 250 megahertz clock and on each edge of that clock at a 500 megahertz rate it's providing to you if it's in its cache words of data bytes of data very novel and innovative techniques for making Rams that have very high school but perhaps not as fast latency response time but very high throughput the other thing that's actually very interesting about this approach is is their business model which is almost as interesting as their their technical model they're trying to do a kind of dobby thing where they in the company I believe are not actually interested in building the chips themselves but their licensing this technology much the way Dolby licenses their noise reduction technology to other companies that actually build on the devices clever scheme it works okay questions on that stuff the last part of memories before we head next week into IO is a brief look and I don't think I want to go over it in terribly gory detail at the 21 o 64 it's memory structure which the book does go over in great gory detail but it's worth looking at because this particular CPU has almost every one of the kinds of bells and whistles that we've been talking about over the last couple of weeks for making systems like this go faster just do a quick orientation here this is the instruction fetch side this is the data fetch side so if we look at the instruction fetch side first we have an instruction TLB and this is a 12 entry fully associative cache in parallel with the lookup in the instruction TLB using the higher bits of the address there's a cache which uses the lower bits of the address notice the lower 13 bits of the address which are they offset into 8k page going down into the instruction cache and this is a 8k by direct mapped cache and remember that because it's direct mapped and because they don't want to use any of they don't mix the the virtual page number bits and the page offset bits it can't be any bigger than 8k so an 8k by direct map instruction cache on the data side similar kind of organization a translation lookaside buffer which in this case is 32 elements also also fully associative notice that the lookaside buffer for the data side is significantly larger than a look aside buffer for the instruction side because the data side tends to exhibit much less locality and again an 8k bite direct map cache with right through policy so because they're doing right through which means there's gonna be a high level of traffic out to the next level of the hierarchy there's a right buffer here so things which have to get dumped out of the data cache gets stacked up in the right buffer waiting to go to the next level of cache and the next level of cache these cameramen are doing a great job of sort of following my finger around here the next level of cache is a unified cache the level 2 cache external to the chip which is again a direct map cache in this case it's a write back cache and the size varies depending on how many chips you want to put out there you know could be half a megabyte could be 8 megabytes notice that they done a victim buffer so that things which are getting thrown out of level 2 cache gets saved in the victim buffer I think this is only one entry and we talked about why that made sense in particular for a direct map cache to avoid pathological cases where alternating references would cause a Miss for every reference so they use that trick they use the right buffer trick they use the delayed right buffer trick in the data cache remember we talked about the problem with writing to caches that you've got to do the tag check first before you do the right because you might be writing the wrong data and one way to avoid that is to delay actually doing the write until a subsequent write cycle later on and that allows you to do one cycle data cache hits so in fact they have laid write buffer let's see what else what other of our tricks that we talked about they have well they have an instruction prefetch buffer remember we talked about how one way to make an instruction cache go faster especially a cache that's often being used for sequential memory accesses like an instruction cache would be is to when a cache miss occurs fill up the cache but then use your idle cycles to prefetch the next data which might cause a Miss in the cache so they have that as well the book goes through this in great glory detail I in the next couple of slide tried to simplify the great gory detail just a little bit and I don't think I'll go through it here but you can and I think it's fairly self-explanatory all the steps are numbered to correspond as in the books explanation to the numbered circles on the diagram and you can spend a pleasant half-hour pointing multiple fingers around the diagram following the way that the data goes the bottom line in all of this is how does it perform and this is a summary of a figure which is in the book that describes how well it does this shows what the total number of stalls in the CPU are for the level 1 instruction cache remember which is 8k the level 1 data cache which is 8k and the level 2 cache which is off-chip but in this case was 2 megabytes those are some to be the total cache penalty and CPI and then there are the other penalties for other stalls in the pipeline and then a total penalty well if you look at the speck in the integer benchmarks things are looking pretty good the star penalty in the instruction cache is about just over a tenth of the cycle two-tenths of a cycle and data almost nothing in the second level cache so the total cache penalty as a fraction of the total execution time is only about 1/5 so that's doing pretty well in case of the floating-point benchmark the numbers are a little bit different the the instruction cache miss rate or soul penalty is a lot lower presumably because there are a lot more tight loops the data cache miss is higher presumably because it's accessing big arrays that don't get cached as well again the level 2 penalty is almost negligible but it's still overall performs quite well in the sense that the total penalty for the cache system is only a fifth or so of the total execution time however one could cynically say that a chip like this was designed to make the spec benchmarks run well and if you run something which is not a spec benchmark like a commercial benchmark which has a lot of transaction processing and sorting then things don't quite look as good and in fact in this case there's about a half cycle each or just under that for both instruction and data cache misses and now the total cache miss is getting up to 40 percent or so of the total execution time of the CPU so maybe the next set of benchmarks ought to include something that looks like this so that the next set of processors that come out and 5 years from now will try to reduce these numbers as well we're out of time we're out of memory next time we talk about Io "
ZerbMpQODeI,27,"Computer Architecture
About this course: In this course, you will learn to design the computer architecture of complex modern microprocessors.

Subscribe at: https://www.coursera.org
https://www.coursera.org/learn/comparch",2017-12-05T14:24:12Z,Computer Architecture - Course Overview,https://i.ytimg.com/vi/ZerbMpQODeI/hqdefault.jpg,intrigano,PT4M35S,false,3254,24,1,0,1,welcome so in this course we're gonna be learning about computer architecture and this course is an adaptation of a course which I teach at Princeton University called ele or for electrical engineering for 75 and I'm David when Slav I'm a professor here at Princeton and my background is mostly looking at how to build many core and multi-core microprocessors and in the past I've actually built two of the world's fastest many core processors in industry and before that I've worked in academia building many core microprocessors so about 15 years of processor design experience in today's lecture we're going to be talking about introduction or some opening to what is computer architecture why do you want to learn a computer architecture how it's different than previous courses that you might have taken so something like a computer organization class or a logic design course and then we'll talk about some content today which is looking at instruction set architectures and how a instruction set architecture or big a architecture is different than implementation or micro architecture and why it's a good idea to split out these two ideas so let's take a step back and look at the course administration of this class and how this class is going to be organized so as I said I'm David when sloth this class is roughly going to be the equivalent of 280 minute lectures a week so this is the same format that's used at Princeton University for this course and it's we're gonna try to segment into shorter segments to sort of give you bite-sized nuggets with questions and answers intermixed to check your understanding as the as the video goes on two books that I wanted to talk about this is the computer architecture a quantitative approach by John Hennessy and David Patterson this is a very very good book if you are going to be doing computer architecture I highly recommend it or and it's a heavily heavily heavily suggested book for this course I do want to point out that there's a lot of different versions of this book floating around we're gonna be using the fifth edition and you should go get the fifth edition the fifth edition is very different than the fourth edition it's updated as of 2012 so it's very fresh and then a auxilary text which is useful for a portion of this class I'll mention it as the chenla pasti book because the two authors modern processor design and fundamentals of superscalar processors the reason we're going to use this book or a reason that we're going to I recommend some readings out of this book is that it has a lot deeper coverage of superscalar processors than the computer architecture book so this this is a great book to be to begin with but it doesn't cover how to build superscalar processors in great depth this book goes on to do that and that's something we're going to be talking a lot about in this course a lot of the content of computer organization or a traditional computer organization class I'm gonna repeat in the first three lectures of this course or the first three and a half lectures of this course and the reason for this is because I teach everything from first principles and once you get everyone on the same page but it's we're gonna go very very fast through that material so if you've not had a computer organization class it may be possible to take this class but I highly recommend taking a computer organization class before this class because this is the second class in sort of a computer architecture series where you'd have computer organization and computer architecture and we really do rely on the prereqs but if you are watching the first three lectures you say I know all this yes that is correct you should know all this if you don't know all this then you should probably go back and retake computer organization or take a computer organization class but don't drop the class if you take the first three lectures and say oh I know all this and just just leave at that point because we're just gonna breeze through that content very fast as building from first principles you 
psJFMhmChK4,27,"Practice GATE Questions from the subject Computer Organization & Architecture (GATE CSE) Exam. In this lecture, Prof. Anil Prasad will cover Design of Ram and Rom, with GATE MCQs. Use Code â€œPRASAD10â€ to get 10% off on your Ekeeda GATE Subscription. 

Welcome to Ekeeda GATE, your one-stop solution for GATE preparation. 
We will cover the entire syllabus, strategy, updates, and notifications which will help you to crack the GATE exams.
During the live session, our educators will be sharing a lot of tips and tricks to crack the exam. GATE aspirants who are preparing for their exam will be benefited from this channel. 

ðŸ‘‰ Past live doubt lectures: https://www.youtube.com/playlist?list=PLm_MSClsnwm_glYmBNVsz1f5tdr69_NlU

âœ¤ For More Such Classes Get Subscription Advantage:
ðŸ‘‰ GATE CSE: https://ekeeda.com/gate/gate-computer-science-and-information-technology/computer-organisation-and-architecture
âž¤ Use Referral Code: ""PRASAD10"" To Get 10% Discount on Ekeeda Subscription

âœ¤ Download the Ekeeda - Learning App for Engineering Courses App here:
âž¤ Android: https://play.google.com/store/apps/details?id=student.ekeeda.com.ekeeda_student&hl=en_IN&gl=US
âž¤ iOS: https://apps.apple.com/in/app/ekeeda/id1442131224

Ekeeda Subscription Benefits: -
1. Learn from Top Faculties
2. Live Doubt Lectures
3. Structured Online Courses
4. Learn from Top Faculty

âž¤ Subscribe to Ekeeda Channel here: https://www.youtube.com/channel/UCltVEZ6GecWntoZ19FvaWhQ

âž¤ Subscribe to Ekeeda GATE & ESE Channel here: https://www.youtube.com/channel/UCQoO0SZIdkw2zkXe9wwWkew",2020-11-21T03:04:40Z,"Computer Organization & Architecture (GATE CSE) - Design of Ram and Rom - 20 Nov, 7:30 PM",https://i.ytimg.com/vi/psJFMhmChK4/hqdefault.jpg,Ekeeda,PT1H2M16S,false,272,5,0,0,0,[Music] [Music] [Music] [Music] good evening guys am i edible am i edible is it visible is the screen visible i want you people to you know participate so that i will be able to go ahead and how many are here just to say higher hello here so that will go ahead and continue i'm just inviting for your reply so that i will continue because i lost the track of previously what happened in authorized it is i think we've been discussing about different types of instructions i guess yeah um just um i want the people to respond only a few people are actually watching so that i'd be able to go ahead fine yes okay yeah anyhow i'm just gonna start a bigger pattern my my video is not visible there's a small technical which i'll be able to rectify by the next week but before that we just gonna finish what we have you know passed in the previous week just let us see uh if i'm not wrong i think let me just yeah in fact we people are discussing about the memory reference instructions right we are in fact you know going through the memory reference instructions we discussed in and add load store bsa and b union right um in order to just explain you this anyhow i uh covered it in the previous week i will just have a walk through this you know memory reference instructions for this let me open my ppt you yeah now let us discuss about various instructions in terms of the location of the outlet when it comes to the location of the operand there are three different kinds of instruction formats one is memory reference instructions second one is register reference instructions and the third one is io instructions what is memory reference and what is register reference and what is io instruction okay here when it comes to the memory reference instructions when it comes to the memory reference instructions um all these instructions operands will be available within the main okay there are several instructions and stands for end operation adds transfer add operation load stands for load operation store stands for store operation buying is branch unconditionally which is equal to a jump instruction in the high-level language program bss transfer branch and save the written address branch and save the written items hi lokesh bharara and you're here and nobody else fine lokish pura i do have a doubt that is where did i stop exactly in the previous week because last week we had a break for diwali and that is so uh i lost the track of what happened previous week so that so that i will be able to go ahead okay from now from then what i stopped where did i exactly stop you remember first of all am i audible yeah i was explaining and who else is available right now who else is here in the class what is the girl's name the location of the opera not a type of of the type of operand i am explaining that the location of the opera if i am not wrong i was discussing about memory reference instructions register reference instructions and io instructions yeah there are four people right now and there is one more guy there is one more girl who is regular with lokish what is it girl's name yeah fine anyhow i'm just gonna go ahead with um these instructions here i was explaining about and you just uh yeah kirti i don't think keep these available right now if she is there and should have said hi or evening find anyhow i'm just gonna go ahead with my regular flow of explanation yeah um here we do have as you can see on the screen there are seven memory reference instructions there are seven memory reference instructions i'm going to use my pen here and add a lower store bun bse and ise okay we discussed already about end add load store b and bse also and is end instruction i desired instruction load is load okay you know that load when i say load 450 from the memory location 450 uh the content will get transferred accumulator right when i say store from the accumulator to memory memory location 450 the content will get transferred right yeah when it comes to buen buen is branch unconditionally which is equal to a jump statement in the high level language okay and bsa is branch and save the written address branching save the return address and which is equal to a call instruction which is equal to a calling for a call to the subroutine yeah am i edible am i edible right now a call instruction to a subroutine okay in fact i was explaining how when a subroutine is called how the program counter value will get saved this is what exactly i was explaining you guys okay i'm gonna close this yeah now is my voice is audible and clear good there is one new guy who's called mr light fine just better you rename yourself with your proper name so that i'll be able to address you guys okay so here in the last week i left exactly at this point when i perform a call instruction how to save the program counter value program counter will get saved in stack program counter will get saved in stack that is it i i finished with this then i as we will talk about iss instruction okay yeah let me explain you guys is z what is isa is it stands for increment and skip is zero increment and skip increment and skip if zero what is this increment and skip is zero business how it is going to work let us see okay again we are going to draw the block diagram of our main memory this is our good old friend all the time it just helps us to understand the architecture better and better every time if you could remember i've been drawing this memory continuously for giving a complete insight view of the computer architecture because without main memory there is nothing possible because everything is loaded since everything is loaded and stored in the main memory we do require the memory to be drawn all the time for every now and then okay let us say that this is the current instruction that is being executed okay that is um this is the 23rd instruction okay and the program counter is 24. okay the 23rd instruction is let us say i ass increment and skip of 0 240 okay what is is increment and skip 0 this instruction stands for we're going to increment and skip if zero what there is an address you know that this is the instruction in which there are two parts one is the up code and address of the operand you people knew already vouchers this is opcode and this is the address part of the offer now what to be incremented first of all when we execute isd instruction we will go to the respective location of the operator how do you know what is the address of the operating address of the operand is specified within the instruction itself okay so in this case we go to the address which is specified in the instruction go to 240 in which there is an operand okay the operand value is equal to 50 okay now we will get 50 into the cpu we will get 50 into the cpu we will send this 50 into a l in alu we are going to increment 50 okay so we will perform increment operation what is incrementing the content will get increment then 50 will become 51. okay did i make it myself clear that is we're going to increment the operand which is available in the address that is present in the instruction so we will go to the operands address and we will collect the operand we will increment the operand within the cpu of course every time when we perform an arithmetic operation that arithmetic operation will have to be performed only within the cpu okay now after performing we are come we are going to compare this operand with 0 if the operand is 0 then we are going to skip the next instruction the 24th instruction will get skipped once again increment skip 0 increment skip with 0 stands for the address part of the instruction will give you an address of an operand going to the address of the operand we're going to collect this operand after collecting this operand we will perform incrementing operation after incrementing if the operand turns out to be a 0 then we are going to skip the next instruction 24th instruction will not get executed rather 25 will get executed did you understand mr lokesh did you understand this that is incremented skip skip zero is we are going to increment the operand after incrementing with the operand is equal to zero then the next instruction will get skipped otherwise next instruction view won't get skipped in our case operand value did not become 0 because 50 plus 1 is equal to 51 but not so right the next instruction will not get skipped okay this is all about i assign did i make it myself clear good now these are all these are all what we call the memory reference instructions there are a total of seven instructions if you remember our wrap code is equal to our upcode is of three digits total instruction total instruction is of 16 digits turtle instruction is of 16 visits in which the first digit is more you remember location and the next this is one next three digits are opcode and then the least significant 12 digits are nothing but the address of the operand you remember mr lokesh if you say yes then i will go to the next step this is our instruction format this is our instruction no no my question is you remember this instruction we in fact we're designing a processor in our processor we designed and we designed the num the few registers in which there is an instruction register the size of the instruction register is equal to 16 the way the instruction register has been divided is three into three fields the first field is a mode field second field is an upward field and the third field is an address part of the operand right now the op code is three digits opcode is three digits by using three digits i can have two power three combinations which are equal to ah 8 right did you get it now 8 meaning from triple 0 to triple 1 yes or no now if you could remember if you could stay focused the total memory reference instructions instructions are equal to just seven which means that from triple zero to double one zero from triple zero to double one zero are given to memory reference instructions okay but triple one combination a combination of triple one hasn't been used for memory reference instruction it is not a memory reference instruction then what it is if triple one is not given to the memory reference instruction then what it is then triple one could be either register reference or io instruction right i explained you guys in the preview in my previous lecture so how to differentiate in between register reference and the io instruction definitely we will use the mode to design if it is not a memory reference instruction then there won't be any address there won't be any addressing mode okay these two fields will be free we can use these two fields for different purposes once again if it is not a memory reference instruction we don't require an address because only for memory reference instruction we don't require a memory address because the operands will be available in the main memory if you want to access the main memory we'd require a memory address okay if there is an address there will be corresponding addressing more if there is no address there won't be any addressing so what we do here is we are going to use this mode field to differentiate in between register reference and the io instruction when the op code is equal to triple one then it will be either a register reference or io instruction but what it is just look at the move if the more digit is equal to a zero followed by triple bin then it will be a register reference instruction if the mode it is equal to one followed by triple 1 then it is an io instruction you got this point okay of course you you definitely you do okay this is as simple as this now let us understand all these register reference and io instructions i use a ppd for that okay yeah as you have seen this and add load store bun bsa ise next this classification is register reference instruction okay this part next to this will will you know represent the hexadecimal code of the instruction you know that 0 triple 1 is for register reference instruction right did you understand location what is this combination zero triple bond of course zero triple one will tell you that the instruction is a register reference instruction you're getting it i told you that in order to differentiate in between registered reference instruction the io instruction we will use the first two digit if the first digit is equal to a zero followed by the op code triple m then this instruction is nothing but a register reference instruction okay now this is only tell you this is going to tell you only that this is a registered reference instruction but where is the instruction in order to design the instruction what we do is we will use the least significant 12 digits if you remember earlier we use these 12 digits we use these 12 digits for address in the memory reference instruction in the memory reference instruction least significant these 12 digits will represent did represent a memory address but now we don't need a memory address because it is not a memory reference instruction if we don't need an address all these bits will be free so that we can use this for some other purpose what we'll do is anyhow 0 triple 1 is only giving us the the type of the instruction rather you know format of the instruction that this is that this does belong to register reference instruction will use this for designing the instruction itself the op code itself so what i'll do is if you can understand this pattern for every instruction i'm using a unique code there are 12 digits okay i am flipping each digit for each and every instruction for the first instruction i use one in the first position for the second instruction i'll use one in the second position third position did you understand this okay for an example the first instruction is cle what is cla i'll explain you about this instruction first instruction is cla 0 triple 1 is common now this i need to give an op code for this op code in order to design the upgrade what i'll do is i will use i will i will clip one digit each for every instruction for every instruction okay for cla in the 12 digits i will put one in the you know most significant uh most significant digit i put one here for cle i'll put one in the second position for cma i'll put one in the third position is it making some sense okay so likewise i am going to design all these instructions so each of this combination will represent one of the instructions for an example one triple zero double zero double zero double zero double zero preceded by zero triple one will represent c l a okay for this hexadecimal combination will be seven eight double zero zero triple one is equal to seven one triple zero is equal to eight zero zero zero zero is equal to zero and double zero double zero is equal to zero okay now c l a is zero triple one zero one double zero zero likewise so each and every instruction will have its unique combination seven is common because the first two four digits will not change for any instruction for register reference okay in the hexa01 is equal to 7 okay so now let us discuss only let us discuss only the significance about the significance of the instructions the purpose of each and every instruction c l s times for clear the accumulator what do you mean by cla see when when an instruction clea gets executed the contents of the accumulator will get clear the contents of the accumulator will get clear regardless of the contents prior to this instructional execution the contents will be flushed off to zero when you execute cla okay what is really there is there is an additional digit to accumulator which is called extended accumulator you came to know about it in one of my previous sessions the accumulator size is 16 digits remember the size of the accumulator register is 16 digits right size of the accumulator digital size of the cumulator is 16 digits but when we perform any addition or subtraction or multiplication we can have a guarantee that the contents of the accumulator will still be equal to or less than 16 diseases there may be a 17 digit yes or no so at that time in order to accommodate the carry we will have an additional digit which is called an extended accumulator we we employed an extended accumulator in order to accommodate the carry in order to accommodate the carry let's do it in order to accommodate the carry okay so when you execute cle instruction good when you execute when you execute cle instruction what will happen contents of the extended accumulator will get clear regardless of the contents before the execution of cle the contents of accelerated accumulator will get flushed out will be equal to zero after executing this cma what is cme cma stands for complementing accumulator what is complementing tell me location answer this what do you mean by complementing the cumulative that's my question what do you mean by complementing all right ones to zero zeroes to one's right complementing is just nothing but the ones will be turned out to be a zero zeros will be turned out to be once that is what allah complimented yes or no you know complementing you know once complemented two's complement not available zero become one and vice versa yes it absolutely okay so ah complement accumulator when when instruction cma gets executed contents of the accumulator will get complemented when cme gets executed contents of the extended accumulator will get complemented okay that's it that's it okay c-i-r-c-i-l what is here c-i-r stands for circular shift right c i l stands for circle of shift left okay what is circular shift you know what is circle shift circular shift mean the contents of the accumulator will get rotated okay you know shifting do you know or you don't for an example i will write it here there is the contents will be let us say 0 1 0 1 okay let us say it's my cumulative is the four just four resets what will happen after rotating right rotating right rotating meaning each and every digit will occupy the next right most sorry rather you know the right subsequent bit position this 0 will come here this one will come here this 0 0 will come here this one will you know get rotated and will occupy the first position so after rotate after right rotating the contents will get transformed into 1 0 1 0 0 1 0 1 will get transformed into 1 0 1 0 because each and every digit will occupy the next right subsequent bit position okay this is what all about circular shift right okay and then circular shift left circular shifts left transfer just in contrast to cir here in the previous case we are shifting right now in this case we are going to shift left so what will happen is this one will come here this zero will come here this one will come here this zero will occupy the least significant bit position okay which means that zero one zero one will become one zero one zero anyhow okay so that is all about circular shift right and circular shift length okay and another thing is increment what is incrementing incrementing is simply incrementing but what do we increment here for everything there is only one operand here for all the register reference instruction the default operand will always be the accumulator either circ rcla cle cme cme for everything only the operand one and only the operand on which the operation will get performed is accumulated simple i hope you got my point for every register reference instructions needless to say the obvious operand will always be accumulated got me yeah also incrementing meaning the contents of accumulator will get incremented after executing this instruction spa okay what is spa as you as you can see here skip the next instruction if accumulator is positive i will demonstrate this instruction by taking an example okay let me draw again main memory let us say the current instruction is 46th instruction the 46 instruction is um what is that um sba this instruction is sp skip the accumulator sorry skip the next instruction if the accumulator is positive okay spa skip the next instruction if the accumulator is positive for that what we need to see is we will check the contents of the accumulator if the contents of the accumulator positive is positive then the next instruction will get skipped okay rather the next to the next subsequent instruction will get executed we are going to skip the next instruction 47 minute skip rather 48 will get executed this is the significance of spa but stay focused okay for that there is something more to add to this point you know accumulator register accumulator is a 16 bit register in which the most significant is it that is the 15th digit 0 to 15. the most significant digit is a what do you call a sign digit if it is zero it is positive rather if it is one it is a negative all right in order to identify the contents of the accumulator either those are positive or negative we need to check the most significant digit checking one digit in the accumulator will always be very difficult rather what we do is rather than coming to the accumulator and checking it what we do is we will directly check the flag register if you remember there is a flag registered in the system which shows the status of the accumulator i explained you already okay the flag register will show the status of the program what is the status of the program every time when an operation is performed due to the execution of an instruction flag register will get updated i explained you about several flags okay including the important flags such as z s c v there is a z flag there is a sign flag there is a c flag there is a b flat let us say this is my l this is my area now one instruction is getting executed during the execution of the instruction um the alu is going to perform one arithmetic or logical operation okay when a alu performs an operation finally it has to uh you know send the output the output of the alu will anyhow will get transferred to the accumulator many times we learned about this okay in any architecture the output of the alu will get transferred directly to the accumulator okay at the same time the output of the alu will also update this flag register okay now if the output of the alu is zero z flag zero flag will be equal to one if the output of the alu is negative then sine flag will be even otherwise if it is positive it will be equal to zero if the output of the alu has a carry then carry flag will be equal to one if the output of the alu has an overflow then overflow flag will be equal to one the overflow what do you mean by overflow overflow meaning accidentally sign is getting reversed okay sometimes what will happen is sign will unnecessarily get reversed okay uh during the execution of some instruction for an example when you add two positive numbers the result must be a positive number there is no way that we can get a negative result when we add two positive numbers for an example i am adding zero triple by zero triple b which means that we are adding plus seven to plus seven let us say the size of the register is just 4 digits okay but as the most significant digits are equal to 1 there will be a carry that will be produced and it will occupy the signed digit so at that time sine will get reversed you understanding location this is very simple of course plus 7 plus 7 should be equal to plus 14 but here in our case plus 7 plus 7 as the register size is limited is actually reversing the sign is it did you get it but actually it has to be a carry that should go to the carriage but rather it is reversing the sign doesn't did you understand location yeah good now whenever overflow digit is equal to 1 it shows an important status of the output of the instruction from the l okay the alu computed and due to the limitation of the size of the register the actual sign has been reversed okay so whenever overflow digit is equal to 1 the sign has to be reversed did you get it the sign has been reversed so what we do is when our overflow digit is equal to one this see this whenever our product is equal to one that means that there is a carry but due to the size of the limitation of the size it is occupying most significant is it so whenever there is overflow this c must be equal to 1 this sign must be reversed here this is how we are going to deal with sine reversal i am not going to talk much about this because this is the simplest phenomenon and i have already explained you in one of the previous sessions okay fine anyhow that is the difference between carry and overflow carries the phenomena that is whenever two equivalent numbers are getting added and the result is you know an additional digit then we are in a situation called carry but whenever a sign digit has been reversed unnecessarily due to the limitation of the size of the register then we are in a situation called overflow whenever overflow is equal to one then what we do is we will reverse the sign digit and set the carry design this is how we are going to deal with the sign reversal okay fine so what is spa skip the next instruction if the accumulator is positive skip the next instruction if the accumulator is positive okay we will go and observe sign digit here if the sign digit is equal to 1 means the accumulator is negative okay if the signed digit is equal to 0 then the accumulator is positive if the sign digit is equal to zero we are going to skip the next instruction rather we are going to skip we are going to execute the 40th instruction otherwise we will execute an extra instruction similarly sna if the accumulator is negative next instruction will be skipped otherwise next instruction won't get skipped similarly szda skip if accumulator is 0 we will observe the 0 flank if the 0 flag is set to 1 that means that the contents of the accumulator are equal to 0 so what we do is we're going to skip the next instruction otherwise we are not going to skip the next instruction okay and as that e finally we have one more instruction called sdd what is sd okay sd sd stands for skip the extended accumulator is equal to zero skip the next instruction if the extended accumulator is equal to zero okay so what is extended accumulator you know that the additional digit additional digit and carry both are equal to one we're going to observe carry digit whenever carry digit is equal to 1 then we don't skip the next instruction but whenever carried is it is equal to a 0 that means that extended accumulator is equal to 0 then we will skip the next instruction did you understand this why carry and extended accumulator both are equal of course obviously what is external cumulative extended accumulator requires to be filled whenever there is carry only rate yes or no so obviously extended accumulator and carry both are equal to 1 okay so the next instruction is hard okay write it down somewhere i am going to explain you some important thing about halt instruction okay what is hard instruction yeah let us say this is one instruction the address of this instruction is 21. okay yes sir if this instruction is getting executed what will be the program counter value location what will be the program counter value array current instruction is 21 cp is executing 21st instruction this is current instruction yeah right current instruction is 21. for the current if the current instruction is 21 for 21 what will be the program counter value yes this is 22. program counter value is equal to 22. yeah this is always correct when twenty first instruction is getting executed the program counter value will be equal to 22. when 20 second instruction is getting executed the program counter value will be equal to 23 and so on that's it this is applicable for all the other instructions accept halt instruction okay accept halt instruction for any other instruction when those are executed the program counter value will be the address of the next instruction obviously but let us see in 21 there is a halt instruction okay if twenty first instruction is a halt instruction then the program counter value will not be 22 rather program counter will also be equal to 21 that's it okay for any other instruction when that is executed the program counter value is the address part of the next instruction is the address of the next instruction accept halt instruction only for halt instruction the current instruction address and both program counters value are exactly equal okay program counter value will not be updated for halt instruction if you digest this we will enter into the details why for halt instruction program counter value will not get updated if you could understand this i will i will get into the details we will see why it is okay right but but it is thumbnail for every other instruction program control value is the address part of the next instruction except hard instruction it is applicable to every other instruction but not for halt instruction got me if you can understand this i am going to give you the complete detail about this hard instruction i am going to take you to uh the extent that you can understand why the health and swipe of the heart instruction program counter value will never get updated you there am i audible i think for me he's been struck yeah you're right but it is used to switch the interrupt service fine but but you do not you know think too much uh using interrupts to understand this concept will be somewhat complex rather i'll simplify it yeah good now stay focused here um let us see i do have memory okay this is my main i will draw one more i will give you a overview of memory from the perspective of programs in the main memory there are n number of programs okay these are programs these are not instructions now i am giving you the view of the main memory in terms of the programs this is programmable this is program 2 this is program 3 program 4 and finally we do have something called a program and here now um now the system has been started like let us say the computer system has been switched on okay and the main memory has been loaded with the number of programs that are needed to be executed currently now could you please tell me let us say i will give you also another example that cpu decided to execute program 4 right now okay program 4 is being executed okay let us say program 4 the address range is from 300 to 400 program force address ranges from 300 to 400 any instruction in between 300 to 400 falls under programs 4 okay if you want to execute program 4 you need to execute all the instruction right from 300 to 400 inclusive okay now i do have a question forget about halt instruction when three zero one is getting executed obviously the program counter value is three zero two right when three zero two is getting executed obviously the program counter value will be equal to three zero three and hence uh so on yes i know but now i do have a question answer me very carefully after executing programming for which program will be executed by the cpu answer carefully after executing program 4 which program will get executed by the cpu i am waiting after my question is very simple after executing program for which program will get executed simply you fall in the trap that i have created for you that i have ditched for you there is no way we can't tell which program is going to get executed the instructions that are available within the program will get executed continuously one by one contiguously but the programs will not get executed continuously in the main memory this is the fundamental rule that you need to understand there is a scheduler let me take the help of the operating system in this case there has been a scheduler there is a schedule scheduler will decide there are several factors will be considered by the schedule maybe a shortest job first or longest job in like like there are several scheduling criterias okay let us say i am using the shortage java for scheduler shortest drop shipping first eduard is going to pick up the shortest job among the jobs that are available in the main memory after executing the current uh program if you get it there is a scheduler that is going to decide which program is going to get executed next okay and there is no way i can tell you that after this program next program will definitely be executed there is no rule and i don't know neither you nor me knew about the next program that is going to get executed after the current program gets executed this is a thumb rule there is a should unit the scheduler will decide the fate of the program that is going to be executed next by using a scheduling criteria it could be shortest job first where the shortest remaining time first our longest job the longest remaining time first our round robin or whatever okay so here let us say the current instruction is 400 which is the final instruction final instruction of the current program did you get it so what generally had been done in the previous days of computing was the programs used to get separated by using a halt instruction okay after and after finishing the current program there has been there had been an instruction such as halt used in order to separate this program from the next program so let us say i reached the far end of this program that is the 400th instruction meaning in 399 itself the program has been finished the final instruction has been available in 399 instruction so obviously the final instruction will be hot what is called halt meaning the system will be halted till the scheduler decides the next program to execute that is the reason why when halt instruction gets executed program counter will not get updated rather program counter will repeatedly be updated to the current instructions address soon okay i'll tell you once again okay now there is a program now currently p4 is being executed p4 is in between 200th instruction to 300 instruction okay for every instruction the program counter value will get updated but the final instruction three zero zero program counter value should not be updated to three zero one right did you get it if the program control value gets updated to three zero one we are violating one important rule such that the program counter value should not be updated to the next program settings the program counter is the current program counters current programs counter program counter cannot count the first instruction of the next program so did you understand program counter value should be only for the current program the program counter should not be updated to the next program at all so what we do is the final instruction will be written is just nothing but a halt instruction okay when hard instruction gets gets gets executed program counter will be repeatedly updated to 300 what is called halt will be realized by using one instruction called branch unconditionally to the same location bu n 300 okay what is branch instruction whenever what is branch branch unconditionally meaning we'll jump to 300. during during the branch instruction we will branch to the address part of the instruction that is here halt instruction will be realized by implementing a jump instruction to the same location that's it okay you might also have a doubt that always program counter needed to be executed needed to be updated then how hard instruction can start updating the program counter to the next instruction rather than to through to the current instruction the answer is simply this one that is halt instruction will be implemented by using a jump to this current instructions address what is busine jump to the address 300 what is 300 current instructions address again it gets executed buen 300 again it jumps to the same instruction i hope you got it did you or not okay so this is all about heart instruction for halt instruction program counter value will never get updated this is the thing did you get it located yeah no no hard meaning halt instruction there is no such instruction called halt instruction halt is a status of the computer system in order to implement in order to you know present a halting situation what we do is we will we will perform a jump repeatedly to the same instruction so computer system is said to be in the hard state right there is no instruction such as hard instruction there is a state such as hard state okay so how to have the computer system halted i should start streaming the instructions i should start the stream of the instructions from execution how can i stop it there has to be a point at which the computer system has to be halted that point is an instruction that instruction for that i do use a jump instruction jump to the same location again and again and again and again again when it happens the control will not uh go to the next location if the control goes to the next location then we will come out of the hard state next sequence of the instructions will get started next sequence of the instruction will start its execution when we want to halt we will use jump to the same location uh phenomena i hope you've got it okay so this is all about halt instruction so you remember this so in most of the the you know previous questions there had been a mention of the halt instruction most of the guys went wrong this is an important phenomena most of the people they didn't know even the computer architecture may the major compute major people who teach us computer architecture computer architecture don't know about this okay so uh considered to be you guys are very lucky to understand this because you don't know from this source you are going to under you're going to have that important details of a subject a topic so write it down this and better you you know explain it to some guy who is absent to my class so that you're going to enhance your computer architecture skills okay that's it that is hard instruction after which i'm going to take you to the next set of instruction those are the io instructions io instructions first to four bits are equal to one one one one because triple one is an up code combination for both register reference and io instructions in order to differentiate we use the faster digit if the first visit is equal to a 0 then it was a registered reference instruction but now it is an io instruction you got my points so now one triple one followed by i will use you know the least significant twelve digits to compose instruction one triple one one zero zero zero 0 0 0 0 0 0 0 0 it is the first instruction which is nothing but i n b if you convert this to hexa you will get f a double 0 1 1 1 1 1 is f 1 triple 0 is 8 0 0 0 is zero zero zero zero zero is zero next recombination is out that is f one triple will will not change rather zero one double zero zero zero zero zero zero zero zero zero likewise every instruction will flip one digit each in the least significant 12 digit to get composed okay so the first instruction is inp inp when inb gets executed a character will be accepted from the input device okay a character will be transferred from input device to the accumulator when you press the character when you press the character the character will get transferred to the input register okay but where that character should be transferred the character should be transferred to accumulator actually you know that accumulator is the main register i will talk more about this finally the character has to be transmitted to the cumulative when you press the key immediately the character will get transferred to the input register but input register is not a final destination so what we do is we are going to execute an instruction called inp when inp gets executed then a character will get transferred from input register to the cumulative out when out gets executed a character will get transferred from output sorry out accumulator to output register from accumulator to output register okay ski there are two flags in the computer system one is input fly one more is output flag more about these flags i will explain you i'll teach you in interrupts but at this stage you understand that there are two flags input flag and output fly input flag will be raised whenever input device trying to communicate with the computer system output flags will be raised when our output device is trying to communicate with the computer system so when these one of these instruction gets executed corresponding flag will get skipped if sk gets executed input flag no matter what input flag the the the value of the input flag whatever it is computer ignores sko value of the output flag whatever it is the computed system simply ignores ion when this gets executed the interrupt facility for the program will be switched on when iof gets executed in any program interrupt facility for that respective program will get switched on okay so so this is all about the various different instructions which are classified into three different categories the first one is memory reference second one is the register reference third one is i1 so next week we will take up another important topic till then take a long seven day break okay good night same day same time we're gonna make next week you 
th2wcy0zJ-o,27,"computer organisation
you would learn pipelining processing",2017-12-05T15:39:57Z,pipelining processing in computer organization |COA,https://i.ytimg.com/vi/th2wcy0zJ-o/hqdefault.jpg,Education 4u,PT17M39S,false,201627,2428,110,0,75,hi students the next topic is a pipeline processing so what is this pipeline any computing a pipeline is nothing but it is a set of data processing element that is nothing but a set of data processing data processing elements which are connected in series which are connected in which are connected in series so here where the output of one element output of one element is the input of the next element so it's nothing but as a set of data processing elements which are connected in series the output of one instruction is the input of the next instruction so this is a pipelining processor so before going to know about the pipelining process let us how the program is executing in the form of pipelining let us see so a program will be present in memory so we have to fetch that program from memory so whatever the data that is present in the memory so whatever the instruction that we want to be execute that will be fetched from memory so here the program can execute it in five phases so whatever the instruction that is present in the program will be executed in five phases so instruction gap from memory and executed in five phases so this phases or instruction fetch instruction decode opcode fetch opcode execute and a branch store okay so these are the different five phases that don't take place when an instruction is executing in the processor so first first we have to fetch the instruction from memory fetch the instruction from memory after fetching the instruction you have to decode that instruction so whatever the instruction that you fetch it from memory you have to decode that instruction so after decoding the instruction you have to fetch the operand so whatever the operand that the instruction means the data is present that operand you have to be fetched after fetching the operand you have to execute that operand so after execution finally we have to store thee or brand store operand so these are the five phases that are take a taking place while a program or instruction is executing in the processor so now coming to the pipelining how this instruction is executing in the form of pipelining in a program there are so many instructions so one one by one the instruction is executing in the processor so each instruction will be executing in the form of pipelining so before going to know about the pipe learning process let us see the difference between the pipelining and non pipelining so let us assume whatever the instruction that is fetch it from memory will be executing in the form of non pipelining process non pipelining processing so how the instruction is executing in the form of non pipelining process so first instruction fetch after that instruction decode next operand fetch open execute then operand stood so one instruction is executed and stored in the memory so this is a instruction so after the completion of the one cycle it goes to the another instruction the again it takes then another instruction from the program it fetched that instruction from memory instruction decode Oprah and fetch operand execute and finally Oh prime stone okay after that I plus two so like the like well it is exert taking the instructions each instruction from program and executing one by one after the completion of one instruction it taking the another instruction after the completion of another instruction it is taking the next instruction so this is a non pipelining process so here the wastage of timing is more because eat after the completion of one instruction only you have to go to the another instruction so see the pipelining process so how the instruction is executing in the pipelining now coming to thee pipeline processing so how the instruction is executing with the help of pipeline processing so in the first I instruction facet fetch the instruction instruction Decor our plan fetch a brand execute or prankster so in non pipelining after executing storing the value of the first instruction it takes the second instruction but in the pipelining so whenever one phase is completed it takes another instruction means instruction fetch instruction decode operand fetch operand execute or prime store so this is I plus one instruction next is the I plus two so I plus true instruction fetch instruction decode operand fetch Oh plan execute next open store okay so this is a pipelining processor see the difference here after completing of one instruction only picks another instruction but in the pipelining processor only the phrase is completed so if the instruction fetch phase completed then it takes another instruction so in the face of instruction decode so whenever the first instruction is in the face of instruction decode it takes another instruction fetch so in the in the case of operand fridge suppose the first instruction is in the face of operand fetch and the second the second instruction is in the instruction decode fridge and the third instruction is in the face of instruction fetch here each face will be different so whatever the instruction you are taking from the memory these phases are not coinciding with each other so one is in operand fetch stage and another is an instruction decode fridge and another in is an instruction fetch phase so all are in different phases so here the phrase wise interleave is there so whereas in this non pipelining this is instruction wise interview you call this as instruction wise interview and you call this as a phrase wise internally so here you call this as a TP TP is nothing but a face duration so face duration so this TP each face duration means how much time we're taking for each face and this is complete you call it as Tian Tian is nothing but is a total instruction duration so this is the total instruction duration so this is the way of pipelining processing means if you take any program from memory to how to execute that program that program will have different types of instructions each program will be divided into different instructions so each instruction will executed in five phases different phases so if you take in non pipelining process this is the way it the pro each instruction will be executing in the process and the pipelining process after the completion of one phase it takes another instruction after the completion all of them other phase it takes their other instruction so here the total number of cycles will be means the phase duration is one two three four five six seven so here the phase duration is only the seven whereas in here this is a fine fine fine means five threes 15 so here the phase duration is 15 and here this is only the seven so the time Vestas is also reduced in the pipelining processor and here the pipelining processor will execute faster when comparing to the non pipelining person okay so layer here the pipelining is widely used in the modern process so the main it will be used in the modern processors so and here it also improves of system performance the pipelining processor will use it for system performance increase the system performance we'll go for the pipelining process so if there means a the system performance will be increase in the form of throughput throughput is nothing but here's the number of works done at a given time okay the main idea of pipelining in computer is a processor executes the program by fetching and executing the instructions one after the other so let us see one example how this pipelining process will be executed so let us take one example so let us example it will be the you want to execute one instruction a I star bi plus CA so this is the instruction you have to be execute this is a program that will be executed so here the instructions will be for I is equal to 1 2 up to 7 let us assume up to 7 when seven instructions will be there 7 cycles you have to be hopeful ok here each sub operation is to be performed in a segment within a within a pipeline here each segment has 1 or 2 registers and the tour will be connected in the combinational circuit so let us take the sub operation of this instruction in each segment of a pipeline let us store r1 air will be moved to r1 register and bi is more to r2 resistor so these are the inputs so we are giving just inputs inputs of a AI and bi so these are the inputs and next r3 is doing the operation of R 1 star R 2 it is multiplying so multiplying the both of primes so those are prime values will be stored in the registers so you have to do the operations only with the help of registers first move the values into the registers and you have to do the operation only with the help of registers you can't multiply direct or parents or direct it okay so R 3 R 1 star R 2 after that so our four you move the CI value so whatever the CA value that will be more to the r4 so alpha is so a star bi means R 3 R 3 and CI is our food so both I have to be added so the final result will be stored in the r5 okay so this is an example of the pipelining so let us see how this will be written in the form of a diagram so let us take this example a star bi plus CI so so this is the equation that is AI star bi plus CI so first we are moving the values into the registers so AI will be moved to the r1 register and bi is mode to thee or to register okay so what next so we have to multiply AI and bi so do multiplication so you have to place the multiplier after that multiplication the value will be stored in the r3 r3 register okay so again you have to place the CI in r4 register so now the r3 contains a I into VI and after contains the CA so after that you have to add this this value and this will be added so place the adder so that is r3 AI into BA plus CA my final result will be stored in the r5 register so this is the output so this is the way how the instruction will be executed in the processor instruction by instruction okay so let us see how the contents of this values will be stored in the form of a pipelining so here the AI bi and CI we have taken the AI from zero to seven one two seven so this is the content of the registers in pipeline so let us take an example of this how here the instructions are executing in the form of pipelining so this is a segment 1 the segment 1 has having the registers R 1 and R 2 R 1 R 2 is holding the values a B and segment R R 2 R 3 R 4 R 3 is doing the addition operation and sorry multiplication operation and r4 is storing the value of Zeeman and the final result will be stored in the r phi so this is the segment 3 so it is doing the calculation of multiplier as well as means addition it is doing the addition okay so first clock runs during first clock pulse the segment a1 and b1 so during second clock pulse it is a2 and b2 and in the segment 2 it is performing a 1 into B 1 and this is the C 1 so it is nothing is doing so in the third clock pulse a 3 B 3 it is doing the multiplication of this and here it is adding a 1 into B 1 plus C 1 ok so here 1 operation 1 instruction is executed so next again a 3 a 3 B 3 so this will be added 3b3 they start the segment we still doing the multiplication of a2 b2 and it's storing the value of c 2 and the result will be here a 2 B 2 plus say 2 which will be take place in the fourth clock pulse so in the same way it is executing in the pipelining process means here you can check each face will be different so here it is fetching and here it is a decoding and here it is executing so each all the faces will be in different stages no two faces will be colliding so one instruction is executing in one face another instruction is executing in another face okay so this is like that it is executing okay an a6 here and what totally we had taken the 7 0 to 7 a 7 B 7 so the here is our plans will be fetch after fetching it has to be decoded and after decoding it will be executed so in one face it is day in one cycle it is fetching in another cycle it is decoding and in another cycle it is executing ok so this is the example of the pipelining processor thank you 
d5wvFcIzUwk,22,"Subject: Computer Science
Courses: Computer Architecture and Organisation",2019-02-05T05:39:57Z,Choice of Benchmarks,https://i.ytimg.com/vi/d5wvFcIzUwk/hqdefault.jpg,Ch-13 Computer Science and Engineering,PT32M34S,false,39,0,0,0,0,"[Music] welcome to the next lecture so in this lecture we will be seeing the choice of benchmark so in the previous lecture what we have said is that how do we compute the execution time of a program so and then we know that yeah these are the ways through which you execute you can compute the execution time of the program now still how you can say that this computer is better than this Jois of benchmark that means what you will be using which benchmarks to use such that you can say that okay by executing this set of instruction I can say that my computer is giving so and so performance so basically here the basic concept is how to compare the performances of two or more computers this is what we are discussing from the last lecture as well so we need to execute some programs and measure the execution times set of standard programs are used for this for this comparison and we call this as benchmarks so benchmarks are nothing but some set of programs which are set as benchmarks basically for this comparison and various metrics have been proposed to carry out the evaluation we will be discussing that next so let us see some early metrics that are used one is millions instructions per second this is computed as instruction count divided by execution time into 10 to the power minus 6 so this is depending dependent on what so basically we are trying to get how many millions of instruction that are executed per second depending this is dependent on instruction set which makes it difficult to compare MIPS of various computers with different instruction set because see different computers will have different instruction set different kind of architectures but then how we can say that this can be a metric that can be used to evaluate for all so this becomes difficult so this as this is dependent on the instruction set because the instruction set cannot be seen for two computers mips also varies between programs running on the same processor why does this varies because different compilers will generate different codes suppose say you have ran a particular program and it has generated some set of codes the same program can be run under compile can may generate little different code so at that point of the time also this MIPS will be different and also it has been observed that higher MIPS rating may not mean better performance so we cannot say that if the rating that is the MIPS rating is high that means it is it performs much better let us take an example a machine with optional floating point cook processor so when a core processor so the meaning is that we have a machine with an optional floating point coprocessor so when Co processor is used overall execution time will be less because you you are using a coprocessor in which the task will can be performed in a much faster fashion so in turn your execution time will become less but for doing so you may use some complex instructions so if you use complex instructions then your it will it will give you a smaller MIPS value so when you use a coprocessor your overall execution time becomes less but you are using more complex instruction for execution that is the MIPS will be much less same way for a software routine it takes more time but it is giving higher MIPS value why because there will be more number of instruction that are getting executed but in turn the time using a software routine will be much much more so this is a fallacy this is a problem here so we are using a coprocessor which is a making the entire process faster but still we are getting smaller Mills but another which is using a software routine which is getting higher MIPS but at the same time it takes more time as well so MIPS rating is only valid to compare so MIPS rating is not valid to compare everywhere it is only valid to compare the performance of two or more processors provided that the following conditions are satisfied if the following conditions are satisfied then only we can say that MIPS rating is valid to use what are the factors first one is the same program is used the same instruction set architectures use the set of instruction should be same and the same compiler is used if you have all these things together in place then only we can say that MIPS rating can be taken for performance comparison so in other words we can say that the resulting programs used to obtain the MIPS rating are identical at the machine code level with the same instruction you must have same instruction count you must have same those machine code level instructions then only you can see that you can use MIPS as a metric to evaluate the performance this is million fourteen point operations per second M flops so it simply computes the number of floating-point operations executed per second now this obviously will be more suitable for certain applications where we will be using floating-point computation let's say for certain application there are not so much floating-point instructions so in that floating m flop will be much much less but that doesn't means that the performance of that is poor so here again different machines implement different floating-point operations and more suited and as where it is more suitable it is more suitable for applications that involve lot of floating-point computations so different floating-point operation takes different times addition of a floating point will take less time may be compared to the division of a floating point so we cannot really say that how well that M flops will give you a M flops is a metric which will give you a correct performance evaluation compilers have no floating-point operation and have M flop rating as zero there can be some compilers that do not have a floating-point operation and for that this M flop will be zero so the rating will be 0 for M flop hence this is not very suitable metric across machine and across programs M flops cannot be used as a metric across any machines or across any programs because different machines have different features different characteristics so it might not be a good idea to rely upon the metric like M flop let us take an example consider a processor with three instruction classes a B and C with the corresponding CPI values being 1 2 & 3 respectively the processor runs at a clock rate of 1 gigahertz so for a given program written in C 2 compilers produced the following executed instruction counts so instruction count for a type instruction is 7 for compiler 1 2 for compiler 2 for instruction type B type instruction and 1 for C type C classes similarly for compiled it to the number of instruction count for e type is 12 we type is 1 and C 2 is 1 let us see how do we compute the MIPS rating and the CPU time for the to program version so we have been given with the CPI values for the various types of instruction that is a B and C as 1 2 and 3 respectively and the processor runs at a clock rate of 1 gigahertz so these are the parameters which are already given let us see how we will calculate the MIPS rating and the CPU time so MIPS is clock rate in megahertz divided by CPI and CPI is CPU execution cycle divided by instruction count and CPU time will be X instruction count multiplied by CPI divided by clock rate or multiplied by clock period let's say for compiler 1 7 is the total number of instruction type e which is multiplied with 1 that is the CPI for that particular type instruction similarly 2 multiplied by 2 so basically we are doing 7 x 1 to x 2 and 3 x 1 4 compiler 1 so 7 x 1 to x 2 and 1 x 3 divided by total number of instruction total number of instruction was 7 + 2 + 1 that is 10 so 14 divided by 10 which is coming to one point 4 0 similarly MIPS rating will be 1,000 megahertz divided by 1 point 4 0 that is 1 gigahertz the clock rate is 1 D card so we converted it to megha Hertz because we have to find out in terms of MIPS so that is 1,000 divided by 1 point 4 0 that is coming to 7 1 4 point 3 now what is the CPU time the CPU time can be calculated by 7 + 2 + 1 total number of instruction in 2 in millions because this is given in millions so into 10 to the power 6 into 1 point 4 that is the CP i-- x clock period or divided by the clock rate so we get the time as 0.01 4 seconds so this much second it is taking to execute for compiler 1 and the MIPS rating for this is 7 1 4 point 3 let us take for the next compiler in the similar fashion we compute the CPI 12 into 1 1 into 2 plus 1 into 3 divided by the total number of instruction and we get 1 point 2 1 as the CPI similarly MIPS rating can be find out by 1,000 megahertz divided by 1 point 2 1 that comes down to eight twenty six point four minutes and similarly for CPU time we will use the same instruction count multiplied by CPI divided by clock rate which is coming down to 0.01 seven so now you see that the mix of this is higher so it has got higher millions instruction per second but the execution time of compiler 1 is less so here you can clearly see that the execution time of compiler 1 is less but the MIPS of compiler 2 is more so Maps cannot be the right choice for in such cases so MIPS rating indicates that compile it to is faster while in real reality the reverse is true now let us take an example so this is a C loop what we are doing inside the C loop for K equals to 0 K less than 1000 k plus plus we are simply adding a constant value stored in variable s to a key that is first irrelevant and we are storing back in a key similarly for the next one and this is going on in a loop let us write the assembly language code for this particular seaport segment so these are the few things you have to consider T 1 stores the address of s s is a variable which is a constant some value is stored here and T 3 stores the value of s and T 2 points to the first location of this array of this particular array so here initially what we are doing we are loading the word from this location 0 of dollar T 1 dollar T 1 will is having the address of s so value of s is stored in T 3 we are adding an immediate value to teach what T 2 contains T 2 point contain contains the points to the first array that is first location of the area and we have to compute something for this how many times thousand times so we are multiplying four thousand we are adding four thousand to t2 because each are four four four by so four multiplied by one thousand so which is coming down to we are adding it to t2 and we are storing it in T six so these six stores the final value so we have to go till that value to execute it next word inside the loop these are the following statement that are getting executed first what we are doing we are loading the word from the first location first location of the array that is a of 0 we are storing it in t4 then what we are doing the value of s is stored in T 3 and the array value is stored in T 4 so T 3 and T 4 we have to add and we are storing it in T 5 so finally we are adding T 4 and T 3 and we are storing it in T 5 and finally we are storing back this T 5 the added value in 0 of T 2 so in that location we are again storing it back and finally what we have to do we need to increment to the next location so the first part is done now we are moving to the next location so for the next location it is added with 4 again and then it is transferred there now branch if not equal we are doing such that whether we have reached to that point or not T 6 has is equal to that T 2 because at every point we are adding 4 to it so when it reaches the last element it will come out of the loop when till it is not equal T 6 is not equal to T 2 is not equal to T 6 we will loop when it is equal it will come out so these are the following assembly language code that we are executing for this set of codes the code is executed on a processor that runs at 1 gigahertz that is the clock period is 1 nanosecond there are 4 instruction types with CPI values as shown in this table now see a new operation which are a low operation add immediate add immediate these are ALU operation and the CPI of those operation is 2 similarly you have load load is load word load word the CPI is 5 you have store word the CPI is 6 and you have a branch in type of instruction where the CPI is 3 so these are the CPI's of various kinds of instruction and this is the program code which contains all these kinds of instruction type like lu these are the ALU type instruction load word is load type store word is tow type and branch if not equal is the branch type instruction now let us see the code has 2 instruction before the loop and 5 instruction in the body of the loop that executes 1000 times is it true so outside the body of the loop you see you have two instructions and inside you have five instruction 1 2 3 4 5 and each of these instruction is executed 1000 times because this loop is executing 1000 times so what will be the total instruction count there are 5 instruction and each instruction executes 1000 time so 5002 instruction outside the loop plus 2 it will become 5000 to number of instructions executed and fraction fi for each instruction type so let us calculate this number of instruction executed and fraction of instruction fi for each instruction let us first calculate the total number of instruction so outside the loop there is one Lu instruction and inside the loop there are two Lu instructions so these two Lu instruction each will be executed 1000 times 1000 times and this instruction will get executed one type so inside the loop there are 2 nu instruction which is executed 1000 times and this is executed one more time so 2001 similarly you can calculate for all load store and branch store and branch are only one one instructions are there this is store and discipline this is executed Towson time this is executed 1010 so what is the frequency total number of instruction of such kind divided by the total instruction that are there which is coming to 0.4 that is 40% this is coming to any percent this is coming to any person and this is coming 20% now how do we calculate this is the frequency of a loop kinds operation this is the frequency of load type and so on so total CPU clock cycles is 2001 multiplied by to 1000 1 multiplied by 5 1000 multiplied by 6 1000 mile to Clyde bytes so we are taking all this from this CPI we are multiplying the CPI with the total number of instruction that we have found out previously and we are getting the total cycles as this much 1 8 0 0 7 what will be the average CP item this is the CPU clock cycles divided by instruction count so this is the total CPU clock cycle divided by instruction count you get the CPU verse 3.6 now you can calculate the total execution time which is IC which is total instruction this is 5000 to CPI that we have calculated and this is the clock period which is coming 2:18 microsecond this is how we calculate it now how do you see the clock rating clock rating will be clock rate divided by CPU which is coming to two seventy seven point eight MIT's so the processor achieved its peak MIPS rating when executing a program that only has instructions of type with lower CPU CPI that is a new type instructions so if you own leagues execute CP CP such kind of instruction where the CPI is less that is you see the CPI of ALU type is only two but for store load branch is more now if you only execute that is Lu which only those type of instruction where CPI is less then you can get the peak MIPS rating that is coming to 500 nits but if we use with this mixture where the CPI is found out by calculating taking into consideration all the types of instruction and the frequency at which all these instructions are occurring then it will be coming to something lesser MIPS so next let us see choosing programs for benchmarking now how do we choose programs for this benchmarking suppose we are trying to buy a new computer and there are several alternatives possible so how to decide upon which one is the best the best way that is that can be used is to run the actual application that you are expected to run that is the actual target workload that means that particular computer you will be using more floating-point operation so you should have such kind of program in place which you will run on that machine and you will see that how what is the performance coming so actually you are running certain kind of certain applications that is that that will give you the best result so choosing the programs for benchmarking is really very important but not possible for everyone to do this while purchasing so what we do we often rely on the methods that are standardized to give us a good measure of performance so there are some standardized method that are used which can be considered as a good measure for this performance so different levels of programs are used for benchmarking one is real application can be kernel benchmarks some ty benchmarks and some synthetic benchmarks let us see an overview of all these things what are real applications here what happens we select a specific mix of suit of programs that are typical of large application or workload some of the examples are spec 95 spec CPU 2000 and etcetera spec stands for system performance evaluation corporation and this is the most popular an industry standard set of CPU benchmarks so spec in 95 consists of 8 integer programs so these are some of the information now I will be giving you for this spec standard spec that is their respect floating point 95 consists of 10 floating-point intensive program spec CPU 2000 consists of 12 integer programs and 14 floating-point intensive programs and spec CPU 2000 consists of 12 integer programs so these are some of the information and 17 floating point intensive programs so as we are moving from 95 to 2000 to 2006 the numbers are increasing so we are putting more workload because the advancement in these clocks clock speed is increasing so we can actually perform more operations so that is how it is moving so these are spec 95 programs integer what what all kind of programs that are present a game based on artificial intelligence a simulator for Motorola 88 8 chip G new compiler compression and decompression utility lisp interpreter image compression and decompression utility Perl interpreter a database program so the spec 95 program consists of these following benchmarks similarly spec 95 programs consists of these flow these programs and these are all these spec 95 programs of floating-point programs so they have a mesh generation program shallow water modeling quantum physics Monte Carlo simulation solving hydrodynamic navier's stokes equation multi grid solver on 3d potential field and so on quantum chemistry simulation and so on so these spec 95 programs were having these programs similarly C int 2000 that is integer consists of these following programs so these are the 12 programs apart from so they added something new which is VLSI place and route so these were also added in this group Theory interpreter were also added which was not there previously in spec 95 so these are some of the programs of CP CFP 2000 so it has got quantum dynamics these are already there's neural networks were added pollutant distribution is added nuclear accelerator is added and many more now let us see what is kernel benchmark here what happens basically he computationally in tensive pieces of code are extracted from real programs so let's say there are part of the program where the computation requirement is moved so they take out those part of the program from there and what they do unlike real programs no user would be running the kernel benchmarks they are solely used to evaluate performance so so basically there are some part of the program real program where the operations which you are performing is much much more so we take out those portions and what we do I mean unlike real programs the user will not be running the kernel benchmarks definitely they are used solely to evaluate the performance this is just used to evaluate the performance and as we know that kernels are also best to isolate performance of specific features of a machine and evaluate them some of the examples are Livermore loops Linpack etc and some compilers were reported to have been using benchmarks specific optimizations so as to give the Machine a good rating that means let's say we have so many benchmarks now so now these are already available and you can use this to evaluate your performance so some compilers typically use some of those features to accelerate the speed of those programs really but it may not work for any application it may work for specifically for those application but if you are not using such kind of constructs that are used in those programs then you will not be getting better result these are some toy benchmarks are also used they are small pieces of code typically between 10 200 lines and they're convenient and can be run easily on any computer they have limited utility in benchmarking and hence sparingly used some of these are some of the examples no coming to synthetic benchmarks what do you mean by synthetic by synthetic of what we mean is that those are generated so we know that this particular machine has this this much type of a li operation this much type of store load of store and load operation so some synthetic benchmarks are generated which will actually resemble to that particular frequency of operation which is performed for certain programs but they are synthetic they are not real benchmarks basically so somewhat similar to the principle to kernel benchmarking but they try to match the average frequency of operations and operands of a large program just now what I have said let's say we have a program and we know that for this program 80 percent will be such kind of instruction ALU operation and to any person will be store load operation so we also generate a particular program such that it uses same kind of features eighty percent will be ALU and twenty percent will be other synthetic benchmarks are further removed from reality than kernels as kernel code is extracted from real programs while synthetic code is created artificially to match an average execution profile so these are made artificially to match average execution profile some of the examples are wheat stone and dry stone these are not real programs so these are synthetic programs so we came to end of lecture 30 so where we have seen that what the choice of the benchmark how do you choose a particular benchmark so that entirely depends on the application for which you are designing or you require the CPU for what kind of application thank you "
v_J1NcqH1hg,27,,2020-08-12T01:46:03Z,Microprogrammed Control Unit,https://i.ytimg.com/vi/v_J1NcqH1hg/hqdefault.jpg,Chebrolu Engineering College- CHEC,PT33M12S,false,62,1,0,0,N/A,hi good morning to all this is vahidha working as an assistant professor in ece department in jaipur engineering college today i am going to discussing about micro programmed control the micro programmed control is used to generate the control signals inside a processor so already we know the control signals are required inside the processor these control signals are generated by using two ways the first one is hardwired control already we discussed in previous lectures about heart wide control how it generates the control signals okay so in there we used a step control step counter okay so in that hardwired control unit the control signals can be generated using a control step counter and a decoder or encoder circuit and now we discuss an alternative scheme which is called micro programmed control this is the second way for generating control signals inside the processor okay so in micro programmed control the control signals are generated by a program similar to machine language programs okay so this micro program control generates the control signals which are generated by a program similar to a mission instruction programs so before going into the micro instruction micro programmed control first we have to know some terms okay so first we have to learn some different terms the first one is control word control vote is a word whose individual bits represent the various control signals okay so control word is a word whose individual bits okay so control board one contains the bits okay so the individual baits represent the various control signals each of the control step in the control sequence of an instruction defines a unique combination of ones and zeros in the code word for example consider this example okay so now we will write see the code word corresponding to the step okay so code word is a word whose individual bits represents the various control signal okay so now we are going to writing a code word corresponding to this step 7 okay so in step 7 what control signals are activated z out r in and end okay so z out control signal r1 in control signal and end signal these three are activated okay so now we have to write a code board for the step 7 see so this is a micro program instructions see in case of our seventh micro instruction what control signals are activated in our example we are activated z out control signal r one in and end so z out control signal is activated here we are represented as one and r one in is one and end signal is one and the remaining all control signals will be zero see my here also y in is 0 place 0 okay so accept z out r1 in end control signal the remaining all are 0 so this is the code word okay so code word is word whose individual bits represents the various various control signals so like this way we will write the code suppose in case of first instruction so what signals are activated pc out e r in read select four add z okay so for writing code word for these first micro instruction we have to activate the those control signals pc out mer in rate add setting and the remaining control signals will be zero so this is a code word for the first micro instruction okay so like this way so for the seven micro instructions we have written here seven code words so now by using micro programmed control unit we are going to generate these code words okay from these code boards the processor will activate the corresponding control signals okay from these four boards the processor will activate the corresponding control signals and a sequence of code words corresponding to the control sequence of a mission instruction constitute the micro routine for that instruction a sequence of code votes corresponding to the control sequence of machine instruction constitute the micro routine for that instruction and the individual control votes in this micro routine are referred as micro instructions okay so you learned you learned three different terms code word micro routine and micro instructions and next the micro routines for all instructions in the instruction set of a computer are stored in a special memory okay so all these all these micro in micro routines okay all the micro routines for all instructions in the instruction set of a computer are going to be stored in a special memory that special memory is called the control store okay so here these controls to control store will stores the micro routines for all the instructions which are in instruction set of your computer okay this is the control store the control unit can generate the control signals for any instruction by sequentially reading the control votes of the corresponding micro routine for the control store so what this control unit do the control you need generate the control signals okay so it is it generates control signals for any instruction by sequentially reading the code words of the corresponding micro routine from the controls store okay this suggests organizing the control unit which has shown in this figure so this control unit or control store perform this operation okay and next to read the control board sequentially from the control store a micro program counter is used so this is the micro program counter in order to read the control boards sequentially from the control store we are using a micro program counter so this is a micro program counter okay micro program counter so every time a new instruction is loaded into the instruction register so for every time in an instruction register will be loaded with a new instructions means the instruction which is currently being executed is stored in the instruction register the output of the block labeled as starting address generator okay so next the output of the block labeled starting address generator is loaded into the micro program counter okay next the micro program counter is then automatically automatically incremented with respect to this clock pulse okay so this micro program counter is automatically incremented with respect to this clock pulse causes a successive micro instructions to be read from the control store stroke sorry control store or control unit okay so all these will causes the successor micro instructions to be read from the control store hence the control signals are delivered to various parts of the processor in the correct sequence hence from this control store we will get a sequence of control volts by using this control word the control signals are delivered to various parts means of various registers of the processor in correct sequence okay and next one important function of the control unit cannot be implemented by the simple organization of this a basic organization of micro program control unit this is the situation that arises when control unit is required to check the status of the condition codes or external inputs okay so here we got a situation that is the control unit or the control store is required to check the status of the both condition codes and external inputs in order to choose a between alternative courses of action in the case of hardwired control this situation is handled by including an appropriate logic function in the encoder circuit in case of hardwired control okay now in case of micro programmed control an alternative approach is to use the conditional branch micro instruction okay in order to include both a condition codes and external inputs in case of micro programmed control we are using conditional branch micro instruction in addition to the branch address these micro instructions specify which of the external inputs conditional codes or possibly bits of the instruction register should be checked as a condition for branching to take place so here we are using conditional branch micro instructions okay so for this one we have to check the external inputs and conditional codes they should be checked as a condition for branching to take place okay so suppose if considered this example here we are written if branch is a less than zero the br the instruction branch less than zero may now be implemented by a micro routine such as so after loading the instruction into the instruction register so the first three steps from 0 to 2 comes under the fetching phase after fetching the instruction the instruction is stored in the instruction register okay so after loading the instruction register a branch micro instruction okay so the branch micro instruction transfer the control to the corresponding micro routine which is assumed to start a location of 25th address 25 25th address in the control store this address is the output of the starting address generator block okay so here we are checking the branch instruction here we have written if n is equal to 0 okay so if n is equal to 0 then the branch to micro instruction 0 means we will back to the 0th address instruction and we will execute this one again ok so if n is not is equal to 0 then we will come to the 26th address instruction okay so means the offset of the the offset is going to be added to the instruction register out and next select y is enable the next edition will be performed after edition operation the result is going to be stored in the z in okay and next that address is transferred to the program counter by activating z out control signal so so by activating z out control signal the address will be stored in the processor internal bus by activating pc in the address is transferred from processor internal bus to the program counter it gives the end of the execution of these micro instructions okay so the micro instruction at location 25 is going to test the it is going to test the end bit of the condition code if this bit is equal to zero a branch take place to the location zero to fetch a a new mission instruction otherwise means if n is equal to 1 the micro instruction at the location 26 is executed to put the branch target address into the z as in step 4 the micro instruction in location 27 loads this address into the program counter in order to support micro program branching so okay so in order to support these type of branching micro programs the basic micro programmed control unit so means this one micro programmed control unit should be modified like this okay here we are having starting address generator now this starting address generator block becomes starting and branch address generator okay so what this block will do this block loads a new address into the micro program counter okay so this will loads a new sorry the address of a new micro instruction into the micro program code when a micro instruction instructs it to do it so in order to allow implementation of a conditional branch input to this block so okay so here we have to provide a conditional branch so for providing or for implementation of a conditional branch inputs to this block consists of external inputs and conditional codes as well as the content of instruction register okay so external inputs and conditional codes here in this control unit or control store the micro program counter is incremented every time a new micro instruction is fetched from the micro program memory okay so the micro program counter is incremented every time a new micro instruction is is fetched from the micro program memory except okay so the except the conditions are so we are having three accept conditions so where the micro program counter will not be incremented the first one is whenever a new instruction is loaded into the instruction register the micro program counter is loaded with starting address of the micro routine for that instruction okay so whenever a new instruction is loaded into the instruction register the micro program counter is loaded with the starting address of the micro routine for that instruction this is the first step and the second one is when a branch a micro instruction is encountered when a branch micro instruction is encountered and the branch condition is satisfied then the micro program counter is loaded with the branch address when the micro program counter is loaded with the branch address when branch micro instruction is encountered and the branch condition is satisfied whenever the branch condition is satisfied means in our example we have written if n is equal to 0 if n is equal to 0 is satisfied then we will go to the 0th step see here whenever the branch instruction is satisfied if n is equal to 0 then the branch has transferred to the 0th micro instruction so then from 25th address we will transfer to the zeroth address so during this situation the micro program counter will be loaded with the branch address if this condition is satisfied so previously the micro program counter contains the address of 26th but if this condition is satisfied the micro program counter will be updated with the branch address that is zeroth address now the micro program counter will have will stores the zero address and next third one is an end micro instruction is encountered okay sorry first to see so during a branch condition is satisfied then so from the control store we will back to the starting and branch address okay so from here now this block will generate the branch address these branch address will be stored in this micro program counter means the micro program counter is loaded with the branch address the next third one is when and end micro instruction is encountered the micro program counter is loaded with the address of the first code word in the micro routine for the instruction fetch cycle it means at the zeroth address in our example okay so whenever an end micro instruction is encountered the micro program counter is going to be loaded with the address of first code word okay so it is going to be loaded with the address of the first code board in the micro routine for the instruction fetch cycle so this is about micro program control unit so this block will allows both external inputs conde and conditional codes means it allows the branching conditional branching and this block will do doesn't allow any conditional branching micro instructions so this is about micro programmed control unit okay so hardwired control and micro programmed control units both these units or control units are used for generating the control signals which are needed in the proper sequence for the processor okay so in case of hardwired control the encoder or decoder block will directly generates the control signal depending on the inputs given to that block but in case of micro programmed control unit it it instead of generating the control signals it generates the signals by a program similar to the machine language here depending on the code board the processor will activate the corresponding control signals depending upon the code word a code word is nothing but a word or whose individual bits represents the various control signals of the processor okay so this is about hardwired control and micro programmed control unit okay so now i think all of you understand how the processor will activate and deactivate the control signals okay thank you you 
h8EijwzPths,27,"I have Collection Of Advanced Computer Architecture-Princeton University Videos and I Want to Share With You.If You Like My Videos Please Subscribe To My Channel Below://
https://www.youtube.com/channel/UC1SsXZdTPDLUAEGtPwc02SA

EXTRA TAGS::
Princeton University,Advanced Computer Architecture,princeton university tuition,Advanced Computer Architecture,advanced computer architecture notes ,computer system architecture,computer architecture book,parallel processingÂ ,parallel computingÂ ,parallel programmingÂ ,advanced computer architecture syllabusÂ ,computer architecture courseÂ ,software architectureÂ ,computer architecture booksÂ ,computer booksÂ ,fundamental of computerÂ ,computer networkingÂ ,advanced computer architecture lecture notesÂ ,advanced computer architecture and parallel processingÂ ,parallel computer architectureÂ ,component of computerÂ ,computer architecture and parallel processingÂ ,parallel systemÂ ,cs2354 advanced computer architectureÂ ,parallel processing in computer architectureÂ ,computer systemÂ ,advanced computer architecturesÂ ,parallel computerÂ ,computer systems architectureÂ ,computer engineering courseÂ ,computer engineering booksÂ ,architecture of operating systemÂ ,computer trainingÂ ,advanced computer architecture courseÂ ,advanced computer architecture question bankÂ ,fundamental computerÂ ,architecture classesÂ ,parallel computersÂ ,parallel and distributed computingÂ ,books on computer architectureÂ ,advanced computer architecture question papersÂ ,computer hardware booksÂ ,what are the component of computerÂ ,computer and societyÂ ,advanced computer architecture booksÂ ,computer architecture pattersonÂ ,parallel processing applicationsÂ ,computer instructionÂ ,information about architectureÂ ,operating system and application softwareÂ ,about computer engineeringÂ ,computer system softwareÂ ,advance computer architecture pdfÂ ,application of parallel processingÂ ,advance computer architecture notesÂ ,parallel computing architectureÂ ,books on computerÂ ,applications of parallel processingÂ ,computer system and architectureÂ ,patterson computer architectureÂ ,example of parallel processingÂ ,massive parallel processingÂ ,computer architecture patterson pdfÂ ,advanced computer architecture ebookÂ ,free architecture booksÂ ,book of computerÂ ,parallel processing applications in advanced computer architectureÂ ,advanced computer architecture hwangÂ ,information on architectureÂ ,parallel processing systemÂ ,software and hardware of computerÂ ,about computer architectureÂ ,be computer engineeringÂ ,parallel processing architectureÂ ,computer architecture and design 5th edition pdfÂ ,software application architectureÂ ,www computer architectureÂ ,computer architecture slidesÂ ,component of the computerÂ ,computer system architecture bookÂ ,fundamental of computer booksÂ ,books of computerÂ ,about computer systemÂ ,computer engineering bookÂ ,distributed and parallel computingÂ ,operating systems booksÂ ,computer systems design and architectureÂ ,computer text bookÂ ,applications of parallel computingÂ ,computer instruction setÂ ,parallel processing computerÂ ,pdf architecture booksÂ ,computer systems bookÂ ,computer software architectureÂ ,information about computer engineeringÂ ,computer course bookÂ ,set architectureÂ ,books computerÂ ,information of architectureÂ ,pdf computer hardwareÂ ,advanced computer architecture previous question papersÂ ,",2015-12-10T01:19:42Z,Advanced Computer Architecture-,https://i.ytimg.com/vi/h8EijwzPths/hqdefault.jpg,Preety Singh,PT10M7S,false,32,0,0,0,0,okay so let's take a look at our data path here and see where predicate thin to the day I pack and we're going to focus actually just on the conditional move predicate or predication instruction here we're not going to look at full predication just yet on the data path but it follows a similar idea okay so what's what do we need to to do where do we need to add to our sort of boring nip style five-stage pipeline to add this instruction okay instruction comes in moves down the pipe oh this is interesting I know this is a really cool trick let's just if this condition is not true let's just kill the right back to the register file it's brilliant we just have we just suppress the right back we don't have to actually change your day that always put a and gate in here and this and gate depends on you know this condition simple it's easy maybe this is what we should do looks simple we just add an and gate with the big x's and life that life is done okay well that looks looks good can we bypass this value so can we have an instruction that directly follows this move 0 or this conditional mu 0 and reads rd well where are we changing our d or not change the artery we're making that decision well on this pipeline because we did it in the right back stage it doesn't happen till down here down here are right back stage or this wire that's the right back wire runs all the way back into the writing table on a register file okay that doesn't really help us a whole lot especially for trying to bypass we're trying to bypass out of here ral you back around because at this point we haven't figured out any way to suppress this so we don't actually we're not able to actually suppress that mmm so what do we what do we think about this so how do we how do we go out doing this so let's let's talk about how to actually bypass out for this conditional move instruction because conditional move just a simple comparison 10 which will do that in one cycle we don't really have to wait to get into the pipe to do that and we're going to bypass in into a back-to-back instruction okay so how do we how do we do that well are bypassing doesn't work what if we somehow pipe forward the original value and the new value okay so what do I mean by this so this this instruction is very interesting is much more interesting than your standard like add instruction so why is it interesting well let's look at the semantics very closely here move 0 is going to write Rs 2 r.d or it's going to write our d 2 r.d I say why do I need to write our d 2 r.d well in the bypass path when we provide this value around back to the our bypass registers are forwarding logic here or bypass muxes or a forward forwarding logic we need the old value of our d so in a traditional sort of some sort of risk pipeline here we're only going to fetch our two sources and we only write one location so we're going to fetch RS or RT and then we're going to write to our d now all of a sudden in this instruction we need to read RS okay we need to read that because we need to overwrite rd with RS if we need if the condition is true and we need to read the condition RT but aha we may also need to read our d here this is because when we get to this stage here and we want to use this bypass path to forward the value of what our d is going to be in the future we need the original rd so sort of to draw this a little bit more succinctly because this is pretty important we have if the register value of our T equals zero we have our of our d gets RS that's the easy one we can count the registers here once one source to sources one destination it looks simple and one know whatever forever always forgets is there that else case here and what does this else case say well the else case is going to say register rd gets register our d you might say well our rd already had our d that's true but our bypassing or forwarding logic didn't have that so we need to actually read this rd so that means we need to read one two three and we need to write one location okay so that's going to cause us some problems over here because all of a sudden we had our register file which had two read ports and we need to now have three read ports so we need to add an extra read port on our register file and this could be expensive so if we actually want to build predication it's going to have some cost we might if you want to build predication and actually bypass something like a predicated conditional move we're going to have to add another read port our register file and that that actually has some costs and this is especially costly if you look at something like a VLIW so let's let's take for example a three-way vliw some like the tile era processor so two three way three wide vliw each of those is going each of those ways or each of those pipelines is going to read too if you don't have conditional move will say and it's going to write one value so it's going to have six read ports and three right ports so it's a ten port register file no excuse me sa9 port register file to begin with and I'll follow up suddenly add something like conditional move here and we need to add these extra read ports we're going to go from a nine port register file to a 12 por register file where you have let's take like this you need to have three right ports and nine read ports that's a that's hard to do it's you know it's hard to build these really heavily ported register files okay so to sum up here our problem problems with full predication is that you need to add another pork to the register file you need to bypass the predicates so what I mean by that is your computing predicates and you want to use it in the next instruction so if we go back to this instruction sequence here we compute these predicates and we would use it very carefully or very very quickly after it we don't have to wait to the end of the pipeline for this product is to be computed so effectively is going to make a make it so that we're going to have a predicate register file sitting somewhere here and have by passing around the predicate register file or forwarding of the predicates to to get the the predicates there faster or get the predicates to be used in the next instruction and you don't have to add extra pipeline registers to pipe forward the old value because you may need to keep the old value in the bypass and in fact actually a lot of times when people do these things they actually always write the register file and just pipe forward both and at the end make the decision or along the way they make a decision to go to the bypass or not but then sort of when the instruction finishes definitely make the decision so we're going to actually have to add more pipeline registers to pipe forward the old value that was in in this case rd you 
nv0yAm5gc-E,27,"Lecture By: Mr. Varun Singla

Facebook page link : https://www.facebook.com/gatesmashers/",2018-06-02T07:35:52Z,L-4.2: Pipelining Introduction and structure | Computer Organisation,https://i.ytimg.com/vi/nv0yAm5gc-E/hqdefault.jpg,Gate Smashers,PT3M54S,false,185437,3090,65,0,106,hello friends welcome to gates Michelle let's see the details of pipelining with its definition and designing so the formal definition of pipelining is it is a process of arrangement of Hardware elements of CPU such that its overall performance should be increased but love hum pipelining Koyuk si process snake it is melee Hardware scope purchase garea or Luka home implement career name i'm already existing hardware school any cpu Co is three case a range career so that performance you have a increase on each AAA but performance increase actual mckessie okie was created a second line career simultaneously execution of more than one instruction will take place in the pipeline MATLAB take time pay a construction execute neo give multiple instructions will be taking place on disco music earth in a pit term overlapping overlapping a model of care jessa gave my last video made ricotta cake time pay eight process you have a porridge of executes hotel escape barge of Doozer process start out there that is a non pipeline non-pipelined matlab a process start wa giacomo completely you thought of the Dussehra process start videos data but he Siva Siva have a performance key problem of the pipelining making a good eight process this a particular stage which allow to do Sara process kisi or stage pay overlapping state may later or ECG Chicago Bulls there overlapping or you have a many stages could represent kiya stages camp a benefit care occurring a Hapuku a stage neo keith obviously a process start auga jabot complete Olga Tama doosra process ko input curve on wa lakin algorithm by multiple stages divided cars at all stages i income segments people there so actually me who gotcha eight process ho sakta kisses stage to pay children do sarah process stage one pitch all cetera process stage three but la laguna the process is children Alec lock stages Cooper so anyhow may feel chaotic II ache time pay a process me multiple process is executed or you schema Jess a performance you have o increases your Thea it's his stages Joey stages concassÃ© defined getting a calcium particular kidney stages of Sofia is correct simple example at risk reduced instruction set jahar path stages mayhem pipelining co u skirt or pants trade is made aware instructions execute her Thea SEM further examples risk is Karenga in the next video he casts a or Cohen conscious stages actually Miho Tina or deuce robbery happy cast space-time diagram it's space-time diagram k through him represent Katya kick-ass a instructions actual made a real time to execute or theories compose a realization model of multiple instructions kiss kiss trying to execute ot Oscar realized karmic ileum space-time diagram used the Teja hoppy hamari pass x-axis main clock cycles of the y-axis my Americas a lot stages appear or yaja pea sorry stages or a beach may her eighth stage Key Beach make an interface register this comb latches we both there in Cabana fit care intermediate results Co store car character because each stage Keijo output a buddhist red stage key input obviously output to Maria harmony input the output aha generator key to stage an age of output generate Chievo stage Kelly a input like in the stage by input um dengue cases Kelly Mucha cocaÃ­na key intermediate Lee who's a result ghost or catnip a dega to Scalia Muse got their registers go or yes sorry K sorry devices Joe hello eight o'clock you through connected a tacky Jessie I'm a clock cycle apply carrying a sorry devices don't have a execute upon a pelagic arm have a properly execute carrying a or who CG go home represent Korea with the space-time diagram or ham they can geeky casino on pipeline or pipeline may performance car different sorta thing 
UC_ROevjIuM,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2019 (https://safari.ethz.ch/architecture/fall2019/doku.php)

Lecture 1: Introduction and Basics
Lecturer: Professor Onur Mutlu
Date: September 19, 2019

Slides (pptx): https://safari.ethz.ch/architecture/fall2019/lib/exe/fetch.php?media=onur-comparch-fall2019-lecture1-intro-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2019/lib/exe/fetch.php?media=onur-comparch-fall2019-lecture1-intro-afterlecture.pdf",2019-09-26T07:04:19Z,"Computer Architecture - Lecture 1: Introduction and Basics (ETH ZÃ¼rich, Fall 2019)",https://i.ytimg.com/vi/UC_ROevjIuM/hqdefault.jpg,Onur Mutlu Lectures,PT2H24M11S,false,31288,376,1,0,9,"I hope you are all here for a computer architecture is that good cool it's a good time to be studying computer architecture as I will hopefully show you the website of the course is already up so if you want to check it I don't have it on the slide but I'll show them show it tomorrow but you can also find it relatively easily by changing last year's website 2018-2019 basically and you can find all the material from last year's this this year it's going to be a little bit different course but the basics are still going to be the same okay well let's get started so let me introduce myself first I'm on remote low I've been here for four years about now and I'm a professor here before that I was at Carnegie Mellon University for some time I still have some PhD students over there I go back and forth once in a while but less so now I got my PhD from UT Austin I did my thesis on run at execution a way of tolerating wrong memory latencies it's a fascinating topic I think if you're doing a PhD in a topic you should definitely be excited about it for sure because it doesn't make sense to spend five six seven eight years of your life on something you're not excited about I think I actually did a lot of internships at Intel and AMD while I was a PhD student and you may see some examples of things that I described based on those experiences and when I finished my PhD I went to Microsoft Research and started the computer architecture group at that time in 2006 they were very interested in doing computer architecture and right now everybody is doing computer architecture actually they were a little bit forward-looking perhaps at that time and they started a computer architecture group and I was the first person over there and we grew the group and you will see some of the works that you've done also with respect to that and I worked at Google and VMware and a couple of other companies over the course of my career this is the best way to reach me if you want to send me well I'm not sure if this is the best way actually the best way is really whatsapp but this one is a bit a bit better than maybe some other ways I'll give you the phone number later on if you stick around in this course in my experience people usually drop after the first course or the first class or the second class maybe they think I'm too hard or something but this is fun you can see it all of this information from my website this guy do research and teaching in computer architecture which is why you're here computer systems hardware security by informatics and I think these are all related to each other you will see will cover a lot of topics that are related to things that I'm doing research on clear this area little the Advanced Course so we're going to cover relatively advanced topics as well how many of you took the the digital circuits class with me okay only two I thought more people were registered but maybe they're not here maybe they know that this is being recorded and they're just gonna watch the recording has anyone else taken another course with me okay probably the seminar course yes I see some hands on the back okay so that's good we have a good mix of people okay we're gonna cover a lot of these basically we're gonna talk a lot about memory and storage today and not about today we're gonna talk about nemi's or for sure but during the course also we're gonna look at issues like hardware security safety predictability reliability fault tolerance which is important I mentioned that earlier right we're gonna see an example of it in a very recent chip very soon very briefly not not nothing nothing very special we're gonna talk about hardware software cooperation and we're going to talk about specialized architectures for things like my informatics health medicine machine learning which is really the big thing that's going on right now in architecture as well so we do research in these areas also this is actually a slide I used to show two PhD students who are incoming these are some of the research areas that I work on this is a busy slide but you can look at the picture basically this is essentially what we're going to cover in this course also a lot of what goes on in a chip what in the memory sight what goes on in accelerators like graphics units and what are some upcoming things that are happening in memory and storage like emerging memory technologies and I will give you another example you're actually at a lucky place I think in computer architecture because in 2019 a lot of things happened in computer architecture that have changed a lot of things as you will see in a little bit which was not true in 1999 for example yeah I will give you an give you that perspective also but basically these are some of the things that we're going to talk about you don't need to read everything on the slide actually I want one thing I would recommend in following my lectures it's better not to try to read everything on the slides slides might have a lot of stuff if you try to read them you will lose what I'm saying it's better to basically follow what I'm saying and maybe get the key things on the slides like these pictures right the clues okay but we're gonna cover a bunch of stuff so as I as I as I promised I was going to show you a several thing today many interesting things are actually happening today in computer architecture which was not happening a long time ago certainly when I was studying doing my undergraduate degree computer architecture people are thinking who cares about this field but I cared about this field I oh yeah I'd like to think that I'm thinking forward but basically people were that the paradigm was you design processors that are designed for single threat high performance out of order superscalar execution and that was the only paradigm people didn't really think about some different memory system people didn't really at least generally think as much about parallel computers certainly not machine learning accelerators machine learning was not even on the table at that time people didn't care about it people were thinking this this wouldn't work that was 1990s today we're in 2019 it's a very different place because of the changes that have happened in many many directions which we will cover in this course things like this are happening right now this is and memory technology that you can buy today you can put it on your computer and it's persistent meaning it's non-volatile today if you look at DM technology for example it's volatile you can you write data to it once you lose power power gets lost but this thing is there right now you can buy it and you can have very fast access to persistent data essentially think of your files being stored in memory you like this at very low latency as opposed to going to disk for milliseconds well it's just these are microseconds may be okay but this is like nanoseconds on the order of nanoseconds let's say well if you don't think it's on the order Dino second that will become in the order of nanoseconds in a few years so technology in process is non-volatile memory we're going to cover this this is called 3d xpoint intel introduced it this year it sits in even though Intel I don't think he admits it still it's the technology is really phase change memory as far as we as far as all the indications suggest it's not publicly said but all the indications are just that it's phase change memory it's a technology we will cover basically it's a technology where you store data in the form of resistance which is very different from existing technologies that we have well this is also existing but of the existing technology that we had where you store data in the form of charge you lose charge very quickly charge gets lost and you cannot retain charge in DM at least across power cycles whereas in this one you can retain charge because you've changed the resistance state of the memory from high resistance to low resistance all versus the high resistance and that's how you encoded the data and this is non-volatile this didn't exist last time I was teaching this course so you guys are lucky basically in the end this is this exists now and you can buy it you can experiment with it there are papers that are being written on it and you will cover this fundamental technology so what will this change this this basically enables very fast access to storage so people are using it for example very fast boot up or the machines people are also proposing mechanisms if you have this non-volatile memory very tightly integrated into your your sir then maybe your processor doesn't need as much power maybe you can you can be running somewhere with very very low power where you don't have good access to power in some random place maybe space who knows and you can run intermittently and you can intermittently store data very quickly to this sort of memory so that you don't lose the computations that you've done so imagine this sort of applications these sort of applications can be enabled only by thinking about this technology and thinking about how do you architect the system around it okay that's one example there there could be many many other examples so once you have this technology can imagine many other things we'll see the underlying details of the technology and how to take advantage of it but clearly whenever you think about whenever you see a technology that I introduced in this course don't think just what I just about what I described because what I described is limited to what we know today but as an architect as we will see also you should really think about the future like 10 years down the road 20 years down the road and you will have careers much longer than that down the road what can you how can you take advantage of this technology that's how you can really improve things going into the future the space example is one example I don't think anybody is using this particular technology in space right now but it could be used if it's developed in the right direction okay did anybody hear about this before yes okay you had a question yes isn't that what absolutely yeah so this is one this is one benefit of this technology compared to what you could put on an idea them you can put you can squeeze a lot more bits inside a chip and we will we will cover that because the much more scalable technology can make the the memory cells much smaller then you can make them in the um and it's not much more scalable so over time you can make them even smaller compared to the app so there's a disparity and we will talk about DM quite a bit because DM is facing scaling challenges and this is exactly why the technology was developed and if you let me give you the since this is the introductory class with actually you know in the entire class I would like to make it more interactive going forward if you have questions it's best to ask them but also think about how long it takes for a technology like this to develop did you hear about phase change memory before did anybody hear about phase change memory before yes did you know that today but use CD rewritable CDs before are you all too young for it I see ok sometimes I ask this question I figure out that I'm very old because people have not used some of the technologies but CD rewritable CDs I guess still are not that bad so rewritable CDs actually employ a phase change memory it's this it's it's a very similar technology maybe different materials but it operates based on the same fundamental principle do you store charge based on the resistance of some phase change device and you you I'm sorry you use not store chart you store data you encode data based on the resistance of some phase change device and you read data based on some property of that resistance in in three writable CDs you shine light on the CD and the reading process happens optically so it's really an optical reading process which is extremely slow because of the optics well in that in this particular case at least so this technology was developed in the 1960s actually if you trace it back it goes even earlier than in 1960 celibate and people use it for rewritable CDs but people were really interested very recently very recently meaning in the late 2000s on replacing DRM with with with the technology like face change memory but it was not easy to do because you cannot you cannot have an optical reading process in in a machine like this basically and certainly when you access your main memory so what what a lot of research has done was to develop the read device this is called the read device basically this is the device that enables you to read this technology very fast and reliably and those read devices were developed over the course of mm say especially Intel and IBM did a lot of work to develop those many devices and those read devices once they were developed reliably they enable this technology that's why you're seeing this technology today and the first architectural works which we will cover also we actually did one of the first architectural works in this area in 2009 in one of the top conferences we presented the work there were three works that were being present at the same time on using phase change memory as a replacement for the year and remember I remember that's 2009 and the research usually when you see a paper that was published in 2009 you can guarantee that the research was done at least a couple of years for a couple of years before that probably more than that actually when you start doing that in 2007 so this technology came about to reality in 2019 so that's another lesson here basically if you're an architect you have to be patient you may have a great idea but for that idea to really come to reality it's going to take time especially if you're working on a new technology that's completely different that is dependent on many many factors including manufacturing here here people needed to develop the manufacturing capability to actually reliably manufacture those read devices as well as the patient's memory cells okay now I bored you know okay okay we'll keep going this is just one example so I'm already behind what I've scheduled but that's okay we're gonna take it take it more relaxed view of this course now okay so this is one example you're lucky not just because of this and you can imagine use of this but there's another interesting thing that happened this year which is this chip how many of you have seen this chip okay multiple so how do you see this did you follow up hot chips okay on the internet yes ml this is very popular these days because of how it looks like right basically this is there's a huge push for designing machine learning artificial intelligence accelerators today because the data that you have is so much and you need to process it really fast especially to train these machine learning models people are finding that CPUs and GPUs they're not really up to the task even FPGA is I think you really need to specialize the processor that you built so that you can do these computations very fast and very efficiently and you can put MIT you can do them with many of them at the same time and this is one solution that a recent startup came up with this is not they manufacture that it hasn't been successful yet that's true for this one also actually even though Intel has manufactured it it has it hasn't proven itself yet this chip also has not proven itself yet but there's something different in this chip as you can see this is the largest GPU that you have today and this is okay this is you can scale this basically this figure is to scale and the largest GPU is 21 billion transistors 815 square millimeters and this is that they call the wafer scale engine because really they take the whole wafer and essentially cut this wafer into that sort of chip as opposed to cutting the waste four into small chips like what's normally done so it's a very bold move it's a very bold technology it's not clear how the reliability will work out because we don't have the numbers yet but you can see that you have one point two trillion transistors that's a bit crazy right even move couldn't predict this broadly but you can see the area right since you have the entire wafer or a good chunk of the wafer you get forty six thousand square millimeters that's about forty seven X the size of this chip and what this does is basically it does acceleration of machine learning tasks a lot of matrix multiplications floating-point multiply and additions and a bunch of other stuff of course they don't disclose exactly what they do but you can imagine what goes into machine learning training which we will talk about later on also and you can see that they're four thousand four hundred thousand cores in this one so if you're interested you can look at the links and you can find more information about this as well but this is also another reason why studying computer architecture is really important today because actually I know some of these folks who built this chip you know I I worked with some of them during my internship at AMD they had the experience to actually think ahead and design something like this now again we don't know if it'll be successful but this is a very interesting move if it's successful it can change the paradigm maybe people will start producing weight for scale engines going forward it who knows but of course you should always think about nothing comes for feet like this thing is small and it's made small so that it's reliable but you may have a lot of defects in a huge wafer right the question is how do you handle those defects how do you power up such a huge chip like this thing we know how to power today how do we power up this huge thing I don't know well clearly there are some answers but we don't need to go through them right now how do you cool down the sort of chip if you're doing so many computations at the same time it's going to clearly generate a lot of heat we don't even have heat sinks this actually take the heat out of this chip today okay that's another example any questions okay I'll give you another example this is again 2019 this was also introduced in hot chips like the previous one this is another startup again this is not this has not succeeded yet but people are trying hard basically what this is is it's a processing in the image and you have this DM module what these folks have done is the Augmented the chip the memory chip with essentially processors they call the data processing units its standards dims so you can plug it into any computer but now your computer has capability to do computation inside the memory which is very interesting it's a limited amount it's actually a full processor over here and they have some instruction set architecture you need to write your programs to make sure they work of course but the programming part is not that terrible as long as you fit your application to to run on these processors inside the memory again this is something that you could not imagine 20 years ago 20 years ago people thought that processing in memory was dead even though it's an old idea we will cover it people thought it's not an interesting idea because we have processors that are running really well right they're getting the job done might change but today because we have such a big memory bottleneck people are looking at solutions like this ok and that the the processor inside the chip clearly has a lot of computation but a huge huge amount of memory bandwidth available to it you can look at the numbers and their presentations I don't want to pick the numbers because this is this is still speculative right but this is happening people are actually put inputting processing inside the memory ok did you know about this one before anybody ok this is probably some people know ok ok let's take a look at another one this is another one maybe not as huge as saira browsers wait for scale engine but this is done by a car company Tesla they figure out that our cars would run better if you actually design the cars together with the processors that that drive them essentially this is their machine learning chip that is supposed to be used in their cars according to them it's used and they have a nice video that's explaining this chip you can take a look at it they give more detail than on the slide what is this is essentially another machine learning accelerator which we will cover also and you get a lot of transistors on this one also not as much as the trillion transistors but still this is a relatively large chip and you also have a GPU pretty reasonable and you also have a bunch of CPUs inside so it's a heterogeneous architecture that consists of machine learning accelerators CPUs GPUs and somebody's to program it so that it can make the Tesla cars on faster if you're interested you can read the this thing in more detail basically they showed significant performance energy efficiency improvements compared to a state-of-the-art GPU that's what that's why they wanted to build it because they needed a specialized solution and they didn't have that specialized solution basically existing chips are not satisfying the demands of these workloads that we're we have today that's true for machine learning that's two for graph analytics that's two for genomics as we will see that's true for many many areas where we have lots of data and me to process data are really fast and we need to really make sense of data really fast and this is one example that's why these things are happening today and there's a there's there's an interesting thing over here you can see that there are two of these chips together they put two of them to ensure that if one of them fails the other one will continue the reliability is important this is a very old idea actually it was developed in 1970s it's called dual modular redundancy you basically have the two components that act together that act in unison and if one fails you use the other one or you basically run two components at the same time and check the results and if one of if one of them is if they disagree with each other then you know that there's a problem somewhere that's dual module of course it's expensive right but they they thought this was a good trade-off now you have to pay pay the cost of two chips to get one thing done right but they thought this was a good trade-off and if you want to be more reliable you can go triple modular redundancy you can have three chips and if you you can extend this to n modular redundancy where n is greater than 1 right so clearly reliability is also important so that's why I think we're in a very complex space today we want performance clearly and the way we are getting performances with these specialized chips we're also getting efficiency with these specialized chips but we need part we need to we need ways to power them up so power is a problem terminals are a problem reliability is a career problem over here and we haven't even talked about security yet ok actually there if you if you watch this video they talk about security a little bit but how do you keep these chips secure especially if they're doing self-driving right if someone hacks into these chips and ensures that you crash or you go somewhere you don't want to go how do you how do you contract that it's not easy basically and it all needs to go into the hardware because hardware has a lot of security problems as well I didn't put them over here but probably most of you heard about meltdown inspector by now right how many people heard about knock-down inspector ok probably most people that's that's good if you haven't probably search for meltdown inspector on you will see ok so another example this is relatively old now the machine learning accelerators actually started at least the big ones started with this one this is Google's tensor processing unit generation 1 which was developed around X 2015 2016 and the first paper was written in 2017 you can read this paper for more information on it but they they thought that this was important to develop because their data centers did a lot of machine learning tasks and existing processor CPUs GPUs were not enough to the task and they had a huge amount of computation and competition their data centers and they were able to build this chip and I believe it's this is an example of success because it's actually employed as far as I know and all of Google data centers to accelerate machine learning tasks and this is the first generation if you're interested you can meet this paper and if you take in my course you've learned about systolic arrays basically the way it operates is really like a systolic array you can see that there's a this a matrix multiply unit and basically it's it's kind of invisible to the software software thinks that 12 256 byte input is read at once and that that inputs updates one location of each of the 256 accumulator rams that you have over here basically it does matrix multiplication and you can read the paper for how it does matrix multiplication and actually I have some more detail in the next few slides but this is the second generation they already have third generation fourth generation up coming so the the for example this one is designed for training as well so machine learning consists of two things we train a model you figure out how you should basically train a model based on a lot of data and you later use that model for inferring for classification for example right so this chip is designed for training and usually chips designed for training need to need to tell a lot more memory a lot more computational power because you're really dealing with a whole lot more data to really build a model whereas inference you can be a lot more efficient although inference tasks are getting larger as the networks are getting deeper and deeper today okay so you can see that the memory is also improving in this chip there now they have high bandwidth memory which is much more higher bandwidth and higher power also compared to the dr3 and they needed more chips and you can see that they have complete floating-point operations and their performance is much higher so you can see that the requirements as the requirements go up the chips are becoming bigger and bigger as well as more and more powerful and the memory systems need to keep up as well okay so this is just to show of that basically they're they're doing systolic computation this is really an example of a modern systolic array I'm not going to go through the slide and in more detail but there are some interesting tidbits over here from a correctness perspective software is unaware of the systolic nature of the matrix unit but performance it does worry about the latency of the unit this means that programming this is important so that you can get to get the best Layton sees and this is the larger scale of the system basically you have this matrix multiply unit but there's a lot of other stuff that you need to add to the host processor to make it work essentially you can see that you need to set up the data you need to input it in a regular way so that the data that's coming from here matches the weights that are coming from here these are the weights and of the neural network and matrix multiplication is done and then you get the data back and then maybe even feed that back in so you can read this for in more detail any questions I know this is high level stuff but this is important stuff to to go over okay basically today there are many concepts being investigated and on top of that not just investigated but people are building systems with them like processing in memory actually I didn't put the neuromorphic computing over here there's a research prototype from Intel Loihi for example that's a that's an example of neuromorphic computing IBM as I want your research core types also on that one I chose the previous ones because these are actually essentially commercial people who are people wanted to have these commercialized and that's why there are startups in it the other ones new morphic ones are still there's still time for them to become more commercial but who knows they may become commercial also new accelerators clearly machine learning tests as you've seen graph analytics ask and going forward genome analysis tasks there is a big need for them and clear the new systolic architectures and new memories even though this is the last one over here it's it's also very important because if you look at the tesla chip for example i didn't look so it inside that if you watch the video you will see immediately that 80% of the chip is memory basically computation even though computation is very very important to you to do all of those matrix multiplications they're not occupying most of the area they're not occupying most of the power most of the power is consumed in memory most of the area is consumed by memory most of the reliable teachers are also due to memory so really these are compute engines they're exciting machine learning tasks but most of them are really dedicated to memory as we will see that's why memory is so important when people are really trying to design around that memory as much as possible and I believe the way for scale engine one of the motivations for this big great for scale engine was actually so that you can get enough memory on that chip so that you can you can do the training very very efficiently because if you don't have enough memory if you don't have enough memory bandwidth also your training is very slow and they realize that and I think the reasoning was how can we put enough memory in this chip well if you go way for scale then you have a lot of memory right if you think about a small chip you can only put so much memory in it okay okay basically as I said computing landscape is very different today compared to twenty ten to twenty years ago and applications and technology both demand normal architectures applications we've seen but also technology like the fact that DM technology is not scaling very well or or or or maybe you have reliable tissues you think differently about architectures and as a result today every component and its interfaces as well the entire system designs are being reexamined so if you look at this picture that I showed you earlier we're really reacts amening all of the components today including the storage devices over here including networking devices actually which I didn't put over here but networking is really an integral part of a chip or platform today those are being reexamined today because networking is also getting faster and how do you keep up with the the faster networks it becomes important okay so basically let me motivate a little bit more today this is actually happening I think people are really thinking out of the box with new designs they're trying to revolutionize the way computers are built but they're really doing it by understanding both Hardware in the software all of the things that I showed you starting from 3d xpoint the machine learning accelerators like to all of the machine learning accelerators and and the in-memory India I'm processing they're not just hardware you really need to design the software together with the hardware these are really prime examples of software hardware code designs and if you read the Google paper they will say that the software is extremely important they need to figure out how to design if you watch the Tesla video for example they have built a compiler for this thing and programmers need to program in particular ways so that they can take advantage of their chip so you cannot just assume that you have some hardware that accelerates your task that was the case in 1990s Intel used to build much faster single thread processors and everything magically became faster those days are gone now today everything is about making motor hardware and the software co-designed and faster at the same time so you really need to understand both of them going forward so if you're if if you're designing systems there there used to be a time when you design large-scale systems distributed systems for example and people used to argue that oh I can I can I can take some building blocks and hardware like processors multi-core ships for example and I don't need need to deal with hardware anymore those days are gone basically today whenever you're designing distributed system you take into account the fact that you have some specialized machine learning accelerators you need to figure out how to actually communicate with them if you have a very fast Network for example if you're trying to so one way of designing distributed systems today is your computation notes and you have memory notes this way you can specialize the different notes and maybe have a very fast network between them how do you actually communicate so that latency doesn't become involved neck between computation and memory nodes in a distributed system in a data center setting that becomes important and you cannot ignore the fact that you have accelerators maybe here or there because if you have computing memory engines now things get blurred right you may have computation nodes but you may have you may have the ability to do computation in the memory part as well in the big memory nodes or storage nodes if you're able to do a computation over there now you're the way you think about software has to become different and you have to think differently because if you actually use the use the same way of scaling systems you're not going to get enough efficiency you have you really have to use these accelerators today that's why all of these big companies are building X ray is basically if you look at any company that you think is traditionally software company and I think like Microsoft is traditionally software right Google Facebook they're all traditional software and they're all building their own hardware today and they're all using them actually I didn't talk about Facebook because they didn't really talk as much about their chips but they do have their chips that they talked about as well briefly okay basically today is the time really to invent these new paradigms for computation communication and storage both at the architecture and the system level and I will recommend this book to you this is really I think it's it's a very insightful book written by the philosophy of science scientists who works in philosophy of science Thomas Kuhn it's called the structure of scientific revolutions has anyone read this book okay well I'm really surprised you read the book was this in a class assignment or for fun for fun okay that's great then you probably know this book very well I used to give this book as a prize for students who did really well in my assignments I never know if they read it of course but I'm really glad some people have read it I said I believe it's taught and philosophy of science lectures but don't quote me on it basically this book talks about the way science evolves and I think even though even though we're in a computer science department a lot of what we do well I think a lot of some of you are engineers also which is fine I don't think there's a distinction between science and engineering in the end it's really a spectrum what we do a lot is not just science it's a lot of what we do is really engineering as well because we that's how you really build these systems but what is really described over here for hard sciences like physics biology astronomy these are really hard sciences is a cycle of revolution and the way it happens is basically there there's a certain period in science where there's no clear consensus people don't know what to do okay let's ignore that for now at some point they figure out what to do and they go into this normal science and all science there's a dominant theory that's used to explain things this is business as usual for example the Earth revolves around the Sun right that's very normal science for us today there was not normal science if you go back some number of years right and this is air there's actually story in this book related to that even something as fundamental as that changed because of our evolution essentially people start questioning does the earth revolve around the Sun this evidence doesn't support that and then they were dismissed people were saying no clearly Sun revolves around the earth right and you if you were and essentially people poked and poked and poked which is really their evolutionary science where underlying assumptions re-examined because you start building evidence here's evidence one evidence - evidence three evidence for evidence five the earth does the Earth revolves around the Sun it's not the other way around and at some point there's a revolution and you basically switch and that's how are these revolutions happen and this book examines that I think that that's applicable to computer architecture also for example normal science in 1990s was these auto borders superscalar engines that are higher clock frequency and wider deeper pipelines they're they're the normal science basically they're going to be how we build processors and clearly over time evidence built up against them right the evidence was ok it's very hard to power them you don't get a lot of performance and basically people started poking holes as a result right now I believe we're in the period where maybe somewhere here and here of course it's very hard to without hindsight it's very hard to classify where we are right now but clearly we're not in the normal size today we don't have a dominant paradigm but if there's something that's becoming dominant it's really this AI accelerators but it's too early to tell whether they will be part of the paradigm or not but right now everything is reexamined today and that's essentially why I like this book because it really clearly explains where we are today also but and if you're interested you can read these versions I should put the German version also but I don't I don't know if there's a credible translation I like this translation because it's the one I read okay any questions nothing so far is this boring interesting okay hopefully but no hopefully not boring okay let's talk about some some fundamental so I'm going to be still a bit more high-level but even more higher level because I think fundamentals are even more fundamental than what I just discussed this is really interesting fundamental stay the same across the disciplines as well so let me start with this one probably those of you who've taken my course already have seen this so you can you don't need to answer it does anybody know what this is this entire structure not the train has that has anybody been there yes I write here does anybody want to say where do this what's the name know you've been there it started off on yes it is style - yes and the first time I was there actually I was visiting Zurich at that time before coming here and I really was fascinated by the structure its bond of Stalin or another answer it's really the first major piece of a famous architect does anybody know who that architect is probably yes Santiago Calatrava well I guess this doesn't tell you the answer but but basically why is this different because it's not a straight line thing it's really you can see this description over here straight lines and right angles are rare and it's actually follows some a different paradigm called zoomorphic architecture and this was designed by an ETH alumnus PhD in civil engineering Santiago Calatrava you can read more about him if you're interested then he has a lot of other interesting designs as well interesting and expensive designs I think xeric paid a lot for bond of style - at that time even though by it's not the most expensive train station by any means but still it was expensive but it's new technology and xeric transition to that new technology at that time and nobody cares about his price right now right that's that's that's that's really fundamental and once you transition while you're transitioning to a new technology in any domain it doesn't matter you really have to go through some pain and you need to pay money there's no other way around as far as I know there's no free lunch and that's true for architecture also that's true for computer architecture also if you want to transition to that huge AI chip well you have to pay some money and you also need to go through a lot of pain to make sure it happens and that's essentially how bono Sullivan came about even though it's a very small example there are much worse examples but it's different from this in the end right I know that's not that I would take it that's not punished I'll open this looks like any other Bonoff actually somebody somebody who follows my lectures figured out that and told me that this is not in Switzerland this in Germany and I was fine with that my my my goal was to make the point not to not to make sure that this wasn't Switzerland but I guess if you really want to be specific I should find something and switch them but you can find a lot of these in Switzerland also okay so these are actually relatively small but this is a this is a big one this is another example does anybody know what this is don't be shy I'd like to make this interactive yes oculus yes how many people have been there okay more than two really only that many this in New York I thought more people within New York this is actually at the heart of New York and this another train station actually it's not just the train station but this big shopping mall underneath also and this is also designed by the same architect its Santiago Calatrava essentially it's not as cheap as style open according to Wikipedia this was four billion dollars and New Yorkers had a lot of issues at this some part of some some New Yorkers had a lot of issues and while this was being built there was a lot of controversy and in the end there was not enough money to complete what Calatrava envisioned that he would complete as you will see in a little bit but now the transition to the new technology is done nobody cares about its cost you go there and you enjoy it in fact some of my students who took my courses in the past they send me pictures after going there with this kind of they apparently enjoy it right so basically that's that's again an example of transitioning to new technology it was not easy to transition to this technology and it's not perfect in the end in the eye of the architect at least or in the eye of the other people as we will see also but new technology is there and people enjoy it and the cost is not a concern so let's take a look at this a bit basically so this is katawa according to him oculus resembles a bird being released from a child's hand now I'll let you imagine that over here the roof was originally designed to mechanically open to increased light and ventilation to the enclosed space as you can see but that didn't happen why because there's not enough money another fundamental principle of cost is always a design constraint if somebody tells you that they're not limited by costs they're not thinking about it for some reason either they're lying probably they're not lying but they're they're really not thinking about there's always a limit to the cost that you can pay now if you want to enable new technology you should pay a lot of costs but there's always a limit okay so there's some strength so clearly as with any technology their strengths and praise as you can see may be through this for this person from New York Times says it's a pleasure to report for once that public officials are not overstating the case when they describe a design as breathtaking so there's a healthy skepticism against public officials over here also which is good always but of course there there are design constraints that people had to go through for example this person also from the New York Times wrote in the team in the name of security Santiago Calatrava bird has grown a beach its ribs have doubled in number and swings have lost their interest is of glass you can see a bunch of other stuff over here and then they basically conclude with it may not evoke a slender stegosaurus more than two labert looks like a very interesting I think this is a nice nice critique market actual critique clearly but that's what we do in architecture also a computer architecture also it's a different nature but it's very similar in this case they think this it's resemble this thing and if you really want to know more about it you can read more about the guitars if you appear into these things I'm sure you can see them and a Natural History Museum also okay so clearly there no one is immune to the design constraints basically security is a key design constraint and that's true that's increasingly true in computer architecture today in fact I could have easily started this lecture with security because if you look at the security space even in the security area not the computer architecture area people are fascinated by the hardware security how can we keep Hardware secure because they've there there have been decades and decades of work in terms of security software security and people have figured out that the route of trust really lies in Hardware how do you actually build trust and security into Hardware and people also recently figured out that the hardware we design and we assumed to be secure is really not secure like the meltdown inspector and will roll hammer also essentially if you have a lot of these security issues in hardware today and that's true or that's very fundamental basically that spans across generations and there's clearly a trade-off between security and performance and security and cost and these are trade-offs that you need to deal with in all other domains as well not just in computer architecture another example over here the design was further modified in 2008 to elevate the opening and closing roof mechanism because of budget and space constraints so cost as you can see over here okay the transportation hub has been dubbed the world's most expensive transportation hub almost three point four seventy four billion dollars as you can see over here even then you're the reason I put this over here is even if it's so expensive you're still limited by cost as you can see so cost is very fundamental okay so so whenever you think about new technology always think about the cost but don't be scared by the cost because if you really want to enable a new technology you have to accept that cost is going to be there you just need to manage it so basically all of these are really a complex trade-off space and you will see that the trade-off space is even more complex than what I just described we didn't even talk about the energy we didn't even talk about other stuff related to this building for example we will see in a bit but really you really as an architect you're really dealing with all of these and you're really making trade-offs between different things okay another question probably not many people know where what this is anybody yes yes that's right it's in Pennsylvania it's very close to Pittsburgh actually one hour 15 minutes it's a better view of it because this thing doesn't do justice to it it hides the waterfall underneath it but this one really brings about the waterfall right now you can see that this called falling water it's well we will see the architect in a little bit but you can see that this has some design principle which is really imitating the waterfall that it's on top of and a bunch of other stuff which it's it's organic architecture it's really it's supposed to be in harmony with the nature over here so this also another masterpiece of another famous architect and I use this one because this is really where I used to teach and this is an assignment that I used to give to students and I think everybody enjoyed going there and another architect who was very principled just like college all what the cava is it's a Frank Lloyd Wright perhaps the most famous American architect not alive anymore but and you can see basically based on some lists you can see that this parent is supposed to be one of the 28th places to visit before you die so if you haven't done it yet I would suggest doing it 28 to the small number because the world is large I'm curious how they determine that list because I didn't look at that list okay so your first assignment you're not going to get credit for it but you can go and visit bond of style open and see if you have a different view of it based on what you've just discussed an extra credit if you repeat it for oculus although I understand it's a bit more expensive to go there and you can repeat it for flowing water it may be even more expensive to go there actually you can drive from New York to following water but it will take a while probably six hours and it'd be good to appreciate when you go there the beauty and the out-of-the-box and creative thinking because these things are really creative leaps the way I think of them and there are many examples of this I just pick some examples these are things that push the boundaries forward if people were building train stations just like precedent train stations that we wouldn't have these things deep really thought creatively to actually make these happen and they were persistent to make these happen they were I'm sure they got a lot of pushback in fact they did get a lot of pushback from many many people but they didn't stop even though they got the pushback they also pushed back and they wanted to make these happen and clear there are many many trade-offs in the design of the runoff for example or the other ones their strengths and weaknesses and goals of design it's good to think about this and I'm telling you all this because this architecture is really a mindset in the end and getting into the architecture mindset is very important because once you have that mindset I think it's also a design mindset and I think if you can drive principles on your own for good design innovation too and I think do dates any time during this course if you do it please send me an email I'm happy to look at these emails I actually still have these pictures from oculus from some of my students rational students but later during the course maybe better but doesn't matter I think ok you can apply because later is better because you can we may be able to apply what you learned in this course okay but let's do I think I think this is really critical in architecture today ok so let's do our first assignment today again this is not going to be very long it's very simple basic I want to find the differences of this and that and you've already seen this and you already seen that and it's not going to be a literal assignment basically you can list all of them after you complete the first assignment but you can see that there are a bunch of different lost once you try to find the differences between different architectural designs the first question that comes to my mind is what metric I'm I'm evaluating these things with that's why architecture is a very very metric oriented area you really need to define on what basis you're comparing these things and on what basis can have lots of answers right it could be functionality doesn't meet the specification that's probably the most basic thing the questions who specified it right it's also good to know that a reliable T is it reliable or is going to collapse tomorrow that's not guaranteed also right space requirements how much space does it occupy cost which is different from the space expandability can I expand it is it reconfigurable is it configurable there are many many things related to this comfort level of users happiness level aesthetics a bunch of other stuff right and I got clear didn't list all of them you can define these metrics and also you need to define a methodology to quantify or you qualify these metrics right for example comfort level of users how are you how do you quantify that right that's true for the architectures we built also right this thing over here at least is more usable for me compared some other things how do you get there right again this is designed by many many architects in the end but it's it's it's it's it embodies up pretty much all of this actually and more clearly because if these were the only concerns that I wouldn't use it because I need to get performance out of it also right and that performance is not here in this case because it's hard to define performance for a building I think okay so basically evaluate the goodness of designs always a critical question and you will you will struggle with it during the course of this course and if you go a lower level you will see that there are even other metrics like energy for example you will see clock cycle time you will see throughput there a bunch of other stuff that you need to evaluate things with so basically I think I also like to think about a key question how was College are able to design especially his key buildings right you can have many guesses over here and I'm not going to go through all of these but clearly he is a very hard worker persevere you know dedication over decades and all of this creativity out-of-the-box thinking a good understanding of past is my good judgment the intuition he had a strong skill combination in math architecture art and engineering he was able to get money that's important also and you need to be lucky a bit and all of this the stuff initiative but I think these two are very very important if you have these two then you can perhaps drive a lot of these other things I don't know about creativity but it's really principal designs he was basically principled that's true for many many architectures also that we will cover that have succeeded and that will probably succeed going into the future and there is strong understanding of commitment and elven commitment to fundamentals fundamentals of architecture fundamentals of design and hopefully the goal that I have in this course you will be exposed to and hopefully develop and enhance many of these skills in this course will be very trade-off oriented in the end we look at principles does this design violate the principles for example we will look at principles are reliable too very soon and security and we will always be very close to fajn that even though we will cover very recent research we will always talk about fundamentals that's the really interesting part I think you can always be very fundamental but also cover very recent research like the AI tip that we discussed it makes very fundamental trade-offs way for scale versus chip scale clearly opens up fundamental trade-offs in reliability cost power that we just discussed I already discuss all of those three and you really need to think about them but if you really want to make that happen you will need to stick to the principle and basically follow a lot of these things I think I think this is the job of an architect in the end but let me talk about principle designs this is in college Allah's words basically he says that there are two overriding principles to be found in nature which are most appropriate for building optimal use or material and the capacity of organisms to change shape to grow and to move he was not completely successful with oculus as you can see right because not moving but I don't know about the optimal use of material but essentially he's he's guided by some principles in the design and this is somebody else saying these constructions are inspired by natural forms like plants bird wings and the human body and this another example from Lisbon has anyone seen this one let's go you can see the name and it's another train station but this is this is the blueprint of it it's another example of the ZOA morphic architecture I don't know what these are humans are animals I guess endomorphic shapes doing some stuff and that's the principle over here you can see lists over here and what does this remind you of I think we've already covered this one easily what does this remind you off this is Sevilla Spain in Spanish here where are Spanish I think anyone has anyone seen this one before I seen this one when I was 2 Syria and when I actually went to Lisbon the first time this was the first place I had to go to and I did it's very easily accessible from the airport so what is this remind you of anybody Bert harp yes I think harp is probably one of the earliest answers what else it's supposed to be a pigeon you can see the eye over there right but of course I mean once it's done over there anybody can assign any meaning to it right you can be their credit ok this another quote from the other famous Arctic that we discussed right basically Frank Lloyd Wright was said this architecture should be based upon principle not upon precedent as a result he didn't build this one he built this one and the principle was really organic architecture which we already briefly talked about and you can see that when you go to organic architecture the well-known example is falling water but they're a bunch of other stuff that is based on the same principle I like this place as you can see I've been there many times I have it the Stata law for many times also but it's a bit closer ok so basically I think the major high-level goals of this course is to understand the principles and understand the president's also because you really need to understand both to really innovate and think out of the box and they know such understanding hopefully you can evaluate trade-offs of different designs and ideas develop principle designs and develop normal out-of-the-box designs and hopefully the xperia whatever you do after Ward's you don't have to be a dear architect I think these principles really span the fields if you do any sort of design you will need to deal with this sort of trade-offs in the end and you just you just talk about this focus is really on principles presidents and how to use them for new designs and clearly the focus we have it's computer architecture we could keep going on in the architecture but as we go deeper and deeper my knowledge exhausts very quickly but in computer architecture I can go deeper and deeper and deeper and deeper and deeper and will be here for a year probably but not in real architecture if you want real architecture probably you should take a course from real architects an expert in real architecture ok so let me cover this part first and then we'll take a break basically this is my PhD advisor here Pat and clearly I was influenced by him but this is a slide that I like from his lecture notes he used to have these lecture notes that are transparencies and even into the late 2000s he used to have these transparencies then then he transitioned to PowerPoint and he it was a crosstie transition for him but he figured out that this new technology is very good that's another example of a caustic transition to a new technology you know because how are you going to use transparencies if there is no transparency here we have transparencies here or overhead projectors here so it's not a problem here but if you take your transparencies to some school and they don't have an overhead projector you're a problem right so it's good to make the transition anyway but basically an architect really needs to look everywhere yeah to eyes everywhere if you really want to be a good architect backward forward up and down this is my rendition of that slide over here this can you look backward understand the past very well understand the trade-offs and designs upsides and downsides past workloads basically build a very good understanding and that understanding will enable you to look forward and this clearly requires a lot of thinking and it comes up a experience also builds up and also we're heating this requires a lot of reading because some of these designs don't exist right now right I read a lot of papers from the 1960s for example that talk about very very different architectures it's always good to think about them and maybe reevaluate them today looking forward to the future basically I think this is really important an architect is really a dreamer in the end you can create new designs only by dreaming and by listening to the dreamers you may think about someone who has a crazy idea many people may call them crazy but a good architect based on experience may figure out and distill the key idea over there and take it to the next step basically and maybe enable a way for scale chip actually for scale chips were quite crazy you don't know if they're they'll succeed at this point but some good architects listen to the dreamers even memory processing is another example over here this is a really important being open minded and pushing the state of the art evaluating new design choices I had something else to say over here but maybe I need some coffee okay looking up is very important because really problems come from the top these are problems that people have different algorithms that people develop machine learning is very good example of this if an architect is cognizant of what's going on in the machine learning they can if they understand that problem and their nature they can build these accelerators much better than anyone else for example that's true for other problems coming ahead for example genome analysis could be another one who knows protein folding could be another one right basically you can develop architectures and ideas to solve these important problems look down is also important like if you really want to be a complete architect or you also need to understand the technology what's at the bottom this is the scoobies lab for example phase change memory technology it could be quantum computing technology it could be I don't know mechanical micro electric our director Mike - electronic mechanical systems names technology different types of technologies basically how can you make use of them right understand the capabilities so that you can actually make the right trade-offs and use the right technology right and also you can predict and adapt to the future of the technology right you're really designing not for today when you're an architect you're really designing for n years ahead well you can also decide when you're designing for right of course as n grows larger you become more speculative and your success probability reduces probably right if you're designing for just tomorrow that you know what the work codes are and years later you don't know what the workloads are so you really need to think ahead a little bit then predict and you can enable the future technology and this is also true for technology also not just to for workloads but also for technology like when we were doing the early phase change memory work we were getting a lot of pushback also saying that this technology will never be successful right this was in 2007 2006 at least for me memory clearly it was successful for CD rewritable CDs right but we believe in the technology I mean it our homework who did the scaling studies and we also believe in the technology you got to be lucky also luck is a part of it over here and then we did the architectural work to enable that technology and 12 years later 10 to 12 years later we have that chip 3d xpoint chip from Intel right it's main memory you can buy it so you can use it on your own so essentially I think this looking down is very very important also basic our reasoning was DeeAnn technology's not going to scale you need to replace it with some technology and the technology we picked was really phase change memory at that time because we thought that was the best in terms of the scaling characteristics now if you do that study today maybe some other technology will be even more interesting ten years down the road but that's the architects job is understanding what technology will be good some n number of years ahead okay so let me give you some takeaways yeah so basically I think the first takeaway is being an architect is not easy at least this one you're starting I think it's it doesn't it looks like a daunting task right II need to have eyes everywhere but I think you build up this thing over time it's not just you become an architect right away these creative consider many things in designing a new system and have good intuition and insights into the ideas and trade off I think this is also really important this is a bit hard to quantify clearly having good intuition is really important because now you can actually do pick the right solutions at the right times but it's fun and can be very rewarding I wouldn't do anything else basically although I like combining architectures with many many things like the genomics bioinformatics as we will see and it enables a great future like many scientific and everyday life innovations that we take for granted today we're not have impossible without architectural innovation and they've enabled very high-performance systems like this thing is enabled today but has anybody used the early cell phones in 1990s let's say no probably not maybe most most of you were not born in 1990s we know you're not that young I think but basically this thing is here because we have flash memory that is enabled it's really yeah I mean that's part of the campano the components clearly right there many many of that component but I credit flash memory a lot because that is enabled very high amounts of capacity in these devices true for these devices as well but there have been in my view many many things that went into it this this thing has many accelerators like more than 20x urges in it okay I mean I would talk about this but self-driving vehicles would be the next one and there are many many other next ones maybe genome analysis the machines medical machines that you can carry around with you initially doctors will carry on with them probably maybe later you can carry on with you so many other things so hopefully my goal in this course then able you to become a good computer architect so I hope you're here for this in systems programming or any kind of computer systems course you think about C as a model of computation and maybe you talk about cross there's a little bit but you don't go into depth you really think about the programmers view of how a computer system works in digital design which some of you have taken from me you think about digital logic as a model of computation and you really get the hardware designers view of private computer system works so but but we're in the middle as an architect we're really in the middle actually our digital designs closer to computer architecture also so if you've taken they still design with me then you're closer to this course but if you're taking a traditional digital design course it's all about these two logic you don't go into as much architecture the way I teach the digital design here is really more forward-looking I would say it covers a bunch of computer architecture concepts over here but really in the end you talk you think a lot about hardware designers view so the key question is what happens in between and we're going to talk about what happens in the team they're not we're not going to exactly answer all of these questions this is an advanced course if you take in my computer our digital design course we've answered some of these questions but we're going to be in the middle let me ask the question how many of you have taken a systems programming or computer systems course hopefully most of you how many of you have not taken don't be shy okay I mean you don't need it that much in this course this part so it's okay but it's good to think about systems level issues also what about this one digital logic digital design some sort of computer architecture how many of you have taken it okay many of you how many have not taken it don't be shy again okay okay this one you may need to brush up sometimes because we may assume some things but I think most of the things will be self-explanatory and we're going to redo some of the labs and also so stay tuned for that okay so basically we're going to see a lot of the my architect or micro architect to use the micro architects view how to design a computer that means the system design goals and the choices you make in the middle over here critically affect what's up here and what's down here you basically are really determining the destiny of the software programmer as an architect and also the hardware designer I mean an architect usually is also a hardware designer but that's not always the case so if you look at a big companies like Intel Nvidia for example use the architects design the architecture of the system and the building blocks and there's there are also logic design teams that do the logic design that's what I mean by determining the destiny of the logic design teams basically really if you if you specify an architecture that's not easy to build those logic design teams will basically go like this I guess if I had hair I would do it okay so I'll keep that in mind this is really about software and hardware in the end let's see how many more things that I have and then I would like to take a break okay I have a few more slides if you don't mind and we'll take a or break but we're going relatively slowly for my case so does anybody know about Tammi Richard Hamming Hamming distance yes okay you should know about Richard Hamming because Hamming distance is everywhere if you don't learn about it but this is him and he said that the purpose of computing is to gain insight and I very much agree with that these can be gain and generate insight by solving problems how do we ensure problems are the key question is how do you ensure problems are sold by electrons so this is something that we will see a lot in this course it's really all about hardware and software the stack the computing I call it the computing stack some other people call the levels of transformation for example but you always start with a problem and in the end eh solve the problem at least an existing dominant technology of the electrons in the end everything consists of electrons so you can I think this is safe to say for any technology today now that we know what so basically the key question is how do you communicate the problem to the electrons at least at currently we don't know how to speak to the electrons right I don't know how to communicate with a single electron electron please go from here to here and do this I don't know that so we'll build up a stack basically if you translate that problem to the algorithm an algorithm has some definition which I'm not going to go into in detail but basically there is a very precise definition for an algorithm it's a step by step procedure that's guaranteed to terminate where each step is precisely stated such that it can be carried out by a computer so it needs to be finite ending it needs to be definite precise and it needs to be effectively computable every step should be effectively computable by a Turing machine I mean we have taken a computation Theory course theory of computation ok that's good so you would know what an algorithm is I think computation theory theory and architecture actually very close to each other in the sense that I think architecture is very fundamental and theories of course clearly very fundamental where architecture is a big practical side of the theory you don't exercise as much theory you will see in this course but it's really about the it's it's really about something very fundamental that it okay now you can have many algorithms for the same problem clearly and of course you take the algorithm and translate it to a programming language and create a program and then that gets executed on their own time system which has many components which could be virtual machines operating systems memory management and then it's really start get translated into this instruction set architecture a set of instructions it's basically this is really the interface or contract between the software and the hardware at this point that's why it's yellow over here it's really what the programmer assumes the hardware will satisfy we're not going to cover as much of this in this course we covered a lot of in the digital logic course it's an interesting area especially thinking about new instructions that architectures for these accelerators are really really important today how do we make them easily programmable how do you actually actually thinking about the whole stack where these new accelerators are really important but in this course we're not going to talk a lot about instructions at architecture and then there's the microarchitecture the canary levels which is really the hardware side let's say it's really an implementation of the instruction set architecture and there could be many implementations of the same instruction set architecture you can have an add instruction for example that could be implemented thousand different ways again which we're not going to go into that you've seen that in digital circuits if you're taking it if you haven't seen it that's not also important the level of abstraction we're gonna have here is really if you have an adder and we can add with it right okay and then mic architecture gets implemented in logic and then these are basically the building blocks of micro architecture the gates and gate NAND gate or gate and you know that if you have a NAND gate that's universal essentially you can translate any boolean function to be implemented with NAND gates or nor gates and then you implement with some sort of devices right these are different types of transistors for example which we're not going to be definitely not going to go into in as much except in some memory parts we'll talk about briefly okay so the focus we have is over here essentially in this course but we're going to touch upon these parts as well and I think whenever whatever we discuss actually I should probably expand this we're going to touch more about these parts in this incarnation of this course but you should always think about what happens over here with the machine learning accelerator or genomics exciter for example how do you change the algorithm to take advantage of the hardware not just that but how do you design the algorithm in the architecture microarchitecture together and the ISA in between so that you can really solve the problem which is really what the goal is in the end right you really trying to solve the problem how do I make machine learning training thousand times more efficient than today well you develop the algorithm and the hardware together essential that's essentially what's happening in the field today okay listen aside I like going through these asides because some people didn't talk about Hamming but basically this is having fundamental work if you're interested I would recommend reading it this is where he introduced the Hamming distance and error correcting codes and Hamming distance hopefully most of you know it's really the number of locations in which the corresponding symbols of two equal lengths the strings are is different for example if I take let's see if I have a good example over here I don't have a good example over here you think number and length these are six character strings right and the Hamming distance between them is six because they don't match in any of that characters but if you had if you had number and numero then they would match in and um so the having distance would be only three now this is an example okay we'll talk about edit distance later on which is different number and numero will have a different edit distance compared to hamming distance and you can guess what that is probably right actually they match in a lot more places that then you think if you if you're able to shift it one of the strings to the left or right okay we'll talk about that but basically this worked a lot the theory of codes that is used for error correction and detection it's very fundamental and a lot of the work that is built on air correction has built on this one we'll talk about error correction in memories later on and you will see that a lot of the concepts are built on Hamming distance and also I would recommend especially if you're interested in doing research the masters or PhD level this is a talk that was delivered by Richard Hamming at Bell Labs when he was - - all of his colleagues on how to do research this is principles on how to do research I definitely recommend this one here's a video also but I think the transcript is better than the video itself it goes over a lot of principles on how to do good research ok so another loads of transformations this it's always good to construct they deconstruct what you've seen I think this clearly now I broken this such that the user is an integral part and I think going forward really to think of it that way user could be anybody right user could be someone who has a problem someone who's designing the algorithm someone who is designing the programming language or program or doing the programming itself someone who's designing these systems or directly interacting with them someone is designing the ISA and directly writing code to it and it could be also someone who's designing the microarchitecture right or may be user needs to interact with the microarchitecture in some way which is really interesting I think so basically it's good to deconstruct these levels of transformation and think about how do we incorporate user into the into the stack because in the end you're designing computing computers for someone and that someone is really the user it could be human user it should be a machine user or it could be some other user I don't know but in the end you have to really specialize and customize for the user okay so let me talk I think this may be a good place to take a break actually what do you think let's take a break for twenty minutes okay I think it's time to continue is there ready awake we're getting there cool now we haven't lost as many people as I thought we would lose but let's see okay so we've covered the transformation hierarchy or levels of transformation or the computing stack let's say and I'd like to point out that this actually is important in in some other way which is really from the abstraction perspective these levels of transformation create abstractions when we drink the Sibbett basically between the layers between or between the levels so and this is useful in the sense that a higher level only needs to know about the interface to the lower level and not how the lower level is implemented so you go from programming language to is a let's say you just need to know the interface right you don't need to know what they are how I of the ISA is implemented you go from you don't need to know over the microarchitecture for example the lower level details are abstracted from you at the higher level now that's a good thing for example another example is high level language programmer does not really need to know what the ISA is and have a computer execute instructions this is great for productivity no question about that no need to worry about the decisions that are made in the underlying weight levels somebody else made them or you made them at some point what you forgot about them now you're programming the device rate for example programming in Java versus C versus assembly versus binary versus by specifying the control signals for each of each transistor every cycle which one do you prefer probably not the last one all right you don't want to really program every single transistor in your machine you really want to be at here in fact Python right you really want to be at a higher level so that you don't need to deal with a lot of the stuff that goes underneath you don't want to assembly for example so this is good for productivity but if you want to get performance of course there might be a different issue right if you really want to get very high performance maybe you want to go into assembly great maybe you don't want to do binary because it's really a translation binary is basically ones and zeroes right there's really no useful exercise going from assembly to binary except somebody needs to do it and that somebody is really the compiler right but between C and assembly there's a huge difference clearly and between Java and C there's probably another huge difference so the question is then why would you want to know what goes on underneath or above and I've already given your reason right there productivity abstractions great for productivity but if you want to get performance that's one reason to go into what happens underneath or above so basically why do we want to cross the abstraction layers and the chips that I've shown you earlier to motivate this class they all break these abstraction layers basically if you're a programmer of that chip you cannot be you cannot say I don't want to know what's going on in Hardware you really need to know what's going on in Hardware already there and that's how they get the high performance why because they have a problem basically as long as everything goes well maybe not knowing what happens underneath is okay but once you have a problem then you'd rather know about what's going on underneath I give this example over here right this thing right now it's going well it's recording nicely its operating fine if it's not going well what do I do if I knew exactly how it was implemented underneath I could potentially fix the problem right but I don't so I'm clueless so I had to call someone who knows what's going on underneath in this interface it's very fundamental again with any kind of design you run into this issue and in computing you run into this issue also so there are many kinds of problems performance problems whether to program your alters running slow correctness problems what if the program you wrote doesn't correctly you think it's correct but it doesn't run correctly so what's going on right maybe it's implemented incorrectly underneath right maybe the is a specification is not something that you expect right but to be able to know that you really need to know what goes on underneath and this happens a lot to adjust your system just sat down and you have no idea why I think this should not happen but it's happening how many of you experienced this problem it's good to know I'm not the only one thank you and someone just compromised your system and you have no idea how basically all of these are problems and there you can list many many of these things right and you can solve these problems only by knowing what's really underneath and also at the lower lobe these are higher-level problems and the lower level and the hardware whether it's the hardware you design is too hard to program there could be programs over there also if you don't know how the software is supposed to use it or if you don't provide a good interface these essentially you may design some hardware that's useless and we've seen many of these cases in in real industry actually if you don't design that a chip ai chip a machine-learning chip so I said usable by your programmers to begin with then you have a problem right I'm even very large companies have difficulty in getting a very different hardware adopted in real life and I'll give the example of Intel Intel has been a very innovative company for decades and decades and when they were transitioning from a 32-bit is a to 64-bit is a they wanted to take the opportunity and they wanted to completely change the instruction set architecture maybe it squeezed that x86 is good but we want to completely change it to by taking the opportunity to go from 32-bit to 64-bit so they completely redesigned the ISA to be 64 bits and they put a lot of burden on the software compilers this is called IA 64 i'm you know how many of you are familiar with i-864 okay so we all right good I'm here we programmed in is 64 nobody and nobody is programming is 64 today well not nobody about very few people and Intel basically put a lot effort and a lot of money into this new architecture is 64 which is actually really nice I say that in many respects maybe not in all respects but in all in many aspects it's very nice I say but it didn't succeed why because well I don't wanna say too hard to program but maybe it was too hard to adopt for many people it was really changing everything all the software bays that used to execute Nexus 6 now he needs to be moved to is 64 even a company like Intel could not pull it off well what happens you have a competitor AMD at the time they did not make that design decision they said we're gonna make the minimal changes to the instruction set architecture so that we're going to add support for 64 bits and they called it x86 64 as opposed to moving code by a 64 and everybody adopted that easily including Intel so em these solutions now everywhere right now so basically this is an example if you make things too hard to program or if you don't compensate for the fact that things are too hard to program your hardware may not be useful as all of the Itanium hardware is not used right now as far as I know it's used probably somewhere and for some things but it's not general-purpose it's not what it was imagined well what if the hardware you designed is to store because it doesn't provide the right primitives to the software software is doing something and your hardware doesn't support it there's another example if you for example the early how many of you know the term risk our ISC ok reduced instruction set computer right now it's very popular with risk 5 right but the early risk philosophy is make the hardware as simple as possible and ensure that the software and software handles everything like high performance that's the philosophy that's a very principal design actually it's a good principle but you need to be careful sometimes when you how much you push the principles so this risk philosophy the current risk engines are very different by the way this quiz philosophy initially what they said but if you come from this philosophy from a very pure perspective then you eliminate a lot of instructions right for example why do you need to have a multiplier in hardware you can have ads and shifts right and basically the software converts multiplies into ads and shifts and you don't have a multiplier now good idea that idea if your program is doing a lot of multiplies a terrible idea but if you're executing all of these ads and shifts many many times to do a single multiplication but if you had a hardware multiplier now you have the right primitive expose to your software which is supposed to do a lot of multiplies right so those risk engines the earlier with the engines that didn't have multiplies quickly we averted that design decision okay we might want to add some more complexity and hardware to have this multiply so there are a lot of examples like this and if you're a programmer and you want you're trying to figure out why your multiplies are running slowly you may want to know what's going on in the multiplier right especially in a systolic architecture like the like the machine learning accelerators that we've seen a lot of them are systolic architectures as a programmer you have a big role in orchestrating how the data arrives at the functional units and how it gets processed you don't basically want to waste cycles that's exactly what Google people are saying the programmer doesn't need to be aware of this matrix multiply unit but if they're aware they can reduce the latency significantly that's essentially related to this okay what if you want to design a much more efficient than higher performance system which is essentially exemplified by all the things that we've discussed all those machine learning accelerators then you have to really cross these abstraction layers you cannot say I only know about this layer and I don't care about the rest when vice versa for some other a layer that's not how things are today okay basically hopefully in this course we're gonna cross these abstraction layers a lot over many many examples and the hope hopefully this will enable us to better understand how the processor take this processor broadly it's not just the CPU it's also the memory it's also the interconnect it's also all of the accelerators related to it we don't do a good name for it I think maybe platform it's the best name but basically how this works underneath the software layer and how the decisions made in the hardware affect the software in the programmer hardware and the software software interface I should say over here and hopefully this will enable you to be comfortable in making the design optimization decisions that cross the boundaries and we will look into software quite a bit also prefetching is one example where hardware software cooperation is very much needed today in my opinion ok prefetching is to provide memory latency you have a long memory latency you don't want to pay that memory latency for every single memory access so what do you do you have an automated mechanism that brings the data into the processor before you actually need to access it and that's employed that's not it if you think about it's not a hardware so concept it's not a software concept it's a concept it's it's basically something that is there you can implement in hardware or human to learn the software and if you I would like you to approach the course this way also anything that we discussed here can be implanted in software or hardware it's it's really amazing how much you can push the software down by making the hardware as minimal as possible in the end what you really need is just transistors right and push the potential you can make the transistor and software also but I haven't figured out how to do that yet as long as you can you can control all of the values that goes into the transistors and software that's the lowest level of software you can get in software so it's it's it's good to have this mindset that way you can actually figure out where you push to put the hardware software boundary ok ok let me give you some examples in the remaining time on crossing these boundaries these are going to be based on my research which I know best I think and things that I'm fascinated about this is actually an old old multi-core chip is AMD Barcelona in 2006 I had this picture and you can see that the chip has course some caches these are actually private caches they're also l1 caches inside the core and there's a shared l3 cache over here and there's a memory controller there's a memory interface and there are memory chips over here as you can see I still use this picture this just thirteen years old but another take away is not much has changed from the system perspective today but we're going to talk about some revolutionary ideas later on how we can change the system and basically at the time we were thinking about these multi-course we were thinking the trend was actually having many cores on a chip and this chain is already there I think we've already seen these chips actually some of these chips are many many more courses you've seen in the in the cerebral chip they had they have 400,000 course now the question is what is the core of course that's always a problem like I put quotation marks inside the cores over here because that midians definition of a core is very different from a MDS or Intel's definition of a core these are very big powerful high-performance hefty course whereas these are very very simple course that's why it's good to take this core with a grain of salt but basically the reason to have multiple cores on a chip is because it's hard to scale the performance of a single core in general I think it's also very fundamental it's a lot easier to stamp out multiple simple course and this is simpler and lower power than a single large core if you can paralyze your program really well that goes to a simple course you can also get better performance and better power efficiency the key is of course if you can paralyze your program really well we're going to get that get to that later on when we talk about heterogenous architectures we're going to talk about bottlenecks and programs like when you do a multi-threaded programming you have a lot of bottlenecks in terms of some threads may be critical threads and they may be delaying other threads in that case you don't have perfect parallelization right that thread is blocking all of the other threads because it's holding a lock and it's doing a long lake since the operation on that lock on that shared data structure now how do you handle that if you have these cores that are very simple only one core would be making progress and everybody else would be waiting for it right that's an example of where multi-core doesn't work really well so if your programs are doing that a lot maybe should not be designing a multi-core system right anyway I'm digressing a bit but these are the sort of trade-offs that you make when you design a specialized architectures but this was a trend in starting from 2002 to even right directly this multi-core is going on but it's it's a little bit leveled off in the sense that people realize quickly that having these homogeneous multi-core systems where every core is the same is not going to buy us very scalable performance into the future because software is not very easy to paralyze in many many domains there are cases where things are used to paralyze like GPUs but even GPUs want to become more general-purpose as they want to become more general-purpose you want to put some more specialized engines for example today's GPUs are able to do a very very high performance machine learning training but they're not doing a lot of that training inside the GPU core itself they have specialized accelerators I don't remember what they were called mpu is probably something I don't know what Nvidia calls them but some sort of neural processing units let's say they're specialized and you can use that yes ok tensor cars yes thank you yes there's so many different names for these things TPU is Google's the intensive course than videos name yes so you need specialization insight this chip also that's true for all of the other chips as well ok but I'm going to tell you a story about multi-core right now so forget about all that specialization if the problem becomes worse with specialization actually so basically the thinking was you have a very heavy parallel processing on a single chip you get much faster applications and get newer applications and this was true actually this became true over time the GPU is actually enabled a lot of new applications for example now maybe not not multi multi-core maybe was not as successful as GPUs in the sense but GPUs are fundamentally multi-core and they have a special nature of multi-core which we will cover in this course also ok so if you if you have many cores on a chip butty really ideally want is n times the system performance with n times of course right as you grow the number of course you should get exactly linear scaling now later we will cover cover multi processors at some point in this course and you will see that you don't normally get linear scaling and if you get super linear scaling meaning that if you get to end the performance with n times of course then you should always question that result how did this happen right it could be because of an unfair comparison maybe your baseline is not good or it could be because you're not just scaling the number of course you're also adding more memory to the system usually that's the case actually usually it's it's 101 or one over the other and if you find a case where it's neither and something else let me know there's another thing that's very fundamental you either have an unfair baseline or you have you're adding something else okay so where do we get today this was the question that we asked when we were very interested in doing the students was the first work that I did actually when I was at when I started Microsoft Research I was thinking about how do we actually scale multi-core systems into the future and we had these multi-core systems that are coming up from Intel and AMD at that time but I diem also and we wanted to understand the bottlenecks that we have in the system and we thought memory was a big bottleneck and you will see more of that starting from later today or tomorrow but this is one study that we did on an Intel Pentium D for example Intel Pentium D is a great special device at that time it was not it was really literally stamping to Intel course next to each other and not much not adding a whole lot else that to handle the multi-core issues and we ran two applications in the system is just one example one applications MATLAB and the other is GCC some of you may be familiar with both or either I use both in my life I don't know if it's a good thing or bad thing but I like MATLAB I like math lot better than GCC actually that doesn't matter what I'm going to show you should not happen if you like MATLAB basically when you run these two applications together in two different courses a two core system and when you compare the performance of each application compared to NS run alone on the same system you get this result MATLAB slows down by seven percent GCC slows down by 3x so when the two applications are running together one application slows on a whole lot more compared to the other application and we call these the unfair slowdowns essentially and the key and so let's assume that you're a software designer you run this application on a cloud and the cloud somehow schedules MATLAB and GCC together on the same machine on two different cores and your MATLAB slows down significantly what do you do right you have no idea if you don't know what's going on underneath you have no idea so it's a very good example of crossing the abstraction layers and so we actually went ahead and as a software designer you may think in the operating system I will reduce the priority of this and increase the priority of this somehow and if you do that you get the same result nothing changes why because you have only two processes in the system and the operating system schedules both of these single threaded processes on the two cores simultaneously priority doesn't matter at that point because priority matters if you have more than more processes and the number of cores and then how long you allow each process to executed in each core is determined by the operating system now if the operating system has two cores and two applications it basically says I don't care about the priority I'm assuming that whenever I put the application each application on a core they're going to make progress but that assumption actually violated in this case this makes much faster progress than this one so there are a lot of assumptions that over here the operating system is assuming that whenever it puts a process in a core it's going to make progress this actual is still true but that's actually that assumption is true but the operating systems are still not as much aware of what's going on in hardware there is they still assume that whenever you schedule a thread they're going to make progress but that thread may be waiting for memy may be getting delayed in some place only when you have an i/o request to to the to the disk then you get these scheduled but with the smaller Layton sees like memory latency you don't get these scheduled the operating system has no idea what's going on after it schedules the prosthetic or this another a place where thinking needs to be done the operating system architecture interface actually that's a place where that's that's very much ignored even today in my opinion you see all of these accelerators they're very specialized but they're really bypassing the operating system in a sense and the operating systems are not really improving as much because they're there the the disconnect between operating system and architecture is not really being closed okay so I've given you this result let me call this the memory performance walk as you can see this was published the music security it's a security paper and when I presented this work in using security I was in a session called low levels now security conferences actually want all the low level stuff because they know that the hardware security is the prime place to look into ok we call this a memory performance org and we said that you could actually construct an attack like this and we actually construct a much worse program than MATLAB as well but you can read the paper for that so there are multiple questions over here some of which we kind of touched on can you figure out why the application slow down if you don't know how the underlying system and how it works I assume the answer is no unless you have a counter example they can figure out ok can you figure out why there's this parity and slow down if you don't know how this is makes use to programs basically there is a disparity in slowdown so it's not just both of them are slowing down equally if both of them are slowing down equally then probably have better guesses right maybe somehow they're getting delayed and they're getting delayed equally because they're interfering with each other we'll see you the example so and then the next question is of course once you figure out what's going on can you fix the problem but it's very hard to fix this problem without knowing what's happening on their needs you really need to know what's going on underneath of course I haven't given you enough information right maybe I'll give you maybe I'll ask you the question why is there any slowdown you think so you have two cores and they're executing to applications ideally none of them should slow down right but we're seeing slowdowns one of them is flowing by 7% the other slowing by 3x any guesses yes that's right so that's one hypothesis right they could be sharing memory meaning that whenever one is exiting memory the other one cannot access it as a result it could get delayed they can generalize that to main memory caches basic any shared resource that they have they could delay each other okay any other guesses any other reason why they might be slowed down okay that's right so that's the interference maybe you're answering the second part yeah we will get back to that hold that thought there could be other reasons for slowdown if you know the system right maybe if you're actually running two programs at the same time you're putting too much burden on the power infrastructure and the hardware or the operating system is traveling one of the cores or both of the course right it could be possible the system is not designed for full fast execution of both programs at the same time that could be another reason it's not the reason in this case but there could be reasons like that whenever you get slowdowns you make eye contact you look into power and thermal issues and today dynamic voltage and frequency scaling is employed very heavily and aggressively in processors and that might be a reason yes yeah exactly that's another resource shared resource interference problem I agree although these applications are not doing that GCC is very local MATLAB is very local nicely basically what you're saying is there's interference from some other software meaning some operating system tasks are getting scheduled and they're delaying yeah that's certainly also possible actually in this case it was not happening could be controlled and make sure that that doesn't happen but yes that's certainly possible also these a lot of these things that you're discussing are really about shared resource interference yeah some shared resource and the applications are interfering with each other or somebody else is interfering with these applications and power is also a shared resource marrying multiple applications together even terminals are a shared resource when you're running multiple applications together you're really putting a lot of a lot more burden on that shared resource as a result somebody else is throttling you okay so that's that's why there it could be slowdowns but what about the disparity in slowdowns why is there a disparity no your answer I think makes sense because the program's Mart may be behaving differently right in terms of how they access to shared resource for example if there's a shared cache one application when it's running alone it makes perfect utilization of the shared cache it gets very good locality the other application it doesn't the cache is useless for that application now when they're running together the application that has very good cash flow Cal T gets kicked out from the cache because the other application is behaving badly let's say that B in the sense that it's very memory intensive its kicking out blocks of this application that's not benefiting from that as there is out this one is not slowing down because the cache is useless for it anyway but it's kicking out blocks for this application that could make use of the cache if it were running a lot then you get a disparity in the slowdowns right basically the disparity happens because applications benefit differently from the shared resource and whenever they're interfering with each other you're really causing trouble to the application that benefits more if it were running alone and you're the other applications not flowing down as much okay so how do we solve the problem if we don't want that disparity hold that thought for now let's talk about this particular case but before we talk about this particular case I'm gonna give you why this happens in this particular example but all of these are actual potential examples whether before we go on can anybody think of what else could be the reason for disparity and slowdowns yes exactly that's exactly the problem over here but yes basically they both benefit from the shared resource in that case what one is getting delayed too much because the other ones very aggressive right that's a bit different from what I just discussed earlier about the cash in the cash one of them is not benefiting as much the other is benefiting and when you put them together that amplifies to a significant disparity but here both of them should get the resource and benefit from the resource but one of them is delaying the other one because it's very aggressive that's a very good point any anything else okay so you may wonder why this is important basically actually this is important in many many reasons but I'll give you some reasons basically you want to execute application a parallel in a machine in multi-core systems you want to consolidate more and more for efficiency if you don't want to build a cloud for example you want to build hardware with lots of course and you want to ensure that the the application you want to be able to schedule many applications in those cores you want to make them unutilized you want the cloud to be utilized essentially mobile phones another example if I have 10 cores over here I want to be able to do stuff on those cords right especially if I have multiple stuff to do now I'm not to be able to listen to some music for example while I'm doing well my virus checker is running while I'm doing email dot dot dot I'm limited by in the end my interface not not the cores over here and the interface out of this that's also important basically that's a totally different subject but the interface between the human and this phone is not very great perhaps today and that's another reason why things may not be as utilized underneath right okay but these we want to do many more things at the same time that's true for automotive actually there's a huge push in automotive systems to actually have a single chip doing everything related to a car if you don't design that single chip today the different chips do different things one chip does the breaking one chip does the DVD control one chip does the driver assistance one chip does something else and there in the end they're more than 100 chips I think somebody quoted 300 at some point I don't know if that's true but more than 100 chips in a single car today but people are trying to consolidate more and more so that they have more better control and maybe better predictability and easier software as well so there's a push for consolidation automotive systems too I think that's one of the reasons why Tesla is trying to build its own chips as well but we want to mix different types of applications together also and those requiring quality of service guarantees an example could be this right you may you may want to detect pedestrians really easily and quickly you don't want to make mistakes over there but some other ones may not be as important right your DVD player who knows and there may be other ones that are even less important right so basically you want you want the system to be controllable in high performance that's why you don't want these huge disparity and slow down slow laws may be unavoidable as long as you can control them right we're going to talk about that also basically can you actually design a system with shared resources with applications with different requirements and satisfy the requirements all of all of the applications at the same time clearly if you keep putting lots of applications at some point you're not going to be able to satisfy the requirements of the applications because they are limited resources but how much can you push the system such that you can put as many applications as possible while keeping the controllability controllability meaning that I want this application to be slowed down by no more than this some some some performance guarantee basically and doing it for all applications in the system while and if you if you are able to do that then you can be high-performance and controllable at the same time ok so let's go to this particular example as I said this was Intel Pentium D although we replicated the result for AMD and different processors as well but let's take a look at this system if you look at this system you have cores yeah private l2 caches you have some interconnect and you have a shared memory controller and shared en banks essentially the memory system is shared and part of the interconnect is also shared actually and if you run MATLAB and GCC this is what happens MATLAB has a lot of requests GCC has one request once in a while just like you described and somehow the memory controller prioritized matlab's requests like this and GCC the poor GCC doesn't get its request service for a long time and we identified the problem as unfairness in the memory controller so this is really because the memory controller is really not aware of these cores not a varial of the fact that requests are coming from the different course and as a result its policies are unfair let's take a look at that a bit so we're going to me we're going to give even deeper now we talked a little bit deeper whipped up the system and identify the problem to be here now what are the components of the problem why is the memory controller being unfair that's the next question so let's take a look at table it's single d n bank operates and then you can generalize from to the multiple dmx over here but if you look at a single d n bank it's a two-dimensional array of cells columns and rows and it's an abstraction also internally actually there is even a lower level but we don't need to go into that right now in turn the embankments azure many cells and other structures that enable access to the cells but this is really my abstraction over here in fact internally there are smaller areas also which we're not going to go into at this point we may go into later but if you look at a DRAM chip internally you have a row buffer and the data first needs to be brought from a row into the row buffer so that you can read it initially this row will feel empty let's assume that and let's assume let's take a look at how things are accessed let's assume that you're trying to access some address address is broken down to into row or column it's two-dimensional essentially you need to first activate the role supply they're all address to the DM chip and the DM chip essentially activates 0 which means that it means the data in the entire row into this row buffer that row can be like two kilobytes 8 kilobytes we'll see that later on now now the data is in the row row buffer now you the memory controller can send the column address to get the data out of the row buffer it basically sends the column address 0 and the DM chip selects the right bytes or the swab offer and multiplexes them out through the selector or multiplexer and send to the memory controller that's how you access road 0 activate your 0 and then we eat from columns you okay this is good that's the basic operation now we've gotten a little bit lower level right of course we could go even more lower level and look at the exact structures of these but it's not necessary in this case right you've seen how this multiplexer and how this decoder and how the cells are constructed in digital circuits for example that we're not going to go into that detail here let's assume that the next axis is 2 rows zero column 1 let's take a look at what happens from 0 is already in the row buffer right that's the realization meaning that the memory controller doesn't need to send draw address again because what's the point they're always already there the memory controller needs to only send the column address and this phenomenon is called a row buffer hits because the data that you're looking for is inside the row buffer so it's a hit so this is really a cat essentially it's really really cash the data from here to here okay again this is an abstraction level you can call it a cache but really the way it operates it's really also not a cache it's really gives you a caching effect okay so another memory killer needs to send only the column address so there's a much faster request you don't need to wait for thorough address there you just send the column address and the DRAM chip multiplex is out one of the day dollars sounds good right now let's take a look at the next address next address is to same row column 85 now you know the drill basically it's again a row buffer hits row is already in the row buffer in the bank and the memory controller sends a column address and the DRAM chip sends out the appropriate column through the multiplexer now these two requests are much faster than this request now let's take a look at another request this is to Row 1 column 0 it could be from the same application or some other application doesn't matter now the memory control see is that your zeros here so it's not the data at once each one is looking for robot now it needs to do an extra operation in DM it's called pre-charge meaning writing that let's call the robot for conflict first of all conflict in the sense that you are looking for some row but some other row is present in this row buffer so we need to resolve the conflict and get the right data so how do you resolve the conflict you pre-charge the array abstraction is writing back the row buffer back into the cells but that's really an abstraction again the way it really operates underneath which you will see later on this row buffer is a really fundamental component of DM it's there's no writing back it's really a feedback loop that's formed between the row and the row buffer and they basically when a row buffer is activated you have this continuous feedback loop anyway now you'll see that later on now that's the first step now the row buffer is empty now we to open row 1 so you need to send an activate command to row address 1 and the DM ship activates that row bring the data into the row buffer and then the memory controller can now send column address 0 finally after opening the appropriate a proprietary in the row buffer and the diem ship sends Colin zero so clearly the throw conflict access took much longer right than the show hit axis it took also much longer than the first axis over here because the first axis didn't need to write the row buffer back pre charge the array meaning prepare theory for the next axis okay so that's the realization now we realize something different basically the row buffer behavior is different when you have hit Rob all four hits versus conflicts role conflict memory access takes significant longer than all hit access and it turns out controllers that are designed for single core systems sake take advantage of this fact what they do is they use the scheduling policy called first ready first come first serve they base the prioritize or hit memory access assume that you have a queue of memory axes that you're waiting to service and which one do you select one option is to select the oldest one right that's first-come first-serve that's really not scheduling in the end that's what you take whatever comes first I mean it's a form of scheduling but they're really not to spending time to think about what you're doing but in this case you're explicitly trying to optimize for the fact that the row buffer is acting as a cache and you're trying to minimize maximize the utilization that you get out of that row buffer right you work very trying to maximize the row buffer hits that's the priority so the accesses that hit in the row buffer hour prioritize over others assuming all else is equal you prioritize the older accesses or others for example if there are two accesses that hit in the row buffer you pick the older one that's the tiebreaking if there are no access is that hitting the row buffer then you pick the older one of the knoll accesses that didn't throw over so this is a very simple policy they're gonna reconstruct that later on even more this process is actually reasonable it tries to maximize diem throughput where I make taken right to the overall go for it also minimize the latency is actually by taking advantage of offer but fortunately there's a problem and the problem happens when you have multiple applications share the DM control if you have a single pet single application it's not a problem for the most part although even there there are some issues because memory controller turns out it's a shared resource between a core as well as an i/o device so whenever you're accessing the memory through a core and i/o device may be accessing memory also through the memory controller so similarly she exists in that case also that's much less of a problem because you're not running multiple applications at the same time iOS all not always constantly trying to access memory but that is a problem as well so keep that in mind so why so the problem is even worse when multiple applications share the DM controller and the arm controls are designed to maximize the I'm data throughput as we've seen as a result of this policy the arm scheduling policies are unfair to some applications so Robert and there are two types of unfairness this you all hit first I've already given you the answer otherwise I would ask you but I would ask you how we solve the problem later on the Rohit first unfairly prioritize application of high role before locality imagine application that always sits in the row buffer and you're employing a policy that's wrong at first clearly you're prioritizing that application if some other application is not hitting in the row before it's getting a lot of roll conflicts you're not prioritizing that in fact you're starving that application and that's why this is not fair but there's a another aspect of unfairness in this policy even oldest first don't there ignore this one Rohit first you don't take advantage of the row buffer you just do first-come first-serve you think this is a fair policy but it's really not a fair policy because if an application is memory intensive it's generating lots of requests at the same time to memory and another applications not memory intensive miss generating let's say one request once in a while so that poor request is getting delayed because this application floods the memory system essentially so even if you just have oldest first you're impaired again and this exacerbates the unfairness throw it first does that make sense any other unfairness issues in there five CFS is a very simple policy but okay I don't think there's anyone any other ones now this program becomes more complicated if you can start considering rights reads some other requests like prefetches we haven't even talking about rights and reads those actually become more hairy because the each handle writes and reads differently whenever you're writing into DM you cannot read from the end that leads to this from the same rank so there's a problem over there meaning that if one application is extremely write intensive and other applications not so write intensive you may have a similar issue over here so the problem is actually much more complicated than at what I what I'm describing here but this is really the fundamental part of this policy okay so as a result of this unfairness the memory control is really vulnerable to denial of service attacks so you can actually write programs to exploit the song fairness if you know that you're running your program on a cloud and for whatever reason you want to be malicious just write a program that prioritize itself knowing this policy right now maybe the attack surface is not maybe cannot steal stuff with this but later actually people show that you can time how often you're getting row buffer hits and row buffer conflicts so you can write an application that could evict the rows from some other applications that you're running together with and it can time how often it's getting row buffer hits and misses so you can guess which rows are being accessed by that application and based on that you can infer some secret keys it's fascinating I'd recommend you to take a look at it maybe I'll reference that paper maybe John yes yeah all of them people are able to solve those yeah that's true for I mean you have all of those problems with spectra and meltdown and everything right go hammer as well basically although those layers basically a good hacker can break those but so this could be some other security problem in some other way because the fact that they're the problem is really fundamentally interference because you're able to predictably call the interference to some other applications that you're running together with you can induce these row buffer conflicts in that application and based on the patterns of robot for conflicts that you're getting you can guess which parts of memory that application is accessing and based on that information you can guess what is the secret key that is manipulating of course there's a lot that's involved in this attack but people have shown that these attacks are probabilistic aliy possible just like spectra and meltdown are probabilistic ly possible okay so that's a different type of attack that's called by the same fundamental interference problem in this case the attack is less severe in the sense that you have a performance attack it's a denial of service attack you can write the program to exploit unfairness and this is the program that we wrote to exploit that unfairness basically in the paper that we published in 2007 this is a streaming application essentially it's very simple as you can see it's copying one area to another array large array and it's doing this in a streaming manner basically it's going through elements sequence by sequence and we ensure that every element accesses a cache miss you're always going to memory by making the size of index or elements the same size of the cache line cache block so that you're never getting hacked caches so you can see that this very sequential memory access very high robot for locality and very memory intensive these gary access the cache miss over here okay so this is the performs hawk whenever you do studies scientific studies it's always good to have a control application and whenever you want to construct a control application you want to be you want it to be as similar to this as possible and that's essentially what we did we also want created this random access application essentially it does exactly the same thing copies this area to this other area but it doesn't do it in sequence order it doesn't thing I random order it selects the indices randomly to copy otherwise it's exactly the same of course when you want to do the study you cannot do it exact like what I described this is this is also an abstraction this is of the code that we really write because if you do this study the way I just described over here you'll find that this is much much much much much much slower compared to this one can anybody guess why yes exactly you have a random function call right and this one clearly doesn't have that random function call and this random function call actually takes a long time because random number generation is not easy and in these systems so basically if you really want to do the study you do it differently and the way it's non differently is you pre generate the indices earlier and you have an index array also that you need to access such that this thing takes the same amount of time this as this thing over here now you can do that and then you time it of course starting from the beginning of the for loop to the end of the for loop okay that's some experimental methodology for you but basically if you do it right this has random memory access very low over locality and it becomes similarly memory intensive if you don't do it right it's not similarly memory intensive it's very compute intensive over here memory Slavonic but not is not as similar as this one but if you do it right then it's almost exactly the same in terms of memory intensity you can read the paper basically I think we get something like maybe 600 misses per thousand instructions in these two when we run our real systems ok so this is the control application you can see that they only differ in terms of the robot for hit gate over here everything else all of the other operations are the same so let's take a look at what happens when these two applications are around together the the blue one is the streaming application t0 the red one is the random at the X application and this is the memory request buffer I'm going to focus on only one bank over here and remember there's similarly memory intensive so all this first policy is not a problem here we're trying to test the robot for hit the prioritization and its effect so let's say the rover zero is opened initially and streaming application accesses row zero it's a robot for hit and then the random acts application generates requests streaming application also generates requests at the same rate let's say and the expectation in the common case or always the expectation assuming everything is regular the expectations that you have requests from both applications and the request from the streaming application goes through zero requests from random acts application goes to random rows and as a result what does the underlying scheduler do the scheduler picks the a picks the request that hits in the row buffer because it's employing the row buffer hits first policy so it picks streaming applications requests which means that's delaying this application right clearly and this application keeps of generating other requests streaming applications still keeps generating requests and the memory controller keeps picking the streaming applications requests as you can see and this will go on for a while until the streaming application stops generating requests to rho0 yes I see so in this case no basically there is no information about which application they request is coming from if you have that information yes you could do a lot of things and that's one of the solution directions to examine but the system that we every we were examining had no information literally they put two cores and kept the memory controller exactly the same as it would be in a single core and that's it and that is part of the problem and the solution as we will see well there could be multiple solutions we will discuss but one of the solutions is making the request buffer aware of the different applications and doing something more fair okay so you can see we could go on forever like this and essentially random access applications starving and you can do the calculations assuming that the memory controller the behaves just like we discussed if you have a row size of eight kilobytes which is reasonable in a cache block size of 64 bytes which is also reasonable you have 128 requests of streaming application service before a single request or random access application and again this is not the worst attack you can make an attack even worse right assuming that the memory controller always prioritizes rohit requests without stopping that prioritization you could go back and keep accessing the store again and again again again somehow right you can make that role on cashable completely so that it doesn't get cached in the caches so you can always access that role now the life is not as that because memory controllers usually have a also have a hard cutoff saying that if if I haven't serviced a request for some number of cycles let's say n number of cycles I'm going to service that so that's one way of solving the problem it's not a good solution because they're very hard cut off right that still prioritized the application that's doing very heavy intensity it's not completely fair basically and also how do you determine that cutoff is it 10,000 cycles is it ten million cycles is it 300 cycles because if you do it too early then you don't take advantage of the robot for locality so we're going to talk about some of these solutions it's really fascinating and this sort of thing happens in all types of memory it happens in SSDs it happens in main memory this is the first work that exposed in main memory but this has T's also very interesting it happens in cache is also about caches are a little bit less problematic although as we have large and larger caches they'll also become even more programmatic going forward but at least the smaller caches they're fast you can access them fast so the latencies are not as bad but the same fairness issue happens in the cache is also okay so this is one example demonstrating why the problem happens now of course now we understand why the problem happens there are two reasons well there's one fundamental reason memory controller is prioritizing is that is unfair it's prioritizing for DRAM throughput memory throughput or memory latency and it doesn't consider unfairness but there are two reasons why it doesn't consider all this first policy is unfair fundamentally and also a rogue hit first policy is unfair fundamentally oldest vs. are unfair because memory intensive applications are if you're generating a lot of requests naturally many of your requests are older compared to other requests or of some other application that doesn't generate a lot of requests straight Rohit first unfair for the reason just we just saw so let's talk about solutions this is a place maybe you can generate some ideas yes [Music] I see okay that's I like that idea so basically what you suggest is a form of batching you have this batch of requests and you have to complete them before you move to the next stage or next window and regardless of whether or not row buffer hits or conflicts and within the window you have some prioritization policy let's assume you still want to take advantage of the row buffer hits hits I like that that's part of one of our solutions to the problem now that will alleviate the problem is it the most fair thing theoretically is that the most fair thing I don't know but that does alleviate the problem actually any other thoughts yes okay yeah yeah I think that's that's certainly one way I think her solution could be combined with that also actually one way is actually batching without being aware of the threads but a better way is actually batching while being aware of the threats I agree with that then the question is basically how how do you do it fairly there there's a there's a devil in that sentence and fairness is always a problem actually whenever you talk about fairness in any this is also very fundamental whenever you talk about fairness in any any place in human life also there's always a definition of fairness what is fair and what is not yes okay [Music] okay then you don't get into this problem basically each each core goes to a different controller I think that's that's reasonable accept and we will we will talk about solutions like this the problem is it's not scalable to have as many controllers as the number of course so you'll run into this problem in in each individual controller in the end but I think the the direction of your idea is good in the sense that maybe if you have multiple controllers you can alleviate the problem in the sense that in some controllers you direct the requests of applications that are interfering badly to different controllers because not not only interference is as bad so if the applications are similar to each other let's say they have similar robot for hit rates and they have similar intensities maybe the problem is not as bad actor that's what our results show yes okay I like the memory priority I think I will ask a similar question that I asked him how do you determine the slowdown okay yeah these are all the interesting ideas and I they're they're all proposed in some way or form the the downside of course is overheads that you get if you have a list of popular applications and if you if that database can provide you the baseline performance I think that's a very good approach the question is how representative your databases in in many cases it's possible I think but we will see later that you could actually estimate some of these slowdowns and hardware also so your approach is actually no no I don't think so I know I don't think so I think it's the the the the the way you describe your idea is good except there how do you actually guess the slowdowns is this still an open question I think yes okay that's that's also good I mean all of the these ideas you guys are generating good ideas good basic a lot of these are actually proposed in some way or shape or form that's actually employed what you suggest is called least attain service scheduling basically you keep track of how much service each core has attained and you try to prioritize the core that is attained the service the least service so far and we have a paper related to that of course the the key questions how do you actually do it but but the basic idea is there actually some operating system schedules that the operating system level use that idea because similar issue exists in the operating system right the operating system tries to schedule between multiple applications to a core which one do you select if you select one that has been selected too much then it's as a taint service a lot so you select the one that has attained service less yes a question about life so if we ever like database or index or like identify these different programs so I like their their face one times is that vulnerable to light other programs like imitating the other program said like exploiting it more shared like basically can't you you mean you can take the data and the database oh I see here yeah I see I see I see basically you can fake you can you can use a fake slow down baseline slow down to inflate your real slow down right it may be a sign performs inflate your yeah I think so I mean it's it's certainly I have a thought about that solution as much but they're these are think these are issues that you need to think about if you go down a particular path of solution for sure yeah that I think that cookbook that goes back to the representativeness of the baseline performance of an application okay any other thoughts yes the memories inordinately if they are this like proportional to the performance let's say it's a same role we have higher probability to it to enter this request and if it's a nicety basically you're suggesting adding some randomization such that you don't do that strictly but based on some value I think it's certain it's also possible let's I like these sort of randomized processes that could help fairness I think people have shown also that randomize the randomize processes can help fairness these on some weights usually the question is how do you determine the weights because one application needs to likely do it 50/50 do it 60/40 or do you do it based on some other metric now these sort of ideas have also been explored in the context of operating system scheduling for example if you know the work called lottery scheduling it's an operating system scheduler that was proposed essentially with that same basic idea we can search for the car Lots burgers teases is on this topic lottery scheduling maybe we'll cover it at some point in this course even though it's not the subject of this course I really like that direction and a lot of the ideas again as I said it's very fundamental it may be done at the operating system level but could be applicable to the lower level as well yes memory access if you see that one processes taking homage and starts flushing the other or always yeah yeah yeah yeah definitely I think that's another solution the similar to what we proposed certainly trying to balance the slowdowns right somehow keeping into account taking into account do do some accountant to figure out how much imbalance there is and keep the balance like that like a thermostat as you said okay these are all very good ideas but now we're running out of time so keep generating ideas I think I'd like to keep the spirit in this class so that you can think about new ideas some of them actually new some combinations maybe new also and feel free to talk to each other as well if your interest in these topics but let me cover this because we're almost done with this lecture so what is the right place to solve the problem and I think I've heard ideas from different places programmer mmm nobody I heard I heard no programmer ideas because this is really not a programmers problem right it's actually programmers problem but it's not a problem that can be solved by the programmer because programmer programmed this particular application and some other random application is interfering with it so it's not a solution that can be done at the programming level or at the compiler level I believe of course actually many compiler optimizations try to optimize for locality so they actually create these performance hogs potentially but that doesn't mean that this is problem that's caused by compilers Hardware I've seen some ideas like that can you do it inside the DRAM here's inside the circuits I think this is some food for thought I'm not sure if this is right Lola abstraction can you add something into the circuits for example to minimize this effect I don't know get rid of the row buffer maybe somehow but that's very fundamental also all memories have some sort of buffering structure because you cannot directly read from the array or you need to buffer what you read from the array to amplify the signal because what you have in the array is a very long signal okay these guy I will finish with this one there are two other goals of this course hopefully you will think critically and I think what we're doing right now is an example of that basically hopefully we'll think critically about why is this happening should it really happen how do we solve the problem is this the right solution is this the right place to do the solution or is this a good problem to actually solve to begin with right all of these be covered at some level and broadly also because I don't want to restrict your thinking to a particular type of solution whenever you see a solution it's good to think about the broader aspects can you actually solve the problem in some other level or cooperatively between multiple levels right some of you provided the idea of putting the priorities inside the hardware for example that I think is a very good idea and that's already starting to happen but that wasn't happening when we were writing these papers basically finally things are moving so you have to be patient as an architect okay so this is the paper that I described briefly and this may be actually one of the optional meetings for your homework 1 assignment or put up potential readings basically we'll have some paper reviews in this course hopefully you'll enjoy doing some of them I haven't decided which ones yet but this could be one but you can read it on your own also why they Thomas what my collaborator for a long time and he's an ETH alumnus also it is PhD over over here ok and if you're interested there's flirting reading also so let me finish with this takeaway basically I think breaking the abstraction layers between different components and the transformation higher levels and knowing what's underneath really enables you to understand and solve the problems I think we've done a deep dive into one example of this tomorrow we will do some other eight dives any questions ok so I guess I'll see you tomorrow then "
cs2u5j6QCPY,22,Instruction Medium: English,2020-06-14T01:30:03Z,Computer Architecture and Organization Lecture Annotation Video 4,https://i.ytimg.com/vi/cs2u5j6QCPY/hqdefault.jpg,Khalid Mahmud,PT29M1S,false,178,7,0,0,0,my heartiest welcome to you all do this installment of lecture annotation video in this lecture we will see firsthand how the first generation of computer actually execute an instruction and data passing between the main memory and the CPU now let us begin at the first I want to draw your attention to this figure this whole figure is the organization of the CPU and the main memory of the I a is computer IAS we all know is a first generation of computer now here is a correction of sorts they should be 4:09 files and this should be 4 0 9 6 and they should be 4 0 9 5 and this should be 4 0 9 4 you should be prepared about this error because I will ask you about them now let's start now I have told you before in your online class that you should draw this figure beforehand before I do spend because I will describe and simultaneously go through the slides now let's go to the first description of this organization of the CPU and memory of the IAS computer and first we have to draw our attention that two main parts of the CPU there are two main parts of the CPU one is program control unit and another is data processing unit now what does this program control unit performs its fittest instruction and what does this and data processing unit performs it executes instructions therefore program control in it is otherwise known as a unit or instruction unit and the data processing unit is called a unit or the execution unit so from where the program control unit fetches instructions from it fetches instruction from the main memory and interprets them which is why it is known as program control unit because it's controls the execution of the program or in other words it controls the instructions of the program and then the data processing unit it processes the data it calculates the operations and stores the results in the main memory so that is why it is called data processing unit let me go back here this is the program control unit it is also called the eye unit which fetches the data and this is the data processing unit dpu this call the unit or execution unit which executes the instruction and stores the data to the main memory now let's go to the next slide the main components of the PC you are we have already known we have already learned that PC you is the I unit or the instruction unit now the instruction register IR which stores the output that is currently being executed and the program counter PC which automatically stores and keeps track of the address of the next instruction to be fades PC has circuits to interpret of course and to issue control signals to DBU that that is the data processing unit and main memory m and other circuits involved in executing instructions the PC you can modify the instructional sequence when required to do so by branch instructions now the instruction register we will see in this figure here is the instruction register and here is the program counter visible instruction counter instruction register and program counter collaborates to decode the instruction instruction register provides the instruction to be executed and problem content decides what is the next step of the install the program it next instruction of the program the CPU should execute and where the next instruction in the main memory resides now let's go further here we talk about another to register that we that are fun in the PC you with your address register air and the instruction buffer register I BR I be our instruction buffer register it is to be noted that the IAS has the unusual feature of fetching two instructions at a time from main memory M therefore the first instruction to be executed is stored in the instruction register Iyer and the extra second instruction is stored in the instruction buffer register I beer for holding the second instruction and in the address register air in the PC you that holds the address of a data operand to be change from or sent to main memory now this is the address register which holds the address of a data operant that we are will are to be a we are executing and it said and it searches the data from that address or if we want to store a data in the main memory it holds the address of depth memory location now here for exists register and the instruction buffer register perform this task if we want to get a data from main memory the address is stored in address register if you want to send any data to the main the address of the memory location is stored in the address register and for the IBM or next instruction buffer register it stores the instructions which is to be executed after the instruction in the IR or instruction register has been executed now these are the basic description of the registers of the P seen you or program control unit now we are going to go for the DP u or data processing unit now in the dpu we see that there are ill you to general-purpose 40 bit data registers accumulator and data register and it also has a special purpose data register door mq multiplier quotient register intended for use by multiply and divide instructions now what do we see here is that we are talking about the data data processing unit or the dpu it consists of the following units at its medic logic unit general-purpose got a registered ear accumulator and multiplier quotient register these helps us execute any types of instruction in the first generation computers now annex Matic logic unit or the ALU which contains the circuits that performs addition multiplication etcetera as recorded by the instructions of course and several data registrants which store data words temporarily during program execution now let's move forward so that we can focus on the main memory in detail in the main memory we have 4 0 9 6 or 4k world or 4 0 9 6 into 40 bit array of storage cells so what I have already shown you is that this is the main memory and we have this 4 0 9 6 number of array indexes or error locations each of 40 bit each index is of 40 bit it is a kind of error it would be very easy to get that idea to picture the idea in terms of error and then each storage location in M is associated with a unique 12 vaped number called its address which the CPU uses to refer to that location for example if we want to address or reference this location it has a 12 bit advance and each location or each index is similarly defined in a 12-bit combination of things or bits which is the which identifies a location specifically to read data from a particular memory location we need this 12 bit address or store we must have that address from here on we will address this addresses the a variable X so for any address X it has a 12 bit number which is called address and which it subsequently stored in the program content or accumulator register or address register note accumulator now let's move forward if we want to read any if we want to read a data from the main memory we need the control signal to specify that it it is for read methods only and if we want to write or store a data we want we want the control signal to specify that it is a write signal that has to be write instruction that has to be performed so in this slide we will see how these things work we are kind of familiar with this type of mechanism in our we we have done some read or read operations in our C course and there we we have seen that if we want to read something from a file only read operation is executed in we have to open that shalini read moon if we want to write to a file we have to open it in a Reitman this is what happens here the CPU accomplishes achieve more complete the read operation by sending the address X it is a 12 bit address to M the main memory M accompanied by control signals that specify read its width a control signal ring in response by transferring a copy of a copy of MX the make of the data in the memory location X the word stored at the address X to the CPU where it is loaded into the data register what happens here is then if we want to read a data we request to the main memory with X and read and then the main memory gives a copy of the data and it goes to be loaded in the data register dear in a similar way the CPU writes new data into the main memory by sending to him the destination address X a data word D to be stored and control signals that specify right that is if we want to write a data word D in memory location of X we need to accompany the instruction kind of like this memory location data and then the right instruction or control signal now this is how the main memory works for reading right now let's go further this is kind of the instruction sets collection the is the four generation computer of is had around 30 types of instructions which were used to perform a program these were chosen to provide a balance between application means the Machine focuses on numerical computation for scientific application and computer hardware costs as they existed at that time week they could have instruction more than 30 types but in order to focus on the numerical computation and computer hardware cost they reduce that instruction set to only 30 types and to represent these instructions they had used a notation called hardware description language or HDL or register transfer language RTL that approached their approximates the assembly language used to prepare programs for further computer for the computer the designers of the is computer also used such a descriptive language also in other words they used to describe Hardware description language and transfer language which approximated the assembly language hoop debate the problems for the computer and the designers always use this kind of descriptive languages so it would be easier for anyone to pick up that description and be proficient in executing the instructions now here we see an instruction set and is program which adds two numbers stored in the main memory this is the first instruction this is the second instruction and this is the third instruction now the target of this program is to add two numbers stored in the main memory the first task is to load the contents of the memory location may be known location of hundred into the accumulator let's draw a kind of memory main memory M now this is the memory location of em and we have a data one here the first instruction accumulator is filled with the data that we have in the memory location hundred and then we add this data the data in the accumulator with a data in the memory location 1 0 1 the data 2 and finally we store the value of the addition in memory location 1 0 2 it would be difficult but you would get this idea 1 0 2 will have the result of that addition program and this is how this instruction set actually works now this is the instruction execution process the first thing the is computer the do is to fetch and execute instructions in several steps that form an instruction cycle since two instructions are pegged into a 40-bit world that is a memory location each memory location is 40 bit and it has 4k of of the main memory and now these instructions are fetch sage two at a time as we have learned earlier one is put in the instruction register IL and another is put in the instruction buffer register next instruction buffer register ib r and it's address fields is any less in the address register air any kind of embrace that needs to be executed in the instruction is stored in the original address register air the other instruction is transferred to the ideal for possible letter execution whenever the next instruction needed by the CPU is not in ib r the program counter PC is incremented to generate the next instruction address now these are the some of the instruction set of the IAS computer this instruction is used for data transfer this instruction is used for data processing and this instruction is used for program control goto is a kind of keyword we have already seen it branches a program take the next instruction from that particular memory location and the data transfer is such that the multiplier quotient is transferring data with the accumulator and the data processing that is adding the memory location data with the accumulator data and storing in storing the result in the accumulator temporarily and in the program control we go to a specific memory location and get that data from the left half of memory location for the next instruction and this is the basis of working procedure the basic working procedure of an IAS computer now here we see the critical limitation of first generation of computers it is kind of memorizing topic so I will let you self-study in here this particular slide should be so studied by you if you have any question ask me in the boggle minute and I will try to relieve you of your worry now let's go to the second generation of computer the second generation is a bit improved from the fourth generation we will see here in then we will talk from here in the next video hopefully you get the description or the basic principles of how the is computer works if you have any questions you should feel free to ask me I will start from here in the next video thank you all for listening to my lecture I hope that you understand it if you face any problems feel free to contact me and I will answer your questions regarding this material thank you 
z4ycnUz5tNE,22,"Pipelining and it's types
Arithmetic pipelining
Instruction pipelining
Advantages and disadvantages",2019-04-11T13:06:35Z,Pipelining and it's types-lecture81/coa,https://i.ytimg.com/vi/z4ycnUz5tNE/hqdefault.jpg,asha khilrani,PT13M59S,false,17089,195,13,0,10,previous video may have made a copy real life may by planning kasi use your T hair or instruction use Kirkham pipelining cassia cheaper thing I'm a notary technician Quijote correct by planing Oscar types care thing the pipelining care pipelining is an implementation technique where multiple instructions are overlapped in execution the pipelining a technique adjust my multiple instructions your hair the overlapping they may execute two things open a previous video my song Shahada the pipelining Hakata increases the overall instruction throughput the instruction throughput Barada and any number of instructions executed per unit time Giada ho jayegi to come time mayhem Java instruction Co execute cursor thing with the help of pipelining it is used where operation can be partitioned into subtasks topic is your operation kohung sub tasks my partition curse acting hand though he hung yaja pipelining applies a sec thing clicking instruction Co home multiple stages may divide correctly they fetch the code or execute you seen a hunky advanced by pipelining apply cursor play though a circuitous just come further subdivide nahi no such thing as pay pipelining you sneaking over Satya pipelining technique is efficient for applications that need to repeat the same task in many times with different set of data the iike hopper uses a pipelining koja hypocrisy cheese cookies tasks could repeat corner multiple times making different set of data early days ham care such thinking our production companies you have your manufacturing companies you have voice choir you skirting a pipelining calcium key this is a project cars you have voted card you have a developer here to card me Palika of a C+ the poor thing the part should develop who taught the health of your own Co assemble key authors will colour keyer doctor panting Keys earthy preschool michael zitnick car starts to see me car develop Kearney the same dusk leaking closer reputation because WT take a monkey set of data and gala girl of acoustic color Rinko Kikuchi color black over to set up data different air they consume same - pokey agura him multiple times repeat further - yeah because contains a part of Laputa attenti wasn't really busy things with next car key parts would have look on I start editing - well nothing within - manufacturing companies did you have a fight planning technically you Scotty hey products for manufacture tournament show me the courtesy pure performance ombre effect a motto faster circuits where are they begin to costly technique a two-second hardware cool steps he arranged for the cake time a.m. multiple tasks operation perform cursor key so pipelining mayhem yahiko think of pipelining K is a process of arrangement of hardware elements of CPU such that its overall performance is increased on CPU K components your hardware who stripes your inch birthing key maximum time for a CPU be zero or see if you can see busy tomato Java skip as your hardware Evo overlap very may use cadre monkey operations John Kasich a simultaneous and overlapped execution of more than one instruction takes place in a pipeline processor so exit to Orion they can overlapping or you could order to do so fetch or over to overlapping them a simultaneous execution with a each other instruction curve kiss my pipeline processors of next time decreasing design of basic pipeline pipeline Casa design is athene a pipeline processor a pipeline has two ends the input end and the output and the pipelining me though and so take input and Orick output and between these ends there are multiple stages of segments such that output of one stage is connected to input of next stage and each stage performs a specific operation the pipelining Magnum carton a multiple stages when are they or UTC connected o'the input or output say or a key stage vowel produce three stage publisher data with the help of latch ear registers K through a common clock paper cutting or high stage you have open eyes specific tasks perform dirty they say up case at through basic pipeline or linear pipeline hanging here interface registers or latch or buffer user interface register Centro ectasia output to see stage go in put Katella dear mooney capital H a buffer are used to hold the intermediate output between two stages but your intermediate output are a two-stage escapees no screw hole to connect Ely use or things all these stages and interface registers in the pipeline are controlled by a common clock or jitan AP stages hand or chohee registers and let you say focus cook whiskey through control cage earthing a common clock it through control cage are things I'm sure pipelining able to types your thick with your thematic pipeline or equal to the instruction pipeline automatic pipeline Cup you scared me it is mostly used in most of the computers it may be computers and submit Matic pipeline useless are they they are used for floating point arithmetic operations multiplication of fixed point numbers et Cie is mainly copy use within floating point arithmetic operations ever may perform Carneiro thing often multiplication cannot F is point number sir floating point numbers metric points while a number 2 point 5 into 10 to the power 3 J silica Priya coating wine number it is V Plus can i - can i multiplication Caroline yeah fix two point numbers you say 2 into 3 got nahco3 fixed point numbers have to impair the multiplication can i toby arithmetic pipelines are usual time next is arithmetic pipeline can be applied to various complex and slow automatic operations to speed up the processing time to automatic pal plankton cob useful to java makisi complex operation go perform car now have the slow automatic operation to perform car no te to complex over operation Cooper from Konami time Jolla Giga to trying to reduce technical area processing time Cobra Nikoli processing speed for veronica Lee automatic pipelining canoe sutra a big sample from the clay thankful loading point automatic pipeline chicken floating-point arithmetic PI pi PI namkha decree addition of two floating point numbers two floating point numbers you say a K 2 into 10 to the power 3 or a head tag 3 into 10 to the power 5 suppose T so excuse me mean donahoo numbers were a dagger Knight Ohama pata hai ki toh new hair is way to number has remain tis a hookah or exponent over turning Apple camera hanky you don't know add new circuit if you don't know exponent I different at the subsea Pelican Quiroga exponent wanna compare karna hoga - huh make comfier exponent stage when eyeing a stage one is cuando input stange a be this moment is a part of our exponent part oka through a stage chaotic into exponents will compare the ready is such a key K 3 is equals to 5 a cam nahi trans less than 5 is come a cool a beautiful skies make yoga align exponents exponents who aligned karna padega any same car nobody got this car 10 to the power 5 is 4 10 to the power 5 and Oliver eiga the 10 to the power 5 banana clay 10 to the power 2 C multiply go knock out 10 to the power 2 Z divide Canova through jaga point 0 0 2 into 10 to the power 5 twist made on ok exponents for Sam a line close here Phil came over in tone ok mentees Ayane point 0 2 or 3 a personal I adore thank you three cows as a three point zero two of normalized results to normalize third make yoga mantis alga 3.0 to or exponent okay 10 to the power 5 of escucha who 3:02 cursor to normalize can image 3 0 to 2 which i got 10 to the power 3 stripes is your hair floating point numbers who add car Nikoli four stages values with a belly compare connect the exponent sphere encore line carnegie different magnet is awkward can agree for normalized scanning kiddies the arithmetic pipelines the floating point addition of two numbers second type instruction pipeline an instruction pipeline a stream of instruction can be executed by overlapping fetch decode and execute phases of an instruction cycle the instruction pipeline Leon Calcutta fetch decode and execute phases of the instruction cycle K encore lap they may execute Kawasaki ahem Giovanni previous video member theater this type of pipelining is used to increase the throughput of the computer system throughput mutlar number of instruction executed per unit time per unit time a kitten a instruction executes only two pipelining k through Quiroga Yi increase which I go to key come to image other instructions executed a here overlapping for may execute car nipper the process of executing the instruction involves the following nasal steps fresh the instruction from main memory decode the instruction fetch the operands execute the decoded instruction to PCB instruction to execute chronically a maple instruction Co main memory so fetch karna Bertha exes karna Bertha to Chapel instruction execute the head to see beautiful type ellos instruction comeon memories a fetch filter which could decode kratom at the potaka peg is my addition car nice abstraction can i yeah compared connects type cooperation is instruction to through perform carvanha finish me jo operands his Edward okay on KO v square or she finally use instruction to execute cut the head - ya Habibi Allah Galactus perform what the instruction to execute connect it is a series a pipelining pipeline k4 may execute corrosive thing to is maybe hum multiple instructions were overlapped we may execute there was a claim with the help of pipelining let's make it three stages only fetch instruction decode instruction fetch the operand execute the instructions placed four stages now zerga instruction ballet opera instruction geared usagi kiss keepers fetch keepers equals the instruction to Falconeri memory so v square leg is staged instruction go to decode stage secure Potocki a zygote Keiko's this type construction had a subscription a multiplication a three compiles and acoustic construction and fit was good you are Prince ji young gave Oh memories a fetch keys I engage the stage me or finally useful CPU they kills instruction quod see you dear jaga Toyama / humming jump instruction carousel - chamomile jaga - stripesy instruction piping were courteous mukesh fiscal space time diagram banter instruction pipeline processor may space-time diagrams iam but as I think I guess instructions you had who were leaping will me executor I though I won stage one may execute or subsidize a future I even show her stage to me yoga through Stein y2j agar stage one instruction is cumulative of fetch or ticket to joke instruction one fresh ova ova stage two mature record me whatever decoding over to second instructions your head walk your dog a fracture over scoffs etch or scar the code you have over left we may execute only freaking Oracle to buy even to have scale operand fetch over only through string second instructions your hair would decode or alga or third instruction color above fetch or over next time unit makeover are even to her execute aura hoga stage for mohkumat are executed over or i2jt operand fetch your only I three care over the code or over or Eiffel Tower of a fetcher over Jesse I even executed economy output Malaga up to occur next stage me I'm sorry for instruction fetch sugar suppose I was for instruction to the for instruction fetch sugar till next step me next clock cycle make yoga I to execute or a yoga I think our of ask your brain treachery on your Eiffel Tower of a decoder over to Jesse I to execute oj goes output of a mill jaga the next stage maker I three execute over or i4k operand fetch over angry with next clock striking me I through output multi-core I for kiyoka execute or over or finally execute okay output Malaga so it's type C to her overlapping whose instruction pipeline the instructions key make some decree advantages of pipelining more efficient use of processes more efficient user a mother processor juego maximum time busy rahega it increases the throughput of the system throughput muslim number of instruction executed per unit time entry so Jangi it can say circuitry tose circuitry but chiotti hey kyon ki hyung hardware host IP arranger thinking i moscow parallely excess kirpan disadvantages care pipelining involves adding hardware to the chip so cost of the system increases kiss me I'm a hardware who chipped a honey hardware in Greece karna Bertha pipelining ku increment connect leakage hardware me I had karna battleship pay excuse I see first of the system but jetty inability to continuously run the pipeline at full speed I'm fully speed by pipelining Quran nahi kar such thing because of pipeline hazards which disturb this smooth execution of the pipeline which pipeline has are so thin he was Jessie hung pipelining Coe fully utilized fully speed may execute me he does such thing for the speed may use and he cursing what is a pipeline hazards and year of next semester me pierogi computer architecture my Turkish problem so teenagers cleavage si hum the pipelining cook continuously execute nahi kar sakthe hai kuch delay our thing this gives us a time but Georgia 
4vHyk_YcHE4,22,Computer organization and architecture lecture series 19,2019-03-18T18:00:43Z,I/O Organization (COA KTU Syllabus),https://i.ytimg.com/vi/4vHyk_YcHE4/hqdefault.jpg,Computer Science and Engineering mentor,PT12M21S,false,2475,60,1,0,3,hello welcome to my channel computer says an engineer Amanda ring away the very end a child is subscribe me the delaying it they very subscribe za marketed as a poorly then you give me video sister better friends and sharing yeah thank you thank you for watching my video today we are starting with the third module topics I know organization the third module actually input/output organization ok input/output organization means we know that one of the basic features of a computer is the ability to exchange the data with other devices right number there is in almost all and beyond minds computation in the other part we know Internet of Things is an emerging media and so it computer must have the ability to explain something explain the data with other devices yeah that is actually happening through this part I organization here the relevant so far your organizational okay first of all I will tell you how we are accessing the I your devices accessing your devices you please look at this figure in the previous videos in best structure I have already told you that you know the processor memory input/output devices etcetera are connected to a best structure which is an indica munication structure that december's ok so here you can see here the processor and memory is there and similarly so many i/o devices are there I or device one of your device do etc and your devices ends are there so all of these are actually connected to Epis structure we know basis actually consists of three ten years of lies one is address area one is data carrier and one is control carrier okay when the processor places a particular address or address lines of the address bus address lines of the bells their devices correspond to this address responds to the command issued on the control ends that is clear you can see a base type base consists of three lines the Italian is there at the slain is there any control line is there so process from the processor will place some address on the address lines of the bus okay a preprocessor a place a in a dresser that may be the address of any of the input/output devices connected to this picture upon tristan a corresponding Ayatollah ru devices are I know that I you device this will respond to that come on commanding a malanga Nevada canary here operation uh no right to operation on on the Konami logo duck on the base in the control in Solana concerned address Rula input/output devices input device Abba output device out there devices he looked into the condo circulatory to see that which of the operation have to be performed okay so and in Anna a communication or a condom and based on the address space used by the i/o devices there are two schemes for are you one is memory mapped i/o memory map are you and I you my prior you I pray based on the address space used to by the input/output devices we are classifying this came simply to member in my pranayama I will tell you what is the difference in memory mapped i/o the address space used to by memory address space used by memory and i/o devices are same your device's I see okay today the Nama Korean number ran prime minister or the poor she initially nimbala come to attribute TNS I'm a the booting on any process not recommended booting in actually in this area operating system memory leak in order above one portion of the RAM is always it is dedicated for operating system the remaining parts are actually used further executing programs above II address space own making itself a memory you say in in dava input/output devices new you seen in dawa opinion trustees separately written illa same I died laughing you see him but then the reveal and it is recommended a banana memory my prior yeah they input output devices remember it king of the Sharia okay I'm gonna relocate it silly the instructions actually use the fourth performing memory operations and also for performing ie operations are same for example if I move better in coma R 0 in another room instruction go to correlate data in Monona register it is actually used to play the keyboard for store in the input data characters so you data in mirela Karic turner and nipple a Kmart enemy they just ask they remove JM and related inspection another number I even am only instruction but it's a pure memory in locations on again I'm thinkin a gotta move a comma B many MB memory location on in one a location and it'll be like a movie over here you can see the same instructions that we are using for both performing our operations and for performing memory based operation so this is relevance a memory mapped i/o app always remember that in memory mapped i/o address space I used to be memory and i/o devices a say but in the case of i/o my prior what happens this different different address space are used different address space and the memory Commendatore atmosphere sir for example if for normally the body on English you were there memory erase dream maybe memory Kirika working or sick are you can use Rena about yeah dress spacing you seem the memory my you say not recipes may be different okay no poverty love ordinance shall different insertion cetera Cuba memory come at you and getting you same I know operations of us watching a mandatory instruction a tequila memory operation support for gmid you see ya but I know my prior usually no more three canto regarding inshallah I'm Acharya I your device 6 on Angelo memory devices a memory Anakin okay Oh do Westlake and galaxy they connect it but Samoan children anniversary or like areas the fly insists say there is a basis reckoning different data kill address space at the front down and best-selling areas are occurring different area the same address space is used to buy memory and i/o the memory Anna Maria okay same address missionary you say yeah they opposed no coach Aleppo if we had the special it 100 another address oh no no Jo 101 will address a sellable memory location of our shell up Europe I'm your devices in identity Wow after either a the in the address on the live I'm makin and another basis eternal memory respond a you know input turbo devices responding and on the Humana came the other new hundred TV I used is some additional signal some additional sector and signal we are used it for distinguishing whether it is their memory operation or are you operation okay Apple it would have occurred on the Torah your operational the memory operation memory addressing ignoring it is coming alive your device you say that no that is will respond to that okay so these are the two classifications of higher based onthe address space memory my prior your hand I owe my prior the necessary will tell you what is an eye you in their face are you in their face are you interface means which is an interface circuitry btv the best structure and the input/output devices between best structure and then input/output devices okay and they put in the fish sandwiches are done in their before exam operate the system operate the system nurse at an angle operating system is there in the operating system it's actually act as an interface between hardware and the user let the hardware user in the Molalla interface and operate a system at the polar bear so input/output devices in the Miller or interface in the eye interface in a lemon are you in the face Maria a diagram oka appear a new perspective is dillema carriers under on address address it carries yet on the data here is here I'm not controlling addition okay about eternal darkness I know in their face okay input devices input device this there okay yo la we are using the phase rule of munna subtotal even katakana on address decoder on the control circuit on the date and status which is this okay a pod restrict order in the Shana machala whenever an address is placed on her dress lane that address is decoded by the address decoder the FAA the input/output devices in IANA classify a memory mingle address recorder are concerned addresses le cÃ´tÃ© little for example if orbited the Sun number II you see in English I 4-bit address which to deny my input output device is near a person am 16 input/output devices near a person in 2 raise to 4 yeah no but you say you do undersea at this facility remember a 16 input/output devices select a their devices in IANA the percentage they can marry a man returned to Seco double date for the address and the consent must say she will be sent it to the concerned how your device pin a control circuit control security sensitive connected to the control as you can see here so whatever operations is actually coming on the condolence that is a connects to by the control circuit and it will be intimated to the concern input device Allah Allah render registers on a Delta resistor such status register sir this is actually connected to the beta lines either it will be a little Delta buffaloed a terminal it is still a determinate entangled at eight hour sorry data line siloam little like this the data is transferred to the date that is is this and the ballot status the distance non-religious normally you see Mandir you start the statistical shipping and basic haeyo operation is the first module of it till again ability to endure at the Carnot order their way the corner ok then you will get the idea but status registers okay I actually started services in the Dunwich Ella if we do processor them input output device soon allah communicating in a seminoma korean processor is actually a very fast device make sure that your device l input device Echota output of a sigh equal to b keyboard the monitor will display devices the cake attack everybody slow it and act in the compare to the processor now both invalidated run slaves and made the process for a very fast but they will bend get a slow runner so some synchronization mechanism we are in need of about synchronization control new entity memory you see in the resistance status its we started suggesting the value in a base see the turn up on LEP portal internal integrity bowl didn't raise it turned off now the boiler and england data display and the processor look in China so he would render educational status in society you see a oneness is inhc input of input devices you see in the status Vienna is say n on the s South output device you see in the status piranha is so okay if they attend status is a cell son we are you in the face rod of our order circulatory so are you in the face actually consists of these three components address decoder condor circles and data and status which is this okay about you interfaces and actually act as an interface between the bus and input/output device 
yPcGVsO0mCc,27,"These videos are useful for examinations like NTA UGC NET Computer Science and Applications, GATE Computer Science, ISRO, DRDO, Placements, etc. 
If you want to enroll in our courses please visit https://www.Digiimento.com or call us at 9821876104. For NTA UGC NET Computer Science and GATE Test Series visit www.gatelectures.com
You can also add me on facebook at https://www.facebook.com/HimanshuKaushikOfficial

Subscribe to our channel and hit the Link button on the video.

#Call_9821876104 #NTANETJune2020",2017-09-29T14:26:34Z,ECE ESE/IES Computer Organization Architecture (COA) cao lectures  gate ece,https://i.ytimg.com/vi/yPcGVsO0mCc/hqdefault.jpg,NTA UGC NET Computer Science CSE,PT7M8S,false,7977,59,8,0,9,"hello everyone so in this video we are discussing about the announcement of the package that is for electronics and communication students for electronics and communication es si that is engineering service examination or you can say Indian engineering services this is the package which is common with subject which is common and this is a computer science subject it is a mixture of three subjects which is computer architecture organization operating system database management system and some basic from the programming fundamentals or programming concepts now the problem with this subject is it is not a electronics and communication core subject so that is why the students were from electronics in communication they tend to leave the subject in their AAC preparation and let me tell you one thing while you are preparing for engineering services are mentioned or gate it is important to prepare all the subjects thoroughly because if you leave that any single subject and there must be some other students who is studying that subject thoroughly see all the topics of the exemptions I have seen people that they say that don't do all the subjects just focus on if there are 12 subjects focus on eight to nine subject to do them thoroughly you can leave three or four subject but that is not true because there's so much competition in the market there are people who are preparing from last three to four years or five years just for annealing for its examination gate examination so you are competing with those students who are preparing from a long time and they have covered each survey subject so thoroughly very good okay so I want to say that if you want to prepare or esea then do not leave any subject even if it is forms of the sense background so here we are starting a package there is only for electronics and communication students okay we are not from ACC background okay but we have seen the slavers of this subject I've taught this subject for a sec students in the coaching institutions now I know that even I am from commit science background I've never given the examination you have never given the EAC or IES examination but still I have taught this subject and students got very good marks in this subject for EAC examination okay even though it is a very small part in that case okay but still if you want to prepare for this subject we are starting a package for this subject in this research papers actually this Luis is divided into three and parts here you can see the first part is basic architecture which is CPU I organization memory organization peripheral devices now this complete part is a part of the computer architecture and organization okay now here if you see operating system processes characteristic applications this again there is a memory management here this is memory management here there's a memory organization we have a memory file system and security this specific part is a part of operating system there is OS ok now there are some part which is there in computer science that is process synchronization and process scheduling that particular part is not here in this operating system subject now we have the third part is databases different types characteristics design transaction concurrency control this particular part is one subject ok so this part from here to here this is the DBMS part in computer science apart of this we have index management we have structured query language SQL and we have topics related to the normalization so for electronics and communications for me you are just discussing about databases different types of databases their characteristics and design transaction concurrency control a last part is the elements of programming languages ok now the students who are from electronics and communication background for you it is a mixture of all the subject it is like you know it sometimes you get very confused in this subject now we have created out videos in a very thorough manner thorough in the sense that we have already created these kind of videos for computer science background we have taken some some of those videos from the company science part and we have embedded those videos for this particular package and a part of this you are getting the previous or partial solutions from this particular parts from AAC from gate gate computer science and gate electronics and gate compress I said DAC that products and communication and delayed examination bark examination and the idea examinations we have taken their taken questions from all these examinations and we have embedded them in their package so if you are preparing for this subject for electronics and communication engineering services then you can take this package okay this will be very helpful for you this fourth participant some programming languages now how to watch the demo lectures see we have already uploaded demo lectures on YouTube so there are a lot of students after watching these many video lectures if you see our YouTube channel we have uploaded more than 900 videos which is double or triple than any other person who have uploaded media on YouTube right we have considered more than 3000 video still now and after those 3,000 videos we have uploaded 900 videos on YouTube so which is lot more than anyone else who has uploaded the videos for AAC indicated on YouTube okay next is you can watch the demo lectures on YouTube a part of this what is the extra that you are going to get we are currently recording the previous question solutions for AAC electronics and communication AAC background so you are going to get this previously partial solutions and this is these solutions videos will be available by the end of October month we have already started recording them but all these videos the model 200 videos that are already available so we have already created two hundred plus videos for this particular package the fees of these packages are pays three thousand which i think is a very very normal things considering that it is a combination of four subjects okay and currently we have more than 200 videos for this particular package and by the end of October or November we are going to add 100 more videos in this package so it will be around 300 plus videos that will be there and duration of every video is six to eight minutes only how to restore this packet just email us at admin at the Red Gate lectures dot-com and for the subject just write that AAC electronics and communication ESC complete octet organization and you can whatsapp me online eight to one eight seven six one zero two so my team is there and my team is going to reply for this whatsapp messages and this email and you can pay by a net banking ATM debit card credit card everything is available okay so if you want to register for this package just let us know and they are going to there we are will be there to help you out with it okay and the biggest question is how to resolve your doubts or queries so if you find any doubts in CODIS you can just email your doubt app doubts at the Red Gate lectures comm so all the registered student they have a proper doubt solving mechanism we can resolve your doubt by doing a Skype call or we can resolve your doubts by doing phone calls email solutions etc so top it out solving mechanism is there okay I hope this is helpful for you all the best for your preparation "
Msx-_f3QpgU,22,"â€œComputer Architecture is a specification detailing how a set of software and hardware technology standards interact to form a computer system or platform. In short, computer architecture refers to how a computer system is designed and what technologies it is compatible with.â€

â€œComputer Architecture is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constrains.â€ 



â€œComputer Organization refers to the Operational Units and there interconnections that realize or recognize the specifications of Computer Architecture.â€

â€œComputer Organization refers to the level of abstraction above the digital logic level, but below the operating system level.â€



Instructions specify commands to:
Transfer information within a computer (e.g., from memory to ALU)
Transfer of information between the computer and I/O devices (e.g., from keyboard to computer, or computer to printer)
 Perform arithmetic and logic operations (e.g., Add two numbers, Perform a logical AND). 

A sequence of instructions to perform a task is called a program, which is stored in the memory.

Processor fetches instructions that make up a program from the memory and performs the operations stated in those instructions.

What do the instructions operate upon?
* Data are the â€œoperandsâ€ upon which instructions operate.
* Data could be:
Numbers,
Encoded characters.
* Data, in a broad sense means any digital information.
* Computers use data that is encoded as a string of binary digits called bits.",2020-09-30T04:45:12Z,Lecture 02 CSE 231 Computer Architecture and Organization Spring-2021,https://i.ytimg.com/vi/Msx-_f3QpgU/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT40M10S,false,167,12,0,0,15,[Music] computer architecture and organization is subjective um for performance computer organization and architecture [Music] um [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] examples of instruction set architecture by isa cpu architecture administrator memory architecture input output organization upon iot device camera studio camera cpu architecture forward single bus cpu multiple bus cpu hardware controllable micro programs summary starting from [Music] primary is secondary memory cache memory virtual memory memory hierarchy memory architecture chapter 5 study performance input output organization i o device addressing i ordered transparency execute cycle complex instruction set computer so complex instruction set hoca reduced instruction set hook eight hundred computer and architecture cooler mode what is the computer already after a a computer is a sophisticated electronic calculating machine that accepts input information processes the information according to a list of internally stored instructions and produces the resulting output information local corbin the assembly bully so simply put a computer is results by input information accept corby she information will okay process kobe according to a list of internally stored instructions so a computer so what are the functions performed by a computer the functions performed by a computer are number one i'm rescuing the departure accepting information to be processed as input storing a list of instructions to process the information processing the information according to the list of instructions providing the results of the processing as output um at the computer category information by input information accept corey information like a process output information by resulting outward information should produce scorpio the energy output produced input accept core input process output produced input accept core input process output [Music] structure according to the list of instructions g-shock instructions say instructional video providing the results of the processing as output processing and follow-up out which sophisticated electronic calculating machine that accepts input information process the information according to a list of internal stored instructions and produces the resulting output information above computer onyx described procedure what are the functions performed by a computer now what are the so what is computer architecture computer architecture is a specification detailing how a set of software and hardware technology standards interact to form a computer system or platform in short computer architecture refers to how a computer system is designed and what technologies it is compatible with so akane computer definition computer architectural definition memorabilia computer architecture a specification how a setup software and hardware technology standards interact keep having acting digital software when this state of software manages to hardware otherwise to state of hardware technology interact with you guys with cool technologies so computer architecture is the art of determining the nature of the user of a structure and then designing to meet those needs as respectively as possible within economic and technological constraints today computer architecture is the art of determining the needs of the user so amra jakhon at the computer technological constraints um so what is computer architecture computer architecture is the art of determining the needs of the user of this structure and then designing to meet those needs as effectively as possible within economic and technological constraints computer organization build the keyboard say what is computer organization computer organization refers to the operational units and their interconnections that realize or recognize the specifications of computer architecture architecture it is the definition of stories computer organization computer organization refers to the level of abstraction above the digital logic level but below the operating system level economic protocol that cannot launch a computer how computers are going understand high-level design concepts understand computer performance and get a harder job get a software job so i'm not get your basic discussion at the job oh shake her name to the sub-block economic capacity arithmetic logic unit control units control eta units processing you need multi-party social media processing unit the central processing unit cpu a united state arithmetic logic unit [Music] input unit accepts information input unit information accept copy from human operators from electromechanical devices in information the input nano results operation on the input information as determined by instructions so control unit coordinates various actions like input output and processing so i'm going to take energy information transfer information within a computer other transfer of information between the computer and i have device other perform arithmetic and logic operations so arithmetic operations congola joke logic operations key logical and logical not logical or operation logical nand operation logical exclusive or operation so logical and logical or logical not operation shake low perform quora a sequence of instructions to perform a task is called a program which is stored in the memory i'm really programmable software is a sequence of instructions instructions which is stored in the memories a sequence of instructions to perform a task is called a program which is stored in the memory processor keycards correct encoded characters so data could be numbers data could be encoded characters although digital information click on digital information aj [Music] computers use data that is encoded as a string of binary digits called bits computer database a string of binary digits by very visible string occurring encoded input unit vector binary information must be presented to a computer in a specific format computer and mouse real world damn keyboard audio input but uh binary information okay specific format a computer represented so binary information must be presented to a computer in a specific format this task is performed by the input unit so interfaces with input device accepts binary information from the input device presents this binary information in a format expected by the computer transfers this information to the memory or processor so inputting it keycaps corresponding input unit what are the tasks of input unity input unit input devices [Music] in a format expected by the computer computer formatted input devices taken data for binary information input so what are the tasks performed by the input unit it interfaces with input devices it accepts binary information from the input devices presents this binary information in a format expected by the computer transfers this information to the memory or processor so the joker number by banning block but they're called the input you need to cut scheme so input unit data is represented as a series of bits okay memory you need to call instructions bt store correct processor instruction recording data read current from the memory author data right call it to the memory during the execution of a program active processor joke on program execution correct so in theory instructions and data could be faced one bit at a time in practice the group of faced at a time  in the corridor at a time actor instructions money act upbeat one bit by bit third constructions um instructions did a store called at a time she competed overnight award length she computer award length of social media so group of beats stored at the time of a group of bits retrieved at a time shitty word so number of bits in a word is termed as the word length of the computer length in order to read or write to and from memory a processor should know higher to look at a bala processor memory taken out of a memory camera computing address is associated with each word location protector word location reactor address registry interpretable cj access any word location in a short and fixed amount of time based on the address at the supervisor so ram provides fixed access time independent of the location of the worldwide location memory and processor have to communicate with each other in order to read right information lectures at the computer system memory in order to write information so a communication collagen obviously so in order to reduce communication time communication time common are you know a small amount of ram is tightly coupled with the processor i'm not getting memory with an instructions a small amount of ram is tightly coupled with the processor processor cache memory so cache memory is a small amount of ram gt basically [Music] processor ninja show memory modern computers have three to four levels of ram units with different speeds and sizes so multiple reactable launcher random access memory memory access randomly accessed but secondary students are slower so access to the data stored in secondary storage is slower but take advantage of the fact that some information may be accessed infrequently so at the memory you need a course secondary storage devices are primary storage eyes is sufficient to store large amounts of data and programs so amra chile primary storage rob doing gigabyte ram at courthouse primary stories can be added but it is expensive so secondary is rest of numbers secondary strategy to go to the access time to birthday but call striking to come so higher access time implies lower cost lesser acted access time implies higher costs and arithmetic logic you need output you need science university of science and technology computer science and engineering 
JPMvDMvpLns,22,This is a lecture on I/O Controllers for COMP375 Computer Architecture and Organization at NC A&T State University.,2020-03-27T18:05:42Z,I/O Controllers,https://i.ytimg.com/vi/JPMvDMvpLns/hqdefault.jpg,Ken Williams,PT19M3S,false,857,22,0,0,2,"this is lecture our i/o controllers for cop 375 computer architecture and organization at North Carolina A&T State University internal components for computer are the CPU the memory in the bus when you write a program you just do calculations and change variables these access the memory the bus and the CPU but you have to read and write to i/o devices you read from a file your right to the screen you know your right to a printer those are IO devices they're external here's a short list of i/o devices that I made up as I just looked around my office you have all sorts of things some are built into the cabinet of the computer like the discs others are obviously external if they keyboard in the mouse and there's also other devices out there they run different speeds this chart is a logarithmic scale so H increment larger is 10 times faster so you can see the Gigabit Ethernet as a billion bits per second actually believe that displays her even faster than that and you can see hard disks like this is an old chart taken from the textbook down at the bottom you can see things at the keyboard how fast can you type anyway so the keyboard and the mice are very slow compared to many of the other devices they have computer mice but now rats it may be fast like a Giga bunny the devices differ in several different qualities they are different speeds as mentioned before granularity granularity is the size of the unit that you can read and write keyboards can type individual characters disk drives generally read and write blocks of maybe 4,000 bytes and also how you control the devices the aisle controllers are the interface between the different devices and the internal components of the computer here is the usual graph I've shown many times representing the architecture of a computer and you'll notice the i/o controller connects to the bus and also connects to the i/o device generally I only draw one IO controller usually there are many i/o controllers there's typically one io controller for each type of device or each type of interface there may be multiple devices connected to each aisle controller the i/o controller is the interface between the device and the internal operations the Missis of the system the bus the memory and the CPU the CPU does not access the i/o devices directly instead it requests the i/o controller to take actions to the i/o devices so the CPU sends requests the out of control and the iowa controller runs in parallel what the processor aisle controllers connect the i/o devices to the system you think of an i/o controllers plugging into the bus and then having our devices plugging into the controller it communicates with the CPU and the RAM over the bus most controllers can access the memory read and write directly from the memory and they also get commands and return status to the CPU single-aisle controller may control multiple devices most computers have many i/o controllers one for each type of device or each type of interface the CPU generally commands the i/o controllers to take actions the i/o controller is responsible of several different activities first of all it's an interface translation that is the i/o devices do not communicate in the same manner as the bus the bus has its protocol for communicating and the i/o devices have their own calls mostly our devices have a serial connection whereas the bus is very parallel so there's also a different speed suddenly as you surely our devices are very slow say your mouse it doesn't send off a lot of information to the system whereas the bus runs very fast and there's addressing you have to be able to specify an individual device the CPU has to indicate which device it wants to read or write to multiplexing is the connecting of multiple devices to the computer through the i/o controller in other words you send a command to the i/o controller and the out controller will specify the individual device that's supposed to take that action you're going to have multiple devices reading and writing simultaneously to the machine and the i/o controller handles the flow of data to two devices or more buffering is the saving of information between the device the bus runs very quickly so if the i/o controller is going to send information out to a relatively slow device then it will read a block of information from the memory across the bus and then feed it out to the device at a speed that the device could handle it bio controllers also detect any errors they advice may be able to detect and possibly correct them in some cases the i/o controller will take action to retry or correct an error in other cases you will simply inform the CPU that an error has occurred controlling some devices takes multiple steps and the i/o controller may initiate the multiple steps there's at least one controller for each type of device different devices may connect to the same controller with the same interface different types of devices may all plug into the computer through a USB port you need one USB controller to connect with them CPU has to be able to specify which device it wants to access it does that as if it was accessing memory i/o controllers have an address on the bus they have an address that the CPU can send information to by reading and writing almost exactly the same way is it reason writes from memory there are basically two different IO addressing schemes one case the i/o address is a completely independent of the memory addresses the Intel processor uses the scheme another way the i/o devices overlap the memory addresses and they have the same memory addressing space as you would store variables this is known as memory mapped i/o here is the system where you have separate address space for your i/o devices so you have two sets of numbers from 0 to a big number if you're addressing memory you then the bus has a wire saying this is a memory access and it will access the memory and get the RAM usually it's some high address the BIOS or a ROM overlaps the addressing and so when the CPU addresses that high edges it's getting information from the BIOS ROM for i/o if it specifies it once an i/o it as the individual il controllers address the i/o controllers can have a overlapped memory actress's i'll controller address 42 and memory address 42 there's just different address spaces memory mapped i/o the i/o controllers and the memory use the same memory space that is you might have a memory address 42 but you can't have an i/o controller with that typically the i/o controllers are at one end of the memory often the high end so the i/o controllers have very large addresses you address an i/o controller is typically three different addresses for each controller one address is for data transfer now some IO controllers get all their information to and from the RAM or others at least get some information directly from the CPU so there may be a port to access data and your send data to the device there's another port or or address for controlling the device you have to specify which device you want you might in this port to specify the device address tell whether you want to read or write or what is the operation you see if you wants it done with the i/o so we'll send that information to the i/o control for it so it would write information to the controller port it would read information from the Status port and as it wants to know how is this device working you can get a device status finding out whether it's is it running as an idle is there an error the last request you made completed already so you can look in the status port so these are three separate addresses pretty much as if they were three separate memory addresses or three separate variables you could write something or store something into the data transfer you could store a control word into the control port or you could read the status variable here is an example for a very simple primitive IO device where we have a status we have three addresses the device status the I already met which is a particular bit of the device status and then the device data port or you can store data this would assume a memory mapped i/o scheme so in the first instruction we're picking up the device status and moving it into the eax register within an that register with the I already bet check and see if it's set if it's not set meaning if you and it and the result is zero it goes back and does it again so this program spins around repeatedly checking the device status until it changes in the device ready bet becomes on now this has the disadvantage the computer is doing nothing else but spinning around looking for the device status on the other hand some machines aren't doing anything else you can think of some very small device that's checking it's one an only i/o port and it's not it isn't everything to do until that device is ready so it keeps checking finally when the device is ready it will pick up a data byte and move it into the device address for data transfer so we would write that bite into the device if there were multiple to vote if there were multiple bytes to be written then there would be another loop around this to pick up the next byte and go back to check i/o and see if the device is ready some IO controllers need to do some analysis and filtering of the data as it comes in from the i/o device let's consider an Ethernet controller Ethernet packets have a header that include an identification as to which computer is supposed to receive this message and the Ethernet protocol generally all computers receive all packets of information if the packet is not identified for this computer then the controller will discard or simply not give it to the CPU so the i/o controller has to analyze the header check to make sure that the information was received correctly if everything looks good then and only then transfer it to RAM until the CPU that our packet was received some controllers have to transform the data in many ways so that it can be used by the computer system the answer is B almost all devices are slower than the bus when you're driving on the street behind a city bus those things are slow when you want to measure the speed of an i/o device you can look at its latency in his throughput the latency is how long it takes from when you first request a command to occur and - it actually starts happening the throughput is once it starts going how many bytes per second or how many how much data can be pushed through the machine some devices require multiple steps in order to make them work others simply say send this byte out this device and that's all it takes as information comes in the controller can detect errors and react proper to them and even attempt to recover from errors some controllers do not provide a lot of support for controlling the device all they do is send the commands from the CPU to the device below is an example of a floppy disk controller we don't use floppy disks much anymore and for good reason with this dump controller cpu tells this floppy drive to start spinning floppy drivers would stop spinning when they weren't being used so started up and then once it starts gets spinning at the proper speed the i/o controller will interrupt the processor until it's running at the fast speed then the I'll the processor will tell the controller move the disc reader arm out to the proper location so looking to move the arm out of the proper location and then interrupts the CPU when the arm is in position the prosthetist start reading to transfer the data into memory and then once the data has been transferred in memory the disk interrupts the CPU to tell it the transfer is complete some controllers have embedded processors to handle a lot of the processing of the information this all flows the world from the CPU allowing the i/o controller to run independently of the CPU allowing the CPU to get more processing done of course each device runs slightly differently and the i/o controllers have to be able to operate each one this proves performance for all devices and processors in this example the processor sends a request to read to the smart IO control giving it the location on the disk that were wants to read and the location in memory where it wants the data to be stored the disk then does all the steps it starts the thing spinning moves arm out reads the information into memory and then when it's all done it will interrupt the CPU to tell it that the operation is complete or that an error has occurred most high-speed IO controllers use direct memory access or DMA it allows high speaking movement of data between the i/o controller and the memory some devices do bit buffer chaining that is instead of specifying an individual memory address for where data is to be stored they can give blocks of Mario's put some of the information here and then some of the information in another location and the rest of the information in a third location so in this example we have a linked list of addresses you give the linked list to the i/o controller which then puts the first somebody bytes read into the data buffer one the next so many bytes depending on how much is specified and to buffer to and remainder into buffer 3 putting the data into different areas in memory so this would be often called a scatter read or if you're writing from multiple buffers and gather right it's particularly useful you have operating system information and user data that has to both be sent out to the i/o device for in severe Network packard the network header may be created by the operating system whereas the data to be sent comes from the user address space you can do I old buffer training so that you write some of it from an i/o buffer in the operating system that specifies the header and then the rest of it from the user data space where the data is taken some computers go beyond just changing the data and chain a series of i/o commands in this way there's a linked list of aisle commands each specifying what the operation should be done the address in memory the address of the device and how much should be read read or written and these are chained together so the processor gives the head of the list to the i/o controller until the i/o controller to start man while the i/o controller is going down the linked list doing the commands reading and writing the information it is possible for the CPU to add more commands on the end of the list so that it just keeps on going and the i/o controllers keep reading on writing devices and the CPU keeps giving it more and more to do this of course improves the efficiency of all devices watch out for homework is going to be posted on blackboard in the near future will be due next week "
4g4mvKLo0bI,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2018 (https://safari.ethz.ch/architecture/fall2018)
Lecture 7: SIMD Processors and GPUs
Lecturer: Juan Gomez Luna
Date: October 10, 2018
Slides (pptx): https://safari.ethz.ch/architecture/fall2018/lib/exe/fetch.php?media=comparch-fall2018-lecture7-simdandgpu-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2018/lib/exe/fetch.php?media=comparch-fall2018-lecture7-simdandgpu-afterlecture.pdf",2018-10-16T22:09:38Z,"Computer Architecture - Lecture 7: SIMD Processors and GPUs (ETH ZÃ¼rich, Fall 2018)",https://i.ytimg.com/vi/4g4mvKLo0bI/hqdefault.jpg,Onur Mutlu Lectures,PT2H40M28S,false,2636,34,0,0,1,"okay can you hear me yes okay cool so if you already let's start today the lecture is lecture 7 Cindy processors and GPUs we are going to talk about one processing paradigm that takes advantage of regular parallelism in your applications last week as you remember we had two lectures about DRM in the first lecture professor mu to explain the main memory and dieldrin fundamentals and that was on Wednesday and on Thursday we had four interesting talks about the research that we have in our group related to DRM this fourth talk we are giving up by Hasan Minhaj and Jeremy so this is a agenda for today we are going to talk about Cindy processing including vector and array processors we will see they are based on the same principles but they have different characteristics and we are going to study them and then we will talk about the special type of sim deep processors that actually combines characteristics of both vector and array processors and these are the graphics processing units that are they let's say the current form of Cindy processing and more most successful form of Cindy processing these days so both Cindy processors and GPUs take advantage or exploit data parallelism we are going to see over this lecture that they are very good at exploiting regular data parallelism so every time that we have an array of data elements that we have to process we can use vector or array or processors or GPUs and they are going to perform especially good when especially when the parallelism is regular when all the computation that we have to perform on every data element is the same let before we go into the details let's first talk about the Flynn's taxonomy of computers I'm pretty sure that you're already familiar with this Mike Flynn in 1966 published this paper where he classified computers according to two dimensions and the two dimensions are instructions and data the first category is sis D single instruction single data it's the case where we have one single instruction that operates every cycle on one single data element this is like the traditional CPU model right where we have sequential execution of instructions on different data elements the second category is single instruction multiple data and this is what we are going to talk about today about including array processors and vector processors and here the thing is that we have one single instruction and because the warlock that we are going to operate on has inner end regular data parallelism with one single instruction we are going to be able to operate on many different data elements at the same time third category is multiple instruction single data and this is like a very special type of very particular types of processors that have about some some specific characteristics and some specific applications among this means the processors we have systolic arrays that you might be familiar with them they are actually being used very much these days because they form like the they are like the basic structure of the Google tensor processing unit that is used for training deep neural networks we also have a streaming processors here and another type in this category could be the microns automata processors and the last one is multiple instruction multiple data this corresponds to multi-threaded architectures and multi processors here we are going to have several thirds or multiple threats of execution and they are able to operate on different data and actually follow completely differences of completely different instruction streams but let's talk about Cindy in this lecture so as I said for Cindy processors it will be good that we have inner end data parallelism in more workloads because concurrency arises form from performing the same operation on different pieces of data that's what we call Cindy examples you have an example there the dot product of two vectors we will talk about it later but if you want an even simpler example just think about incrementing the elements of one vector of one array you have maybe one million elements in your array and you just want to increment all these elements so we are performing we are actually we're going to execute the same operation on all the data elements of the vector or think about vector addition to arrays each of it each of them 1 million elements and we need to add element by element so these are like the [Music] like the ideal cases for Cindy processors in contrast with dataflow you're already familiar with dataflow architectures in dataflow architectures actually the concurrency does not arise from operating on different data elements at the same time but on executing different operations in parallel and also in contrast to thread or control parallelism that could be something like me mainly write multiple instruction multiple data where we would have different threads of control executing in parallel Cindy as I said exploit operation level parallelism on different data it's it's a form of exploiting instruction level parallelism what is instruction level parallelism the ability to execute different instruction at the same time and this is also a way of doing it but the only particular thing is that the instruction that we execute is the same on all the different data that we operate on ok so Cindy processing operating on multiple data elements in time or in space we are going to have multiple processing elements and these are going to arrange in different manners we we talked to possible different manners that's why we talked about that time space duality this results in the two possible Sindhi processors that we can have and that we are going to study in this lecture the first one are the array processors as you will see in your ray processors one instruction operates on multiple data elements at the same time using different spaces and the other one are the vector processors where one instruction operates on multiple data elements in consecutive steps using the same space we have a nice pictorial representation of Iranian vector processors and we are going to discuss this a little bit more in detail so first they observed a code that we have here this instruction stream and three-point-three yes okay so in this okay in this instruction stream you can see a load instruction in this load instruction we are accessing four elements of array a and we are storing them in a vector register V R then we are going to increment the contents of vector register V R so that's what we are doing with this addition this vector addition operation here arbitration instruction then we have a vector multiply we are multiplying by two each of the elements in the vector array and when we are done we store the result in array a again so these four elements in array a the way that these four instructions are executed for four data elements in array and vector processor is different in here you have your time line and in an array processor the very first the very first instruction that we will issue in the first cycle is the load instruction and as you can see the four processing elements that we have in the array processor are executing the same load the same instruction on different elements because we have four elements zero one two three each of the processing elements takes care of loading each of the four elements as soon as we load from memory into the vector register we can execute the addition operation and that's what we have in the second cycle in the cell cycle the processing elements will execute the multiply operation and in the fourth cycle they execute the store operation this is how the array processor will execute these four instructions and what about the vector processor in the vector processor we have four units for from processing elements but as you can see each of the processing elements is specialized on one specific operation on one specific instruction so the first thing that we will do in the first cycle is issuing the load or executing the load for element zero and this is something that will be done by this load processing element that we have here in the second cycle assuming that we have already got the value a zero from memory and we have it already in the vector register we can execute load for element one and the addition for element zero in the next cycle in the third cycle we will have load for element 2 addition for element 1 and multiply for element zero and so on right so if you observe what we are doing and this is the time/space duality what we are doing is that here in the array processor we execute the same operation at the same time while in the vector processor we execute different operations at the same time and in space we have different operations in the same space because these processing elements are able to execute any of these four instructions and in the vector processor we have the same operation in the same space so that's the main distinction between array and vector processors we can also compare Sindhi processors and in particular in these two next two slides we are going to compare cindy array processors to very long instruction words our architectures because they are also a form of exploiting is instruction level parallelism actually the reason why this B liw architectures were proposed and where design is because many workloads there is not so much inner and regular data parallelism but still there is irregular parallelism that can be exploited so that's why what will be liw architectures deep for them to work correctly they needed a smart compiler that was able to identify instructions that were independent in the code when compiling the code and this way we could have something like this for example here you see the program counter is pointing to one instruction that contains four different operations where if you observe these four different operations they are operating on different elements this means that there are no dependencies between these four operations in contrast in the array processor what we have is a single instruction and a single operation that is going to be executed on different data elements that's what we have here in our program counter is pointing to this single instruction here at and we have four processing elements and these processing elements are going to execute this instruction on four different data elements it's okay any questions so far okay yeah let's focus more on vector processors even though we are over the course of the lecture we are going to I mean all the different concepts are going to be explained by using the vector processors but most of them they also they also apply to the array processors so vector processor a backdoor as you know is a one-dimensional array of numbers and many applications scientific and commercial programs use vectors this is something that you already know here you have a very simple code that we are going to use as our running example in this simple code what we do is what we have is a for loop that operates that goes through three vectors 50 elements of three vectors and what we are doing here is calculating the element wise average of a and B and store the result in C so because all the 50 operations that we need to execute here exactly the same they can be efficiently executed on a vector processor there are only and some basic requirements that should be fulfilled and actually are fulfilled here is that we need to load and store vectors and from memory or to memory and we will use vector registers for storing I mean for for yeah for storing intermediate data and these vectors these vector registers have a certain length so that essentially is going to be related to the number of processing elements that we are going to have in our simply processor let's take a very quick look at this with one example here okay okay so we have a what some code in particular let's think about the vector load instruction and with lives vector load instruction we want to access memory this is our memory our memory has I don't know and elements and all the elements are stored linearly in memory something like this this is element zero this would be yeah this is element zero element 1 2 and so on okay so when we want to execute this vector load instruction it's important the first thing that one of the first things that we need to define is the vector length which is the number of elements that we are going to operate with one single instruction and as I said before it's related to the number of processing elements that we have in the vector processor so the second thing that we have to define is the vector as try the vectors try defines the distance in memory of the elements between the elements that we are going to operate on let's start with the simplest case vector as a vector astride equal to 1 this means that when we execute this vector load instruction from array a and let's consider that array a starts here in address 0 what we will do is reading or loading from memory vector length elements that are at distance 1 so for example P for vector length is equal 4 we could be with this vector load a instruction we would be reading loading these four elements into the vector register so what we do here is calculating and addresses and accessing these addresses actually for vector line is four it would be something like this ok another possibility is that we have a different stride let's say that or stripe is four before stride is four then what the address is that we will access are a zero a plus 4a plus eight a plus 12 for the vector length equal to four so we would be reading this element and this element and this is probably eight this element so we will be accessing here here here and somewhere else okay yeah so this let me go back to the slides these are like some of the basic requirements we will talk like they're also about the vector mask that it's going to be useful for conditional operations so the vector a vector instruction performs an operation on each element in consecutive cycles the vector functional units are pipeline and in each pipeline stage operates on a different data element these allows us to have deeper pipelines but it is important to fulfill some requirements first there there in trabecular dependencies are not allowed so if we want to operate on let's say four elements as we had in the previous example or vector length is four we should make sure that there are no dependencies between the operations that we are going to execute on the different on the four elements that are in the vector are register second is that we won't have control flow within a vector so there are no branches the four is the four operations are going to be executed at the same time we need to know this try as you have seen in the previous example asteroid will be one or straddle asteroid will be four and also no no in this asteroid has some benefit for example the fact that we can have efficient prefetching prefetching I guess you are familiar with the concept of prefetching essentially means go into memory and bring in some data that is not going to be executed indeed immediately but after some time and this is something that is really possible in vector processors because we know what's this right so if we are operating on four elements and then we will have to operate on the next four elements because we know the stride for example stride equal equal 1 or stride equal 4 it will be possible to prefetch it would be possible to go to memory and start bringing the data from memory before it's needed so this is one of the vector processor advantages advantages but there are more because there are no dependencies within a vector pipelining and parallelization work really well and we can have very deep pipelines we can have many stages in your pipeline and we won't need to stall the execution of any instruction or a forward data because we know that all the elements all the operations that are in the same vector register are completely independent each instruction generates a lot of work and this means that every time that we go to memory and fetch one instruction we are going to be able to operate on many data elements at the same time and these the good thing is that reduces the instruction fetch bandwidth requirements because with one single instruction we will be operating on v length for example 4 or maybe 32 elements at the same time so probably one potential benefit from these is that because we don't need to go to memory so frequently we might have smaller caches and and still our processor will give us very good performance another advantage is that we have highly regular memory access patterns and this is for example in GPUs it's like the ideal way of accessing memory to get the maximum possible bandwidth and we don't need to explicitly called loops in principle there are no loops we will only use loops when the size of the vector that we need to the total number of elements that we need to operate on is larger than the vector length we will talk about that later but unfortunately vector processors also have some disadvantages and they are as we already know they are very good for regular data parallelism but they are very inefficient is that if the parallelism is irregular and still there are many applications in which we have irregular data parallelism or even sequential execution so for example what when you try to search for a key in a linked list it's an inherently sequential operation you have to go step by step and this until you reach the desired key and this is something one particular operation where a vector processor or a Sindhi processor is not going to perform well and it's not the only disadvantage another disadvantage is how difficult it's to Prolog can be to program them and this is one itself form from this paper by Fisher in 1983 it says to program a vector machine the compiler hand coder must make the data structure in the cold feet nearly exactly the regular structure built into the hardware that's hard to do in first place and just as hard to change so because if we really want to take full advantage of the processing capability processing power of the Sindhi processor we need to have very regular data structures that can be accessed in a very efficient way and can be operating on in a very efficient way another limitation is relate our potential limitation is related to the bandwidth to memory because now we are going to operate on so many data elements at the same time we really need to have a memory that is able to give us I mean all the bandwidth and all the amount of data that we need every cycle right and this is fine as long as we do enough operations on the data elements that we read from memory so if our arithmetic intensity or compute intensity is high enough this means that every time that we read something from memory we operate we execute the several or many operations on this data everything will be ok but if our computation is very lightweight and we are just essentially moving data we will be always accessing memory and this will overwhelm the memory bandwidth one way of dealing with that is using memory banks that we are going to see next ok let's talk a little bit more in depth about the vector processors we have already talked about the vector registers in each vector data register we hold in M beat values for example n might be 32 we might have 32 elements in the vector register and M might be 64 bit for example if we are operating on double position elements we have vector control registers we have already talked about them wheeling that is the number of elements that we have in the register we tried the vectors tried what's the distance between the elements that are going to be accessed from memory and then we have the vector mask yeah the maximum V Lang is a number that depends on the hardware let's say it's end and the vector mask we will see one example later but what we are going to do with the vector mask is masking the operations that we execute on the different elements that that are stored in the vector register and this allows us to do some conditional execution for example in casein or one example the example here is that we might want or not to operate on elements that are equal to zero so depending on this predicate that we have here the outcome of this predicate that we have here the corresponding bit in the vector mask will be 1 or 0 and this will mean that the operation is actually executed or not about the vector functional units we have already mentioned that one of the benefits of vector processors is that they allow us to have a very deep pipeline and the fact that we have a deep pipeline we have many stages and pipeline means that we will be doing less things in each of the stages and this way we can have a fast faster clock cycle so we can operate at a higher frequency yeah and and this is possible because we start from the assumption that the operations that we are executing on the bacteria's are independent because there are no dependencies among the different data elements ok here in this slide you can see or or one example or very first example of vector machine this is the gray one yes you can see and in this figure here we have the vector registers here on the Left we have some vector functional units here for floating-point we also have some color units because even though this is a vector machine still we need to have or it's good to have s color processors because something that we already mentioned for sequential computation for a scalar computation the vector processors are not good so it's still good to have some cores or some processing units that occur that can execute scholar operations and and here the left you have the memory that actually says it's a memory with 16 banks we will go into the details later meanwhile they'll let me show you this picture from the Cray that we have here in ETH is in cap in the building where we are so maybe you have already seen it or you might want to visit it it's a like a very beautiful matching indeed and here you have some slides about this Cray XMP actually if you compare this figure here with the one that we have just seen for Craig one you might see that it's very very similar you have some design details for example this XMP 28 has two CPUs as to a scholar course here you have some details about the functional units address functional units to calculate the address to access memory scholar functional units vector functional units and some more configuration details here and this is like we have say more Cray who is called the father of several computers yeah he said something like this if you are plowing a field which would you rather use two strong oxen or 1024 chickens well it depends on your computation right so if we are going to operate on many elements at the same time probably we will go for the chickens because I mean assuming that the computation that we need to do on the different data elements is completely dependent if there are some sort of regular computation or some sort of a scholar or sequential computation we might need to use the too strong oxen okay let's go back to the cray-1 and here in the Craig one as I said before we have 16 memory banks so let's take a look at how are we going to use these 16 memory banks and it let's take a look at how we load and store vectors from and to memory because this is a vector processor we need to access read and write multiple elements at the same time and we already know that the elements that we are going to store in a vector register are in memory and the distant and there is a distance between these elements that we are going to access we are going to operate on and this distance is the strike for now we are going to assume that the stride is equal to 1 and we are going to see how this element these V Lang elements that we have in memory at a distance 1 from each other are going to be loaded in consecutive cycles because if we really want to be efficient we should try to be able to access if every each one of these elements in one single cycle but unfortunately memory is not usually that fast we cannot access memory in just one single cycle we will usually need more cycles to access memory in Cray one for example the latency to access memory was 11 cycles and this is the value that we are going to consider in in the example that we are going to see next so what should we do if we want to have one ready element every cycle but our memory takes longer than one cycle what we do is bank in the memory what we do is interleaving elements across multiple banks we are going to see how these multiple banks are operated instead of having one single monolithic memory one single monolithic what we do is chopping this entire memory into different parts in particular in this example 16 part 16 banks and for each of these banks we have one memory address register where we are going to have the address that we want to access in the in each of the bands and also we have a memory data register where we are going to store the value that we read from memory or write to memory all these banks as you can see are going to share the data bus and the address bus so let's take a look at how the this is going to work here with with one example okay so if we have one single Bank if we have a monolithic array we will look something like this this is address 0 this is address -1 for example this is or entire memory and here we have alpha 0 others 1 and so on up to address M minus 1 right if we want to access this memory read or write we need some memory address register and we need some memory data register with the memory address register we will select one specific address here and if we are reading the corresponding value will be moved from the memory to the memory data MDR memory data register hmm please stay sorry this takes a certain number of cycles M in your example this is going to be 11 cycles it's a single part why is it single part because we can only access one element at a time we can only have one address in the memory address register so that's why we say that the symbol single part and it's just one single Bank what we can do in order to have a more efficient access to this memory is dividing this memory into banks in your example we are using 16 banks this is bank zero bank 1 bank 15 and now what we do is storing the addresses were storing the values corresponding to the different addresses in an interleaved manner so that address 0 is Bank zero address one is in Bank one other 15 is in back 16 15 and address 16 is again in Bank 0 this would be address 31 and now in each of these banks we have or memoria memory address register memory data register memory address register and memory data registers that are connected to the corresponding bank all these banks share the same address and data buses so many others register is connected are connected to the database memory data registers Arkansas resort to the other service memory data registers are connected to the data bus and now let's see one example of how are we going to access all these 16 banks what we can do because we are still we will still have the same access latency the bank accelerate latency is going to be still M cycles or 11 cycles in our example let's assume that or a stride is 1 if our stride is 1 we will in the first cycle cycle 0 we will issue the first load in this case of if we are accessing for example element 0 we will issue this load to Bank 0 right after we do that in cycle 1 we can issue the load to bank 1 and in cycle 2 we can do the same for bank to because you know in this cycle we start that the access to bank 0 and our bank latency is still 11 cycles we know that in cycle 10 we will have data element 0 right and because in cycle 1 we started the access to bank 1 right 1 cycle later we will have data 1 and then 2 3 etc and still or Bank access latency is 11 cycles but our throughput is our bandwidth is one element every cycle thanks to the use of the 16 banks this is how vector processors are able to access memory in a very efficient way there are obviously some limitations and what's the limitation it is related to this try and how this is try relates to the number of banks because now let's go to the worst case scenario in which our stride is equal to 16 what means that we are going to operate on element 0 16 32 and so on and it turns out that all these elements are stored in bank 0 so it will be impossible to avoid the entire bank access latency for all the elements that we will be reading from a bank 0 so we will start the day access to element 0 in cycle 0 in cycle 10 we will be able to obtain data element 0 but we cannot start the access to data one until cycle 11 and we will have data 1 in cycle 20 so we hung ability only our we'll have to deal with the entire bank access latency for the access to all the elements okay let's go back to the slides is that clear you have any questions okay cool so yeah so this is what we have just explained memory is divided into bands that can be accessed independently in principle and in the example and here in this slide what we see is that all the banks are sharing the same address and the same database the reason for that is cost related reason because if we want to have more buses or we want address or databases we will need to have a larger number of pins and this entails more cost when building or chips and but still because we have the banks we are able to start and complete one band access per cycle if we have n bands we will be able to sustain in parallel accesses and yeah we have already talked a little bit about that but here you can see a little bit more detail about how the addresses are calculated because we will have one base address that is the address of they are really the vector that we are accessing in memory and we have some stride that give us the distance between the console with between the elements that we are going to access we need to have this address generator that is generating the others that will be a store in the memory address register and will be the address to access all the different banks so here we just need one address generator because we are going to start the access to each Bank and a different consecutive cycle hmm this is how we calculate the address the next address is the previous address plus the stride and one important several conditions that we should fulfill in order to sustain this throughput of one element per cycle is that in principle we are assuming that the strike is one also that the consecutive elements are interleaved across banks and let's let's first explain what this means let's go back to here so what this means having consecutive elements interleaved across banks means that we have element zero here we could we have element one here we have element fifteen-year in different bags consecutive elements are interleaved across different banks we could have a different data mapping right and for example have address 0 here address 1 address 2 and so on so in that case having a straight one good B wouldn't be a good choice right because all the elements with the straight one would be in the same Bank and then we will run into the same situation as we have before with the stride equal to 16 ok and the last condition that should be fulfilled is that the number of banks is greater or equal to the Bank latency that's essential as you can see you have just seen in the operation this is going this access is going to be really if the number of banks that we have here is enough to hide the latency of issuing all these accesses and before we have finished before we finish the generation of the address and the issue of the vector load to all the 16 banks that we have we are already receiving the first data element data elements you in this case okay let's go for a little bit more detail with this code example is the same code that we have before this element wise average and now for in order to start what we have here is the color code yeah so if we yeah that this would be like the or assembly code and you have all the instructions here that we need to execute this high level code that we have here and so the and there I have right hand side you have the latency of each of these instructions so the first instruction is move we are storing most immediate we are storing in register r0 we are storing 50 because it's the number of iterations that we will carry out for this loop that we have here because it's exactly the number of iterations that we have in your original high-level code then in racers are 1 2 & 3 we store the addresses of arrays a b and c and then here we start the loop that will be executed 50 times so as you can see the first instruction is a load instruction that accesses memory they address the address contained in register 1 and reads from memory and stores the value in register for we assume here that we have how to increment addressing so just assume that this the content of this register is incremented automatically just for not having like increment operations later for just for simplicity of the explanation then we have the second load where we access the address pointed by register 2 in this address we have vector B array B so we go to memory and we store the value in register 5 we need for both operations 11 cycles is the access latency that we are considering to memory in this running example then as soon as we get register there we have registered for and register 5 we have the corresponding values there we can execute the addition and store the result in register 6 this takes 4 cycles and then we shift right you know that shifting right 1 bit is the same as dividing by 2 so that we need 1 cycle to do this shift and when we have the final result in r7 we will store the value in the corresponding memory address pointed by register 3 we repeat this 50 times and that's why we need to have this decrement and branch if not zero so we are comparing to we are checking that the value so we decrement and we check that the value of this register is 0 or not if it's not 0 we jump and go again to execute the loop so if you come if we count the total number of instructions that we need here the total number of dynamic instructions is 300 for essentially is the six instructions that we have in the loop because every six instructions we will jump for 50 times this is 300 insert plus the four move instructions that we have in the beginning that that's why we have 300 for dynamic instructions and we can count because we know the latency of each of these instructions we can count how many cycles will we need to execute the entire computation in the first case assuming that we have a very monolithic simple monolithic memory with one single Bank we cannot pipeline the access to the different addresses to the two these two load instructions here so we would need two times eleven cycle to access the source operands right so that's why total number of cycles that we will need per iteration will be 40 and the total number of cycles is two thousand four because we are going to use Bank memories register painful memories later let's assume that we already have a Bank memory we have for example sixteen banks if we have sixteen banks and we assume that these two accesses these two load accesses row to different banks then we can consider that both can be can be executed concurrently so in that way we would be saving ten cycles in every single iteration and the total number of cycles to execute or code will be fifteen hundred and four cycles the question why sixteen banks in this example we just have to load so in principle we wouldn't need more than two banks if we want to have current access to memory but why 16 banks because this is what we are going to need for the vector processor and we need 16 banks because something that we have already explained we need 16 banks because 16 is greater than 11 that is the number of cycles that is or memory or Bank access latency so that's why we always have to find a number or to choose a number of banks that is greater of equal to the total number of cycles that we need to access memory so because the operations that we are going to all the operations that we are going to be executing in each of the iterations in this loop are independent we can say that this is a vector right so we'll look this means that I mean this means what what it means it means that all the execution of every iteration is independent of the execution of any other iteration so we can say that there are not there there are n loop hurry dependencies here and because that's the case we can factorize this loop and when we vectorize this loop the first thing the first advantage that you can see here is that we have a much more compressed code we have the same number of operations in less number of instructions recall that this was actually one of the benefits of the vector processors the fact that every time that we fetch one instruction from memory from the for example the instruction cache we will be operating on many elements at the same time so we don't have so many instruction fetch the bandwidth requirements okay so this is or code observed at the two first instructions here are to set the value of the vector length in this case is 50 because we have 50 iterations in the loop the stride is 1 because you know for for every iteration we increment this index I by 1 so that's why all the elements that are going to be accessed are at distance 1 stride is 1 and then we have our vector log to access 50/50 elements from array a and store them in vector register b0 then next vector load works as array B and then we can execute this vector app have the intermediate result in register vector is 2 then we shift write all the elements contained in vector register 2 we will have our final result in vector 3 and from vector register 3 we store in the final destination in array C in memory now here on the right hand side you can see the latency of all these instructions the latency of the individual instruction of the individual operations is exactly the same for example one cycle in case of the move instructions 11 cycles is the memory access latency because but because now we have a vector processor we are going to consider that we have a vector processor and we have a pipeline execution not only in the access to memory thanks to the memory banks but also because our functional units are pipeline we will have the total number of psych cycles the total latency to execute this load from memory will be 11 plus vector length minus 1 let me just show you very very quickly one example here oh sorry so for example for the back to load in cycle 0 we start the access to the first element right after 11 cycles we will have data element zero and then one cycle later in cycle 12 we have data element 1 in cycle 13 data element 2 and so on and in cycle 60 we have or last we will have our last element so observe that during this time we are getting 50 elements from memory and every time that we get one of these elements Dave will go to the corresponding position in the vector register right but if you have served here what's the what's a total latency of this vector load is 11 that is the bank access latency plus vector length is 50 minus 1 okay in case of a vector add for example if we have a vector add in or example the vector that has a latency of 4 cycles the total latency for the 50 elements will be 4 plus vector length minus 1 so that's why we have we have these latency values here you have a question makes a lot of people doesn't if it's already pipeline yeah I think were say the 11 cycles because I mean you so you you you have to all over the entire pipeline right so so this is the thing that the very first access the latency of the very first access to memory same as the latency of the very first edition in your vector functional unit you're going to see it completely because there is nothing else in the pipeline at that time yet so you see for example in the case of the vector art you will see the four cycles for the first and because right after the first ad for the first element started you can reach the next ad for the next element as soon as the four cycle the these four cycles finish and you get the first result you will only need one cycle more to get the next result and then the next result etcetera exactly the access latency that you see completely they access latency for element zero same as the execution latency for this vector ad or this shift right operation is the one corresponding to the very first element but let me continue because we have a timeline in in a later slide that I think that it will be much more clear so yeah so here you can count the total number of instructions is seven so remember it was three hundred four for this color processor here in the vector processor it's only seven dynamic instructions and for now we are going to assume that there is no chaining chaining means that there is no no data forwarding vector data forwarding and this essentially means that we cannot use the output of one vector functional unit until all the elements in the output register are ready so yeah so this is for now we are not applying this optimization that we will apply next so this means that before so going back to the code before we start executing this sheet operation for example we need to make sure that all the 50 additions that this vector addition will execute have finished so that's what no chaining means also we assume that there is only one memory port so recall that yeah we call that in each of these in this Bank memory we have or memory address register and our memory data register this is this represent one single part because this means that every cycle we can only be accessing one single address so that's what it means we have just one single part and we are actually sharing the same part for loads and stores and there are 60 memory banks and this is how it will look let me actually show you here also with the dock you can because I think it's going to be better as we will be able to see both the code and the timeline and I think that it will be easier if I can fit everything here more or less okay I think it's okay so yeah so this is the kind that I mentioned before this first cycle here corresponds to this move instruction second cycle to this instruction and now we start the vector load we will see the entire 11 cycles for the very first element but because we have 16 banks we can issue one load every cycle so right after that we will have so in this point in time we will get V yeah we will get element 0 of array a and one cycle later we will get element 1 1 cycle 8 later we will get element 2 and after 49 cycles in this point we will have the 50 elements loaded into our vector 0 right because we have one single memory port we cannot be working with a not b loading two different vector registers at the same time so we cannot start this vector load until this point so that's why again we see here the entire level cycles plus 49 and because we have no chaining that was one of the things that we there is no chaining remember that's one of the things that that are assuming here we cannot start the execution of this vector ad until all the elements from the first vector load and all the elements from the second vector load are in the corresponding vector registers right after that we start the addition four cycles for the first element then 49 more cycles for all the 15 elements to be completed then we have one cycle for the sheet right 49 more cycles and finally the final store 11 cycles plus the last 49 cycles in total 285 cycles that are already a very good improvement almost seven times faster than or scholar code was is it clear okay yeah but we can still do it faster actually we we will we will be able to do it faster if we allow our processor to chain to the forward elements that are a result that are already produced and forward them to the next or a different functional unit and that's what we have here so in this example you can see this is a vector load we are accessing memory and loading data elements in b1 and because we have Bank memory every cycle we will be receiving one new data element from memory as soon as we get one they got data element zero for example from memory we can forward it to the next functional unit that in this case will be the functional unit that executes these multiply operation and same for the multiplied to the addition operation so it will will be something like this we have memory or load unit as soon as this load unit gets the first element from from memory that will be stored in vector one we can change it and to use it as an source operand to this functional multiplier multiplied functional unit and as soon as we get the first result from this multiply functional unit we can chain it to the next that in this case is an addition so what if or in your previous example we apply vector chaining this means that we don't have to wait for the entire load to finish before we start the execution of the addition so as you can see as soon as the second vector load finishes for the first element in this load right after these eleven cycles we can start the execution of the addition we can start counting these four cycles that we need for the execution of the addition and as soon as we finish finish the addition for element zero in the vector we can start the shift register operation right here observe that it could be the same it would be the same for the store we could start the store operation right after this cycle because right after this cycle we have shifted right one element one bit to the right so it's the same as dividing by two as you know and right at that time we already have the final result for the first element in the array right but why can we start right here why should we wait until here because they exactly because the load unit the unit that is success into memory it's already is still busy generating addresses for this vector load because we have one single memory port okay so we can do it better if we add more memory ports in or a bank yeah some days supply that's the second victim also can continue so I think I think I didn't get a question why cannot be also accessing the with the second back to load yeah so it's it's because we have one single memory port we have one single memory address register and one single memory data register in the memory address register we need to have the address that we are reading or writing and that address must be there so we cannot have two accesses to the same Bank at the same time let me show you the example actually it's going to be easier if we see it here so we have poor Bank memory right this is Bank zero and here we have bang bang and here we have Bank 15 and in the memory that we are considering we have one single memory address register and one single memory data register so if we want to access read some specific address from memory we need to have the corresponding address here in the memory address register so this would be used by the first vector load so the second vector load assuming that the obviously the second vector load also needs to access some element here in this Bank zero has to wait until the first vector load has finished and the memory address register is free that's why we can do it better if we add one second port with its own memory address register and memory data register and also we can even have one third memory port with its own Mir and ER for stores so we can read two addresses from memory at the same time and also at the same time we can write to some other others in memory so that's what what we can do to in order to still improve and we will have the same port in each of the bank's right is it clear app ok so if we apply that optimization then we yeah we can end up having something like this only 79 cycles because as you see we don't need to wait for the entire we don't need to wait for the entire vector load to finish in order to start with the second vector load same as we don't need to wait for all these vector loads to finish in order to start with the final vector store and that's why we are able to improve the performance of this code by 19 times compared to the scalar execution okay okay yeah I think we can continue a few more minutes before we we have a break let's discuss a couple of things first thing one question what if the number of data elements is greater than the number of elements in the vector register because you see in our example or total number of elements that are called or high-level code was operating on was 50 and we say okay or vector lens is 50 so we can operate on the 50 elements at the same time what usually we will have to operate on more than 50 elements in real life so what if we have to operate on 500 or 5000 of 5 million elements what will we do if our vector length imposed by the hardware is 50 or maybe is 32 or something so yeah we will have to use loops in this case so we will break the entire loop all the iterations of the this color called high level code that we had we will break them into as many iterations as necessary and in each iteration we will be operating on vector land elements here you have one example because sometimes it won't be I mean it won't be the total number of elements that we have to operate home won't be divisible by the vector length of the hardware so what if the data elements that we have are 527 and our vector length in our hardware is 64 so we will have to do something like this first of all eight iterations with vector length equals 64 and then one iteration with vector length equal 50 in order to finish the computation finish the execution for this tale of 15 elements this is what is called vector Street mining and this concept is related to a street mining that is as a form of source surface mining essentially means that if you want to access if we want to access the deposit or the mineral let's say for example goal is you have to remove all the residual materials that are on top of it right like maybe sand or rocks or wherever we have to remove them in order to go to what is really interesting so that's something similar to what we have to do with these 15 elements that we want to get rid of them execute them as soon as possible or maybe at the end in in this in the case of the vector processor and then go to the real vector processing execution that will be taking advantage of the entire vector length of the 64 function 50 60 from processing elements possibly that we will have in our vector processor and then we have a second question here what if vector data is not stored in strided fashion in memory observed at us until now we have assumed that even though the data elements that we want to operate on might not be consecutively stored in memory they are in regular organization they are at a certain distance and and and every two consecutive elements are always at the same distance that is destroy but unfortunately this is not something that is always going to happen in some specific applications for example when we operate on a sparse matrices we will need to use gather and scatter and other operations that allow us to use indirection I think I'm going to give you this example very very quickly and then we will have the break so let me quickly explain what a gather and scatter operation is it's the type of operations that will the type of memory accesses that will allow us to execute that code like this in will in which you can see that there is some indirection here so in order to give you an example let me describe how we would access one a sparse vector or or one a sparse matrix in memory by the way sparse matrices and sparse vectors as you may know are very popular these days for example they are used to represent graphs to store and graphs I are like very important data structures in this era of the big data right so this is something that we are going to very frequent frequently find these days assume that this is or array C if we call Fikret back to the example recall that we had array C and we will access array C according to the indices that we are going to read from array D so what we will have is that here we have a racy and interestingly most of the elements in this array C are going to be 0 so we don't we want to use them we only want to use the elements that are different to zero that's the type that what we do when we operate on as far as vectors on our sparse matrices and maybe we have our first element different to zero here in address 1001 and then the next one in his icing address 2050 and then the next one is here for example in address 3072 so in order to access this memory this these elements in memory what we will do is using this array D this kind of index array and in this index array what we have is the addresses where the elements that are different to 0 are stored so that the elements in already are pointing to the elements in array C so when we access memory this way what we are doing is going to first already reading this element and that is adding these 1,001 to the address of array C so that we will be getting the value that is actually stored in memory for in this case then we will do C plus 2050 and we will get tanked and so on so this is what we do with a scatter and gathered accesses and here you can see some example code for a gather operation first we access array D in memory we load the indices that are containing already we load them in the back to register D and then with this back to register D and using this load indirect instruction we access our AC and we store whatever we read from memory in Arabia and then we can continue with the execution with the computation next next instruction is a normal vector load where we access vector B then an additional finally the store in array a and it's the same for a scatter example you have a one scatter example here this is the index vector this would be the array B in the previous example this is the data vector this is what we want to store somewhere in memory observed that the difference between the previous example and this example is that in the previous example we are gathering we are reading from memory from different positions in memory that are given by the index array and here what we are doing is scattering what we are doing is writing so we will write this 3.14 in in base in base plus zero because the first index vector is zero next in that index vector is 2 so we write these 6.5 in base plus 2 okay okay let's take a break maybe yeah 10 minutes and we will continue okay I think we can continue if you're ready so now we have one new question here what if some operations should not be executed on the entire vector so what if some operations we just want them to be executed on on some elements on the vector depending on some predicate right for example here in this loop what we have is some multiplication but we only want to execute this multiplication if the value of the corresponding element in the array a is different Co so in this case is where we are going to use mask operations because these were we are going to use this vector ask register that we already defined in the first part of the lecture what we will do is having some instruction to evaluate the predicate and depending on the value of the predicate we will set the mask and then depending on the value of the mass we will execute the next instructions this is what is called a predicate execution and actually this is a way of implementing control flow instead of using branches we use this predicate register we use these masks next next week professor Moodle will explain a branch prediction and control flow handling and one of the ways of doing control flow handling is predicated execution so I'm just going to introduce it here and you will see it in much more detail next week so here you have another example with masking in this case we have a nib cell and if else if a I is greater or equal B then we do C equal a else we do C equal V and the way that we implement this is by first comparing a and B and depending on this comparison we will get someone 0 or 1 1 in the corresponding bit for the corresponding element in the vector register and we obtain B mask then we in step 2 we store a into C for those elements that have 1 in the vector mask then we complement the mask and then we execute the store from B to C so as you can see if a is greater or equal to B as is these these two cases vector mask is equal 1 so we will execute this step 2 if it's not like for example this case here we'll have to wait until day until we complement the masks and then we execute this store in step 4 the way that we actually do this in hardware might be the in a simple way we calculate the mask and then depending on the value of the corresponding based bit in the mask we will actually enable the right operation in the in the output register or not so for example here because the bit 0 in the mask for element 0 is 0 we want enable this right so this value does act has actually this opera yeah this this output operand that has actually been obtained after going through the entire pipeline won't be written into the final destination and a smarter way more efficient will be to have this density time implementation in which what we do is we first go through the entire vector mask and we only issue those operations in this case maybe those additions or those multiplications that corresponds to elements who are the corresponding bit in the mask is equal to 1 so as you can guess this is going to be more efficient so in principle it will be better but there are also some trade-offs right because in this simple implementation we just need to feed all these input elements into the pipeline and get the output and depending on the value of the mask will be right or not but here we need like a first step in which we need to go through the entire mask and then decide on the scheduling of the operations ok let's go back to some more issues related to strike and banking and now we are going to see another example because we have been talking about how to access memory how to access the memory banks and how to ask with difference rights but we have only considered two cases so far right one case was when straight was equal to one the other case that we had in the example was straight equal to 16 where all the accesses went to the same Bank how are we going to have an efficient memory accesses access do we always need to have a straight one to have a efficient memory access no with the other difference rights we can also have a efficient memory access and a because the only thing that we need to ensure if we want to be able to access all the banks at the same time is that the number of banks and the stride are relatively prime this means that they are not divisible so for example if we have 16 banks we will have we will be able to access the system balance at the same time if our stride is 1 but we will also be able to access the 16 banks at the same time before stride is 3 and actually you could do the calculation for the banks that are being accessed if or stridency h3 sorry and you will see that all the consecutive accesses are going to different banks we are going to see an example in a little bit more detail in this case for matrix multiplication first of all let's define the storage of the array that we are going to consider in memory as I'm pretty sure that you know depending on the architecture and also depending on even the programming language mattresses are stored in memory in a different way for example in C mattresses are storing a row major layout in Fortran their store in a column major layout and we definitely need to go for one of these two layouts or maybe at different organization because even though some data structures like matrices are two-dimensional or three-dimensional or even more memory is the memory is a linear array so addresses are always increasing linearly right so we have to figure out a way to map two-dimensional organization two-dimensional data structures into this linear memory array in more example we are going to assume a row major order row major layout here we have two matrices a and B and we want to multiply these two matrices so let's take a look at how these two matrices are stored in memory in your matrix multiplied multiplication this is matrix a matrix a is a times B matrix a has four rows and six columns matrix B has six rows and ten columns so here we have [Music] six columns so here we have element C 0 1 2 3 4 5 this is element 6 and so on here we have matrix B this is the first column if this is element 0 this is element 1 this will be element 9 because we have ten columns so this is 10 20 30 40 and 50 and we want to execute the matrix multiplication so if we want to execute nitrogens duplication we have as you know to execute the dot product of Rho times column so we are going to go through this row in this direction and through this column in this direction if we want to use a vector processor to perform this computation we will probably first go to this row and store it in some let's say vector register called a zero and we will store this entire column in some vector E so for example B 0 but how are these two a and B store in memory because our memory is a linear array so actually something like this even though it's a bank memory from the point of view of the addresses is still 1 linear array so our matrix a probably will start here so this is element 0 because we are assuming row your row major layout this is element 0 this is illumine Wang and I mean 2 and so on and here we have element 5 so this is Rho 0 of matrix a lilamon six until 11 so this is rolling off matrix a and so on and at some point in memory we will find matrix B this is element 0 matrix B is also storing row major layout so this is element 1 9 so this is Rho 0 and here we start with Row 1 until element 20 so this is Row 1 observe that here in B 0 what we want to store our elements that are not consecutively stored in memory we want to access and store in the vector register this element 0 this element and this element 20 so the problem that we have here is that even though for a we are accessing consecutive memory locations corresponding to this first row for this first column in V we are accessing with a certain stripe that in this case is 10 right is that a problem well it depends on the number of banks and how it relates to the stripe in this particular case we have a stripe thing and if we consider the worst case the memory in which we have only ten banks the problem is that all these elements 0 10 20 will be stored in the same Bank but also if we have let's say 16 banks observe that thing and 16 are not prime relatives so 10 and 16 share some factors right - so if you calculate the addresses and the corresponding banks you will see that some of the others is that we need to access at the same time we'll be in the same Bank so we will still have some Bank conflicts so we cannot get full bandwidth or through output from this Bank memory this is mostly what you have here in this slide so difference traits may lead to bank conflicts and what should we do to minimize them or to avoid these bank conflicts as much as possible there are different ways of doing it the the most naive idea is to have more Bank if I have more banks I minimize the probability that two addresses go to the same bank so I minimize the probability of Bank conflict but obviously this has a cost right so one possibility is to have a better data layout and this will depend on the application but in this particular case of the matrix multiplication it's something that we can actually do for example what we could do here is a transposition of matrix B so that it will be matrix B T in memory if we take this matrix and B we transpose it what we will do is this column B 0 here will turn into the first row so we will have element 0 here 10 20 30 40 and 50 so now our vector 0 he's going to restore in consecutive all the elements in vectors here are going to restore in consecutive memory locations 10 20 30 and so on and now the strike here is one so we will have accesses to different banks for all these elements that we learn to the that are going to be packed in the same vector register so our memory access will be much more efficient so that's one possibility but unfortunately this is something that it won't it won't be possible always right it will depend on the application in with the pen on the characteristics of our data structure there are some more sophisticated ideas solutions in order to find a better mapping of addresses to bank and one example are the randomized mappings the randomized mappings essentially used some kind of hash function in order to find the bank where we are going to store one specific address and in this way we randomized memory accesses and it's it's a way of minimizing the probability of Bank conflicts are an important reference here I think that Professor model already mentioned this work last week is Bob brows pseudo randomly interleaved memory in this cat 1991 so that's a very interesting paper to take a look at okay let's continue we will finish soon with this description of vector processor and of Sindhi processors recall that we distinguish in the beginning of the lecture between array processors and vector processors modern Sindhi processors are actually a combination of both including GPUs that are the prime example that we are going to cover in more detail in the rest of the lecture very very very briefly recall what we have seen in the beginning of the lecture the comparison between array processor and vector processor in a rip so we have an array of processing elements all of them are able to execute the same operations and several different operations like for example here load multiply and store in the vector processor what we have is some kind of specialized processing elements that execute all the load instructions for all the load operations for one vector or all the add or all the multiply operations for one vector so in the case of the array processor we have the same operation in the same time different operations in the same space in the case of the vector processor we have different operations in the set at the same time and so we have different operations at the same time and sorry this the same operation in a space it's here same operation in space different operations in the same time and how are these instructions executed so let's now take a closer look at the at the functional units how are you going to execute this vector art here in this vector hat we have two input vector registers AMD and one output vector register that I see how many units are we actually going to have in a vector processor we don't need to have more than one actually we could have one single functional unit where we start in put a certain input the different elements that belong to vector registers a and B and there we will have a deep pipeline and after going through the entire pipeline we will get the result in this case you have C 0 in the next cycle we will get C 1 etc if we want more so here what we are doing is essentially exploiting the parallelism in the pipeline we are exploiting the parallelism by having different computations at the same time in different stages of this pipeline we can have even more parallelism by having more functional units in this case here we have four functional units and in each of these functional units we can issue one addition in this case every single cycle so here for example we start the computation of 8 12 plus B 12 and here 8 13 plus B 13 and so on so assuming that in this example maybe we have vector length equal 28 so what we will do is divide in these 28 additions in groups of four that are issued at the same time to the four functional units so now we are exploiting parallelism in time and in space okay and if we take a closer look at this functional unit what we will see is that they look like this this would be like the vector unit structure you see different functional units maybe this one corresponds to addition let's say and this one corresponds to multiplication and absurd that the source operands for these functional units are coming from some register file where we have the vector register we talk about a partition register file or partition vector registers because depending on the functional unit here we will need to access the different elements of the vector register in this case this Lane because this is the name that each of the parts of the vector unit has this in this Lane we are executing all the operations related to element zero in the input or output registers elements one will will be here elements two will be here and so in this specific example we have four vector lanes and so what this means is that even though the vector length that we define in our code might be certain number 50 it was at the example that we had before now let's consider that if this vector length is 32 the number of functional units that we actually have or the number of vector lanes that we actually have doesn't need to be the same number so in the example that we are going to see we will consider a vector length equal 32 but only eight lanes in each of the vector units we have three vector units here one load unit one multiply unit and what one add unit and when we every time that we issue one load instruction for 32 elements that are contained in the vector register or will be contained in the vector register what we do is doing it in four cycles and in the first cycle we execute for the X the first eight elements in the eighth available vector lanes in the next cycle for the next eight elements and so on and assuming that we can have chaining here as soon as we receive the values these eight values from memory we can issue this multiply operation this multiply instruction and we can start working with the first eight elements and after one cycle we will work with the next four elements and so on so observe that then we will have the addition observe that at some point in time here we are executing 24 operations in the same cycle and if we continue the execution with one new load and then one multiply and then one add if you count the total number of operations that are carrying out here is 192 in a total of 10 cycles so it's really powerful even though we don't have as many vector as we don't have as many vector lanes as the vector length that we define in your program how can we really take advantage of this I mean ideally this is something that I mean how can we really take advantage of Sindhi processors and vector processors if for for example this is something that we as programmers can do but also compilers can help because there are some cases where we can have some automatic called vectorization recall this example where we are adding the elements from two vectors is a vector add four elements and we start the resulting array see this or a scalar sequential code we could be something like that for the first iteration we load a we load B we add and restore for a second iteration load load at a store it turns out that a compiler could notice or a programmer but the compiler can do it for us a compiler could notice that there are no loop carry dependence ease in this in this loop so it's what is called a vectorizable loop and if it's a vectorizable loop this means that we can easily convert this as color code into some vectorized code where all these vector instructions are executed or all these operations that will belong to the same vector instructions are executed at the same time so we will have one to load here vector node here vector add vector store and in a way that we are executing multiple iterations at the same time okay we will go back to the programming of the Sindhi processors but meanwhile let's have a very quick summary of vector and Cyndy processors we have this distinguish between vector and array processors we have seen that we can get very very good performance improvement but it is this is subject to the fact that we have regular data parallelism and it's important to have this regular data parallelism because this will ensure the bacteria's ability of our code recall the example of the cray-1 even though the cray-1 was the fastest vector machine it was also the fastest s color machine because the engineers were conscious about the Andals law you might have 1 million vector lanes you might have an infinite number of patrol lanes but if a small percentage of your code is still a scholar in ER in this color is sequential and cannot be vectorized cannot be parallelized and also will be limiting the performance of your code so we British you that you're already familiar with underflow and you must know not for example if 10% of your code is not vectorizable the maximum speed-up that you can get even though you have 1 billion vector lanes will be 10x so that's why create one was also the fastest color matching of its time and now we are going to take a look at some current examples or modern examples of IESA including Cindy operations you can find Cindy operations in almost every port process or nowadays intel has MMX and SSE and AVX instruction in IBM PowerPC you also have scenes instructions in harm the example that we are going to discuss now is corresponding to the Intel MMX instructions as an example of seeing the operations in a modern ASA's right before we start with the GPUs so what what they the Intel engineers notice is that there were many applications that could benefit from some form of Cindy processing in particular in the 90s graphic processing image processing started to become mainstream started to become widely used and that's why they decided to include this is instruct extensions in particular mm x means multimedia extensions because this kind of applications were the main target of these new instructions and essentially what they did was defining some new arithmetic operations on short bit numbers for example here we can add 4 8 bit numbers by using one of these packed operations in a way that from 32-bit register that can be used as a scholar register what we do is dividing it into four parts and in each of the parts we are going to have an 8-bit independent value in order to do that it was necessary to modify the ALU to eliminate eliminate calories between consecutive 8-bit values right here you see the example a plus B and the result store here in a vector in register s2 let's see a more well here you have a little bit more detail about this there is no real need to define a vector length for these specific instructions because it's the opcode in the instruction the one that the term means what's the data type that we are going to consider assuming that our vector registers are 32 bits in total we can have so are 64 bits in total we can have 8 8-bit bytes or four 16-bit words or so long one important restriction is that the stride is always equal to 1 you can read all the details in this paper but now let's take a look at this example in this example we want to overlay the human in image 1 on top of the background image 2 this is what is called an operation that is called chroma keying right this is what you see when you watching the TV the weather forecast for example and so here we have the weather woman and this is our background in these particular cases our blossom black background and this is the result that we want to get if you want to see some sequential code for this it's as simple as this we will have to go through the entire image the entire image X I X that is this image with the with woman and chroma behind her the blue promo behind her so if the corresponding pixel of the image is blue then in the new image what we are going to store is the corresponding pixel in the blossom image if if it's not blue because it's the shape of the woman then we will store the original pixel and that's what we can do much much faster if we do it in parallel with some kind of Cindy processing capability right with some kind of Cindy processor so this for these kind of operations these instructions were included in the Intel ISA and here you see the very first thing that we will have to do is generating a bit mask and with this bit mask what we are going to do is evaluate in this predicate we need to compare the value the values in vector register mm3 that are coming from image eggs to a vector register where all the elements are equal to Lu with this part compare equal by instruction we compare these two and we store in the output or in the output vector 0 if if the corresponding pixel is equal to blue or 1 if the corresponding pixel is sorry the only way around 0 if it's not blue equal if it's so 1 if it's blue so then the next thing once we have the once we have the bitmask in array mm one we have to execute two more instructions pack an and that is here and pack and not pattern is to mask the blossom image with the mask and pack and not is to mask the woman's image the result will be something like this as you can see in the output vector registers only those pixels that are really useful for us are remaining so here these pixels from the blossom image correspond to the part of the image that is the actual background and these pixels here from the woman's image are the pixels that actually correspond to the woman we finally finalized by pouring mm4 and my m1 in a way that we will get the final image you have the code here you can read more details in the paper and yeah but I guess that this is a good example of what Cindy processing missing in current CPUs these days and now let's go let's go to the GPUs graphic graphics processing units that as I said in the beginning are the let's say the most successful type of Cindy process or these days and actually as you're you know they are some kind of combination of array and vector processors we are going to see a little bit more details in the rest of the lecture one interesting thing here is that and/or actually one of the things that has made GPUs to become mainstream these days is the fact that now programming them is much easier because we are not going to use seemed instructions anymore now the way that we program GPUs is by using threads you will see how we will go back to the parallel Civil Code that we were analyzing in the beginning in the first part of the lecture but before that we are going to distinguish between two important concepts programming model that is something some software related concept and execution model that is a hardware related concept and we will see what's the difference and we are going to see how these two concepts apply to GPUs what is the programming model refers to the way the programmer expresses the code for example you as a programmer can write sequential code or can write some data parallel code for example in the form of seeing the instructions or can write data flow or some kind of multi-threaded code with maybe openmp or maybe P threads in order for for for execution among meanly machine that's a programming model the execution model is how the hardware is actually going to execute the instructions that you write in your code and the way that the hardware is going to do that will depend on the underlying hardware you might have an out-of-order processor you might have a vector or an array processor you might have a GPU or a multi threaded processor this distinction is essential to understand what GPUs are these days because in the GPUs we have a programming model that is called SPMD SPMD meaning single program multiple data observe that we no longer talk about the instruction about the instructions in the programming model we talk about programs and this s PMD programming model is executed or implemented by Cindy processor it's going to be an underlying Cindy processor the good thing here is that we are to have much more flexibility and still be able to achieve very high throughput thanks to the Cindy processing units now let's go back to the code how are we going to exploit parallelism here in this vectorizable code that we have been discussing over the course of this lecture well one way to exploit the instruction level parallelism here is simply use a cinema chain sequential execution or we can go for a date apparel matching like a Cindy processor or maybe some multi-threaded matching or multi-core machine for example Mindi type of processors or SPMD let's start with sequential thing we can execute this on a pipeline processor you're already familiar with pipeline we can execute this on an out of order execution processor that allow us to exploit instruction level parallelism or by having some instruction window where we can have different instructions executing at the same time actually if we execute this code in an out of order color probably what will happen is that the out of order core is able to execute several of these iterations at the same time because the out of order course as you know they let's say they do some kind of hardware base and rolling so this loop here would be dynamically unrolled by the hardware because the the the processor can start issuing instructions that belong to different iterations another type of processor where we going to execute the sequential code is for example a superscalar processor or a very young instruction word processor because there are different instructions here that are independent they could be packed into the same large instruction and be issue two several processing elements that will execute this since this operations at the same time so this is the first programming model the the sequential programming model what about the data parallel model Cindy well because of the fact or because of the realization that is each iteration is independent we can have all these instructions or all these operations being executed at the same time so for example these two loads can be or more loads can be in the same vector instruction you already know how this works we will be accessing array a in memory and getting some vector length elements from array a and storing them in the vector register 1 in this case then we have the vector load for array bead and we have one vector addition we had the element wise addition and store the result in vector register 3 and then finally we store so we can execute this addition as in the machine as the ones that we have already explained in in in this lecture in a vector machine or in a vector processor or in an array processor and the third possibility is having a multi-threaded code multi-threaded programming model where instead of having a better instruction what we have is different threat of execution and each of the threads is going to take care of each of the iterations of the loop and this is what we typically execute on myndy machine for example if you take this code and paralyze it by using P threads or by using open impede what you will be doing is creating one thread for each of these iterations and same as we can execute this and I mean the matching we could also execute it on the so called particular model SPMD single program multiple data and this single program multiple data is the programming model the GPUs use and can be math can be executed or implemented on simply machine so that's what GPUs do yeah so GPUs are simply machines Cindy processors but instead of being program with simple instructions they are programmed using threads each of these this thread has its own context so they can be treated independently we are going to take a look at the details later and the way that these threads that the programmer defines using this SPMD programming model are going to be executed is by being grouped into something that is called a lot or away from the terminology depend on what GPU bender you go with so yeah we will call them warps in the rest of the lecture so what the hardware is going to do is all these threads that you as a program has have defined are going to be grouped in sets or collections of a certain number of threads in particular 32 in media architectures 64 in AMD architectures and these 32 or 64 are going to be or vector length and are going to be executed on the available hardware the underlying Sindhi processor OC or Sindhi vector units that we we will have in the hardware let's take a closer look now this 4 here what in your cindy processor was a vector instruction now it's going to be the the the set of load operations that are going to be executed by the threads which belong to the same world in this case you have warp 0 and then for warp 0 we will be creating the instruction for the next load and then the addition for this warp 0 and then they store for this warp 0 so what we do as programmers is defining threads assigning each of the iterations to the different threads and then the hardware is going to take groups of this thread group of for example 32 of these threads are going to put them together in something called the warp and they are going to be executed on the Sindhi processor ok yeah let's continue here we have like the comparison of Cindy and Sinti that is another way of calling the SPMD programming model and there are two major advantages of doing this way let's take a look at them the first advantage is that each thread can be treated separately we can execute each thread independently on any type of scalar pipeline we can do some kind of mini processing and then we will see that it is potentially possible to group threads into work flexibly so let's first take a look at this first advantage we talked about multi threading of warps because GPUs are not only an alliance in the processors but they also take advantage of other processing paradigms of course they are pipeline processors and they are also multi-threaded processors we talked about fine grain multi-threading because the way that the hardware is going to execute the instructions that correspond to the different warps is in a multi-threaded way instead of saying okay we have my I have my warp 0 here and I need to execute these instructions for warp 0 and this and when I say this when I say I'm saying these instructions I mean this load load at store instead of executing all the instructions for warp 0 recall that warp 0 is the collection a collection of 32 threads instead of executing the first load for work 0 and then the second log for work zealand and the addition for warp 0 what we are going to do is interleaving the execution of different warps so that's why this is like this cartoonish representation you can see in so in one particular point in time you will be executing load for work 0 but then you the hardware might execute the load for work 1 and then maybe the addition for work 20 obviously it cannot start executing the addition for work 20 before executing the load instructions for warp 20 so if in this third cycle we see that we are executing the addition for work 20 is because in previous cycles we already executed load and load we already executed that the corresponding operands from array a and from array B so as soon as you access array a as soon as you access array B the next instruction the addition instruction is ready to be executed but in the meantime you are that the hardware does the the work scheduler is issuing instructions belonging to all the warps on to the hardware here we can see it in a little bit more detail this will be a warp we are talking about 32 threads these 32 threads are packed together they will execute at the same time it is said that they are executed in lockstep because they always execute at the same time they and for this reason they have a common PC this is our sim D pipeline here and we start issuing instructions on this singly pipeline and executing instructions on the functional units that we have in this cindy pipeline and these instructions come from different works were seven were eight warp three so maybe in cycle 0 we issue the load instruction for warp zero and in the next cycle in cycle 1 we issued the first load instruction for what 1 or maybe for work 20 and then the next instruction might be the second load for warp 0 and so on the way that these instructions are scheduled is quite free in the sense and assuming that the dependencies are of course respected but the warp scheduler has the ability to identify what works are ready to be issued and then issue the corresponding instruction here you can see a little bit more detail about what we are an entire GPU you have memory here with the corresponding memory controller some type of interconnection Network and the shader course this so all shader cores are actually what we are calling the cindy pipeline or the cindy course and within each of them you can see that we have program counter alameen actually we have as soon as in many instances of this program counter we will have a different encounter for each warp we can also have a mask same as in vector processors we had our masks we have an instruction cache here we have the decode stage and right after that we can start the execution on the Cindy pipeline that actually as you can see is composed by a number of scholar pipelines that represent each of the vector lanes recall the concept of vector lanes that are all the different functional units that we had in the in the cindy core one of the advantages of the GPUs is that they execute this fine grain multi-threading recall that idea said the work scheduler is able to schedule whatever its instruction is ready to be executed we talk about eligible instructions another observer depending on those ready instructions or eligible instructions the the scheduler will decide for example here to issue one a specific instruction maybe a load instruction or an additional instruction for warp saving and then for other works and then here warp 8 and then here warp 3 so this is what is called the fine grain multi-threading because we are interleaving instructions that are coming from different warps and the main advantage that this way of execution has is that we are able to hide long latencies to in some specific parts of the system for example when we access memory obviously it's going to take longer than when we execute one edition or one multiplication right recall our vector or example of vector processor and there we considered that the cycle the number of the bag access latency latency was 11 cycles in current GP used the latency of a memory access from the from the Cindy pipeline to the memory get reading some value from memory takes hundreds of cycles and that would be a problem right if we are executing one word instructions that belong to the same warp and we have to wait for 200 cycles until we get some value from memory and then we have to we execute that second load and and and we get another value and another 200 cycles that we will have to wait that will be actually very inefficient right because we can not start the next arithmetic operation for example an addition operation we cannot start it at until we get the values from memory the good thing with the fine grain multi-threading architecture is that while we have some work some words that are going to memory to access data from memory we can keep executing instructions from other words instructions from other words that are ready on this course that we have here on these scalar pipelines that we have here or these vector lines in this alien use so that's what is called the like latency hiding mechanism we can hide long latency operations like memory accesses by executing other instructions that are that finish much faster and are ready to continue the execution much faster so that's essentially one of the main advantages of GPUs when accessing memory so yeah you already know this is light and actually we were using this is like when we were talking about vector processors now we can use it again to talk about Cindy as to to talk about GPUs because essentially GPUs are some form of vector or RI processors now instead of having one vector instruction what we consider is that we have one word instruction and this operation here this addition of two elements a and B from arrays a MV and result being a store in array C it's going to be the equivalent thing to the vector instruction instead of being a vector add this is a 32 thread word instruction and instead of talking about vector units observe that with the same picture we can talk about Cindy execution units and vector lengths in the GPU so instead of talking about a partition register file what we talk here is about registers for each thread and remember you might remember that the in the in the previous slide what we have here was was element 0 4 8 and elements 1 5 9 and here where we have now these elements but they are in the registers that correspond to a specific threats for example thread 0 4 8 or thread 1 5 9 and so on also this is like it's we have just shown it before applied to vector processors now we use it to explain how our work is executed assume that we have 32 threads in each work but we only have 8 vector lanes we only have eight of these color pipelines in each of the scene the pipeline's that we have just seen before so in the load unit we issue the first instruction for warp 0 you see now instead of talking about vector instruction we are talking about warp instruction or instruction forward zero in this case we issue on the this instruction on the load unit and we start the execution because we only have eight lanes we cannot start the execution for the 32 threads in the same cycle we have to do it in four cycles that's why we will be doing it in a way like this right so for the first eight eight threads then the next a thread and so on and in the multiply unit in the same in the next cycle the scheduler could schedule one multiply instruction and this multiply instruction the work scheduler has decided that is for work one so here we will be executing a multiply operation for the eight first first threat of war one and in the addition you need in the at unit we have an addition operation for warp two and then a new load operation for warp three work for work five you see yeah so there is I mean I guess that their demo team the real motivation for that if you go to the for example Nvidia or GPU architectures the real motivation might be related to engineering decisions if you take a look at the evolution of NVIDIA GPUs for example you will see that the GPUs that were launched in 2006 or 2007 are like this one they have 32 threads per warp and eight lanes in each of the sim de pipelines that they call this to be multi processors by the way if you go to the next generation that was launched in 2010 you will see that it has 16 vector lines for every 32 threads if you go to the next one that was launched in 2012 there you have 32 lanes but interestingly the last generation if if I'm not mistaken they have returned to the 16 lanes so I guess that it depends on engineering decisions there is no specific reason in terms of the theoretical point of view that we are analyzing here okay one one thing to keep in mind here is that the work scheduler this unit that is in charge of getting instructions on issuing them on the hardware on the execution units and the garbha scheduler is not going to issue one for example multiply operation for warp zero until the load operation for warp zero has completely finished I mean if they both are depending on each other if the multiply operation will use the output of the load operation obviously okay yeah so this is a little bit more detail about how the threats are grouped into words they are you in this figure data you have 16 threats and here we are assuming that they work size is 4 so from 60 threats we the hardware creates four works of four threats here you see some simplify simplified CUDA code that is CUDA is a programming language to program GPUs observed that this for loop has been decomposed into this CUDA kernel that actually the kernel is a CUDA function and as you can see the the addition here is exactly the same we read from memory and element in a for an array a we read an element on array B and we add and store the resulting array C observe that instead of having a loop as we have in the sequential code what we have here is a thread ID however we calculate this thread ID and then we will be accessing in a b and c the corresponding elements of the arrays by using these thread index here you have very similar code a little bit with a little bit more detail actually this is for a two dimensional organization of threads as you can see we have thread index x and thread index why because in CUDA we can define threads are arranged in a linear way or two dimensional way or three dimensional way okay yeah in traditional Sindhi we have one single thread we have sequential instruction execution instruction operations are executed in lockstep the programming model is Cindy in GPU work base Cindy in principle they don't have to be executed in lockstep but actually they are went because these works are Cindy it's indeed and each thread can be treated individually so and we don't need to know the back toilet we don't need to know but we know right we know that is 32 for example it's a website in immediately textures but the programmer can define as many threads as he or she needs and then the hardware will take all these threads and will start packing them and putting them together in different warps of 32 threads the is a is this color because the way that we write the code is as every thread was independent of any other thread and this is actually something that you have just seen in the in the previous sample code and this is called the SPMD programming model so in SPMD defying threads they can synchronize at some point we are not going to enter into this detail maybe in a future lecture we will have a chance to talk about GPU programming but we won't spend time on that right now what we are going to finish with right now is with the second potential advantage of Sinti architecture recall that the first thing has been that each thread is treated separately the second potential advantage is that the hardware could potentially group threads flexibly this is actually something that doesn't happen in current architectures but wouldn't be that difficult to implement and would provide some more efficiency from the hardware point of view and this could be necessary to increase the efficiency of the program execution because the SPMD programming model allows us as programmers to write a code in the way that we want we we are allowed to write a code as if we were writing the code for a mini machine we're because we are writing through and every thread can be doing a different thing the truth is that if we really want to get maximum efficiency from the GPU we should try to have those threads that we know that will be in the same warp doing the same thing right but sometimes because maybe we are not very good programmers or because our program or application is so irregular that it's impossible to do we will need to do to use control flow instructions and and and and when we use conditional control flow instructions what might happen is that we have let's say this is a control flow graph of or for code we will have threads that which in principle are going to belong to the same world we will have threads following different control flow paths for example here we have thread one going this way through basic blocks ABC E and G and then we have threat to that goes this way and thread three and so on so each of these threads are going through different control flow paths and this is something that is not in principle is not allowed in vector processors but here it's something that we can do and the way we can we are going to do it is using something similar to the vector masks that we actually had in in vector processors this is a way that we are going to implement branch divergence so here you see one example this could this could be a control flow graph form for our forum if else depending on the value of certain variable or centers and certain address in memory we good go or one thread will go through part a or through path B the way that this is executed is by using a mask actually let me give you a very brief example here okay so in the slide let's go back to the exile to us right briefly in the sliding or control fragra graph what we have is the branch and here we have path a and here we have path B and we see that some threats this we can go see that this is a warp okay so some threat of the war will go through path a other threats will go through path B how is it actually executed in the hardware so what we will have this is maybe some instruction before the branch and here we have the branch for for this first instruction we have all or 32 threads that we learn to the same wire maybe for example warp 0 are executing all at the same time there is no conditional execution for this instruction here but then we arrive at the branch I don't know maybe in the branch we have something like if a I is even for example or if it's odd we will go through one path or through the other path so what we will do here in the branch is calculating a mask so for all these 32 threads from 0 to 31 we will have one beat 0 or 1 in the mask and this may be 0 then the 32 threads go through path a but actually only those that have one in the mask will be really executing maybe the other ones are also executing as well but recall the simple implementation of the of the mask they won't be committing the final result to the corresponding output register right so the actual execution it's only for these threads where for which the mask is equal to 1 and then we go through path B so before going through path we we have to complement the mask such that the beats that they were won before now are 0 and the other one the ones that were C ran our 1 so we will have now these thread working district working and maybe someone else somewhere there so this is how GPUs implement predicated execution as you can see it's a similar way to the conditional execution that we have seen before for the vector processors and about predication as I said you will have more details next week so one potential so what's the problem here the problem here is that when executing path a and when executing path we we don't have all threads working at the same time all the threads of the work working at the same time so to some extent we are wasting that vector lanes right so one potential thing that we could do is grouping those threads that actually are active and can be a group because they don't overlap the idea would be something like this because recall in GPUs we have different works working at the same time on the same instructions so recall path a we we execute path a four-vector for work zero and then we will execute probably path day for work one and then path B for work zero and then a path B for work one so if we have two works warp eggs and work Y that are going to go through the same path one thing that we could do is merging with them as much as possible in a new newly formed work that in this case will be would be warped said in a way that we move these threads from work Y to work X because the corresponding thread in the in work X is not going to be executed because it's max it mask is our predicate value is zero it's still in some cases we won't get any actual benefit because as you see still we have some overlap here we if we have some overlap we cannot completely merge the two warps here you can see one example what we could do for two words the blue and the or pink or red one in some cases we can merge threads belonging to the two warps in a new form work and here you see one nice example of what we could do in the execution of an entire power so this would be the baseline observe that there are many like empty slots because the mask is zero for those cases by using the dynamic War formation we will have dynamically form Barb's here you see because there are there is a nd empty slot here we can move this thread from here to here still we don't get any benefit because there is some overlap but in other cases we will have some benefit like for example for this instruction C and instruction the where we are reducing for work instructions into two work instructions and finally we will get some savings in terms of execution time okay yeah we are almost very close to finished weather with the lecture today this is just one last thought about this about this idea of the of the dynamic war formation because let me show you let me go back okay yeah so observe here here in this case for example we have a path a for warp zero path a for what one we see that we can form a new warp here because we can merge somehow threats from these warp one can be moved to their slots that are available in work zero but still we have the problem that we have some warps some threads that overlap right so because we have some threads that overlap we will still need to execute two instructions or or two warps but observe that we have some free slots still here right so we could potentially move these guys here right and then end up having one single work or one single instruction well this is something that we could do and this is what this slide is about can we do it can we move flexibly any threat to any Lane well we could we could if we allow the harbor to do it and we add the extra hardware that is needed but serve that the problem with this would be that moving one thread from this Lane to a different Lane we require to move all the registers that are the operands with which the corresponding thread works and these registers might be a lot of them so usually GPUs are like maybe 128 256 registers registers per thread so it's quite a bit of data movement from would be quite a bit of data movement from one lane to another Lane which probably will kill all the potential benefits of doing that okay this is almost all except just a few last last slides to show you some real world GPUs here you have NVIDIA GTX 280 fight that was launched in 2008 it has eight cores each of these course is what we have called a Sindhi pipeline and within each of these Sindhi pipelines we have eight vector lengths this is a example that we discussed before the pictorial representation would be something like this here this will be like that the entire seam the pipeline and here you can see the different functional units different the eight vector lanes and in particular in each of these lanes we have two processing elements one of them is for multiplier operations the other one is for multiply operations we also have this on cheap storage these registers that will be later partition and and among all these available lanes for the different threats here you can see complete big picture of the entire GPU and here you can see a little bit about the evolution of the nvidia gpus since 2009 were when this 285 was launched until 2017 this is the most powerful GPU up to date and you have seen you can see how much the the number of stream processors that is the number of vector lanes the total number of vector lanes in the GPU has increased from this number maybe it was 240 right until more than 5,500 or so and also how much the total throat put in gigaflops has also increased and this is the b100 it's very in here you can see the a in block diagram it's interesting because now in each of these sm's that are this Cindi pipelines what we have called Cindi pipelines you can see the different functional units like we have for example these are floating point 64 so it's for double precision operations here we have integer processing elements floating point for 32-bit single precision and then they also have this new fancy thing that they're called the tencel course that our specialized course for deep learning for training neural networks essentially they implement in her where a super optimized matrix multiplication okay this is all thank you very much for your attention yeah maybe in a future lecture we can continue talking about this and introducing a little bit about GPU programming but we will see if you have any burning questions or maybe we can keep talking after the lecture thank you very much "
5K3hTWboOxc,27,"Computer Architecture
About this course: In this course, you will learn to design the computer architecture of complex modern microprocessors.

Subscribe at: https://www.coursera.org
https://www.coursera.org/learn/comparch",2017-12-10T06:11:32Z,Computer Architecture - Introduction to Parallelism,https://i.ytimg.com/vi/5K3hTWboOxc/hqdefault.jpg,intrigano,PT18M1S,false,1457,5,0,0,1,okay so we're today we're talking about parallelism and synchronization from a parallel paralyzation perspective we're worried here about parallel programming and how to build computer architectures that can run simultaneously multiple programs or multiple threads within one program and today we're gonna talk about some models of that synchronization and some primitives that help you solve it like synchronization primitives and memory fences this is a little bit of background and motivation for nowadays why we're going to multiple processors so we're a show here Moore's Law plotted on a log plot number of transistors going up exponentially our sequential performance has sort of gone up and then sort of stopped at this point if you go look at the new core i7 or some of that it's kind of flattened out here it isn't continuing to go up and in fact the numbers that they actually publish for sequential spec ins are actually parallel numbers these days because they're using multiple cores to make spec in a inspect go a little bit faster and clock frequency has also find out we didn't get to our 10 gigahertz processor and largely that was due to us hitting some power challenges here but is also just incredibly hard to go design these very high frequency processors so instead we started used these more and more transistors to implement multiple cores so we started off with two cores and then four cores and you know we've gone up from there Intel I think is now selling a eight core part and their ten core parts coming out soon AMD has a 16 core part but it's not a true 16 core part I think it's to a course or glued together on it on a chip and there's some other reader stuff up here some actual many cores multi-core processors with more more coarse but people did multiprocessor build multiprocessors before the multi-core revolution here back in these days there were multi processor systems you could take multiple chips and put them together somehow into small systems or large systems and for the rest of class between now and in the term we're gonna be talking about parallel computing systems both systems that have multiple cores on one chip and multiple chips in a system and for there's some subtle differences between the two of those things but a lot of commonality in today's lecture we're going to focus on symmetric multiprocessors and memory systems for sharing of data and using shared memory to share data but there are other ways to share data and what are we touching on that in two lectures when we talk about messaging in more detail the I bring up symmetric multiprocessors because it's the simplest model the reason about all processors are up here and memory is down here and so soon there's no caches in the system to begin with and everyone is equidistant from memory and it's one big memory you've multiple processors which can either be running multiple threads in one program or multiple programs a couple other interesting things is you know these concurrency challenges don't come up just from having multiple processors there are other things they can go and communicate with memory or communicate via memory we have all these different IO controllers down here network cards disk controllers graphics cards and they also want to read and write memory so in your memory system even on a uniprocessor system there are multiple agents that want to read and write memory simultaneously ok so let's talk about synchronization the dining philosopher problem is an example of a synchronization challenge and we're gonna talk about two major synchronization ideas but there are more than are shown on the slide mainly there's sort of broadcast models and other things like that but for right now what is a synchronization it's some way to synchronize communication or arbitrate communication in a restricted fashion so we're going to have to hear where's how about producer-consumer that's the figure on the right here a producer as the name implies produces some values and the consumer consumes the values this is the most basic form here one producer one consumer you could think about having one producer multiple consumers or multiple producers multiple consumers or multiple producers one consumer a lot of this same ideas hold here so that's that's one model that people like to use another model of people like to use there's some shared resource and you want to make sure that not more let's say then one process or is trying to access that shared resource and this resource could either be a disc a graphics card or it could actually be a location in memory and we're gonna call this mutual exclusion and the reasons called mutual exclusion is its exclusive who can access the resource at one time now to generalize form of this that's the most basic form is only one processor or one entity can go and access the resource at a time a more general form of it is some number of processors or resources can go access that at one time and this is the more general semaphore question which we'll be talking about a little bit later so what I mean by that is you could have P processors or let's say P is xx and you have a resource that no more than two processors can go access at the same time you might say why would you want to do that well a good example is something like a network card that has to outbound queues will say so you can actually have multiple processors using it but if you try to have three processors use that one network card at the same time well is only two up on cues and has to decide somehow to share those so that's an example of a true semaphore versus just a mutex okay so let's go through some code examples here because these are good to be instructive and let's look at actually before I do that you can even have on a uniprocessor system synchronization challenges and how is that possible well as I said even in a uniprocessor system you sometimes have multiple concurrent entities so our instance your disk drive doing a DMA into memory at the same time as a process is trying to read from that block for instance and you need to guarantee for instance that the disk block is completely read before the processor goes and tries to read the block that's one way that's happening in your processor system another way is happening in processor system is you t processor systems can many times be multi programmed so the operating system would actually time slice between different programs and you'll get in early of instructions in a pre-emptive environment at least of different processes at the same time so you might be running one process the timer tick goes off you stop that process you switch to another process to start executing code from that some point the timer trick goes off and you switch back to the first process so you get on a uniprocessor system you can have synchronization challenges okay so let's let's look at a producer-consumer example let's look at in the abstract first so we have producer here a consumer there we're going to use memory and one big symmetric block of memory for all of the communication at this point we have a queue between the producer and consumer shown here and then we have a head and a tail pointer so it's a FIFO says where the are first in first out queue it's gonna say where the head is where the tail is I can sort of move after each other and let's say it's circular so they wrap around at some point you have a register value here on the producer and you can move the tail pointer into this register and then we have three registers over here the tail pointer register the head pointer register and a register for the value we receive and the reason I point out these different registers is just to show that the producer owns this register the consumer owns these three registers and as you can see here this is our tail and that says our tail so there'd have different copies of our Channel and they could potentially get out of sync here we'll never see some some fun high jinks happening okay so let's let's look at a basic sequence of code here where the producer wants to put an item X into the cube for a consumer to read sometime in the future first thing it wants to do is it's going to read the tail pointer into its register it's going to then this a load store architecture here it's going to do a store of value X into the tail which is going to put it here now we're gonna have to bump the tail we can't do this atomically so we actually have to add 1 to it in our register space so we just increment it in our own register and then we're gonna store this back into the pointer in memory seems simple enough let's look what the consumer does consumer loads the head and the first thing that's really gonna do here is it's gonna try to figure out if the head equals the towel because that will mean that the queue is empty and it just needs the block loads the tail pointer so it was the head pointer in this register teleporter in that register and sees if they're equal if they are equal it's going to jump up here reload the tail pointer into the tail to see if it got updated by this thread and just spin here for forever until the tail pointer is not equal to the head pointer which means are some data available and then we can fall through and in the fall through case we are going to load from the head into our so into this register this is so we did the read and we're going to update the head pointer to the next location save it off and then do something on this value we got so processes just do do something so let's programs a little naive cuz we've been talking about out of order processors we've been talking about out of order memory systems and in a perfect world this is assuming that instructions are executed in order and that there's no interleaving between the consumer and the producer here and vice versa in the memory system so anyone think there's any problems of this homes this just worked fine yep okay so so you're saying if the consumer runs before the producer does anything you might just try to take values off the queue well so let's guard against that and say the the start case is head pointer equals tail pointer so that won't happen you'll sit spitting here now because no one's like jumping up and screaming up and down at this point you're sort of all zooming that these operations happen in order well we've talked about out of order machines which reorder loads relative to stores we've talked about very interesting interleaving we talked about out of order memory systems so much.we jumping up and down here and saying what do we guarantee about loads and stores to different addresses so we said all we know is that a load and a store are skewing me a store and then a load that it is serialized in that order on the same processor to the same address will happen in order our out-of-order superscalar in our verse out of order memory systems we talked about have defined nothing about ordering between processors or even memory operations inside of one processor so that's what we're really gonna be talking about today is how to reorder those instructions and how to prevent that from causing havoc okay so let's let's look at this and let's number these purple instructions so we're a number store store in these two loads and the reason we we look at those in particular is this is basically updating states that this thread is reading from and these are the two instructions that update that thread and these are the two instructions that read that thread or read that state rather so the programmer assumes that there's some level of causality here incorrectly producer is assuming if you look at this code that if three this load happens after this store here that there's some relation between one and four namely that one has happened by the time for happens not a good assumption and our memory systems especially in our out of order memory system because if we recall our our reporter memory system said nothing about store ordering we talked about the only thing we talked about was having a load the same address as a store and not moving that load past that store but otherwise these two stores our out of our processor can very easily reorder and our out of our memory system can very easily reorder likewise over here there are just two loads these loads can happen completely out of order in an out of order computer and out of our memory system no guarantees there so this assumption is bad in a multiprocessor system uniprocessor system might make sense multiprocessor system bad assumption so let's look at some sequences that are problematic I mean namely let's say two and then three happens and then four and then one happens so what happens when that well to update the tail 3 says Ruiz the value of that new tail pointer but at this point we actually haven't stored the value into the queue because one never happened we go execute for and it just reads garbage from memory and then finally the value gets updated totally valid in order for memory systems we talked about up to this point nothing wrong here ok let's say four and then one happen so goes to load load the head then the value gets written and then two and then three happened this is just completely out of order kind of no good semantics here so this piece of code which looks like it should have the programmer would assume to have good semantics basically has no no useful semantics unless we define what those semantics are you 
ybqZNgUBRkg,27,"In this lecture, basic structure of IAS computer and its Memory formats has been discussed.",2021-01-01T18:56:35Z,Lecture 09 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/ybqZNgUBRkg/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT11M47S,false,45,2,0,0,10,my the first electronic computer to be built at the institute for advanced studies in princeton's new grc machine [Music] [Music] buffer register memory address resistable instruction register control signal circuit signal generate corresponding shift control circuitry memory buffer foreign [Music] output equipment [Music] [Music] foreign foreign foreign 
4_ju0Ml8Yi0,27,This video contains the description about MISD computer in Flynn's Classification in Computer Organization.,2020-01-08T10:59:32Z,PART-3: FLYNN'S CLASSIFICATION OF COMPUTERS | MISD | FLYNN'S  CLASSIFICATION | COMPUTER ORGANIZATION,https://i.ytimg.com/vi/4_ju0Ml8Yi0/hqdefault.jpg,DIVVELA SRINIVASA RAO,PT8M1S,false,312,3,0,0,0,now the third type of computer is MISD computer multiple instruction stream and a single data stream so this is the M minus B or the magician or a - big architecture this a my SP architecture mainly consists of young number of control units I young number of processing units and the one only one shared memory unit so this shared memory unit can be divided into several number of memory modules that are memory module point to memory module yes okay now so each every control unit can read the instructions from the corresponding memory module as instruction stream so that are stranded to the corresponding processing unit so the processing unit can execute instructions that are standard from the control unit one with the data taken from the shared memory unit now processing unit one execute that instruction whatever the output that we are getting that output can be extended to the processing unit to this processing unit can read the instructions from sending from the control unit to I execute that instructions with that data taken from the processing unit one okay whatever the output that can be obtained from processing unit two that are standard to the next processing unit suppose this is the next processing unit B Union so P Union can execute the instruction that extended to the control unit young with the data that is obtained from processing unit previous processing unit okay so then execute the instructions there whatever the output we are getting that output can be sent back to the processing memory module and data stream okay so this is the process can be done in a - D computer ok this MIMP computer has only de a - d computer has only the political concern but there is no practical implementation okay up to now there is no computer has the my SP architecture okay so once again I am Telling what how the execution can be done in a - D computer in a my USB computer it consists of young number of control units that are control unit one to control unit again I'm young number of processing units or the data processing unit one to processing unit young and I'm one shared memory unit this shared memory unit is divided into young number of memory modules and many one mm - mm yeah okay now there are two units order each and every control unit can read the instructions from the corresponding memory module as a instruction screen okay now control unit one standard the corresponding instruction stream to the processing unit of one the control unit - can send the corresponding instruction screen to the processing unit to control unit the yen can send the corresponding instruction stream to the processing unit yes okay now this process unit can take the instructions from the corresponding control unit because that data and intricate that instructions with the data taken from the shared memory has it data stream okay now what are the inputs for processing unit one the data taken from the shared memory unit as the data stream is the one input under one the control unit can send the instruction screen one that are read from the memory module one as the instruction stream one this is the second input so now the processing unit can take these two inputs and execute that instructions which the data taken from the shared memory unit now whatever the output we are getting from processing unit one that output can send it to the processing unit 2 as a input now the processing unit 2 can take the instructions from the control unit two that are taken from the corresponding memory module now the processing unit 2 can execute the instructions is to base data data taken from the processing unit 1 now whatever the output we are getting from the processing unit e to that are standard to the next processing unit suppose next processing unit is bu-yong p you can take the data from the processing unit to 2 as 1 and another input is the instruction stream can read from read by the control unit and from the corresponding memory module now whatever the output we are getting from the processing unit again so that output can be standard to the shared memory unit as a data stream okay so now in this way the execution can be done in a - deep computer this computer has only a critical concept computer but there is no practical implementation is there up to now so there is no computer has the mi SP computer architecture okay so this example for a - big computer or ecology and Milan C dot and BP computer this computer has only a - be critical concept but there is no implementation of a - the architecture okay so the abbreviations are see you control unit PU processing unit and then memory module is instruction stream bs data stream now we go for a my MD computer 
LIamOGfFplk,27,"Here, we discuss what computer architecture and organization is all about.",2018-05-01T06:48:21Z,Computer Architecture Vs. Organization  - Session 1,https://i.ytimg.com/vi/LIamOGfFplk/hqdefault.jpg,Shriram Vasudevan,PT10M5S,false,4350,59,0,0,9,friends it's going to be a new set of learnings from a new subject I'm going to handle computer colonization on the architecture for my next 20 sessions hopefully I'm going to take my PS as reference so maybe his architecture with Laurel store as the fundamental stuff will be handled along with lot of demos and exercise as well as what I do for other subjects so this will give you an understanding what computer architecture is all about what are usually solemn oath what lowdown story is all about how to follow and understand a microprocessor all these are going to be handled here so state you allow about 25 sessions of play belief so it may need about two months time frame to complete this course and this stretch so the agenda for the first four sessions are three sessions are you find less right now I will talk about what computer architecture is all about difference between the computer architecture and organization the instruction set architecture is a types instruction set architecture is abbreviated as IEC so is a types will also be done the first three four sessions we will cover all this and then we will go and do that deeper I maybe is stuff which is more interesting and fundamentally that's needed for you to understand anything related to microprocessor let's first start this way when you are asked to write a program for factorial you will just write a code for doing the factorial operation but do you know what happens internally this is more important actually you see the right-hand side of the picture you are first given at the specification that it says that you need to write the code for factorial R you need to write a code for some purpose additional first 10 numbers something in there that is the specification you get then what will you do you will immediately start writing the code in C or C++ or Java or whatever unit and then what happens internally is the equivalent instruction set will be pulled out so for me to perform C equal to a plus B in see programming I need to have the adder function there addition function there so although you do it through instruction set and a comma B comma C will happen so the equivalent instructions should be pulled out first for you to accomplish some task through your code that's that's what we are really seeing here so first city specification you grab the specification and the spec is available to you and you have written the C code or C++ code then you need to pull the equivalent instructions from the available instruction set for the specific processor then the architecture to accomplish that task will be internally finalized for you to go for an addition operation you need to have an adder there and you need to have couple of registers which can probably sell the input to the adder and output has to be storing into a register so this architecture has to be finalized then comes the digital electronics partner how do we build that we need a NAND gate we need on our gate we need a neck circuit what is the cleaver there so I need to really go with understanding how do I get the someone carry out of the adder and that part comes with the electronics and when you go deeper into it et cetera transfer level as you know how do you build that the electronic components I have a gate but how do they build it I need to go with the fundamentals of it and that's very important so we need to understand them and then further level is much more important which is nothing but your physics and chemistry which talks about how do you make the transistor we need silicon do we need some other chemical so it talks about bringing up a transistor now we have seen from the top to bottom approach now if you see from the bottom to top approach you need sand silicon which helps you in making transistors and few other chemicals physics chemistry comes over there then you get the transistor you make a digital component between cell component like a gate and then the gate will help you in realizing the architecture with the logic and then you write the code for it to be appropriately move and finally that meets your specification isn't it beautiful so this is how the computer is all about this is how the computer is working fine how these days so this is very important for us to understand that there are there are lot many stuff that goes really inside deeper to get a simple instruction executed and that's what is going to be computer architecture an organization so to you to understand what is the logic how is the logic chosen we need computer architecture yes that's that's what is all about control architecture and organization and we talk about two terms here for one is architecture and another and these are vacations what is architecture all about what is the organization all about this is very very important and yeah we need to go deeper into it architecture will always be concerned with what to do organization will think about how to do it architecture will tell you what to do whereas organization will tell you how to do so this is pretty important for us to understand so architecture will tell you what to do and organization will tell you how to do as I told you so architecture is always connected to the design and organization is always connected to the building process architecture will design our basis will implement that's how we can easily understand it architecture will carry out all sort of analysis and it will zero and what are all the types of instructions what are all the artists emotes that we can use what are all the registers that are available this kind of high level design would be taken care of by architecture where as organization will help you in implementing all this so computer architecture carries a higher precedence it carries little more respect than computer organization because only after you design you can implement so the design phases all about architecture and the implementation phase is all about organization so one can connect it one can connect the architecture as simple as high level design so a simple example will help you guys in understanding this better I am going to build a calculator and if we are going to give support for a division modulo operation etc is decided and this decision-making is called as architecture whereas when we implement it appropriately that comes as organization so all the features which a computer has all the features which a processor will have will be decided during the architectural analysis and organization is the next phase to the architecture are you can say this way very simply our initiation is five hundred percent dependent on the architectural decisions now I have taken the calculator example something back and I told you that that decision to support a division will be decided in the architectural phase yes we support division operation now how do we support division are we going to keep a direct atom instruction are we are going to have multiple levels of subtraction to implementation will be decided in organization phase based on the requirements and based on the facilities available with you so the decision to go with repeated subtraction or otherwise will be decided with our initiation so architecture will tell you what to do division for example our initiation will tell you how to do the same Lucien can be done easily through a simple label instruction are repeated subtraction that's it this organization and architecture when you talk about there a lot of new topics which could come out in which a lot of new terms which could come out in picture I am going to cover all those shortly but before we go to the next level it's obvious that we need to understand the comparisons better so computer architecture versus organization let's let's just understand five differences quickly high level design in feature description architecture a realization of high level design through implementation organization I design I implement connected to the instruction set support for the data types maracle's memory considerations etcetera more from the implementation aspect is given importance which means subtle components what are all the components of I need to select what all the electronic components that I need to select how do we select the peripherals how do I select or how do we implement the ALU operation all these will be taken care of in organization the logic will be given more in products in computer architecture the implementation of that logic will be given more importance in our initiation what to do the burning question how to do again the burning question here in our initiation site I already given you an example of multiplication I mean the mission area now it is multiplication I am going to support multiplication how am I going to do it 80 85 does not have support for multiplication we need to go through the PD variation that's an organizational feature but they decided to support multiplication or only through a repeater condition simple let us understand this and we can conclude this session in this way we have architecture to add two numbers I have an 8-bit data 8-bit data either will be there result of media this is the architecture they tell that we need to add a conductor now our musician register r1 has got 8-bit content register r2 has got another date with content I am going in depth little more I have an 8-bit data bus I have another 8-bit data bus Ione bit the adder the result comes out some is stored in another register Carrie is stored in another register and there is a control signal to control over all our operations this is organization for you and I hope I have given you the introduction about what organization and architecture is all about I will come back to you with more contents in my next session shortly thank you very much for following the channel if you have any particular queries or if you have any comments please type it in the comment section I will be able to respond you river queries shuttle thank you very much 
x4PpZhnnzXM,27,,2020-10-14T10:22:52Z,Lecture 05 CSE 317 Computer Architecture and Organization,https://i.ytimg.com/vi/x4PpZhnnzXM/hqdefault.jpg,"Engr. Syed Mir Talha Zobaed, M.Sc. Engg.",PT25M46S,false,75,5,0,0,14,architecture architecture and organization 317 computer architecture and organization take course syllabus business [Music] arithmetic logic unit performs arithmetic and logical operations on the data received from the memory or an input device logic unit logic operations perform courtesy arithmetic operations you know biological operations you know register at microprocessor within the computer and logical operations of the data received from the memory or an input device arithmetic logic unit memories input devices logical or operational accumulator bcd are stored in the memory in a sequential order sequential in binary to the output port harmony edge instruction execution of the follow-up sheeting a binary format output port between this process register stores the temporary data classically it performs the computing functions a 8085 by 8085 microprocessor 18 basically art beaten microprocessor jt intel company real estate microprocessor semiconductor company [Music] [Music] it is used in washing machines microwave opens mobile phones or that app needs a button for debugging 0 eight zero eight five by eighty five micro ssr so what are the features of eight zero eight five micro percentage at a database of three arbitrary addressable solution program counter hotel beta stack pointer which is registered 8085 microprocessor requires 5 volt supply to operate at 3.2 megahertz single phase clock so single phase clock clock signal 3.2 megahertz spectacular plus 5 voltage supply it is flip flops arithmetic logic unit temporary resistor accumulator it interrupt controller among serial i o controller above i can actually timing and control address buffer data address buffer you can address bus router because input output operation load correct data load correction but data memory the racket restore operations is a load store operations input output operations but arithmetic operation biological operation eclipse often performed high token data data 8085 increments the program counter whenever an instruction is being executed so that the program counter points to the memory address of the next instruction that is going to be executed terminate program counter value increment is an r8 bit register having five one bit flip-flop terminal arbitrary register economy pasta one bit of flip flop depending upon the result destroyed in the accumulator accumulator and mode the flag registered a pasta flip flop after manager carry sign carry bit sign bit zero bit auxiliary carry bit cadillac a flag register a flag resistor auxiliary carrier decoder keycard instruction register is information relationship ticket decode currently manage that port so destruction register and decoder is an 8 bit register when instruction is faced from memory then it is stored in instruction register instruction decoder decodes the information present in the instruction register airport which say timing and control unit timing and control signal provide characteristic a timing and control unit microprocessor between operation perform corrosional jet projection signal below control signal shitty generator timing and control signal so key key control signal read operation right operation address later in the ball even control signals status signal s0 s1 statistic input output memory airport dms signal direct memory access signature hold among hold a reset signal reset nuclear university of science you 
TPExiABPN14,27,"In this video, we will discuss the basic architecture of the Computer systems, like the number system, information formats, computer architecture, organization, and design, etc.

All the references for this course are taken from the book 
'Computer System Architecture, 3rd Edition' by author 'M. Morris R. Mano'
https://www.pearson.com/us/higher-education/program/Mano-Computer-System-Architecture-3rd-Edition/PGM197215.html.

Feel free to share your feedback in the comments section, this will help us to improve the delivery of content.

Web Links:
~~~~~~~~
ðŸŒ My website / blog:
- https://www.anupinder.com 
- https://www.superwitsacademy.online,
- https://www.cloudsimtutorials.online
- https://www.superwits.com

ðŸ’Œ Sign up to my weekly email newsletter - https://events.superwitsacademy.online/
ðŸ“¸ Instagram - https://www.instagram.com/anupinders/
ðŸ¦ Twitter - https://twitter.com/anupinder
ðŸ“£ Facebook - https://www.facebook.com/SuperwitsAcademy

ðŸ“š Course on Superwits Academy:
~~~~~~~~~~~~~~~~~~~~~~~
5 Days Cloudsim Simulation Toolkit Workshop: https://www.superwitsacademy.online/courses/5-days-workshop-on-cloudsim-simulation-toolkit
Essential Cloudsim Tutorials: https://www.superwitsacademy.online/courses/essential-cloudsim-tutorials
Essential iFogSim Tutorials: https://www.superwitsacademy.online/courses/essential-ifogsim-tutorials

ðŸŽ¥ Check out My Youtube Gear:
~~~~~~~~~~~~~~~~~~~~~~~
ðŸŽ™ï¸ Condensor Mic: https://amzn.to/30qKUKe
ðŸ’¢ Mic setup: https://amzn.to/3i3lMPV
ðŸ–±ï¸ Wireless Mouse: https://amzn.to/31h0gAl

Who am I:
~~~~~~~
I'm Anupinder Singh, Author of www.superwitsacademy.online course as well as Ph.D. Scholar @ Thapar University. I make videos about Cloudsim, Cloud Computing, Machine Learning, General Programming.
Also, Iâ€™d love to hear from you. Tweet @anupinder directly will be the quickest way to reach me, but if have something to ask in detail, feel free to email me at hello@superwits.com. I try my best to reply to things.",2017-08-16T04:00:19Z,Lecture 0-Introduction to Computer Organization and Design,https://i.ytimg.com/vi/TPExiABPN14/hqdefault.jpg,SuperWits Academy,PT15M48S,false,2461,7,3,0,0,I love there welcome to our new series on engineering subject which has being used various computer science students to get in the wonder how a particular digital system is designed and then structure together get a certain kind of processing of data so in this particular video we are going to actually talk about very brief um score line four things one is that how this will information representations and excuses is done then basic structure are four simple missile computer then we are going to look around Network is an architecture organization and design of the things they should have greater and then at last we will be comparing two very common architectures that is one human and Howard and they have a specific way of sanity to do now let's start with the basic of previously completed and this your computer system uses three binary number system which has only two digits now when I say two today digits this is a zero and one there are only two and when they are simply individually they are actually named as Rick so this is technically unit for the representation of a particular information within the system right so in the channel it can be either 0 I will give another one so you actually use a group of bits to represent certain information and mind it the count of bits used to represent the data is defined in terms of power of two which is raised about some n where n is either equals to 0 or greater than 0 right explains never a negative value now the you clock effects can be causing this great symbol such as decimal letters digits letters of alphabet and so on it depends upon that what sort of information you want to the picked for a particular application now let's take an example let's say we have a group of X in this list that I usually that particular value is this if you see it's a 8-bit number right so each number is particular it has an option to put zero in one so here if you see we have 0 1 0 0 0 0 1 1 right if you talk about the unsigned decimal number it is representing Cystic 67 now do we know it is accessible so we actually need to do some calculation here so I am just writing down this number so that we can do certain calculation and give you some stuff at how a digital system you can actually detect a particular value right so now as we know we are we have a binary number here and we wanted to convert it we want to convert it into a decimal number right notice I'm fine so that means we are not considering this most significant bit as a sign the next assignment we are considering it as a part of data so what we can really do is if we go over the general equation form so this is the start from 2 - 4 0 that is 0 index value 1 2 raised to power one 2 raised to power to waste by 3 two ways to power 4 2 raised to power 5 - raised to power 6 and 2 raised to power 7 and finally when we wanted to get a decimal number what exactly we do is we say that we go mad at this value but multiplied by 2 raised to power 7 plus 1 multiplied by 2 raised to power 6 plus 0 into 2 raised to power 5 plus 2 sorry 0 multiplied by 2 raised to power 4 plus 0 into 2 raised to power K plus 0 into 2 raised to power 2 plus 1 and 2 2 raised to power 1 and plus 1 into 2 raised to power 0 so if we go and check the spelling becomes bottom this will be 0 plus 64 plus 0 plus 0 plus plus zero plus two plus one which is equivalent to 67 so this is how we actually find out the UH thing that's in the number fungus and quite a big number now the other hand may be able to say that the format we use to see BCD BCD says that we have we have a combination of project numbers right so it starts from some zero zero zero then it goes to 0 0 1 so we keep on adding 1 the binary 1 2 tsps value so we get 0 1 0 so that we have a 0 we go 1 1 then we have 0 1 0 0 then we have 0 1 0 1 then we have 0 1 0 and so on right so 67 0 this represents 1 2 3 4 5 right and then we have 6 so if you try to compare as we are saying that we see is actually working for big so what we have to do is we have to actually divide this apec number that is 0 1 0 0 0 0 1 1 into 2 cares of 4 bits right so this is the first thing in this is the sequence there and we start from the right hand side so when you try to match this value here it is actually depicting three could be sitting here as if you talked about this 1 0 1 0 1 0 1 0 0 this is actually 4 so finally what we get is 42 43 here's a number right so this is how actually represent whereas if you see here the legaspi's it wasn't written off when I'm using SI codes and that's that particular scenario the 67 is actual ism in capital C later if you want to confirm it again going security SI table right so this is therefore purely dependent in the condition of mix is purely dependent on the operation which is in processing and what sort of data we are actually looking at right so that is what what exactly depend upon so for more information you can actually go and refer to in book by the author Morris manners and the book name is to direct architecture career system architecture that is the name of the book that he can refer to so let's let's get started with the another part that is this little video so whenever you talk about it is little bit is actually consists of two different things when it'll be hardware and another one is this software so when we talk about the hardware advil is actually comprising above electronic components and the next mechanical devices both set of companies are proposed to purposefully connected to use set up kind of an operation results whereas the software which is actually a driving in degree for that go to graegus is a collection of various set of instructions and data that we compute Hardware manipulates to perform various types of data processing tasks this is very important to understand so when we when we are talking about the digital computer there this is the basic model which we get every it is a computer will at least consist of a central processing unit which is further containing two different units one is automatically logical unit and one second one is you control unit apart from that it is connected without memory some kind of primary memory which is normally known by name of random access memory then it is also connected with the input devices and the output devices so input devices can be your keyboard right it can be some kind of Mouse OCR's and so on similarly output devices can be Monica can be a vector and so on right so and put out and output processor is actually responsible for taking input from the device input device and then not sending x4 to these and possibly for further processing or with that and one one so X n let us move towards the output device for display purpose this is what exactly we have and again I am reminding you that this particular CPU centric assessing unit is actually consistent Rock two things one is the ALU and second one is the control unit control unit the basic job of the control unit is they can go signals to all the different devices and connected with the same person range right that is what we have an at my technological unit is basically responsible for doing all the automatic operation as well as the logical operation research approach we've done on the binary data so I am saying it's working on only binary data right the information representation that we talked about here is only for the purpose of the user readability right whereas the C computer clearly works on the binary data right either we talk about the control signals are automatic and logical operation moving on to the next part of the computer you will generally heard of three terms one is the computer architecture one is a computer organization and community sign they are correlated to each other so let's start cooking with our picture it actually concerns with the structure and behavior of the computer acting by the user and includes information that means data formats that use the format in which the data is supposed to be represented as we have already discussed about the unsigned decimal number BCD numbers or letters and so on instruction set which are actually supposed to know run as a program on your hardware and the techniques for addressing memory because we are you talking about a data and here the information which is supposed to be stored somewhere so for that some kind of addressing mechanism is also required so this is one of the most important thing that is the butyl architecture to find the structure and behaviors and when you talk about the structure it actually concerned with your specification of various functional modules like processor and memory and structuring them together into the computer system and this defines that what sort of computer system we are going to use and this this is the nine where you define that is it as a server computer is it a general-purpose computer or is it a specific system which is there for a specific application right so this is how you define now when you talk about the computer organization it's more about these facts and so it is concerned with the very hardware components are together to form a digital computer system and whereas a Cabrio design is something which is generally used by the computer computer designers and it is basically concerned with the development hardware for the addition computer taking into consideration a given set of specification now this is very important set of specifications corresponding to that we actually design a computer system right so that is what what I say menu structure then we actually defined that our very subsistence or the some kind of DSPs or the other some kind of general purpose computers or laptop and so on so these set of specification actually define that what what sort of device you are going to develop or manufacture and then what sort of application can run on it right and the instruction set which actually gets out is all after in a setting of this particular path after this is what you need to remember and these three terms are very important if you know all these three terms you will be able to differentiate on on the various kind of specifications not a computer system comes in as we know that there are there are certain specifications of what sort of specifications are there and on the basis of this protection you actually going to find the architecture of the system that's the important part so here we have two different architects with the subtlest Equality von Neumann architecture which is very popular this is the stored program kind of for specification system here as well as another one is the Harvard architecture so if you look very carefully the volume M is the same what we discussed in our this particular slide right so the only things which were highlighted here is that the random access memory contains both data as well as code right so it's a shared memory a very important thing shared memory model is used here whereas if we talk about the hardware architecture we have actually two different memories here one for the program and another for the data and mind at these two memories in half different size so let's say this is 64 K this can be a 32k memory right so this is how you can actually you know differentiate this is one of the most important and major differentiation between both these architectures another is the shared channel is used by the central processing unit for raisin right whereas within Harvard architecture individual lines are used for both the memories and the system can read and write at the same point of so they're both the memories have read and write at the different channels which are connected to the central versus community right so this is what what exactly is your difference on the basis of block diagram which you which you can actually you know visually different differentiate now let's go further and talk about the other differences so as I mentioned that in one human architecture program and data are stored in the same memory there is in Harvard it is the program and data are stored in separate memory also both memories can be of different sizes important one in von Neumann architecture shared channel to access memory for program and data whereas in our architecture is a dedicated channel basis memory for program and data individually for von Neumann architecture the code is executed serially and takes more clock pulses obviously we are using a shared channel there is an avid architecture the code can be executed parallely and Nexus clock pulses can be used for us and one demon architecture control unit is very simple because there is only one single control passages so it's easy to manage and easy to fabricate to whereas in hardwood covered architecture to separate the control buses are required because we are using two different memories and apart from that design version it's quite complicated and as well as it's expensive and you have replicated a particular system in one human architecture if you see the conventional processors are count their PC and servers are increasing examples for this whereas for hardwood the use of DSP right this is signal processing microcontrollers are their processors for embedded systems like mobile phones and PDAs and so on so this is what the application part is so you can actually resemble that what what exactly hardware Howard architecture is used for so whereas one human is for the other process so wherever you find the general-purpose computing the you will find one human architecture whereas for specific kind of application you will find the Harvard architecture so just summarizing this particular video there is you Computers Computers works with binary information which can be used to represent variety of data this is a bit of an sister for various electrical as well as an extra components which are structured in the specific wave for an application there are two digital computer architecture available one women and our architecture each of them have their own pros and cons one human is applicable on general-purpose computing system like PC laptops etc how it is our architecture is applicable on computing system which are built for some specific application like digital processing system microcontrollers cetera so I hope this particular video gives give you certain basic understanding about the computer organization and design so thank you very much for watching keeping for the other videos 
buiqGVSuyy4,27,COMPUTER SYSTEM ORGANIZATION-R. D. Sivakumar,2014-02-08T06:27:16Z,COMPUTER SYSTEM ORGANIZATION-R. D. Sivakumar,https://i.ytimg.com/vi/buiqGVSuyy4/hqdefault.jpg,Sivakumar. Dhanasekaran,PT5M7S,false,1711,21,1,0,2,a pleasant welcome to all now we are going to study about computer system organization video lecture and II content created by our deceiver Kumar MSE MPhil M Tech assistant professor of computer science and assistant professor and head department of M comme ca I another Johnny came on college Sivakasi mobile zero double nine double four zero four double three four three email Siva Emma si si T at gmail.com website www.ardexamericas.com puter system organization definition a modern general-purpose computer system consist of one or more CPU devices controllers common bus and memory bootstrap program includes roam interrupts bootstrap program room for a computer's to start running its needs and in the initial program to run called bootstrap program it is told in read one in only memory or electrically erasable programmable read-only memory known by firmware the bootstrap program load the operating system and to start executing that system storage structure storage structure consisting of registers main memory and magnetic disk primary storage secondary storage primary storage main memory so volatile storage device programs and data cannot be reside in main memory permanently computer programs must be in main memory to be executed instruction execution cycle instruction execution cycle first fetches an instruction from memory and stores that instruction in the instruction register the instruction is then decoded and store some internal register after the instruction is executed the result may be stored back in to memory main memory main memory consist of array of words the load instructions moves a word from main memory to registers the store instructions moves the content of registers to main memory secondary storage the most common secondary devices is magnetic disk magnetic disk hold large quantities of data permanently example floppy disk CD pendrive hard disk interrupt the occurrence of an event signaled by an interrupt from either the hardware or the software Hardware may trigger an interrupt by sending a signal to the CPU by system bus software may trigger an interrupt by executing a special operations called system call interrupt service routine when the CPUs is interrupted it transfers executions to a fixed location this location contains the starting address where the service routine for the interim is located then the interrupt service routine executes input output structure data transfer between the CPU and input/output devices handle the data transfer in three ways the first way is programmed input on output and the second ways interrupts initiated input and output on the third ways direct memory access programmed input and output in program to input and output the CPU sends the head command to input output devices and wait in the program loop until it receives response from input and output devices interrupt initiated input and output in interrupt initiated input and output the CPU sends the head command to input and output devices and then go all true drew some useful work when the input and output device is ready the input and output device sends an interrupt signal to the CPU then the CPU read the word from input and output device direct memory access the DMA the DMA controller takes over the buses to manage the transfer directly between the input and output devices and memory thank you you 
7NEJR38nvhw,27,"RGIT Nandyal - NPTEL Videos (CSE Department)

Website : http://rgitnandyal.com/",2015-06-22T09:20:39Z,Introduction & High Performance Computer Architecture Course outline,https://i.ytimg.com/vi/7NEJR38nvhw/hqdefault.jpg,RGMCET Nandyal,PT55M9S,false,228,2,0,0,0,"you dear viewers I welcome you to the lecture series on high-performance computer architecture in 40 lectures I shall try to provide an overview of computer architecture at different levels and various aspects of advanced computer architecture and today we shall start with the first lecture and the title of today today's lecture is introduction and course outline so in this lecture I shall try to give an introduction of the course and give an outline of the different topics that I shall cover in this course and here is the outline of today's lecture first I shall give a historical background because today we find a PI an array of computers powerful array of computers surrounding us performing different purposes and providing the need of of our daily life and this does this did not happen in a single day and it has taken many years maybe hundred years to reach this stage and how gradually we have arrived at this stage I shall give a kind of evolution that has taken place to reach this particular stage and in this respect I shall discuss about five generations of computers and then I shall talk about the elements of modern computers here what are the different components that affair that builds a modern computer then I shall introduce to you the instruction set architecture which is essentially a programmers view of the processor and then instruction set processor which essentially represents the way the processor is realized then I shall talk about some related topics like Moore's law which has helped in in the progress of the computers and the most law is the is the force behind this gradual evolution of computers and then I shall discuss about parallelism at different levels as we shall see to reach higher performance parallel limbs the parallelism is the key idea and how parallelism have been incorporated at different levels that I shall discuss and finally at the end I shall give an objective of the course and an outline of the course if you look at the history of computers as I mentioned the history encompasses more than 100 years so computing equipments are available maybe for hundreds of years and the the period can be divided into two categories first one is the mechanical error that means prior 1945 and then after 1945 we can say that electronic era so in the mechanical error the computers were built by using mechanical components like I mean wheels and other things for example abacus which was invent developed in China that dates back to 500 BC that was used for the purpose of I mean calculation of numbers then mechanical adder subtractor machine that was built by blaise pascal Blaise Pascal in France in 1942 this again belongs to this mechanical era because no electronic component was used in building it similarly differencing in that was built by Chas Babbage for polynomial evolution evaluation that was developed in England in 1927 then binary mechanical computer was developed by I Connor Jews in Germany in 1941 then electromechanical decimal computer was developed by Howard I can in 1944 in Harvard and then mark one by IBM okay so this is the mechanical era coming to the electronic era we can again divided in 5 generation the first generation was before the invention of transistor obviously before the invention of transistors the electronic building block or component that was used is vacuum tubes and and that also use relay memories so what is a vacuum tube vacuum tube was is a kind of tube small tube where with a filament which emits electron and by controlling the flow of electron the currently the big by controlling the flow of current you can actually realize the two states on and off and that is how you can realize digital computers so vacuum tubes were used and relay elements which are essentially switches which are used to store information for we used as memory and these computers obviously when used realized his vacuum tubes they were very large in size so they were usually these computers were actually occupying big rooms and obviously dissipating lots of power and computing power although the computing power was relatively much smaller in comparison to today's standard so maybe thousand times slower and lesser computing power then present a pc that we use nowadays then these computers were essentially single-user systems so single is assistants mean that operating system and that was used can provide I mean can allow only one user to use the computer and so far as the programming language is concerned it was in a very I mean a primitive stage that means you can use only machine or assembly language so these computers were single is er systems and the programming language that was available was machine or assembly language obviously whenever you write a program in machine n as a machine machine language or assembly language it is very painful machine or assembly language and to write a program I mean programming was a big big challenge so these computers were not very user-friendly you can say and examples of this type of computers are ENIAC Princeton is an IBM 701 so these computers actually the first generation of computers were primarily developed because of the requirement of Department of Defense of USA so they wanted to calculate the trajectory of a cell that is launched from the worships and where it will fall like that so to do that calculation you need a computer so these computers were primarily developed for that purpose so in 40s then came the second generation computer in between nineteen 55 and 64 so in this decade computers were realized by using transistors so transistors were invented invented and which was used I mean for the realization of computers so transistors diodes and so far as the memory is concerned magnetic ferrite cores were used to store information then high-level languages were used with compilers that means now you can write program in high-level language and so compilers were developed so that was a very big step so far as the users are concerned and also it allowed batch processing that means earlier the computers were developed for single user now you can do batch processing that means a particular computer can be used one after the other by different users so the computers like IBM 7090 cdc 1604 UNIVAC larc these are the examples of these second generation computers built using transistors and diodes then came the invention of integrated circuits so by integrated circuits we mean you can put more than one electronic component like transistors diode and so on on a single silicon wafer so depending on the number of active devices that you can put on a single silicon wafers you can categorize into different type known as a small scale integration SSI small scale integration where you can put maybe one to ten active devices then msi where you can put ten to hundreds of devices like transistors and diodes then LSI large scale integration wherein you can put thousands of devices and nowadays we are in the era of VLSI where you can put millions of transistors or active devices maybe same ad million two billion mini and two billions of transistors we can put on a single IC so these are the because of the this evolution of integrated circuit has led to development of powerful 3rd generation computers using s but in this third generation only SSI and msi devices are used and so far as the operating systems are concerned multiprogramming and x hearing OS were used and so a single computer can be used by a large number of users and all of them will be having the feeling that they are the sole users of the computed so in this category was IBM 360 370 computers so these IBM 367 370 mainframe computers became very popular and widely used throughout the world and in addition to that you have got CDC 6600 Texas Instruments ASC and PDP 8 these are different 3rd generation computers which were developed by different manufacturers and widely used then came the fourth generation as I said with the advancement of VLSI technology this for generation computers use VLSI circuits and vlsi circuits and as a consequence they were very powerful and so far as the operating system is concerned multiprocessor operating system were used that means these four generation computers it was possible to realize multi processor chips a single IC can contain multiple processors so multiple operating systems were developed various high-level languages were flourished and parallel processing was was was very popular using these 4th generation computers that means it was possible to have parallel processing I shall discuss about what are the different types of parallel processing possible in the course of this lecture and the various computers based on this for generation where IBM 3090 backs 9000 clay XMP and then came the fifth generation which are which has started maybe from 1991 to present day so these uses ulsi circuits ultra large scale integrated circuits and or very high I mean si si so these are the different types of very high scale integrated circuits were used in realizing fifth generation computers and to realize massively parallel processing and earlier so far as the multiprocessor parallel processing were concerned all the processors were homogeneous that means same type of processors were used in those in those parallel processors but in the fifth generation it was possible to have heterogeneous processors that means one can perform def I mean graphics processing one can perform integer processing and so on so different types of heterogeneous processors were combined to realize a thing for computer systems and there are the fifth generation compute examples of this fifth generation computers are Intel Paragon fujitsu PPP 500km PP so these are some representative examples but obviously this list is not complete there are many more p generation computers coming to the elements of modern computers you can see a modern computer is an integrated system consisting of machine hardware system software and application program so three very important components that makes a computer workable usable to usable are number one is hardware you require hardware second is your system software and third is your application programs various application problems and this can be represented with the help of this nested circles so as you can see hardware is at the core or center of these three circles so hard ha this hardware is being is the interface between the application software and hardware is the system software system software is essentially the operating system and different types of operating systems have evolved over the years and these system software operating system allows applications software to run on this hardware so and the actually to the system software or the user or programmer the functionality of the processor is characterized by instruction set so a a processor can execute a set of instructions and which can be used to write a program so program can be considered as a sequence of instructions so you can pick up instructions from the instruction set and realize a program so a programmers view of the processor is essentially the instruction set and that is why it is called instruction set architecture so instruction set architecture or is a in sort plays a very important role and it is a kind of abstraction and all you all programmers can look at it so this predefined instruction set is called instruction set architecture now this is a on instruction set architecture serves as an interface between the hardware and software as i have told so here you have got the hardware and here you have got the different types of software like system software application software and this instruction set architecture serves as an interface between the hardware and software so in terms of processor design methodology and is a can be considered as its specification of of a design that means so whenever you go for designing a system you have to provide specification so the specification this is a instruction set architecture can be considered as the specification and this instruction set architecture is I mean to realize the instruction set architecture you implement a or synthesize a hardware so the specification is the behavioral description what does it do what the processor can do then the synthesis step attempts to find an implementation based on the specification and then comes the processor which is an implementation of the design and how's it constructed that means instructions person such as processor design concerns with how a processor is constructed so it is also referred to as micro architecture so realization of an implementation a specific physical embodiment of a design is done using VLSI technology and nowadays so I in this course we shall discuss about the way this is how it is being done but before that as we shall see later we shall introduce the instruction set architecture I believe before I proceed further it is essential to distinguish between these two terms which you will encounter quite often may one is architecture another is organization so what is the difference between the two so a computer architecture and computer organization here as you can see architecture is a short form of instruction set architecture so it is also known as instruction set architecture as I mentioned this is a programmers view of a processor so that means 888 specifies what are the instructions of our different instructions that you can perform that means the data transfer data manipulation like that so addition subtraction multiplication and division and then then they move instructions to transfer data between processor and memory processor and i/o load store like that and then it will also provide a given idea about the different types of resistors which you can use for storing information temporarily file executing a program so registers provide intermediate into I mean storage and then the you have you can have various addressing modes this amounts allows you to access operands in various ways from resistors and memory and also from i/o so this is a architecture is essentially a programmers view of the processor on the other hand as i mentioned organization represents the high level design that means the way the processor is implemented how many cache memories it has got how many arithmetic and logic unit has got what type of pipelining control pipelining is being used what type of control design is being done whether it is a hardware control unit or whether it is a mic micro program control unit whether the processor is a single cycle multi cycle pipeline like that so these are all decided as part of the organization's so this is also called micro architecture so this is sometimes known as micro architecture so the structure of a computer that a machine language programmer must understand that is your architecture to be able to write a correct program for that machine and a family of computers of the same architecture should be able to run the same program so here as you can see same instruction set architecture can be realized by a series of series of machines or processors they may be realized by using you in different ways one can use transition can be realized by using transistor circuits another can be by integrating VLSI circuits and and they can be in the implementation can be done in different ways but as long as they execute the same instruction set architecture then we can say that there is a kind of binary compatibility that means a program written for one machine can be run on another machine so this is a very important concept and this binary compatibility plays a very important role whenever we go for designing computers and we go from one generation to next generation of processors so you have to incur I mean you have to take into consideration the binary compatibility whenever you realize the next generation processor so that the programs developed for the earlier generation can be used can we cannot be I mean not be thrown away it not be wasted and can be used so now coming to one very important concept that is your mousse law Gordon mode he was one of the founders of Intel he proposed a law based on some his observation so the computer whatever do what has been done what has been found that the computer performance has been increasing phenomenally over the last five decades and this enhancement this performance improvement is an outcome of you can say moose law so this was brought about by Moore's law what is that most Moore's law states that transistor per square inch roughly double doubles every 18 months so it is not much lies not exactly a law but this particular rule you can say transistors per square inch roughly doubles every 18 months that has been hold good for nearly 15 years and this is the this is gordon moore each one of the founders of Intel and this is what he stated back in nineteen sixty-five in his famous paper he wrote it for electronic magazine electronics magazine so he was asked to write a article predicting the future of electronic circuits so the title of the article was cramming more components on to integrated circuits and that was published in april nineteen and nineteen nineteen ninety-five issue of electronics magazine and in that article he predicted that transistor density of minimum cost semiconductor chips would double roughly if every 18 months and obviously transistor density is correlated to processor speed as we shall see so this this shows the this shows how moore's law has remained valid for for a large number of time for about 50 years you can see here only the Intel processors are being shown and on this side on the y-axis you have got the number of transistors and on x axis we have got the number of the years so as you can see as the as we go from 1972 2010 the number of transistors is reaching from 1,000 to several millions several millions of transistors are used to realize processors so starting from simple intel 4004 which was obviously having few thousands of prosecco for I mean transistors to present-day dwell code and multi-core processors requiring multi billion transistors maybe tens of billions of transistors so this shows the the how mood law Moore's law has really influenced the growth of computers and Moore's law is not about just the density of transistors on a chip that can be achieved but about the density of transistors at which cost per transistor is the lowest that means it not only says about how many transistors you can fabricate but it also says how economically you can do the fabrication that means how that means you have to realize I mean transistor ICS with more number of transistors in a economic way cost-effective way so as more transistors are made on a chief the cost to make each transistor reduces but the chance that chief will not work due to defector arises of course this this problem is there and that is that you have to take care of it by reliable processing take and i take a technology and more observed in 1965 there is a transistor density or complexity at which a minimum cost is achieved so based on I mean a minimum where the transistor density or complexity at which a minimum cost is achieved he proposed his law which has become famous so we can say that the initial computer performance improvements came from the use of innovative manufacturing techniques advancement of VLSI technology and which actually based on moore's law you can say and improvements due to innovations in manufacturing technologies have slowed down since 1980s of course the growth the rate at which it was growing has slowed down since 1980s so smaller feature size gives rise to increase resistance these are the two reasons number one is smaller feature size gives rise to increase resistance this is one problem second is larger power dissipation so as we shall see the pentium processor generates about hundreds watts of power dissipation so 100 watts of power dissipation from an IC is a very large power dissipation so this this power has to be dissipated with the help of suitable packaging and cooling technique and so as you put more and more transistors the power dissipation increases the cost of packaging and cooling increases this is one parameter which is which limits the increase in number of transistors on a chip so a decode and as I mention a decade ago ago chips were built using 500 nanometer technology so 0 point 5 micron technology you can say and in 1971 10 micron process was used and more most processors are used are currently fabricated on 65 nanometer or smaller process so nowadays because of the advancement of VLSI technology thanks to Moore's law which has been followed the it has progressively I mean the dimension has reduced gradually from 500 nanometer tu su tu submicron technology now as you can see we nowadays we use 65 nanometer or smaller process maybe 45 or 33 nanometer so Intel in January 2007 demonstrated a working 45 nanometer chip and Intel started mass-producing Inlet 2007 based on this 45 nanometer chip so you can you it is very easy to pronounce this 45 nanometer 33 nanometer and like that but you think about the diameter of an atom so diameter of an atom is of the order of point 1 nanometer so we can see that we are very close to the diameter of an atom so the precision at which the VLSI technology VLSI fabrication is taking place nowadays is not far away from the the diameter of an atom ok so this particular table gives you the imaging decades of microprocessor evolution so as you can see from 1972 1980 the transistor count was two k2000 k and clock frequency was point 2 1 2 3 megahertz and number of instructions per cycle was point 1 that means you require 10 cycles to execute an instruction and in nineteen eighty to ninety 90 the number of transistors increase from thousand k 2 1,000,000 and the speed of increase from three to thirty megahertz and number of instructions per cycle also reduced so you can execute more number of instructions in a sync point 1 that means you can more or less execute one instruction per cycle and then in 1992 2000 the number of transistors increased from 1 million to 100 million and the speed increase from 30 mega has to 1 gigahertz and number of instructions per cycle changed em reached Oh point one point nine to one point nine so you may be asking how is it possible to execute more than one instruction per cycle later on we shall discuss about that that can be done by using superscalar architecture and later on we shall discuss in detail about it then came the two thousand to two thousand the present day technology technology where you can have 100 million to two billion transistors on a single chip and speed is from 1 to 15 gigahertz so 1 gigahertz means 10 to the power 9 Hodge so you can see the speed at which twill work and then the number of instructions for cycling is again between 1.9 to 2.9 and it can be still more so the processor performance has become twice as fast after every two years and memory capacity has increased twice as much after every 18 months roughly based on following Moore's law and I should mention about another name called a mid and Conway actually they described a method of increasing hardware design designs by writing software so whenever the number of transistors in a processors increases it is not possible to do the design manually so you will require autumn automated technique so Gordon moved developed this are developed this hardware description language by which you can you can automate the design of a processor that means whenever you are going for designing processors with millions of transistors you have to use CAD tools computer aided design stools and mid and Conway was the founder of that I mean they proposed the this method for creating hardware designs by writing software so by writing software you can design hardware that was a important step question arises how can you improve performance so initially as we have seen the improvement occurred because of the advancement of VLSI technology but in subsequent years other thing other the performance input improvement came from exploitation of some form of parallelism so some form of parallelism were used in in these in these processors first is instruction level parallelism what do you mean by instruction level parallelism the structure level parallelism means you know normally the execution of instruction take place serially serially that means you execute if H and instruction then execute so you can say that instruction cycle can be divided broadly into two parts instruction fetch plus instruction execution so first you perform page one instruction then you execute it then you fetch another instruction is this is for instruction one then you fetch another instruction and then execute another instruction that second instruction so in this way it goes on serially there is no parallelism but subsequently techniques were developed for instruction level parallelism and pipelining is the first technique which Explorer which used instruction level parallelism in pipelining you will see the instructions can later on we shall discuss in detail we shall see the instructions can be executed in overlap manner that means they say that instruction fetch when you are execute performing instruction fetch of instruction one instruction execution of instruction one you can perform instruction fetch or instruction to and then again instruction execution of instrumenal instruction execution of one instruction 2 is going on then you can perform instruction fetch of instruction 3 and in this way it goes on so later on I shall discuss it in more details and also there were other techniques which are used which is known as dynamic instruction scheduling in which a multiple instructions can be scheduled dynamically with the help of hardware and particularly whenever wherever you have got multiple execution units and then out of order execution can be performed and then superscalar processor where you have got multiple processing elements within a single processor then you can have vla VLIW architecture which which which can use superscalar architecture so the compiler we will pack several instructions in a I mean several instruction in a single instruction and which can be fetched and executed serially so these are the this this instruction level parallelism I shall discuss in detail in my lecture then there is another level of parallelism second level of parallelism that is your thread level parallelism so thread level parallelism you can say it is medium grain and different threads of a process are executed in parallel on a single processor or multiple processors as you know you are familiar with what is process process is nothing but a program in execution then a single process can be can have multiple threads you can say multiple threads which can be executed in parallel if you have got multiple processing unit for example in a superscalar architecture you have got multiple processing elements so different threads can be executed in different processing elements so for example you are performing a loop program in a loop program different iterations can be considered as different threads and which can be executed on different processing elements in a single processor so a loop can have multiple threads so this thread level parallelism is another medium grand compared to instruction level parallelism which is fine grained so you can say you can categorize in three types first one is instruction level parallelism instruction level parallelism or in short I LP this is your fine grained fine grained the second is your thread level parallelism and which is medium grain and third type is as we shall see later process level process level parallelism we and which is a choir screen you can see so these three tire levels of types of parallelism can be used and this simultaneous multi-threading is a technique for improving the overall efficiency of superscalar CPUs using Hardware multi-threading that means in a single processors where you have got multiple functional units of a that is present in a superscalar processor you can have this hardware multi-threading that means this thread level parallelism is exploited with the help of hardware and which is known as smt or simultaneous multi-threading and then of course you can have software level multi-threading on multiple processors or choir scan multi-threading software multithreading that can be done on multiple processors or course so this is your symmetric multiprocessor smps this is very popular here shared memory multiprocessor architecture so here as you can see you have got multiple processors multiple processors each processor is having a private cash each of this processor is having a private cash and all of them are connected through a bus to main memory and i/o and this is so this this memory main memory and I you are shared and sharing is done through a common bus and it is called symmetric multiprocessing the reason for that is each of these processors will take the same time to access main memory as well as I oh so it is so the access time for main memory and i/o devices is symmetric for all the processor or uniform so it is also called uniform processor architecture and process level as i mentioned process level or choir scan architecture where you have got different processors that can be executed in parallel on multiple processors and you can use symmetric multiprocessors as I have already shown also you can have distributed memory multiprocessors as usual see in this diagram so this particular model which I have already shown here each processor is has got a private cash and there is a shared bus through which main memory or shared memories accept and accessed so this is called uniform memory access because each of them can access it in a uniform manner and it is also called symmetric multiprocessors as i have told then you have got non-uniform memory access here as you can see the the memory is distributed main memory is distributed and when when this processor is accessing this memory obviously access time will be smaller when this processor processor to is trying to access the main memory attached to processor 1 obviously it has to do it through this network or called inter Connection network so whenever it is accessing through this interconnection network the access time will be longer and also this axis time can be variable depending on the then on the type of network being used and availability of the network so this is your distributed memory multiprocessor where you have got multiple processors and the different memories attached to different processor are accessed through a interconnection network or it can be a local area network now what is the objective of this course so I have discussed broadly given an idea about the different types of processors that you can have and also we have discussed about the different types of parallelism that is possible instruction level parallelism thread level parallelism process level parallelism and different types of processes architecture now what is the objective of this at this advanced computer architecture course we shall see that modern processors such as intel pentium md athlon etcetera use many architectural and organizational innovations that has not been covered in the first level course so each and every student who are attending this course must have attended a first level course on computer architecture and organization so in that first level course these advanced topics were not in covered so which are and and it is very essential for them for the computer science students to learn these details of these processors so particularly the various innovations that have been used in implementing these processors then innovations in memory bass and storage designs as well so as we shall see we shall discuss about hierarchical memory organizations the way the performance gap between memory and processor can be breached then we shall also see multiprocessors how multiprocessors can be realized and clusters can be implemented and and in this way you can say the objective of this course is to study the architectural and organization innovations used in modern computers so in a single sentence we can state the objective of this course study the architectural and organizational innovations used in modern computers okay now let me give you an outline of the course that I shall discuss and the course has been divided into several modules five modules so in module one I shall present the review of the basic organization and architectural techniques so fundamentals the fundamentals of different I mean processors like RISC processor what is RISC processor what is sis processor and what are the characteristics of RISC processors and what are the differences between risk and sis processor and then the classification of instruction set architecture these we shall discuss in this particular module and also we shall discuss about the the way the performance of processors are measured so it is very whenever we say high-performance question naturally arises how do you really measure the performance so we shall discuss about the technique by which the performance can be specified and performance can be measured and we shall review these performance measurement techniques and then I shall discuss about the basic parallel processing techniques as I have already told instruction level thread level and process level and I shall also discuss classification of parallel alkyd architectures in this module one the various parallel processor architectures that is possible then coming to module two I shall focus on instruction level parallelism and as I mentioned the first approach which exploits instruction level parallelism is pipelining so basic concept of pipelining will be introduced and based on that basic concept we shall discuss about the the arithmetic pipelines how different arithmetic operations like floating point addition then into your multiplication can be 44 pipeline that I shall discuss but most important is the instruction pipelines which are used in all modern processors so I shall discuss in more details about instruction pipe lining and particularly whenever you go for instruction pipelining I will find that there are different types of hazards that is present in a pipelined processors that means whenever you do instruction pipelining you want to utilize each and each and every cycle processor cycle but unfortunately because of various types of dependences like data dependence control dependence and structural dependence you will find that it is not possible to avoid these hazards and I shall discuss about the three different types of hazard that is your structural hazard data hazard and control hazard and also we shall discuss various hazard resolution techniques that can be used then as I mentioned I shall discuss about dynamic instruction scheduling that can be that is important in the context of your super scalar architecture and also branch prediction technique which is related to control hazards how we can predict branch and we can minimize the effect of control hazards then we shall discuss about instruction level parallelism using software approaches and as I discuss about super killer techniques speculative executions and highlight how these various techniques have been implemented in modern processors so I shall review some of the modern processors coming to module three I shall discuss about memory hierarchies as I may as I mentioned the speed of processor is increasing and later on we shall see the speed of memory is not increasing at the same rate so how do you bridge the gap so to bridge the performance gap one important approach that is being used is known as hierarchical memory organization where memory or is organized in a hierarchical manner in terms of speed so process and the memory which is very close to the processor is known as cache memory then you have got main memory then the third third type of memory is secondary memory so I shall discuss about these different types of memories like main memories cache memory design and implementation virtual memory design and implementation then secondary memory technology and also I shall discuss about red which is used in the context of secondary memory redundant array of independent disks that I shall discuss and how it is used to improve reliability as well as performance turning to module 4 I shall discuss about thread level parallelism and in this context we shall discuss centralized versus distributed shared memory architecture then various interconnection topologies multiple server architectures and then symmetric multiprocessors and in this context there will be a problem known as cache coherence problem because whenever you have got say private cash and shared memory that will lead to cache coherence problem leading to some inconsistency in the information that is stored in memory and how does overcome we shall discuss and then multi-core architecture SN is essentially an extension of multiprocessors in which the deaf different processors are implemented on a single chip and I shall discuss about modern multiplier give a review of modern multiprocessors coming to the fifth module where process level parallelism will be considered I shall discuss about distributed memory computers different type different alternatives possibilities and three different types of Computing's which are increasingly becoming popular one is known as cluster computing grid computing and cloud computing that i shall cover in this process level parallelism so with this we have come to the end of this lecture and in the next lecture we shall start with instruction set architecture thank you "
44B6YmSuk0I,27,"Digital Design and Computer Architecture, ETH ZÃ¼rich, Spring 2020 (https://safari.ethz.ch/digitaltechnik/spring2020/doku.php?id=start)

Lecture 2a: Course Goals & Logistics
Lecturer: Professor Onur Mutlu (https://people.inf.ethz.ch/omutlu/)
Date: February 21, 2020

Slides (pptx): https://safari.ethz.ch/digitaltechnik/spring2020/lib/exe/fetch.php?media=onur-digitaldesign-2020-lecture2a-course-goals-logistics-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/digitaltechnik/spring2020/lib/exe/fetch.php?media=onur-digitaldesign-2020-lecture2a-course-goals-logistics-afterlecture.pdf",2020-02-26T09:10:36Z,"Digital Design & Computer Arch. - Lecture 2a: Course Goals & Logistics (ETH ZÃ¼rich, Spring 2020)",https://i.ytimg.com/vi/44B6YmSuk0I/hqdefault.jpg,Onur Mutlu Lectures,PT23M23S,false,7275,120,0,0,3,okay shall we get started it seems like people are not scared yet so most of you are here is this come is this a comfortable seating arrangement no right that's it's quite packed but there's quite a bit of interest and this is one of the largest lecture rooms I think I'm not sure if there is a lecture room that that can that can house all of us in a single room actually some people are in f5 if you if you're if you're aware of it okay so we're gonna pick up where we left off yesterday as I promised remember you are covering some principles and talking about course goals in the meantime and I introduced architecture computer architecture as a resembling architecture building architecture and we talked about some Santiago Calatrava works which are principle designs and before I came to ETH I used to teach this course at Carnegie Mellon University in Pittsburgh and I did know less about College Java at that time after I came to ETH I included the College Java works which are quite interesting but when I was at CMU I used to actually use the analogy from something else which is this building has anybody been here no it's in the heart of Pennsylvania very close to Pittsburgh but if I call I say falling water this is basically a building called falling water it's built in a waterfall and at the time it was constructed one of the well it was one of the most innovative designs it's actually still a quite innovative design if you take architecture 101 you will learn about this building so basically this building is constructed on a waterfall and you can see that the it's imitates the waterfall it's an example of an organic architecture and the architect of this building clearly was a principled man he did design this building he did not design something else so if I ask you what is this it's really the masterpiece of a famous architect does anybody know who the architect is so it's difficult to know who the architect is I assume if you don't know the building yes okay so you know the architect did you know the building okay maybe I didn't see your hand up yeah that's the difficulty on being in a large group so it's possible that people actually know but I cannot see all the hands maybe a machine learning engine can do better than I do right it can detect all of these things because it can have sensors in all of these places that's it that's one that's one area where a computer can do better than a human I think because I can only see a limited visual field whereas a computer can have a much bigger visual field as you can imagine okay so this is the architect Frank Lloyd Wright he was similar to court job in many ways in the in the sense that he was a principled man him he followed his principles and he also construct expensive buildings and Fallingwater is one of these buildings and you can see that after its completion it was Wright's most beautiful job and it was listed among a life list of 28 places to visit before you die might be a good idea I would recommend that for sure I actually went there many times while I was there and I recommended my students to do the assignment over there and I think you get inspired quite a bit okay so let's take a look at this this picture doesn't do justice to falling water I think the previous picture was much better but this is one example architecture and this is another example and clearly these are differences right similar to the style - versus your vanilla train station that I showed you earlier this is a house falling water versus your vanilla house in a similar setting clearly there are many differences over here which I'm not going to go into cost is one difference but performance could be another difference over here I don't know aesthetics is that other difference over here clear this is not listed as one of the places that you must see before you die I can guarantee you that but it's not a bad place to live also right so it's it's all about the trade-offs that you're making in the design okay so again I will not go through all of this we talked about color Java I can go through many many architects related to this but clearly this was a masterpiece and the architect was able to design it and they're all there's a lot that goes into that design very similar to when you design computer computer architecture is actually computing platforms a lot of these actually play into that also but I will focus especially on the last two as I mentioned earlier basically you need to have a very strong understanding and commitment to fundamentals as well as principle designs and clear there are principles behind these designs and in in the course our goal my major goal is really to give you those principles in the design of computer architectures how the computer works from the ground up so that you can see those good principles because some of the good principles even though the computing architectures change and evolve the principles always remain ok and you will be able also uncover some of these other skills hopefully ok so this is the quote from the architect himself actually he was a hard-headed man Frank Lloyd Wright and he said basically architecture should be based upon principle and not about upon precedents precedent is what comes before so it's very easy to replicate what comes before and I think this is actually a good code to apply in many areas of life not just design itself right if you think about law legal systems it should it be based on principle or should be based on precedent sometimes it's a combination of both actually but you can always argue that it should really be based on principle but not precedent because President may be wrong right there have been legal systems where law was discriminating against people right does that mean the president is correct and principles can be made may be all may also be wrong right so it's good to keep that in mind and I think architecture in general is a very broad field and computer architecture is a very smaller incarnation of it but I think it's good to think about this philosophical implications of how you're designing a system going into the future also actually I should especially machine learning systems have some a flavor of what I just mentioned right you design a machine learning system it's learning from some data and your data may be completely biased right in the end the decisions you make may be completely biased right actually we've seen many examples that machine learning systems because of the way they learn they associate some particular jobs let's say with a particular gender because that's the way it is and they learn based on that and if if someone says talks about a doctor and if the machine learning system is translating it to a language that as that is that is gender based that becomes a heat right because there's bias in the data through which the machine learning system is trained right that's that's just the way the data is right but that doesn't mean that that precedent which is the data should dictate what happens into the system going into the future right so this is good to keep in mind you can actually read about this there are actually really interesting studies that show how language translation systems are biased when they actually are learning from data and when they do the translation because they have to pick a pronoun he or she and when they actually do the translation for a particular 4dr they picked the heat and when they actually do the translation for I don't know teacher let's say or nurse that's better actually perhaps they pick the sheet and that becomes the norm going into the future I mean rely on these systems even though they are completely biased right that's not necessarily a good thing going into the future we don't we don't want to be the slaves of this precedent which is really biased in the end okay anyway that's I hopped on this because I think this is really important I mean this this person clearly didn't a mission machine learning systems going into the future but this applies to a lot of things that we design it's a he clearly thought about design itself and machine learning systems are part of the design that we're doing going into the future and their computing platform designs okay well because he didn't want the precedent base design he didn't replicate or he didn't incremental II improve something like this he did something completely different that no one ever else no one ever taught of in in the past and this is actually based on his principle organic architecture you can read about it it's very interesting but I'm not gonna harp on it again again basically it's a flower for your architecture which promotes harmony between human habitation and the natural world through design approaches that actually combine both in some sense okay so a principle design and also strong understanding of and commitment fundamental so let me do the takeaways before we go into some other things basically I think my the reason I actually talked about a lot of these examples is because I would like to mention that it all starts from the basic building blocks and design principles we're going to design computers in this course and we need those principles and we also want the knowledge and of how to use and when to apply them that's really important again as I said earlier underlying technology may change underlying context may change it may be architecture building architecture it may be computing architectures may be a machine learning system but all of those might change but principles remain the same in the indicators of building architecture you may be constructing something with steel versus wood but methods of taking advantage of technology bear resemblance they may not be exactly the same but some of the principles you make and methods used for design depend on the principles that are employed so let me give you very high-level examples right there basically all of these are multi-core engines at some point these is a relatively old slide so it doesn't have the latest and greatest multi-core engine that you may that be you may have in your pocket but these are all based on the same building blocks in the end transistors logic gates and memories caches interconnects but you apply the principles in different ways to cover different trade-offs to design different systems anyway we're going to see actually pretty much all of these in the lecture but we're not going to of course talk about every single one in detail but we're going to see the principles that build up a GPU for example let's say GPU at that time it had four hundred forty eight cores now right now actually it's GPUs are much larger as you've seen yesterday there was a chip that had four hundred thousand course it was not a GPU but it was a machine learning accelerator so there are other computers their basic building blocks and design principles that go into them I'm gonna go through this relatively quickly and these basic building blocks remain the same actually okay this is what I mentioned just now actually this is a jog your memory so what are those basic building blocks these are actually what we're going to learn hopefully you learn about electrons as I mentioned yesterday so we're not going to cover electrons but we're gonna start with transistors and then build logic gates and then build combinational and sequential logic circuits sequential logic circuits are the ones that provide storage elements and memory so that you can remember combinational logic doesn't remember it just computes and we're going to build other building blocks on top of these course caches interconnect and memories and we're going to talk about all to actually interconnect these things or combine these things so that we can actually do computations in a much faster ways does that sound interesting okay so hopefully we're gonna cover a lot of principals that are employed in existing systems so basically you will get to know a lot of stuff that's inside here okay so but I guess you have to pay something to get something and your payment is studying hard does that sound fair and there are two books I recommend in this course I mean you can follow either one especially when I recommend both of them I would write I would suggest reading both of them but in the end you're not really required to read anything in the even though I call these required reading assignments you can you can do very well in this course as long as you understand the material and what I talk about in lectures and you do the assignments especially the homework assignments that we put out which are not graded but you can do them on your own because there are solutions out there so but but the reading assignments will help you get to a point where you understand things a lot more and these two books are very different in their approach I really like both of them actually and it's very hard to pick one of them and in general I don't want to buy I don't want people to be biased in one side that's why I actually go through two books in fact if you have this opportunity it's good to compare the approaches of two books they're different approaches educationally ampatrick pedagogically that's why I keep these two books but don't feel if you think that reading is a lot don't feel compelled to be both you can pick one you can pick your favorite you can do some profiling and say okay I like this one better and go with that right that's the idea we will cover the material in the class anyway the material that you will be responsible for this is really for your deeper understanding so this is today for this week we get reading assignments which you should probably do by next week chapter 1 and Harrison Terrace and chapter 1 and 2 in pet and Patel these are basically really the principles that you need to get started and this is something that you should really do on your own I'm not going to talk about this hopefully people know about binary numbers have you studied binary numbers okay that's good that's why I don't want to cover it because it's a really boring lecture so study this on your own it's going to be very easy if you have questions but me or the TA is know so there's their lecture slides on binary numbers okay okay so what are the goals of this course that was part of the like title right basically we are going to look at digital circuits and computer architecture or digital design and computer architecture and my goal is to enable you to understand the basics to understand the principles of design and understand also the precedents so precedents are important because you should know what comes before so that you can do better than that so Frank Lloyd Wright knew what came before and he did better than that and whenever you actually know what came before like this you can do much better than that right I can do much better than that I can claim but I don't have time to do much better than that anyway that's a joke so babies on such understanding hopefully you will learn how a modern computer works underneath evaluate the trade-offs of different designs and ideas implement a principal design you will basically implement a simple microprocessor and hopefully you will learn to systematically debug increasingly complex systems because you will have to do some debugging while you're doing the implementing this principal design on an fpga board initially we will start with simulation and then you will use an fpga board to actually implement this very very simple microprocessor and hopefully all of this will enable you to develop at some point new out-of-the-box designs not necessarily in this course but going into the future it will help your education and again my focus on basics principles and precedents and how to use them to create and implement good designs trade-offs are really important we're going to cover a lot of trade-offs because hopefully as it is clear there is no right or wrong in a sense it all depends on what you're going to what your target is what your design goals are and what kind of trade-offs you're trying to make to achieve that design goal right you can achieve a design goal in multiple different ways but the question is what are the trade-offs to do there and every single idea has an upside and a downside there is no idea that only has upsides or only has downsides every single idea is an upside and downside and upsides and downsides can sometimes depend on the context in which the idea is used so we will see that when we come to discuss a lot of ideas okay so why these goals well hopefully because you are here a computer science degree how many people are here for a computer science degree okay how many people are here for electrical engineering okay I could easily replace this for the electrical engineering I think it's the same basically in the end it's really a continuum electric engineering computer science but anyway regardless of your future direction learning the principles of digital design and computer architecture will also be useful to design better hardware design better software because once you understand the hardware you could do much better with software you can design better systems in the end algorithm architecture Co designs that we talked a lot about yesterday you can make better trade-offs in design and hopefully we can make better trade-offs in other designs not necessarily computing designs but designs that you see out in the world right at RAM for example like we discussed yesterday you can understand why computers behave the way they do and solve problems better hopefully I think these are more problem solving directions that I'm also very very interested not necessarily the knowledge that you get but also the thinking that you will develop out of this course especially when you're thinking about the different trade-offs hopefully you'll be able to think in parallel because you're actually just going to design things that are going to operate naturally in parallel hardware actually is completely parallel because when you look at a hardware design a lot of signals will be happening at the same time concurrently and when you're designing hardware you have to think in parallel to make sure that all of those signals do the thing that you really want that hardware to do in a given clock cycle we'll talk about the clock cycle also okay and then hopefully you will think critically that's really important because whenever you see an idea you should be able to think critically like whenever you see the two designs that I showed you following water versus the vanilla house in the middle of the woods you can think critically to decide the different trade-offs any questions okay so I'll go through these slides relatively quickly I'm gonna give you the course info on logistics these will be up online and you can ask questions about them later on also I don't want to spend valuable lecture time going through these nitty-gritty in detail I'm gonna focus on it a few things I mean I just introduced myself yesterday so hopefully you know all about this we have a large team that will be giving lectures as well as providing assistance our head assistants this one over there one so you should know him Hassan who's not here right now is the vice a distant he's my PhD student Frank will sometimes come give lectures he is also a lecture in this course he's not here right now but there are also other key assistant and get guest lectures Mohammed Louise Joe Addison I'm not going to introduce them all because they're not all here necessarily but you'll get to know them and ministerium there Aldo Rahul and Konstantinos so a lot of them are doing actually their PhDs or they've already done their PhDs in computer architecture so they're experts in these area ok and they're also a student assistants most of these folks have taken these this course in the past actually some of them i'm taking this course t8 this course assisted this course multiple times so there are seasoned assistants they're going to help you especially with the labs ok and there are labs that you should sign up for and we're going to assign the assistants to them that's why there was activity so this is important whenever you need help post your question to the Q&A forum sometimes your fellow students can answer it sometimes we can answer it so that's the best way actually to get help you can also write an email especially if it's a private issue to the instructor myself or this list which it goes to all of these myself as well as the assistants you can come to office hours and they're listed I'm not going to go through them you can find them this may change over time we'll announce the changes and if you really want to know what's going on you should really go at the website how many people visit the website of the course ok that's good so you know and you can find it basically everything is in the website if you didn't come to lectures if you don't want to come to the lectures you can go to the website and click on the videos and watch them right that's and click on the PDFs and download them and look at our powerpoints and go and play them at your pace so you can take this course at your own pace essentially ok basically the website is really single point of access to all the resources and you will you should also check our athe mail because we may send email related to the assignments over there and of course you can consult the myself and the teaching assistants as well okay you know when the lectures are I already said this attendance is for your benefit I don't require anything we don't take a cent attendance and hopefully attendance is useful so you will value it but if you don't really that's fine also I don't mind okay some days we all have guest lectures and exercise sessions but we will see that's dynamically okay lap sessions are online and you should definitely attend the lab sessions again I don't believe we take attendance over there but you will need to do the labs to actually pass the course and you will need to provide mandatory lab reports and labs will start this is next week right every 28th ok next week next Friday I think ok and you can find the handouts over here so for the labs choose your preferred group in Moodle you can have these slides already online actually you can go and link click on those links and choose your preferred group choose your partner and if you've taken this course in the past and if you want to use your lab grades from previous years there's a procedure that you should follow and you can see that procedure online final exam is a part of the evaluation I think it's the 70% of the great it's a hundred eighty minute with an exam and basically you can find the examination rules I'm not going to go through them right now also you can find them in the first page of the previous exams and all of our previous exams are online so you can actually go and look at the exams and look at what kind of questions may come up and you can see that the questions a lot of the questions are actually predictable but they require you to understand material basically like I have a favorite GPU graphics processing unit question I always ask on exams and you will see that it's a it's a great question I mean whenever you change the question you need to know what's going on to really be able to answer that question essentially you cannot just memorize an answer and actually solve the question but if you know of the principles and you can solve that question relatively easily okay okay basically some exam questions are similar to question the optional homeworks and optional homeworks actually incorporate some questions past exams also so it's an interesting loop here but but is all but I didn't say over here sometimes you may get a question that tests understanding but there's not necessarily exactly the same flavor of what we've asked in exam in a past homework and that test understanding also clearly so optional homeworks are optional but they're highly recommended so I would recommend doing those I'd recommend going through the past exams also okay I think that's all I have for logistics basically these are the reading assignments for this week please do them as at your pace these are the reading assignments for next week meaning we're going to cover combinational logic and more coming coming from next week and you can also check the course website for all future readings we will have required readings recommended readings and mentioned readings required readings are things that are really directly covered in lecture but clearly we didn't cover everything in the reading because reading is much longer than lecture but also I sometimes cover stuff that's not in the reading so it should be careful about that also not nobody actually wrote about what we covered in the previous lectures it's not in the papers it's not in the books okay any questions otherwise we're going to switch the lecture to be 
hOeIkAYraTE,27,"Computer Architecture, ETH ZÃ¼rich, Fall 2020 (https://safari.ethz.ch/architecture/fall2020/doku.php?id=start)

Lecture 24: SIMD Processors and GPUs
Lecturer: Professor Onur Mutlu (https://people.inf.ethz.ch/omutlu/)
Date: December 29, 2020

Slides (pptx): https://safari.ethz.ch/architecture/fall2020/lib/exe/fetch.php?media=onur-comparch-fall2020-lecture24-simdandgpu-afterlecture.pptx
Slides (pdf): https://safari.ethz.ch/architecture/fall2020/lib/exe/fetch.php?media=onur-comparch-fall2020-lecture24-simdandgpu-afterlecture.pdf",2020-12-29T17:28:47Z,"Computer Architecture - Lecture 24: SIMD Processors and GPUs (ETH ZÃ¼rich, Fall 2020)",https://i.ytimg.com/vi/hOeIkAYraTE/hqdefault.jpg,Onur Mutlu Lectures,PT2H31M16S,false,980,21,0,0,0,okay hello everyone how are you good uh professor moodle cannot be here today so i'm going to teach this thing cmd processors and gpus which is my favorite lecture uh how was the exam yesterday was it okay good okay we'll see when we when we waited um yeah okay so what's different today with respect to previous lectures it's different the fact that uh we are not focusing today so much in in memory even though we would talk about memory you know that most of the previous lectures have been around uh dram how durant works during reliability security and so on and now we are going to start let's see a new uh at least today we we work with a new block of contents of the course which is processing paradigms and today we will gonna talk about two processing paradigms that are very related or at least one of them is based on the other one gpus are based on sim d processors and that's what we are going to talk about today so first part of the lecture will be about cmd processing and here we will discuss or we will introduce vector and array processors and in the second part of the lecture we will talk about graphics processing units i'm pretty sure that many of you must already be familiar with these two topics maybe if someone has already taken the computer the design of digital circuits course with professor mullu you might have seen already these slides but yeah because probably is not the case for everyone i think it's uh it's good and also um this lecture is uh going to be also the introductory of a possible uh later lecture that we will have about gpu and heterogeneous programming which is a topic that is quite useful and and also very much related with some of the research lines that we have in the group so uh over the course of the lecture please don't hesitate to stop to ask questions and if we can have like some sort of discussion that also uh could be really nice okay so uh essentially what are cmd processors and gpus uh in essentially what they are is parallel machines they are data parallel machines they are machines that are able to execute computation on multiple data instances at the same time so what we exploit here is what is called the data parallelism but as you will see this is not the only thing that we're going to be able to execute for example uh in gpus where we can also have some sort of task parallelism we will talk about that later but in principle this is what it is it's data parallelism and uh especially i mean they will be especially good when this data parallelism is regular okay so before we start it is good that we take a look or we somehow review something that i'm pretty sure that you already know is the flynn taxonomy of computers mike flynn in 1966 classified computing systems in four main categories based on how do they execute instructions to operate on data the simplest of this category is csd is the first one single instruction single data this is what we understand by a sequential machine okay or a scalar machine uh where we operate one single instruction on one single data element the next one is cindy single instruction operates on multiple data elements we have a lot of elements maybe hundreds thousands or millions of elements and the instruction that we want to execute on these individual elements is the same for all of them so that's what we call uh the cindy paradigm in this paradigm we have the two main types of processors that we will talk about in the first part of the lecture is are the array processors and the vector processors then we have multiple instructions operate on single data element this is not so something that you will find so frequently in real world machines but there are uh some of these uh like for at least similar ones like for example systolic array processors streaming processors or um you might have heard also about i think professor moody talked here about the microns automata processor could be seen as as some sort of misd and the last one is mindy multiple instructions operate on multiple data elements we have multiple instruction streams examples of these or let's say that the most straightforward examples of this are multi-processors and multi-thread processors like for example the multi-core processors that we have these days in every uh computer or even cell phones okay where are we today in this cmd right single instruction operates on multiple data elements let's talk about data parallelism how are these cmd processors going to exploit data parallelism they are going to carry out they are going to perform concurrent execution uh because by executing the same operation on different pieces of data this is what we call a single instruction multiple data and there are many examples of possible single single instruction multiple data execution in the real in real world applications for example the vector the dot product of two vectors right when you're doing the dot product what you do first thing that you do is multiplying one element of one vector with another element of the other vector or let's say element zero of the first vector multiplies by element zero of the second vector element one by uh element one and so on right so we have data parallelism there because we are uh we have multiple data that we have to operate on and it's simply because we can execute the same instruction on all these instances of data here just very briefly what's the what's different with respect to data flow machines or data flow execution it's different because in in data parallelism the concurrency arises from the fact that we need to perform the same computation on multiple data elements in data flow machines uh the concurrency arises from the fact that we can execute multiple different operations in parallel and we say in a data driven manner because these operations only are only fired when the data is ready okay and also the contrast with thread or control parallelism which would be something similar to programming with multiple threads like p threads uh openmp and so on which is what we usually doing multiprocessors or in multi-core machines there we have different threads that can execute completely different programs right in cmd what we are doing is exploiting operation level parallelism on multiple data and this can be seen as a form of instruction level parallelism meaning that we have multiple instructions that can be executed in parallel but it turns out that this instructional level parallelism the instructions are always the same ones right it's the same instruction that is applied on multiple data elements so in simple process processing single instruction operates multiple data elements and this is something when we actually run the computation when we actually have this concurrent execution that we were saying in the previous slide this is something that can happen in time or in space we will typically have multiple processor processing elements not always you will see some examples later but we will typically have multiple processing elements and and and we have this time space duality that differentiates between what we call the array processor and what we call the vector processor in order to see this very clearly with a very clear example we have the next slide and and and here you can observe already the the main differences between the array processor on the left hand side and the vector processor on the right side uh as you can see in this array processor we have four processing elements and the four processing elements are exactly the same the only thing that is different is that they have a different index right from processing element 0 to processing element 3. in the case of the vector processor we have different we have several processing elements as well but these are specialized they are not able to execute every instruction as is the case in the processing elements of the array processor here what we have is processing elements each of which is specialized on a certain type of instruction and you can already see that here we have one of them for load instructions add instructions multiply and store okay and how do we execute a program on these processing elements in the array or the vector processor okay let's assume that this is our program very simple one what you see is that first thing that we do is a load instruction so here we go to memory to this array a and we read some elements in this case four elements right from a0 to a3 we read these four elements and we place them we keep them in this vector register next we operate on the vector register we add one to every single element to the four elements that we just loaded into the vector register after that we multiply by two and finally we store the result in the same array that we had in memory right how do we execute this on an array processor so because we have four elements we want to read the four elements at the same time so first thing to do is this load instruction is executed on the four processing elements in the first cycle or at least we issue the instruction in the first cycle it might take more than one cycle to execute right but we can we can issue the four loads at the same time in in the same cycle when we are done with the load instruction we schedule this at instruction and because these processing elements are somehow um flexible or general purpose because they can execute many different instructions so we are executing this at 0 1 2 and 3 using the same processing elements and then we have the multiplication and then we have the store operation how do we do that in a vector processor the first thing that we have to do is issue one load this load 0 will load element 0 of array a one cycle later we can issue load one and if we already have the value that we have to read from memory a0 then we can start operating on it right let's assume that we can do all these instructions in one in one single cycle right so right after we get the value a0 from memory we can start adding one in the corresponding processing element and in the next cycle load two add one and multiply zero and then load three at two multiply one and store zero and so on okay so observe the difference in the case of the array processor we have the same operation at the same time in the case of the vector processor we have different operations at the same time in the same cycle if we look at the space recall the time space duality that we were talking we were mentioning in the previous slide if we talk about the space then we have different operations in the same space in the same processing element in the case of the vector processor we have the same operation on the same processing element okay okay we were comparing before with data flow machines and with multi-thread machines one more comparison that is worth doing you might also be familiar with the vliw paradigm very long instruction words this is a sort of machines that held by a very wonderful compiler that is able to extract instruction level parallelism from the programs they generally they create a program where every single instruction is a very long instruction that actually has like multiple instructions in it multiple instructions that are independent and can be issued at the same time so they can be executed on the processing elements at the same time because they are independent and we can exploit uh instructional level parallelism in this way why is this different from cmdb processing well it's different i think it must be clear that here we have multiple processing elements as well but on each of them we are going to execute a different instruction that will be likely working on different data elements as well while in a cmd processor and a cmdr reprocessor we have one single instruction and this instruction is executed on the four elements okay operating on different data instances okay so yeah let's continue with uh vector processors uh vector processors i mean receive this name because they operate on vectors right and you know already what's a vector you know that this is uh and uh in principle one dimensional array of numbers it can also be like two-dimensional or three-dimensional um etc but in principle uh we will only talk about one-dimensional arrays many scientific commercial programs use vectors this is something that you already know and and here we have our first example uh operating on um on two vectors a and b and we are calculating the average of element wise average of these vectors and storing the result in array c so this program um is like the typical vectorizable program that we want to execute on a cmd processor so here we have multiple instructions so we have instructions that operate on vectors not on scholars and there are some basic requirements or basic things that we are going to find in a vector processor these are uh first of all we need to be able to load and store vectors and for that what we use is these vector registers recall the example that we had before we execute the load instruction we have to go to memory and in that example we were reading four elements at the time and when we read the four elements from memory where do we put them we have to put them in our register right so these registers are called vector registers and each of them will contain multiple the data instances instances or or uh elements of the vectors um but these vectors and also so these vector registers have a certain length right so they i mean we they will have the size that that they have right for example 50 and we can 50 we can fit 50 elements in in them and this is what we call the uh vector uh length usually this vector length is going to be stored in a specific register in a special register that will contain the vector length and then there is another register that is important for the data access for the memory access is the vector stride in the example that we saw before or in this example as well what you have what you can see is that we always access consecutive elements right we would start from element 0 until element 49 and we are going to read them all so here what's the stride uh among consecutive axes the stride is one right no matter i mean we are not talking about the vector length here but assume that the vector length is 50 so we can read the 50 elements uh at the same time and we could do that but the stride among these elements is just one in some cases it won't be that way right in some cases we are going to have a strided access we might be for example reading uh only uh um even elements or only odd elements right and in that case our stride would be two so when we are writing a a vector program a vector processing program and and we need to access memory with a certain stride we will have to enter that stride into this register so vector instruction performs an operation on each element in consecutive cycles this is what we saw if we go back to um the vector i mean this comparison here we operate on different elements in different cycles this is the case in vector processors right so that's what we have here the good thing of doing that is that it is possible i mean because these execution units these processing elements are will be typically pipeline the good thing of using them for vector processing is that um they can be deeper pipelines we can have more pipeline stages and i mean i'm pretty sure that you're familiar with uh pipelining right you might remember that in a pipeline in a pipeline we have multiple stages the more things or the more computation that we have in each of the stages the longer the cycle will be right because we have more uh combinational components there so we need more time to traverse them all so the the the the cycle duration of the machine depends on how many operations how many things we have to do in each of these pipeline stages that's what uh is called the critical path this critical path in the pipeline is what defines what's the um the frequency that the machine can operate at right so uh if we have deeper pipelines we can probably have less things to do in each of the pipeline stages and this way we can have a higher frequency i mean that's more or less uh the reasoning and why is that possible why is it possible to have deeper pipelines when we are executing vector instructions it is possible because there are no intravector dependencies recall how these vector instructions are executed in the vector processor it's we start the operation on every single single element in consecutive cycles because in each processing element we operate on the entire vector and because the computation that we apply to the different vector elements is completely independent of each other whatever i do with element a0 or element b0 is independent of what i do with element a15 right so there is no intravector dependencies so that's what this means and there and there is also no control flow within a vector recall in a conventional pipeline in a conventional pipeline you execute load instructions add instructions multiple instructions but you also execute branches right because you want to have control flow in your program and the problem with these branches is that sometimes you need to wait a few cycles until the branch are the branches are resolved right um this is something that that doesn't happen in a pipeline processor and one of these pipeline processing elements if we execute vector instructions why is that because there is no i mean there is no that the execution of the operation on element a1 doesn't depend at all on the execution of element a0 so that's what this means we don't have this control flow within a vector so there is no there's never a need to for example install the pipeline because we have to wait until a branch is resolved or these kind of things that happen in in conventional pipelines and also we were talking before about the strided accesses here the stride is known right we i was saying uh if you want to access only the even elements of the array you will have to enter in these um in this uh stride register you will have to enter number two so that the hardware knows that the accesses have a stride of two right and because this is something that the hardware knows in advance then the memory access can be very efficient right for example it is possible to prefetch data you might be it might turn out that at this point in time i'm operating on elements 0 to 49 and only operating on 15 elements because the vector length is 50 but the size of the array is much larger right for example 200. the thing is that if i know the stride i know that i will be accessing all the elements that are a certain stride for example with stride two i could be reading vector zero or array a zero two four six and while i'm operating on these ones i can be accessing memory and for example bringing from memory prefetching element a20 22 24 and so on and this is something that is doable that is possible because we know in advance what's going to be the stride of the memory accesses so that's why we can also have efficient um memory access here as well okay advantages i mean we are already talking about the advantages okay right but let's summarize them here there are no dependencies within a vector so this allows pipelining and parallelization to work very well so we can have deep pipelines because there are no dependencies each instruction generates a lot of work why is that good because i only need to read from memory one instruction to operate on for example 50 elements right so in a conventional sequential machine you need to go to memory to read instructions for i mean for operating on every single data element and in this case we are just fetching one instruction from memory to operate on multiple data elements so from that point of view this is more efficient highly regular memory access patterns it's what i was just mentioning this can also have some benefits in terms of exploiting the memory bandwidth better and no need to explicitly code loops uh what does this mean i mean just recall this uh for loop that we have they are from 0 to 49 there is a for loop there right so every time that we want to execute this of this each of these instructions that is computing the average of the elements of a and elements of b we need to compute and then check a condition and then jump right so these are the branch or the control flow instructions we donate them here we don't i mean in principle when we are operating inside a vector we don't need them right because we are operating on the 50 elements at the same time but there are also disadvantages of course a very easy to understand disadvantage is the fact that we need the parallelism to be regular what happens if the parallelism is irregular can you use the cindy machine well maybe you can use it as well but for sure it won't be so efficient uh we are going to talk about this again later when we talk about gpus because in gpus it's possible to uh to execute irregular parallelism it is possible to do it right but for sure it won't be so uh efficient you you cannot achieve achieve the same throughput as if you are executing regular computation but at least you can do and then another disadvantage of these vector processors is that they are also typically they are difficult to program and this is something that already uh fischer in 1993 pointed out you can you can read this extract from the paper to program a vector machine the compiler or hand coder must make the data structures in the code fit nearly exactly the regular structure built into the hardware that's hard to do in first place and just as hard to change so this means programming is difficult why is that because when we are writing a um a vectorized program or where or even when we are writing a program for a gpu we need to try to make things regular as much as possible or at least make sure that things are going to be executed in a in a regular manner or at least as much as possible okay based on this um on this um challenging uh uh thing that we have here programming cmd processors we might have our gpu uh meteorologist programming lecture later another day we will probably take a look at this slide again by then and another disadvantage is that the memory bandwidth can easily become a bottleneck especially if memory and compute operation balance is not maintained what does this mean this means that cmd processors can be very efficient right you can have multiple uh processing elements working in parallel and they can be operating on many input elements at the same time also in these vector processors and as we have just discussed we can have deeper pipelines because we can have different deeper pipelines it's likely that we can operate at a higher frequency so what this means essentially is that we have a lot of compute power we can execute operations very very fast i mean let's say arithmetic operations for example very very fast right but in order to be able to execute these instructions we need to be able to gather data from memory very quickly as well right and this is this is why this is going to be typically the bandwidth here because as you already know because this is something that professor mundo has explained and and that's all actually what has motivated all these research that we are doing in processing in memory accessing data from memory is much more costly than operating on these data and this is something that for sure is clear in in vector processors and in gpus these days even though they are incorporating these new memory technologies like for example um hbm they are still bound by the memory bandwidth okay um actually uh i mean you can even compare what's the ratio between the number uh so between the uh peak flops the peak floating point operations that you can execute on a gpu these days with respect to the bandwidth that this gpu provides and if you compare this to a gpu that was built 10 years ago for example you will see that with this ratio is even worse this means that in 10 years gpus have changed have evolved a lot they have much more compute throughput they have much more bandwidth but the distance between these two is larger now than it was 10 years ago so the memory bottleneck problem exists and it's even worse these days uh well we were talking about this compute memory operation balance but also the fact that we need to make sure that the access to memory at this we do or part let's say with workshare is like let's try to make the memory access efficient by mapping data appropriately to the memory to the memory banks we will talk about the memory banks right now okay vector processing in more depth any questions so far is everything clear okay good so uh we have already talked about the vector registers each vector register is going to be able to hold n m bit values okay so this is one of our vector registers as you can see here we can store up to n elements and the size of these elements is m bits okay and we have in this case like three different vector registers then we also have the vector control registers we have already talked about two of them vector length and vector stride but there is a third one which is super important as well is the vector mask vector length is actually the maximum vector length in the previous example i was saying oh let's assume that our vector length is 50 what does this mean this means that we can be operating on 50 elements at the same time right so we can be executing like um yeah computation on on on 50 operands at the same time but this vector length is the maximum vector length that as you can guess it's related to the hardware it's related to the number of processing elements that we actually have in our system what happens if instead i mean the maximum vector length is n the maximum vector length is 50 but we only want to operate on 15 elements not 50. what would we do so what we have to do is in our program we will have to say that this vlan is equal 15 not equal 50 okay and then we have the well we talked before about the vector stride and here we have the vector mask register this is going to be very useful especially when we when our computation is not so regular right so we were saying before cmd processors or vector processors are very good when the parallelism is regular and when we uh use all the vector lanes when we use all the elements in the vector registers and we operate on them on them in parallel right but what happens if that is not the case what happens if for whatever reason we don't want to apply the same computation the same instruction on this element but we want to do it on this one and this one and this one and this one well in that case we can use the vector mask essentially the vector mask will contain a value zero or one depending on uh the fact that we want to operate or not on a particular element okay for example we say okay if the element is zero we don't want to operate on it if it's zero the the value of vector mass i will be zero and then we don't do anything so that's the reason why we need this vector mask anyway we will see a better example later in a few slides okay vector functional units what's what we call the vector functional unit is the collection of functional units that operate on a vector right we already know that they can have a deep pipeline which means that we can have a faster clock cycle and we don't have to worry that much about uh control of the deep uh pipeline because uh the vector uh i mean the the operations inside the vector are independent this is something that we have mentioned before let's assume that this is our pipeline it's a pipeline with six stages recall what i was saying before recall so think for example that these were uh um conventional scalar pipeline right like in the the mips pipeline that the predator that you have studied and it turns out that uh here for example we have to access memory and the instruction after the load instruction that is accessing memory is dependent on this load instruction so what would happen is that if we are accessing we are going to memory to read something and this takes a few cycles let's say 100 cycles or so the next instructions that are dependent on this one will has will have to be stalled right until uh the the the load is resolved until we get the value from memory and then we can use the latest subsequent instructions can use the the value read from memory this is this doesn't happen in vector computing why is that in vector processing why is that because we have in this case two input vector registers v1 and v2 they contain multiple operands and what we do in this pipeline is that we first start the computation for element 0 in these vector registers then element one then element two and when we fill the entire pipeline we would have we would be operating on six different elements of these uh vector registers but the computation on them is completely independent so uh control of this pipeline is pretty simple as you can see okay one example of vector machine a pretty old one is gray one uh yeah you can take a look at that uh um technique technical board or data sheet from 1978 if you want and this is like a schematic of uh of uh this uh the internal organization of this vector machine as you can see here we have vector units but also scalar units for example here you can see scalar compute units there are floating point units here there are vector units there and here we have scalar registers and here we have vector registers you can see up to eight vector registers right so this is uh our very first example of real world vector machine actually we have a very similar one uh at uh cab you might have seen it this uh cray x mp 28 which is beautiful these days is no longer a supercomputer now it's a coach you can see it here it's pretty comfortable and because if you really want to run some computation here you better go and buy one raspberry pi you will see there as well the they they both are have uh similar peak uh throat put yeah 400 megaflops so yeah you can take a look at this it's a really beautiful one and here in the next slides you have a little bit more information about the internal organization this particular model has four cpus and inside each of the cpus you have what we have seen in the in the previous slide as well like you know scalar functional units floating point vector functional units and so on a little bit more information about the different models of this gray xmp uh different number of cpus memory size number of banks etc a few more details here about the functional units address functional unit scalar vector floating point and about the system configuration here and the different models like you know information about type of uh of memory um storage units etc and this is the uh person who was responsible for these uh these computers right uh seymour cray uh this is uh so-called the father of super computers and and i really like this sentence uh if you were plowing a field which would you rather use two two strong oxen or 1024 chickens and here you have uh you have them so what would you use well it will depend right it will depend on what you really have to do it depends on i mean all these these different processing paradigms you you will choose the one that or in principle you have to think about choosing the one that adapts better to your workload right the thing is that in the end uh we cannot build uh the perfect machine for every single workload and usually we'll have to you know computer architects have to make to take many trade-offs and and decide okay let's have something you know like a few let's say vector units for example but let's also have some uh scalar units as well because they are useful for many other workloads and this is actually something that cray already knew and that's why even though this gray one was the first vector machine this doesn't mean that it was also it wasn't also the fastest uh scalar machine of its time so not only a vector machine but also the fastest uh scalar machine okay and much of the success of uh this vector processing is based on the fact that we can access memory uh pretty efficiently and why is that because we can use multiple banks in memory in this particular machine for example 16 memory banks um let's let's see what we mean about the memory banks and about accessing memory in an efficient manner for for vector processors so we already know that we need to load and store multiple elements at the same time or more or less at the same time we will see how we do it because we want to operate on them because we have multiple processing elements that can run in parallel so that's why we need to read and write multiple elements at the same time from memory uh this we already know that these elements are separated from each other uh by a constant stride for now we're gonna assume stride equal to one and um we as you will see what we are going to do is that we are going to be able to load elements in consecutive cycles and such that we can start a load uh of one element per cycle that this is something that we are going to be able to do by using multiple banks inside memory um okay so uh yeah so the the question is also um how can you do that if the memory access a single memory access takes more than one cycle recall the very first example that we have for array and vector processors and i told you okay let's assume for now that every memory access every instruction every operation here takes one single cycle but unfortunately that's not the truth right you you know that uh processing elements are usually pretty fast you can um execute for example an additional multiplication in just a few cycles but if you have to go to memory you have to go to vram then you start you know talking about the order of magnitude of 100 or 200 cycles or something like that so yeah so that what we can do if our memory accesses take longer than one single cycle what we have to do is uh bank the memory so what we are going to do is take the whole memory or we are going to divide it in multiple pieces that we are going to call banks and that's what is called the memory banking in this example we have 16 banks instead of having one single monolithic big memory what we do is that the whole memory capacity is divided uh in uh 16 banks in this case as you can see each of the banks have its own mdr and mar i would like to remember to remind you what this means this is memory data register memory address register so how do we typically operate on this we have an address bus here so what we do is from the cpu we use the address bus to bring the address that we want to access in memory to this mir and this mir is connected in this case to bank zero and this allows us to take the contents of this address and put them here to the mdr and then by using the database we are able to bring this to the cpu so this is how each of the banks individually works right but now what we want to do is uh operate on the 16 banks in parallel at the same time and how do we do that well what we can do is that we can issue one request every cycle and because reading or writing from or into each of these banks takes more than one cycle we are able to start issuing requests to the multiple banks and a few cycles later we will start getting the values that we want to read from memory right so in this particular case we can sustain up to 16 accesses in parallel or any accesses in parallel if we have n banks this is one more nice slide to show you how the address is generated think about something like this recall that we have a stride right we have a base address which is going to be let's say the the start the the starting address of the array and then the elements that we are going to be reading or writing are going to be determined defined by this stride so we need some sort of address generator like this and this is the address that we will place in each of these mar okay so yeah so the way that we calculate the next address is the previous address plus the stride and and and this is just one example right if the strike is one and consecutive elements are interleaved across banks and the number of banks is greater or equal than the bank latency then we can sustain one element cycles throughput you understand that what does it mean it means that here we have 16 banks and we're going to be reading one element from each bank why is that because let's try this one and because the consecutive elements are stored in consecutive banks that's what this if says and the last part of the if says the number of banks is greater or equal to the bank latency why is that why is that important because we want to uh receive one element every cycle so for example here imagine that the number of cycles that we need to read one element from each of these banks is 11. if it's 11 we will have to wait 11 cycles to read element 0 from this bank 0 right so um because we don't want to just simply weight uh what we do is putting more banks next to this one and during these 11 cycles what we are doing is that in the first cycle we start the request or start the read in bank zero in the next cycle we start the next read from bank one then from bank two then from bank three and so on and in every 11 cycles later we are still you know issuing requests to the rest of banks 11 cycles later we can start reading the element that we needed from bank zero you see now imagine that instead of being 11 cycles is 32 cycles okay what what would happen during the first 16 cycles we are issuing requests but then after that we would have to wait 16 more cycles until we can actually get the first value from memory right so that's why if we want to really you know like have uh or memory dimension in a nice way we want to have more banks than cycles we need to access memory okay you have a question for example but say the bank latency is like five then if not when we need 20 banks instead of because we can pre-fetch then four data are designed from four different times and we can just overlap the ladder so it should not be like bank latency into number of processing um you're asking if the bank latency is related to the number of processing elements that we have or number of banks should be greater oh the number of banks related to the number of processing elements um yeah i mean that that might make sense i mean it will depend on the on the specific architecture for example if you are thinking about an array processor and these are in an array processor where you have let's say 16 processing elements and each of them can execute any every instruction like addition multiplication and so on so you compute something using additions and right after that then the next instruction instruction is a load right so yeah in this case it could make sense to say okay if i have 16 processing elements let's have 16 banks as well and so that we can issue the request for all of them at the same time okay that's let's say intuitive but doesn't really need to be like that you know because you could have more banks or you could have less banks as well but how efficient this is actually going to work will depend on how how many cycles you really need to read from memory okay we can maybe discuss later specific examples and and we will likely have uh questions in the homework where uh you will have to think about about this dimensioning okay okay uh let's uh take a look at uh a good example here it's a element-wise average right you you remember that code that we have seen before from i equals zero to 30 through to 41 we are going to calculate the element-wise average of the elements of two arrays a and b right and we store the resulting array c so this is an example of a scalar code let's see let's think about a sequential machine cpu for example and in this cpu what we are going to uh run to compute that uh element-wise average that we have there is something like this this would be like our assembly or matching code right so uh take a look at what we have here first thing is uh we load 50 in register r0 why is that because we need to perform 50 iterations to compute over the 50 elements that each of the arrays have then we have the address of a and the address of b respectively in registers r1 and r2 and then the address of c in r3 next thing that we have is load instruction to load here into r4 something that we are reading from memory and in particular we are reading uh r1 which is a and then we have here some some sort of auto increment addressing um we do the same for r2 which in this case is array b and now we add these two registers the contents of these two register r4 and r5 store the result in r6 and then we shift right because we want to uh divide them by two right and then the result in r7 is what we write into memory we store into memory in whatever address is determined by uh this razer three and finally the only thing that we do here is we decrement in this case register zero and compare the value to zero and if it's not zero then we jump or branch and go here and start and we do this like 50 times right so if you count how many dynamic instructions we have here after you know like running the 50 iterations of this loop this means uh 304 dynamic instructions okay now recall that we have to read these 304 dynamics dynamic instructions from memory hopefully not from the main memory hopefully from the instruction cache but we still have to fetch them from memory okay the problem with this scalar code is that i mean it's it's uh quite inefficient right why is that well first of all let's assume that okay yeah first of all uh let's think about a memory with one single bank if we have a memory with with one single bank and we compute how many cycles in total do we need to execute these 304 dynamic instructions what we will have to do is adding up all these latencies that we have here i didn't mention that but here in this column what we have is the latency of each of these individual instructions so for example load instructions take 11 cycles so the calculation in the next slide is say okay i have 50 iterations so this is 11 plus 11 plus 4 plus 1 plus 11 plus 2 is the number of cycles from single iteration so then 50 of them this is the total number of cycles here 2004 cycles this is in the case that we only have one single bank in memory but because we don't want to be unfair and we are going to compare to vector processors now uh we are going to consider that this in order processor this uh very naive cpu or can already use like a memory with 16 banks so what this means is that unless these two are mapped into the same bank we're going to be able to read them at the same time so let's assume that these for example a0 is in bank zero and b zero is in bank one they are in different banks so we can be reading both at the same time right so the good thing is that these 11 cycles can be overlapped right you already know that right so in the first cycle we issue this request in the next cycle we issue this request 11 seconds later we get this value and one cycle later we get this value so if we do that then we can reduce the total number of cycles to 1504 cycles okay okay yeah here you have one why 16 banks i already explained this right it's depend depends on what's the memory latency but now let's assume that i mean because you know the only computation that we had in that for loop to calculate the element wise average is very regular we can vectorize this loop meaning that we can execute that computation on a cmd processor or on a vector processor so if we vectorize with with this loop and we write this using vector instructions we can have something like this first thing to do is set vector length equal to 50 okay which is the number of iterations that we had in the loop next thing is vector stride equal to one why is that because we are all accessing all consecutive elements and then vector load to read the 15 elements from array a from vector a and then another vector load to read the 50 elements from b and then we add element plus element all of them in these two vector registers b0 and b1 then we shift right and finally we store so now latency of this the latency of this is here each of the individual operations take exactly the same amount of cycles but the good thing is that because we are using a vector machine as soon as we issue one instruction one load or one add or one shift right in the in the next cycle we can start with the next operation so that's why for example for this uh vector load in the first cycle we um issue the load for a0 and in the next cycle for l for a1 the next circle a2 and so on so after 11 plus vlan 50 minus 1 cycles we will have all the 15 elements read from memory make sense so here drastic reduction of the number of dynamic instructions is only seven not 304 so recall what i was saying we don't need to fetch so many instructions from memory and now for now we are going to assume chaining we will see what's chaining changing means that we can you know like forward uh data from one vector unit to another from one vector processing element to another vector processing element for now we ignore this possibility so we assume that we cannot start the execution of an instruction until the entire input operand vector register is ready we consider that we have one memory port meaning that we have one port per bank so we can be only so if we are using one memory bank we can only be reading or writing one element at a time and we have 16 banks and the mapping that we assume is this one is word interleaved meaning that consecutive words consecutive elements go to consecutive banks and now we can see this timeline how the uh this code that we have here is executed right i already mentioned this but yeah so these two is just one single cycle is stored in the vector length register and the vector stride register and right after that we have the vector load so in the first cycle we issue the request for element zero one cycle later the issue for the the request for element one element two and so on after 11 cycles because the latency of a memory access is 11 after 11 cycles we have a0 one cycle later a1 one cycle later a2 so in total after 11 plus 49 cycles we have this vector load executed now observe we have one single memory port so we can only start the next best vector load when this first one is completely done and again it's exactly the same 11 cycles to read from memory b0 and then one cycle later we will have the remaining 49 elements of vector b for array b and then we start the addition again for the first addition we need four cycles and then we will be obtaining the addition results uh in the next 49 cycles for shift and for a store so this in total is 285 cycles which is already a strong reduction with respect it's like five times faster or so than the uh thing for the sequential machine right now let's assume that we can use uh vector chaining with essentially means data forwarding essentially means that if we have if we are reading these elements from a and from b observe that as soon as we get here b0 we could start this addition right why is that because we obtained we we read a0 from memory and in this point and b0 from memory in this point so right here we can already start the addition of a0 and b0 and this is actually something that makes a lot of sense right why is that because we not only have load and store units in our system we also have addition units we have multiplication units and maybe many more right so it doesn't really make sense to say oh okay you can start the computation of the addition here but let's wait until here so during all this time we could be using this addition unit but we are not doing it right so we are wasting the resources so vector changing makes a lot of sense and that's uh essentially what uh it is about right that's like okay i obtain a partial result from the load unit and partial result meaning you know what one of these individual elements so as soon as i get it i can fit it into the multiply machine in the multiply unit for example here and as soon as i obtain uh the result of this multiplication i can change the result and send it to the or forward it to the addition unit right so this is uh chaining so if we apply chaining or to our element wise average program what we could do is exactly what i was uh telling you right after uh obtaining from memory b0 we can start the computation of the addition for a zero plus b zero right and as soon as we are uh we we have this addition a zero plus uh b0 so we can start the shift right right so observe that we are changing from here to here from here to here and you know there's no chaining here simply because we have to wait for the store until the two loads have finished and why is that okay this is uh 182 cycles so like 100 cycles less than the previous implementation but still we have some problems here and the problems is that we cannot parallelize memory accesses but you see that these two vector loads cannot be pipeline cannot be in parallel and this vector store is exactly the same right we have to wait in order to start the vector store we have to wait until the last the second vector load is completely finished right uh but observe that we got we could have started the store here right because here is where we have uh have already a zero plus b zero divided by two so we have it here and and we we still have to wait a few cycles and the reason why this happens is because we have one single port in each memory bank so possibility here is to of course at the expense of uh having a more expensive hardware uh we could have two load ports and one store pour in each bank so if if that's the case this means that for example we have two load ports so this means that we can be reading two elements from two different arrays that are mapped to the same bank right so we could be reading at the same time uh a0 and b0 assuming that they both are placed into the same bank in bank 0 for example right so this is what we have here we would only need to wait one cycle why is that because i mean the address bus and the data bus is shirt right but right after this cycle we can start the load vector load for b and uh and for the store operation because the store port is also independent of the load ports we can also start the store operation right here we don't have to wait until uh until this second vector load is done so in total like 19 times faster than the sequential machine so it's quite good okay questions yes you yes why is there an uh you mean this so so the the thing is that you you generate one address every cycle that's clear right so uh what you have uh here is that the first let's say cycle zero or cycle one you generate the address for the first memory access and one cycle later you have the address for the next memory access oh you're generating addresses for a and b yeah that's that's the that's the thing right we have two load ports so now you can assume that you can generate uh the addresses at the at the same time because of the data bus or under and the address pass i mean this okay also take into account that this is a simplification right so we don't go into all the details how this is this is let's say an abstraction of how of how this really operates but if you check the let me go back to the slide it's here right this is live so what you can assume is that uh this address bus is shared so um yeah you you might even have like several ports here but still for you know moving the address from the cpu to the individual banks you need to share this this is you can assume that this is why you have to wait one cycle but as i said this is a you know simplify this an abstraction that's uh because that's what we have to do to do to to explain the concepts right but that's more or less like that okay yeah so next thing what if the number of data elements is greater than the number of elements in a vector register okay that's a important question why is that because i mean in the previous example we were assuming that vector length was 50 and we have 50 processing elements there or somehow i mean we can be operating on the our vector registers at least can hold these 50 elements at the same time but that won't be always the case right what happens if uh or vector length is 16 or for example in this example what happens if for vector length is 64 meaning that every vector register can only hold 64 different values right but now instead of operating on an array or a vector of 64 elements we operate on us 527 elements so what do we need to do in this particular case we will first set the vector length register to the maximum possible which is 64. and we carry out like in this particular case uh eight iterations uh where the vector length is 64. so after the eight iterations we could have operated on five 12 elements but then we have 50 more elements right because we have 520 in total so what we will need to do is changing the content of the vector length register to 15 and then operate on the 15 elements okay yeah so this is called vector strip mining and uh the name is coming from uh from surface mining or strip mining which essentially is a mining technique that that consists of getting rid of the that you don't need and simply take what you really want like the gold or silver or or whatever right so in our case in the case of the vector registers uh the the goal is the data that you are really going to operate on and the uh and the shitty part is the you know the this uh tail that you don't really need in the vector right okay next question is what if uh vector data is not stored in a strided fashion in memory so this is uh interesting right because as of now we were saying oh let's consider that the straight is one meaning that we are going to read all consequent elements of the of the two input arrays or the two input vectors or now let's consider that the stride is two and we are reading a zero a two a four and so on and and this is fine and actually we have uh already said talked about the you know that the efficient memory access that we can have we can prefetch data because we know what's the next data element that is going to be needed and so on but unfortunately this is not going to be always the case right in some cases we will need to have irregular memory accesses and for example you can think about sparse matrix computation we had the other day constantinos right presenting his work uh where he uh figured out ways mechanisms techniques to uh improve the operation on sparse matrices or or vectors um yeah so this is something that uh uh also applies here right if we operating on um dense mattresses then that's perfect because you know the accesses for sure are going to be regular we will actually see an example soon but if we are operating on sparse matrices then is not only going to be like that you might even remember that constantinos was talking about indirect address accesses right he talked about the csr compression format and and he said oh you're going to have indirect memory accesses so if you have to operate on sparse matrices or sparse data structures you will likely have memory accesses like this one where you first need to access to read one array d and this array d contains the address that you need to read from array c right so uh the um problem here is that the accesses to memory are not any more regular so they are probably going to be costlier uh yeah but anyway uh cmd machines in the processors usually have this sort of load indirect that can uh starting from a base address uh in this case rc is the the base address contains the base address of a racy and having some indices uh stored in in this case vector d which is uh something that we just read with the previous instruction so we can generate the addresses from these two from the base address plus the index inside the array c and then go to memory and read some values and store them in vector c in this in this case this is what is called a gather instruction why is that because we are gathering data from memory and we are putting these data elements and inside a vector register and the opposite operation is called a scatter operation right and and here you have a example of that this is the data that we need to store for elements and these are the addresses in array c for example where or the indices in array c where we have to store these values so this is uh what we are going to store in memory okay so this is for example this 314 goes to index zero this 6.5 goes to index 2 and so on and here we talk about scatter operation scatter access right so we have the values that we need to store in a register and then we scatter them across the whole memory or the the whole memory space occupied by the output array okay yeah so as you can guess also i mean is this more or less efficient than regular accesses well it depends on the actual implementation but as you can guess this is typically going to be less efficient right and let me just give you an example so these elements might be in different cache lines right and you know that memory success at the granularity of a cache line so what happens if for cache line size is four for example i mean four elements right uh so these four are in one cache line these four are in another cache line and now in the same vector store we are writing these four elements and these four elements are scattered acro across two cache lines meaning that we need to write into two cache lines if we are reading is exactly the same so we are reducing ortho output by half right okay okay next thing is conditional operations in a loop and this is where we need to talk again about mask operations we already mentioned that before but now we are going to see a more concrete example recall that we have this vmask register which is a bit mask that determines which data element should be act upon or not so in summary this vector mask is a register and this register contains as many bits as vector as or vector length so we have one bit per vector for element inside the vector array and if the value of the vector mask of the value of this bit is one we want to execute the operation if it's not then we don't want to do that right and this is very useful whenever we have conditional operations so for example here we only want to perform this computation in case that a a i is is not zero so uh what we do here is we first read a few num a few elements from vector array a from array b and now we compare all the elements that we have in v0 we compare them to zero if they are zero then the result of this comparison is um is zero so that's what we will have in the corresponding bit in the bit mask in the in the vector mask uh if these two are different then the result is one and this one is the corresponding bit in the vector mask and then the next operations are going to be conditioned or predicated to the contents of this vector mask so for each of the elements in v0 and v1 these two vector registers we will only execute this multiplication if the corresponding bit in the best vector mask is one okay and this is how we can implement a conditional execution in a vector machine right so now good thing of doing this the good thing of doing this is that we can exploit irregular parallelism right by using this so for example here i don't know we have n elements and maybe half of these elements are equal to zero and the other half are not right so we need to operate on only n uh over two elements uh so we still have parallelism irregular parallelism because we are not applying the same computation on the same elements uh good thing is that we can't really use the machine but thing is that we are not using all the lanes right all the all the vector length we are only using half of it okay as i said this is what is called a predicated execution so the execution is predicated on a mask bit okay yeah another example with masking yeah you have a different example uh here if a i is greater or equal than bi then you do this you write a into c if not you write b into c so how would we program this so first thing to do is comparing a and b comparing the elements in a and b to get the v mask after that we store into c those elements of a for which the mask is equal to one then we complement the vector mask so that now uh the the elements so yeah we are complementing it that's clear uh so now we store only those elements of b into c those elements of b for which the mass is now equal to one right so uh here you have the uh first i mean the the the example the original value the first value of the uh vector mask if we compare a to b a zero is not greater or equal than uh b zero so that's why this is zero these two are equal that's why this is one okay yeah and now uh just to finish with this mask vector instructions so uh we would typically do something like this right so this is our um our vector pipeline um and and we start computing on c0 i mean we start calculating c0 c1 c2 and so on so c0 is whatever a0 plus b0 for example c1 is a1 plus b1 and so on and for each of these operations the mask has a value 0 1 and so observe that all the computations all the operations go through the pipeline and only when we have the actual value we check the vector mask and we decide if we write or not so in this particular case for c0 the mask is zero so we don't enable the writing it's kind of wasting cycles right so because that's the case it's better to use this or might be better to use this density time implementation in the density time implementation what we do is that we don't start the computation for some particular elements if the mask is zero so this is why you can see that we are only issuing the computation for element one element four five and seven and that's what we have here in the pipeline right so what is which one is better and what are the trade-offs so in principle this is more efficient right the problem with this one is that we need to scan the mask before we start so we are already adding some overhead before we actually start right so this makes the hardware more complicated more expensive but the good thing is that it will be more efficient okay okay something similar to this is what uh current gpus do to to be able to uh execute irregular computation so when we don't have all the same operation on on all the elements of the vector then that's what we have to do i don't know if i okay i might be doing something wrong with this yeah maybe we are going to have a break soon so we can check okay okay do you want to have a break now yes okay let's uh have a break for 10 minutes is that fine okay i think it's uh time to continue okay so we were here some issues strike and banking this is uh something that i might have mentioned already but now we are going to discuss a little bit more is what should be the relation between the stride that we have in the memory accesses and the number of banks or the yeah and the banking well we want them to be prime relative or relative prime what does it mean this means uh this means that the stride itself this number is prime relative to the number of banks if that's not the case then we will have bank conflicts and let let me give you an example let's let's think about the matrices right you you know that there are uh two possible ways that well at least two possible ways that we can map a matrix into memory memory is a linear array addresses are linear right and and matrices are are not linear they are two-dimensional so how do we store the rows and the columns of the matrix into the into this linear memory there are two possibilities at least first possibilities we store all the elements of row zero then on the elements of row one all the elements of row two and so on and the other possibilities all the elements of column zero all the elements of column one and so on okay well let's assume that it's row major which actually is uh the most uh typical one in uh in the programming languages like c for example uh let's assume that we have these two matrices a and b and we want to multiply them uh so as you can see this matrix a is four times 6 and this matrix b is 6 times 10. so we can multiply here and to perform the multiplication what do what we do is the dot product of a row of a and a column of b both are stored in the same memory in the same system so both are stored in row major order this means that in memory they are stored like 0 1 2 3 4 five six seven eight nine ten and so on and in this case is zero up to nine and after nine we have ten and after uh 19 20 and so on okay so this is how they are mapped in memory so so how they are stored in memory but now all memory is multi-bank it has multiple banks so i don't know how many banks for now but uh let's assume that the interleaving is uh the granularity of a world meaning that a0 is in bank zero a1 is in bank one a2 is in bank two and in this case b0 is in bank zero b1 is in bank one and what happens with b 10 is it in bank 10 well if we have 10 banks it will be in bank 10 i mean if we have 11 banks it will be in bank 10 right but if we have eight banks for example where is this element zero it will be in bank three right okay so this is how they are going to be mapped and now what's the problem here because as long as we access consecutive addresses perfectly fine right if we have a vector load that reads these six elements of the first row row zero of uh a then they are going to be in consecutive banks so we can read them the six of them at the same time and if we do the same for b it's perfectly fine as well but what happens if we want to perform the dot product of this row and this column and these guys are not in consecutive banks then it will depend on what's the relation between the stride which in this case is 10 as you see and the number of banks so if the total number of banks let's say is 10 then we are probably uh so no okay if the total number of banks is 10 we are perfectly fine for these six elements right because from a0 to a5 they are going to be mapped into or stored into consecutive banks but what happens with these ones we have only 10 banks right so if we have 10 banks it turns out that elements 0 10 20 30 and so on are all mapped into the same bank in bank zero and the problem with that is that we cannot start i mean we cannot perform the accesses in parallel concurrently but we will have to wait in order to read b10 we will have to wait until the read the access to b0 is completely finished right are we always going to have this sort of problem we will always have this problem as long as the stride and the number of banks are not relatively prime right so if or the number of banks that we have is 16 for example if it's 16 then that's fine we have enough banks to be able to access b0 and b10 at the same time and and maybe b20 also maps to a different bank but at some point there will be some stripe that will be a multiple of 16 and this means that we are not going to be able to access these two in parallel okay yeah so this is uh essentially what we what this slide is about and and what we are uh doing here is describing what bank conflicts are so bank conflicts are going to happen anytime that we want to access at the same time two data elements that are mapped into the same bank because we can only sustain one access per bank in principle of course might depend on how many parts we have right but for now let's assume that we can have uh only one port per bank okay uh is there any way of minimizing the potential backup bank conflicts for sure we will have bank conflicts when accessing in a conventional machine for sure we will have conflict bank conflicts when accessing this uh matrix v is there any way of minimizing these banks yeah there are possible ways first first way is more banks okay that should work but for sure it's more expensive it's not it's not a very a very good idea another possibility is to have a better data layout to match the access pattern we can play different tricks and actually if we have a chance to to have a gpu programming lecture i will give you uh at least one way of reducing the number of bank conflicts when programming gpus one possibility let me very briefly mention what i mean one possibility is to add some padding padding means that we are adding some you know the scattering some uh memory uh positions that we are not really using for example here instead of having a matrix of 6 times 10 what we could do is reserve a little bit more space for this matrix such that instead of having these uh and assuming that we have 10 banks remember that that was the original example here instead of having uh this guy here we would write this guy here in the next position and the position here this memory memory address will be empty so this is padding so what we are doing is inserting some memory positions that we are not really using to store the actual values such that we are able to shift these values in this case to the right to the next bank and this way we would be avoiding the bank conflict anyway that's uh one possibility we will we will talk about that one uh probably in more detail in a later lecture and then another possibility is to do a better mapping of others to bank and for example a randomized mapping there are for example hash functions that you can use observe that here the way that you obtain the i mean naive example like this one the way that you obtain what's the bank where one particular address is mapped to is just by looking at the um least significant bits of the address right so if you are element zero element one element two are mapped to bank zero bank one bank two and so on but we could randomize this in some way and say okay element zero is going to be in bank zero but the element one is going to be in bank seven and element seven is going to be in bank one why not we can do that right and we can do that using uh hash functions and this is something that was proposed uh in in first time in this paper by bob browning 1991 pseudo randomly interleaved memory um it might be good but i mean it also has its own drawbacks right because if you i mean for example from the programmer's perspective if you know how the data is mapped into the different banks you can reason about that but if you don't know what the hardware is actually doing then it's much more difficult right if there is a hash function in between that you don't know exactly where is mapping what particular address that you're working with then it might be more difficult for a programmer to to reason about all of these okay uh yeah so we are already uh finishing this first part about array and vector processors we have seen the difference between these two data parallelism in time and or in space and gpus are a prime example and we are going to talk about gpus because gpus as you will see have um you know have characteristics of things of or or things of both paradigms and not only about uh ray and vector processors but also about fine-grained multi-threading machines we we will also talk about that okay um one more thing is uh how to achieve this concurrent execution we with 70 processors we we operate on multiple data elements at the same time by using one single instruction but after saying that we have seen how a vector processor works and in a vector processor we have seen that there are some specialized units and each of them computes you know that performs the load the addition the multiplication and so on and and we said also that this uh these are like pipelines so when we need to for example execute one addition of element a zero plus v zero uh in reality what we are doing is using this uh addition unit in a pipeline manner right so we are not really operating at the same time right but conceptually it's at the same time for sure but that would be in the case that we only have one single functional unit it might turn out that we have more than one functional unit something like this we could have four addition units but we don't operate on only four elements we can operate on for example 28 elements so the way that we are going to schedule the computation here is by taking advantage of both time and space right so uh observe that in the very first cycle we can issue the addition for calculating c0123 in the next cycle for four five six seven and so on so this way in the end the program is exactly the same it's v at a plus v store the result in c the way that we really execute this on the hardware depends on the hardware if we have one single functional unit it will be something like this if we have multiple it can be something like this gpus these days are more similar to this thing as we will see and each of these functional units are somehow arranged in this in in you know this way you know so when we talk about the functional unit here the functional unit would be these uh four pipelines because the four of them are for the same type of instruction for example addition or for example multiplication and then each of these columns that we have here is called a lane it's called a vector lane and observe that inside in this vector lane what we have is registers right these are the um elements for example element 0 and element 4 are going to be here here we have element 1 and element 5 here we have element 2 and element 6. and this might be for example the multiplication unit that one is the addition unit and whenever we need to use either this or that we would be going to this part of the register file as you can see the entire register file is partitioned and part of the registers in the register file are assigned to uh each of the lanes so whenever we need to use this we will go to these registers we will grab the corresponding elements and we will start operating on them but from the point of view of the vector register as we have defined it in the beginning of the lecture the vector register would be the entire thing element 0 vector register is here element 1 of vector register is here element 2 of the vector register is here i hope that is clear ok how do we schedule the computation in a hardware similar to this one how do we scale computation we would be doing something like this for example here we have 32 elements in in each vector register but we only have eight lanes okay so this means that we can only start the computation for every for eight operations not for the 32 at the same time so for example if we have a load unit and we want to load 32 elements from memory first thing to do is starting the load for eight of these elements and in the cycle the next cycle for the next eight elements and in the next cycle for the next eight elements and that's uh what we're gonna do for all these units for example the multiply unit how many lanes do we have we only have eight lanes we only have eight lanes but each of these functional units that we have there are pipeline so every cycle we can start the multiplication for eight elements and in the next cycle for the next eight elements right okay we will see this slide uh again later yeah as you can see here in one single cycle for example this one we are operating on we are performing 24 operations we are operating on 24 input operands okay um yeah to do that uh we need to vectorize our code uh i mean in principle we were saying oh vectorizing code writing vector code is it's very uh it's difficult right and because it's difficult compilers have been trying to do this automatically for a long time so um what a compiler needs to do if we want to uh vectorize a code automatically is to do some loop dependence analysis and the compiler would be able to vectorize a loop for example the loop that that we have there on the top of the slide will be able to vectorize it if there are no loop carry dependencies meaning that this iteration two is completely independent of iteration one and if that's the case then we can generate vector code and we can uh perform these loads and these loads and these additions and these store operations at the same time you know in a sim d fashion um yeah this will be particularly possible when we have regular data level parallelism if if it's not regular or if we have loop carry dependencies these kind of things then the automatic vectorization won't be possible or won't be that easy and in the end we are always going to need to have the possibility of executing scalar operations recall what i was saying before about the cray one even though it was a very good vector machine it was also the fastest scalar machine at its time okay and we are already done well there are some examples in in real systems of cmd operations and what we are doing going to do next is very briefly talk about some of these examples or in particular one of these examples which are the mmx instructions that were introduced in the 90s in intel machines so essentially the idea here is to make use of the registers that are already available in the system in the x86 architecture make also use of the functional units that they are already in that computer and what we are going to do is uh using them to operate on let's say soup words so if these are 32 bits and these 32 bits originally were this register s0 what we can do is dividing this register into four chunks of eight bits each and in each of these eight bits groups of eight bits or bytes what we are going to do is storing uh different operands in this case elements a3 or a0 to a3 and in the other 32-bit register we store elements b 0 to b 3 and now we can do a pack at 8 which means is packed because the 4 elements are packed into the same register is 8 because we are operating on eight bit elements and is the addition right because we are doing this plus this and the result is stored in these bits of the register s2 so that's what mmx instructions do they um they use the registers that were already in the system 64-bit registers and in a 64-bit register you can either store one 64-bit element or two 32-bit elements or four 16-bit elements and so on and later what you can do is say okay now i'm going to execute one uh packed at eight or one pack at 16 and depending what's the size of the input we would be using one of these instructions you can take a look at the paper as well it might be a required reading i don't know that yet this is that we will decide that later um yeah and here you have a a very uh simple example actually i i think that you can take a look at yours uh look yourself about this at this example i mean these mmx instructions were included or were um yeah it started in the 90s in these intel cpus because you know in the 90s many multimedia applications started to be very very popular same as these days we have many for example ai applications and and we are starting to incorporate all these applications in in our computing systems in the 90s and something similar happened to multimedia applications and that's why uh intel engineers thought okay it might be a good idea to you know incorporate in our isa specific instructions that can operate on multimedia data right and that's what essentially mmx means is multimedia execution so that's where the acronym is coming from in the example uh here as you will see that actually the example is in the paper as well what you will see is a very uh um i mean simple way of doing image overlaying which essentially is like you know like in uh when you're watching the tv the weather forecast you you see the the forecast uh woman there and at the back you you will see the the map of zurich or switzerland or or wherever uh so then what they really have is a chroma and and later in the post processing they replace all the pixels that are green or blue or whatever they replace them with the actual in this case is uh uh this uh kind of blossom background but in the case of the weather forecast it will be like like the um you know the map or whatever so essentially what this computation does is going one by one through each of these pixels and say okay if the pixel is blue this means that it belongs to the background and if it's blue what i'm going to do is replacing in the output image i'm going to replace that pixel with the pixel from the map or the pixel from this blossom background and if it's not blue it's because it's the woman so if it's a woman then i simply copy the value of the pixel to the output and that's something that can be done in a cindy machine right why is that because the computation that we need to do here is exactly the same one for every single bit and yeah you can take a look at this by yourself you will see that first it's a kind of mask generated and then by using this mask we are able to remove the pixels from the background and replace them with the actual map or blossom background or whatever you want to put in the background okay so this is the mmx um extensions to um to the to the from from intel machines and now we are going to start with the gpus first thing here is uh gpus are i already mentioned that they are a combination of vector processors array processors plus fine-grained multi-threading very quickly uh remind you by why what fine-grained multi-threading means you probably used to um you know to see uh a pipeline like this right this can be a pipeline observe that here you have instruction fetch and and here you have an instruction register and then you have some control logic that reads this instruction and generates some signals or some micro operations to you know to tell all these the rest of the components you need to perform an addition or you need to store this in memory and so on right and and here we have our register file where we store the values the contents that we are operating on and after that we have a an alu and after that we have the memory access and so on right so each of these stages is typically in a different uh pipeline stage right and and within these stages what we have are pipeline registers i mean between these stages so if you think about very simple in order cpu what we are going to have in each of these pipeline stages are different instructions that are at every point in time and every cycle they are in a different phase of their execution you are for for every instruction you are either fetching it from memory or either decoding it or either uh using the alu and so on so in the pipeline we are able to achieve some sort of parallelism because if we have instruction level parallelism we can start the execution of an instruction before the previous extract instruction has committed has completely finished its execution right but the problem with one of these pipelines is that there are instructions that are dependent on each other recall that we mentioned that in the beginning of the lecture for example if you if you have a load if you have an access to memory uh when you reach to this stage of the memory access if it turns out that the data is not in the cache then you need to go to dram and this takes much longer right and because this stage takes much longer if the instruction that you have after that it's for example an addition that needs to use needs to use the data that you read from memory then you will have to stop you will have to wait until you bring the data from memory right and that's the problem that we have in one of these simple pipelines so one thing that we could do is say okay because you know these days we have a lot of parallel programs we want to have a machine that is able to run multiple threads at the same time maybe these threads belong to the same obligation or not maybe they are simply different processes uh in this example for example for example we have four so if we have four threads and we interleave the execution of instructions coming for from these four threads into this pipeline when we have to stop because uh for example we have a memory access the next instruction the instruction that is in the elu belongs to a different thread and if it belongs to a different thread then it doesn't need to stop right and probably the next instruction that will belong to the same thread is not yet issued is not yet executing and when we issue the instruction when we start executing this dependent instruction we have already finished with the memory access so that's essentially what the fine-grained multi-threading means and this is something that we can find in cpus but we can also find in gpus as as we will we are going to see okay let's start with the gpus they are uh cindy engines underneath that's the first thing to take into account they are a combination of vector and array processors but they are special in the sense that they are not programmed in the traditional cmd programming way with these vector instructions and so on they are programmed using threads such that we can write code for each of these threads individually but for sure it doesn't make sense to have like very different programs you know for each of these threads we will likely have the same program for all of them and that's what we call single program multiple data single program multiplied the data is a programming model as the name itself itself means right uh we are going to have a single program that is going to operate on multiple data elements the way that this program is later executed on the hardware will depend on the hardware for sure i could write a single program multiple data program and execute it on a multi-core cpu but what we are going to discuss here is how to execute this on a cmd machine which is a gpu underneath so that's why first thing to do is to distinguish between what we call the programming model and what we call the execution model because one thing is the hardware so the software and the other thing is the hardware so programming model refers to how the programmer expresses the code and there are different ways of doing this one way is writing a sequential program in c for example or in fortran you just write a sequential program another possibility is to write a data parallel program for example using vector instructions or data flow or a multi-thread program using openmp or p threads that's the programming model and for gpus we have cuda and we have opencl which are the programming model for gpus and then the execution model is how do we actually execute this program on the hardware that we have uh and this will depend on the actual hardware maybe it's an out of order processor maybe it's a vector processor maybe it's an array processor maybe it's a data flow processor etc so the execution model can be different from the programming model for example for neumann model from neumann programming model can be implemented on an out of order processor you are familiar with or out of order processors right in out of order processors we don't need to follow the exact sequence of instructions at this is defined in the sequential program but in the sequential program there is a sequential execution right but in the end the way that these instructions are executed on the hardware depends on the hardware and for example in another for the processor if two instructions that are one after the other are completely independent they can be issued at the same time or almost at the same time and they can be executed in parallel so observe that we are doing concurrent execution execution while our program is completely sequential you understand the difference right [Music] and in gpus what we are going to do is exactly the other way around we are going to write a program for each of these hundreds thousands or millions of threads that we that we want to run on the gpu but the way that this program for each of these threads is going to be executed is as if the gpu was a cindy processor so we write pro we write code for individual threads and later the hardware is going to collect these threads and put them together in cmd lanes that ran next to each other and these threads in groups of 32 threads for example which is the vector lane vector length in nvidia gpus will execute in parallel we will see uh more example we will see examples yeah in in with some detail here okay how can you exploit the parallelism here do we have parallelism in this code yes right we have parallelism in this code we have a for loop where each of the iterations is independent so it's something like this this is a scalar sequential code that we are writing there but now we can see how we are going to execute this on different machines what happens if for machine is sequential or if it's data parallel like a cmd machine or if it's multi-thread like a mindy or spmd so first of all sequential machine cz so well okay we we will execute it like that right but depending on what the machine is indeed it might be a in order pipeline processor or it might be an out of order execution processor right so if it's an out of order execution processor we can have execute we can have independent instructions being executed as soon as they are ready and we don't really need to follow this sequence of instructions that we have written in our program right so in in to some extent what the out of order processor is doing is kind of loop on rolling the out of order processor for example could see that this load is completely independent of those two loads right so it could issue these instructions at the same time or almost at the same time and this load and that load and that load might be might be service almost at the same time why is that because the out of order processor operates that way so it's a it's a way of extracting instruction level parallelism from the code and execute these instructions in parallel on the hardware and we could also i mean for example use a superscalar or a vl iw machine as well right for example these instructions here and those instructions here are independent we could pack them in the same very long instruction work okay that's for csd now for cindy well the realization here is that each iteration is independent of each other so if they are independent we can execute them in parallel right we can have something like this and we can pack these two loads into this one single instruction which is a vector instruction and uh and and that's all vectorized code and then these two loads is another vector instruction and then these two ad is a vector add and then these two is a vector store right and we can do that because even though our program was sequential but we can have for example a compiler that detects that these iterations are independent and and then this compiler generates code for a cmd processor for a cmd machine and the third possibility is that because the each iteration is independent we assign each of the iterations to a different thread observe the difference one thing is talking about lanes in ascending machine another thing is talking about threats in a multi-threaded machine right you see the difference right uh the the the important difference here is and if the cindy machine if we want the 16 guys or the 32 guys run really at the same time they really need to be doing exactly the same thing right because it's in the single instruction but if it's a multi-threaded machine we have multiple programs right we have multiple threads and each of them could potentially be doing completely different things but in this example no in this example they are doing exactly the same thing and this is what we call single program multiple data meaning that we have multiple threads these multiple threads are going to run the same program on multiple data and this is what we do in gpus this can be executed on a md machine this could be executed in a multi-core machine but we are going to execute this on a cmd machine in on a gpu okay yeah uh related concept is this single instruction multiple thread it's uh more or less like the same thing it's it's more like nvidia terminology so a gpu is a cindy or simply machine but it's not programmed using sim the instructions we just program it using threads and then we will have the hardware later that will take these threads and put them together in a cmd unit and execute them all at the same time so these groups these uh sorry threads that are grouped together are called a warp or a wavefront in amd terminology at the very end of the presentation you will find kind of uh table that i prepare for you know this terminology because there there is something like this the official terminology of the of the lectures which is let's say like uh vendor agnostic and then we have the uh terminology for the different vendors right that's why we included that table at the at the very end for you to clarify um any possible confusion okay so a warp is essentially a sim the operation formed by the hardware so instead of having on a gpu instead of having one vector instruction for these two loads what we have is one warp that is going to execute these two loads or in reality 32 loads at the same time because a warp consists of 32 threads but in the end the way that they execute on this on the hardware is exactly the same as a cindy machine because they share the same pc for all these threads that belong to the same work by pc i mean the program counter right which is the address of the instruction to be executed the address in memory of the instruction to be executed okay and then after warp zero is done with the loads then warp one goes to the next load and then it goes to the addition and then it goes to the store and this is how the program is going to execute okay so graphics processing units are cmd processors not exposed to the programmer cindy versus simply i think that we have already you know talked about this you know what is cindy cmt is essentially the same as single program multiple data as we have already described and there are two major advantages of this type of machine with respect to conventional cnd processors vector or array processors and are these two things that we are going to discuss um separately so the first thing is uh each thread can be treated separately we can execute each thread independently you know so it is possible to write code for each thread individually i told you before okay we are going to have one single program for uh all the threads to operate on the entire amount of data that we have but if we want to have threats diverging that's possible as well i could say in the very beginning of my program i could write if you are threat zero do this if you are thread one do that that's possible i can run that program on a gpu and the gpu internally is just like a cindy processor of course that's not going to be very efficient right because if you have a cd processor and every time that you launch a new instruction only one of the i mean you're operating only on a single data element then you are wasting vlan minus one lanes right and you don't want to do that but you can do it if you want so um yeah let's let's go back to the to the example here assume that the warp consists of 32 threads so if you have 32 000 iterations and uh you are going to assign one of these iterations to each thread you're going to need 1024 depending on on what decay means warps now we are going to have warps executing these instructions these load instructions how many were 1000 warps right and how are we going to execute these warps do we go first all the instructions for warp 0 and then all instructions for word one and then all instructions for warp two can we do that that could be one possibility right but the problem if we do that is that this instruction here is depending on these and depending on these so we cannot start the execution on this instruction until these two have completed right so what do we do in the meantime what we can do in the meantime is scheduling instructions from different warps recall that we have 1000 warps right so we will have to execute each of these four instructions 1000 times for the 1000 warps right so instead of first do all the instructions for warp zero and then all the instructions for one word one and so on what we can do is interleaving instructions from different warps because they are completely independent of each other right there there is no dependence across warps there is dependence across the instructions of the same warp so that's what we do that's why we call we talk about fine grain multi-threading of warps and that's why i briefly reminded you what uh multi-threading fine-grain multi-threading means induced slides that you can take a closer look by yourself so now if we have a you know or cmd units there or cmd processor where we want to execute these wires we may want to first issue the this instruction for warp one and then this instruction for warp 20 obviously if we do this is because we already executed this and this for warp 20 right otherwise we could be doing like uh crazy scheduling and we really want to schedule things in the right way so and after that we might want to you know schedule this instruction for warp 2 and then this instruction for warp 26 okay okay so yeah so this is uh essentially how the different warps are scheduled onto the cmd pipeline so for example here you see next one is warp 7 warp 7 but which instruction okay for example instruction zero so we start the execution of instruction zero for warp saving and right after that we say okay what instructions are ready and it turns out that instruction one for warp eight is ready because the operands are ready right and if the opponents are ready that can be the next instructions to the schedule the next instructions to issue make sense okay and here we have a you know nicer picture of the internals of the gpu and here you can see this is the cmd execution unit we can talk about scalar pipelines but each of these scalar pipelines is only a cindy lane a vector lane so this is one length this is another lane this is another link and thread zero will execute here thread one thread two thread four and so on okay this is very simplified this is a little bit more um a little bit more detail and and you can see the entire pipeline here this is instruction fetch decode here we have the partition register file where we have our vector registers right and then we have the alu that can execute addition multiplication so on and then we have a data cache and then sometimes we will have a miss right if we have a miss what happens we have to go to the next level of cache l2 gpus typically have two levels of cache l1 and l2 so if it's not in l1 you will have to go to l2 which is shared this is one single cmd pipeline you will typically have multiple of these so each of them has an individual or yeah private d cash but then they have a shared l2 cache and now you go to l2 and you also me so you have to go to dram and now you know how much time this takes hundreds of cycles maybe 500 cycles which is a lot right so what can you do in the meantime because this memory access was for for example for work one what can you do in the meantime are you going to install the pipeline and keep this completely idle while you are waiting for the values for warp one no you can execute other instructions for example arithmetic instructions which belong to the execution for other warps and you can be using the pipeline in the meantime so those are the benefits of this fine grain multi threading with fine grain multi threading we are able to tolerate long latencies for example like access to memory because while we are accessing to memory uh for one warp we can be executing instructions for other warps so this is what is called the latency hiding mechanism it works it works well for sure but it also depends on uh on the workload itself right if the workload has a lot of data accesses and very little amount of computation in the end you will have to wait right and that's actually the main bottleneck i would say in real gpus okay you remember this slide right now we don't talk about cmd vector or array processors now we talk about gpus and now uh this ad instruction is not about a vector ad is the ad instruction for one warp for example warp zero and it turns out that warp series is composed by 32 threads so depending on how many of these functional units do i have depending on how many of these lanes do i have i will schedule them it will schedule the different operations in a different way if i have one single lane then i need to first c0 then compute c1 then compute c2 then compute c3 and this is what i would be doing for the 32 threads that belong to this warp if i have four of these lanes then i can operate on four of them at the same time you also remember this one right now we we still call them lanes and on each of these lanes we run different threads you see so we write our code for 1 million threads and and each of these threads has a thread id and the con i mean the values that thread 0 needs to operate on will be here in these registers the values for thread 1 will be here and so on and you also recall this slide right we were talking about vector instructions we don't talk about vector instructions now we talk about warps and now our warps have contained or are composed by 32 threads and we want to execute for example a load or a multiply or an addition for these 32 threads but we only have eight lanes so the way we schedule the 32 threads of the warp into the eight lanes is by doing it in four cycles but observe that conceptually is exactly the same as as we have seen for vector processors now instead of talking about vector instructions we talk about instruction issue for warps okay any questions here okay um yeah so this yeah so this is like uh um yeah a little bit more of it right so how do we distribute the computation here we have uh uh 16 data elements and for these 16 data elements we are going to uh i mean to use 16 threads in this case we assume in this example four threads per warp so what the hardware does we write our program right we write our program with 16 threads and we tell to each of these threads you need to operate on this on this on that and then what the hardware will do is say okay i have 16 threads and i'm going to uh you know group them in groups of four and each of these four is going to be a warp and when we execute the instructions corresponding to these four threads are going i'm going to issue i'm going to launch these instructions for for the entire warp at the same time right so that's more or less what we have already seen uh in the past in the previous slides so if we want to write the gpu code we um we we will still be using the cpu we will still be writing serial code that that's for sure so gpus essentially are co-processors we have gpusing or laptops desktop machines in our cell phones but uh you know we still have a cpu and that's because we we still need it right because there are there has a lot of uh workloads that can be run in parallel uh in parallel machines like gpus but but still there are uh serial or sequential parts of the code that we need to execute on a regular cpu so what we do in these cases is we write our program in somehow this way we will have some serial code that runs on the cpu and at some point we reach the parallel part of the execution and what we do is calling uh the gpu by by this is this is kind of similar to the actual syntax in in cuda so here what we're essentially doing is calling a function that is going to run on the gpu and at some point this function will finish the execution and then we have more sequential code and then we have might have another function these functions executed in the gpu or device observed that we call it here device are called kernels and for each of these kernels we launch a number of threads and the way that these threads are organized from the software perspective from the programmer's perspective is in blocks you see so these are blocks of threads in total we might have 400 blocks 400 threads but what we are launching here is for example four threads of 100 so sorry four blocks of 100 threads each right observe one thing this block is a software concept is independent of the warp but we will see what's how how do they uh connect to each other we're gonna see that in the next uh yeah in in two or three slides and this is a very very simple um gpu program cuda code this is the cpu code here observe that this element-wise edition you're pretty familiar with that after yesterday and and here is how do we implement these uh for a gpu so what we write is essentially the same as we have there right the body of the loop is this addition and we also have the addition here right i mean the only difference is that okay we are separating it in some some way you know so we this this is in in memory we go to memory and we store uh in this variable this this will be a stored in a register actually we go to memory store in a register and then we go take the contents of the two registers at them and store in the output position and observe that these memory accesses are indexed by this tid and this tid is computed here is the threat index inside the block block index times the dimension of the block you see right remember in the previous slide i was saying okay you are the programmer and you're going to write the program to launch four threads four blocks of 100 frets each right so this is going to be you have 100 threads per block so this is going to be a number between 0 and 99 and then you have 4 blocks right so this is going to be a number between 0 and 3 and the size of the block this block dim is 100 and this way i can access the 400 elements that these two arrays a and b have this is how we write the program but observe that the program these are not vector instructions right these are scalar instructions right and here in the very beginning i could write if tid equals zero do this if t id equal one do that and i could have completely different programs i mean it's actually the same program it's one single program but completely different execution path for the different threads you see the difference already right between cuda programming and gpu programming this is a little bit easier i would say it's a little easier to write it's difficult equally difficult to optimize that's for sure okay uh here you have the same thing but uh here with a little bit more detail you can take a look at this by yourself so uh this part here is the kernel is the gpu function and this part here is the thing that is run on the cpu because the gpu is a co-processor we start the execution on the cpu and when we reach this kernel call we call the gpu and execute this onto the gpu okay and now uh we will be finishing soon but let's clarify something that is uh very important here what's the difference between or how to relate blocks with warps because observe i started the the explanations talking about warps and talking about how these warps relate to the cindy lanes that we have inside the inside the gpu right but after that i started talking about the software and i said oh we are going to write software for threads and the programmer organizes or arranges these threads into blocks and now the thing is okay if when if i'm writing code for threads and blocks how do these mapped to the warps right well actually it's quite simple essentially what we do when we write or what the hardware does when uh receives the code that needs to execute for a block of threads this is one so-called streaming multi-processor in nvidia architecture but essentially for us this is like a cmd pipeline okay or a let's call it cindy compute unit if we want so when when this cmd pipeline receives the program for one particular block what it does is dividing the block into works so if the programmer said oh each of these blocks is going to have 100 threads what the hardware does is dividing these hundred threads into groups of 32 threads each of which will be called a warp or a cmd unit of 32 threads so now you might be thinking oh how do i divide 100 stress threads in groups of 32 well obviously 100 is not a multiple of 32 right so the last warp will have some unused threats or unused lanes right so that's uh like one of the first uh hints programming hints for for people who who learn to program gpus is you always use threads that are multiple you always use blocks with a number of threads that is a multiple of 32 because you really want to use all the lanes for the warps for i mean that are available for for each of the warps right and you can take a look at the and the picture here and see how i mean the different elements that we have inside this streaming multi-processor and recall what we were saying before gpus are a mix of vector array processors and finding fine-grained multi-threading processors right vector processors why is that because each of these lanes is a pipeline and uh we can have um i mean yeah so i was saying vector right so in vectors in vector processors we have different types of units so for example we have these sps that execute integer and floating point operations like additions multiplications and so on and here we have these other units that are specialized for load and stores and this we and then we have this sfu which means a special function unit that can execute other type of operations more complex operations like transcendental functions trigonometric operations for example so you see different types of units but then each of these units might might also be able to x or is also able to execute different instructions so to some extent they are like array processors and like vector processors and also they are finding multi-threaded because uh here we have the warp scheduler and the dispatch unit that are continually continuously every cycle go into the instruction cache and say okay this instruction for warp zero let's start executing it this instruction for warp one let's start executing it and so on okay good yeah so word-based cindy versus traditional cmd this is just a comparison between uh traditional cindy and warp base cindy i think that we have more or less going over all of this you can read the the slide more carefully and also remind you as well what the spmd means a single procedure or program multiple data we are writing one single program that is what all these threads are going to run but each of them is are going to operate on on different um data elements uh at some point they they can also synchronize this is something that we will probably talk about when uh when we continue with gpu programming and yeah essentially multiple instructional streams that execute the same program and operate on different data and the second thing just very very briefly before we finish is uh another advantage of the simti architecture is that we can group or at least potentially we could groups group threats into warps flexibly this is something more it's more like a research proposal rather than an actual implementation on real hardware but it's it comes from a very reasonable concern which is uh that you don't want to i mean you you in principle you want to avoid you already know why you want to avoid like you know separate different control flow paths for each of the different threads even though you can write that you can write your program in that way but you you in principle want to avoid that and why is that because in the end they are only like cmd pipelines and when you reach the you know a divergence point like here this is a branch and some of the threads are going to be uh through path a others through path b and when this happens what will happen is that you are issuing instructions for the same warp twice right instructions for paths a instructions for for path b how do you really implement this it's a cmd machine it's a simply as a vector or array processor right so it has a mask so in your mask [Applause] when going through path a this is one zero one one zero zero zero one and then you complement and execute path b and then this is zero one zero zero one one one zero in the vector mask right problem with this i mean the good thing is that it works the bad thing is that we are wasting some of the lanes right so we are not exploiting all the throughput that the gpu can give us okay so what was proposed by uh yeah some paper you will see the title of the paper somewhere is to dynamically merge threads such that we have threads that were originally going to belong to different warps but the hardware says oh i have divergence here i have some threads here that are not going to execute but i also have another warp that has some threads that can merge with the other ones right could execute at the same time so what you do is that you compose what the hardware the wire scheduler does is that composes like a artificial warp in this case and you see that this warp z contains threads that originally belong to warp x and warp w and this way you can have a more efficient execution that's uh you have a nice example here that this is the uh title of the paper dynamic word formation and scheduling for efficient gpu control flow and and here you can see a very uh nice example pretty simple as well and this is another example uh we might assign you this for reading you will you will have a chance to to to read this in detail but let me very briefly introduce we have already mentioned like two main bottlenecks in in gpus one of these is the divergence that we have just discussed the other thing is the access to memory for example very low latency operations and in this paper they try to deal with both so with respect to so this is like the typical uh default scheduling that we have in gpus is round robin so you first schedule instruction for warp zero then for warp one then for warp two and so on and even though you can achieve some uh latency hiding not so much if you if you see uh this timeline here all threads all warps start computing arithmetic operations and at some point they need to access memory and this takes a lot of time a lot of cycles right and and you can not resume the computation in inside the pipeline until you have obtained all the data from from dram and yeah of course there is some latency hiding here right because we are somehow overlapping a little bit of computation with a little bit of data access but not that much why is that because there are many accesses to memory here so this is i mean it's made the main bottleneck so the what in this paper they propose is to have like larger warps and then schedule a kind of subwarps that are dynamically formed for example this is the entire warp this is the mask and instead of you know scheduling these four and then these four what we do is ignoring those for which the mask is zero and we only schedule those for which the mask is one so if we do that i mean this is for uh this is the way of uh reducing what uh no this is for branch divergence it's not for the memory access but you see more or less how it works it's kind of similar to the previous technique and for the uh long latency operations that we do is like a two level work scheduling they organize the warps into smaller groups such that uh they you know they first schedule round robin inside a group and then schedule ryan robbing across groups and this way as you can see it's quite clear you can achieve more overlapping right than there you can achieve more overlapping and save execution cycles in the end and this is the paper and because it's more than 4 p.m i think that we are going to finish here there are a few more slides probably if i have the chance to give you a gpu programming lecture another day we will probably start with these ones very quickly um yeah and yeah let me know if there are any other questions or maybe yeah we can finish and ask questions in moodle or or whenever you want okay thank you very much for your attention you 
uBfiX2nIR3c,27,complex instruction set computer (cisc) introduction and characteristics,2018-02-26T14:50:18Z,cisc architecture | COA,https://i.ytimg.com/vi/uBfiX2nIR3c/hqdefault.jpg,Education 4u,PT5M54S,false,58770,504,26,0,16,our students let us continue with the next object in the subject computer organization and architecture is disk processor in the previous video I explained about the RISC processor so now coming to the sisk assist is nothing but it is a complex instruction set computer so whereas in the previous video I said the risk is nothing but it is a reduced instruction set computer so the name itself is saying that reduced instruction set will be there whereas in here the complex instruction set will be there so the main the sisk is designed to minimize the minimize the number of instructions per program so the main use of this processor is to design the number of instructions per program and ignoring the number of cycles ignoring the number of cycles per instructions whereas in rest one instruction will execute in one clock cycle okay so here in a complex instruction set it is designed to minimize the number of instructions per program and also it ignores the number of cycles per instruction so the most thus this processor is mostly used in the desktop desktop computers or laptop computers so whereas in the previously the risk is used in tablets smartphones so for those devices we will they will use a RISC processor for the sis processor this is processor village is using a desktop or laptop computers so here the compiler has to do very little work to translate the high-level language to machine code because the length of the code is relatively short in the SIS whereas in the risk the length of the code is more so here the length of the code is relatively short so a very little ram is required to store instruction so here a very little ram is required is required to store instructions why it requires only the small little Ram because here the length of the code is relatively short so let us see the architecture of the [Music] sister so it is desire designed to decrease the memory cost because more storage needed here in large programs resulting higher memory cost so to resolve these the number of instructions per program can be reduced by ambering the number of operations in a single instruction so here they are just adding a number of installations in the single instructions to resolve the problem like number of instructions per program that can be reduced so the number of instructions per program can be minimized or reduced by just embedding the number of operations in a single instruction so that is the main advantage in this is processing so let us see this here insist processing you are having the control unit and instruction and data path so here the instruction and data path will be placed in one unit and here it's having the microprogrammed control unit micro program control unit and here the cache will be their cache and main memory okay so this is the architecture of is this processor now let us see what are the characteristics that are present in the resist processor characteristics this is processor characteristics so the first characteristics variety of addressing modes will be used in Syst processor variety of addressing modes and here large number of instructions will be present and that will be minimized by just embedding the number of operations in a single instruction so large number of instructions is required and variable length of instruction formats will be there variable length of instruction format instruction format will be variable length and next several cycles several cycle several cycles may be required several cycles may be required to execute one instruction so to execute one instructions it required several cycles and also I already said this is processor they are ignoring the number of cycles so you can use several cycles to execute one instructions and next instruction decoding logic is complex here instruction decoding logic is complex ok so these are the instruct our Pacific's of this sis processor thank you 
C8hYhBrjGmo,27,"Computer organization lectures for GATE, Complete Computer Organization lecture series. Computer Architecture and Organization for GATE, Computer Organization tutorial. 

. _______________________________________

1. Digital logic design tutorial (DLD Tutorial):

https://www.youtube.com/watch?v=baF-cxSl8TA&list=PL4hV_Krcqz_J4K8dEFsqzm3zJIgzFF9MQ&index=2&t=16s

2. Computer Organization Tutorial: 

https://www.youtube.com/watch?v=ayJBTJLt4cQ&list=PL4hV_Krcqz_JaY3JmbrDgy5tipHrOmGBW&index=2&t=6s

3. Computer Networks Tutorial:

https://www.youtube.com/watch?v=yTnAB4IMU8g&list=PL4hV_Krcqz_KLIzfuShdbDiAdyrhJbwF6&index=2&t=1s

4. Operating Systems Tutorial:

https://www.youtube.com/watch?v=Fd9ucp6_hho&list=PL4hV_Krcqz_KyOBQEm6825QQJ6m2c1JRY&index=2&t=1s

5. Database Tutorial (DBMS Tutorial) | SQL Tutorial

https://www.youtube.com/watch?v=mwlKkUmhLeU&list=PL4hV_Krcqz_IbPUf3mAJbje5XQdPORrYi&index=2&t=0s

6. C programming Tutorial:

https://www.youtube.com/watch?v=zmLv-IjU000&list=PL4hV_Krcqz_JhUAojsTolbrTarJPrtzvM&index=2&t=1s

7. Algorithms Tutorial (DAA Tutorial):

https://www.youtube.com/watch?v=l51gzYCnA8k&list=PL4hV_Krcqz_L_qeClFzxcr9sJCIF5MUGe&index=2&t=10s

8. Data Structures Tutorial | DS Tutorial:

https://www.youtube.com/watch?v=56OA2C9Uxmc&list=PL4hV_Krcqz_KzWhCr3zJj3_z4wkSLSP7v&index=2&t=0s

9. Problem solving using Data structures and Algorithms Tutorial:

https://www.youtube.com/watch?v=wwWGOkYk500&list=PL4hV_Krcqz_LqMkNHswMN868hL1Klj2Li&index=2&t=1s

10. Probability Tutorial | Permutation and Combination Tutorial:

https://www.youtube.com/watch?v=6DeqpQFUPpI&list=PL4hV_Krcqz_Kp449S66_fmbaaVakWAAjc&index=2&t=7s

11. Interview Puzzles Tutorial:

https://www.youtube.com/watch?v=eOUYaaSkwq4&list=PL4hV_Krcqz_JyjXz-8DDzz3XgdkZiMF3F&index=2&t=258s .
.
12. Aptitude Video lectures for Placements | GATE | SSC | Bank PO | Quantitative Aptitude lectures

https://www.youtube.com/watch?v=oiPb-qAWME0&list=PL4hV_Krcqz_KjZl0UzQGnXTPC4xsbA0fS",2018-12-16T11:44:07Z,Pipelining-9 | Operand forwarding example | GATE 2007,https://i.ytimg.com/vi/C8hYhBrjGmo/hqdefault.jpg,GATE Video Lectures - Success GATEway,PT5M30S,false,4596,67,3,0,4,now come to example of a print forwarding it is a gate 2007 question consider a pipeline processor with following stages I am I do F execute and write a bit the IAF I do F that is instruction decode and operand fetch and WB takes stage takes one cloth cycles each to complete the operation the number of clock cycles for yet state depend on the instruction the add and subtract takes one clock cycle and multiplication takes three clock cycles in ax stage operand forwarding is used what is the number of clock cycles taken to complete the following sequence of instruction add R 2 R 1 R 0 these are the instruction this way maymays question course all carajo more than 4 previous year equation same type shift data changes but some which may agree a Volta oprand forwarding nahi hain toh case a la gota operate forwarding head next by default up operate forwarding use courage agar instruction may clearly mention a minimum number of cycles but so much may know up Johnson Akana agar may spoke I 1 I 2 and I 3 name Baldy Tom no instruction fetch a clock instruction decode be a clock and execute takes addition for one clock ok it is a in last piece right back but the point that is I I won I won I won clear No yeah I even estate now it is fetch for I to fetch will take one o'clock decode start with certain it next instruction decode next multi execute yeah hop in a previous of no execution complete or chuka we can he started execute but multiplication takes three Co got a point no and right back we'll take one more clock i three what the answer a cannot fetch complete ho jayega decode village start but the code topic complete the iboga jockey mujras ultima he mill data is some make you care emotional there is a there is a so now earlier okay okay okay up they co are to say yay con see dependency that is read after write read after write mclubbe via happy book form a mooch a date amelia a gary data dependency hair joke eat a so yaha say before say Mooji decode my data millennia or a monopoly execution is start cardia no option now decode may you hopper be same dependency but a point a very same dependency hey Toby Mirko wait turnip ha n a saving would covet karna hoga because hamara previous instruction execution complete iya he nahi Jacopo execution complete to up execution chillÃ¡n incentive no and APCO result a rotor table here ah hoo job data dependency hair Orozco home soldier a praying for wordings next who's came up it is a subtraction to execution eight o'clock Liga or right back to eight o'clock orlag 88 doe teen five six seven eight eight number of clocks they tease that's why answer is B but so much main a bianca at night Watson up sheep ad under K up Co right back complete Carnegie vet Karenina here fetch kar sakte Oh operating from execute to this register to this register clear that is called operand for body at a point and most of the gate questions Jewish pancake was simple EC way miss all virgin okay you 
