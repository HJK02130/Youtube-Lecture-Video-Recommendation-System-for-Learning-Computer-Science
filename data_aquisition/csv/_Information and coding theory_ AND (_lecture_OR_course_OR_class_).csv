id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
sM1Iwt7Lfrw,27,"Entropy for a source is defined as the average amount of information per symbol. It is also a measure of uncertainty of the source. In this lecture, we continue with our coverage of discrete-time information sources and present the following topics:
Define the source entropy
Study the entropy of the binary source
Prove that: 0 ≤ H(S) ≤ log2M
. Haykin, Communication Systems, 4th Edition, John Wiley & Sons, 2001 (Chapters 9 and 10)
T. Cover and J. Thomas, Elements of Information Theory, John Wiley & Sons, 2006.
G. A. Jones and J. M. Jones, “Information and Coding Theory,” Springer ISBN 1-85233-622-6, 3rd Edition.
R. W. Hamming, Coding and Information Theory, 2nd Ed., Prentice-Hall Inc.,1986
R. B. Wells, Applied Coding & Information Theory for Engineers, Prentice Hall, NJ 1999.
R. E. Ziemer and W. H. Tranter, Principles of Communications: Systems, Modulation, and Noise, 6th Edition, Hoboken, NJ : Wiley, 2009.
S. M. Moser and P.-N. Chen, A Student’s Guide to Coding and Information Theory, Cambridge, 2012.
Information Theory Class Notes: at http://apal.naist.jp/~kaji/lecture/",2021-04-02T17:56:54Z,L6. Properties of Source Entropy,https://i.ytimg.com/vi/sM1Iwt7Lfrw/hqdefault.jpg,Wael Hashlamoun,PT22M26S,false,65,2,0,0,0,welcome everyone this lecture is about the source entropy this is a brief outline of the lecture first we define the source entropy then we study the entropy of the binary source and finally we prove the important property that the entropy is non-negative and falls between 0 and log base 2 of m where m is the number of symbols in the source alphabet we start as we have started also in previous lectures with our model of a discrete time information source the source under consideration is a discrete time source the number of symbols in the source is finite and countable the source is discrete in time in the sense that it produces one symbol out of the impossible symbols every time unit the output of the source in the first time slot is a random variable x1 the output in the second time slot is another random variable x2 and so on over the source we define a probability distribution p of s equal s sub m equal p sub m that is there is an associated probability for each symbol s i produced by the source s1 is produced with probability p1 s2 is produced with probability p2 and s sub m is produced with probability p sub m in the previous lecture we defined the information content of each symbol in the alphabet using this relationship i of s sub m equal log base 2 1 over p sub m in bits using this definition of information we can say that the information content of symbol s1 is log base 2 1 over p1 that of s2 is log base 2 1 over p2 and that of s sub m is log base 2 1 over p sub m so these are the individual information produced by each one of the symbols s i now we define the average information pair source symbol symbol s1 occurs with probability p1 and carries an amount of information log base 2 1 over p1 symbol s2 occurs with probability p2 and carries an amount of information log base 2 1 over p2 and so on from this we conclude that the information content of each symbol is a random variable and this is the associated probability with this random variable uh this random variable is a discrete one and therefore we can compute the expected value or the average amount of information of this random variable as a product of the probability of occurrence times the amount of information for each symbol this is expressed mathematically as the expected value of the random variable this is also called the entropy of the source and it equals the product of p i times log base 2 1 over p i and the summation is carried out over all symbols in the source so the entropy of the source is given by this formula p i log base 2 1 over p i and the units are in bits pair symbol we have two interpretations for the entropy the first entropy is a measure of the information content in the source as we can see from here it's the expected value of the amount of information in the source and also it's a measure of the uncertainty in the source since information is related to the uncertainty in the source through this relationship one over p i as the probability decreases the uncertainty increases and the amount of information increases so these two properties or these two interpretations are related closely with this definition of the information for each symbol symbols with higher probability have less uncertainty symbols with small probabilities have higher amount of uncertainty that is they carry larger amount of information in the special case when the symbols in the source are equally likely the entropy is given as 1 over m log base 2 of m and this equals log base 2 of n now we consider the binary source consider a random binary source s with probability assignment over its symbols as probability that s equals one equal p probability that s equals to zero is one minus p the entropy of the source is h of p equal minus p which is the probability of 1 log base 2 p minus the probability of a 0 which is 1 minus p log base 2 1 minus p in bit sphere symbol in this figure we plot the entropy function h of p as a function of p which is the probability of one we see that when p equal to zero the entropy is zero when p is one the entropy is zero and when p equals one half the entropy is maximum we also notice that the entropy function is concave upward and attains its maximum value when the symbols are equally probable when p equal to zero this means that symbol one occurs with probability zero in this case zero occurs with probability one at this end one of the symbols occurs with probability one and therefore the source entropy is zero when p equals one also this symbol this symbol occurs with probability one and therefore the entropy is zero h of p equals to zero at p equals to zero and at p equal one and this is emphasized by the fact that one of the symbols occurs with probability one and when one of the symbols occurs with probability one this means that there is no uncertainty in the source which means that the entropy of the source is zero h of p is maximum which equals to one this is the maximum value when p equals one half that is symbols are equally probable this means that when p is one half the source has maximum amount of uncertainty now we prove this lemma for an m area information source the entropy h of s is non-negative and falls between 0 and log base 2 of m m is the number of symbols in the source alphabet the minimum value of the source which is 0 occurs when one of the symbols occurs with probability 1 and the rest with probability 0. the maximum value is attained when symbols are equally probable and the maximum value is given by log base 2 of m we can easily prove that the minimum value of the entropy equals to zero and this is attained when one of the symbols occurs with probability one in this case we can make use of this limit limit as p equals to 0 of p log p which also equals limit as p approaches 1 of p log p and this is equals to zero as we know the entropy of the source is given as minus summation of p i log base 2 of p i when one of the symbols is 1 so we have 1 log base 2 of 1 equal to zero for one symbol and for the rest of the symbols we have zero log base two of zero which is also equals to zero next we show that the entropy is maximum when source probabilities are equal that is when p i equal 1 over m actually we will approve this in two steps but before doing that we introduce what is called the relative entropy d of x and y this is a distance measure between the distribution x and the distribution y uh the divergence or distance between distribution x and distribution y it's defined as d of x and y the distance or the divergence between distribution x and distribution y equals summation from j equal 1 to m p sub j log p sub j over q sub j p sub j is the distribution under consideration for the random variable x q sub j is a reference distribution with probabilities q 1 q q2 up to q sub m of course x is a random variable with the distributions p sub j this is our given probability mass function y is a reference random variable with distribution q sub j first we prove that the divergence d between distributions x and y is non-negative larger than or equal to zero we can rewrite this divergence as d of x and y equal summation p sub j log p sub j over q sub j we can reverse the order for q sub j and p sub j which equals minus p sub j log q we take the minus sign to the opposite side minus d of x and y equal summation p sub j log q sub j divided by p sub j we also know this fact about the logarithmic function that log x is always less than or equal to x minus one this is the curve y equal len x y equal log x and this curve always falls below this straight line y equal x minus 1. this means that this summation p sub j log q sub j over p sub j we can replace this logarithmic term with this quantity which is qg divided by p sub j minus one so this quantity is smaller than or equal to this straight line now we can we can simplify this term as less than or equal to q sub j this term cancels out with this term so we are left with q sub j minus p sub j minus p sub j and we also know that the summation of all probabilities equals to 1 so this summation is 1 this summation is also 1 which means that d of x and y is greater than we can reverse this inequality and therefore we have d of x and y greater than or equal to zero therefore the relative entropy or the divergence between x and y is an a negative quantity of course when these probabilities are equal that is when q sub j equal p sub j this ratio is one and so log one equal to zero this means that the relative entropy between distribution x and distribution y is zero only when p1 equals q1 p2 equals q2 p sub m equals q sub m in this case the distance or the relative entropy equals to zero otherwise the divergence is non-negative it's always greater than or equal to zero so this is the first part in proving that lemma that the divergence d between distributions x and y is non-negative now we come to the second step now we let y be a uniform distribution in this case we have assumed that the reference distribution is a uniform so let y be a uniform distribution in this case the probability for any symbol in this distribution is 1 over m we compute the relative entropy between our distribution x and this reference distribution as d of x and y equals summation p sub j log p sub j over q sub j we can expand this summation as p sub j log p sub j minus p sub j log q sub j we can easily recognize that the first summation is nothing but the negative of the entropy of the source x so this is minus h of x as for the second term we have p sub j log q sub j q sub j equal 1 over m since the symbols in the reference distribution are equally probable so this is p sub j log 1 over m which equals minus h of x minus this is a constant we can take it out of the summation minus log one over m and we are left with the summation of p sub j but again the summation over all probabilities of this distribution p sub j equals 1 therefore d d of x and y equals minus h of x plus log base 2 of m therefore d equals log m minus h of x and we also know the relative entropy d is greater than or equal to zero therefore we conclude that h of x is less than or equal to log base 2 of m so h of x is less than or equal log base 2 of m we also notice that when p sub j equal q sub j when p sub j equals q sub j the value of this logarithm is zero therefore d equal to zero and in this case h of x equals log base two of n otherwise h of x is less than or equal to log base 2 of n to conclude the entropy of a source is a non-negative quantity which falls between 0 and log base 2 of m the minimum value is attained when one of the symbols assumes a probability of 1 and the rest are zeros the maximum value of the entropy is attained when all symbols are equally likely this concludes this lecture 
FaZzm2bNdP0,28,"The International Congress of Mathematicians (ICM) in Seoul, http://www.icm2014.org/

Invited Lecture

Speaker: Mark Braverman

Title: Interactive Information and coding theory",2014-08-18T13:30:23Z,ICM2014 VideoSeries IL14.2: Mark Braverman on Aug14Thu,https://i.ytimg.com/vi/FaZzm2bNdP0/hqdefault.jpg,Seoul ICM VOD,PT46M54S,false,596,2,1,0,0,"lisha mesna fix this year and record fellowship enzymes in engineering in tours to 13 and I fed the brain science faculty order for excellence in teaching and scholarship from Kristin global engineering last year and he had NSF Career order in 212 map a Braverman we'll talk on interactive information and coding if we welcome his hop in newman its anchor much for the introduction and thanks for coming to my talk so i'll talk about information interactive protein theory so just before we start talking about interactive odin theory let's a couple of slides on classical coding theory so what is coding theory but it's it status waste encode information and which are called codes and their fitness for specific applications and the major applications that are important in practice our compression so you have some information is like the store without spending too much storage or transmission error correction that perhaps the most important application and cryptography where you want to transmit information in a way that doesn't get tempered or someshit and the main technical tool that proved extremely successful and understanding and kind of making out the limits coding theory is information theory and traditionally the focus has been data transmission and there as you'll see we understand what there's a lot and an almost perfect theory originated by a Shannon that explain what's going on and that's it so Shannon's information theories perhaps one of the most satisfying theories out there for just taking a problem completely explaining what what's happening and some of the kids that take away in six that today are obvious but we're Tobias at the time is first of all that you can decouple under the channel from understanding the message so and you can study those separately in the context so it's channel capacity which quantifies the ability of a channel to share information and message entropy which quantifies how much how many bits you need to to transmit the message and then after after this is some work you can be very precise answers to coding theory so questions and this is all part from the late 40s and early 50s so a case side is Shan Shannon entropy so as for most of the software assume the losses binary channel suggests the channel region Smith zeros and ones and they are received perfectly as I mentioned you can study the channel and messages separately so this is the message part of it so a message access distributed according to some distribution view and the inherent amount of bids takes it transmitted x is instrument shellings entropy which is given by this formula so I hope you've seen it before if not here it is the most important thing is not what the formula years but that there is a hoarder and that's that's a very impressive fact even though by now it's a very simple ok so the first theorem in this theory that it needs at potions entropy is noises coding theory and it says that the cost of communicating many copies of X if I have a stream of data that arise distributed according to you scales is ho-8 so this is up to Laurel return position source code and see if see see if it did not bc animax the cost of transmitting and copies of x and we take this limit then it will scale to hmx up to lower returns and it gives so the word Ferguson information here in electrical engineering is operationalized it means that even if you are an engineer oh all they want to do in life is just transmit texts and not build a theory this tells you that you should still come up with this and care about it H of X because it has an operational meaning so this is sylvan is something that exists in practice you take this limit you get h of x which means the page to access is also a real quantity and what i want to play miss h of x is nicer than c NM x h of x shenzhen is nicer and let's just see some examples how it is nicer and the reason you i want to talk about those examples is because this would be a template on which removes an interaction so this is a very simple example suppose we have said in uniform treats over a binary channel you can see that the best encode and you can do it if you're just trying to say the single tree this is this you if you want to brief extreme coding you need to kneel and cold the three singles is here 10 11 and you get an important cause of five thirds okay if you have to send two treats it turns out that you can do twice one trait and it's just a simple calculation you get 29 / neither is it's a simple algorithm that you'll find out and it's in this case it's trivia so one thing that doesn't have this activity and that's kind of annoying on the other hand Shannon's entropy is edited and so the entropy of two trips is just log 29 which is log 2 3 plus local Shannon's I'll trip is nicer in that it is additive / independent messages and the transmission cost is even though it's almost additive and for a good reason because actually Shannon's entropy and transmission foster within one bit of each other so even if it's not if it cannot be tuned unedited because it's all within one bit but still it's a mathematical construct this is way nice because it avoids some of the kind of its it's obvious that this happens just because of integer problems so and channels entropy is a nice continuous relaxation of them and it gives us a precise invention penalize the spot in theory we get this formula so CMM is n times bak two three plus minus the globe and it should plus little woman so we perfectly understand the cost of sending treats as n goes to infinity I mean again and guarantee it's not a very impressive result but the fact that from this general theory I get it almost for free is an impressive maybe less two years later so today we will the discussion will be about generalizing information for the theory to interactive reputation scenarios and interactive competition in general is you instead of just transmitting data over a channel you would like to use a communication channel to solve some problem and both noise less a noisy the entire point in theory everything changes once you bring interaction so all these problems make sense and kind of give rise to interesting problems and today we'll essential real focused almost entirely on the noises hilarious so the channel will stay just a license binary channel and in computer science this this area has been studied extensively in the area of communication complexity so let me introduce the communication complexity model will focus on the two party setting today so this model in computer science was first introduced by yahoo in the late seventies and in this segment we have two players Alice and Bob and they're given some x and y which you can think about this input source states of the world as my mail may not be correlated and they want to influence finality f it depends of both x and y just think about this computer defined function so they're given inputs they want to compute the function but they have to do it over a communication over the telephone and the goal is to communicate as little as possible so for example a typical functionality where you can actually do much better than brute forces quality so Allison vulgar each given a very large file that they want to decide whether those profiles are important okay so more formally we will use will spend most of our time in the randomized multicenter randomized model they had Alice and Bob have access to shared randomness source which they both can read and it doesn't cost them communication to read the shirt random the source and then a communication protocol is just a formalization of a conversation so what does the conversation the conversation is a sequence of message of functions or messages in which the current function the current message is allowed to only depend on the current speakers input on the sheriff of attractiveness and on the messages so far so that's just formalizing this so that defines a communication protocol and the goal is in there to be able to do what you want to do so Plan C for example are our files Accord and the communication cost of the project is just number the total number of pins exchange that's the standard cosmetic and ok so you can this induces a definition of communication complexity so the distribution of communication complexity is just what is the shortest protocol that solves f with error that most epsilon is respected distribution you and so basically it's fine find me the shortest protocol that does but what I want you to do and it's not actually it's if I ask you to define and communication complexity with error you would probably say something like give me a practical that is correct on all inputs is probability 1 minus X instead of 1 minus X attraction of inputs like Italy by a very elegant minimax argument using duality minimax duality you can actually show that those nachos are the same so without essentially without loss of generality we can talk about distribution the distribution on setting distribution of letters okay so it turns out that communication complexity is incredibly useful and the coolant given unconditional Lord ones so especially in various restricted computation models such as streaming data structure distributed computing so essentially i would say that along with darren ization it's that second major technique we have forgiven unconditional lower bounds not for general computation model but essentially for all restricted one was like baby structures so and a prominent problem that drove allows many of the applications and a lot of the basic research and communication complexity is descended giantess problem and the related set intersection problem and so I want to stay those problems explicitly so the says so Alice involved each are given a set of Assad Superman Struan so a set of elements from the set its absolute of elements from the set of 12 n which also can be viewed as a 01 vector of length and you get a 1 if the element is in you get the 0th element is out and the set intersection just asks both players to compute the intersection of their cells and Santa jointness so this intersection the output is ended and sadly jointless us is there an element of common so that there is just one bit so Alice has a list of criminal spot has a list of bank accounts and they wanted to find there is an element in common and the question is of course you can do it by using n plus 1 communication right by just Alice consent her input to Bob and what can the return just one bit is there is there an element or not it's actually a non-trivial serum from the late 80s and early is to show that the communication of myself do giantess is linear and in case you get bored or lost here's a nice exercise I just told you how to solve dis join us in n bits of communication so can you do it in point nine piece and so that's reasonably doable if if you can do it try 4.6 a point for it so we know that you'll get stuck somewhere but let's be ambitious and try glancing this this question so in the end of the talk by the end up gettin obstacles so both of these problems interesting especially intersection but also intersectional disjointed there is a lot of structure inside of it so intersection is just a Patrick to be tens right so it's actually n times n instant and parallel instances of a two-bit and each so they want to decide x1 and y1 x2 and y2 and that gives you the intersection hug and this joint this is a disjunction of sections so it's not sure it's very natural to us what's the connection between the single end and the comp the communication costs of the elements that we are interested and this connection between hardness of a single problem and the hardness of its pieces is actually a very natural approach to lower bounds in general and this is design called direct some problems or direct some theories you're working so a direct some problem in general asks whether you when you try to solve n copies of the task you have to pay as much n times as much effort as one home or do you get the volume discount when you buy in bulk so in the case of communication complexity the question becomes does the communication cost of n copies cost and that is as much as as one coffee and it's not always true so here is a notorious example it's false if i want to multiply an n-by-n matrix x vector and i want to build a circuit just hyper circuit for this purpose it has to have something like n square gates it's a very simple information theoretic argument because you just count the major sis Cal the circus you have to have n square gates to do it if I want to build a circuit to multiply this matrix by n vectors well that's not too long ago right because I can apply matrix multiplication and major complications better than the cube so direct Sun which we fails here and we don't know how badly it fails for all we know it fail completely this could be 2 plus epsilon so we shouldn't take it for granted and it's actually you know there are famous examples where it failed miserably and kind of let the surprising results and more generally so how does the communication costs of many f scale as n goes to infinity so the direct Sun question doesn't scale is cesium as a communication cause of one puppy or not so remember that in the simple case of transmission there was a similar question of the transmission cost of n copies of X and their dancer was very succinctly twas hmm so information complexity which is the first interactive information construct that I want to talk about is the corresponding scaly limit for interactive communication so in one slide information complexity to communication complete is the interactive and a lot of channels and droopy so it's the communication complexities what challenges and trapeze routines mission so let me define it for me interactive information complexity for that I'll need a crash course information theory and again the most important thing about everything that's written the next two slides is not the content but the fact that it exists that you can say all of these things in pretty sexy intelligent math it's much more important than the actually finish so for two random variables the additional entropy of x given Y is then sits the typical uncertainty that remains an X if I tell you what wise so it's always less than the original 70 of X but it could be the same it could be less and for example one can show that the joint entropy of x and y breaks down as their took your y plus Delta P of x given that we already fixed y and this this fact is known as the chain rule and plays an important role in this area and the second quantity is mutual information so mutual information is the difference between entropy and conditional entropy and it's it answers the following question how much knowing X reduces the incessant invite so for example if they are independent then h of x given Y is the same as H of X in the mutual information 0 if for example x and y are the same then this is 0 and this becomes just the entropy of X because knowing why reveals everything above decks and then you can combine those you can talk about additional mutual information and there is a chain rule for mutual information that just even it if you see this is the first time you are seeing this definition if you just stay this line English it makes perfect sense so whatever x and y tell you about to see is whatever x tells you about z plus whatever white tells you about z if you already know it you can just parse it it makes sense you as always an ass you have to be careful to not get carried away but it actually is it's nice that it gives the language to talk about those things so remember that the first step in defined in the communication complexity was to define the communication cost of a protocol it wasn't a very impressive definition it was just a number of bits exchange but then then should be the first step so the information cost of a protocol given some prior distribution on the inputs is the mutual information between is what the protocol is what the protocol reveals to the part is about each other's in you let Alice and Bob run the protocol and then you ask them okay what have you learned so instead of counting beats you just measure what have you learned by the end of the execution and so this is exactly what that is lands because she knows else and we are asking what does by each share about why and this is the corresponding processor and note that this quantity depends both and buy a new obviously depends on high it also depends on you imagine if you is a singleton then you should then information plus 20 no matter what the protocol is because there is no nothing to learn bill everyone the whole world knows what x and y so let's see a specific example suppose we are trying to solve the quality actually a very natural protocol for equality and then the one that's used in practice Alice fix a hash say an md5 hash of X sends it to Bob and Bob compares his ashes and answers whether a physical two are not let to be specific let's say that the prior is that is probability half action you are equal and this probability half x and y are independently uniform so the probability that this practical effects is equal why who always output their equal if they are different then there is a tiny probability of a hash conditions of like 2 to the minus 128 but you can live with it anyway I'm interested what's the information cost of this protocol so what does Alice let's just invite everyone does Alice learn from this direction how many bits of information well that's that's the easy part 1 right because before she didn't know anything and it was half as likely that if they're equal and others like that they are not and then then should ask your learns whether they're it or not ok now the 100,000 one question what's the second term so Bob receives 120 feet so it cannot be more than that how much does bobler roughly so who thinks it's a hundred twenty-eight okay who thinks it's less any other suggestions how much yes it's more yeah so it's more like 64 or 65 to be precise why because half the time they're equal and then pop only learns the fact that they're April there is nothing else it's not like she cares you can compute md5 himself from this back and this fact is only worth one bill to him half the time it's worse actually 129 because he learned that they're not equal and the 128 bits of the hash so in 66 in this case so this is the definition of the information costs and now by just by analogy complexity the information complexity of a problem is just try to minimize overall protocols that do the right thing instead of communication cause the information cost of the product so in plain English solve the problem I don't care how long you talk but reveal as little information as possible to each other so that's that is it finished and it's an easy fact that information is always bounded by communication another interesting observation is that we actually need the increment here so even for the case of a simple lonely too deep end you get a sequence and infinite sequence of protocols this ever-increasing communication costs an ever-decreasing information for us so this Cynthia was never actually attained and then it turns out that actually information you get it once you've set up the definition you get that information is rico further face communication and so you get the same damn log of shannon's the nicest gordon 24 interactive communication and in fact the proof is simple enough that we will actually almost completely from one direction right now since its a math dogs which should contain one proof so let let's prove the greater than direction in the next five or ten minutes or so so we want to prove that the limit the schedule amount of communication causes greater or equal than the affirmative of communication to flex this greater equal to the information to instill problem so as I mentioned it's easy to see that information complexity information costs information requests are bounded by communication complexity just because you can to learn if you communicate a thousand fifty don't learn more than thousand bits ceniza fact from basic information theory and what will actually show is this that information complexity is perfectly edited remember when we were making the list of why Shannon's entropy was nice this was one of the properties additivity over independent random variables and here it's additive ET / independent us so what I want to show a perfect direct sum theorem for information complexities that solving many copies scales exactly like n times the cost of solving one and then actually it's easier to talk about general tasks not just functions so I asked is just the thing you do is two variables and unleash you can fail so solve a functional computer functions if the error X and respect to music pass and task is also output irrigate computer creation i think is a more general task for sample from some distribution that depends on both x and y is a test so that we have two tasks and i want to show that the information costs information complexity of the direct some of the of doing those two tests in parallel is the same as the sum of the information cause complexities of the individual passes so i want to show this activity this direction if the left hand Direction is trivial right because if i need to book two things i can always ignore the fact that then you things and do thing 1 and then good thing too so directs am only says the easy direction the heart direction is to show that maybe if I need to do thing one and thing two I can combine them together in a clever way and do much better than doing an individual maybe I don't dance and at all if somehow if I if I manage to be efficient so this is the direction we need to prove and luckily we have so many so few moving parts that by the value distilled to this level there there isn't much you can actually do so the crew just works out so we start with a protocol fight for this to pass whose information costs with I and out of it we want to construct two protocols 14 task T 1 and 1 fantastic tool such as their information cost of this book protocols and after I that's the only thing we can do and if you manage to do this we prove this statement ok so we start with a protocol for 41 prostitutes so it takes as input x1 and x2 on Alice's side y1 and y2 and website and it does something that in then sauce t1 and t2 in a very tangled contract way so and we want to out of it we want to make a protocol by one that takes x1 and y1 and dusty one so the first line of the protocol is by one of x1 y1 right so it's the title so that's easy the last line we only have one thing that we can use so the last line better be run by something that has x1 y1 in the first coordinate so so good so we only have two or three lines is between three and okay so what should be written in those two relax well the only difference between here and here is that we we need to get excellent widen from somewhere okay so one we need to populate it with some values okay one with natural approach would be to have alisoncole Xcode and box ankle biter and be done with it and then ran the protocol so what's wrong with this approach excellent white hooker come from some distribution youtube I never promised you that the distribution is a product distribution so there might be a correlation with the next and vital so if they sample them independently they don't get the right distribution on the whole thing collapses so to work around it you publicly sample X 2 according to you too so publicly you can do whatever and then give an X to add bulk and privacy sample item so the reason they cannot as i explained the reason they cannot just sample excellent why you privately is because of correlations the reason they don't just publicly sample x2 and y2 is because then we information population that work out for good reason because then you actually to reveal too much information this is kind of the most conservative way in which you can sample explain why do and in for vital you do the skew symmetric thing so you values the sample by one then privately sample expo and okay and then the analysis is just a simple calculation let's just see how it works from four by one so we remember we publicly sample X 2 and for a privately sampled by you so what does Alice why one she learns whatever by teachers about why one to someone who knows x1 and x2 right because she knows x1 because she was given x1 and she does X 2 because she frightened because x2 was publicly something so she knows and Butler is about x1 whatever pie reviews about x1 to someone who knows y 1 y 2 n expo so he knows why one because it was given it he knows by 2 because he sampled it and he knows x would be just unusual turn because X to sample public anyway so that's that's the expression we get for i 1 and you get a similar inbred expression for i2 and now it just isn't done them together so to get those four terms if you rearrange them it starts looking like a chain rule so actually so whatever pie reviews about why won 27 finos x1 and x2 plus whatever pie reviews about why 227 who knows x 1 x 2 and x 1 together is what pie reviews about y1 and y2 27 who knows so I the next up and you get exactly I so by the time you you know that the serum structure II your freedom is extremely restricted so the proof is bound to be either simple or false in this case it's simple okay so that's somewhat weeks so we got this theorem that information is equal to amortize communication I didn't prove that in direction but let's take the contagion anyway must for lower bound applications their greater than directions is the more important one so intersection now let's go back to intersection disjoint so remember intersection is just n copies of two-bit end and this joint this is a disjunction of em to be tense and so the communication complexity of intersection we now can write an exact formula for it it just the information complexity of the two-bit end plus minus a sub linear term and so all we have to do is understand the two-bit and function so this is this is now an even simpler problem so it's a problem about two bits alice is given a bit walking is given a bit and they want to compute the end of the movie so they want to know the way both at once or not and we want to do it when you're given as little information as possible about each other's inputs to each other so the answer to this question for each view is just a number but it's so for interesting news it's so actually we want to know that and respect to the worst view for reasons that i don't want to spend time on so we want to know with respect to the worst view then how much information we need to reveal to each other to solve the to pretend this is a number it's a number between one and two it's not more than two because we only have two big it's at least one because for example if Alice has a 1 and then compute engine masculine whatever boxhead broadcast so if Alice has the one in vogue has a random bit that's already what was one so and the answer is this number roughly one point forty nine to two and this it's almost explicit in the sense that it's a maximum of some function that unfortunately contains logs and infractions so I don't think it's it's a union what a function is pretty nice unfortunately I don't think there isn't any expression to the roof of its derivative but it's an explicit number and you prove it by finding the exact value of the information cost of complexity of n with respect to all prior simultaneously so what it boils down to you view this IC of M mu 0 as a function on the space of distribution so what is the space of distribution it has four degrees of freedom minus 1 because they have to add up to one so it's r 3 r plus 4 over heart over r plus and it becomes some function minimization problem since it's a maximum of a certain family than satisfy some inequalities so once you get the function you can just verify those inequalities and that's how you prove a lower bound actually for the upper bound interesting i can just give you the protocol so here is not tell protocol for for computing the end of two bits while revealing as little information as possible to the participants so Alice has an X bulk of the Y and they want to decide whether both of them have a one or not so here is what they do so I describe it he does the fall in it if she has an x equals to one she just sent her private rented private variable 821 if if harry took is zero then she picks a random number between 0 and 1 uniformly at random and Bob does the same thing now they have a counter that will go from 0 to 1 so think about it as a clock will go from 0 to 1 and when your number is so you find your number on this counter so for example Bob might have a bond analysis is 0 then it looks like this then you raise your hand when your number is called that's the entire foot so you can see that it computes the end of the tube it correctly if both bits are one then the country will go all the way up to one in the world one otherwise the country will stop somewhere in between ok so why is it of course it's not approve but let's try to get an intuition why is this a good protocol for not revealing too much information well let's see if bob has the one there is nothing we can do he will learn the answer he will learn what Alice might as well not working on now if I was has a zero we can hope that she will not learn the answer she will not learn what focus and then did in this case if I was raised her hand say nine point six she doesn't know whether bug has a zero and maybe his beard just happened to be point eight or if both as a wife so of course she's more kind of every as time passes she becomes more and more convinced that Bob is more likely to have a one she proved she previously thought but she still has quite a bit of doubt about what Bob has and it turns out that this is the optimal amount of Doug so this is dr. on protocol for for the two-bit and as a corollary you get that the communication of negatives of intersection in scale this point 49 22 and plus order terms so you get a precise answer for this question and you can give a precise answer also for this jointless some additional work you get that the dragoness the communication purposes of disjoint business tiny error scales is point 48 to seven so sorry about the point for question so that's impossible 16 is possible but I don't know of any simple kind of protocol so the way you describe this protocol states that rising other protocol take many copies of it compressive using slip in both it's not an explicit protocol and actually we can show that to achieve this constant you need to use a limit number of interaction around back and forth rounds he is a limited if you use a constant number around you you lose something with respect to discuss so more general connection to interactive compression so we've seen so remember we wanted to answer the direct Sun problem and we've just seen that now we know what the scaling limit of this thing is so the direct sound question is a grill is actually equivalent to a skin is the information complexity of F on the Gulf its communication complex because this is the schedule negative the communication of medical visible or less outwardly is CCMS order of ICU so this question it becomes an interactive compression problem with us if I give you a way if I give you a protocol that solves f by talking a lot but not revealing a lot of information can you convert it into a protocol that actually communicates not too much so this is the interactive compression problem in the non-interactive setting up when Putin does exactly that so if I give a nun informative lecture for example then my husband code and you can actually compress it to a short one but if you're having an informative conversation it's not known whether you can compress it into a short one and it's actually in its strongest form its problem falls so there is some partial progress on this question which is easier to stay in terms of the direct sum theorem that you get and the best so the best compression result so the better you compress the stronger direct sum you get so the bigger gee becomes so if you had perfect compare would be home again order of n Sato then and the best you can do we know how to get is something like square root of N and actually the recent breakthrough baikonur Colin grass it's very unlikely true that the strongest direct some pods so it's unlikely that you can do better than and over again and actually I wouldn't be completely surprised at that tight answers either of this book I hope it's actually this one it's something like an over again but that remains to be seen also it turns out that you can prove tight threshold phenomena with this communication complexity in the following sense so if if you are given we know that to solve n purpose of F you need something like n times I see over communication you can actually prove that if you if you use much less less than this communication your success probability actually drops exponential so your success probability becomes so you have a threshold phenomena where you go from almost surely succeed in polls for the fame and it connects actually the result it's a relatively recent connection you can connect this theory to the theory of artists simplification that's more classically brazos parent editions theorem so hopefully we'll see more of that in the future so I haven't taught so as far as big problems in interact Gordon series so today I talked about interactive the interactive animations entropy the interactive analog of compression which we don't know unfortunately also there is an interactive analog of error correcting codes that I discuss in the paper but I won't talk about here so in the last two minutes that they have let me discuss briefly discuss the road head as a hope it will proceed so on the one hand we have one-way communication which is a beautiful complete theory which we understand almost perfectly and that's what information for the theory does on the other hand we have unrestricted computation with questions like p versus NP and so it optimistically something like this is mystically it's more more like this let's try to stay optimistic and we would really like to make it more like this in interactive computation somewhere in the middle states it's still useful enough to get some unconditional lower bounds at least in restricted and models of computation the gods stated results since the 1980s and it would be very nice to complete this theory nice we've made nice progress there and so information from Lex interactive folded in the census trying to organize distillery for interest in the book player case and the first it would be interesting to see how far to the right you can push it there are things in between here like multi-party communication that I didn't get to talk about which would be the next natural step to try but it should be interesting isn't way thank you inclement announced time to help of patients are coming for this let's talk any patients come see what alright so the question was that we have that there are results in secure multi-party computation that essentially this new part is show that you can compute anything without revealing any information to the participants yeah we are acutely aware of those indeed this is a major hurdle to define three-part information complexity in the natural way of course you can always start with operational definition and see where that takes you but but yeah definitely that's a problem so if you just try to cop it too far to 2nd the natural extension of these two three parties will be yo get all the nice direct some theorems the long view of the form 0 plus 0 plus 0 is equal to 0 but I don't think it's hopeless entertain and I think maybe we haven't discovered the right point it is yet that's the maybe they won't be as nice or maybe it is like going from a jurist answers maybe he'll be terrible but we'll see any other pics to us a comment is not and thank you again to the speaker "
2rADxu7XMr4,22,"Online Mathematics seminar by Professor Navin Kashyap (Indian Institute of Science), held on 2 February 2021. 

Title: Capacity and Coding for Binary-Input Memoryless Channels with Runlength-Limited Input Constraints

Abstract: Discrete memoryless channels (DMCs) with constraints imposed on input sequences form an important class of finite-state channels, which model a variety of situations in digital communications and storage. Of particular importance are binary-input DMCs (or BI-DMCs) with (d,k)-runlength-limited (or (d,k)-RLL) input constraints, as they serve as useful models of random noise affecting data stored on magnetic or optical storage media. These channel models have a long history, starting with the work of Zehavi and Wolf (1988) on the binary symmetric channel with (d,k)-RLL input constraints. However, our understanding of the information-theoretic capacity of these channels is still quite limited. In this talk, we survey some of the recent developments in this field, paying special attention to the feedback capacity and coding for these channels, about which we now know a great deal more. For concreteness, we will focus on the case of the binary erasure channel with (d,k)-RLL constraints. 

The talk is based on joint work with Oron Sabag, Bashar Huleihel, Haim Permuter, Aashish Tolambiya, and Arvind Rameshwar.

Bio: Navin Kashyap received a B.Tech. degree in Electrical Engineering from the Indian Institute of Technology, Bombay, and an M.S. in Mathematics and a Ph.D. in Electrical Engineering: Systems from the University of Michigan, Ann Arbor. He is currently a Professor at the Department of Electrical Communication Engineering at the Indian Institute of Science. His research interests lie primarily in the application of combinatorial and probabilistic methods in information and coding theory.

Prof. Kashyap served on the editorial board of the IEEE Transactions on Information Theory during 2009-2014. He was appointed as a Distinguished Lecturer of the Information Theory Society for 2017-2018. He is a recipient of the Swarnajayanti Fellowship awarded by the Department of Science and Technology. He is at present an Associate Editor for the SIAM Journal on Discrete Mathematics, and the Springer journal Cryptography and
Communications.",2021-02-05T01:18:26Z,Mathematics Seminar | Navin Kashyap,https://i.ytimg.com/vi/2rADxu7XMr4/hqdefault.jpg,NTUspms,PT1H43M57S,false,245,2,0,0,N/A,all right then okay hello everyone okay um welcome back to our seminar series information theory in singapore itis this is our first talk in 2021 and my name is hama and welcome back to virtual singapore okay so as you heard just now okay what you see here is singapore's changi airport which is getting a bit busier but still very quiet during this unusual times okay and of course by everyone uh let's hope that this strange time pass soon and we can you can visit this airport in person sometime soon now for those who are joining us for the first time the information theory in singapore its seminar series is organized by a team that includes nehuma tani from national university of singapore kuichai and tuan tang yuen from sutd singapore university of technology and design and me i'm alkia from nyan technological university our aim is to promote advocate and spread the joy of coding theory and information theory within singapore and around the world we have five talks lined up for these two months february and march and today is our first talk now before we get going let me address a few logistical issues please please keep your microphone and video muted for the duration of talk if you have questions you can post them to our chat group and we'll keep a lookout for it we'll address questions in the qina session at the end of the talk and with that it's my great pleasure to introduce professor navin kashyap as a speaker professor kashab is currently a professor at the department of electrical communication engineering at the indian institute of science his research interests lie primarily in the application of combinatorial and probabilistic methods in information including cabling professor kashab served as the editorial board of the ieee transactions on information theory during 2009-2014 and was appointed as a distinguished director of the information theory society from 2017 to 2018 is a recipient of the srana jayanti fellowship awarded by the department of science and technology and is at present and associate editor for the science journal on discrete mathematics and the springer journal on community cryptography and communications well in my few years as a researcher navin's papers has always been a delight to read these papers not only address difficult problems in communications but also introduce introduces many beautiful and deep mathematics and i always find something new to learn so today i believe i will learn something new too so the title of this talk is capacity encoding for binary input memoryless channels with run length limited input constraints professor kesha please thank you very much and thanks to all the organizers for for inviting me to give this inaugural talk in this series [Music] i think at least looking at the program from last year it was uh you know set up very highly distinguished lectures and i hope that i can live up to those standards um and that's thanks also for the people who are attending this talk um so now can you tell me whether like this time limit is going to be strictly adhered to or is it like somewhat flexible uh very flexible okay yeah so i'm not really sure like how much it might take i think sure sure and also i think uh yeah so this talk um i mean i'm gonna it it could be that like it is kind of it may not be very familiar material for many of you i mean the problem setting and formulation etc is pretty standard pretty straightforward and indeed very most of you will find it very comfortable but the techniques finally and the kind of uh you know the uh the other techniques that we'll be getting that we'll be using to address these questions could be quite uh different from what you have been encoded from what they have encountered in the standard information theory or coding theory course so let's see um so it's more control theoretic i suppose um yeah so anyhow i'm going to and like there is a lot of ground that i'd like to cover i suppose and so it might be going a bit too fast but you know i'll highlight the salient points of what i would like to you know sort of focus on and at the same time of course the talk is recorded and then provide enough references etc so that you know anybody who wants to learn more about this material can can do so um so this talk is uh is joint work with a bunch of collaborators it started out with uh heimdallr and his group uh auron zabar hot is a phd student of his and he's i think now a postdoc at celtic i think uh is also a masters student of hines then arvind and ashish are students of mine are in this my phd student and ashish is a master student here okay so without further ado let me get going right so i mean it's just a slide on motivation i don't really think there is much need to motivate this problem for the audience that i am aiming it at because i think it's kind of natural most of you are quite familiar i think with uh with run link limited codes uh with run link limited constraints uh and so constraint coding etc uh many of you are certainly i see many experts here uh so all i'm doing to that is add a channel uh so you know we're adding a memoryless channel through which unlinked limited codes or run limited sequences will be passed and you want to ask questions about what the capacity of such a channel then will be since the channel is memoryless by itself if you did not have any constraints at the input of the channel then capacity etcetera are at least well understood but when you start putting constraints at the input capacity becomes less understood in fact as you will see highly sort of very little understood concept um anyhow motivation just comes from well you have discrete memoryless channels which are ubiquitous in their applications randomly they model random noise uh that affect the transmission storage of digital data uh con encoded sequences uh well we have constrained coding constraint coding is also well mod highly well motivated uh it is again all over the place in storage media it is used all over the place in storage media magnetic tapes hard drives cds etc flash memory dna based computing and storage uh and like many of i know that uh at singapore people also work with you know the power line coding for power line communications or especially the simultaneous information and energy transfer sub-block constraints uh which is i think mayhew and his group have been doing for quite a while um so these are i mean so i don't really want to i don't need to motivate constrained coding so i'm just putting the two things together two very well motivated uh concepts the discrete memoryless channels and constraint codes and so from a theoretical standpoint uh when you put these two things together that is you have constraint coding constraint codes or constrained sequences at the input of a discrete memory less channel overall this class of channels overall channel then falls into the class of a channel with memory it is because the constraint itself holds memory we need to know what to send the next bit to be sent into the channel will depend on what the past bits were because if the next bit to be sent in violates the constraint then the chant that would not be allowed to be sent into the channel so this is we will see that we will only be focusing on of course finite state channels where there is a finite amount of memory associated with the constraint and for the for the most part in fact i will focus on the on the binary eraser channel to keep things simple and ideas more or less concrete we will focus on the binary eraser channel with run link limited constraints at the input with all i think familiar or most of us i think here at this talk will be familiar with the dkrll t so we say that a binary sequence is uh is dkrl constrained or a dk run length limited constraint if each pair of successive ones in that sequence is separated by at least d zeros and every run of zeros is of length at most k so there's a constraint on the run lines of zeros okay so zero run a maximal zero run must have length at least d and at most key um so this is again some some fun example that i put down here of a sequence satisfying a two seven rll constraint so that runs of zeros have length at most at least two and at most four in this example i don't think i think the binary eraser channel is also quite well known or uh familiar with to the audience to this audience that is uh it's a binary input channel and each input bit either gets transmitted faithfully to the output with probability one minus epsilon or with probability epsilon gets erased at the output okay and it's by itself the channel is memoryless in that if there were no constraints at the input if the inputs were unconstrained in the channel is truly a discrete memoryless channel all right and for dk run length rl constraints it's again well known that they they have they come with this what are called graphical presentations that is uh to generate sequences satisfying the constraint one can look at these graphs the presentations of that constraint and these are edge labeled directed graphs uh and as you walk along parts of this graph and you read off the labels of edges along paths you will get sequences satisfying the constraints so these are all classical this is the so-called what we call the sharon cover of the uh the first thing the graph on top gdk the shannon cover of the dkrl constraint when k is less than infinity when you allow k to be infinity that is there are no upper there is no upper bound on the length of zero runs then here is the uh this is the shannon cover of the d infinity constraint uh so one piece of notation that i will keep carrying with me are these so two things i suppose gdk as well as sdk script script gdp and script sdk are is the graphs that these these directed graphs these labeled graphs and s t k are is the is the vertex set of these graphs okay so k can be infinity so the set of vertices of these graphs will be denoted by sdk states s is s is supposed to represent the term states all right so the particular problem uh okay so we have a discrete memoryless channel with run length constraints at the input but we also potentially allow feedback that is like so we consider a discrete memoryless channel with an input constraint and sometimes but also we will allow feedback i mean we can choose to ignore the feedback if we don't want it but like the the general problem that we are sort of considering is the setting of a discrete memoryless channel with an input constraint and feedback and feed so here you have a constraint encoder that is uh you know message bit you have some messages which have to be encoded and transmitted into the channel and the code the sequence of the the sequence of symbols that enter the channel must satisfy the input constraint the channel itself is memoryless by itself but like because but the constraint will add memory uh the feedback is noiseless feedback with a with unit delay that is causal noiseless feedback that is when output at time i comes out of the channel it gets fed back to the input that is the next input into the channel the input at time i the ith input into the channel depends not only on the message being transmitted message to be transmitted but also all the outputs that have taken that have come out of the channel uh up to time i minus one okay all of those get taken into account in order to determine the message bit uh the the next symbol to be put into the channel to be transmission to the channel uh at the end of the transmission you let this happen for a certain number of uh time time instances or time steps and then you d you make a decision about what the message being what actual message being transmitted was so you after having received n outputs and output symbols you make a decision about what the message being transmitted okay so this is the setup of this discrete memoryless channel with an input constraint and feedback see if so if we did not have constraints at the input if the encoder were unconstrained if that like there were did not have to satisfy any uh input constraint uh then this is classical it goes back to the work of shannon of course and we as is well known feedback does not increase the capacity of a discrete memoryless channel okay however but what feedback can do even in that case is that it can simplify coding schemes right even if the capacity of the channel is not even if the channel the feedback does not increase the capacity of a channel uh it can still simplify coding scheme so for example if i look at the binary eraser channel uh the capacity of that is well known to be one minus epsilon either with or without feedback with without feedback you'll need to use some sophisticated error correcting code etc but if you allow feedback it's a very simple coding scheme that you can use to send the particular bit zero or one you just repeatedly transmit that bit until that bit is successfully received at the other end that's all because each time you know whether like because of the feedback you will know whether the bit was that was transmitted in the last time instant got actually successfully received or not so the transmitter will know uh so you can use feedback and just keep repeating until it can until that bit gets through so no sophisticated error correction etcetera required here um or eraser correction and like to see what the rate of this coding scheme is the expected number of channel uses for successful transmission of a single bit would of course be the mean of the binomial of a geometric of a geometric distribution a geometric random variable so it is one over one minus epsilon and so so that is an expected number of channel users for a successful transmission of one bit so the coding rate if you think of it is basically arbitrarily close to one one bit that is the number of the amount of information that you want that you want to convey and amount of channel users it takes you divided by the number of channel users on an average that it takes you to convey that amount of information so 1 divided by 1 over 1 minus 0 1 divided by 1 over 1 minus epsilon which just gives you 1 minus epsilon and that's it okay so it's as simple you can achieve capacity in a very simple coding scheme using just i mean it's it's a variable length coding scheme but again you can make it fixed length by by resorting to the law of large numbers and so on which is not something okay it's not not something that i really want to focus on we can we'll leave it with variable length coding schemes um okay so so that is what feedback allows you to do even if it does not increase the capacity it considerably allows us to considerably simplify coding schemes on the other hand in the setting that now we're interested that this talk is focused on of uh dmc is with input constraint and feedback feedback in fact can and usually does increase the capacity these are channels now with memory overall because of the constraint and the input and for channels with memory overall generally speaking capacity does increase when there is feedback involved and in this case we will see particularly we'll see concrete examples where feedback does actually increase capacity strictly increased capacity okay i've been talking about capacity so this is kind of starting of the first technical part of this talk so these are the two expressions that i will work with um capacity of dmcs with constrained inputs these are all classical isn't well at least the first part of it is highly classical it goes back to the work of at least gallagher even prior to that i would say it's just been taken out of gallagher's textbook capacity without feedback for an input constraint channel because it's basically if it is a special case of a finite state channel the capacity is well known to be you look at n letter expression and letter expression you let n go to infinity you take the mutual information between n input symbols and corresponding uh n output symbols i divide that by divide by that by n you take the maximum over all input distributions that uh sat that are supported on sequences that satisfy the input constraint so that is important that is the probability that associated with the sequence probabilities these are distributions at the input distribution such that the probability that it assigns to sequences that do not satisfy the particular constraint that we are operating the probability that assigned to sequences that do not satisfy the constraint to zero so only sequences that satisfy the constraint get assigned non-zero probabilities by such input distributions so we can call them admissible if you like input distributions and then you let the limit as n goes limit n go to infinity the limit again so this is well known that this limit exists by sub additivity arguments uh so this is what is called the capacity without feedback um it's an n letter expression and like a and it has meaning in that like it is not just an information theoretic definition it is there is an interpretation there's a coding theorem associated with it that is like you know if you have as long as you're operating at rates below capacity you can get arbitrarily you can you have feedback coding schemes that or you have coding schemes that for this for this channel essentially random coding schemes that uh um you know for which the probability of error can be made as small as you like and if you're operating at rates above capacity again you have a converse that is probability of error will get arbitrarily close to one right uh capacity with feedback again there is a corresponding expression for it also well known uh here this exposition i am based up basing it on this work of young kauchichat although i would say again this this expression perhaps goes back even well before tatikonga i mean it was already known to like people like jim massey in the 1980s uh maybe and it probably goes back even earlier but like just the development that i'm going to talk about goes back to the work of tatikondakun does phd thesis to be precise so the feedback capacity is given by again an expression it's an n letter expression uh this in this quantity within the summation is called what is called the directed information this guy here is called directed information so it replaces mutual information by something called directed mutual information um and because of so yeah and it is a specific definition that it is the mutual information between the input at time t and the output at time t but condition on all the previous outputs up to time t minus 1 and you take the summation d going from 1 to n you can let it you divide this by n and let them and go to infinity and that gives you an information rate with feedback that is achievable and you take the supremum over all input distributions that are again that respect the input constraint so here just the structure of the input distributions this kind of interest or is something that you should that i will try to explain so it's a probability on so it's a probability probability distribution on symbols at the at the th time instant at time instance t so input symbol at time t has a particular probability distribution and that probability distribution depends on the particular state of the input the state of the constraint up to time t minus at time t minus 1 just prior to when that input has to be sent into the channel and so the state will take on values of from one of the states of the presentation of the input constraint of the dqll constraint and also since we are allowing feedback it's so the input to be sent into the channel at time t will also depend on all the previous outputs that have come out of the channel up to time t minus one the only thing is that like yeah so we must make sure that like these are uh so supreme is taken over so these distributions this family of probability distributions must respect the input constraint in other words if um so x t will take for so the con in the in the uh when we're talking about dk run length limited constraints uh st minus one is one of the states of the presentation of the constraint and so you if x t is x t will be equal to one if and only or xt is allowed to be equal to 1 that is the probability of of throwing an xt into the channel at time t uh probability of throwing in a 1 into the channel at time t will be not nonzero will be strictly positive if and only if there is an edge from there is an edge labeled one from that particular state of the presentation uh there's an edge labeled one leaving that particular state of the presentation so that is at that state you're allowed to actually input a one into the channel without violating the constraint okay so that is how you're going to be taking so again yeah so these are the fam so the supremum is taken over these type of distributions input distributions okay any questions here i mean this is basically i mean how you derive these things is not particularly you know important to us we will be working with just these expressions but again there are these are there's coding theorems that back all of these things that is there are feedback as long as you're operating at rate below this expression there are feedback coding schemes that let for which the probability of error can be made managingly small and at rates above this expression there is no coding scheme for which rate can be made less than epsilon it will be the rates in fact will go towards one so the probability of error will in fact go to and so on okay [Music] okay so so the kind of questions that we will address in this talk are well number one again these are multi-letter expressions involving supreme limits and what kind of things uh can these actually be computed easily i mean these are like this is a family of input distributions i mean i mean actually you have to and you have to define you you need one such input distribution for every time instant t so in fact there's an infinite family of input distributions for defined for every value of little t um so you know so it's not at all clear whether these things are even tractable maybe you can define them but can you actually evaluate them not at all clear and that is one question and the second question is can you even if you were able to evaluate the capacity expressions well we know that there are random coding schemes because you can prove coding theorems using random coding schemes but can we do better can you design explicit capacity achieving coding schemes which are efficient with efficient decoders and coders and so on okay so these are the questions that we'll try to address in this talk um yeah it's just like so at this stage it's i guess maybe i'll just give you a quick outline of what i intend to say in this talk uh the technical aspect technical part of it so we'll give i'll give a brief review of the relevant literature start with that there's a lot of literature i'll try to just touch upon at least what i would consider the significant ones are significant ones directly related to this talk then so i will start with we'll start with the simplest possible case uh of a constrained channel input constraint channel a binary eraser channel with a one infinity rl constraint at the input one infinity just means that between any two ones uh that is every every zero run must have length at least one there is there can be no consecutive ones okay so the input is restricted to have no consecutive ones no consecutive ones are allowed at the input channel so we'll i'll discuss that i'll discuss that first at some length then we will start by then based upon that i will take off and give more general results or general techniques for handling dmcs with input constraints especially and for in fact like the more the most the vast majority of this talk will be focused on the case of feedback capacity for a very simple reason we just know a lot more about it surprisingly perhaps and in fact there is a very well there's a well established mechanism for handling feedback capacity and uh it also gives rise to coding schemes in fact we can actually from this mechanism that we use to handle the feedback capacity if coding schemes also kind of arise naturally capacity without feedback is an entirely different ballgame not just is i mean people have worked on it a lot but it still remains kind of a mysterious animal uh there's not much that is concretely known about it bounds on capacity without feedback etcetera we do know but the the most important thing is that we really don't know how to code you know for input constrained channels without feedback there are i mean we do something to achieve that is the capacity achieving if you want to get to capacity of these channels all right so here's some background on like a relevant literature as i said there's a lot of it and it's not a comprehensive listing by any stretch of the imagination uh but it goes back i would say the origins are in the work of zahavi and wolf so this is initially i'm just talking about the literature and capacity without feedback um so xavier wolf motivated by again magnetic recording magnetic recording applications looked at a binary symmetric channel with run length constrained at the input they gave some bounds on the capacity lower bounds and also upper bounds then um this was also picked up by shamayan kaufman a couple of years later uh there's not really much i mean there were some minor works here and there but like you know in the intervening years between 88 let's say in about 2006 when well yeah so then i'd say the next very step up here would be the work of arnold at all where like they looked at a general setting of finding or trying to estimate the capacity for general finite state channels of which channels with input constraints are a special case so they use the monte carlo based monte carlo simulation based methods for estimating capacity uh fontov at all gave a generalized generalized remote algorithm for this classical playhead remote algorithm for discrete memoryless channels to the setting of finite state channels with in fact finite state input processes which is very naturally set fits into our setting of discrete memoryless channel with finite state input processes um yeah so but yeah it is it is not its application is kind of limited this it's uh it does not really give the same guarantees uh as the greyhound algorithm does in the case of a discrete memorized channel but let me yeah but it is a numerical method it is a tool that one can apply um there is the uh then like hanan marcus this is guangya han from who's hong kong university and brian marcus probably does not really need an introduction to this audience uh who so they they worked on general discrete memoryless channels with input constraints in a series of papers uh using they are mostly the techniques that they were using or like what they were kind of approaching it from is uh is like if you if you throw in and let's see if you throw in for example a markov process into the input of a discrete memoryless channel what you will get at the output is what is called a hidden markov model here in markov process and there and if you want to understand what the information rate or the capacitor mutual information between the input and the output is you will have to really understand what the entropy rate of the output markov process output hidden markov processes internal markup processes are notorious markup processes we know how to how to it's a very simple we learn this in the first course information theory how to what how to compute the information the entropy rate of a markov process but we also learn in the same first course and information theory that a hidden markov process is very difficult to actually evaluate the entropy rate for there are bounds that you can find in the book on core and thomas for example so like they spent a lot of time and marcus did to try to get build tools for computing or how how how the entropy rate of it and markov processes actually behave for example is it an analytic function is it an analytic function of the process itself of the description of the process things of that nature see for example so the reason for looking into something like this is that if you wanted to use these techniques to get to capacity of some of some class of channels within this class then you want to maybe for example you might want to use some variational methods maybe gradient methods to nudge your optimization problem in the right direction right you want to maximize the entropy rate of an output process and you know subject to with your process being the input process being within some class of distributions and if you're out even if you know for example that the output process behaves in a in a if the enterprise of the output process behaves nicely or smoothly with respect to the input distributions then you can try to use gradient based methods so they spent a lot of time doing these type of things trying to put that on a rigorous setting then han by himself has been also working on stochastic approximation type algorithms for giving numerical lower bounds on the capacity of such channels uh khan gange and his student young yong long li looked specifically at the binary erasure channel with a with no consecutive ones the one infinity rl input constraint they gave in particular they were able to characterize effectively the what they call the asymptotic capacity asymptotic in a sense that at epsilon the eraser as long as when the eraser probability goes to zero in that small regime of vanishingly small areas of probability what like what does the capacity look like and they were able to give an expression for that then like on the other hand these are all essentially you can think of these as like the prior work is mostly concentrating on lower bounds i'm trying to estimate the capacity from below by taking input processes and trying to evaluate information rates of those input processes the upper bounds uh i'd say the most imp the kind of the upper bounds that are maybe the more general most general family of upper bounds are what we will what come from this method called the dual capacity method given sometime i might go i might be able to get to this method at the end of this talk let's see anyhow so tangaraj initiated study of this applied this method in to the study of binary input channels with run length constrained inputs and this was also picked up in a later paper that we also worked on and kind of extended this technique a little bit more and finally uh um yeah so in icita paper like earlier just last year we my student arvind and i had some dynamic programming based lower bounds on the capacity of these kind of channels for with feedback i would say the story really of capacity with feedback starts with the work of tatakunda the phd thesis of tatikunda under the guidance of sanjoy mit there at mit and so they realized that this capacity calculation can be formulated as an average reward stochastic control problem so mr comes from a stochastic control background and like he realized that it fits in very naturally in that setting and there is a once you know that it fits in that setting there are there are tools developed within that community dynamic programming methods that are completely set up for the evaluation at least the numerical evaluation of these of this of this type of of this type of framework so capacity can actually be numerically evaluated using tools from dynamic programming and so they did this and they showed the results for a binary symmetric channel with a no consecutive ones which again which is this one infinity rl input constraint uh then much uh for about 10 years further down the line like heim permuter who had already worked on like giving analytic trying to approach this dynamic programming method to get analytic expressions for capacity of various finite state channels uh then with the help of a student so we put a student uh auron's about to work on this like for whether you can actually do the same sort of a thing uh can you actually analytically determine the capacity of feedback capacity of a of a pmc with with a running constraint at the input uh is that so we so so saturn you know worked this out and uh and not i will explain some of this stuff here and it is completely non-trivial solution um so he was able to give an exact determination of the feedback capacity of a binary eraser channel and in fact this was also later extended to a binary a general binary input binary output channel for example a binary symmetric channel with a one infinity rln rll input constraint then then once this once the array once there was a method of approaching this attacking this problem then like you know one could consider or think about how ways of generalizing this beyond a particular one infinity rl or particular instance of the bec or a general bibo channel so they gave this class of techniques based on what are called q graphs i will explain this in this talk as well on the to approach the feedback capacity of finite state channels in general then in further work uh pellet data uh gave the exact determination of feedback capacity of a binary aerator channel with a zero krl input constraint which actually got the best paper award at the site a couple years back and then my student we just again submitted a paper on the d infinity rlll input constraint on for the same channel this has been just submitted to the site okay so that is the background and now without further ado like i will let me get into the problem formulation um you know the technical okay the expression the feedback capacity expressions and so on so i'll start with like a feedback capacity as i said for the most part of this talk because we know a lot more about it techniques are quite nice well developed now uh capacity without feedback it is quite a bit is it's still the wild west we still don't know much about okay so i may touch upon it given sufficient time at the end of the talk questions if there are any questions please feel free to ask them yeah so so here is the expression for input constraint feedback capacity for binary eraser channel where you have a one infinity rlll constraint there is no consecutive ones at the input it's a very simple that the complicated n letter expression involving limits and suprema etc becomes a simple one parameter optimization problem it's an optimization of a rail of a real variable where this real variable p belongs to the interval 0 to half c sorry the real variable belongs to the interval from 0 to half and it is this expression that needs to be optimized it's as easy as it gets okay you can throw whatever optimism you can throw calculus at it okay uh okay it is not a closed form expression in that like i don't know what particular optimizing value of p is and so that i can't just plug that value in and get an expression it will be a transcendental equation that i will get if i for example differentiate this uh if i take the derivative of the objective function and set it equal to 0 i'll get some equation for p that i have to solve and take solving that numerically plugging that value back in into the into the objective function give me the expression for capacity all right but it is simple it's remarkably simple and for example if you set in if you set epsilon equal to 0 here you will precisely see yeah so i'll come to that so here is the plot of the capacity observe that it has this interesting concave shape okay if you recall the capacity of the of an unconstrained binary eraser channel is simply a straight line going from one it's a one minus epsilon plot right it goes it's a straight line going from uh it's a straight line that will go from one here all the way down to zero that would be the capacity of the capacity expression for an unconstrained dec for a constraint vc whatever specifically this one infinity rl constrained at the input you get a concave shape for the curve different from for example any other channel that we know for a discrete memoryless channel discrete memoryless channel capacities are generally convex right for example a binary symmetric channel capacity if you look at it as a capacity for a binary symmetric channel as a function of the array of the error probability it's a convex shape one minus h of t is a convex stream but here it is concave so this is interesting uh then so and when you put an epsilon equal to zero here it just evaluates to log of the golden ratio which we which of course as we all as many of us know it is the capacity of it is just the noiseless capacity of the uh one infinity rl constraint okay so if there are no arrangers then like any then there is no need any any uh we don't need extra we don't need extra coding over simply the noiseless constraint all right so how do you how does this compare with for example the non-feedback capacity so we do know that now the feedback strictly increases the input constraint capacity this can be gleaned from both like this work of lee and han who as i said pointed looked at the expression for the non-feedback capacity in a small neighborhood for epsilon close to zero so and then we can by looking at that expression and comparing it with our expression for feedback capacity we can show that the two expressions the derivatives at zero actually differ so the two expressions are not going to be the same uh for epsilon values close to zero uh arrange your probability is close to zero uh this but then tangra techniques using the dual capacity method actually give upper bounds that are gave upper bounds for the noise for the non-feedback capacity for the binary binary eraser channel with the one infinity rll input as you can see i think that is the in the inset you can see that as the any of the dotted bounds the red dotted bound red dashed bound or the blue dashed bond which lies strictly below the black curve which is the feedback capacity these are upper bounds on the non-feedback capacity the dashed bounds of the non feedback capacity upper bound which lies strictly below the the black curve which is the feedback capacity is this for all um for all channels or just for the b this is a binary channel so no i mean i mean the fact that feedback strictly increases this this result can be shown for like all channels with the one infinity constraint or just uh we know at least for the binary eraser and the binary symmetric channel that's much better one more question the your result on the previous page i'm just curious is this a constructive result i'll come to this i'll i'll show you what i'm going to thank you thanks okay so i'll keep coding schemes once you know this what you're looking for coding themes are by the way completely non-intuitive completely non-trivial like but you have to know yeah you have to essentially go through the process that we went to to be able to get to those goodies all right but are completely explicit as well and once you once you see them they're mind-blowingly elegant all right um the so let's see so all of this really stems from the dynamic programming formulation of feedback capacity um and so let's let's get into that uh like see there is this is not the time listen i will only be able to give you an overview of this is not that place to learn about this this is how good this is like deep stuff not hard and like you know lots of people have spent many many years working on this type of thing and i'll just say that like our problem fits into this setting that is really all that you should be taking away from this how it fits into the setting i'll give you the ingredients of that but i don't really expect that like if you've never seen this before you will not understand what is happening in all likelihood okay so the con so this is our expression for feedback capacity now experts on them who know anything about dynamic programming you would immediately recognize this as being looking like something what is called the reward of an infinite horizon average toward dynamic rule in the stochastic control community this would be a natural thing to look at so so people did make a lot of people have made this connection now and so again starting with the work of tatikonda and a lot of people have exploited this connection as well since then and this is actually i've stopped this at 2015 there have been considered many papers since then as well i think this slide was actually drawn up around 2016. what is a dynamic program okay a dynamic program is effect just a it's a discrete time dynamical system it has a certain number of it has a certain ingredient it has some ingredients which i will try to explain so there is a discrete time dynamical system that evolves in time according to a according to this update equation zt or these z's are the states at time t okay the so there is a state evolution that happens across time it's a discrete time system and there's a state evolution of that of that of that system the state of time t is called z t it takes some it takes values in a state space z which could be continuous it could be a compact set for example and but then you can say that like it can also be random that is the initial state it will be initialized by sum z naught the initial state that not could be a random variable random state itself with some initial distribution okay then there are also the other thing is actions and what are called disturbances actions are see finally what what we want is there is something called a reward that is going to be associated with the trajectories of of the states of of this dynamical system uh at every step of the way there's a certain reward or cost associated with taking that step if it's a reward you want to maximize the reward that you get if it's a cost you want to minimize that cost it's all therefore there's two sides of the same coin now when you're sitting at a particular state at time t minus one you want to decide where to go next in order to maximize what is called the long term average cost so it is not a short term you want to not just maximize an immediate reward you want to maximize the reward over a long period of time that is you want to maximize your cumulative reward that you get over over a long period of time so you decide upon an action to take where to go next so there is a set of actions that are allowed so they take values in some action space and you can depe how you how you decide what action to take could be defined by a policy there is a particular mechanism of choosing actions which is called a policy and the mechanism of choosing actions could depend on so it depends on everything that you have done up to that point okay so it is entire history of the system up to that point the state of the system is actually encapsulated by a very simple thing because the way the system is set up so the initial state of the system and then what are called the disturbances of the system up to time t minus one so now i am sitting at a state zt at time t minus zt minus one at time t minus one i want to decide where to go next so i take an action that i believe will maximize my long term average reward uh eventually further down the line i take an action that that sets me along in the right direction i i i choose an action that allows me to go to a state that kind of is in the right direction of where i want to go in the long run however this environment nature is not allowing me to do exactly what i want it also throws in a curve it's a disturbance that like well it's some random variable but which will throw throw off the system which will not ex it will cause the system to not behave exactly as the way i intended it to that i would like it to but like it will there's some noise that gets thrown in it's called the random disturbance and that is some distribution of that disturbance as well so the actual state at time t is going to depend on the state at time t minus 1 then the the action that i take at time t which isn't up to me to decide what i want to do but the disturbance is not within my control but that disturbance is also going to be affecting the next state that the system goes into okay so there is there is a stochastic behavior that is why it's called stochastic control so this ut is a control parameter and then you you you're able to see what state you're in at time t minus where you are in you're able to see but you're not able to precisely control where you want to go next you have some lose control over it but finally there is also a disturbance that the environment actually also throws it throws at you so this is the so i talked about the reward function this reward function depends on the state you're in as well as the action that you take because actions may have costs or rewards associated with them and the state you're in also has costs the state you want to go to a state that you are in will also affect what a reward that you get next days so rewards are each at each step you get a reward and you want to as i said the goal is not just to maximize the immediate reward it's not a greedy thing you want to maximize your long term average reward okay so you want to max the goal of the dynamic program is to maximize this quantity what is called the long-term average reward every step of the way you get some reward and that you are looking at the arithmetic average as you go from you take n time steps divide by n let the limit as n goes to infinity and the expected value is because there's a lot of unexpectedness in the system randomness in the system there is random choice of uh you know there's random disturbances there is random initial state even the policy could be random so all of that gets gets thrown into this expectation so there is expectation there is an expected behavior of a long-term average reward that i would like to maximize so this is called an infinite horizon average reward problem okay and you want to choose a policy a way of choosing actions at every step of the way that allow you to that maximizes this long-term average to work so i want to choose a policy choose a policy that is a that gives me it gets me to the reward row star which is the supremum over supremum of the long term average rewards across policy spine now compare this this is directly comparable i mean now see this is this this is the long this is what the dynamic program the goal of the dynamic program is to is to get to here this maximum average reward maximize this long term average you want but then look at the compare that to the capacity expression i mean there is a one to one correspondence between these things i mean as long as you set it up in the right way you will see that there is this can be there is a way of setting up the problem of capa feedback capacity so that the ingredients exactly fall into the ingredients required for a setting for formulating a stochastic control or this dynamic programming problem that is all that we need to know from this how you do this precisely is not relevant okay so i took this to my particular talk here that there is a correspondence is what we really want to know that we need to know okay once you set it up what do we do next how do you actually get to this once you set up this problem what can you do to it how do you actually evaluate this it looks like a pretty complicated thing your policies could be complicated so how do you actually evaluate what the optimal policy is how do you evaluate what your optimal long term average reward is what the maximum average award is how do you evaluate these things there are tools for this one tool is a so-called bellman equation there's a certain operator that is a complicated operator i don't as i said this is not the place to learn about it but people who want to know this this hedge function is what is called the the cost to go function the reward to go function h is basically tells you if i'm if i'm at state z at a particular time instant and what is uh if i use from that point on if i use the best policy what is the best average reward i can get in the long term okay so that is this t of if i use so h is basically what is called the uh the long it is called the reward to go function or the or the cost to go function anyway there is a dp operator that tells that like every step of the way uh what how the long term cost function gets updated so there is a recursion that tells you how it is precisely what is called dynamic programming if you know anything about dynamic programming its a recursion dynamic programming is basically just a record it tells you how the long term average average cost gets updated once you take a single step okay that is that is basically what this dp operator is doing is telling you what the recursion is between the long term average audit at a particular at a particular step at a particular time instant and then what happens the next time instant anyhow so there is a operator that operates on functions uh from the state space to the real numbers and if you set up this highly non it's a highly non-linear operator if you set up this sort of f it's not exactly a fixed point equation but it is a what is called the bellman equation that is you look at the operator operating on a function h uh if that operator operating on the function h the reward function the long reward to go function h spits out the same reward to go function h perhaps but up to a shift okay shifted by some constant then uh this equation is essentially like a fixed point equation but fixed point but except not necessarily a fixed point fixed point would be when row is zero it's a shifted fixed point if you like uh then this equation is what is called the benmen equation so if you can find a solution to the spelman equation that is the dp operator t is just is an operator that is defined from the problem setup you can define it explicitly based on the problem set up you can set up this equation the problem is the thing is you want to now find a solution to this equation that is you want to find a bounded function a reward to go function z h and a row for which this equation is satisfied for which this update equation is satisfied this fixed point equation the bellman equation is satisfied if you're able to do so then this row that you get from the sequence from the solution to this equation is in fact rho star it is actually the optimal maximum average reward and this is one method of approaching problems like this you set up a complicated nonlinear equation to solve if you're able to solve it then you have your answer okay and yeah so i will so this is the setup okay this is how there is the correspondence between dynamic programming and capacity calculation there is there are states action disturbances in the rewards there are there are information theoretic or probabilistic you know kind of counterparts of this that like if you define this in this one to in this case if you make this correspondence between these con these quantities on the right and the corresponding ingredients of the dynamic program you get precisely the the feedback capacity formulation of the feedback capacity formulation as a dynamic program the thing i want you to focus on here is actions actions are you remember what is an action action is your particular particular state and you want to decide where to go next actions for us are going to be input distributions if these are constrained input distributions that is like the input distribution what is the distribution on the input at time t given that the state of the constraint at time t minus one is known to us okay this is exactly like you know dkrl constraints if my if if i'm sitting at a particular state of my presentation at time t minus 1 i want to determine where to go next what do i want to throw into the channel in the next time instant the this this input distribution is precisely what i require to you know to optimize over if i want to calculate channel capacity that input distribution is precisely my action in the dynamic programming formulation okay and the action i will say here is uh i allowed to depend on all the outputs up to time t minus one that is one take away from this the action that i take at time t that is this input distribution that that i want to have at time action at time t the input distribution into the channel at time t it is depends on all the outputs that you have seen up to time t minus one this is known to both the encoder and the dq okay all the output cm up to time t minus 1 would be known to both the encoder and the decoder the action is only taken at the encoder the decoder is somewhat expected to follow the the outcome of the action so let me so let's let's go on i mean i don't i just want you to get some feeling for this and i also want to get you to get i want you to get a feeling for how complicated this this thing will you know is actually is in practice so this is a dp operator for the binary erasure channel if i throw in a function h at it h of z the the what the operator throws out is some complicated function like this again a function of z but it's a much more complicated function which involves like a binary entropy function and some supreme over other there's a supremum over some optimization problem it's another optimization problem it's a complicated thing this is a dp operator so you want to set up this guy you want to say that you want to you want to set up this equation rho equal to i mean rho plus h of z equal to this okay you want to solve this equation not obvious it's not easy it's a it's a complicated nonlinear equation to solve so what do you do how can you solve something like this there are numerical methods so at least try to understand what is happening in the dynamical system you can just take this operator take some start with some you know what is called a value this is also called h is called a value function take some value function keep applying the operator to it numerically do some number numerically evaluate the dp operator and keep doing this over and over again and then you will see some sort of behavior start happening so after you do this value iteration algorithm after 20 iterations of value iteration algorithm you will see that the value function starts to look like this for the particular case of epsilon equal to point five arrays of channel it is a probability half the actions are there's a particular interpretation of this but anyhow this is numerical methods trying to determine what the value function is going to look like see you if you if you the hope is that after you keep iterating long enough you'll start to converge to something that is the value function will start to converge towards the fixed point of the bellman equation that is the hope fixed point being that is this it will get to us you will get to a value function that satisfies the bellman equation sort of after sufficiently many iterations so what you will see happening is you'll see that at some after 20 iterations the value function looks like this apply the dp operator to it again what is going what you should see next is you'll see a shifted version of the same value function okay at that point you know that then okay convergence is starting to happen and the the quantity the amount of the shift will precisely tell you what the what the optimal average reward is so this can be essentially calculated numerically but our goal is beyond that go beyond that and do this analytically as well so what happens is also another thing that we were able to see that like once you calculate once you did all of this we could also simulate the system once you are able to see what you know what what your you know from from this bellman equation numerical from the numerical method of the bellman equation you are able to get up you can get some actions optimal actions to take at every what the action function looks like action function as a function of the state you can see that also from the you can get it from this from this value iteration algorithm you simulate the dynamical system under this of action function and what you will see eventually is that the dynamical system uh starts to will see will will will settle into one of exactly three states for the particular case of epsilon equal to five epsilon equal to half eraser channel the dynamical system that the the dynamic program the dynamical system uh you know the dp uh dynamical system according to which the dp evolves settles into one of three states it just keeps settling between three states this is a rather remarkable thing because the actual the state space of the dp is actually a continuous state space is a state of it is actually a probability distribution okay the state space is a pro is a state is a space of probability distributions on input on the input alphabet but then it is saying that like you only settle on three probability distributions within that state space only three probability distributions at the input matter only three input distributions you're going to shuttle between three input distributions depending depending on which particular state of uh the dp you're in okay which is a rather remarkable thing and from this this student or on some conjured up a solution from this purely staring at this okay this is all these did stared at this used like basically reverse engineering to conjure up a solution to that bellman equation the solution looks like this it is pure guesswork looking at the shape of the curves try to do curve fitting finally you get something that looks reasonable then once you once you believe that this is a solution proving that this is a solution is not a problem you can do this easily getting to the form of the solution is completely guesswork i mean guesswork means informed guesswork based upon numerics okay and that is how we that is how this original thing was actually solved and one more thing came out of all of this that the once you know how to solve this problem you can also then from the solution the bellman equation as i said like you know the dp this the input distributions it turns out only three input distributions were only three types of input distributions were required to get to the optimal reward to get to the feedback capacity and you have to shuttle between one of these three input distributions that is one of these three types of uh constrained input distributions that you know that is these one of these three types of actions uh it is like if you if you are there are only three dp states the states are like labeled by z equal to zero z equal to one and z equal to one minus p epsilon at each state you take one of the at for each state you have these actions actions are stochastic matrices uh which depend like there are two rows and two columns because like here we're only looking at the zero one rl constraint and uh so it is all you need to know is whether the previous bit was a zero or a one to determine what the next bit is to be um if the previous bit is a one you know the next bit next bit to be transmitted into the channel has to be a zero which is always the second row the second row of this stochastic matrix tells you it it is the probability of the next bit to be transmitted into the channel being equal to zero or one uh given the fact that the previous bit transmitted into the channel was a one the previous bit transmitted into the channel was a one the next bit has to be a zero so the probability of the next bit being a zero is one the first row of this stochastic matrix is is the row that gives you the distribution of the input bit when the previous bit centered in the channel was a zero so there is no constraint anymore on the next bit to be transferred into the channel all right anyhow uh i think uh yeah i'm pretty sure i've lost most of you um let me kind of tell you that this is all very abstract perhaps but it gives rise to concrete coding schemes and i will explain the coding scheme one size but that coding scheme cannot really be derived without going through this process of obtaining this optimal policy when i get to the coding scheme the coding scheme i can explain to you by itself standalone but where did i dream this coding seem up from you would there is there'd be no basis for draining up a coding scheme like that unless you had gone through this process of deriving the optimal policy okay all right so let me i think i'm probably i'm not going to be able to get to explaining all the fine points of all of the of the policy etc but let me get to the coding scheme and the coding scheme as i said actually is a paradigm you have to think about it differently and at that time that kind of thinking only can come only is natural when you look at this ahead of on optimal policy so coding scheme is the following i mean it's a completely explicit coding scheme for a one length limit for a one infinity rln input constraint thrown into a binary area in a binary eraser channel where you get feedback from the output okay the output is fed back into the input fed back into the enquiry so the message we will take the message so you have m messages to be one of m messages is going to be picked and transfer and pick to be transmitted so the messages are uniformly distributed over a set of messages set of capital m number of messages we will consider these messages to be points on the real number like on the interval between 0 and 1. so messages are simply mapped to points within the interval from zero to one okay one of these messages is that true message to be transmitted the transmitter knows this but the receiver does not so the job of the transmitter is to convey enough information to the receiver so that the receiver is able to zero in narrow down the possibilities for the actual message being transmitted based upon the transmissions based upon what is what the receiver is seeing the advantage that the transmitter has is it is also able to see what the receiver is seeing but of course with the delay of one bit causal cannot anticipate what the receiver is going to see but once the receiver has seen what it is seeing the transmitter also sees what the receiver has seen i hope you were able to get that bit of a tongue twister there now the coding scheme itself is based upon two labeling of the intervals from zero to one you put a line at you know so labeling one gives all the messages gives a label 0 to everything to the to the into the sub interval from 0 up to 1 minus p p being some particular probability probability value between 0 and 1 which would choose to be the maximizing parameter for that optimize for this uh objective function like that came into the feedback capacity expression remember so you have you know in hindsight this is the optimizing p epsilon is optimal value for p but it this the scoring scheme will work for any value of p for to get to the largest rate you will need to get to the you have to use the optimizing value of p for this expression so pick a value of p between 0 and half and you draw a line within the interval from 0 to 1 at 1 minus p for labeling 1 and for labeling 2 you draw a line at p so the for labeling one like anything to the left of p left of the line one minus p gets a label zero and then the right of one minus p gets a label one for labeling two it is anything to the left of p gets a value one gets a label one anything to the right of p just gets the label zero these labels are actually used to determine the current bit to be transmitted by the encoder okay okay so for example always start with labeling one so initialization is always with labeling one so remember one of these message messages is the true message okay i will denote that by zero i mean by the shaded bit by a shaded circle so always initially labeling one is used since the true message falls within the sub interval from one where the label is zero so the encoder the transmitter will initially transmit 0. so their bit transmitted is 0. now let us see what is happening in the channel so in successful transmission so labeling so the dp's there were three remember the optimal policy were concentrated on three dp states there were three states for the and at each state you had a particular policy to choose at that state so this is the state the policy diagram that i showed a few slides back okay so that is what i'm showing at the bottom here so this is this red circle is the ground state this is the unconstrained state you can do anything you want in the state that is there are no constraints upon what you can throw into the channel at that state so at that when you are in this state when both encode when the encoder knows that it is in this state the decoder also actually will will know that it is in that state uh then you always use labeling one the first labeling okay to determine what is to be transmitted next so this is what the encoder has done so this is the message bit the shaded it's the shaded circle so the x1 equal to 0 is the bit to be transmitted because the message lies in sub-interval from zero to one in labeling and according to labeling one it is a zero to be transmitted um note that the optimal actions you know as i said like you see the the the action is basically the probability distribution of what you want to send into the channel at knowing that you're at this particular state so i want to send the probability so this the first of the the first row corresponds to unconstrained there is no the previous bit is zero previous channel transmission was zero so there are no constraints there are no constraints about what you can put into the channel next so the probability with which the next bit to be transmitted into the channel is 0 is 1 minus p epsilon 1 minus p and the probability with which the next bit to be transmitted in the channel is 1 is is p that is the probability distribution input distribution this is not going to play a role here because when you are always in this state in the ground state in the state here the red state only when the previous bit is a 0 the previous bit is 1 will take you to some other state um okay but this is the policy now at this stage whenever you know that you're in this state you'll always use labeling one to carry out your transmissions and that labeling just you just and all you're doing is determining where your message is in sub interval corresponding are you in that sub interval corresponding to labeling label 0 or are you in the sub interval corresponding to label 1. this is known to the encoder always the decoder does not know this supplier so what happens in transitions so a zero gets transmitted note that since all messages are equally likely the probability that a zero gets transmitted when you are in this state when you are in this by this labeling the probability that the zero gets transmitted is precisely one minus p epsilon okay i mean maybe up to rounding errors because it's a discrete probability set there's a discrete set of messages but like there are a large number of messages you can see that all messages being equally likely then the probability of choosing a message within the sub interval is basically one minus a p of one minus p probability of choosing a message within the one sub interval is t and that is consistent with the optimal action at this state at the state at the red state at the red circle state so a zero is transmitted and the zero successfully received if a zero successfully received so it is this transition the red transition that is taking place in the in the policy diagram the optimal policy diagram and so that keeps you within the same state within the same dp state according to the optimal policy so you are going to continue to use the same labeling one to determine the next transmission as well but now both the encoder and the decoder know that the first transmission was successfully a zero so all of these guys can be eliminated at both the encoder and the decoder end because that is correspond those are messages corresponding to the first the first transformation being a one both the encoder that decoder know now that those messages are out of the question those could not have been transmitted okay now so now we can focus entirely on the set of messages corresponding to the label 0. you expand that out to now you like kind of you do you zoom this out so like you expand this and to expand the set of messages to cover the entire interval again from zero to one and you use once again the exact same labeling the labeling one okay to determine what to transmit next so labeling one is determined by again setting a line here one minus p epsilon and to the left of the line if the message that you were to be transmitted now lies to the left of this line after expansion you sent a zero the next bit next transmission is zero otherwise the next transmission is the one now i chosen this black circle in such a way that after expansion on to from one from the sub interval of length one minus p all the way up to the interval of length one ah the the black message now goes to the sub interval labeled one so the next transmission is going to be label one is going to be a one so the encoder is going to transmit that next so that is what the encoder is does so time instant one that encoder has transmitted one okay what happens so suppose that one is successfully received if the one is successfully received so the encoder has transmitted one then the transition that is taking place in the dp state diagram is this guy here it is taking you to a different state now because both the encoder and the decoder know that one has been received it's a successful transmission non-erased one is received that breaks with the next state but then according to the constraint the next input into the channel cannot be a zero cannot be a one because one has been transmitted the next constraint says that i cannot transfer to one again so the only thing that the encoder has can do is to transmit a zero and no matter what the receiver sees whether it's a zero or an eraser it knows that having seen a one previously it knows that encoder must have transmitted to zero in the next time instant and then it goes back to this state here both encode and the decoder know now that after the transmission of the next zero we're back in this state okay all right um and once again once successful after a successful transmission you can eliminate the possibilities of things that could not have been transmitted and you after an elimination you'll be left with a sub interval of messages and you expand that sub interval of messages again back all the way to the full interval you keep doing this as long as there are no arrangers but once erasures happen you go to a different state you go to this third state within that dp state within the dp state diagram uh so here let us see what happens in the energy process okay so first time instant like a zero was transmitted and then it got erased okay the message the transmitter transmitted is zero in the first in the first transmission it got erased in the channel so that shows this y y one is zero but like again so it goes arrays in channel and now what what is that decoder what what what what what is then what is the next thing to do if an eraser happens according to this dp state diagram the transient diagram you must go into this the state that you get to is this dp state 1 minus t what do you do what is the action at that state the action at that state is this fellow here okay the action at that state is this fellow here so whatever you do has to be consistent with this action the optimal action at this state according to the optimal policy is this fellow here so whatever you do at this state must be consistent with that action so what is the what do you what do you do here so what happens now is that we switch labeling once you get an eraser one if you are using labeling one first and you see that you got an array at the output you switch to the next labeling you switch to labeling two okay so what happens then is the following see if you what what what that means is the following you switch to labeling two why do you do that it is to keep yourself consistent with the optimal policy labeling two tells you to transmit a one uh if your message now lies in the sub interval uh the first sub interval of length p or a transmitter zero of the message now lies in this next the remaining part of the sub remaining part of the interval from zero to one of length one minus p this is consistent with the optimal policy why is that the case see this remember the rows of this matrix of this optimal policy matrix correspond to the conditional probability of what the next thing to be transmitted into the channel is conditioned on what the previous thing transmitted was the first row is the previous thing transmitted was a zero the second row is previous thing transmitted was a one the previous thing transmitted was a one you must always transmit it to zero in the next time instant the previous thing transmitted was a zero then you have some freedom in choosing in determining what to transmit next with probability p epsilon over 1 minus k epsilon you can transfer to 1 with the remaining probability you can transfer to 0. so in the previous time instant if you are transmitted as 0 if your message was such that you transmitted a zero then you want to make sure that in the next time instant your transmitter one conditional probability of transmitting of one is p of p over one minus t and that is that is given by that is consistent with this labeling this labeling makes you allows you to do that because among all the messages among all these one minus p fraction of messages lying in this interval a p fraction a p further fraction gets the next uh gets the label one in the next labeling the labeling two and that is this that gives you this problem so what is the what is then the conditional probability if you know in the previous transmission if the transmission was a zero what is the conditional probability of transmitting a one that is given by p epsilon over one minus t epsilon consistent with the optimal optimal policy at this um at the area dp state dp state corresponding to an eraser received okay and uh so this is how these two labelings are basically used or defined and what happens next now again if you get a second eraser you flip back to the original labeling labeling one again and the labelings have been chosen in such a way as to keep yourself consistent with the optimal policy if a 0 was transmitted in the previous time instant then the probability with feature transmitter 1 in the next time instance is consistent with the optimum policy if a 1 was transmitted in the previous time instant as would have happened here x2 is 1 then you are forced to transmit a 0 according to labeling 1 again because that message the sub interval now i mean after a 1 you must necessarily transmit a 0 when if you get an eraser okay so as long as you get erasers you keep flipping between these two labelings and you keep doing this as long as you get any eraser as soon as you get a successful transmission then you get into one of the other two dp states a successful transmission will get you to one of the two other two dt states or which i've already described what to do if you're in the dp state called one you always use labeling one thereafter for the next time for the next transmission if you're in dp state what is called zero that you can only get to that dp state if the previous transmission was a one in which case you must transmit to zero and the receiver also knows that okay this is the policies of coding scheme i mean it's a completely out of completely out of left field and without knowing the optimal policy there's no way you could have come up with such a good instinct and this is an optimal coding scheme if you actually you can carry a rate anal carry out a rate analysis of this you will see that like if you define the rate to be the expected number of information which per successful transmission over the expected number of channel uses because it's a variable rate coding scheme expected number of bits per how many information bits are there for successful transmission see at every time instant you're transmitting either a single bit okay and you want you want to know okay and what is the probability of that bit being a zero or a one either according to label zero or label one the probability of that b being a zero or a one is always p epsilon because how many are the fraction of bits the fraction of messages because all messages are equally likely fraction of messages in each of the labeling the fraction of messages which get the label 0 which get the label 1 is p so at every time instant the probability of transmitting a p unconditional probability of transmitting a peak an unconditioned or whatever happened in the past unconditional probability of transmitting a 1 is always p so you're always transmitting a bernoulli p bit so if that p bit gets through in a successful transmission the amount of information that we have conveyed in that successful transmission is just the entropy of that bit so that is the entropy that's a successful transmission contains h of p information bits on an average you can this is intuition you can actually make all of this eager if you like i mean it is all explained in the paper if you want to read it up okay that is number one number two the denominator how many channel users do you need for successful transmission well it is the same geometric series you keep getting erasers until you get a successful transmission it is happened with probability epsilon you have to you might get a chain of success you might get a number of iterations you have geometric progression of images until you get a successful transmission so the number of transmissions up to you get up to the point that you get a successful transmission on an average the number of transmissions required is one over one minus epsilon but then there is a caveat uh if the last guy that you successfully transmitted turns out to be a one if the one got successfully received then you know automatically that you have to waste an additional channel use to satisfy the constraint the next channel used must be a zero and that's a waste of a channel it's a waste of a channel use that channel use will convey no additional information it is just to satisfy the constraint because the previous bit that was transmitted was a one the next bit to be transmitted has to be a zero so you waste that extra bit with prob so with probability t you will waste that extra extra bit so the expected number of channel users is one over one minus epsilon but with with probability p there is an extra channel used so which is which you get added to the expected number of channel users all right and you therefore the rate is exactly the top the the objective function that we had in the feedback capacity expression and like you when you pick your probability p so as to maximize this objective function and that gets you the capacity and i think i might as well stop here like with the rest of it like i mean you know i i there's a lot more to go but like we can extend all of this like you know these ideas basically are extendable once you get this idea the main takeaway from this is the following that dp policies when solving the dp gets an optimal policy from optimal policies we can get coding schemes for the binary binary eraser channel these type of coding schemes are based on labeling of the interval from zero to one for the binary symmetric channel coding themes are more complicated they are based on what is called the principle of posterior matching i i may not be able to get to that but there is some there is other things so let me just give you sort of what else we know about this i don't want to get into any more detail about what techniques what are techniques we use what else do we know about the feedback capacity problem alone we know for example that for the 0k rll constraint the exact feedback capacity for the binary eraser channel is known there are similarly there are similar expressions involving optimization over a single parameter single real variable as long as the input constraint is a zero krl or one two rl constraint so this was in the paper of pallet at all and we have now also d infinity is actually still open by and large uh we know we have an answer partial answer for d equal to two and small epsilons within a particular range arrange a probability within a particular range so there again we know that uh what the feedback capacity is it's again given an expression like this um yeah so these are some plots compared comparing our feedback what we know about the feedback capacity for input constraint dc's for the d infinity case there are upper and lower bounds which are all pretty close together so we know a fair amount but we don't know it exactly yet beyond the bec for the b binary for the general binary input binary output channel auron zabag actually solved the dp for that and i i will assure you that that bp is far more complicated for the dp4 for the bellman equation for the for the binary erasure channel look at the solution it is a it's a crazy solution this is complete it was just pulled out of thin air and um yeah so the feedback capacity is given by this again it's a single parameter optimization problem once you know what the optimal policy is again based upon solving the bellman equation dp etc you get an optimal policy once you know the optimal policy is from that you can conjure cobble together a coding scheme in the case of the binary for the case of a bibo channel binary input binary output channel the coding scheme is based on the principle of what is called posterior matching yeah so this is like these are the curves that look for the binary symmetric channel the capacity curve looks like this for any for the one infinite rll input constraint the capacity curve the red curve is an organ this is just the one minus h of p curve the unconstrained capacity constraint capacity is this guy here this time it is con it's a context shape and again we know that here in this case the feedback does strictly increase capacity the the bounds the black curve is the feedback bound the feedback expression expression of feedback capacity and the other curves are upper and lower bounds on on the non-feedback capacity of the binary symmetric channel with a one-infinity rll input constraint as you can see there's a gap um i i guess i'll stop here with maybe with one comment that like uh see for in the in the capacity without feedback case we have very little we really don't know much we can know we can get lower bounds and upper bounds lower bounds are almost invariably obtained by picking your favorite type of input distribution input processes that respect input constraints typically markov processes and try to evaluate throw that markov process into the champion throw our markup process into the channel and try to evaluate this information rate either do it numerically or somehow analytically using bounding numerical methods are monte carlo simulations this generalized they had a remote algorithm stochastic approximation analytical methods gives you bounds give you bounds like so for example we know that for the dkrl input constraint binary binary erasure channel with asia probability epsilon the feedback the non-feedback capacity is always lower bounded by the noiseless capacity times the capacity of the aeration channel so i don't we don't know if actually this is not an equality we know that it is lower it's a strict lower bound we know that too but what what the actual value is for cdk epsilon we don't know we can't even extend this kind of inequality beyond the binary erratic channel i don't know that such an inequality exists it has been conjectured by zahavi and wolf back in 1988 that even for the binary symmetric channel you should get something like this that the capacity of the constraint for a noisy binary symmetric channel should be lower bounded by the noiseless capacity of the constraint times the capacity of the binary symmetric channel we don't know if this is true we don't have a proof upper bounds well feedback capacity is upper bound so that's another good reason for look at feedback capacity feedback capacity gives you concrete upper bounds uh and the other thing is this dual capacity method that yeah which is something that you ought to know about but unfortunately i don't have time to get into but yeah and one thing is that none of these methods give insight into coding schemes so this is actually an open problem still coding schemes for these type of questions with non-feedback coding schemes without feedback non-random without more explicit coding scheme than some random coding scheme um so yeah so these are really wide open problems and like and three another class of really wide open problems on which almost zero is known nothing is known as how to handle for example two dimensional input constraints so with that i will stop and like i hope you go i got conveyed something out of this it is not material that you will ordinarily see i think but i think it's yeah it's it's it's uh yeah i would say it is it is kind of interesting at least that you require non-conventional techniques to handle this type of problem okay all right thank you thank you nabin ah this is a very interesting talk um yeah before i ask my questions does anybody have any of the audience have questions for professor kesha uh hi nevin uh thanks for the yeah uh great talk yeah so you're sharing with us a very nice and comprehensive work on the design of this uh rlr code for the bec channel with risk feedback feedback and result feedback so it seems it's very interesting a academic problem but i am not very sure for for such kind of constraint called miss feedback so yeah it seems it is very practical because it seems to me it may not be applicable for data storage channels because if you have feedback during the encoding and decoding problem uh the process right you may have the problem of error propagation okay because your encoder decoder are working based on the assumption that the previous transmitted bit is successful right but we know the channel will always have noise and interference so there's one problem another problem is a delay yeah well okay so yeah yeah thanks for the question and yeah well i'll address it like there are many different questions there well firstly feedback is not you know feedback is actually not in like the encoder and the decoder are seeing the same thing so the noise does affect whatever the noise coming out the feedback is the feedback the output into the input it is not only for successful transmission it is so whatever is being seen at the output is being fed back into the input okay but your input is working based on the assumption that what you assume is ever free right no no no no no not really not no no no no no what you receive is what you receive but that is being faithfully conveyed back to the encoder saying this is what i have received that is what the encoder is operating under so the feedback is noiseless not the feedback what's this feedback yeah okay okay so so your feedback can can be can be erroneous that may have whatever that is correct fair enough that is true that is number one that is of course true so that so this will not and that that's another very difficult problem noisy feedback it cannot be handled through these techniques that is not that is one thing um yeah so that that is correct uh what is what else did you say and what else did you ask me um sorry either possible delay delay yes delay is another thing that this does not this this does not handle uh this formulation does not handle by this formulation yeah does not handle delay uh that is correct um um okay so like so from a practical standpoint you mean like if i yeah for from from a from a practical meaning if you want to use this to divide device coding schemes for use in practice maybe not not very useful but what i would say what the most important thing from for this for studying the reason for studying feedback capacity is that it does give you upper bounds on things that we don't know anything about that is the feedback that is capacity without feedback this is yes yeah yeah and these are non-trivial upper bounds and upper bounding techniques are actually difficult also for this problem yeah lower bonding techniques are okay i mean you know you feed in something you know it's not it's a supreme it's not it's a maximization problem so you feed in anything achievable is an upper the lower bound how do you come up with upper bounds upper bounds are hard to come up with because converses are generally trickier so there are these two techniques upper bounds either one is the feedback capacity upper bound or the so called dual capacity upper bound the world capacity upper bound is generally tighter than feedback capacity upper bound but feedback capacity of a bond is also it's it's a cleaner formulation let us say feedback capacity upper bound it can be there's a well established method for getting to that upper bound yeah yeah true it's very interesting yeah problem yeah and also it's tough problem yeah naveen can i ask you a question um typically when when you prove these coding theorems um i mean you have this idea that the block length goes off to infinity but you showed that when you have feedback you have constraint sometimes you don't have to have any of these fancy coding schemes right does that does that hold true for all the known results for feedback uh constraint channels yeah yeah yeah yeah yeah like the feedback so after infinity right yeah so at least like you know so this like there is this well known uh not well well known let us say within the particular context of if you look at memoryless channels unconstrained memoryless channels with feedback there is a coding scheme this posterior matching or like this business of posterior matching this is a well known i mean this work goes back to the work of horsteen in 1961. so from 1961 onwards this has been known that there is a particular way of doing it it is kind of it is sort of reminiscent of if you like arithmetic coding i mean yeah i don't know i can't i don't want to explain it beyond that it is kind of keeping track of posterior distributions okay depending trying to determine what to transmit next into the channel the based upon updating of your posterior distribution of the input message condition what we have seen in the past so that is called posterior matching uh there's a way of doing that there's a recursive way of doing that and it it's a very once you see what is going on it's a very simple coding scheme and like it achieves capacity directly to prove that it does is non-trivial but it achieves capacity doesn't require uh infinite block and codes nothing like that no infinite block link means like yeah so these are variable length coding schemes so yeah you know so if you keep um you must keep an error budget so you can you have to do one of two things either let the variable link you cannot you cannot have any bound on that on the length that you require to get a particular bit to get through yeah right like i did in the eraser in the first case how many times i need to transmit the array a bit until i'm sure that it has successfully gotten through you know in the case of in the original like the the motivating example that i gave yes here right this here like you keep repeating until unsuccessful until you get it through i mean here you don't keep a bound of total i mean i can't bound i can't say that like you know i can only use the channel x number of times to me until i am sure that this gets through so either you have to keep if you want to keep a bond then you must have an error budget yeah because there's always a chance that like you know your arrangers will sort of you'll keep getting you're getting you'll get desperately unlucky and you will not be able to get this through in that within that particular error budget okay so your variable encoding seems already already have that so either you you can keep a tie you can ask for like you stick to a particular block length but then you will get a small amount of error yeah but if you allow for variable length then yeah you i mean it's not infinite block length per se but there's no bound on the block link but your your multi-letter capacity regions uh capacity expressions all are asymptotic in nature correct it's just that the single letter versions for the results we know somehow um are quote unquote practical uh angela what you said about um the stochastic stochasticity of the network yep right okay thank you yeah okay so i have a question uh so uh what was the main obstacle in using the dp method for channels with uh feedback because it's just a matter of omitting one one parameter of the the the memory no the feedback of the output range i mean without feedback it doesn't apply the method does not apply uh the i mean the dp need not i don't yeah the problem formulation is no longer like fits into the formulation of dp it just does not um [Music] i can see if i can say that [Music] yeah okay um yeah yeah so this like um um um yeah see i mean like okay what like uh i guess let me um the so these type of problem formulations are based upon the fact for dp it is kind of like these uh this particular reason that dp works here is based upon the fact that there is something common that both the encoder and the decoder c okay it is they all both of them see the out not only do they have a common knowledge of the coding scheme both of them have common knowledge of all the output seen up to time t minus one okay and that is crucial in making sure that the encoder is able to develop a simplified coding scheme that allows the decoder also to follow the encoder you see so okay so i guess somehow your german equation can be no the you can't set up the problem itself the problem itself cannot be set up in the form of a dp like for for the non-feedback case actually i thought if you have a finite state channel you can you can set up the dp now i mean like like i see the finites like a finite state channel no you can't the reward functions etcetera well yeah yeah i think the reward function the reward function cannot be set up in the way that you need the reward function to be set up yeah i think i can somehow see [Music] yeah so i mean like i said you see the the thing is that like this this mutual information term cannot just be split into something like this terms like this that x t and y t conditioned on things that you see in the past this x and y n term you will not be able to do this kind of a causal conditioning okay yeah yeah okay so that's the problem the main problem is that yeah the causal conditioning will not be possible okay i see okay i see [Music] okay that's interesting so so as your as your as your [Music] for your rll if you're when your d i think i i guess when your your constraints get more complicated your number of output space also uh increases i guess exponential sorry can you say that again so just how you show for the one infinity you need three three different yeah spaces okay uh i don't have any i don't have anything else here right okay yeah that's a very good question but okay let me go one i'll come back to this but let me so while i was thinking about what you were asking earlier uh there's one more thing that i will say in this paper that uh so we studied this like in like in the paper the i sata paper of 2020 that i submitted like we did look at dynamic dp for capacity of non-feedback capacity of finite state channels you can't do it by itself okay we can't like by itself it cannot be the capacity itself cannot be formulated as a dp but we can formulate lower bounds on capacity as a dp we tried that we didn't really we got some things but like it was not nothing that gave us really any serious improvement what was over what was already known ah yes yes yes okay yeah yeah sure yeah because you can come out coding skills from there just to kind of ensure this uh yeah but they'll be far they're not they won't be capacity achieving yes so that is our goal in fact that is a goal for us in fact is can you do a good lower bounding can you do can we at least get to good coding schemes based upon lower bounds through dp and he based lower bounds okay so that is something that inactive that we are actually actively working on we have i see yeah sure i mean you also have this paper on upper bounds right using dp and not uh yeah that's right so that is yeah that's a funny there's a funny use of dp uh so that uses this that is this paper of light hell i mean this is all it is it's actually using the dual capacity method to upper bound but to evaluate the dual capacity so you use the dp yes okay sure okay make sense yeah okay thanks yeah so it's not bp i mean yeah it is to evaluate to calculate a certain dual capacity you calculate a certain expression that you get in the upper bound you use dp so you can do that like you know so that type of thing one can do well what you're saying is that the problem itself doesn't admit itself to a dp formulation but like you can bound it number one below by problems that do admit dp formulation got it okay thanks okay cool okay so the other question what was your question okay yeah so um says asked about like the how complicated does that become yeah so i'll tell you that uh uh um yeah so for these at least for the particular like the the problems for which we know answers uh like the zero k rl and one two rll i think uh the zero krl i think it's a fake i'm not completely mistaken it is i think the number of dp states required is of the order of k only k may be k plus 1 or k plus 2 number of states needed in the optimal policy so 1 2 anyways a specific number doesn't matter for d infinity the infinity we are trying we are seeing that d infinity it looks like there's an exponential blow-up okay that is we require i mean the the lower bound that i gave here uh used a policy uh for which the number of dp states number of states in that policy is basically number of different distinct input distributions is of the order of 2 to the d plus d d plus 2 to the power of d okay okay and we don't have the exact answer and so but we expect that it is of that at least something like that is d plus 2 to the d is what you will need and in fact we don't even know if it is necessarily a finite state optimal policy because i said the optimal the state space itself is a comp is a continuous state space so policies can actually take and take on any value within that state space but then it turns out somehow that like in these problems it just happens to collapse into a finite set of uh you know states but that is a another set of another problem that we have been that i've been i guess mulling over for the last many years or without any insight i've asked experts also why does this happen that why does it complicate it why do why does the dp which you define start off by defining it defining it you define the dp on a continuous state space then it turns out that under the optimal policy it just collapses into a finite state machine interesting so when when does this happen i mean can you so and like is this always true for these type of problems or why can we say something about it we don't know actually we don't know anything about that okay all right all the problems that we're able to solve exactly that does happen interesting yeah all right so um uh do we have any more questions okay all right then okay okay all right so let's uh thank our speaker again um crap okay thank you thank you thank you uh okay all right so our next talk will be uh two weeks later cause is the lunar new year yeah so um two weeks no three weeks later yeah so so on the 23rd of february all right so till then um yeah for those who celebrate lunar year happy new year and uh yeah stay safe and healthy all right thank you thank you thank you everybody thanks happy new year thank you 
