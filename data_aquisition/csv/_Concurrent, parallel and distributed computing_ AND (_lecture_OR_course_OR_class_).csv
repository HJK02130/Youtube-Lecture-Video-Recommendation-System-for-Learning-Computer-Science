id,categoryId,description,publishedAt,title,thmbnails,channelTitle,duration,caption,viewCount,likeCount,dislikeCount,favoriteCount,commentCount,subtitle
RI_LUXqdT8w,22,#Concurrent#parallel #processing #explained #with #example #it #lectures #karanjetlilive #tutorials,2019-02-25T17:28:35Z,Concurrent and parallel processing explained with example,https://i.ytimg.com/vi/RI_LUXqdT8w/hqdefault.jpg,Karan Jetli Live,PT4M40S,false,3601,83,5,0,9,Karen check me life hi guys welcome to my channel and after a long break we are starting with video tutorials again so today we are going to discuss concurrent and parallel processing we will discuss what is concurrent processing what is parallel processing and how they are different from each other so guys all of you for the full lecture all of you stay tuned now coming to the point now what is concurrent processing so there is in concurrent processing when multiple processes or threads they run at almost same time then we call that type of processing as concurrent processing okay for example we have a program a and we have program B or we have a process a or process B okay so now all of you know that is like what happens like V as a user B like the program the operating system okay it divides the program into processors okay that is transparent to us and each process can be divided further into very small lightweight process we call it as thread we call it as third suppose we have a processor a and B so those processes can be divided into small small lightweight processes we call them as text or I can call it as thread E and I think of it as thread B now as in concurrent processing both of these threads they can run almost at the same time okay the boot can be executed and the C okay this type of processing is called as concurrent processing and in concurrent processing was a and in concurrent processing guys I have the segment of saying boys and girls in concurrent processing so the main thing is these facts they can run almost at the same time but they run on a single essentially processor they run on a single processor so you can create processing okay when we talked about concurrent processing the processor is Silla the processor is single and on this single processor multiple threads can execute at almost same time no that is why I say almost same thing because they do not execute exactly at the same time okay but there is a context switching between these first processor processors this then this the gap the time gap between switching from here to here is so small that to user it appears they both are running at the same time so this was concurrent processing okay then comes a parallel processing noise in parallel processing also we have the same thing we have thread a we have thread B okay so they both are executed at the same time so the difference between concurrent and parallel processing is here we have two processors we can have more than one processors and these threads they can run on separate processors they can run on separate processors okay so this thing is called as parallel processing tablet processing can be concurrent okay when we have more than one processors which can execute the threads okay at the same time we call it as parallel processing but we have single processor when we run multiple threads on the single processor and almost same time we call it as concurrent processing so what is the main difference what is the similarity the similarity is there they almost run at the same time here they can run at the same time here the processor is one and here the processor is more than one so this is the difference between concrete parallel processing okay and with guys when we have this concurrent processing with concurrent processing the main advantage of a test we have high-performance system okay when multiple threads can execute at same time okay automatically the system throughput and the system performance it goes up but also it leads to another problems which we are going to discuss in the next lecture so here is all of you if you like our lectures please subscribe to my channel and all of you thanks for watching 
ajjOEltiZm4,28,"In this bonus video, I discuss distributed computing, distributed software systems, and related concepts.

In this lesson, I explain:
* What is a Distributed System
* Distributed Systems Components
* Characteristics of Distributed Systems
* Distributed Computing Concepts
* Types of Distributed Systems
* Why Building a Distributed System
* Issues of Distributed systems
* Advantages and limitations of Distributed Systems


This video is part of the Software Architecture Series,

Software Architecture & Architectural Patterns
https://youtu.be/lTkL1oIMiaU

Layered Architectural Pattern:
https://youtu.be/BCXcIllT7Lc

Microservices Architectural Pattern: 
https://youtu.be/8BPDv038oMI

MicroKernel Architectural Pattern:
https://youtu.be/h3icQDMRLd8

Service-Oriented Architectural Pattern:
https://youtu.be/jNiEMmoTDoE

Event-Driven Architectural Pattern:
https://youtu.be/gIL8rW_eyww

Enjoy!",2019-12-23T07:42:24Z,Distributed Systems | Distributed Computing Explained,https://i.ytimg.com/vi/ajjOEltiZm4/hqdefault.jpg,The TechCave,PT15M19S,false,53007,919,28,0,35,hello geeks this is me a year from a bonus video about distributed systems in this video I try to provide a concise overview about distributed systems and distributed computing as an extra video listen to the architectural patterns series so without further ado let's dive in into the topic first let's explain and clarify what a distributed system is and what it's not but before that let me tell you something that you may not be aware of nearly all the software we use today is to an extent distributed or involve distributed computing how you're gonna know the answer in a bit for now let me just give you some examples distributed systems or distributed computing is all around us Google search engine Amazon platforms Netflix blockchain online gaming money transfer and online banking and the list goes on probably the most straightforward and the simplest example of distributed systems is the client-server model which I assume you are familiar with and if not you can check my short video about it I'll go back to this model for the sake of demonstration later for now let's go back to the fundamental question and see what exactly a distributed system is a distributed system is a collection of separate and independent software or hardware components called nodes that are linked together by means of a network and work together coherently by coordinating and communicating through a message passing or events to fulfill one and goal nodes of the system could be unstructured or highly structured depending on the system requirements in any case the complexities of the system remain hidden to the end-user be it a human being or a computer and the whole system appeared as a single computer to its users alright let me repeat with the written words now a distributed system is a collection of separate and independent software or hardware components called nodes that are networked hidden work together coherently by coordinating and communicating through a message passing or events to fulfill one and goal nodes could be instructed or highly structured depending on the system requirements and the complexities of the system are hidden to the end user making the whole system appear as a single computer to its users so basically it's just a bunch of independent computers that cooperate to solve a problem together I know it sounds simple but it's a hell of a world under the hood before we continue though I just want to say that two programs communic with each other on the same computer is not necessarily a distributed system even though they work together to achieve the same goal a client-server model that uses the same computer is not a distributed system this is important to know of course there is the exception of parallel multiprocessor computers but for the sake of simplicity in clarity it's not tackle complicated examples that's because for systems be called distributed as opposed to centralized or parallel the following conditions needs to be true first no shared clock computers have clocks also called timers which are critical electronic devices that keep track of oscillations and they help the computer to have its own notion of time this in turn helps to determine the order between events and regulates at the time and speed of the computer operations if two programs communicate using the same computer they basically have the same clock this is as to another requirement for distribution systems which is that each element in this system has to have its own processor and harmony is achieved through coordination and synchronization the second principle is no shared memory this is another key feature of distributed systems this means nearly each process has its own independent memory to work with and States is distributed throughout the system concurrency is another important characteristic of distributed systems which means that software and hardware components of the system also called processes are autonomous and execute tasks concurrently and last but not least heterogeneity and lose copy which means the processors are independent and separate from each other and they have different speeds even though heterogeneity in loose coupling are not necessary almost invariably in nodes in distributed systems run different operating systems and components can be built with different technologies and run on different platforms before moving on to basic concepts about distributed computing I just want to mention a couple of notes first distributed system is a dynamic system that allows computers or nodes to join and leave at well this has many advantages as we'll see later but also it introduces some challenges in overhead such as in case of open distributed systems security issues and the extra work of managing the organization and membership of nodes the second thing I want to say is the nearly all existing large distributed systems especially modern ones are overlaying networks what in the universe is an overlay network and not real a network is a virtual network that was possible to build thanks to an underlying network infrastructure but simply an overlay network is just a network on top of another network for instance PHP networks such as blockchain and BitTorrent and overlay networks that is to say there are networks on top of the Internet voice over IP is another network over the Internet so it's also called an online network ok I hope by now you have a fair idea about what a distributed system is and can tell whether system is distributed or not good now let's talk about some important basic concepts about distributed computing okay so distributed computing is a type of computing over distributed systems right so this means that distributed computing is more than distributed systems it's a broader term and it is concerned with building and establishing computing models for distributed systems and working out algorithms to solve problems related to such systems cloud computing is a good modern example of distributed computing other examples of distributed computing solutions are platform as a service infrastructure as a service server LS etc now we will keep things simple and don't call me about basic distributed computing concepts all right so one of the very basic concepts that you should know about is the notion of a node and notice the software or hardware components that has its own processor a memory and is able to communicate with the rest of the system nodes form open groups that is to say a network of nodes that is open to the external world and thus join and the network is easy and also external entities can communicate with the system easily the internet is one big giant distributed system that falls under this category nodes can also form closed groups which are restricted in terms of membership authentication and resource accessibility internets an example of that of course nodes can communicate via messaging mechanisms such as RPC calls brest services etc another important notion is what is referred to as a resource a resource is an asset in the distributed system that could be accessed remotely by nodes in the network or users of the distributed applications that are over the distributed system resources can be virtually anything such as files services storage facilities other networks etc basically a resource is anything that a node can take advantage of or use distribution transparency is another important concept which denotes that everything that happens under the hood should stay invisible to the end users of the system in other words the dispersion of resources the failure of nodes and fail overs migration and replication operations etc all should be invisible to users of the system the way to achieve this is through an important component called the middleware and speaking of which a middleware in this context is a distributed systems layer that connects the nodes together and make them appear as one single supercomputer it is a logical layer on top of the whole system think of it as an operating system that runs over the nodes collectively once this layer is established distributed applications could be built and run over it this middleware layer manages resources provides communication and security services handles barriers and the other complexities of distributed computing etc next is concurrency we have talked about this concept before in the characteristics in simple words concurrency is the fact that multiple operations and activities are executed in parallel these activities can interact among each other to perform a particular operation in this diagram for example a simple distributed program is visualized we could see that in phase 1 task 1 and 2 are executed in parallel and there is a simple interaction between them phase 2 has three operations in parallel in one cooperation between two of them and as we saw earlier concurrency is an intrinsic property of distributed systems all right coordination and synchronization these are two important concepts that tackle the problems of no shared clock between processes and also they solve the problem of data corruption and inconsistency if two entities try to access data at the same time coordination ensure the smooth collaboration between operations and activities and help achieve agreement among them synchronization on the other hand orders and controls access to shared resources the next concept is the architectural model the architectural model dictates how the nodes in the system are organized it defines the structure of the network as well as how knows communicate and interact the architectural model is extremely important in distributed computing for the sake of better management of the complexities of such systems and also for ease of maintenance mainly the architecture is needed for software components and that's because as we said earlier most distributed systems are overlay networks the last concept I want to talk about is global state global state in distributed systems is the union of the states of the separate processes it is sort of a global view of the system that describes its properties at a particular point in time it's sort of the equivalent of a global objects that contains all the global variables used by the software system now why not use one single supercomputer that could do everything we want and save ourselves the trouble why use instead of that several computers and add the overhead of managing and maintaining them that's actually a good question and the answer is that the motives to use a distributed system instead of a single computer are many one of the most common ones is the need for a solution where we need to reach a consensus among parties that are dispersed geographically examples of such cases are online banking blockchain barian platforms etc resource sharing is another reason why we need distributed system such as databases or distributed file systems another example that falls under the same reason are peer-to-peer systems like the BitTorrent Network etc another case where we need a distributed system is one we can not or it's inefficient to replicate data using one computer also when scalability is of crucial importance a distributed system is probably the optimal solution of course there are other reasons why one would choose a distributed system such as availability reliability etc so these are the main reasons why one would prefer to go for a distributed system related to the motives for using such systems are the advantages of distributed computing but before talking about the advantages and challenges of distributed systems let's briefly talk about their types when talking about types of distributed systems or any kind of systems it's important to understand that classification depends on the context and on the level of complexity we're taking into account are we talking about architecture are we classifying the system based on the end goal is the context related to the topology is it related to how couple at the nodes are etc in this section I'm going to address two types of classification the first is general and relates to the type of coupling and scale and the second is related to the architectural model of the whole system when talking about distributed systems in terms of scale there are two main types of distributed computing cluster computing and grid computing in cluster computing systems the underlying infrastructure is composed of identical computers that are closely connected and the management is local and centralized cluster computing is used to achieve high performance and minimize downtime grid computing on the other hand is a type of systems in which heterogeneity is the norm in terms of hardware software and technology nodes of dispersed over a very large area and administration is decentralized such systems are used when a large repository of data is involved and a lot of computing power is required all right and if we're talking about architecture styles on the other hand distributed systems fall under one of the following categories first layered architecture in this architectural model nodes are grouped into separate layers each with a specific goal to achieve an example of a simple layer distributed system is the client-server model next the object based architecture knows in subsystems are less structured and loosely coupled than the layered model communication is a synchronous and the elements of the system can directly interact with other elements through direct calls the third architectural model is the data center architecture in such systems nodes communicate through a common repository the system is based on a data center through which trimeric communication happens and the last one which is more common one is the advanced driven architecture model eventual driven systems achieve their goals by means of events nodes communicate and perform operations through the propagation of and the reaction to events feel free to check my video about the event driven architecture pattern to know more so these are the main architectural styles for distributed systems in a nutshell all right and let's talk now about the pros and cons of distributed computing as always let's start with the advantages the first advantage I want to talk about is reliability a defining characteristic of distributed systems is the reliable interconnection and cooperation between nodes in the whole system this makes it easy to share data between node another great advantage is scalability in fact this is one of the main reasons as to why someone or an organization would opt for a distributed system in distributed computing scalability is a matter of adding more nodes to the system at the correspondent layer or level or what is referred to as horizontal scaling and common advantages for tolerance it means the system and its services will still be operational and reliable even when parts of the system goes down of course as we said before resource sharing becomes not only possible but also easy in subsistence and last but not least increased performance but as all matters of life distributed systems are not a silver bullet and actually talking about the downsides of distributes computing is a separate topic in itself first failure detection failure detection is almost impossible in distributed systems especially if the system is large and evolves over time very elaborate measures need to be taken to mitigate risks of this issue redundancy is a common issue among distributed systems as well another challenge or problem is the difficulty to achieve consistency among nodes and finally performance bottlenecks is a serious issue as well actually when it comes to distributed systems the more we design for failure any kind of failure at all levels the less problems we will have this leads us to the final section issues and considerations of the most important pitfalls when building distributed systems is the four assumptions made by developers such as the network is reliable and secure or the topology of the underlying network doesn't change or that latency is zero this leads to a poor and fragile system that breaks easily and actually one of the crucial principles developers and architects abide by when designing and developing distributed systems is the design for failure principle under this rule there are a lot of considerations to take into account but generally speaking designed for failure is expecting the worst-case scenario in each and every aspect when the system is operational and planned for that all right this should be enough for a brief overview of distributed systems the last thing I want to say is that unlike distributed systems nodes in a network are explicitly visible and have to be explicitly addressed also a network in itself doesn't have to have a specific end goal and it's usually not robust and secure as a distributed system that's difference between a distributed system and a network alright I hope you've enjoyed this video and learned something new don't forget to subscribe until the next video stay tuned 
-bVg1S6Wp9A,27,compare parallel and distributed systems in OS,2018-04-20T16:28:55Z,Parallel Systems vs Distributed Systems | OS | Lec-7 | Bhanu Priya,https://i.ytimg.com/vi/-bVg1S6Wp9A/hqdefault.jpg,Education 4u,PT3M52S,false,50223,332,50,0,19,hi students so we are discussed I bought the parallel systems and the distributed systems now let us see what is the difference between the parallel system and the distributed system so let us take some resources like so if you take memory what is the difference between the parallel system and distributed system if you take the resource as a control and if we take it as a processor interconnection processor interconnection so what is a parallel and distributed system will be different shape manner and the main focus of parallel and distribution now let us see so if you take the memory the parallel system is it tightly coupled it is a tightly coupled system because there is a shared memory so whereas in distributed distributed is a loosely coupled loosely coupled system because there is a distributed memory there using distributed memory means each system is having their own memory next coming to this control if you consider a control the parallel system will have them global clock control so there is a global clock control so all the CPUs will share only this clock control means it is representing globally whereas an distributed system there is no global clock control because because each system is independent to other so all the systems are independent so that's why each system there having their own clock control so industry system there is no blog global clock control next coming to the process processor interconnection in parallel system the processor interconnection is in the order of terabytes overdraw TB bits per second there are bits per second whereas in distributed system the order of Giga bits per second and what is the main focus of the parallel system the parallel system main focus is on performance performance of the system and the scientific computing scientific computing now coming to the distributed system what is a distributed system main focus the distributed system main focus is on performance only but the performance is on cost and scalability it focused on Costin scalability and the distributed system main focus on rehabilitation so this is the difference between the parallel and distributed so in terms of memory control processor interconnection and the main focus of parallel system and the main focus of the distributed system thank you 
CA5ZJu4Trzg,22,,2017-09-18T17:22:12Z,"Parallel, Concurrent & Distributed Programming in Java Specialization",https://i.ytimg.com/vi/CA5ZJu4Trzg/hqdefault.jpg,Rice Online Learning,PT1M31S,false,10090,148,7,0,N/A,hi I'm Vivek Sarkar professor of computer science at Rice University where I lead new research on all aspects of perils of Berkshire's and uphold our departments tradition a wearing distinctive Hawaiian shirts the three courses in the specialization explored the fundamental concepts of parallelism concurrency and distribution in the context of the Java eight standard in the course in parallelism you will learn about common patterns for tasks functional loop level and dataflow parallelism the course on concurrency covers threads and locks isolation actors and concurrent data structures and in the distribution course you will learn about the distributed MapReduce LAN server and message passing patterns as well as combining them with multi-threading you may take the three courses in any order to complete the specialization but they are available individually as well the material for these courses has been developed and updated at Rice over the last several years after 20 years of advanced research and development at IBM where I worked on Java since its early days as well as other parallel software and then eight years of basic research at Rice I decided to create these introductory courses for professionals there's nothing like it out there just like this Hawaiian shirt join me in any or all of these four-week courses as we learn the fundamental aspects of parallel concurrent and distributed programming in Java 8 
D9oZ4Pe20sM,28,"common approach in designing parallel languages is to provide some high level handles to manipulate the use of the parallel platform. This exposes some aspects of the target platform, for example, shared vs. distributed memory. It may expose some but not all types of parallelism, for example, data parallelism but not task parallelism. This approach must find a balance between the desire to provide a simple view for the domain expert and provide sufficient power for tuning. This is hard for any given architecture and harder if the language is to apply to a range of architectures. Either simplicity or power is lost. Instead of viewing the language design problem as one of providing the programmer with high level handles, we view the problem as one of designing an interface. On one side of this interface is the programmer (domain expert) who knows the application but needs no knowledge of any aspects of the platform. On the other side of the interface is the performance expert (programmer or program) who demands maximal flexibility for optimizing the mapping to a wide range of target platforms (parallel / serial, shared / distributed, homogeneous / heterogeneous, etc.) but needs no knowledge of the domain. Concurrent Collections (CnC) is based on this separation of concerns. The talk will present CnC and its benefits.",2013-01-22T16:34:35Z,"Knobe, Kathleen - Concurrent Collections (CnC): A new approach to parallel programming",https://i.ytimg.com/vi/D9oZ4Pe20sM/hqdefault.jpg,cscsch,PT1H18M35S,false,850,4,0,0,0,see I worked for a company called compass or Massachusetts computer associates for something like 11 years in the 80s when all of the super computers were big in a previous life and the compiler worked there for the connection machine and mass par numerics Alliance some government contracts and the like and there I learned what a mess the world was I then went back to school to try and straighten the world out I ended up at digital equipments companies Cambridge research lab in Massachusetts that got bought out by a compaq in an HP and then HP drop that lab and now i'm at Intel with some of the others in that lab so that's my history and I guess in fact every step along the way had some impact on what's happening here what I'll talk about today I just realized that these slides have the names of Mario Delmon and Franklin bah who are my colleagues in in Germany who were here visiting and sharing this talk with me and doing some training elsewhere earlier in the week but I have a slide in the end with the full group so so it's not just the three of us working on this project and the topic today is boxes and circles and triangles and if you know what they are then you can leave now no there's a little more to it I'm going to this is a very high level language so I'm going to start out with the charts and graphs because i'm not going to say a lot about the numbers but i just want to dismiss the notion dispel the notion that because it's a high level language it's going to be inefficient that's just not true and these are this is some work done by an intern who was with me she's an i was in app purchase and applications in the applications field and she's at georgia tech with rich mudak so she did some Jaleski experiments these are the matrix sizes here and these other performance numbers so higher is better this is the serial performance these three are silk OpenMP and scale of pack down here and up here cnc is this one the red one here and that's up there with multi threaded and KL and plasma plasma is being done at oakridge you probably know about process shotgun derivate so she she won the best paper award at the recent IDP yes this year for this work so soleski is a piece of the eigen solver so it's used i think three different times within the eigen solver so the eigen solver is quite a bit bigger and here's the baseline here's multi-threaded MKL and here is concurrent collections so it performs well I should say I'm hoping this is interactive it's a small group and I'll take questions any time just shout out okay good on 12 sorry for core processor forces for these are these experiments for these experiments that's what we did you know we certainly plan to work on bigger things maybe with your help you know this is significant difference in 11 lieutenant uh no i don't i do know that we're in discussions with the MKL group and there's some possibility that they will use this approach in some of their in some of their library routines so they've expressed an interest anything else before I move on to say what the model is okay so from my compiler experience I have a kind of a different take on what the problem is what it is we're trying to address and it's quite different from the usual the other models out there in a couple of different ways but let me introduce it first and and then we can talk about what the differences are so this is my perspective on the problem so most serial languages are over constrained they are explicitly cereal cereal languages so if you have two computations a and B and there's no required ordering in a cereal language you need to put a and B in some order and that's unnecessary and then the compiler has to try to figure out that they don't need to be in that order and sometimes that's easy and let me tell you sometimes it's not they also are talking about location so particular location in an array for example so they may have multiple definitions of an array location instead of having talking at a higher level of just values and so if you have a definition in a use and then another definition any use you may be able to you the compiler may be able to figure out that you can you know invert them this way but the possibility of doing them this way might or might not be and so that's a an over constraint and the programmer probably knew what the answer was but the language they have to write in doesn't allow them to tell us the other issue is that as you're going through the program with your control flow you get to some point in in the control you're about to execute the next block there's a question of if you're going to execute the code and when you're going to execute it and in that world the answers to both of those questions are bound together so the answer is yes you are going to execute it because you got right there in the control flow and the answer to when is now so just as you produce data some point in the program you consume at some later time not necessarily immediately we'd like to produce the decision that something will happen and then later on have that thing happened and sort of break apart those two questions so most parallel languages then are embedded within these serial languages so they inherit all the problems of the serial languages and then in addition they are also over constrained in a couple of ways one is that they're there to specific with respect to the type of parallelism maybe data parallelism is what they do or fork joint parallelism is what they do and they might be specific with respect to the type of target maybe it's designed for distributed memory or only shared memory or something like that and yet with all of these constraints they still don't provide enough for the tuning experts who really have control over what's going on so so that's the goal so the solution is to raise the level of the programming model just a bit in fact it's too low it's the level is too low because it's explicitly serial or explicitly parallel and we'd like to raise it to stating only the semantic constraints and nothing more and that's what we're going to do so I've told you but I'll tell you a little more about what the big idea is but then we'll go through some simple examples at the at the language level and then we'll talk a little bit about the execution so the idea is you don't specify what operations happen in parallel this is difficult might not be difficult for you but for a lot of people it's difficult and it also depends on the target and that is true for you I take it you're quite aware of that problem and in addition so instead of doing that instead of specifying what operations run in parallel we'd like to specify just the semantic ordering constraints and nothing else so these constraints are easier you have to know them in order to write a correct serial program and in addition they apply to the application only they don't have anything to do with the hardware so they're simply semantic constraints so the semantic constraints oh that's why I said it's the meaning of the program they require some computation the program the program itself requires some computations to occur before other ones not the program actually the application you know has some inherit ordering inherent ordering so what are the two sources of these constraints well there's the producer consumer relationship so if you have a producer producing some data and a consumer consuming it then that producer has to execute before the consumer that's pretty straightforward and there's a controller controlling relationship if one computation determines whether another computation is going to execute then the controlling the controller has to execute before the controller that's it basically so what this leads to is quite a more significant separation of concerns than we have in today's languages it's not a total separation and there is some communication still required between the domain expert and the tuning expert but there is a lot more separation than you might have thought or then existed in the previous world so the domain expert doesn't have to know a lot about parallelism they have to know about their about their application and they don't have to know anything other than what they already need to know to write the serial version but they do have to write it a little differently so what they do is write a specification a concurrent collection specification and the goal is that that specification is the mapping of the abstract problem the answer these constraints then as a separate activity there is what we call the tuning expert takes that specification and maps it to the target architecture these are two separate pieces so the tuning expert doesn't need to know a lot about the domain and the work of the tuning expert is dealing with the architecture the actual parallelism locality overhead load balancing distribution across the processor scheduling within a processor all that stuff that we know and love always alone donesies great question I didn't plant that question but it's a great question the tuning expert may be the same person at a different time so even in that world they can think for one time only about the e domain and then switch hats and think only about the tuning it might be another person down the hall or you know in some other country who you know actually knows a lot more about tuning unless about the domain it might be a serious compiler that does a lot of analysis and figures out exactly what to do or it might be a dynamic runtime but all of that will call the tuning expert because what it's doing is it doesn't know anything about the in all of those cases it knows nothing about the domain or very little or doesn't have to think about it much in in any case but their focus is on getting the the tuning for that particular architecture anything else okay so the domain expert doesn't need to know a lot about parallelism and the tuning expert does not need to go a lot about the domain when there is some interaction it's at the level of this specification here so you don't have million lines of code to deal with you might have three pages of you know a page and a half of of this specification to deal with and you can communicate at that level and those are the things that you need to know about so what's the variance in the data size of this particular dative structure you know how how big is it usually and how often does it stray from that typical things like that or how long does this step to typically take but you're talking about the specification not about the whole code okay so we have some notation there's a graphical notation which looks kind of like boxes and circles and arrows so when you go to the board and draw describe your application to your colleague you're probably drawing boxes and circles and you draw arrows between them and these are the the things that you're discussing so who is a computation we call it a step computation step X here in this case is a data item and the textual notation is designed to correspond to how you how you draw on the board and now we have this other thing called control tags on which I'll talk about in a minute because that's the new thing here that is a little different are the questions so in this notation that producer-consumer relationship is written like this so there's step one is producing a particular item that's consumed now by step two so we have arrows between them so this is what you might write on the on the board talking to your colleague and the controller control ear elation ship step one is producing some control information into a tag collection a collection of control tags and this producer relationship is similar to this producer relationship so it's just producing something this relationship on the other hand is it's different and the meaning of that I'll tell you in a minute but basically this says if there's a tag here with a certain value than that step having access to that to those values is going to be executed sometime so that's the control information and i'll tell you more about that because as I said it's you so the way I'll explain the tag collections is by starting out with a loop here a loop nest and the body of the loop so we have an iteration space here and this is you know you're probably fairly comfortable with this so there's a iteration space here and as you're going through this loop the iteration space is produced one at a time so it's 1 1 1 1 or 2 or 3 etc and as you produce each one of these you then execute the body of the loop immediately so it's an ordered sequence of these tuples and you're executing the associated body and as I said before the if and the win are tightly bound together okay so if we want this to look to show what this actually means then this is the thing being controlled that's the equivalent of step to up here and this is the set of indices and step one is producing those set of indices but we're going to save them in this tag collection called t2 okay so now it's not a sequence but it's an unordered set so what that means is you're going to put tags into this tag collection in some order each tag at some time it doesn't mean that you're going to execute the code immediately or even in that order on the order is then constrained also by data dependencies but it's it's not in the order that necessarily that you put the tags into the into the collection so these things IJ and K are the tag components so t2 is a tag with three components a value for I value for J and a value for K and when step to execute it has access to those values in the same way that this loop body over here had access to the I J and K values okay so oh the one other thing is we're not talking now just about integers on these things can be you know graph nodes or tree nodes or sets of elements or anything that has an equality operation in your in your domain there's supposed to be meaningful in your domain we know nothing about the types we don't pretend that we're going to have any access to the types but you do need to provide us an equality operation okay so there this is just to point out that there might well be loops and here's a data dependence that also further constrains the ordering but in this case it might fully constrain the ordering but so the fact that a tag is put into this tax base just says that this will execute some time it says nothing about the ordering and the data dependencies for the constrain the order well it constrains the ordering in that you're probably not going to execute this step until that gets the tag gets put their elbow we can also do a speculation although we don't have it implemented yet so the the grain of the step can under your control so it might be the case that the two inner loops are within a step they're not you know outside the step so the step 2 then will take more time it's a bigger computation and the tag components in this case will only be K so there'll be fewer instances of tags but they represent a bigger piece of work and that's all up to you so that's another place where the the target creeps in a little bit you can use the same language and a grain size a decision first of all can be parameterized and secondly it varies much more slowly than the type of parallelism or whether you've got shared or distributed memory or you know that's you know we'd like to get rid of that issue but it is still there on the other thing to say about that is we're working on a hierarchical variant which would have some impact on this and I did have a student who did a thesis on dynamically adjusting the grain so we're on the case but we're not there yet whoops okay oh I'm going backwards okay so some simple examples now so i'll start with Jaleski socha levski probably familiar with it's an array computation there's a serial chill su you here's another case you are you have to tile we're preparing for parallelism so if there's a parallel algorithm that's a tiled algorithm you still have to tile in order to get some value out of that there are people doing research on how to automatically tile and we are some of the people doing some of that research but but in the current system you have to tile it but you don't have to you still don't have to think about what goes before what so you have the serial Jaleski over here each of the tiles in the in the same column do a try self computation and then each and the lower triangular part over here do an update based on the same row again and then I shouldn't say and then but in addition you do a another serial Jaleski and you continue on like that so to understand how to put together on a concurrent collections program I have kind of a recipe that you follow so we'll walk through the recipe so what are the high level operations when you go to the whiteboard and describe to your colleague what are they one is chill esky one is try solve and one is update the next question is what are the chunks of data well in this case it's only this array over here the next question is what are the producer consumer relations among these computations and the data and here they're all three computations are updating the values and getting old values now and the last question is what are the inputs and the outputs well the the inputs and the outputs we represent as these arrows here and the idea is that those are in fact producer-consumer relationships that the environment is producing values to be used by the program and the environment is consuming values to be used by the program so at this point do you need to know the timing of the data or well the tiling is parameterised likely but you you do need to tile it it's your you're kind of getting ready for parallelism it's making the thing parallel ready not all algorithms are tiled but if you want to use a you know a tiling approach and this is one of them then you'd have to Tyler so automatically tiling it is something people are working on now we don't do it yet okay so you might write that previous thing to your colleague as they say on the whiteboard but it's not precise enough to execute yet so what do we need to do to make it precise enough to execute we have to ask a few more questions not many so one of the questions is how do we distinguish among the instances both the instances of the computation and the instances of the data so the idea here is this is not a streaming silo computation where you have it's not necessarily a streaming style computation where you have a computation somewhere and the data is flowing through it but we want at least the ability to take on each of the instances and place them across the machine and across time in order to do that we need to distinguish what these instances are and so it's a single assignment form where each value is going to have its own identifier its own tech now that doesn't mean it executes that way but at the level of the description which we want to be independent of the target you do write it in the single assignment form so for example itter which is usually not seen in the array dimensions you're gonna you're going to actually put here so you can distinguish between the values of in different iterations okay the next question is what are the distinct control tag collections so it turns out in this case that each of the three computations is controlled by three separate control pad collections that's not necessarily true you could have a collection that has identified financial options or jeans or something like that and you want to do a couple of different things with each one of those and we'll see some of those later but in this case we have a collection for each so what steps produce these in this case we have the iteration space for Jaleski coming from the outside the iteration space for the try solve being produced by cholesky and the iteration space for the update being produced by try self so that's all you have to say and nowhere in that discussion have we ever said anything like can you do this try solve computation for the next iteration at the same time you do still doing updates on the previous iteration we never had any of the kind of discussion and yet if you execute this right now on our multi court system that in fact will just happen so part okay so we're essentially making this ready for parallelism we are not talking about parallelism per se at all and we're making it ready for any kind of rescheduling in fact so if you're interested in rescheduling for power or rescheduling for locality on a unit processor for the memory hierarchy we're talking about rescheduling in this language would be totally appropriate for that parallelism is the hot topic of the day so that's the focus of the talk here so without talking about parallelism we get deterministic results and in a race free in the race freeway okay so i think i have two more examples this one is very quick because i'm not going to show you any details this is the whiteboard level basically with some identifiers about how you distinguish these this is a cell tracking algorithm where these are biological cells and they're watching them grow and die and divide and move around and so they're tracking them under various different conditions and trying to see what conditions they do was and they had a serial algorithm that was published and in the process of talking to them about this we found a couple of errors in their serial algorithm which is nice um so it's not even clear where the parallelism is here but there was so the first question after what was the whiteboard drawing a sesh but essentially is you know what is the control space here where k is the number of the frame coming into this analysis so there's a video frame number which is K and of these six computations k controls four of them the other two which are the correction filter and the predication filter are actually are done once per frame / what they think might be a self on this frame so there are many more instances of those and the computation that produces this tag collection that controls this is in fact its arbitrator initial function so it produces it produces this collection which then controls this okay whoops then there's another collection down here which has a similar function but the values are slightly different so this is a new and improved set of cells that it's finding on the you know from this computation it's it's reassessing its what it thinks cells actually are and so then you're doing another computation down here so that's that one any questions there no ah and this I think is the last one I'll show you this is a face detector so the idea here is that there are frames these are probably these might be either still frames or video frames but they're looking for our face and when they take a frame what they actually do is look in all possible square windows in that frame so if you can think of starting from any pixel you know and imagining that that pixel is the upper left corner of a window you know what are all the square frames that you could square windows that you could make out of it so there are a lot of them but i'm just showing a couple here so this is the 11 window this is for overlapping windows and this is nine non-overlapping windows but so they can be whatever and the idea behind this is to minimize the computation because it's very intensive by having a cascade of classifiers these classifiers are generated by machine learning so I don't actually know what they do but there can be hundreds of them and you know they you could think of them as the first one checks to see if there are any eyes if there are no eyes then it can just be dismissed as a non face right off the bat so and then it eliminates any further work and the idea of the machine learning is to quickly eliminate those things that can be quickly eliminated so the way up to the whiteboard drawing we just have this image and we have all this set of classifiers here and the consumer relationships there's no producing here at this level and then we go to the next phase and all the work here is done in the tags so this initial tag said is all the windows of all the frames and they might be coming in continuously the frames classifier for any window in any frame the classifier one checks it may or may not put out its own tag so if it gets in frame seven window 38th then it may or may not put out seven come to 38 but it won't put out anything else now so what that means is that these are a sequence of subsets of each other and so there's less and less work to be done on each of you so what's the parallelism in here the potential parallelism you could do a bunch of classifier one there'll be plenty of parallel work to do when you do that because all the windows you can do all the classifier tues there may be a lot of work to do there but when you get down to a classifier hundred there may be pretty much nothing left and you know it's hard to tell if you should be doing that in parallel or up or you could have a stream of these going down so you know you have a pipeline of them or you could look at how many there are in each of the tax basis how many are left to do and decide which is the appropriate bucket to pick up of which classifier you should execute so there are plenty of different ways to to paralyze this you could separate the tags and do the frames in parallel and the window serially yeah a zillion different ways and which way you choose depends as you know on the architecture but the semantics is all it's written here so this gives you the constraints on the problem so basically there are these actually also voted dependencies are not really should not necessarily say in the video there's no there there are dependent on the data these are depending on the data so in this case it's not statically known we can't do ot1 comes before two for the same window on the same frame but you can do t one you could you can do t1 after t2 if you're doing a different window or different frame the domain programmer just writes the constraints like this for existence of eyes before you Jake oh so that yeah so the per the programmer writes you know the the programmer well in this case it's the machine learning tool I think their rights you know each of these classifiers and they have to then make it i'll show you how they do that they make it look they make it input works to input this image and output maybe this tag depending on what happens so there's a little modification to the code to make it work in this environment but then the domain programmer specifies just these constraints and gives it off to the tuning expert for this machine to decide how to execute it on the machine and again the tuning expert could be a person or a static analyzer a living this is that the question yeah so we have these three objects there are steps and items and tags the steps themselves have tags they are tagged they perform put and get operations on we assume that they are functional in that they have no side effects that are visible outside right now this is a verbal contract and we're hoping to get more checking in there but at this point it's just this is how you should write the code so so be it's the it's that they're functional that the steps are functional and that the the data is single assignment that gives us this determinism so the items are the means of communication between the producer and consumer relationships so they have to be in this dynamic single assignment form and the items are also tagged and the tags are the means of communication that represents the control dependence and that creates our ability to distinguish the if from the wind something's going to execute yes that's it there so there are three types of objects and there are three types of relations there's the producer relation the consumer relation oh I'm sorry the prescription relation prescription is our term for control and in the cell tracking for instance there was a control collection that represented the different frames that were coming in and it controlled more than one computation I think it controlled for computations right so that's possible there on for the consumer relationship it's possible for a step to consume from more than one item collection so a step can do a get on X's and another get unwise and even within a collection step can do a get on on number of different elements so you can do a nearest-neighbor computation you can get north south east and west you like that whatever function you want to write here okay and similar is true for the producer relationships they can produce into more than one collection and get more than put more than one item in a collection so between any two steps there are only eight different things that can happen so four of them are here they can it's possible that there's a data dependence or control dependence whoops so in this case there's both data dependence and a control dependence in this case there's neither here there's just a control dependence and here there's just the data dependence okay now this was when we have two distinct step collections and we're talking about instances between them but the other possibility is that these two instances are in fact in the same step collection and we get the same four possibilities they can have both they can have neither so since this is from one instance to another you don't see any cycles here they can have just the control dependence or just a data dependence and I'm not sure we should be over at two over at 30 we have plan okay okay okay i'm trying to think of this there are a few slides at the end that I will am I going to see you later I can just show okay then one of them do i think is too should i do that that will show up this video why don't I go to it I mean I think it'll make more sense if I go through relationships here in the depends supposing I just created in thread pool and I assign lots and lots of job effectively patches and threads take those jobs and the dependencies are encoded in the description of the thread of the description of the job because I know that I can't offer and listened to last under the threads know that and they're kind of independent is the advantage of because i wanted it seems to me that would effectively do exactly the same thing the advantage of this is that it's somehow specified in the I mean with this produce a different result than a traditional thread pool with work Steven Adams explicitly defined as the language if you follow you on your defining it explicitly so one one thing that we can do here we can do a lot of different things here this just defines the constraints so in the back end we could have a runtime system which does kind of what you're talking about we could have a runtime system that Maps everything statically distribution and scheduling and does all of that statically if the computation is amenable to that we could have something that works in a you know a shared memory system in a distributed memory system all we're specifying here is the constraints what you're specifying there maybe one of the possible runtime systems that we could have and because it's at a lower level than this be used I think so so maybe I I believe that's the case if not let's talk after so I think and then and the user doesn't think about threads here we have no we haven't said anything about threads we haven't said anything about where when everyone simply don't believe the dreadful until condition for into random met and that naturally it like a true words out which objects objects it created hasn't when there they can be created and executed whatever so our current runtime system works very much like that it sits on top of TBB but it's much easier to write then I mean that for the domain expert it's a lot easier to say they don't have to talk about threats at all they don't have to to worry about changes in their environment okay good that's okay I can do that yeah well the thing is so it's we have a bunch of different implementations the one that's on the website is in C++ we work closely with rice with the vexa car and he has a different implementation his focuses in Java that runtime is in Java we have two preliminary implementations that I'm not sure exactly how real they are one of them is by a colleague of mine that's in Haskell and one of them is by someone at rice again and it's in.net and so in theory from the CNC perspective it could be anything and I know for train is a big issue here a big language and we've been talking about a Fortran implementation with our nashua team who does the fortran compiler so the steps of the so-called inside so there is any there the the language you write the step in is any of these languages and then there would be an API call to a put or get and some structure that says this is a step around the outside but I'll show you in a second so if we take a look at this little piece of the sub graph for the Jaleski thing on the the producer consumer relationship here is represented here by arrow so since this is square brackets that has an arrow into these params this is a consumer relationship so try solve is consuming the array in this one try solve is producing the array and here we have a comma which means that this try solve is consuming both of these instances for example so the control relationship which so you'll be sending one of the whole side no this is just the language in and of itself is a tiny language so whether you overnight reduce this into a Volga I'll show you how it goes so this is just this dotted line here so this represents the the control relationship and then in addition we have some declarations which are types so here the the array subscripts are integers and this is the type of the contents of the of the item so this is a language in itself and we what we do is take this and translate it for you into something that looks like a step so we know from from the relationships here that this try self step is going to do a get on the array and what it's going to produce we don't know what you're going to store it into but we know the type of the thing you're going to store it into so we give you a hint here that that's code you need to put in and we generate this you know just standard stuff and we also know that you're going to do a put of the array we don't know the tag yet but we know you're going to do the put and we don't know the name of the local variable here that's going to have those values so we can tell you that this is the form of your step and then all your implementation logic goes in your language which would be onion in this it's going to be c++ but if we for the Java thing this generates done what was on the previous slide and then generating to skeletal right okay yeah and then you'll go more than 14 senior yeah now it's a little like so so then there's a class library that implements C&C and you can write directly in that class library so that's why when you said is this language I said it's kind of a model to me so you can get into it by writing directly in the class language which you might want to only do after you've done this a few times you can write in the specification language which gives you some nice documentation and as the project proceeds will have more and more static analysis and checking and era you know locating stuff like that or we also have started an intern with us has started a a GUI that looks like boxes and circles and so you can drop and Dragon and create it that way but the the model is still the model so you have a way I think I kind of get the deep choke but the question is now where do i as a programmer and only talking okay the length of your make the language or whatever you call wonderful question on right now what I would like to do is have a debugger which is exactly at the level so you say step through the program and what that does is execute a step a CNC step and you might provide whatever you want for visualizing a data item you know whatever that would mean and you could set break points at puts and gets and you know it sort of understand this higher level we don't have that yet what we do have is some tracing facilities and you can always use your standard debugger the same story is true with performance you know we have intel has a lot of performance tools you can use all of those performance tools on this but i'd like to have tool that knows about C&C and is at this higher level and says you know this producer-consumer relationship is the one that has all the communication associated with so so I definitely like to do that and the rest of the team is anxious to do that as well so let me see then you generate the scope and the use of the domain is that is going to hide in some yeah right the gospel in this particular case all right that you will compile that's this thing so this green is the user code you put in the textual graph if that's how you're getting into the model and you also put in you know after the textual graph creates some hints file or maybe directly you put in the code for your steps this does a translation which produces not only the step framework the skeleton but it also produces some header information that gets compiled I have to say by any secret pilot and we also link that with the existing cnc library okay and that comes out i hope i answered ok so I ok is this going to work on inside that phone so it within the Intel all or you're planning to make it like could i use it together with Jesus gigi does Dawson and you know yes whatever Linux yes so we have a linux and windows version of the runtime and this is any compiler so that will work fine on we don't have all that we don't have a Fortran version okay so i think but i think given that we had yeah okay so i was at the eight different relationships here and i'm going to show you examples just from two of them so this one is the relationship where there's two different steps and there's a controller controlling relation between them so what might that be it might correspond to what you think of as an if statement so s 1 of jay is producing maybe or maybe not producing t2 of j so it will have access to this if it executes it will have access to exactly the same control information on it might be that this s1 is occurring within the context of a an outer loop and for an instance of that outer loop it's producing an inner loop so if you have a que loop and you want to put a J loop inside then for each value of K you're going to produce a bunch of values of j so whoops i use J&I here but i think you can translate and another option is that you have some tag controlling the step and you're inputting data and from that data you're producing some entirely different tag thats probably data dependent so so this one thing just says s1 controls s2 but it doesn't say exactly how but when you write it in in this form with the Ted components and the tag functions which tell you how to transform the tags then you get some more concrete information about what's going on so here's one more which is interesting on this is this is the second one and I won't talk about any more of them so this is the two instances are from the same step collection and there's both a control and a data relationship between them so suppose this is a while loop and here's some code from a while loop so you have the body of the loop you have some data and you have the control on and the input to the while loop is the initial value and I think this is written in white up there so you can't read it I'm sorry and in addition to producing some more data it's also producing some control so if the if test passed passes in the while loop it's probably put out some data and it's going to put out a tag that says do the next instance okay and you're probably going to put out the output items or at least the one the final one and you may also want to tags or some reason okay so that's a while loop but this same story goes with a divide or dividing hunger so does your language have a teacher to detect a masseuse no you can write a an endless program and in fact you can write a program that has finite input and you can write an endless or even no input and has an infinite loop in it and that might not be an erroneous program that might be exactly what you want to compute all the primes you know you don't need any input to that and you just go through forever right or you might have a program like something that does video some kind of video detection surveillance or something that just has continuous input and that's the intent of the program so for finite but we do have at least one continuous program there are some issues including how you know something is dead which are not well addressed in the continuous program and you can't go on forever you know unless you reclaim so there are some interesting issues there so we would need what I call a different variant of the language in order to support that but there's that's the other reason that I don't call it a language because we have a bunch of different variants in mind one for instance that does continuous application one that so that's a little broader than the language I presented a little more constrained is one that only deals with totally analyzable relationships so if the functions that relate the producer and consumer to the step then are totally analyzable then we might want to have a variant that only allows that so in there are other variants as well that we might have we have a customer a potential customer who's interested in real-time scheduler and so that's fine with us but we might have to make a variant for them so that it's a model as opposed to a language in that so they know you come to me the right ways free yeah that wouldn't be true in the soft real-time oh how do you do it because they we have a proof of it actually I don't know if you know Yin spells Berg he he's a theory type in the programming world and he has a nice proof that because of the single is the dynamic single assignment you can never prove Purdue each value is associated with only one each tag is associated with only one value and because of the side-effect free aspect of the steps you can't do anything in one schedule that you wouldn't have happened in a different schedule so so you'll get exactly the same thing whether you execute on a unit processor on a GPU or on a thousand course or you know whatever it is that every step excuse will give you localize memory model is not allowed to in theory that's true as i said the right now we do don't do any checking to make sure that that's true so it's a verbal agreement it's right it's it's stated clearly but it's right now we don't do any checking Yeah right right if you're a good boy it will take care of you but what that means is there's a programming bug if there's a race well there's always means that but it's you know it's a local bug within a single step that is causing the problem it's not an interaction between two of them so do you add any kind of I'm still thinking of them kind of threat rules yeah prioritization so that these things can happen any order but if you did this one plus it will be coming you so that's entirely that is the role of the tuning expert that's the tuning expert and with whether it's a scheduler or a person or whatever that's their role to map dealing with priorities we have a priority based scheduler except that we transition and any of the syntax you should have service that's why the yeah that's why this is a different slice across the problem so we have a single language that allows you to describe your domain and then although we might have a we're working on a language which would apply for scheduling across domains but on any particular hardware you may well want some very special purpose kind of scheduling like these people who want a real-time scheduler and that's perfectly fine oh we're talking about at the domain level is the constraints so we don't we haven't said anything about scheduling in we have five of them and that's well we have five of them and we can choose between them I would like to make that option freely available to you right now it's not but one of them is a priority based scheduler which we're happy to feed through your priorities but TBB isn't happy to do anything with them right now so so that doesn't work too well yet but but it might anything more should I go through the divide and conquer okay so remember the while loop so here's a divide-and-conquer application and again this here looks very much like it did before so we have the code in the middle there's some tech collection that defines it and an item that's consumed the root tag and the root item are probably coming in from the environment or from somewhere else in the code and then when the divide happens instead of as with a while instead of putting out one item in one tag it's probably putting out two or three so it's putting out its children so it might be right child left child for the tag and for right Child Left I'll for the item as well but this oh and then you probably want to output the leaf and this looks a lot like you know if you squint and you don't look at the tag the tag functions then it looks identical so and in fact if you look at a conqueror part it again looks identical at this level but the conquer step is now in fact going to be taking in two children and putting out one so it's a little different in that sense and you probably want to connect these two up so you want the the leaves of the tree going over here and belief tags going over there as well okay so divide conquer and while all look identical okay so this is the part I just went over before so I can I skip over that okay so we have a runtime library in Windows and Linux we have this translator that creates the hints file for you we have a visual studio integration we have some documentation there's some samples on the web that you can look at this is the build structure that I defined before this is sort of doesn't need to be said but we do sit on top of the thread building blocks in this current implementation so these are all the ways that I don't think of it as a language as a model so it's the to representation you can you can write directly in the class library and you can write in the in this graphical interface and also we have this idea of a number of different variants and in fact some of the implementations that we've had starting back when they started at HP have had quite different trade-offs between efficiency ease of use generality and the kind of guarantees that we can make and probably a few other things so making different set of trade-offs would probably be a different language but the same model and oh so these are the three variants that I talked about in before so this is how you think about the execution of your program the program is declared in this declarative way and so you need a way of thinking about what's actually going on and the way I think about it is that as you execute the program the state of a given instance an instance of a step an instance of an item or an instance of a tag goes through some different stages in its lifetime so it's they start out with having no attributes they don't start out I guess and when they acquire an attribute they might you might think of them coming into existence at that point so an item instance may become available at some time if it's that produces an item then that item is available if a step produces a tag then that tag is available so these God's just represent an attribute that that the instance acquires steps are a little more complicated so this is the same step as it goes through its lifetime it may acquire distinct attributes so it can start it can be prescribed that is its control can be available its might have its inputs available and those can happen in either order we there's no particular order it has to happen in but when both of them are true then we call the step enabled enabled just means it's ready to run it's supposed to run and all its inputs are there so it's ready to run at that time and at some later time it will be executed so once it's executed as I said a an instance comes into existence when it has any attribute you can think of it that way and when it's when a step is executed then we don't have to think about it anymore so this stage between having some attribute but before it's executed is what we call the execution frontier it's the stuff that we have to worry about now and step instances go in and out of the execution frontier but also item instances go in and out of the execution frontier so an item can become dead and a tag instance can become dead as well so that's sort of the conceptual model um I don't know if you want to go through this there are three sort of historical models that we sort of have been influenced by so streaming data flow and tuple spaces and I can go through their properties or we can skip past this if you like interest data flow streaming and tuple spaces like Linda can we talk about this a little question you've been influenced by artists work at MIT with parallel hostile elicit parallel programming paradigms well it it is influenced by data flow style yeah so if you think about the execution model it's very data flow style ish but so there's no extra serialization you only need to execute when the data is ready and the steps are functional but we've elevated control to first class so these peg concepts are first class and you can compute and manipulate them yeah you have a question no ok so the semantics are data flow and I describe how I think about the program executing it's a dataflow style of thought on the other hand we can have any style of execution so i don't know if i have this slide here but you can have a dataflow style execution which is kind of like the one we have now but you can also statically determinate is when we had at HP and you can do something in the middle we had one where the address base was determined by the the tuning expert but the the tuning expert who was a person but the tuning expert who was the runtime system determined within each address space what a dataflow style computation would be of the program so it was kind of a combination of both so although its data flow one level you know it's not really add another okay so here are a bunch of just to drive home the fact that the this back ends could be a lot of different things to define a back end you might want to define three different things how the grain size is determined how the distribution across the processors is determined and how the scheduling within the processor is determined so at HP this stuff was called T streams for tagged streams on and so they had distributed memory systems that they were focusing on and so the distribution in those was static was provided by the tuning expert the person and we had two different scheduling approaches one was kind of a data flow like within as I said and the other one was entirely static so there was almost zero overhead in the runtime system this was only for analyzable programs at Intel we have a variance that looks like this the rice one looks similar and we're trying to as i mentioned allow you to plug in your own schedule but it's not as easy as it would look given the current implementation whoops at Georgia Tech I had a PhD student who was working on dynamically adjusting the grain so we had a totally dynamic system that he built at Georgia Tech and was presented at LCP see so the back end could there's a wide variety of ways that you could do the back in here and that's all under the purview of the just B's would be like CC partitions and different front-end systems but the description of the model is this let's see the HP one was C++ the the rice one is Java I think this was siiick les plus also what also a rival these api directly and you said you because then you're totally independent from the language yes well the API is C++ based it's you know sort of method based but so we could see you're thinking of Fortran is that what you're thinking yes the in another language we would have probably an API that has similar functionality but is written with different dots and / ends and you know so it plays well with a variety of tuning experts I've already said this a few times it could be the domain expert only later it could be a different person with different expertise it could be a static analysis or a dynamic runtime we're only talking here about constraints that's all you write in the language are the semantic constraints so this maximizes the flexibility for the tuning expert we don't have any unnecessary constraints here and we're not only writing those constraints that are relevant for a particular hardware we're just writing the semantic constraints so the possible goals include maximizing parallelism minimizing the latency maximizing the utilization predictability memory footprint power consumption you know anything that requires reordering and any optimization you want to do on one system that would differ on another system it might be a good idea to represent the application in this way the possible styles of parallelism might be loop or data parallelism on the one hand task parallelism pipeline parallelism fork join parallelism you know whatever we have no particular bias in any way one or the other all of these are totally appropriate so I already said you can you might use it for power or for locality on a unit processor even that would make sense so it plays well with a lot of different serial languages you know can add Fortran to the fact I thought it would play well with that I don't know we I don't think it would take a lot but it plays nicely with a lot of target architectures so we at Intel have had a shared memory system the current new release has a distributed memory system on it homogeneous or heterogeneous flat or hierarchical we don't yet have a hierarchical cnc description and that would work better when we do that so this is the community I mentioned the the two people that were on the initial slide are certainly on here but let's see I this Jeff alani is the head of the group but it's not that he's a manager he saw the piece that we brought over from HP and when TBB was created he dove in and ripped out the preview scheduler and put TBD in and then all of a sudden we had a windows version we could use so that was nice on and these whoops these other people are also in the same group this is people from the Intel compiler group so this is not a product and there's no guarantee that it will become a product but it's making its way through that process and in fact these people are in the product group working on putting out the release under what if on the Intel site that I'll tell you about in a minute and these are the two HP people that I worked with and these are the people from rice from Georgia Tech this is the intern and she's working with rich go doc you may have heard of the vex a car he does a lot in the high-performance world right so he's the one who took this on and is building the java implementation and in fact they have a an NSF grant the National Science Foundation grant to do some medical imaging work using concurrent collections and no one had Intel including me is even on that grant so I thought that was nice yes yes exactly yeah so that's the Java background too yeah he did his own predecessor 2x10 forgotten the name of it you know oh yeah so this is habanero and the other one was jalapeno jalapeno yes okay and Nikolai was an intern in our Novosibirsk office and he's now a full-time employee he's terrific and yen's is the theory person that I mentioned who did the the proof all I can send it to you but it's not published so give me your name afterwards and I will be happy to do that and for those of you who like to travel l CPC languages and compilers for parallel computing is happening at rice and we decided to do the second annual cnc workshop there we had one in Hudson last year which was the first and it was quite informal and this is going to be a little more formal so but not very formal it's a workshop co-located with the bigger workshop so if you happen to be in the area and you want to show up or if you're sending something to LC pc drop by let us know and so this is where you find the the Intel version and this is where you find the rice version you 
B1zZaTfS_Vk,26,#Amdahl's #law  #concurrent #parallel #processing #speedup #explained #with #example #karanjetlilive #it #lectures#programming,2020-02-07T12:23:45Z,Amdahl's law and speedup in concurrent and parallel processing explained with example,https://i.ytimg.com/vi/B1zZaTfS_Vk/hqdefault.jpg,Karan Jetli Live,PT19M54S,false,17329,640,14,0,111,Karen Kackley life hi guys welcome to my channel and today in this video we are going to discuss angles law so guys if you are studying concurrent programming or you're studying parallel processing all of you must be aware of this and those law so today in this video we are going to discuss what is Anders law and how it affects the speed-up and performance in a multiprocessor environment so guys for the full video all of you stay tuned so guys before we start with angles law I would like to discuss something called as speed up with you speed up so guys what is a speed up now in speed up this says if single-processor finishes one program in one unit of time or in some amount of time so then how much time will multiple processors require to finish that class ok for example if we have one processor ok if the number of processors let it be donated by and if the number of processor is n and is equal to 1 okay so then how much time that processor will take to finish one task ok for example for one processor that time is t1 for one processor the time is t1 so suppose it takes 4 in our example suppose one processor takes one second to finish one task okay so one processor takes one second to finish one task then how much time how much time and the processors will take to finish that task obviously if we do the simple maths so if the number of processors are n then that time required to finish that task becomes 1 by n isn't it if one processor one task and one second okay now suppose if we have two processors we have two processors for example n becomes 2 so ideally ideally so time taken by n processor should be 1 by 2 for example 0.5 seconds that is 0.5 second obviously how much is speed up speed up is double the speed up is now what is the speed up speed up is time required to finish some tasks by one processor divided by the time required to finish the same task by and processor for one processor it is 1 for n processor it is 0.5 so then what is the speed up time required by one processor 1 divided by time required by n processor now if the number of processors are 2.5 so speed up is 2 so speed up is 2 and guys if we pay attention to it it makes a sense it makes sense if one processor is finishing something in 1 second 2 processors ideally should finish that in half amount of time and the speed-up is 2 so so it should be double fast as compared to the single processor ok so this guy's is called as speed-up okay and when we follow this approach for the speed-up okay so what do you feel is the speed-up should be linear linear means if the number of processors increase the speed up should also increase or the processing time should decrease in a linear fashion for example if we take a graph ok if we make a graph ok so on 1 spy on x-axis we have number of sorry on x-axis we have number of processors okay and here is the speed-up okay so if one processor time taken one if two processor so then so okay it should spiritual keep on going up up up isn't it here are the number of processors and here is the speed-up so more the number as the processor number increases the speed also increases the speed also increases okay ideally it should be like this but guys in the real time okay in real time this is not a large case in a real time that we cut it with the red pen in real time this is not the case so what happens in a real time what happens in a real time in real time we have something called as angles law we have something called as and the law which says this is not the case okay so only the number of increase in the number of processors will not increase the speed of processing okay and those losses there are another factor factors involved which can affect the no not which can which effects the speed-up which affects the speed-up and those factors those factors are the factors which state how much part of the program which is being executed is a serial and how much part of the program which is being executed is there how much so these two factors they affect the speed up these two factors they affect the speed of okay so guys before see it further quickly I will tell you serial part and parallel-park know what happens what happens now when we use you know in a program okay we use many different types of structures okay so basically our program is made up of data structures some processing something something and many things right so when we talk about serial part when we talk about serial product in our program if we are using okay some serial operations like traversing a linkedlist and all of you know Legolas is a serial data structure okay so there is no way we can traverse a linked list in a parallel fashion as a net for example we have linked lists we have arrays isn't it so all these they contribute to the serial part of the program isn't it and at the same time in the word program if we are using graphs or they are using please all these are parallel structures okay so they contribute to the parallel part of the program so in theory but what happens we have to wait for one instruction to finish until we can start the another instruction in the serial fashion one by one or one after another but in parallel part we can perform concurrent operations in parallel part we can perform concurrent operations so guys and the law say the speed-up is dependent upon how much percentage of the program is serial and how much percentage of the program is parallel how much percentage of the program is parallel okay and the next part of this tutorial we are going to discuss and those law and we will see how it the parallel and the Styrian percentages effects the speed-up so there is not coming to and the as I discussed few minutes back so what is anthems law it states the speed-up is dependent upon two factors okay actually it is dependent upon three factors the part of the program serials the part of the program which is parallel and total number of processors which is donated by and let us donate the serial part by s let us donate the parallel part of the program or the parallel percentage by P and let us donate the number of processors by M so guys if we have one single processor if we have one single processor okay p1 okay so then how much time it will take to execute the program it will take time required to execute the serial part plus time required to execute the pablum if the number of processor n is equal to 1 if n is equal to 1 okay so if we have n number of processors we have n number of processors as we discussed already discussed if we have T n so then this is our new formula that this is our new formula then it will be total number of time required to process the serial part plus the total number of time required to process the fabula part / and / and so then what it will be S Plus P divided by and S Plus P divided by and so then what it will be it will be for the and the processor and what this equation will be used it will be used for a single processor and this equation will be used for n process okay so now guys now guys if DC total number of time used by a single let it be one let it be one okay suppose T 1 is 1 so then T 1 is equal to S Plus P t1 is equal to s plus P and from here we can compute s is equal to 1 minus s is equal to 1 minus P means serial part is equal to 1 minus the parallel part CL part is equal to 1 minus the parallel part then we can replace this equation in this then we can replace this equation in this so then what do we get T and what do we get as T and bigoted as T and is equal to 1 minus P plus P divided by and 1 minus P plus PP divided by n so this is the total time required this was the total time required to finish a task by n processor and we divide as a program and it is dependent upon how much part of the program is a serial and how much part of the program is parallel and what does this this is called as endles law okay so what we were thinking so if total time required by one processor is 1 and for any processor it will be 1 divided by n but angles say no it will not work this way so total processor total time required to finish the task is dependent on number of processor plus is dependent upon how much percentage of the program is serial and how much percentage of the program is parallel no guys in this case what is the speed-up in this case what is speed-up so speed-up will be how much time single processor does to finish our task as compared to how much time people processors stuff to finish a task so what will be speed up according to angles nah it will be 1 if 1 is the time required by a single processor divided by time required by multiple processor 1 minus P plus P divided by n so this will be the actual speed up and this is what is happening in the reality this is what is happening in reality so guys I hope you understand up to here I hope you understand up to here ok so now we will do some examples now we will do some examples and in that examples I will show you soho actual speed up weekend okay how much in reality the speed increases by increasing the number of processors so guys so guys in this example let us assume this so let us assume the total number of processors are 10 the percentage of the program which is CB parallel is 60 and the percentage of the program which is sequential is 40 so then what will be the speed of the speed of formula says 1 divided by 1 minus P plus P by n ok so 1 divided by 1 minus P so this is percentage when we computed for 1 it becomes point 6 so 1 divided by 1 minus 0.6 plus P divided by N and what is n here n is 10 so 1 minus 0.6 plus 0.6 by 10 gives you the speed up of 2 point 1 7 ok so two point one seven guys in the beginning what we thought in the beginning what we thought if one processor is doing some work in one second and the 10 processor should finish it by tenth of that second but in reality how much is the speed of meeting the team with 2.17 only double by increasing the size of processors number of processors 910 times we can only achieve double the speed okay no similarly guys now let us keep the number of processors as 10 and let us assume the parallel and sequential part let the family in T let the parallel part be 80 and let the sequential part be 20 now what they are saying now this program is more of parallel 80 percent of the program is parallel and 20 percent of the program is sequential so then what we get here is 1 minus point 8 plus 0.08 divided by 1 minus 0.8 plus 0.8 divided by 10 so then what will be the speed up after doing the maths the speed of the V 3 point 5 7 the speed up will be 3 point 5 7 so then what will be the speed up here the speed up is going to be now let us write two point one seven two then three point five okay in this case in this case now what happened now we got little higher speed up or we got little higher speed up because the program is more parallel and less concurrent okay now let us assume let the program the 90 percent parallel and 10 percent sequential 90% parallel and 10% sequential so what comes here is one minus 0.9 what comes here is 0.9 divided by 10 so then what will be the speed up it will be about five point two six about five point two six so then if the 90% of the program is parallel still ten processors cannot make it faster and times sales still ten processes cannot make it faster ten times they can only make it faster five times so then what is the speed-up in this example it will be five point two six no guys again let us assume 99% of the program is parallel which in reality which is impossible 99% of the program is parallel and point one percent of the program is sequential and point one percent of the program is sequential so here comes one minus one point point nine nine and here comes point nine nine divided by ten so after doing the maths what will be the speed up speed up will be nine point two one seven okay so guys now if you look at this scenario if we 99 percent of the program is parallel then the speed-up again is not done so then the speed-up we can achieve up to nine times okay but which is a very very rare case or I would say it is an impossible case we cannot have a program which is 99 percent further but still in this scenario what will be the speed up nine point one seven nine point one seven so guys I hope I made myself feel and I hope you understand and ins law and the variables which affect the performance and speed up okay so there's that's all for today so this was emblems law and it is used a lot and concurrent and valued processing I hope you understand it and guys if you like my videos please subscribe to my channel and I'll be uploading more and more video tutorials on the topics related to information technology and all of you thanks for watching and stay tuned and guys before I Eve if you have any comment or feedback please leave it in the comment section I will try to respond to each and every comment as soon as possible so all of you guys thanks for watching and see you next time 
